<doc id="19719" url="https://en.wikipedia.org/wiki?curid=19719" title="Filter (mathematics)">
Filter (mathematics)

In mathematics, a filter is a special subset of a partially ordered set. For example, the power set of some set, partially ordered by set inclusion, is a filter. Filters appear in order and lattice theory, but can also be found in topology from where they originate. The dual notion of a filter is an ideal.

Filters were introduced by Henri Cartan in 1937 and subsequently used by Bourbaki in their book "Topologie Générale" as an alternative to the similar notion of a net developed in 1922 by E. H. Moore and H. L. Smith.

Intuitively, a filter in a partially ordered set ("poset"), X, is a subset of X that includes as members those elements that are large enough to satisfy some criterion. For example, if "x" is an element of the poset, then the set of elements that are above "x" is a filter, called the principal filter at "x". (Notice that if "x" and "y" are incomparable elements of the poset, then neither of the principal filters at "x" and "y" is contained in the other one, and conversely.)

Similarly, a filter on a set contains those subsets that are sufficiently large to contain "something". For example, if the set is the real line and "x" is one of its points, then the family of sets that include "x" in their interior is a filter, called the filter of neighbourhoods of "x". (Notice that the "thing" in this case is slightly larger than "x", but it still doesn't contain any other specific point of the line.)

The above interpretations do not really, without elaboration, explain the condition 2. of the general definition of filter (see below). For, why should two "large enough" things contain a "common" "large enough" thing? (Note, however, that they do explain conditions 1 and 3: Clearly the empty set is not "large enough", and clearly the collection of "large enough" things should be "upward closed".)

Alternatively, a filter can be viewed as a "locating scheme": Suppose we try to locate something (a point or a subset) in the space X. Call a filter the "collection of subsets of X that might contain "what we are looking for"." Then this "filter" should possess the following natural structure: 1. Empty set cannot contain anything so it will not belong to our filter. 2. If two subsets, E and F, both might contain "what we are looking for", then so might their intersection. Thus our filter should be closed with respect to finite intersection. 3. If a set E might contain "what we are looking for", so might any superset of it. Thus our filter is upward closed.

An ultrafilter can be viewed as a "perfect locating scheme" where "each" subset E of the space X can be used in deciding whether "what we are looking for" might lie in E.

From this interpretation, compactness (see the mathematical characterization below) can be viewed as the property that "no location scheme can end up with nothing", or, to put it another way, "we will always find something".

The mathematical notion of filter provides a precise language to treat these situations in a rigorous and general way, which is useful in analysis, general topology and logic.

A subset "F" of a partially ordered set ("P",≤) is a filter if the following conditions hold:


A filter is proper if it is not equal to the whole set "P". This condition is sometimes added to the definition of a filter.

While the above definition is the most general way to define a filter for arbitrary posets, it was originally defined for lattices only. In this case, the above definition can be characterized by the following equivalent statement:
A subset "F" of a lattice ("P",≤) is a filter, if and only if it is an upper set that is closed under finite intersection (infima or meet), i.e., for all "x", "y" in "F", we find that "x" ∧ "y" is also in "F".

The smallest filter that contains a given element "p" is a principal filter and "p" is a principal element in this situation. The principal filter for "p" is just given by the set formula_1 and is denoted by prefixing "p" with an upward arrow: 

The dual notion of a filter, i.e. the concept obtained by reversing all ≤ and exchanging ∧ with ∨, is ideal. Because of this duality, the discussion of filters usually boils down to the discussion of ideals. Hence, most additional information on this topic (including the definition of maximal filters and prime filters) is to be found in the article on ideals. There is a separate article on ultrafilters.

A special case of a filter is a filter defined on a set. Given a set "S", a partial ordering ⊆ can be defined on the powerset P("S") by subset inclusion, turning (P("S"),⊆) into a lattice. Define a filter "F" on "S" as a nonempty subset of P("S") with the following properties:


If the empty set is not in "F", we say "F" is a proper filter. 

The first two properties imply that a filter on a set has the finite intersection property. With this definition, a filter on a set is indeed a filter. The only nonproper filter on "S" is P("S").

A filter base (or filter basis) is a subset "B" of P("S") with the following properties:

Given a filter base "B", the filter generated or spanned by "B" is defined as the minimum filter containing "B". It is the family of all the subsets of "S" which contain a member of "B". Every filter is also a filter base, so the process of passing from filter base to filter may be viewed as a sort of completion.

If "B" and "C" are two filter bases on "S", one says "C" is finer than "B" (or that "C" is a refinement of "B") if for each "B" ∈ "B", there is a "C" ∈ "C" such that "C" ⊆ "B". If also "B" is finer than "C", one says that they are equivalent filter bases.

For any subset "T" of P("S") there is a smallest (possibly nonproper) filter "F" containing "T", called the filter generated or spanned by "T". It is constructed by taking all finite intersections of "T", which then form a filter base for "F". This filter is proper if and only if any finite intersection of elements of "T" is non-empty, and in that case we say that "T" is a filter subbase.


For any filter "F" on a set "S", the set function defined by
is finitely additive — a "measure" if that term is construed rather loosely. Therefore the statement

can be considered somewhat analogous to the statement that φ holds "almost everywhere". That interpretation of membership in a filter is used (for motivation, although it is not needed for actual "proofs") in the theory of ultraproducts in model theory, a branch of mathematical logic.

In topology and analysis, filters are used to define convergence in a manner similar to the role of sequences in a metric space.

In topology and related areas of mathematics, a filter is a generalization of a net. Both nets and filters provide very general contexts to unify the various notions of limit to arbitrary topological spaces.

A sequence is usually indexed by the natural numbers, which are a totally ordered set. Thus, limits in first-countable spaces can be described by sequences. However, if the space is not first-countable, nets or filters must be used. Nets generalize the notion of a sequence by requiring the index set simply be a directed set. Filters can be thought of as sets built from multiple nets. Therefore, both the limit of a filter and the limit of a net are conceptually the same as the limit of a sequence.

Let "X" be a topological space and "x" a point of "X".


Let "X" be a topological space and "x" a point of "X".


Indeed:

(i) implies (ii): if "F" is a filter base satisfying the properties of (i), then the filter associated to "F" satisfies the properties of (ii).

(ii) implies (iii): if "U" is any open neighborhood of "x" then by the definition of convergence "U" contains an element of "F"; since also "Y" is an element of "F", 
"U" and "Y" have nonempty intersection.

(iii) implies (i): Define formula_9. Then "F" is a filter base satisfying the properties of (i).

Let "X" be a topological space and "x" a point of "X".


Let "X" be a topological space.


Let formula_11, formula_12 be topological spaces. Let formula_13 be a filter base on formula_11 and formula_15 be a function. The image of formula_13 under formula_17 is defined as the set formula_18. The image is denoted formula_19 and forms a filter base on formula_12. 

Let formula_25 be a metric space.


</doc>
<doc id="19722" url="https://en.wikipedia.org/wiki?curid=19722" title="Metallurgy">
Metallurgy

Metallurgy is a domain of materials science and engineering that studies the physical and chemical behavior of metallic elements, their inter-metallic compounds, and their mixtures, which are called alloys. Metallurgy is used to separate metals from their ore. Metallurgy is also the technology of metals: the way in which science is applied to the production of metals, and the engineering of metal components for usage in products for consumers and manufacturers. The production of metals involves the processing of ores to extract the metal they contain, and the mixture of metals, sometimes with other elements, to produce alloys. Metallurgy is distinguished from the craft of metalworking, although metalworking relies on metallurgy, as medicine relies on medical science, for technical advancement. The science of metallurgy is subdivided into chemical metallurgy and physical metallurgy.

Metallurgy is subdivided into ferrous metallurgy (also known as "black metallurgy") and non-ferrous metallurgy (also known as "colored metallurgy"). 
Ferrous metallurgy involves processes and alloys based on iron while non-ferrous metallurgy involves processes and alloys based on other metals. The production of ferrous metals accounts for 95 percent of world metal production.

The roots of "metallurgy" derive from Ancient Greek: μεταλλουργός, "metallourgós", "worker in metal", from μέταλλον, "métallon", "metal" + ἔργον, "érgon", "work".

The word was originally an alchemist's term for the extraction of metals from minerals, the ending "-urgy" signifying a process, especially manufacturing: it was discussed in this sense in the 1797 Encyclopædia Britannica. In the late 19th century it was extended to the more general scientific study of metals, alloys, and related processes.

In English, the pronunciation is the more common one in the UK and Commonwealth. The pronunciation is the more common one in the USA, and is the first-listed variant in various American dictionaries (e.g., "Merriam-Webster Collegiate", "American Heritage").

The earliest recorded metal employed by humans appears to be gold, which can be found free or "native". Small amounts of natural gold have been found in Spanish caves used during the late Paleolithic period, c. 40,000 BC.
Silver, copper, tin and meteoric iron can also be found in native form, allowing a limited amount of metalworking in early cultures. Egyptian weapons made from meteoric iron in about 3000 BC were highly prized as "daggers from heaven".

Certain metals, notably tin, lead and (at a higher temperature) copper, can be recovered from their ores by simply heating the rocks in a fire or blast furnace, a process known as smelting. The first evidence of this extractive metallurgy, dating from the 5th and 6th millennia BC, has been found at archaeological sites in Majdanpek, Yarmovac, and Plocnik, in present-day Serbia. To date, the earliest evidence of copper smelting is found at the Belovode site near Plocnik. This site produced a copper axe from 5500 BC, belonging to the Vinča culture.

The earliest use of lead is documented from the late neolithic settlement of Yarim Tepe in Iraq,
"The earliest lead (Pb) finds in the ancient Near East are a 6th millennium BC bangle from Yarim Tepe in northern Iraq and a slightly later conical lead piece from Halaf period Arpachiyah, near Mosul. As native lead is extremely rare, such artifacts raise the possibility that lead smelting may have begun even before copper smelting."
Copper smelting is also documented at this site at about the same time period (soon after 6000 BC), although the use of lead seems to precede copper smelting. Early metallurgy is also documented at the nearby site of Tell Maghzaliyah, which seems to be dated even earlier, and completely lacks pottery.

Other signs of early metals are found from the third millennium BC in places like Palmela (Portugal), Los Millares (Spain), and Stonehenge (United Kingdom). However, the ultimate beginnings cannot be clearly ascertained and new discoveries are both continuous and ongoing. 
In the Near East, about 3500 BC, it was discovered that by combining copper and tin, a superior metal could be made, an alloy called bronze. This represented a major technological shift known as the Bronze Age.

The extraction of iron from its ore into a workable metal is much more difficult than for copper or tin. The process appears to have been invented by the Hittites in about 1200 BC, beginning the Iron Age. The secret of extracting and working iron was a key factor in the success of the Philistines.

Historical developments in ferrous metallurgy can be found in a wide variety of past cultures and civilizations. This includes the ancient and medieval kingdoms and empires of the Middle East and Near East, ancient Iran, ancient Egypt, ancient Nubia, and Anatolia (Turkey), Ancient Nok, Carthage, the Greeks and Romans of ancient Europe, medieval Europe, ancient and medieval China, ancient and medieval India, ancient and medieval Japan, amongst others. Many applications, practices, and devices associated or involved in metallurgy were established in ancient China, such as the innovation of the blast furnace, cast iron, hydraulic-powered trip hammers, and double acting piston bellows.

A 16th century book by Georg Agricola called "De re metallica" describes the highly developed and complex processes of mining metal ores, metal extraction and metallurgy of the time. Agricola has been described as the "father of metallurgy".

Extractive metallurgy is the practice of removing valuable metals from an ore and refining the extracted raw metals into a purer form. In order to convert a metal oxide or sulphide to a purer metal, the ore must be reduced physically, chemically, or electrolytically.

Extractive metallurgists are interested in three primary streams: feed, concentrate (valuable metal oxide/sulphide), and tailings (waste). After mining, large pieces of the ore feed are broken through crushing and/or grinding in order to obtain particles small enough where each particle is either mostly valuable or mostly waste. Concentrating the particles of value in a form supporting separation enables the desired metal to be removed from waste products.

Mining may not be necessary if the ore body and physical environment are conducive to leaching. Leaching dissolves minerals in an ore body and results in an enriched solution. The solution is collected and processed to extract valuable metals.

Ore bodies often contain more than one valuable metal. Tailings of a previous process may be used as a feed in another process to extract a secondary product from the original ore. Additionally, a concentrate may contain more than one valuable metal. That concentrate would then be processed to separate the valuable metals into individual constituents.

Common engineering metals include aluminium, chromium, copper, iron, magnesium, nickel, titanium and zinc. These are most often used as alloys. Much effort has been placed on understanding the iron-carbon alloy system, which includes steels and cast irons. Plain carbon steels (those that contain essentially only carbon as an alloying element) are used in low-cost, high-strength applications where weight and corrosion are not a problem. Cast irons, including ductile iron, are also part of the iron-carbon system.

Stainless steel or galvanized steel is used where resistance to corrosion is important. Aluminium alloys and magnesium alloys are used for applications where strength and lightness are required.

Copper-nickel alloys (such as Monel) are used in highly corrosive environments and for non-magnetic applications. Nickel-based superalloys like Inconel are used in high-temperature applications such as gas turbines, turbochargers, pressure vessels, and heat exchangers. For extremely high temperatures, single crystal alloys are used to minimize creep.

In production engineering, metallurgy is concerned with the production of metallic components for use in consumer or engineering products. This involves the production of alloys, the shaping, the heat treatment and the surface treatment of the product. Determining the hardness of the metal using the Rockwell, Vickers, and Brinell hardness scales is a commonly used practice that helps better understand the metal’s elasticity and plasticity for different applications and production processes. The task of the metallurgist is to achieve balance between material properties such as cost, weight, strength, toughness, hardness, corrosion, fatigue resistance, and performance in temperature extremes. To achieve this goal, the operating environment must be carefully considered. In a saltwater environment, ferrous metals and some aluminium alloys corrode quickly. Metals exposed to cold or cryogenic conditions may endure a ductile to brittle transition and lose their toughness, becoming more brittle and prone to cracking. Metals under continual cyclic loading can suffer from metal fatigue. Metals under constant stress at elevated temperatures can creep.

Metals are shaped by processes such as:
Cold-working processes, in which the product’s shape is altered by rolling, fabrication or other processes while the product is cold, can increase the strength of the product by a process called work hardening. Work hardening creates microscopic defects in the metal, which resist further changes of shape.

Various forms of casting exist in industry and academia. These include sand casting, investment casting (also called the lost wax process), die casting, and continuous castings. Each of these forms has advantages for certain metals and applications considering factors like magnetism and corrosion.

Metals can be heat-treated to alter the properties of strength, ductility, toughness, hardness and/or resistance to corrosion. Common heat treatment processes include annealing, precipitation strengthening, quenching, and tempering. The annealing process softens the metal by heating it and then allowing it to cool very slowly, which gets rid of stresses in the metal and makes the grain structure large and soft-edged so that when the metal is hit or stressed it dents or perhaps bends, rather than breaking; it is also easier to sand, grind, or cut annealed metal. Quenching is the process of cooling a high-carbon steel very quickly after heating, thus "freezing" the steel's molecules in the very hard martensite form, which makes the metal harder. There is a balance between hardness and toughness in any steel; the harder the steel, the less tough or impact-resistant it is, and the more impact-resistant it is, the less hard it is. Tempering relieves stresses in the metal that were caused by the hardening process; tempering makes the metal less hard while making it better able to sustain impacts without breaking.

Often, mechanical and thermal treatments are combined in what are known as thermo-mechanical treatments for better properties and more efficient processing of materials. These processes are common to high-alloy special steels, superalloys and titanium alloys.

Electroplating is a chemical surface-treatment technique. It involves bonding a thin layer of another metal such as gold, silver, chromium or zinc to the surface of the product. This is done by selecting the coating material electrolyte solution which is the material that is going to coat the work piece (gold, silver...zinc). There needs to be two electrodes of different materials one the same material as the coating material and one that is receiving the coating material. then the two electrodes are electrically charged and the coating material is stuck to the work piece. It is used to reduce corrosion as well as to improve the product's aesthetic appearance. It is also used to make inexpensive metals look like the more expensive ones (gold, silver).

Shot peening is a cold working process used to finish metal parts. In the process of shot peening, small round shot is blasted against the surface of the part to be finished. This process is used to prolong the product life of the part, prevent stress corrosion failures, and also prevent fatigue. The shot leaves small dimples on the surface like a peen hammer does, which cause compression stress under the dimple. As the shot media strikes the material over and over, it forms many overlapping dimples throughout the piece being treated. The compression stress in the surface of the material strengthens the part and makes it more resistant to fatigue failure, stress failures, corrosion failure, and cracking.

Thermal spraying techniques are another popular finishing option, and often have better high temperature properties than electroplated coatings.Thermal spraying, also known as a spray welding process, is an industrial coating process that consists of a heat source (flame or other) and a coating material that can be in a powder or wire form which is melted then sprayed on the surface of the material being treated at a high velocity. The spray treating process is known by many different names such as hvof, plasma spray, flame spray, arc spray, and metalizing.

Metallurgists study the microscopic and macroscopic properties using metallography, a technique invented by Henry Clifton Sorby. In metallography, an alloy of interest is ground flat and polished to a mirror finish. The sample can then be etched to reveal the microstructure and macrostructure of the metal. The sample is then examined in an optical or electron microscope, and the image contrast provides details on the composition, mechanical properties, and processing history.

Crystallography, often using diffraction of x-rays or electrons, is another valuable tool available to the modern metallurgist. Crystallography allows identification of unknown materials and reveals the crystal structure of the sample. Quantitative crystallography can be used to calculate the amount of phases present as well as the degree of strain to which a sample has been subjected.



</doc>
<doc id="19723" url="https://en.wikipedia.org/wiki?curid=19723" title="MUMPS">
MUMPS

MUMPS ("Massachusetts General Hospital Utility Multi-Programming System"), or M, is a general-purpose computer programming language that provides ACID (Atomic, Consistent, Isolated, and Durable) transaction processing. Its differentiating feature is its "built-in" database, enabling high-level access to disk storage using simple symbolic program variables and subscripted arrays, similar to the variables used by most languages to access main memory.

The M database is a key-value database engine optimized for high-throughput transaction processing. As such it is in the class of "schema-less", "schema-free," or NoSQL databases. Internally, M stores data in multidimensional hierarchical sparse arrays (also known as key-value nodes, sub-trees, or associative memory). Each array may have up to 32 subscripts, or dimensions. A scalar can be thought of as an array element with zero subscripts. Nodes with varying numbers of subscripts (including one node with no subscripts) can freely co-exist in the same array.

Perhaps the most unusual aspect of the M language is the notion that the database is accessed through variables, rather than queries or retrievals. This means that accessing volatile memory and non-volatile storage use the same basic syntax, enabling a function to work on either local (volatile) or global (non-volatile) variables. Practically, this provides for extremely high performance data access.

Originally designed in 1966 for the healthcare industry, M continues to be used today by many large hospitals and banks to provide high-throughput transaction data processing.

MUMPS was developed by Neil Pappalardo, Robert Greenes, and Curt Marble in Dr. Octo Barnett's animal lab at the Massachusetts General Hospital (MGH) in Boston during 1966 and 1967.

The original MUMPS system was, like Unix a few years later, built on a spare DEC PDP-7. Octo Barnett and Neil Pappalardo were also involved with MGH's planning for a Hospital Information System, obtained a backward compatible PDP-9, and began using MUMPS in the admissions cycle and laboratory test reporting. MUMPS was then an interpreted language, yet even then, incorporated a hierarchical database file system to standardize interaction with the data.

Some aspects of MUMPS can be traced from Rand Corporation's JOSS through BBN's TELCOMP and STRINGCOMP. The MUMPS team deliberately chose to include portability between machines as a design goal. Another feature, not widely supported for machines of the era, in operating systems or in computer hardware, was multitasking, which was also built into the language itself.

The portability was soon useful, as MUMPS was shortly adapted to a DEC PDP-15, where it lived for some time. MUMPS was developed with the support of a government research grant, and so MUMPS was released to the public domain (no longer a requirement for grants), and was soon ported to a number of other systems including the popular DEC PDP-8, the Data General Nova and the DEC PDP-11 and the Artronix PC12 minicomputer. Word about MUMPS spread mostly through the medical community, and by the early 1970s was in widespread use, often being locally modified for their own needs.

By the early 1970s, there were many and varied implementations of MUMPS on a range of hardware platforms. The most widespread was DEC's MUMPS-11 on the PDP-11, and MEDITECH's MIIS. In 1972, many MUMPS users attended a conference which standardized the then-fractured language, and created the MUMPS Users Group and MUMPS Development Committee (MDC) to do so. These efforts proved successful; a standard was complete by 1974, and was approved, on September 15, 1977, as ANSI standard, X11.1-1977. At about the same time DEC launched DSM-11 (Digital Standard MUMPS) for the PDP-11. This quickly dominated the market, and became the reference implementation of the time. Also, InterSystems sold ISM-11 for the PDP-11 (which was identical to DSM-11).

During the early 1980s several vendors brought MUMPS-based platforms that met the ANSI standard to market. The most significant were:

Other companies developed important MUMPS implementations:

This period also saw considerable MDC activity. The second revision of the ANSI standard for MUMPS (X11.1-1984) was approved on November 15, 1984.



The US Department of Veterans Affairs (formerly the Veterans Administration) was one of the earliest major adopters of the MUMPS language. Their development work (and subsequent contributions to the free MUMPS application codebase) was an influence on many medical users worldwide. In 1995, the Veterans Affairs' patient Admission/Tracking/Discharge system, Decentralized Hospital Computer Program (DHCP) was the recipient of the Computerworld Smithsonian Award for best use of Information Technology in Medicine. In July 2006, the Department of Veterans Affairs (VA) / Veterans Health Administration (VHA) was the recipient of the Innovations in American Government Award presented by the Ash Institute of the John F. Kennedy School of Government at Harvard University for its extension of DHCP into the Veterans Health Information Systems and Technology Architecture (VistA). Nearly the entire VA hospital system in the United States, the Indian Health Service, and major parts of the Department of Defense CHCS hospital system use MUMPS databases for clinical data tracking. In 2015 the Department of Defense awarded a 10 year contract to Leidos, Cerner, and Accenture to replace CHCS. In 2017 the Veterans Health Administration (VHA) announced that it would replace VistA with Cerner by 2024 or 2025. That contract did not affect the 2015 VHA contract for an online appointment scheduling from Epic, so the VHA will continue to use MUMPS. 

Other healthcare IT companies using MUMPS include Epic, MEDITECH, GE Healthcare (formerly IDX Systems and Centricity), AmeriPath (part of Quest Diagnostics), Care Centric, Allscripts, Coventry Healthcare, EMIS, and Sunquest Information Systems (formerly Misys Healthcare). Many reference laboratories, such as DASA, Quest Diagnostics, and Dynacare, use MUMPS software written by or based on Antrim Corporation code. Antrim was purchased by Misys Healthcare (now Sunquest Information Systems) in 2001.

MUMPS is also widely used in financial applications. MUMPS gained an early following in the financial sector and is in use at many banks and credit unions. It is used by Ameritrade, the largest online trading service in the US, with over 12 billion transactions per day, as well as by the Bank of England and Barclays Bank, among others.

Since 2005, the use of MUMPS has been either in the form of GT.M or InterSystems Caché. The latter is being aggressively marketed by InterSystems and has had success in penetrating new markets, such as telecommunications, in addition to existing markets. The European Space Agency announced on May 13, 2010 that it will use the InterSystems Caché database to support the Gaia mission. This mission aims to map the Milky Way with unprecedented precision.

MUMPS is a language intended for and designed to build database applications. Secondary language features were included to help programmers make applications using minimal computing resources. The original implementations were interpreted, though modern implementations may be fully or partially compiled. Individual "programs" run in memory "partitions". Early MUMPS memory partitions were limited to 2048 bytes so aggressive abbreviation greatly aided multi-programming on severely resource limited hardware, because more than one MUMPS job could fit into the very small memories extant in hardware at the time. The ability to provide multi-user systems was another language design feature. The word "Multi-Programming" in the acronym points to this. Even the earliest machines running MUMPS supported multiple jobs running at the same time. With the change from mini-computers to micro-computers a
few years later, even a "single user PC" with a single 8-bit CPU and 16K or 64K of memory could support multiple users, who could connect to it from (non-graphical) video display terminals.

Since memory was tight originally, the language design for MUMPS valued very terse code. Thus, every MUMPS command or function name could be abbreviated from one to three letters in length, e.g. Quit (exit program) as Q, $P = $Piece function, R = Read command, $TR = $Translate function. Spaces and end-of-line markers are significant in MUMPS because line scope promoted the same terse language design. Thus, a single line of program code could express, with few characters, an idea for which other programming languages could require 5 to 10 times as many characters. Abbreviation was a common feature of languages designed in this period (e.g., FOCAL-69, early BASICs such as Tiny BASIC, etc.). An unfortunate side effect of this, coupled with the early need to write minimalist code, was that MUMPS programmers routinely did not comment code and used extensive abbreviations. This meant that even an expert MUMPS programmer could not just skim through a page of code to see its function but would have to
analyze it line by line.

Database interaction is transparently built into the language. The MUMPS language provides a hierarchical database made up of persistent sparse arrays, which is implicitly "opened" for every MUMPS application. All variable names prefixed with the caret character ("^") use permanent (instead of RAM) storage, will maintain their values after the application exits, and will be visible to (and modifiable by) other running applications. Variables using this shared and permanent storage are called "Globals" in MUMPS, because the scoping of these variables is "globally available" to all jobs on the system. The more recent and more common use of the name "global variables" in other languages is a more limited scoping of names, coming from the fact that unscoped variables are "globally" available to any programs running in the same process, but not shared among multiple processes. The MUMPS Storage mode (i.e. Globals
stored as persistent sparse arrays), gives the MUMPS database the characteristics of a document-oriented database.

All variable names which are not prefixed with caret character ("^") are temporary and private. Like global variables, they also have a hierarchical storage model, but are only "locally available" to a single job, thus they are called "locals". Both "globals" and "locals" can have child nodes (called "subscripts" in MUMPS terminology). Subscripts are not limited to numerals—any ASCII character or group of characters can be a subscript identifier. While this is not uncommon for modern languages such as Perl or JavaScript, it was a highly unusual feature in the late 1970s. This capability was not universally implemented in MUMPS systems before the 1984 ANSI standard, as only canonically numeric subscripts were required by the standard to be allowed. Thus, the variable named 'Car' can have
subscripts "Door", "Steering Wheel" and "Engine", each of which can contain a value and have subscripts of their own. The variable ^Car("Door") could have a nested variable subscript of "Color" for example. Thus, you could say

to modify a nested child node of ^Car. In MUMPS terms, "Color" is the 2nd subscript of the variable ^Car (both the names of the child-nodes and the child-nodes themselves are likewise called subscripts). Hierarchical variables are similar to objects with properties in many object oriented languages. Additionally, the MUMPS language design requires that all subscripts of variables are automatically kept in sorted order. Numeric subscripts (including floating-point numbers) are stored from lowest to highest. All non-numeric subscripts are stored in alphabetical order following the numbers. In MUMPS terminology, this is "canonical order". By using only non-negative integer subscripts, the MUMPS programmer can emulate the arrays data type from other languages. Although MUMPS does not natively offer a full set of DBMS features such as mandatory schemas, several DBMS systems have been built on top of it that provide
application developers with flat-file, relational and network database features.

Additionally, there are built-in operators which treat a delimited string (e.g., comma-separated values) as an array. Early MUMPS programmers would often store a structure of related information as a delimited string, parsing it after it was read in; this saved disk access time and offered considerable speed advantages on some hardware.

MUMPS has no data types. Numbers can be treated as strings of digits, or strings can be treated as numbers by numeric operators ("coerced", in MUMPS terminology). Coercion can have some odd side effects, however. For example, when a string is coerced, the parser turns as much of the string (starting from the left) into a number as it can, then discards the rest. Thus the statement codice_1 is evaluated as codice_2 in MUMPS.

Other features of the language are intended to help MUMPS applications interact with each other in a multi-user environment. Database locks, process identifiers, and atomicity of database update transactions are all required of standard MUMPS implementations.

In contrast to languages in the C or Wirth traditions, some space characters between MUMPS statements are significant. A single space separates a command from its argument, and a space, or newline, separates each argument from the next MUMPS token. Commands which take no arguments (e.g., codice_3) require two following spaces. The concept is that one space separates the command from the (nonexistent) argument, the next separates the "argument" from the next command. Newlines are also significant; an codice_4, codice_3 or codice_6 command processes (or skips) everything else till the end-of-line. To make those statements control multiple lines, you must use the codice_7 command to create a code block.

A simple Hello world program in MUMPS might be:

and would be run from the MUMPS command line with the command codice_8. Since MUMPS allows commands to be strung together on the same line, and since commands can be abbreviated to a single letter, this routine could be made more compact:

The 'codice_9' after the text generates a newline.

ANSI X11.1-1995 gives a complete, formal description of the language; an annotated version of this standard is available online.

Data types: There is one universal datatype, which is implicitly coerced to string, integer, or floating-point datatypes as context requires.

Booleans (called "truthvalues" in MUMPS): In IF commands and other syntax that has expressions evaluated as conditions, any string value is evaluated as a numeric value, and if that is a nonzero value, then it is interpreted as True. codice_10 yields 1 if a is less than b, 0 otherwise.

Declarations: None. All variables are dynamically created at the first time a value is assigned.

Lines: are important syntactic entities, unlike their status in languages patterned on C or Pascal. Multiple statements per line are allowed and are common. The scope of any IF, ELSE, and FOR command is "the remainder of current line."

Case sensitivity: Commands and intrinsic functions are case-insensitive. In contrast, variable names and labels are case-sensitive. There is no special meaning for upper vs. lower-case and few widely followed conventions. The percent sign (%) is legal as first character of variables and labels.

Postconditionals: execution of almost any command can be controlled by following it with a colon and a truthvalue expression. codice_11 sets A to "FOO" if N is less than 10; codice_12 performs PRINTERR if N is greater than 100. This construct provides a conditional whose scope is less than a full line.

Abbreviation: You can abbreviate nearly all commands and native functions to one, two, or three characters.

Reserved words: None. Since MUMPS interprets source code by context, there is no need for reserved words. You may use the names of language commands as variables. There has been no contest such as the International Obfuscated C Code Contest for MUMPS, despite the potential of examples such as the following, perfectly legal, MUMPS code:

MUMPS can be made more obfuscated by using the contracted operator syntax, as shown in this terse example derived from the example above:

Arrays: are created dynamically, stored as B-trees, are sparse (i.e. use almost no space for missing nodes), can use any number of subscripts, and subscripts can be strings or numeric (including floating point). Arrays are always automatically stored in sorted order, so there is never any occasion to sort, pack, reorder, or otherwise reorganize the database. Built in functions such as $DATA, $ORDER, $NEXT(deprecated) and $QUERY functions provide efficient examination and traversal of the fundamental array structure, on disk or in memory.

Local arrays: variable names not beginning with caret (i.e. "^") are stored in memory by process, are private to the creating process, and expire when the creating process terminates. The available storage depends on implementation. For those implementations using partitions, it is limited to the partition size, (A small partition might be 32K). For other implementations, it may be several megabytes.

Global arrays: codice_13. These are stored on disk, are available to all processes, and are persistent when the creating process terminates. Very large globals (for example, hundreds of gigabytes) are practical and efficient in most implementations. This is MUMPS' main "database" mechanism. It is used instead of calling on the operating system to create, write, and read files.

Indirection: in many contexts, codice_14 can be used, and effectively substitutes the contents of VBL into another MUMPS statement. codice_15 sets the variable ABC to 123. codice_16 performs the subroutine named REPORT. This substitution allows for lazy evaluation and late binding as well as effectively the operational equivalent of "pointers" in other languages.

Piece function: This breaks variables into segmented pieces guided by a user specified separator string (sometimes called a "delimiter"). Those who know awk will find this familiar. codice_17 means the "third caret-separated piece of STRINGVAR." The piece function can also appear as an assignment (SET command) target.

codice_18 yields "std".

After
codice_19 causes X to become "office@world.std.com" (note that $P is equivalent to $PIECE and could be written as such).

Order function: This function treats its input as a structure, and finds the next index that exists which has the same structure except for the last subscript. It returns the sorted value that is ordered after the one given as input. (This treats the array reference as a content-addressable data rather than an address of a value)

codice_20 yields 6, codice_21 yields 10, codice_22 yields 10, codice_23 yields 15, codice_24 yields "".

Here, the argument-less "For" repeats until stopped by a terminating "Quit". This line prints a table of i and stuff(i) where i is successively 6, 10, and 15.

For iterating the database, the Order function returns the next key to use.

Multi-User/Multi-Tasking/Multi-Processor: MUMPS supports multiple simultaneous users and processes even when the underlying operating system does not (e.g., MS-DOS). Additionally, there is the ability to specify an environment for a variable, such as by specifying a machine name in a variable (as in codice_25), which can allow you to access data on remote machines.

Some aspects of MUMPS syntax differ strongly from that of more modern languages, which can cause confusion. Whitespace is not allowed within expressions, as it ends a statement: codice_26 is an error, and must be written codice_27. All operators have the same precedence and are left-associative (codice_28 evaluates to 50). The operators for "less than or equal to" and "greater than or equal to" are codice_29 and codice_30 (that is, the boolean negation operator codice_31 plus a strict comparison operator).

MUMPS lacks a while statement; a GOTO statement can be used to simulate one. By default, the IF, ELSE, and FOR statements use the rest of the line as their block. The DO statement allows one of them to span multiple lines; periods (codice_32) are used to indent the lines in a DO block. The ELSE statement does not need a corresponding IF, as it operates by inspecting the value in the builtin system variable codice_33.

MUMPS scoping rules are more permissive than other modern languages. Declared local variables are scoped using the stack. A routine can normally see all declared locals of the routines below it on the call stack, and routines cannot prevent routines they call from modifying their declared locals. By contrast, undeclared variables (variables created by using them, rather than declaration) are in scope for all routines running in the same process, and remain in scope until the program exits.

All of the following positions can be, and have been, supported by knowledgeable people at various times:

Some of the contention arose in response to strong M advocacy on the part of one commercial interest, InterSystems, whose chief executive disliked the name MUMPS and felt that it represented a serious marketing obstacle. Thus, favoring M to some extent became identified as alignment with InterSystems. The dispute also reflected rivalry between organizations (the M Technology Association, the MUMPS Development Committee, the ANSI and ISO Standards Committees) as to who determines the "official" name of the language. Some writers have attempted to defuse the issue by referring to the language as "M[UMPS]", square brackets being the customary notation for optional syntax elements. A leading authority, and the author of an open source MUMPS implementation, Professor Kevin O'Kane, uses only 'MUMPS'.

The most recent standard (ISO/IEC 11756:1999, re-affirmed on 25 June 2010), still mentions both M and MUMPS as officially accepted names.

Massachusetts General Hospital registered "MUMPS" as a trademark with the USPTO on November 28, 1971, renewed on November 16, 1992, and expired on August 30, 2003.





</doc>
<doc id="19726" url="https://en.wikipedia.org/wiki?curid=19726" title="Mercury (programming language)">
Mercury (programming language)

Mercury is a functional logic programming language made for real-world uses. The first version was developed at the University of Melbourne, Computer Science department, by Fergus Henderson, Thomas Conway, and Zoltan Somogyi, under Somogyi's supervision, and released on April 8, 1995.

Mercury is a purely declarative logic programming language. It is related to both Prolog and Haskell. It features a strong, static, polymorphic type system, and a strong mode and determinism system.

The official implementation, the Melbourne Mercury Compiler, is available for most Unix and Unix-like platforms, including Linux, macOS, and for Windows (32bits only).

Mercury is based on the logic programming language Prolog. It has the same syntax and the same basic concepts such as the SLD resolution algorithm. It can be viewed as a pure subset of Prolog with strong types and modes. As such, it is often compared to its predecessor in features and run-time efficiency.

The language is designed using software engineering principles. Unlike the original implementations of Prolog, it has a separate compilation phase, rather than being directly interpreted. This allows a much wider range of errors to be detected before running a program. It features a strict static type and mode system and a module system.

By using information obtained at compile time (such as type and mode), programs written in Mercury typically perform significantly faster than equivalent programs written in Prolog. Its authors claim that Mercury is the fastest logic language in the world, by a wide margin.

Mercury is a purely declarative language, unlike Prolog, since it lacks "extra-logical" Prolog statements such as codice_1 and imperative input/output (I/O). This enables advanced static program analysis and program optimization, including compile-time garbage collection, but it can make certain programming constructs (such as a switch over a number of options, with a default) harder to express. (While Mercury does allow impure functionality, this serves mainly as a way to call foreign language code. All impure code must be marked explicitly.) Operations which would typically be impure (such as input/output) are expressed using pure constructs in Mercury using linear types, by threading a dummy "world" value through all relevant code.

Notable programs written in Mercury include the Mercury compiler and the Prince XML formatter. Software company Mission Critical IT has also been using Mercury since 2000 to develop enterprise applications and its Ontology-Driven software development platform, ODASE.

Mercury has several back-ends, which enable compiling Mercury code into several languages, including:



Mercury also features a foreign language interface, allowing code in other languages (depending on the chosen back-end) to be linked with Mercury code. The following foreign languages are possible:
Other languages can then be interfaced to by calling them from these languages. However, this means that foreign language code may need to be written several times for the different backends, otherwise portability between backends will be lost.

The most commonly used back-end is the original low-level C back-end.

Hello World:
Calculating the 10th Fibonacci number (in the most obvious way):
Releases are named according to the year and month of release. The current stable release is 14.01.1 (September 2014). Prior releases were numbered 0.12, 0.13, etc., and the time between stable releases can be as long as 3 years.

There is often also a snapshot "release of the day" (ROTD) consisting of the latest features and bug fixes added to the last stable release.





</doc>
<doc id="19727" url="https://en.wikipedia.org/wiki?curid=19727" title="Michael Faraday">
Michael Faraday

Michael Faraday FRS (; 22 September 1791 – 25 August 1867) was a British scientist who contributed to the study of electromagnetism and electrochemistry. His main discoveries include the principles underlying electromagnetic induction, diamagnetism and electrolysis.

Although Faraday received little formal education, he was one of the most influential scientists in history. It was by his research on the magnetic field around a conductor carrying a direct current that Faraday established the basis for the concept of the electromagnetic field in physics. Faraday also established that magnetism could affect rays of light and that there was an underlying relationship between the two phenomena. He similarly discovered the principles of electromagnetic induction and diamagnetism, and the laws of electrolysis. His inventions of electromagnetic rotary devices formed the foundation of electric motor technology, and it was largely due to his efforts that electricity became practical for use in technology.

As a chemist, Faraday discovered benzene, investigated the clathrate hydrate of chlorine, invented an early form of the Bunsen burner and the system of oxidation numbers, and popularised terminology such as "anode", "cathode", "electrode" and "ion". Faraday ultimately became the first and foremost Fullerian Professor of Chemistry at the Royal Institution, a lifetime position.

Faraday was an excellent experimentalist who conveyed his ideas in clear and simple language; his mathematical abilities, however, did not extend as far as trigonometry and were limited to the simplest algebra. James Clerk Maxwell took the work of Faraday and others and summarized it in a set of equations which is accepted as the basis of all modern theories of electromagnetic phenomena. On Faraday's uses of lines of force, Maxwell wrote that they show Faraday "to have been in reality a mathematician of a very high order – one from whom the mathematicians of the future may derive valuable and fertile methods." The SI unit of capacitance is named in his honour: the farad.

Albert Einstein kept a picture of Faraday on his study wall, alongside pictures of Isaac Newton and James Clerk Maxwell. Physicist Ernest Rutherford stated, "When we consider the magnitude and extent of his discoveries and their influence on the progress of science and of industry, there is no honour too great to pay to the memory of Faraday, one of the greatest scientific discoverers of all time."

Michael Faraday was born on 22 September 1791 in Newington Butts, which is now part of the London Borough of Southwark but was then a suburban part of Surrey. His family was not well off. His father, James, was a member of the Glassite sect of Christianity. James Faraday moved his wife and two children to London during the winter of 1790 from Outhgill in Westmorland, where he had been an apprentice to the village blacksmith. Michael was born in the autumn of that year. The young Michael Faraday, who was the third of four children, having only the most basic school education, had to educate himself.

At the age of 14 he became an apprentice to George Riebau, a local bookbinder and bookseller in Blandford Street. During his seven-year apprenticeship Faraday read many books, including Isaac Watts's "The Improvement of the Mind", and he enthusiastically implemented the principles and suggestions contained therein. He also developed an interest in science, especially in electricity. Faraday was particularly inspired by the book "Conversations on Chemistry" by Jane Marcet.

In 1812, at the age of 20 and at the end of his apprenticeship, Faraday attended lectures by the eminent English chemist Humphry Davy of the Royal Institution and the Royal Society, and John Tatum, founder of the City Philosophical Society. Many of the tickets for these lectures were given to Faraday by William Dance, who was one of the founders of the Royal Philharmonic Society. Faraday subsequently sent Davy a 300-page book based on notes that he had taken during these lectures. Davy's reply was immediate, kind, and favourable. In 1813, when Davy damaged his eyesight in an accident with nitrogen trichloride, he decided to employ Faraday as an assistant. Coincidentally one of the Royal Institution's assistants, John Payne, was sacked and Sir Humphry Davy had been asked to find a replacement; thus he appointed Faraday as Chemical Assistant at the Royal Institution on 1 March 1813. Very soon Davy entrusted Faraday with the preparation of nitrogen trichloride samples, and they both were injured in an explosion of this very sensitive substance.
In the class-based English society of the time, Faraday was not considered a gentleman. When Davy set out on a long tour of the continent in 1813–15, his valet did not wish to go, so instead, Faraday went as Davy's scientific assistant and was asked to act as Davy's valet until a replacement could be found in Paris. Faraday was forced to fill the role of valet as well as assistant throughout the trip. Davy's wife, Jane Apreece, refused to treat Faraday as an equal (making him travel outside the coach, eat with the servants, etc.), and made Faraday so miserable that he contemplated returning to England alone and giving up science altogether. The trip did, however, give him access to the scientific elite of Europe and exposed him to a host of stimulating ideas.

Faraday married Sarah Barnard (1800–1879) on 12 June 1821. They met through their families at the Sandemanian church, and he confessed his faith to the Sandemanian congregation the month after they were married. They had no children.

Faraday was a devout Christian; his Sandemanian denomination was an offshoot of the Church of Scotland. Well after his marriage, he served as deacon and for two terms as an elder in the meeting house of his youth. His church was located at Paul's Alley in the Barbican. This meeting house relocated in 1862 to Barnsbury Grove, Islington; this North London location was where Faraday served the final two years of his second term as elder prior to his resignation from that post. Biographers have noted that "a strong sense of the unity of God and nature pervaded Faraday's life and work."

In June 1832, the University of Oxford granted Faraday a Doctor of Civil Law degree (honorary). During his lifetime, he was offered a knighthood in recognition for his services to science, which he turned down on religious grounds, believing that it was against the word of the Bible to accumulate riches and pursue worldly reward, and stating that he preferred to remain "plain Mr Faraday to the end". Elected a member of the Royal Society in 1824, he twice refused to become President. He became the first Fullerian Professor of Chemistry at the Royal Institution in 1833.

In 1832, Faraday was elected a Foreign Honorary Member of the American Academy of Arts and Sciences. He was elected a foreign member of the Royal Swedish Academy of Sciences in 1838, and was one of eight foreign members elected to the French Academy of Sciences in 1844. In 1849 he was elected as associated member to the Royal Institute of the Netherlands, which two years later became the Royal Netherlands Academy of Arts and Sciences and he was subsequently made foreign member.

Faraday suffered a nervous breakdown in 1839 but eventually returned to his investigations into electromagnetism. In 1848, as a result of representations by the Prince Consort, Faraday was awarded a grace and favour house in Hampton Court in Middlesex, free of all expenses and upkeep. This was the Master Mason's House, later called Faraday House, and now No. 37 Hampton Court Road. In 1858 Faraday retired to live there.

Having provided a number of various service projects for the British government, when asked by the government to advise on the production of chemical weapons for use in the Crimean War (1853–1856), Faraday refused to participate citing ethical reasons.

Faraday died at his house at Hampton Court on 25 August 1867, aged 75. He had some years before turned down an offer of burial in Westminster Abbey upon his death, but he has a memorial plaque there, near Isaac Newton's tomb. Faraday was interred in the dissenters' (non-Anglican) section of Highgate Cemetery.

Faraday's earliest chemical work was as an assistant to Humphry Davy. Faraday was specifically involved in the study of chlorine; he discovered two new compounds of chlorine and carbon. He also conducted the first rough experiments on the diffusion of gases, a phenomenon that was first pointed out by John Dalton. The physical importance of this phenomenon was more fully revealed by Thomas Graham and Joseph Loschmidt. Faraday succeeded in liquefying several gases, investigated the alloys of steel, and produced several new kinds of glass intended for optical purposes. A specimen of one of these heavy glasses subsequently became historically important; when the glass was placed in a magnetic field Faraday determined the rotation of the plane of polarisation of light. This specimen was also the first substance found to be repelled by the poles of a magnet.

Faraday invented an early form of what was to become the Bunsen burner, which is in practical use in science laboratories around the world as a convenient source of heat.
Faraday worked extensively in the field of chemistry, discovering chemical substances such as benzene (which he called bicarburet of hydrogen) and liquefying gases such as chlorine. The liquefying of gases helped to establish that gases are the vapours of liquids possessing a very low boiling point and gave a more solid basis to the concept of molecular aggregation. In 1820 Faraday reported the first synthesis of compounds made from carbon and chlorine, CCl and CCl, and published his results the following year. Faraday also determined the composition of the chlorine clathrate hydrate, which had been discovered by Humphry Davy in 1810. Faraday is also responsible for discovering the laws of electrolysis, and for popularizing terminology such as anode, cathode, electrode, and ion, terms proposed in large part by William Whewell.

Faraday was the first to report what later came to be called metallic nanoparticles. In 1847 he discovered that the optical properties of gold colloids differed from those of the corresponding bulk metal. This was probably the first reported observation of the effects of quantum size, and might be considered to be the birth of nanoscience.

Faraday is best known for his work regarding electricity and magnetism. His first recorded experiment was the construction of a voltaic pile with seven ha'penny coins, stacked together with seven disks of sheet zinc, and six pieces of paper moistened with salt water. With this pile he decomposed sulfate of magnesia (first letter to Abbott, 12 July 1812).

In 1821, soon after the Danish physicist and chemist Hans Christian Ørsted discovered the phenomenon of electromagnetism, Davy and British scientist William Hyde Wollaston tried, but failed, to design an electric motor. Faraday, having discussed the problem with the two men, went on to build two devices to produce what he called "electromagnetic rotation". One of these, now known as the homopolar motor, caused a continuous circular motion that was engendered by the circular magnetic force around a wire that extended into a pool of mercury wherein was placed a magnet; the wire would then rotate around the magnet if supplied with current from a chemical battery. These experiments and inventions formed the foundation of modern electromagnetic technology. In his excitement, Faraday published results without acknowledging his work with either Wollaston or Davy. The resulting controversy within the Royal Society strained his mentor relationship with Davy and may well have contributed to Faraday's assignment to other activities, which consequently prevented his involvement in electromagnetic research for several years.

From his initial discovery in 1821, Faraday continued his laboratory work, exploring electromagnetic properties of materials and developing requisite experience. In 1824, Faraday briefly set up a circuit to study whether a magnetic field could regulate the flow of a current in an adjacent wire, but he found no such relationship. This experiment followed similar work conducted with light and magnets three years earlier that yielded identical results. During the next seven years, Faraday spent much of his time perfecting his recipe for optical quality (heavy) glass, borosilicate of lead, which he used in his future studies connecting light with magnetism. In his spare time, Faraday continued publishing his experimental work on optics and electromagnetism; he conducted correspondence with scientists whom he had met on his journeys across Europe with Davy, and who were also working on electromagnetism. Two years after the death of Davy, in 1831, he began his great series of experiments in which he discovered electromagnetic induction, recording in his laboratory diary on 28 October 1831 he was; "making many experiments with the great magnet of the Royal Society".

Faraday's breakthrough came when he wrapped two insulated coils of wire around an iron ring, and found that upon passing a current through one coil a momentary current was induced in the other coil. This phenomenon is now known as mutual induction. The iron ring-coil apparatus is still on display at the Royal Institution. In subsequent experiments, he found that if he moved a magnet through a loop of wire an electric current flowed in that wire. The current also flowed if the loop was moved over a stationary magnet. His demonstrations established that a changing magnetic field produces an electric field; this relation was modelled mathematically by James Clerk Maxwell as Faraday's law, which subsequently became one of the four Maxwell equations, and which have in turn evolved into the generalization known today as field theory. Faraday would later use the principles he had discovered to construct the electric dynamo, the ancestor of modern power generators and the electric motor.

In 1832, he completed a series of experiments aimed at investigating the fundamental nature of electricity; Faraday used "static", batteries, and "animal electricity" to produce the phenomena of electrostatic attraction, electrolysis, magnetism, etc. He concluded that, contrary to the scientific opinion of the time, the divisions between the various "kinds" of electricity were illusory. Faraday instead proposed that only a single "electricity" exists, and the changing values of quantity and intensity (current and voltage) would produce different groups of phenomena.

Near the end of his career, Faraday proposed that electromagnetic forces extended into the empty space around the conductor. This idea was rejected by his fellow scientists, and Faraday did not live to see the eventual acceptance of his proposition by the scientific community. Faraday's concept of lines of flux emanating from charged bodies and magnets provided a way to visualize electric and magnetic fields; that conceptual model was crucial for the successful development of the electromechanical devices that dominated engineering and industry for the remainder of the 19th century.

In 1845, Faraday discovered that many materials exhibit a weak repulsion from a magnetic field: a phenomenon he termed diamagnetism.

Faraday also discovered that the plane of polarization of linearly polarized light can be rotated by the application of an external magnetic field aligned with the direction in which the light is moving. This is now termed the Faraday effect. In Sept 1845 he wrote in his notebook, "I have at last succeeded in "illuminating a magnetic curve" or "line of force" and in "magnetising a ray of light"".

Later on in his life, in 1862, Faraday used a spectroscope to search for a different alteration of light, the change of spectral lines by an applied magnetic field. The equipment available to him was, however, insufficient for a definite determination of spectral change. Pieter Zeeman later used an improved apparatus to study the same phenomenon, publishing his results in 1897 and receiving the 1902 Nobel Prize in Physics for his success. In both his 1897 paper and his Nobel acceptance speech, Zeeman made reference to Faraday's work.

In his work on static electricity, Faraday's ice pail experiment demonstrated that the charge resided only on the exterior of a charged conductor, and exterior charge had no influence on anything enclosed within a conductor. This is because the exterior charges redistribute such that the interior fields emanating from them cancel one another. This shielding effect is used in what is now known as a Faraday cage.

Faraday had a long association with the Royal Institution of Great Britain. He was appointed Assistant Superintendent of the House of the Royal Institution in 1821. He was elected a member of the Royal Society in 1824. In 1825, he became Director of the Laboratory of the Royal Institution. Six years later, in 1833, Faraday became the first Fullerian Professor of Chemistry at the Royal Institution of Great Britain, a position to which he was appointed for life without the obligation to deliver lectures. His sponsor and mentor was John 'Mad Jack' Fuller, who created the position at the Royal Institution for Faraday.

Beyond his scientific research into areas such as chemistry, electricity, and magnetism at the Royal Institution, Faraday undertook numerous, and often time-consuming, service projects for private enterprise and the British government. This work included investigations of explosions in coal mines, being an expert witness in court, and along with two engineers from Chance Brothers c.1853, the preparation of high-quality optical glass, which was required by Chance for its lighthouses. In 1846, together with Charles Lyell, he produced a lengthy and detailed report on a serious explosion in the colliery at Haswell, County Durham, which killed 95 miners. Their report was a meticulous forensic investigation and indicated that coal dust contributed to the severity of the explosion. The report should have warned coal owners of the hazard of coal dust explosions, but the risk was ignored for over 60 years until the Senghenydd Colliery Disaster of 1913.

As a respected scientist in a nation with strong maritime interests, Faraday spent extensive amounts of time on projects such as the construction and operation of lighthouses and protecting the bottoms of ships from corrosion. His workshop still stands at Trinity Buoy Wharf above the Chain and Buoy Store, next to London's only lighthouse where he carried out the first experiments in electric lighting for lighthouses.

Faraday was also active in what would now be called environmental science, or engineering. He investigated industrial pollution at Swansea and was consulted on air pollution at the Royal Mint. In July 1855, Faraday wrote a letter to "The Times" on the subject of the foul condition of the River Thames, which resulted in an often-reprinted cartoon in "Punch". (See also The Great Stink).

Faraday assisted with the planning and judging of exhibits for the Great Exhibition of 1851 in London. He also advised the National Gallery on the cleaning and protection of its art collection, and served on the National Gallery Site Commission in 1857.

Education was another of Faraday's areas of service; he lectured on the topic in 1854 at the Royal Institution, and in 1862 he appeared before a Public Schools Commission to give his views on education in Great Britain. Faraday also weighed in negatively on the public's fascination with table-turning, mesmerism, and seances, and in so doing chastised both the public and the nation's educational system.
Before his famous Christmas lectures, Faraday delivered chemistry lectures for the City Philosophical Society from 1816 to 1818 in order to refine the quality of his lectures. Between 1827 and 1860 at the Royal Institution in London, Faraday gave a series of nineteen Christmas lectures for young people, a series which continues today. The objective of Faraday's Christmas lectures was to present science to the general public in the hopes of inspiring them and generating revenue for the Royal Institution. They were notable events on the social calendar among London's gentry. Over the course of several letters to his close friend Benjamin Abbott, Faraday outlined his recommendations on the art of lecturing: Faraday wrote "a flame should be lighted at the commencement and kept alive with unremitting splendour to the end". His lectures were joyful and juvenile, he delighted in filling soap bubbles with various gasses (in order to determine whether or not they are magnetic) in front of his audiences and marveled at the rich colors of polarized lights, but the lectures were also deeply philosophical. In his lectures he urged his audiences to consider the mechanics of his experiments: "you know very well that ice floats upon water ... Why does the ice float? Think of that, and philosophise". His subjects consisted of Chemistry and Electricity, and included: 1841 The Rudiments of Chemistry, 1843 First Principles of Electricity, 1848 The Chemical History of a Candle, 1851 Attractive Forces, 1853 Voltaic Electricity, 1854 The Chemistry of Combustion, 1855 The Distinctive Properties of the Common Metals, 1857 Static Electricity, 1858 The Metallic Properties, 1859 The Various Forces of Matter and their Relations to Each Other.

A statue of Faraday stands in Savoy Place, London, outside the Institution of Engineering and Technology. Also in London, the Michael Faraday Memorial, designed by brutalist architect Rodney Gordon and completed in 1961, is at the Elephant & Castle gyratory system, near Faraday's birthplace at Newington Butts. Faraday School is located on Trinity Buoy Wharf where his workshop still stands above the Chain and Buoy Store, next to London's only lighthouse.

Faraday Gardens is a small park in Walworth, London, not far from his birthplace at Newington Butts. This park lies within the local council ward of Faraday in the London Borough of Southwark. Michael Faraday Primary school is situated on the Aylesbury Estate in Walworth.

A building at London South Bank University, which houses the institute's electrical engineering departments is named the Faraday Wing, due to its proximity to Faraday's birthplace in Newington Butts. A hall at Loughborough University was named after Faraday in 1960. Near the entrance to its dining hall is a bronze casting, which depicts the symbol of an electrical transformer, and inside there hangs a portrait, both in Faraday's honour. An eight-story building at the University of Edinburgh's science & engineering campus is named for Faraday, as is a recently built hall of accommodation at Brunel University, the main engineering building at Swansea University, and the instructional and experimental physics building at Northern Illinois University. The former UK Faraday Station in Antarctica was named after him.

Streets named for Faraday can be found in many British cities (e.g., London, Fife, Swindon, Basingstoke, Nottingham, Whitby, Kirkby, Crawley, Newbury, Swansea, Aylesbury and Stevenage) as well as in France (Paris), Germany (Berlin-Dahlem, Hermsdorf), Canada (Quebec; Deep River, Ontario; Ottawa, Ontario), and the United States (Reston, Virginia).

A Royal Society of Arts blue plaque, unveiled in 1876, commemorates Faraday at 48 Blandford Street in London's Marylebone district. From 1991 until 2001, Faraday's picture featured on the reverse of Series E £20 banknotes issued by the Bank of England. He was portrayed conducting a lecture at the Royal Institution with the magneto-electric spark apparatus. In 2002, Faraday was ranked number 22 in the BBC's list of the 100 Greatest Britons following a UK-wide vote.

The Faraday Institute for Science and Religion derives its name from the scientist, who saw his faith as integral to his scientific research. The logo of the Institute is also based on Faraday's discoveries. It was created in 2006 by a $2,000,000 grant from the John Templeton Foundation to carry out academic research, to foster understanding of the interaction between science and religion, and to engage public understanding in both these subject areas.

Faraday's life and contributions to electromagnetics was the principal topic of the tenth episode, titled "The Electric Boy", of the 2014 American science documentary series, "", which was broadcast on Fox and the National Geographic Channel.

In honor and remembrance of his great scientific contributions, several institutions have created prizes and awards in his name. This include:


Faraday's books, with the exception of "Chemical Manipulation", were collections of scientific papers or transcriptions of lectures. Since his death, Faraday's diary has been published, as have several large volumes of his letters and Faraday's journal from his travels with Davy in 1813–1815.




</doc>
<doc id="19728" url="https://en.wikipedia.org/wiki?curid=19728" title="Marriage">
Marriage

Marriage, also called matrimony or wedlock, is a socially or ritually recognised union between spouses that establishes rights and obligations between those spouses, as well as between them and any resulting biological or adopted children and affinity (in-laws and other family through marriage). The definition of marriage varies around the world not only between cultures and between religions, but also throughout the history of any given culture and religion, evolving to both expand and constrict in who and what is encompassed, but typically it is principally an institution in which interpersonal relationships, usually sexual, are acknowledged or sanctioned. In some cultures, marriage is recommended or considered to be compulsory before pursuing any sexual activity. When defined broadly, marriage is considered a cultural universal. A marriage ceremony is known as a wedding.
Individuals may marry for several reasons, including legal, social, libidinal, emotional, financial, spiritual, and religious purposes. Whom they marry may be influenced by socially determined rules of incest, prescriptive marriage rules, parental choice and individual desire. In some areas of the world, arranged marriage, child marriage, polygamy, and sometimes forced marriage, may be practiced as a cultural tradition. Conversely, such practices may be outlawed and penalized in parts of the world out of concerns of the infringement of women's rights, or the infringement of children's rights (both female and male children), and because of international law. In developed parts of the world, there has been a general trend towards ensuring equal rights within marriage for women and legally recognizing the marriages of interfaith, interracial, and same-sex couples. These trends coincide with the broader human rights movement.

Marriage can be recognized by a state, an organization, a religious authority, a tribal group, a local community, or peers. It is often viewed as a contract. When a marriage is performed and carried out by a government institution in accordance with the marriage laws of the jurisdiction, without religious content, it is a civil marriage. Civil marriage recognizes and creates the rights and obligations intrinsic to matrimony before the state. When a marriage is performed with religious content under the auspices of a religious institution it is a religious marriage. Religious marriage recognizes and creates the rights and obligations intrinsic to matrimony before that religion. Religious marriage is known variously as sacramental marriage in Catholicism, nikah in Islam, nissuin in Judaism, and various other names in other faith traditions, each with their own constraints as to what constitutes, and who can enter into, a valid religious marriage.

Some countries do not recognize locally performed religious marriage on its own, and require a separate civil marriage for official purposes. Conversely, civil marriage does not exist in some countries governed by a religious legal system, such as Saudi Arabia, where marriages contracted abroad might not be recognized if they were contracted contrary to Saudi interpretations of Islamic religious law. In countries governed by a mixed secular-religious legal system, such as in Lebanon and Israel, locally performed civil marriage also does not exist within the country, preventing interfaith and various other marriages contradicting religious laws from being entered into in the country, however, civil marriages performed abroad are recognized by the state even if they conflict with religious laws (in the case of recognition of marriage in Israel, this includes recognition of not only interfaith civil marriages performed abroad, but also overseas same-sex civil marriages).

The act of marriage usually creates normative or legal obligations between the individuals involved, and any offspring they may produce or adopt. In terms of legal recognition, most sovereign states and other jurisdictions limit marriage to opposite-sex couples and a diminishing number of these permit polygyny, child marriages, and forced marriages. Over the twentieth century, a growing number of countries and other jurisdictions have lifted bans on and have established legal recognition for interracial marriage, interfaith marriage, and most recently, gender-neutral marriage. Some cultures allow the dissolution of marriage through divorce or annulment. In some areas, child marriages and polygamy may occur in spite of national laws against the practice.

Since the late twentieth century, major social changes in Western countries have led to changes in the demographics of marriage, with the age of first marriage increasing, fewer people marrying, and more couples choosing to cohabit rather than marry. For example, the number of marriages in Europe decreased by 30% from 1975 to 2005.

Historically, in most cultures, married women had very few rights of their own, being considered, along with the family's children, the property of the husband; as such, they could not own or inherit property, or represent themselves legally (see for example coverture). In Europe, the United States, and other places in the developed world, beginning in the late 19th century and lasting through the 21st century, marriage has undergone gradual legal changes, aimed at improving the rights of the wife. These changes included giving wives legal identities of their own, abolishing the right of husbands to physically discipline their wives, giving wives property rights, liberalizing divorce laws, providing wives with reproductive rights of their own, and requiring a wife's consent when sexual relations occur. These changes have occurred primarily in Western countries. In the 21st century, there continue to be controversies regarding the legal status of married women, legal acceptance of or leniency towards violence within marriage (especially sexual violence), traditional marriage customs such as dowry and bride price, forced marriage, marriageable age, and criminalization of consensual behaviors such as premarital and extramarital sex.

The word "marriage" derives from Middle English "mariage", which first appears in 1250–1300 CE. This in turn is derived from Old French, "marier" (to marry), and ultimately Latin, "marītāre", meaning to provide with a husband or wife and "marītāri" meaning to get married. The adjective "marīt-us -a, -um" meaning matrimonial or nuptial could also be used in the masculine form as a noun for "husband" and in the feminine form for "wife". The related word "matrimony" derives from the Old French word "matremoine", which appears around 1300 CE and ultimately derives from Latin "mātrimōnium", which combines the two concepts: "mater" meaning "mother" and the suffix -"monium" signifying "action, state, or condition".

Anthropologists have proposed several competing definitions of marriage in an attempt to encompass the wide variety of marital practices observed across cultures. Even within Western culture, "definitions of marriage have careened from one extreme to another and everywhere in between" (as Evan Gerstmann has put it).

In "The History of Human Marriage" (1922), Edvard Westermarck defined marriage as "a more or less durable connection between male and female lasting beyond the mere act of propagation till after the birth of the offspring." In "The Future of Marriage in Western Civilization" (1936), he rejected his earlier definition, instead provisionally defining marriage as "a relation of one or more men to one or more women that is recognized by custom or law".

The anthropological handbook "Notes and Queries" (1951) defined marriage as "a union between a man and a woman such that children born to the woman are the recognized legitimate offspring of both partners." In recognition of a practice by the Nuer people of Sudan allowing women to act as a husband in certain circumstances (the ghost marriage), Kathleen Gough suggested modifying this to "a woman and one or more other persons."

In an analysis of marriage among the Nayar, a polyandrous society in India, Gough found that the group lacked a husband role in the conventional sense; that unitary role in the west was divided between a non-resident "social father" of the woman's children, and her lovers who were the actual procreators. None of these men had legal rights to the woman's child. This forced Gough to disregard sexual access as a key element of marriage and to define it in terms of legitimacy of offspring alone: marriage is "a relationship established between a woman and one or more other persons, which provides a child born to the woman under circumstances not prohibited by the rules of relationship, is accorded full birth-status rights common to normal members of his society or social stratum."

Economic anthropologist Duran Bell has criticized the legitimacy-based definition on the basis that some societies do not require marriage for legitimacy. He argued that a legitimacy-based definition of marriage is circular in societies where illegitimacy has no other legal or social implications for a child other than the mother being unmarried.

Edmund Leach criticized Gough's definition for being too restrictive in terms of recognized legitimate offspring and suggested that marriage be viewed in terms of the different types of rights it serves to establish. In 1955 article in "Man", Leach argued that no one definition of marriage applied to all cultures. He offered a list of ten rights associated with marriage, including sexual monopoly and rights with respect to children, with specific rights differing across cultures. Those rights, according to Leach, included:

In a 1997 article in "Current Anthropology", Duran Bell describes marriage as "a relationship between one or more men (male or female) in severalty to one or more women that provides those men with a demand-right of sexual access within a domestic group and identifies women who bear the obligation of yielding to the demands of those specific men." In referring to "men in severalty", Bell is referring to corporate kin groups such as lineages which, in having paid brideprice, retain a right in a woman's offspring even if her husband (a lineage member) deceases (Levirate marriage). In referring to "men (male or female)", Bell is referring to women within the lineage who may stand in as the "social fathers" of the wife's children born of other lovers. (See Nuer "ghost marriage")

Monogamy is a form of marriage in which an individual has only one spouse during their lifetime or at any one time (serial monogamy).

Anthropologist Jack Goody's comparative study of marriage around the world utilizing the Ethnographic Atlas found a strong correlation between intensive plough agriculture, dowry and monogamy. This pattern was found in a broad swath of Eurasian societies from Japan to Ireland. The majority of Sub-Saharan African societies that practice extensive hoe agriculture, in contrast, show a correlation between "bride price" and polygamy. A further study drawing on the Ethnographic Atlas showed a statistical correlation between increasing size of the society, the belief in "high gods" to support human morality, and monogamy.

In the countries which do not permit polygamy, a person who marries in one of those countries a person while still being lawfully married to another commits the crime of bigamy. In all cases, the second marriage is considered legally null and void. Besides the second and subsequent marriages being void, the bigamist is also liable to other penalties, which also vary between jurisdictions.

Governments that support monogamy may allow easy divorce. In a number of Western countries divorce rates approach 50%. Those who remarry do so on average three times. Divorce and remarriage can thus result in "serial monogamy", i.e. having multiple marriages but only one legal spouse at a time. This can be interpreted as a form of plural mating, as are those societies dominated by female-headed families in the Caribbean, Mauritius and Brazil where there is frequent rotation of unmarried partners. In all, these account for 16 to 24% of the "monogamous" category.

Serial monogamy creates a new kind of relative, the "ex-". The "ex-wife", for example, remains an active part of her "ex-husband's" or "ex-wife's" life, as they may be tied together by transfers of resources (alimony, child support), or shared child custody. Bob Simpson notes that in the British case, serial monogamy creates an "extended family" – a number of households tied together in this way, including mobile children (possible exes may include an ex-wife, an ex-brother-in-law, etc., but not an "ex-child"). These "unclear families" do not fit the mould of the monogamous nuclear family. As a series of connected households, they come to resemble the polygynous model of separate households maintained by mothers with children, tied by a male to whom they are married or divorced.

Polygamy is a marriage which includes more than two partners. When a man is married to more than one wife at a time, the relationship is called polygyny, and there is no marriage bond between the wives; and when a woman is married to more than one husband at a time, it is called polyandry, and there is no marriage bond between the husbands. If a marriage includes multiple husbands and/or wives, it can be called group marriage.

A molecular genetic study of global human genetic diversity argued that sexual polygyny was typical of human reproductive patterns until the shift to sedentary farming communities approximately 10,000 to 5,000 years ago in Europe and Asia, and more recently in Africa and the Americas. As noted above, Anthropologist Jack Goody's comparative study of marriage around the world utilizing the Ethnographic Atlas found that the majority of Sub-Saharan African societies that practice extensive hoe agriculture show a correlation between "Bride price" and polygamy. A survey of other cross-cultural samples has confirmed that the absence of the plough was the only predictor of polygamy, although other factors such as high male mortality in warfare (in non-state societies) and pathogen stress (in state societies) had some impact.

Marriages are classified according to the number of legal spouses an individual has. The suffix "-gamy" refers specifically to the number of spouses, as in bi-gamy (two spouses, generally illegal in most nations), and poly-gamy (more than one spouse).

Societies show variable acceptance of polygamy as a cultural ideal and practice. According to the Ethnographic Atlas, of 1,231 societies noted, 186 were monogamous; 453 had occasional polygyny; 588 had more frequent polygyny; and 4 had polyandry. However, as Miriam Zeitzen writes, social tolerance for polygamy is different from the practice of polygamy, since it requires wealth to establish multiple households for multiple wives. The actual practice of polygamy in a tolerant society may actually be low, with the majority of aspirant polygamists practicing monogamous marriage. Tracking the occurrence of polygamy is further complicated in jurisdictions where it has been banned, but continues to be practiced ("de facto polygamy").

Zeitzen also notes that Western perceptions of African society and marriage patterns are biased by "contradictory concerns of nostalgia for traditional African culture versus critique of polygamy as oppressive to women or detrimental to development." Polygamy has been condemned as being a form of human rights abuse, with concerns arising over domestic abuse, forced marriage, and neglect. The vast majority of the world's countries, including virtually all of the world's developed nations, do not permit polygamy. There have been calls for the abolition of polygamy in developing countries.

Polygyny usually grants wives equal status, although the husband may have personal preferences. One type of de facto polygyny is concubinage, where only one woman gets a wife's rights and status, while other women remain legal house mistresses.

Although a society may be classified as polygynous, not all marriages in it necessarily are; monogamous marriages may in fact predominate. It is to this flexibility that Anthropologist Robin Fox attributes its success as a social support system: "This has often meant – given the imbalance in the sex ratios, the higher male infant mortality, the shorter life span of males, the loss of males in wartime, etc. – that often women were left without financial support from husbands. To correct this condition, females had to be killed at birth, remain single, become prostitutes, or be siphoned off into celibate religious orders. Polygynous systems have the advantage that they can promise, as did the Mormons, a home and family for every woman."

Nonetheless, polygyny is a gender issue which offers men asymmetrical benefits. In some cases, there is a large age discrepancy (as much as a generation) between a man and his youngest wife, compounding the power differential between the two. Tensions not only exist "between" genders, but also "within" genders; senior and junior men compete for wives, and senior and junior wives in the same household may experience radically different life conditions, and internal hierarchy. Several studies have suggested that the wive's relationship with other women, including co-wives and husband's female kin, are more critical relationships than that with her husband for her productive, reproductive and personal achievement. In some societies, the co-wives are relatives, usually sisters, a practice called "sororal polygyny"; the pre-existing relationship between the co-wives is thought to decrease potential tensions within the marriage.

Fox argues that "the major difference between polygyny and monogamy could be stated thus: while plural mating occurs in both systems, under polygyny several unions may be recognized as being legal marriages while under monogamy only one of the unions is so recognized. Often, however, it is difficult to draw a hard and fast line between the two."

As polygamy in Africa is increasingly subject to legal limitations, a variant form of "de facto" (as opposed to legal or "de jure") polygyny is being practised in urban centres. Although it does not involve multiple (now illegal) formal marriages, the domestic and personal arrangements follow old polygynous patterns. The de facto form of polygyny is found in other parts of the world as well (including some Mormon sects and Muslim families in the United States).
In some societies such as the Lovedu of South Africa, or the Nuer of the Sudan, aristocratic women may become female 'husbands.' In the Lovedu case, this female husband may take a number of polygamous wives. This is not a lesbian relationship, but a means of legitimately expanding a royal lineage by attaching these wives' children to it. The relationships are considered polygynous, not polyandrous, because the female husband is in fact assuming masculine gendered political roles.

Religious groups have differing views on the legitimacy of polygyny. It is allowed in Islam and Confucianism. Judaism and Christianity have mentioned practices involving polygyny in the past, however, outright religious acceptance of such practices was not addressed until its rejection in later passages. They do explicitly prohibit polygyny today.

Polyandry is notably more rare than polygyny, though less rare than the figure commonly cited in the "Ethnographic Atlas" (1980) which listed only those polyandrous societies found in the Himalayan Mountains. More recent studies have found 53 societies outside the 28 found in the Himalayans which practice polyandry. It is most common in egalitarian societies marked by high male mortality or male absenteeism. It is associated with "partible paternity", the cultural belief that a child can have more than one father.

The explanation for polyandry in the Himalayan Mountains is related to the scarcity of land; the marriage of all brothers in a family to the same wife ("fraternal polyandry") allows family land to remain intact and undivided. If every brother married separately and had children, family land would be split into unsustainable small plots. In Europe, this was prevented through the social practice of impartible inheritance (the dis-inheriting of most siblings, some of whom went on to become celibate monks and priests).

Group marriage (also known as "multi-lateral marriage") is a form of polyamory in which more than two persons form a family unit, with all the members of the group marriage being considered to be married to all the other members of the group marriage, and all members of the marriage share parental responsibility for any children arising from the marriage. No country legally condones group marriages, neither under the law nor as a common law marriage, but historically it has been practiced by some cultures of Polynesia, Asia, Papua New Guinea and the Americas – as well as in some intentional communities and alternative subcultures such as the Oneida Perfectionists in up-state New York. Of the 250 societies reported by the American anthropologist George Murdock in 1949, only the Kaingang of Brazil had any group marriages at all.

A child marriage is a marriage where one or both spouses are under the age of 18. It is related to child betrothal and teenage pregnancy.

Child marriage was common throughout history, even up until the 1900s in the United States, where in 1880 CE, in the state of Delaware, the age of consent for marriage was 7 years old. Still, in 2017, over half of the 50 United States have no explicit minimum age to marry and several states set the age as low as 14. Today it is condemned by international human rights organizations. Child marriages are often arranged between the families of the future bride and groom, sometimes as soon as the girl is born. However, in the late 1800s in England and the United States, feminist activists began calling for raised age of consent laws, which was eventually handled in the 1920s, having been raised to 16-18.

Child marriages can also occur in the context of bride kidnapping.

In the year 1552 CE, John Somerford and Jane Somerford Brereton were both married at the ages of 3 and 2, respectively. Twelve years later, in 1564, John filed for divorce.

While child marriage is observed for both boys and girls, the overwhelming majority of child spouses are girls. In many cases, only one marriage-partner is a child, usually the female, due to the importance placed upon female virginity. Causes of child marriage include poverty, bride price, dowry, laws that allow child marriages, religious and social pressures, regional customs, fear of remaining unmarried, and perceived inability of women to work for money.

Today, child marriages are widespread in parts of the world; being most common in South Asia and sub-Saharan Africa, with more than half of the girls in some countries in those regions being married before 18. The incidence of child marriage has been falling in most parts of the world. In developed countries child marriage is outlawed or restricted.

Girls who marry before 18 are at greater risk of becoming victims of domestic violence, than those who marry later, especially when they are married to a much older man.

As noted above, several kinds of same-sex, non-sexual marriages exist in some lineage-based societies. This section relates to same-sex sexual unions. Some cultures include third gender (two-spirit or transgender) individuals, such as the berdache of the Zuni in New Mexico. We'wha, one of the most revered Zuni elders (an Ihamana, spiritual leader) served as an emissary of the Zuni to Washington, where he met President Grover Cleveland. We'wha had a husband who was generally recognized as such.

While it is a relatively new practice to grant same-sex couples the same form of legal marital recognition as commonly granted to mixed-sex couples, there is some history of recorded same-sex unions around the world. Ancient Greek same-sex relationships were like modern companionate marriages, unlike their different-sex marriages in which the spouses had few emotional ties, and the husband had freedom to engage in outside sexual liaisons. The Codex Theodosianus ("C. Th." 9.7.3) issued in 438 CE imposed severe penalties or death on same-sex relationships, but the exact intent of the law and its relation to social practice is unclear, as only a few examples of same-sex relationships in that culture exist. Same-sex unions were celebrated in some regions of China, such as Fujian. Possibly the earliest documented same-sex wedding in Latin Christendom occurred in Rome, Italy, at the San Giovanni a Porta Latina basilica in 1581.

Several cultures have practiced temporary and conditional marriages. Examples include the Celtic practice of handfasting and fixed-term marriages in the Muslim community. Pre-Islamic Arabs practiced a form of temporary marriage that carries on today in the practice of Nikah mut‘ah, a fixed-term marriage contract. The Islamic prophet Muhammad sanctioned a temporary marriage – sigheh in Iran and muta'a in Iraq – which can provide a legitimizing cover for sex workers. The same forms of temporary marriage have been used in Egypt, Lebanon and Iran to make the donation of a human ova legal for in vitro fertilisation; a woman cannot, however, use this kind of marriage to obtain a sperm donation. Muslim controversies related to Nikah Mut'ah have resulted in the practice being confined mostly to Shi'ite communities. The matrilineal Mosuo of China practice what they call "walking marriage".

In some jurisdictions cohabitation, in certain circumstances, may constitute a common-law marriage, an unregistered partnership, or otherwise provide the unmarried partners with various rights and responsibilities; and in some countries the laws recognize cohabitation in lieu of institutional marriage for taxation and social security benefits. This is the case, for example, in Australia. Cohabitation may be an option pursued as a form of resistance to traditional institutionalized marriage. However, in this context, some nations reserve the right to define the relationship as marital, or otherwise to regulate the relation, even if the relation has not been registered with the state or a religious institution.

Conversely, institutionalized marriages may not involve cohabitation. In some cases couples living together do not wish to be recognized as married. This may occur because pension or alimony rights are adversely affected; because of taxation considerations; because of immigration issues, or for other reasons. Such marriages have also been increasingly common in Beijing. Guo Jianmei, director of the center for women's studies at Beijing University, told a Newsday correspondent, "Walking marriages reflect sweeping changes in Chinese society." A "walking marriage" refers to a type of temporary marriage formed by the Mosuo of China, in which male partners live elsewhere and make nightly visits. A similar arrangement in Saudi Arabia, called misyar marriage, also involves the husband and wife living separately but meeting regularly.

There is wide cross-cultural variation in the social rules governing the selection of a partner for marriage. There is variation in the degree to which partner selection is an individual decision by the partners or a collective decision by the partners' kin groups, and there is variation in the rules regulating which partners are valid choices.

The United Nations World Fertility Report of 2003 reports that 89% of all people get married before age forty-nine. The percent of women and men who marry before age forty-nine drops to nearly 50% in some nations and reaches near 100% in other nations.

In other cultures with less strict rules governing the groups from which a partner can be chosen the selection of a marriage partner may involve either the couple going through a selection process of courtship or the marriage may be arranged by the couple's parents or an outside party, a matchmaker.

Some people want to marry a person with higher or lower status than them. Others want to marry people who have similar status. In many societies women marry men who are of higher social status. There are marriages where each party has sought a partner of similar status. There are other marriages in which the man is older than the woman.

Societies have often placed restrictions on marriage to relatives, though the degree of prohibited relationship varies widely. Marriages between parents and children, or between full siblings, with few exceptions, have been considered incest and forbidden. However, marriages between more distant relatives have been much more common, with one estimate being that 80% of all marriages in history have been between second cousins or closer. This proportion has fallen dramatically, but still more than 10% of all marriages are believed to be between people who are second cousins or more closely related. In the United States, such marriages are now highly stigmatized, and laws ban most or all first-cousin marriage in 30 states. Specifics vary: in South Korea, historically it was illegal to marry someone with the same last name and same ancestral line.

An Avunculate marriage is a marriage that occurs between an uncle and his niece or between an aunt and her nephew. Such marriages are illegal in most countries due to incest restrictions. However, a small number of countries have legalized it, including Argentina, Australia, Austria, Malaysia, and Russia.
In various societies the choice of partner is often limited to suitable persons from specific social groups. In some societies the rule is that a partner is selected from an individual's own social group – endogamy, this is often the case in class- and caste-based societies. But in other societies a partner must be chosen from a different group than one's own – exogamy, this may be the case in societies practicing totemic religion where society is divided into several exogamous totemic clans, such as most Aboriginal Australian societies. In other societies a person is expected to marry their cross-cousin, a woman must marry her father's sister's son and a man must marry his mother's brother's daughter – this is often the case if either a society has a rule of tracing kinship exclusively through patrilineal or matrilineal descent groups as among the Akan people of West Africa. Another kind of marriage selection is the levirate marriage in which widows are obligated to marry their husband's brother, mostly found in societies where kinship is based on endogamous clan groups.

Religion has commonly weighed in on the matter of which relatives, if any, are allowed to marry. Relations may be by consanguinity or affinity, meaning by blood or by marriage. On the marriage of cousins, Catholic policy has evolved from initial acceptance, through a long period of general prohibition, to the contemporary requirement for a dispensation. Islam has always allowed it, while Hindu texts vary widely.

In a wide array of lineage-based societies with a classificatory kinship system, potential spouses are sought from a specific class of relative as determined by a prescriptive marriage rule. This rule may be expressed by anthropologists using a "descriptive" kinship term, such as a "man's mother's brother's daughter" (also known as a "cross-cousin"). Such descriptive rules mask the participant's perspective: a man should marry a woman from his mother's lineage. Within the society's kinship terminology, such relatives are usually indicated by a specific term which sets them apart as potentially marriageable. Pierre Bourdieu notes, however, that very few marriages ever follow the rule, and that when they do so, it is for "practical kinship" reasons such as the preservation of family property, rather than the "official kinship" ideology.

Insofar as regular marriages following prescriptive rules occur, lineages are linked together in fixed relationships; these ties between lineages may form political alliances in kinship dominated societies. French structural anthropologist Claude Lévi-Strauss developed alliance theory to account for the "elementary" kinship structures created by the limited number of prescriptive marriage rules possible.

A pragmatic (or 'arranged') marriage is made easier by formal procedures of family or group politics. A responsible authority sets up or encourages the marriage; they may, indeed, engage a professional matchmaker to find a suitable spouse for an unmarried person. The authority figure could be parents, family, a religious official, or a group consensus. In some cases, the authority figure may choose a match for purposes other than marital harmony.

A forced marriage is a marriage in which one or both of the parties is married against their will. Forced marriages continue to be practiced in parts of the world, especially in South Asia and Africa. The line between forced marriage and consensual marriage may become blurred, because the social norms of these cultures dictate that one should never oppose the desire of one's parents/relatives in regard to the choice of a spouse; in such cultures it is not necessary for violence, threats, intimidation etc. to occur, the person simply "consents" to the marriage even if he/she doesn't want it, out of the implied social pressure and duty. The customs of bride price and dowry, that exist in parts of the world, can lead to buying and selling people into marriage.

In some societies, ranging from Central Asia to the Caucasus to Africa, the custom of bride kidnapping still exists, in which a woman is captured by a man and his friends. Sometimes this covers an elopement, but sometimes it depends on sexual violence. In previous times, "raptio" was a larger-scale version of this, with groups of women captured by groups of men, sometimes in war; the most famous example is The Rape of the Sabine Women, which provided the first citizens of Rome with their wives.

Other marriage partners are more or less imposed on an individual. For example, widow inheritance provides a widow with another man from her late husband's brothers.

In rural areas of India, child marriage is practiced, with parents often arranging the wedding, sometimes even before the child is born. This practice was made illegal under the Child Marriage Restraint Act of 1929.

The financial aspects of marriage vary between cultures and have changed over time.

In some cultures, dowries and bridewealth continue to be required today. In both cases, the financial arrangements are usually made between the groom (or his family) and the bride's family; with the bride often not being involved in the negotiations, and often not having a choice in whether to participate in the marriage.

In Early modern Britain, the social status of the couple was supposed to be equal. After the marriage, all the property (called "fortune") and expected inheritances of the wife belonged to the husband.

A dowry is "a process whereby parental property is distributed to a daughter at her marriage (i.e. "inter vivos") rather than at the holder's death ("mortis causa")… A dowry establishes some variety of conjugal fund, the nature of which may vary widely. This fund ensures her support (or endowment) in widowhood and eventually goes to provide for her sons and daughters."

In some cultures, especially in countries such as Turkey, India, Bangladesh, Pakistan, Sri Lanka, Morocco, Nepal, dowries continue to be expected. In India, thousands of dowry-related deaths have taken place on yearly basis, to counter this problem, several jurisdictions have enacted laws restricting or banning dowry (see Dowry law in India). In Nepal, dowry was made illegal in 2009. Some authors believe that the giving and receiving of dowry reflects the status and even the effort to climb high in social hierarchy.

Direct Dowry contrasts with bridewealth, which is paid by the groom or his family to the bride's parents, and with indirect dowry (or dower), which is property given to the bride herself by the groom at the time of marriage and which remains under her ownership and control.

In the Jewish tradition, the rabbis in ancient times insisted on the marriage couple entering into a prenuptial agreement, called a "ketubah". Besides other things, the "ketubah" provided for an amount to be paid by the husband in the event of a divorce or his estate in the event of his death. This amount was a replacement of the biblical dower or bride price, which was payable at the time of the marriage by the groom to the father of the bride. This innovation was put in place because the biblical bride price created a major social problem: many young prospective husbands could not raise the bride price at the time when they would normally be expected to marry. So, to enable these young men to marry, the rabbis, in effect, delayed the time that the amount would be payable, when they would be more likely to have the sum. It may also be noted that both the dower and the "ketubah" amounts served the same purpose: the protection for the wife should her support cease, either by death or divorce. The only difference between the two systems was the timing of the payment. It is the predecessor to the wife's present-day entitlement to maintenance in the event of the breakup of marriage, and family maintenance in the event of the husband not providing adequately for the wife in his will. Another function performed by the "ketubah" amount was to provide a disincentive for the husband contemplating divorcing his wife: he would need to have the amount to be able to pay to the wife.

Morning gifts, which might also be arranged by the bride's father rather than the bride, are given to the bride herself; the name derives from the Germanic tribal custom of giving them the morning after the wedding night. She might have control of this morning gift during the lifetime of her husband, but is entitled to it when widowed. If the amount of her inheritance is settled by law rather than agreement, it may be called dower. Depending on legal systems and the exact arrangement, she may not be entitled to dispose of it after her death, and may lose the property if she remarries. Morning gifts were preserved for centuries in morganatic marriage, a union where the wife's inferior social status was held to prohibit her children from inheriting a noble's titles or estates. In this case, the morning gift would support the wife and children. Another legal provision for widowhood was jointure, in which property, often land, would be held in joint tenancy, so that it would automatically go to the widow on her husband's death.

Islamic tradition has similar practices. A 'mahr', either immediate or deferred, is the woman's portion of the groom's wealth (divorce) or estate (death). These amounts are usually set on the basis of the groom's own and family wealth and incomes, but in some parts these are set very high so as to provide a disincentive for the groom exercising the divorce, or the husband's family 'inheriting' a large portion of the estate, especially if there are no male offspring from the marriage. In some countries, including Iran, the mahr or alimony can amount to more than a man can ever hope to earn, sometimes up to US$1,000,000 (4000 official Iranian gold coins). If the husband cannot pay the mahr, either in case of a divorce or on demand, according to the current laws in Iran, he will have to pay it by installments. Failure to pay the mahr might even lead to imprisonment.

Bridewealth is a common practice in parts of Southeast Asia (Thailand, Cambodia), parts of Central Asia, and in much of sub-Saharan Africa. It is also known as brideprice although this has fallen in disfavor as it implies the purchase of the bride. Bridewealth is the amount of money or property or wealth paid by the groom or his family to the parents of a woman upon the marriage of their daughter to the groom. In anthropological literature, bride price has often been explained as payment made to compensate the bride's family for the loss of her labor and fertility. In some cases, bridewealth is a means by which the groom's family's ties to the children of the union are recognized.

In some countries a married person or couple benefits from various taxation advantages not available to a single person. For example, spouses may be allowed to average their combined incomes. This is advantageous to a married couple with disparate incomes. To compensate for this, countries may provide a higher tax bracket for the averaged income of a married couple. While income averaging might still benefit a married couple with a stay-at-home spouse, such averaging would cause a married couple with roughly equal personal incomes to pay more total tax than they would as two single persons. In the United States, this is called the marriage penalty.

When the rates applied by the tax code are not based income averaging, but rather on the "sum" of individuals' incomes, higher rates will usually apply to each individual in a two-earner households in a progressive tax systems. This is most often the case with high-income taxpayers and is another situation called a marriage penalty.

Conversely, when progressive tax is levied on the individual with no consideration for the partnership, dual-income couples fare much better than single-income couples with similar household incomes. The effect can be increased when the welfare system treats the same income as a shared income thereby denying welfare access to the non-earning spouse. Such systems apply in Australia and Canada, for example.

In many Western cultures, marriage usually leads to the formation of a new household comprising the married couple, with the married couple living together in the same home, often sharing the same bed, but in some other cultures this is not the tradition. Among the Minangkabau of West Sumatra, residency after marriage is matrilocal, with the husband moving into the household of his wife's mother. Residency after marriage can also be patrilocal or avunculocal. In these cases, married couples may not form an independent household, but remain part of an extended family household.

Early theories explaining the determinants of postmarital residence connected it with the sexual division of labor. However, to date, cross-cultural tests of this hypothesis using worldwide samples have failed to find any significant relationship between these two variables. However, Korotayev's tests show that the female contribution to subsistence does correlate significantly with matrilocal residence in general. However, this correlation is masked by a general polygyny factor.

Although, in different-sex marriages, an increase in the female contribution to subsistence tends to lead to matrilocal residence, it also tends simultaneously to lead to general non-sororal polygyny which effectively destroys matrilocality. If this polygyny factor is controlled (e.g., through a multiple regression model), division of labor turns out to be a significant predictor of postmarital residence. Thus, Murdock's hypotheses regarding the relationships between the sexual division of labor and postmarital residence were basically correct, though the actual relationships between those two groups of variables are more complicated than he expected.

There has been a trend toward the neolocal residence in western societies.

Marriage laws refer to the legal requirements which determine the validity of a marriage, which vary considerably between countries.

A marriage bestows rights and obligations on the married parties, and sometimes on relatives as well, being the sole mechanism for the creation of affinal ties (in-laws). These may include, depending on jurisdiction:

These rights and obligations vary considerably between societies, and between groups within society. These might include arranged marriages, family obligations, the legal establishment of a nuclear family unit, the legal protection of children and public declaration of commitment.

In many countries today, each marriage partner has the choice of keeping his or her property separate or combining properties. In the latter case, called community property, when the marriage ends by divorce each owns half. In lieu of a will or trust, property owned by the deceased generally is inherited by the surviving spouse.

In some legal systems, the partners in a marriage are "jointly liable" for the debts of the marriage. This has a basis in a traditional legal notion called the "Doctrine of Necessities" whereby, in a heterosexual marriage, a husband was responsible to provide necessary things for his wife. Where this is the case, one partner may be sued to collect a debt for which they did not expressly contract. Critics of this practice note that debt collection agencies can abuse this by claiming an unreasonably wide range of debts to be expenses of the marriage. The cost of defense and the burden of proof is then placed on the non-contracting party to prove that the expense is not a debt of the family. The respective maintenance obligations, both during and eventually after a marriage, are regulated in most jurisdictions; alimony is one such method.

Marriage is an institution that is historically filled with restrictions. From age, to race, to social status, to consanguinity, to gender, restrictions are placed on marriage by society for reasons of benefiting the children, passing on healthy genes, maintaining cultural values, or because of prejudice and fear. Almost all cultures that recognize marriage also recognize adultery as a violation of the terms of marriage.

Most jurisdictions set a minimum age for marriage, that is, a person must attain a certain age to be legally allowed to marry. This age may depend on circumstances, for instance exceptions from the general rule may be permitted if the parents of a young person express their consent and/or if a court decides that said marriage is in the best interest of the young person (often this applies in cases where a girl is pregnant). Although most age restrictions are in place in order to prevent children from being forced into marriages, especially to much older partners – marriages which can have negative education and health related consequences, and lead to child sexual abuse and other forms of violence – such child marriages remain common in parts of the world. According to the UN, child marriages are most common in rural sub-Saharan Africa and South Asia. The ten countries with the highest rates of child marriage are: Niger (75%), Chad, Central African Republic, Bangladesh, Guinea, Mozambique, Mali, Burkina Faso, South Sudan, and Malawi.

To prohibit incest and eugenic reasons, marriage laws have set restrictions for relatives to marry. Direct blood relatives are usually prohibited to marry, while for branch line relatives, laws are wary.

Laws banning "race-mixing" were enforced in certain North American jurisdictions from 1691 until 1967, in Nazi Germany (The Nuremberg Laws) from 1935 until 1945, and in South Africa during most part of the Apartheid era (1949–1985). All these laws primarily banned marriage between persons of different racially or ethnically defined groups, which was termed "amalgamation" or "miscegenation" in the U.S. The laws in Nazi Germany and many of the U.S. states, as well as South Africa, also banned sexual relations between such individuals.

In the United States, laws in some but not all of the states prohibited the marriage of whites and blacks, and in many states also the intermarriage of whites with Native Americans or Asians. In the U.S., such laws were known as anti-miscegenation laws. From 1913 until 1948, 30 out of the then 48 states enforced such laws. Although an "Anti-Miscegenation Amendment" to the United States Constitution was proposed in 1871, in 1912–1913, and in 1928, no nationwide law against racially mixed marriages was ever enacted. In 1967, the Supreme Court of the United States unanimously ruled in "Loving v. Virginia" that anti-miscegenation laws are unconstitutional. With this ruling, these laws were no longer in effect in the remaining 16 states that still had them.

The Nazi ban on interracial marriage and interracial sex was enacted in September 1935 as part of the Nuremberg Laws, the "Gesetz zum Schutze des deutschen Blutes und der deutschen Ehre" (The Law for the Protection of German Blood and German Honour). The Nuremberg Laws classified Jews as a race and forbade marriage and extramarital sexual relations at first with people of Jewish descent, but was later ended to the "Gypsies, Negroes or their bastard offspring" and people of "German or related blood". Such relations were marked as "Rassenschande" (lit. "race-disgrace") and could be punished by imprisonment (usually followed by deportation to a concentration camp) and even by death.

South Africa under apartheid also banned interracial marriage. The Prohibition of Mixed Marriages Act, 1949 prohibited marriage between persons of different races, and the Immorality Act of 1950 made sexual relations with a person of a different race a crime.

The first laws in modern times recognizing same-sex marriage were enacted during the first decade of the 21st century. , same-sex marriage is legally recognized (nationwide or in some parts) in the following countries: Argentina, Australia, Belgium, Brazil, Canada, Colombia, Denmark, Finland, France, Germany, Iceland, Ireland, Luxembourg, Malta, Mexico, the Netherlands, New Zealand, Norway, Portugal, South Africa, Spain, Sweden, the United Kingdom, the United States and Uruguay. It is also likely to soon become legal in Taiwan and Austria, after court rulings on the subject in May and December 2017, respectively.

Introduction of same-sex marriage laws has varied by jurisdiction, being variously accomplished through a legislative change to marriage laws, a court ruling based on constitutional guarantees of equality, or by direct popular vote (via an initiative or a referendum). The recognition of same-sex marriage is a political, social, civil rights and religious issue in many nations, and debates continue to arise over whether same-sex couples should be allowed marriage, be required to hold a different status (a civil union), or be denied recognition of such rights. Allowing same-gender couples to legally marry is considered to be one of the most important of all LGBT rights.

Polygyny is widely practiced in mostly Muslim and African countries. In the Middle Eastern region, Israel, Turkey and Tunisia are notable exceptions.

In most other jurisdictions, polygamy is illegal. For example, In the United States, polygamy is illegal in all 50 states.

Over a century ago, citizens of the self-governing territory of what is present-day Utah were forced by the United States federal government to abandon the practice of polygamy through the vigorous enforcement of several Acts of Congress and eventually complied. The Church of Jesus Christ of Latter-day Saints formally abolished the practice in 1890, in a document labeled 'The Manifesto'. Among American Muslims, a small minority of around 50,000 to 100,000 people are estimated to live in families with a husband maintaining an illegal polygamous relationship.

Several countries such as India and Sri Lanka, permit only their Islamic citizens to practice polygamy. Some Indians have converted to Islam in order to bypass such legal restrictions. Predominantly Christian nations usually do not allow polygamous unions, with a handful of exceptions being the Republic of the Congo, Uganda, and Zambia. Myanmar (frequently referred to as Burma) is also the only predominantly Buddhist nation to allow for civil polygynous marriages, though such is rarely tolerated by the Burmese population.

In various jurisdictions, a civil marriage may take place as part of the religious marriage ceremony, although they are theoretically distinct. Some jurisdictions allow civil marriages in circumstances which are notably not allowed by particular religions, such as same-sex marriages or civil unions.

The opposite case may happen as well. Partners may not have full juridical acting capacity and churches may have less strict limits than the civil jurisdictions. This particularly applies to minimum age, or physical infirmities.

It is possible for two people to be recognised as married by a religious or other institution, but not by the state, and hence without the legal rights and obligations of marriage; or to have a civil marriage deemed invalid and sinful by a religion. Similarly, a couple may remain married in religious eyes after a civil divorce.

A marriage is usually formalized at a wedding or marriage ceremony. The ceremony may be officiated either by a religious official, by a government official or by a state approved celebrant. In various European and some Latin American countries, any religious ceremony must be held separately from the required civil ceremony. Some countries – such as Belgium, Bulgaria, France, the Netherlands, Romania and Turkey – require that a civil ceremony take place before any religious one. In some countries – notably the United States, Canada, the United Kingdom, the Republic of Ireland, Norway and Spain – both ceremonies can be held together; the officiant at the religious and civil ceremony also serving as agent of the state to perform the civil ceremony. To avoid any implication that the state is "recognizing" a religious marriage (which is prohibited in some countries) – the "civil" ceremony is said to be taking place at the same time as the religious ceremony. Often this involves simply signing a register during the religious ceremony. If the civil element of the religious ceremony is omitted, the marriage ceremony is not recognized as a marriage by government under the law.

Some countries, such as Australia, permit marriages to be held in private and at any location; others, including England and Wales, require that the civil ceremony be conducted in a place open to the public and specially sanctioned by law for the purpose. In England, the place of marriage formerly had to be a church or register office, but this was extended to any public venue with the necessary licence. An exception can be made in the case of marriage by special emergency license (UK: licence), which is normally granted only when one of the parties is terminally ill. Rules about where and when persons can marry vary from place to place. Some regulations require one of the parties to reside within the jurisdiction of the register office (formerly parish).

Each religious authority has rules for the manner in which marriages are to be conducted by their officials and members. Where religious marriages are recognised by the state, the officiator must also conform with the law of the jurisdiction.

In a small number of jurisdictions marriage relationships may be created by the operation of the law alone. Unlike the typical ceremonial marriage with legal contract, wedding ceremony, and other details, a common-law marriage may be called "marriage by habit and repute (cohabitation)." A de facto common-law marriage without a license or ceremony is legally binding in some jurisdictions but has no legal consequence in others.

A "civil union", also referred to as a "civil partnership", is a legally recognized form of partnership similar to marriage. Beginning with Denmark in 1989, civil unions under one name or another have been established by law in several countries in order to provide same-sex couples rights, benefits, and responsibilities similar (in some countries, identical) to opposite-sex civil marriage. In some jurisdictions, such as Brazil, New Zealand, Uruguay, Ecuador, France and the U.S. states of Hawaii and Illinois, civil unions are also open to opposite-sex couples.

Sometimes people marry to take advantage of a certain situation, sometimes called a marriage of convenience or a sham marriage. For example, according to one publisher of information about green card marriages, "Every year over 450,000 United States citizens marry foreign-born individuals and petition for them to obtain a permanent residency (Green Card) in the United States." While this is likely an overestimate, in 2003 alone 184,741 immigrants were admitted to the U.S. as spouses of U.S. citizens. More were admitted as fiancés of US citizens for the purpose of being married within 90 days. Regardless of the number of people entering the US to marry a US citizen, it does not indicate the number of these marriages that are convenience marriages, which number could include some of those with the motive of obtaining permanent residency, but also include people who are US citizens. One example would be to obtain an inheritance that has a marriage clause. Another example would be to save money on health insurance or to enter a health plan with preexisting conditions offered by the new spouse's employer. Other situations exist, and, in fact, all marriages have a complex combination of conveniences motivating the parties to marry. A marriage of convenience is one that is devoid of normal reasons to marry. In certain countries like Singapore sham marriages like these are punishable criminal offences.

People have proposed arguments against marriage for reasons that include political, philosophical and religious criticisms; concerns about the divorce rate; individual liberty and gender equality; questioning the necessity of having a personal relationship sanctioned by government or religious authorities; or the promotion of celibacy for religious or philosophical reasons.

Feminist theory approaches opposite-sex marriage as an institution traditionally rooted in patriarchy that promotes male superiority and power over women. This power dynamic conceptualizes men as "the provider operating in the public sphere" and women as "the caregivers operating within the private sphere". "Theoretically, women ... [were] defined as the property of their husbands ... The adultery of a woman was always treated with more severity than that of a man." "[F]eminist demands for a wife's control over her own property were not met [in parts of Britain] until ... [laws were passed in the late 19th century]."

Traditional heterosexual marriage imposed an obligation of the wife to be sexually available for her husband and an obligation of the husband to provide material/financial support for the wife. Numerous philosophers, feminists and other academic figures have commented on this throughout history, condemning the hypocrisy of legal and religious authorities in regard to sexual issues; pointing to the lack of choice of a woman in regard to controlling her own sexuality; and drawing parallels between marriage, an institution promoted as sacred, and prostitution, widely condemned and vilified (though often tolerated as a "necessary evil"). Mary Wollstonecraft, in the 18th century, described marriage as "legal prostitution". Emma Goldman wrote in 1910: "To the moralist prostitution does not consist so much in the fact that the woman sells her body, but rather that she sells it out of wedlock". Bertrand Russell in his book Marriage and Morals wrote that:"Marriage is for woman the commonest mode of livelihood, and the total amount of undesired sex endured by women is probably greater in marriage than in prostitution." Angela Carter in Nights at the Circus wrote: "What is marriage but prostitution to one man instead of many?"

Some critics object to what they see as propaganda in relation to marriage – from the government, religious organizations, the media – which aggressively promote marriage as a solution for all social problems; such propaganda includes, for instance, marriage promotion in schools, where children, especially girls, are bombarded with positive information about marriage, being presented only with the information prepared by authorities.

The performance of dominant gender roles by men and submissive gender roles by women influence the power dynamic of a heterosexual marriage. In some American households, women internalize gender role stereotypes and often assimilate into the role of "wife", "mother", and "caretaker" in conformity to societal norms and their male partner. Author bell hooks states "within the family structure, individuals learn to accept sexist oppression as 'natural' and are primed to support other forms of oppression, including heterosexist domination." "[T]he cultural, economic, political and legal supremacy of the husband" was "[t]raditional ... under English law". This patriarchal dynamic is contrasted with a conception of egalitarian or Peer Marriage in which power and labour are divided equally, and not according to gender roles.

In the US, studies have shown that, despite egalitarian ideals being common, less than half of respondents viewed their opposite-sex relationships as equal in power, with unequal relationships being more commonly dominated by the male partner. Studies also show that married couples find the highest level of satisfaction in egalitarian relationships and lowest levels of satisfaction in wife dominate relationships. In recent years, egalitarian or Peer Marriages have been receiving increasing focus and attention politically, economically and culturally in a number of countries, including the United States.

Different societies demonstrate variable tolerance of extramarital sex. The Standard Cross-Cultural Sample describes the occurrence of extramarital sex by gender in over 50 pre-industrial cultures. The occurrence of extramarital sex by men is described as "universal" in 6 cultures, "moderate" in 29 cultures, "occasional" in 6 cultures, and "uncommon" in 10 cultures. The occurrence of extramarital sex by women is described as "universal" in 6 cultures, "moderate" in 23 cultures, "occasional" in 9 cultures, and "uncommon" in 15 cultures. Three studies using nationally representative samples in the United States found that between 10–15% of women and 20–25% of men engage in extramarital sex.

Many of the world's major religions look with disfavor on sexual relations outside marriage. There are non-secular states that sanction criminal penalties for sexual intercourse before marriage. Sexual relations by a married person with someone other than his/her spouse is known as adultery. Adultery is considered in many jurisdictions to be a crime and grounds for divorce.

In some countries, such as Saudi Arabia, Pakistan, Afghanistan, Iran, Kuwait, Maldives, Morocco, Oman, Mauritania, United Arab Emirates, Sudan, Yemen, any form of sexual activity outside marriage is illegal.

In some parts of the world, women and girls accused of having sexual relations outside marriage are at risk of becoming victims of honor killings committed by their families. In 2011 several people were sentenced to death by stoning after being accused of adultery in Iran, Somalia, Afghanistan, Sudan, Mali and Pakistan. Practices such as honor killings and stoning continue to be supported by mainstream politicians and other officials in some countries. In Pakistan, after the 2008 Balochistan honour killings in which five women were killed by tribesmen of the Umrani Tribe of Balochistan, Pakistani Federal Minister for Postal Services Israr Ullah Zehri defended the practice; he said: "These are centuries-old traditions, and I will continue to defend them. Only those who indulge in immoral acts should be afraid."

An issue that is a serious concern regarding marriage and which has been the object of international scrutiny is that of sexual violence within marriage. Throughout much of the history, in most cultures, sex in marriage was considered a 'right', that could be taken by force (often by a man from a woman), if 'denied'. As the concept of human rights started to develop in the 20th century, and with the arrival of second-wave feminism, such views have become less widely held.

The legal and social concept of marital rape has developed in most industrialized countries in the mid- to late 20th century; in many other parts of the world it is not recognized as a form of abuse, socially or legally. Several countries in Eastern Europe and Scandinavia made marital rape illegal before 1970, and other countries in Western Europe and the English-speaking Western world outlawed it in the 1980s and 1990s. In England and Wales, marital rape was made illegal in 1991. Although marital rape is being increasingly criminalized in developing countries too, cultural, religious, and traditional ideologies about "conjugal rights" remain very strong in many parts of the world; and even in many countries that have adequate laws against rape in marriage these laws are rarely enforced.

Apart from the issue of rape committed against one's spouse, marriage is, in many parts of the world, closely connected with other forms of sexual violence: in some places, like Morocco, unmarried girls and women who are raped are often forced by their families to marry their rapist. Because being the victim of rape and losing virginity carry extreme social stigma, and the victims are deemed to have their "reputation" tarnished, a marriage with the rapist is arranged. This is claimed to be in the advantage of both the victim – who does not remain unmarried and doesn't lose social status – and of the rapist, who avoids punishment. In 2012, after a Moroccan 16-year-old girl committed suicide after having been forced by her family to marry her rapist and enduring further abuse by the rapist after they married, there have been protests from activists against this practice which is common in Morocco.

In some societies, the very high social and religious importance of marital fidelity, especially female fidelity, has as result the criminalization of adultery, often with harsh penalties such as stoning or flogging; as well as leniency towards punishment of violence related to infidelity (such as honor killings). In the 21st century, criminal laws against adultery have become controversial with international organizations calling for their abolition. Opponents of adultery laws argue that these laws are a major contributor to discrimination and violence against women, as they are enforced selectively mostly against women; that they prevent women from reporting sexual violence; and that they maintain social norms which justify violent crimes committed against women by husbands, families and communities. A Joint Statement by the United Nations Working Group on discrimination against women in law and in practice states that "Adultery as a criminal offence violates women's human rights". Some human rights organizations argue that the criminalization of adultery also violates internationally recognized protections for private life, as it represents an arbitrary interference with an individual's privacy, which is not permitted under international law.

The laws surrounding heterosexual marriage in many countries have come under international scrutiny because they contradict international standards of human rights; institutionalize violence against women, child marriage and forced marriage; require the permission of a husband for his wife to work in a paid job, sign legal documents, file criminal charges against someone, sue in civil court etc.; sanction the use by husbands of violence to "discipline" their wives; and discriminate against women in divorce.

Such things were legal even in many Western countries until recently: for instance, in France, married women obtained the right to work without their husband's permission in 1965, and in West Germany women obtained this right in 1977 (by comparison women in East Germany had many more rights). In Spain, during Franco's era, a married woman needed her husband's consent, referred to as the "permiso marital", for almost all economic activities, including employment, ownership of property, and even traveling away from home; the "permiso marital" was abolished in 1975.

An absolute submission of a wife to her husband is accepted as natural in many parts of the world, for instance surveys by UNICEF have shown that the percentage of women aged 15–49 who think that a husband is justified in hitting or beating his wife under certain circumstances is as high as 90% in Afghanistan and Jordan, 87% in Mali, 86% in Guinea and Timor-Leste, 81% in Laos, 80% in Central African Republic. Detailed results from Afghanistan show that 78.4% of women agree with a beating if the wife "goes out without telling him [the husband]" and 76.2% agree "if she argues with him".

Throughout history, and still today in many countries, laws have provided for extenuating circumstances, partial or complete defenses, for men who killed their wives due to adultery, with such acts often being seen as crimes of passion and being covered by legal defenses such as provocation or defense of family honor.

While international law and conventions recognize the need for consent for entering a marriage - namely that people cannot be forced to get married against their will - the right to obtain a divorce is not recognized; therefore holding a person in a marriage against their will (if such person has consented to entering in it) is not considered a violation of human rights, with the issue of divorce being left at the appreciation of individual states. The European Court of Human Rights has repeatedly ruled that under the European Convention on Human Rights there is neither a right to apply to divorce, nor a right to obtain the divorce if applied for it; in 2017, in "Babiarz v. Poland", the Court ruled that Poland was entitled to deny a divorce because the grounds for divorce were not met, even if the marriage in question was acknowledged both by Polish courts and by the ECHR as being a legal fiction involving a long-term separation where the husband lived with another woman with whom he had an 11-year-old child.

In the EU, the last country to allow divorce was Malta, in 2011. Around the world, the only countries to forbid divorce are Philippines and Vatican City, although in practice in many countries which use a fault-based divorce system obtaining a divorce is very difficult. The ability to divorce, in law and practice, has been and continues to be a controversial issue in many countries, and public discourse involves different ideologies such as feminism, social conservatism, religious interpretations.

In recent years, the customs of dowry and bride price have received international criticism for inciting conflicts between families and clans; contributing to violence against women; promoting materialism; increasing property crimes (where men steal goods such as cattle in order to be able to pay the bride price); and making it difficult for poor people to marry. African women's rights campaigners advocate the abolishing of bride price, which they argue is based on the idea that women are a form of property which can be bought. Bride price has also been criticized for contributing to child trafficking as impoverished parents sell their young daughters to rich older men. A senior Papua New Guinea police officer has called for the abolishing of bride price arguing that it is one of the main reasons for the mistreatment of women in that country. The opposite practice of dowry has been linked to a high level of violence (see Dowry death) and to crimes such as extortion.

Historically, and still in many countries, children born outside marriage suffered severe social stigma and discrimination. In England and Wales, such children were known as bastards and whoresons.

There are significant differences between world regions in regard to the social and legal position of non-marital births, ranging from being fully accepted and uncontroversial to being severely stigmatized and discriminated.

The 1975 European Convention on the Legal Status of Children Born out of Wedlock protects the rights of children born to unmarried parents. The convention states, among others, that: "The father and mother of a child born out of wedlock shall have the same obligation to maintain the child as if it were born in wedlock" and that "A child born out of wedlock shall have the same right of succession in the estate of its father and its mother and of a member of its father's or mother's family, as if it had been born in wedlock."

While in most Western countries legal inequalities between children born inside and outside marriage have largely been abolished, this is not the case in some parts of the world.

The legal status of an unmarried father differs greatly from country to country. Without voluntary formal recognition of the child by the father, in most cases there is a need of due process of law in order to establish paternity. In some countries however, unmarried cohabitation of a couple for a specific period of time does create a presumption of paternity similar to that of formal marriage. This is the case in Australia. Under what circumstances can a paternity action be initiated, the rights and responsibilities of a father once paternity has been established (whether he can obtain parental responsibility and whether he can be forced to support the child) as well as the legal position of a father who voluntarily acknowledges the child, vary widely by jurisdiction. A special situation arises when a married woman has a child by a man other than her husband. Some countries, such as Israel, refuse to accept a legal challenge of paternity in such a circumstance, in order to avoid the stigmatization of the child (see Mamzer, a concept under Jewish law). In 2010, the European Court of Human Rights ruled in favor of a German man who had fathered twins with a married woman, granting him right of contact with the twins, despite the fact that the mother and her husband had forbidden him to see the children.

The steps that an unmarried father must take in order to obtain rights to his child vary by country. In some countries (such as the UK – since 2003 in England and Wales, 2006 in Scotland, and 2002 in Northern Ireland) it is sufficient for the father to be listed on the birth certificate for him to have parental rights; in other countries, such as Ireland, simply being listed on the birth certificate does not offer any rights, additional legal steps must be taken (if the mother agrees, the parents can both sign a "statutory declaration", but if the mother does not agree, the father has to apply to court).

Children born outside marriage have become more common, and in some countries, the majority. Recent data from Latin America showed figures for non-marital childbearing to be 74% for Colombia, 69% for Peru, 68% for Chile, 66% for Brazil, 58% for Argentina, 55% for Mexico. In 2012, in the European Union, 40% of births were outside marriage, and in the United States, in 2013, the figure was similar, at 40.6%. In the United Kingdom 47.6% of births were to unmarried women in 2012; in Ireland the figure was 35.1%.

During the first half of the 20th century, unmarried women in some Western countries were coerced by authorities to give their children up for adoption. This was especially the case in Australia, through the forced adoptions in Australia, with most of these adoptions taking place between the 1950s and the 1970s. In 2013, Julia Gillard, then Prime Minister of Australia, offered a national apology to those affected by the forced adoptions.

Some married couples choose not to have children. Others are unable to have children because of infertility or other factors preventing conception or the bearing of children. In some cultures, marriage imposes an "obligation" on women to bear children. In northern Ghana, for example, payment of bridewealth signifies a woman's requirement to bear children, and women using birth control face substantial threats of physical abuse and reprisals.

Religions develop in specific geographic and social milieux.
Unsurprisingly, religious attitudes and practices relating to marriage can vary.
The precepts of mainstream religions include, as a rule, unequivocal prescriptions for marriage, establishing both rituals and rules of conduct.

The Bahá'í Faith encourages marriage and views it as a mutually strengthening bond, but it is not obligatory. A Bahá'í marriage requires the couple to choose each other, and then obtain the consent of all living parents.

Christian marriages are based upon the teachings of Jesus and the Paul the Apostle. many Christian denominations regard marriage as a sacrament, sacred institution, or covenant, but this was not always the case before the 1184 Council of Verona officially recognized marriage as a sacrament. Before then, no specific ritual was prescribed for celebrating a marriage: "Marriage vows did not have to be exchanged in a church, nor was a priest's presence required. A couple could exchange consent anywhere, anytime."

Decrees on marriage of the Roman Catholic Council of Trent (twenty-fourth session of 1563) made the validity of marriage dependent on the wedding occurring in the presence of a priest and two witnesses. The absence of a requirement of parental consent ended a debate that proceeded from the 12th century. In the case of a civil divorce, the innocent spouse had and has no right to marry again until the death of the other spouse terminates the still valid marriage, even if the other spouse was guilty of adultery.

The Christian Church performed marriages in the narthex of the church prior to the 16th century, when the emphasis was on the marital contract and betrothal. Subsequently, the ceremony moved inside the sacristy of the church.

Christians often marry for religious reasons, ranging from following the biblical injunction for a "man to leave his father and mother and cleave to his wife, and the two shall become one", to accessing the Divine grace of the Roman Catholic Sacrament.

Catholics, Eastern Orthodox, as well as many Anglicans and Methodists, consider marriage termed "holy matrimony" to be an expression of divine grace, termed a "sacrament" and "mystery" in the first two Christian traditions. In Western ritual, the ministers of the sacrament are the spouses themselves, with a bishop, priest, or deacon merely witnessing the union on behalf of the Church and blessing it. In Eastern ritual churches, the bishop or priest functions as the actual minister of the Sacred Mystery; Eastern Orthodox deacons may not perform marriages. Western Christians commonly refer to marriage as a vocation, while Eastern Christians consider it an ordination and a martyrdom, though the theological emphases indicated by the various names are not excluded by the teachings of either tradition. Marriage is commonly celebrated in the context of a Eucharistic service (a nuptial Mass or Divine Liturgy). The sacrament of marriage is indicative of the relationship between Christ and the Church.

The Roman Catholic tradition of the 12th and 13th centuries defined marriage as a sacrament ordained by God, signifying the mystical marriage of Christ to his Church.
The matrimonial covenant, by which a man and a woman establish between themselves a partnership of the whole of life, is by its nature ordered toward the good of the spouses and the procreation and education of offspring; this covenant between baptized persons has been raised by Christ the Lord to the dignity of a sacrament.
For Catholic and Methodist Christians, the mutual love between man and wife becomes an image of the eternal love with which God loves humankind. In the United Methodist Church, the celebration of Holy Matrimony ideally occurs in the context of a Service of Worship, which includes the celebration of the Eucharist. Likewise, the celebration of marriage between two Catholics normally takes place during the public liturgical celebration of the Holy Mass, because of its sacramental connection with the unity of the Paschal mystery of Christ (Communion). Sacramental marriage confers a perpetual and exclusive bond between the spouses. By its nature, the institution of marriage and conjugal love is ordered to the procreation and upbringing of offspring. Marriage creates rights and duties in the Church between the spouses and towards their children: "[e]ntering marriage with the intention of never having children is a grave wrong and more than likely grounds for an annulment". According to current Roman Catholic legislation, progeny of annulled relationships are considered legitimate. Civilly remarried persons who civilly divorced a living and lawful spouse are not separated from the Church, but they cannot receive Eucharistic Communion.

Divorce and remarriage, while generally not encouraged, are regarded differently by each Christian denomination. Most Protestant Churches allow persons to marry again after a divorce, while other require an annulment. The Eastern Orthodox Church allows divorce for a limited number of reasons, and in theory, but usually not in practice, requires that a marriage after divorce be celebrated with a penitential overtone. With respect to marriage between a Christian and a pagan, the early Church "sometimes took a more lenient view, invoking the so-called Pauline privilege of permissible separation (1 Cor. 7) as legitimate grounds for allowing a convert to divorce a pagan spouse and then marry a Christian."

The Catholic Church adheres to the proscription of Jesus in "Matthew", 19: 6 that married spouses who have consummated their marriage "are no longer two, but one flesh. Therefore, what God has joined together, no human being must separate.” Consequently, the Catholic Church understands that it is wholly without authority to terminate a sacramentally valid and consummated marriage, and its "Codex Iuris Canonici" (1983 Code of Canon Law) confirms this in Canons 1055-7. Specifically, Canon 1056 declares that "the essential properties of marriage are unity and "indissolubility"; in [C]hristian marriage they acquire a distinctive "firmness" by reason of the sacrament." Canon 1057, §2 declares that marriage is "an "irrevocable" covenant". Therefore, divorce of such a marriage is a metaphysical, moral, and legal impossibility. However, the Church has the authority to annul a presumed "marriage" by declaring it to have been invalid from the beginning, i. e., declaring it not to be and never to have been a marriage, in an annulment procedure, which is basically a fact-finding and fact-declaring effort.

For Protestant denominations, the purposes of marriage include intimate companionship, rearing children, and mutual support for both spouses to fulfill their life callings. Most Reformed Christians did not regard marriage to the status of a sacrament "because they did not regard matrimony as a necessary means of grace for salvation"; nevertheless it is considered a covenant between spouses before God. In addition, some Protestant denominations (such as the Methodist Churches) affirmed that Holy Matrimony is a "means of grace, thus, sacramental in character".

Since the 16th century, five competing models of marriage have shaped Protestant marriage and legal tradition:

Members of The Church of Jesus Christ of Latter-day Saints (LDS Church) believe that "marriage between a man and a woman is ordained of God and that the family is central to the Creator's plan for the eternal destiny of His children." Their view of marriage is that family relationships can endure beyond the grave. This is known as 'eternal marriage' which can be eternal only when authorized priesthood holders perform the sealing ordinance in sacred temples.

Although many Christian denominations do not currently perform same-sex marriages, many do, such as the Presbyterian Church (U.S.A), some dioceses of the Episcopal Church, the Metropolitan Community Church, Quakers, United Church of Canada, and United Church of Christ congregations, and some Anglican dioceses, for example. Same-sex marriage is recognized by various religious denominations.

Islam also commends marriage, with the age of marriage being whenever the individuals feel ready, financially and emotionally.

In Islam, polygyny is allowed while polyandry is not, with the specific limitation that a man can have no more than four legal wives at any one time and an unlimited number of female slaves as concubines, with the requirement that the man is able and willing to partition his time and wealth equally among the respective wives.

For a Muslim wedding to take place, the bridegroom and the guardian of the bride ("wali") must both agree on the marriage. Should the guardian disagree on the marriage, it may not legally take place. If the "wali" of the girl her father or paternal grandfather, he has the right to force her into marriage even against her proclaimed will, if it is her first marriage. A guardian who is allowed to force the bride into marriage is called "wali mujbir".

From an Islamic (Sharia) law perspective, the minimum requirements and responsibilities in a Muslim marriage are that the groom provide living expenses (housing, clothing, food, maintenance) to the bride, and in return, the bride's main responsibility is raising children to be proper Muslims. All other rights and responsibilities are to be decided between the husband and wife, and may even be included as stipulations in the marriage contract before the marriage actually takes place, so long as they do not go against the minimum requirements of the marriage.

In Sunni Islam, marriage must take place in the presence of at least two reliable witnesses, with the consent of the guardian of the bride and the consent of the groom. Following the marriage, the couple may consummate the marriage. To create an 'urf marriage, it is sufficient that a man and a woman indicate an intention to marry each other and recite the requisite words in front of a suitable Muslim. The wedding party usually follows but can be held days, or months later, whenever the couple and their families want to; however, there can be no concealment of the marriage as it is regarded as public notification due to the requirement of witnesses.

In Shia Islam, marriage may take place without the presence of witnesses as is often the case in temporary Nikah mut‘ah (prohibited in Sunni Islam), but with the consent of both the bride and the groom. Following the marriage they may consummate their marriage.

In Judaism, marriage is based on the laws of the Torah and is a contractual bond between spouses in which the spouses dedicate to be exclusive to one another. This contract is called Kiddushin. Though procreation is not the sole purpose, a Jewish marriage is also expected to fulfill the commandment to have children. The main focus centers around the relationship between the spouses. Kabbalistically, marriage is understood to mean that the spouses are merging into a single soul. This is why a man is considered "incomplete" if he is not married, as his soul is only one part of a larger whole that remains to be unified.

The Hebrew Bible (Christian Old Testament) describes a number of marriages, including those of Isaac (), Jacob () and Samson (). Polygyny, or men having multiple wives at once, is one of the most common marital arrangements represented in the Hebrew Bible. Today Ashkenazi Jews are prohibited to take more than one wife because of a ban instituted on this by Gershom ben Judah (Died 1040).

Among ancient Hebrews, marriage was a domestic affair and not a religious ceremony; the participation of a priest or rabbi was not required.

Betrothal ("erusin"), which refers to the time that this binding contract is made, is distinct from marriage itself ("nissu'in"), with the time between these events varying substantially.
In biblical times, a wife was regarded as chattel, belonging to her husband; the descriptions of the Bible suggest that she would be expected to perform tasks such as spinning, sewing, weaving, manufacture of clothing, fetching of water, baking of bread, and animal husbandry. However, wives were usually looked after with care, and men with more than one wife were expected to ensure that they continue to give the first wife food, clothing, and marital rights.

Since a wife was regarded as property, her husband was originally free to divorce her for any reason, at any time. Divorcing a woman against her will was also banned by Gershom ben Judah. A divorced couple were permitted to get back together, unless the wife had married someone else after her divorce.

Hinduism sees marriage as a sacred duty that entails both religious and social obligations. Old Hindu literature in Sanskrit gives many different types of marriages and their categorization ranging from "Gandharva Vivaha" (instant marriage by mutual consent of participants only, without any need for even a single third person as witness) to normal (present day) marriages, to "Rakshasa Vivaha" ("demoniac" marriage, performed by abduction of one participant by the other participant, usually, but not always, with the help of other persons). In India and generally in South Asia, arranged marriages, the spouse's parents or an older family member choose the partner, are still predominant in comparison with so called love marriages until nowadays. The Hindu Widow's Remarriage Act 1856 empowers a Hindu widow to remarry.

According to some estimates, there wasn't even 1% of divorce among Hindu arranged marriages.

The Buddhist view of marriage considers marriage a secular affair and thus not a sacrament. Buddhists are expected to follow the civil laws regarding marriage laid out by their respective governments. Gautama Buddha, being a kshatriya was required by Shakyan tradition to pass a series of tests to prove himself as a warrior, before he was allowed to marry.

In a Sikh marriage, the couple walks around the "Guru Granth Sahib" holy book four times, and a holy man recites from it in the kirtan style. The ceremony is known as 'Anand Karaj' and represents the holy union of two souls united as one.

Wiccan marriages are commonly known as handfastings. Although handfastings vary for each Wiccan they often involve honoring Wiccan gods. Sex is considered a pious and sacred activity.

Marriage, like other close relationships, exerts considerable influence on health. Married people experience lower morbidity and mortality across such diverse health threats as cancer, heart attacks, and surgery. Research on marriage and health is part of the broader study of the benefits of social relationships.

Social ties provide people with a sense of identity, purpose, belonging, and support. Simply being married, as well as the quality of one's marriage, have been linked to diverse measures of health.

The health-protective effect of marriage is stronger for men than women. Marital status—the simple fact of being married—confers more health benefits to men than women.

Women's health is more strongly impacted than men's by marital conflict or satisfaction, such that unhappily married women do not enjoy better health relative to their single counterparts. Most research on marriage and health has focused on heterosexual couples; more work is needed to clarify the health impacts of same-sex marriage.

In most societies, the death of one of the partners terminates the marriage, and in monogamous societies this allows the other partner to remarry, though sometimes after a waiting or mourning period.

In some societies, a marriage can be annulled, when an authority declares that a marriage never happened. Jurisdictions often have provisions for void marriages or voidable marriages.

A marriage may also be terminated through divorce. Countries that have relatively recently legalized divorce are Italy (1970), Portugal (1975), Brazil (1977), Spain (1981), Argentina (1987), Paraguay (1991), Colombia (1991), Ireland (1996), Chile (2004) and Malta (2011). As of 2012, the Philippines and the Vatican City are the only jurisdictions which do not allow divorce (this is currently under discussion in Philippines).)
After divorce, one spouse may have to pay alimony. Laws concerning divorce and the ease with which a divorce can be obtained vary widely around the world. After a divorce or an annulment, the people concerned are free to remarry (or marry).

A statutory right of two married partners to mutually consent to divorce was enacted in western nations in the mid-20th century. In the United States no-fault divorce was first enacted in California in 1969 and the final state to legalize it was New York in 1989.

About 45% of marriages in Britain and, according to a 2009 study, 46% of marriages in the U.S. end in divorce.

The history of marriage is often considered under History of the family or legal history.

Many cultures have legends concerning the origins of marriage. The way in which a marriage is conducted and its rules and ramifications has changed over time, as has the institution itself, depending on the culture or demographic of the time.
According to ancient Hebrew tradition, a wife was seen as being property of high value and was, therefore, usually, carefully looked after. Early nomadic communities in the middle east practised a form of marriage known as "beena", in which a wife would own a tent of her own, within which she retains complete independence from her husband; this principle appears to survive in parts of early Israelite society, as some early passages of the Bible appear to portray certain wives as each owning a tent as a personal possession (specifically, Jael, Sarah, and Jacob's wives).

The husband, too, is indirectly implied to have some responsibilities to his wife. The Covenant Code orders "If he take him another; her food, her clothing, and her duty of marriage, shall he not diminish(or lessen)". If the husband does not provide the first wife with these things, she is to be divorced, without cost to her. The Talmud interprets this as a requirement for a man to provide food and clothing to, and have sex with, each of his wives. However, "duty of marriage" is also interpreted as whatever one does as a married couple, which is more than just sexual activity. And the term diminish, which means to lessen, shows the man must treat her as if he was not married to another.

As a polygynous society, the Israelites did not have any laws that imposed marital fidelity on men. However, the prophet Malachi states that none should be faithless to the wife of his youth and that God hates divorce. Adulterous married women, adulterous betrothed women, and the men who slept with them however, were subject to the death penalty by the biblical laws against adultery According to the Priestly Code of the Book of Numbers, if a pregnant woman was suspected of adultery, she was to be subjected to the Ordeal of Bitter Water, a form of trial by ordeal, but one that took a miracle to convict. The literary prophets indicate that adultery was a frequent occurrence, despite their strong protests against it, and these legal strictnesses.

In ancient Greece, no specific civil ceremony was required for the creation of a heterosexual marriage – only mutual agreement and the fact that the couple must regard each other as husband and wife accordingly. Men usually married when they were in their 20s and women in their teens. It has been suggested that these ages made sense for the Greeks because men were generally done with military service or financially established by their late 20s, and marrying a teenage girl ensured ample time for her to bear children, as life expectancies were significantly lower. Married Greek women had few rights in ancient Greek society and were expected to take care of the house and children. Time was an important factor in Greek marriage. For example, there were superstitions that being married during a full moon was good luck and, according to Robert Flacelière, Greeks married in the winter. Inheritance was more important than feelings: a woman whose father dies without male heirs could be forced to marry her nearest male relative – even if she had to divorce her husband first.

There were several types of marriages in ancient Roman society. The traditional ("conventional") form called "conventio in manum" required a ceremony with witnesses and was also dissolved with a ceremony. In this type of marriage, a woman lost her family rights of inheritance of her old family and gained them with her new one. She now was subject to the authority of her husband. There was the free marriage known as "sine manu". In this arrangement, the wife remained a member of her original family; she stayed under the authority of her father, kept her family rights of inheritance with her old family and did not gain any with the new family. The minimum age of marriage for girls was 12.

Among ancient Germanic tribes, the bride and groom were roughly the same age and generally older than their Roman counterparts, at least according to Tacitus:
The youths partake late of the pleasures of love, and hence pass the age of puberty unexhausted: nor are the virgins hurried into marriage; the same maturity, the same full growth is required: the sexes unite equally matched and robust; and the children inherit the vigor of their parents.
Where Aristotle had set the prime of life at 37 years for men and 18 for women, the Visigothic Code of law in the 7th century placed the prime of life at 20 years for both men and women, after which both presumably married. Tacitus states that ancient Germanic brides were on average about 20 and were roughly the same age as their husbands. Tacitus, however, had never visited the German-speaking lands and most of his information on Germania comes from secondary sources. In addition, Anglo-Saxon women, like those of other Germanic tribes, are marked as women from the age of 12 and older, based on archaeological finds, implying that the age of marriage coincided with puberty.

From the early Christian era (30 to 325 CE), marriage was thought of as primarily a private matter, with no uniform religious or other ceremony being required. However, bishop Ignatius of Antioch writing around 110 to bishop Polycarp of Smyrna exhorts, "[I]t becomes both men and women who marry, to form their union with the approval of the bishop, that their marriage may be according to God, and not after their own lust."

In 12th-century Europe, women took the surname of their husbands and starting in the second half of the 16th century parental consent along with the church's consent was required for marriage.

With few local exceptions, until 1545, Christian marriages in Europe were by mutual consent, declaration of intention to marry and upon the subsequent physical union of the parties. The couple would promise verbally to each other that they would be married to each other; the presence of a priest or witnesses was not required. This promise was known as the "verbum." If freely given and made in the present tense (e.g., "I marry you"), it was unquestionably binding; if made in the future tense ("I will marry you"), it would constitute a betrothal.

In 1552 a wedding took place in Zufia, Navarre, between Diego de Zufia and Mari-Miguel following the custom as it was in the realm since the Middle Ages, but the man denounced the marriage on the grounds that its validity was conditioned to "riding" her (""si te cabalgo, lo cual dixo de bascuence (...) balvin yo baneça aren senar içateko""). The tribunal of the kingdom rejected the husband's claim, validating the wedding, but the husband appealed to the tribunal in Zaragoza, and this institution annulled the marriage. According to the Charter of Navarre, the basic union consisted of a civil marriage with no priest required and at least two witnesses, and the contract could be broken using the same formula. The Church in turn lashed out at those who got married twice or thrice in a row while their formers spouses were still alive. In 1563 the Council of Trent, twenty-fourth session, required that a valid marriage must be performed by a priest before two witnesses.

One of the functions of churches from the Middle Ages was to register marriages, which was not obligatory. There was no state involvement in marriage and personal status, with these issues being adjudicated in ecclesiastical courts. During the Middle Ages marriages were arranged, sometimes as early as birth, and these early pledges to marry were often used to ensure treaties between different royal families, nobles, and heirs of fiefdoms. The church resisted these imposed unions, and increased the number of causes for nullification of these arrangements. As Christianity spread during the Roman period and the Middle Ages, the idea of free choice in selecting marriage partners increased and spread with it.

In Medieval Western Europe, later marriage and higher rates of definitive celibacy (the so-called "European marriage pattern") helped to constrain patriarchy at its most extreme level. For example, Medieval England saw marriage age as variable depending on economic circumstances, with couples delaying marriage until the early twenties when times were bad and falling to the late teens after the Black Death, when there were labor shortages; by appearances, marriage of adolescents was not the norm in England. Where the strong influence of classical Celtic and Germanic cultures (which were not rigidly patriarchal) helped to offset the Judaeo-Roman patriarchal influence, in Eastern Europe the tradition of early and universal marriage (often in early adolescence) as well as traditional Slavic patrilocal custom led to a greatly inferior status of women at all levels of society.

The average age of marriage for most of Northwestern Europe from 1500 to 1800 was around 25 years of age; as the Church dictated that both parties had to be at least 21 years of age to marry without the consent of their parents, the bride and groom were roughly the same age, with most brides in their early twenties and most grooms two or three years older, and a substantial number of women married for the first time in their thirties and forties, particularly in urban areas, with the average age at first marriage rising and falling as circumstances dictated. In better times, more people could afford to marry earlier and thus fertility rose and conversely marriages were delayed or forgone when times were bad, thus restricting family size; after the Black Death, the greater availability of profitable jobs allowed more people to marry young and have more children, but the stabilization of the population in the 16th century meant fewer job opportunities and thus more people delaying marriages.

The age of marriage was not absolute, however, as child marriages occurred throughout the Middle Ages and later, with just some of them including:

As part of the Protestant Reformation, the role of recording marriages and setting the rules for marriage passed to the state, reflecting Martin Luther's view that marriage was a "worldly thing". By the 17th century, many of the Protestant European countries had a state involvement in marriage.

In England, under the Anglican Church, marriage by consent and cohabitation was valid until the passage of Lord Hardwicke's Act in 1753. This act instituted certain requirements for marriage, including the performance of a religious ceremony observed by witnesses.

As part of the Counter-Reformation, in 1563 the Council of Trent decreed that a Roman Catholic marriage would be recognized only if the marriage ceremony was officiated by a priest with two witnesses. The Council also authorized a Catechism, issued in 1566, which defined marriage as "The conjugal union of man and woman, contracted between two qualified persons, which obliges them to live together throughout life."

In the early modern period, John Calvin and his Protestant colleagues reformulated Christian marriage by enacting the Marriage Ordinance of Geneva, which imposed "The dual requirements of state registration and church consecration to constitute marriage" for recognition.

In England and Wales, Lord Hardwicke's Marriage Act 1753 required a formal ceremony of marriage, thereby curtailing the practice of Fleet Marriage, an irregular or a clandestine marriage. These were clandestine or irregular marriages performed at Fleet Prison, and at hundreds of other places. From the 1690s until the Marriage Act of 1753 as many as 300,000 clandestine marriages were performed at Fleet Prison alone. The Act required a marriage ceremony to be officiated by an Anglican priest in the Anglican Church with two witnesses and registration. The Act did not apply to Jewish marriages or those of Quakers, whose marriages continued to be governed by their own customs.

In England and Wales, since 1837, civil marriages have been recognized as a legal alternative to church marriages under the Marriage Act 1836. In Germany, civil marriages were recognized in 1875. This law permitted a declaration of the marriage before an official clerk of the civil administration, when both spouses affirm their will to marry, to constitute a legally recognized valid and effective marriage, and allowed an optional private clerical marriage ceremony.

In contemporary English common law, a marriage is a voluntary contract by a man and a woman, in which by agreement they choose to become husband and wife. Edvard Westermarck proposed that "the institution of marriage has probably developed out of a primeval habit".

As of 2000, the average marriage age range was 25–44 years for men and 22–39 years for women.

The mythological origin of Chinese heterosexual marriage is a story about Nüwa and Fu Xi who invented proper marriage procedures after becoming married. In ancient Chinese society, people of the same surname are supposed to consult with their family trees prior to marriage to reduce the potential risk of unintentional incest. Marrying one's maternal relatives was generally not thought of as incest. Families sometimes intermarried from one generation to another. Over time, Chinese people became more geographically mobile. Individuals remained members of their biological families. When a couple died, the husband and the wife were buried separately in the respective clan's graveyard. In a maternal marriage a male would become a son-in-law who lived in the wife's home.

The New Marriage Law of 1950 radically changed Chinese heterosexual marriage traditions, enforcing monogamy, equality of men and women, and choice in marriage; arranged marriages were the most common type of marriage in China until then. Starting October 2003, it became legal to marry or divorce without authorization from the couple's work units. Although people with infectious diseases such as AIDS may now marry, marriage is still illegal for the mentally ill.




</doc>
<doc id="19731" url="https://en.wikipedia.org/wiki?curid=19731" title="Midgard">
Midgard

Midgard (an anglicised form of Old Norse ; Old English , Swedish and Danish "Midgård", Old Saxon , Old High German , Gothic "Midjun-gards"; "middle yard") is the name for Earth (equivalent in meaning to the Greek term , "inhabited") inhabited by and known to humans in early Germanic cosmology, and specifically one of the Nine Worlds in Norse mythology.

This name occurs in Old Norse literature as . In Old Saxon "Heliand" it appears as and in Old High German poem "Muspilli" it appears as . The Gothic form is attested in the Gospel of Luke as a translation of the Greek word . The word is present in Old English epic and poetry as ; later transformed to or ("Middle-earth") in Middle English literature.

All these forms are from a Common Germanic "*midja-gardaz" ("*meddila-", "*medjan-"), a compound of "*midja-" "middle" and "*gardaz" "yard, enclosure".
In early Germanic cosmology, the term stands alongside "world" (Old English "weorold", Old Saxon "werold", Old High German "weralt", Old Frisian "warld" and Old Norse "verǫld"), from a Common Germanic compound "*wira-alđiz", the "age of men".

Midgard is a realm in Norse mythology. It is one of the Nine Worlds—the only one that is completely visible to mankind (the others may intersect with this visible realm but are mostly invisible). Pictured as placed somewhere in the middle of Yggdrasil, Midgard is between the land of Niflheim—the land of ice—to the north and Muspelheim—the land of fire—to the south. Midgard is surrounded by a world of water, or ocean, that is impassable. The ocean is inhabited by the great sea serpent Jörmungandr (Miðgarðsormr), who is so huge that he encircles the world entirely, grasping his own tail. The concept is similar to that of the Ouroboros. Midgard was also connected to Asgard, the home of the gods, by the Bifröst, the rainbow bridge, guarded by Heimdallr.

In Norse mythology, "Miðgarðr" became applied to the wall around the world that the gods constructed from the eyebrows of the giant Ymir as a defense against the Jotuns who lived in Jotunheim, east of "Manheimr", the "home of men", a word used to refer to the entire world. The gods slew the giant Ymir, the first created being, and put his body into the central void of the universe, creating the world out of his body: his flesh constituting the land, his blood the oceans, his bones the mountains, his teeth the cliffs, his hairs the trees, and his brains the clouds. Aurgelmir's skull was held by four dwarfs, Nordri, Sudri, Austri, and Vestri, who represent the four points on the compass and became the dome of heaven. The sun, moon, and stars were said to be scattered sparks in the skull.
According to the Eddas, Midgard will be destroyed at Ragnarök, the battle at the end of the world. Jörmungandr will arise from the ocean, poisoning the land and sea with his venom and causing the sea to rear up and lash against the land. The final battle will take place on the plane of Vígríðr, following which Midgard and almost all life on it will be destroyed, with the earth sinking into the sea, only to rise again, fertile and green when the cycle repeats and the creation begins again.

Although most surviving instances of the word Midgard refer to spiritual matters, it was also used in more mundane situations, as in the Viking Age runestone poem from the inscription Sö 56 from Fyrby:

The Danish and Swedish form or , the Norwegian or , as well as the Icelandic and Faroese form , all derive from the Old Norse term.

The name "middangeard" occurs six times in the Old English epic poem "Beowulf", and is the same word as Midgard in Old Norse. The term is equivalent in meaning to the Greek term Oikoumene, as referring to the known and inhabited world.

The concept of Midgard occurs many times in Middle English. The association with "earth" (OE "eorðe") in Middle English "middellærd", "middelerde" is by popular etymology; the continuation of "geard" "enclosure" is "yard". An early example of this transformation is from the Ormulum:

The usage of "Middle-earth" as a name for a setting was popularized by Old English scholar J. R. R. Tolkien in his "The Lord of the Rings" and other fantasy works; he was originally inspired by the references to "middangeard" and "Éarendel" in the Old English poem "Crist".

"Mittilagart" is mentioned in the 9th-century Old High German "Muspilli" (v. 54) meaning "the world" as opposed to the sea and the heavens:


</doc>
<doc id="19732" url="https://en.wikipedia.org/wiki?curid=19732" title="Mage: The Ascension">
Mage: The Ascension

Mage: The Ascension is a role-playing game based in the World of Darkness, and was published by White Wolf Game Studio. The characters portrayed in the game are referred to as mages, and are capable of feats of magic. The idea of magic in "Mage" is broadly inclusive of diverse ideas about mystical practices as well as other belief systems, such as science and religion, so that most mages do not resemble typical fantasy wizards.

In 2005, White Wolf released a new version of the game, marketed as "", for the new World of Darkness series. The new game features some of the same game mechanics but uses a substantially different premise and setting.

Following the release of "", White Wolf put out a new roleplaying game every year, each set in "Vampire"'s World of Darkness and using its Storyteller rule system. The next four games were: "" (1992), "Mage: The Ascension" (1993), "" (1994) and "" (1995). "Mage" was the first World of Darkness game that Mark Rein•Hagen
was not explicitly involved with, although it featured the Order of Hermes from his "Ars Magica" as just a single tradition among many.

The basic premise of "Mage: The Ascension" is that everyone has the capacity, at some level, to shape reality. This capacity, personified as a mysterious alter-ego called the Avatar, is dormant in most people, who are known as sleepers, whereas Magi (and/or their Avatars) are said to be Awakened. Because they're awakened, Magi can consciously effect changes to reality via willpower, beliefs, and specific magical techniques.

The beliefs and techniques of Magi vary enormously, and the ability to alter reality can only exist in the context of a coherent system of belief and technique, called a paradigm. A paradigm organizes a Mage's understanding of reality, how the universe works, and what things mean. It also provides the Mage with an understanding of how to change reality, through specific magical techniques. For example, an alchemical paradigm might describe the act of wood burning as the wood "releasing its essence of elemental Fire," while modern science would describe fire as "combustion resulting from a complex chemical reaction." Paradigms tend to be idiosyncratic to the individual Mage, but the vast majority belong to broad categories of paradigm, e.g., Shamanism, Medieval Sorcery, religious miracle working, and superscience.

In the Mage setting, everyday reality is governed by commonsense rules derived from the collective beliefs of sleepers. This is called the consensus. Most Magi's paradigms differ substantially from the consensus. When a mage performs an act of magic that does not seriously violate this commonsense version of reality, in game terms this is called coincidental magic. Magic that deviates wildly from consensus is called vulgar or dynamic magic. When it is performed ineptly, or is vulgar, and especially if it is vulgar and witnessed by sleepers, magic can cause Paradox, a phenomenon in which reality tries to resolve contradictions between the consensus and the Mage's efforts. Paradox is difficult to predict and almost always bad for the mage. The most common consequences of paradox include physical damage directly to the Mage's body, and paradox flaws, magic-like effects which can for example turn the mage's hair green, make him mute, make him incapable of leaving a certain location, and so on. In more extreme cases paradox can cause Quiet (madness that may leak into reality), Paradox Spirits (nebulous, often powerful beings which purposefully set about resolving the contradiction, usually by directly punishing the mage), or even the removal of the Mage to a paradox realm, a pocket dimension from which it may be difficult to escape.

In Mage, there is an underlying framework to reality called the Tapestry. The Tapestry is naturally divided into various sections, including the physical realm and various levels of the spirit world, or Umbra. At the most basic level, the Tapestry is composed of Quintessence, the essence of magic and what is real. Quintessence can have distinctive characteristics, called resonance, which are broken down into three categories: dynamic, static, and entropic.

In order to understand the metaphysics of the Mage setting, it is important to remember that many of the terms used to describe magic and Magi (e.g. Avatar, Quintessence, the Umbra, and Paradox, Resonance, etc.) as well as the appearance, meaning, and understanding of a character's "Spheres," the areas of magic in which their character is proficient, vary depending on the Paradigm of the Mage in question, even though they are often, in the texts of the game, described from particular paradigmatic points-of-view. In-character, only a Mage's Paradigm can explain what each of these things are, what they mean, and why it's the way it is.

In the game, Mages have always existed, though there are legends of the Pure Ones who were shards of the original, divine One. Early mages cultivated their magical beliefs alone or in small groups, generally conforming to and influencing the belief systems of their societies. Obscure myths suggest that the precursors of the modern organizations of mages originally gathered in ancient Egypt. This period of historical uncertainty also saw the rise of the Nephandi in the Near East. This set the stage for what the game's history calls the Mythic Ages.

Until the late Middle Ages, mages' fortunes waxed and waned along with their native societies. Eventually, though, mages belonging to the Order of Hermes and the Messianic Voices attained great influence over European society. However, absorbed by their pursuit of occult power and esoteric knowledge, they often neglected and even abused humanity. Frequently, they were at odds with mainstream religions, envied by noble authorities and cursed by common folk.

Mages who believed in proto-scientific theories banded together under the banner of the , declaring their aim was to create a safe world with Man as its ruler. They won the support of Sleepers by developing the useful arts of manufacturing, economics, wayfaring, and medicine. They also championed many of the values that we now associate with the Renaissance. Masses of Sleepers embraced the gifts of early Technology and the Science that accompanied them. As the masses' beliefs shifted, the Consensus changed and wizards began to lose their position as their power and influence waned.

This was intentional. The Order of Reason perceived a safe world as one devoid of heretical beliefs, ungodly practices and supernatural creatures preying upon humanity. As the defenders of the common folk, they intended to replace the dominant magical groups with a society of philosopher-scientists as shepherds, protecting and guiding humanity. In response, non-scientific mages banded together to form the where mages of all the major magical paths gathered. They fought on battlefields and in universities trying to undermine as many discoveries as they could, but to no avail - technology made the march of Science unstoppable. The Traditions' power bases were crippled, their believers mainly converted, their beliefs ridiculed all around the world. Their final counteroffensives against the Order of Reason were foiled by internal dissent and treachery in their midst.

However, from the turn of the 17th century on, the goals of the Order of Reason began to change. As their scientific paradigm unfolded, they decided that the mystical beliefs of the common people were not only backward, but dangerous, and that they should be replaced by cold, measurable and predictable physical laws and respect for human genius. They replaced long-held theologies, pantheons, and mystical traditions with ideas like rational thought and the scientific method. As more and more sleepers began to use the Order's discoveries in their everyday lives, Reason and rationality came to govern their beliefs, and the old ways came to be regarded as misguided superstition. However, the Order of Reason became less and less focused on improving the daily lives of sleepers and more concerned with eliminating any resistance to their choke-hold on the minds of humanity. Ever since a reorganization performed under Queen Victoria in the late 1800s, they call themselves .

The Technocracy espouses an authoritarian rule over Sleepers' beliefs, while suppressing the Council of Nine's attempts to reintroduce magic. The Traditions replenished their numbers (which had been diminished by the withdrawal of two Traditions, the secretive Ahl-i-Batin, and the Solificati, alchemists plagued by scandal) with former Technocrats from the Sons of Ether and Virtual Adepts factions, vying for the beliefs of sleepers and with the Technocracy, and perpetually wary of the Nephandi (who consciously embrace evil and service to a demonic or alien master) and the Marauders (who resist Paradox with a magical form of madness). While the Technocracy's propaganda campaigns were effective in turning the Consensus against mystic and heterodox science, the Traditions maintained various resources, including magical nodes, hidden schools and fortresses called Chantries, and various realms outside of the Consensus in the Umbra.

Finally, from 1997–2000, a series of metaplot events destroyed the Council of Nine's Umbral steadings, killing many of their most powerful members. This also cut the Technocracy off from their leadership. Both sides called a truce in their struggle to assess their new situation, especially since these events implied that Armageddon was soon at hand. Chief among these signs was creation of a barrier between the physical world and spirit world. This barrier was called the Avatar Storm because it affected the Avatar of the Mage. This Avatar Storm was the result of a battle in India on the so-called "Week of Nightmares."

These changes were introduced in supplements for the second edition of the game and became core material in the third edition.

Aside from common changes introduced by the World of Darkness metaplot, mages dealt with renewed conflict when the hidden Rogue Council and the Technocracy's Panopticon encouraged the Traditions and Technocracy to struggle once again. The Rogue Council only made itself known through coded missives, while Panopticon was apparently created by the leaders of the Technocracy to counter it.

This struggle eventually led to the point on the timeline occupied by the book called "". While the entire metaplot has always been meant to be altered as each play group sees fit, "Ascension" provided multiple possible endings, with none of them being definitive (though one was meant to resolve the metaplot). Thus, there is no definitive canonical ending. Since the game is meant to be adapted to a group's tastes, the importance of this and the preceding storyline is largely a matter of personal preference.

The metaplot of the game involves a four-way struggle between the technological and authoritarian Technocracy, the insane Marauders, the cosmically evil Nephandi and the nine mystical Traditions (that tread the middle path), to which the player characters are assumed to belong. (This struggle has in every edition of the game been characterized both as primarily a covert, violent war directly between factions, and primarily as an effort to sway the imaginations and beliefs of sleepers.)

The Traditions (formally called the Nine Mystic Traditions) are a fictional alliance of secret societies in the "Mage: the Ascension" role-playing game. The Traditions exist to unify users of magic under a common banner to protect reality (particularly those parts of reality that are magical) against the growing disbelief of the modern world, the spreading dominance of the , and the predations of unstable mages such as Marauders and Nephandi. Each of the Traditions are largely independent organizations unified by a broadly accepted paradigm for practicing magic. The Traditions themselves vary substantially from one another. Some have almost no structure or rules, while others have rigid rules of protocol, etiquette, and rank. Though unified in their desire to keep magic alive, the magic practiced by different Traditions are often wildly different and entirely incompatible with one another. Understanding Traditions as a whole requires understanding each Tradition separately, and then assembling them into a somewhat cohesive whole.

The nine traditions are: the Akashic Brotherhood, Celestial Chorus, Cult of Ecstasy, Dreamspeakers, Euthanatos, Order of Hermes, Sons of Ether, Verbena and Virtual Adepts. 

The Technocracy is likewise divided into groups; unlike the Traditions, however, they share a single paradigm, and instead divide themselves based upon methodologies and areas of expertise.


The Marauders are a group of mages that embody Dynamism. Marauders are chaos mages. They are completely insane. To other mages, they appear immune to paradox effects, often using vulgar magic to accomplish their insane tasks. Marauders represent the other narrative extreme, the repellent and frightening corruption of unrestrained power, of dynamism unchecked. Marauders are insane mages whose Avatars have been warped by their mental instability, and who exist in a state of permanent Quiet. While the nature of a Marauder's power may make them seem invincible, they are still severely hampered by their madness. They cannot become Archmages, as they lack sufficient insight and are incapable of appreciating truths which do not suit their madness. In the second edition of "Mage: The Ascension", Marauders were much more cogent and likely to operate in groups, with the Umbral Underground using the Umbra to infiltrate any location and wreak havoc with the aid of bygones. They were also associated heavily with other perceived agents of Dynamism, particularly the s (who equate Dynamism with the Wyld) and sometimes Changelings. For example, the Marauders chapter in "The Book of Madness" is narrated by a Corax (were-raven) named Johnny Gore, who relates his experiences running with the Butcher Street Regulars. In the revised edition, Marauders were made darker and less coherent, in keeping with the more serious treatment of madness used for Malkavians in "Vampire: The Masquerade Revised Edition". The Avatar Storm was a very convenient explanation for the Underground's loss of power and influence, though they also became more vulnerable to Paradox. In this edition, the Regulars are a cell of the Underground, and like the other cells have highly compatible Quiets.

With the Technocracy representing Stasis and the Marauders acting on behalf of Dynamism, the third part of this trifecta is Entropy, as borne by the Nephandi. While other mages may be callous or cruel, the Nephandi are morally inverted and spiritually mutilated. While a Traditionalist or Technocrat may simply fall prey to human failings or excessive zeal in their ethos, while a Marauder may well commit some true atrocities in the depth of her incurable madness; a Nephandus retains a clear moral compass, and deliberately pursues actions to worsen the world and bring about its final end. To this end, the Technocracy and Traditions have been known to set aside the ongoing war for reality to temporarily join forces to oppose the Nephandi, and even the Marauders are known to attack the Nephandi on sight. Some of their members, called "barabbi", hail from the Technocracy and Traditions, but all Nephandi have experienced the Rebirth, wherein they embrace the antithesis of everything they know to be right, and are physically and spiritually torn apart and reassembled. This metamorphosis has a sort of terrible permanence to it: while each Mage's avatar will be reborn again and again, theirs is permanently twisted as a result of their rebirth: known as Widderslainte, these mages awaken as Nephandi. While some of the background stories detail a particular mage and her teacher trying—and succeeding—at keeping her from falling again, this is very rare.

Other mystical traditions that are not part of the nine exist, and are known as Crafts. Some examples of these are the mages of Ahl-i-Batin (also known as "The Subtle Ones") who are masters of the Correspondence Sphere and former holders of the seat now held by the Virtual Adepts, as well as the djinn binding magicians known as The Taftani and the eclectic nonconformist group of willworkers known as Hollow Ones, however they are far from the only ones.

The core rules of the game are similar to those in other World of Darkness games; see Storyteller System for an explanation.

Like other storytelling games Mage emphasizes personal creativity and that ultimately the game's powers and traits should be used to tell a satisfying story. One of Mage's highlights is its system for describing magic, based on spheres, a relatively open-ended 'toolkit' approach to using game mechanics to define the bounds of a given character's magical ability. Different Mages will have differing aptitudes for spheres, and player-characters' magical expertise is described by allocation of points in the spheres.

There are nine known spheres:

Deals with spatial relations, giving the Mage power over space and distances. Correspondence magic allows powers such as teleportation, seeing into distant areas, and at higher levels the Mage may also co-locate herself or even stack different spaces within each other. Correspondence can be combined with almost any other sphere to create effects that span distances.

This sphere gives the Mage power over order, chaos, fate and fortune. A mage can sense where elements of chance influence the world and manipulate them to some degree. At simple levels machines can be made to fail, plans to go off without a hitch, and games of chance heavily influenced. Advanced mages can craft self-propagating memes or curse entire family lines with blights. The only requirement of the Entropy sphere is that all interventions work within the general flow of natural entropy.

Forces concerns energies and natural forces and their negative opposites (i.e. light and shadow can both be manipulated independently with this Sphere). Essentially, anything in the material world that can be seen or felt but is not material can be controlled: electricity, gravity, magnetism, friction, heat, motion, fire, etc. At low levels the mage can control forces on a small scale, changing their direction, converting one energy into another. At high levels, storms and explosions can be conjured. Obviously, this Sphere tends to do the most damage and is the most flashy and vulgar. Along with Life and Matter, Forces is one of the three 'Pattern Spheres' which together are able to mold all aspects of the physical world.

Life deals with understanding and influencing biological systems. Generally speaking, any material object with mostly living cells falls under the influence of this sphere. Simply, this allows the mage to heal herself or transform simple life-forms at lower levels, working up to healing others and controlling more complex life at higher levels. Usually, seeking to improve a complex life-form beyond natural limits causes the condition of pattern bleeding: the affected life form begins to wither and die over time. Along with Matter and Forces, Life is one of the three Pattern Spheres.

Dealing with control over one's own mind, the reading and influencing of other minds, and a variety of subtler applications such as Astral Projection and psychometry. At high levels, Mages can create new complete minds or completely rework existing ones.

Matter deals with all inanimate material. Thus, being alive protects a thing from direct manipulation by the Matter sphere. Stone, dead wood, water, gold, and the corpses of once living things are only the beginning. With this Sphere, matter can be reshaped mentally, transmuted into another substance, or given altered properties. Along with Life and Forces, Matter is one of the three Pattern Spheres.

This sphere deals directly with Quintessence, the raw material of the tapestry, which is the metaphysical structure of reality. This sphere allows Quintessence to be channeled and/or funneled in any way at higher levels, and it is necessary if the mage ever wants to conjure something out of nothing(as opposed to transforming one pattern into another). Uses of Prime include general magic senses, counter-magic, and making magical effects permanent.

This sphere is an eclectic mixture of abilities relating to dealings with the spirit world or Umbra. It includes stepping into the Near Umbra right up to traveling through outer space, contacting and controlling spirits, communing with your own or others' avatars, returning a Mage into a sleeper, returning ghosts to life, creating magical fetish items, and so forth. Unlike other Spheres, the difficulty of Spirit magic is often a factor of the Gauntlet, making these spells harder for the most part. The Sphere is referred to as Dimensional Science by the Technocratic Union.

This sphere deals with dilating, slowing, stopping or traveling through time. Due to game mechanics, it is simpler to travel forward in time than backwards. Time can be used to install delays into spells, view the past or future, and even pull people and objects out of linear progression. Time magic offers one means to speed up a character to get multiple actions in a combat round, a much coveted power in turn-based role-playing.

One of the plot hooks that the second edition books put forth were persistent rumors of a "tenth sphere". Though there were hints, it was deliberately left vague. The final book in the line, "Ascension" implies that the tenth sphere is the sphere of Ascension (in as much as spheres are practically relevant at that point in the story). As the book presents alternative resolutions for the Mage line, Chapter Two also presents an alternative interpretation that the tenth sphere is "Judgement" or "Telos" and that Anthelios (the red star in the World of Darkness metaplot) is its planet (each sphere has an associated planet and Umbral realm).

The various sphere sigils are, in whole or in part, symbols taken from alchemical texts.


The third revision of the rules, "Mage: The Ascension Revised", made significant changes to the rules and setting, mainly to update Mage with respect to its own ongoing storyline, particularly in regards to events that occurred during the run of the game's second edition. (Like other World of Darkness games, Mage uses a continuing storyline across all of its books).

"Mage: The Ascension" was ranked 16th in the 1996 reader poll of "Arcane" magazine to determine the 50 most popular roleplaying games of all time. The UK magazine's editor Paul Pettengale commented: "Mage is perfect for those of a philosophical bent. It's a hard game to get right, requiring a great deal of thought from players and referees alike, but its underlying theme - the nature of reality - makes it one of the most interesting and mature roleplaying games available."



</doc>
<doc id="19734" url="https://en.wikipedia.org/wiki?curid=19734" title="Malcolm Fraser">
Malcolm Fraser

John Malcolm Fraser (; 21 May 1930 – 20 March 2015) was an Australian politician who served as the 22nd Prime Minister of Australia, in office from 1975 to 1983 as leader of the Liberal Party.

Fraser was raised on his father's sheep stations, and after studying at Magdalen College, Oxford, returned to Australia to take over the family property in the Western District of Victoria. After an initial defeat in 1954, he was elected to the House of Representatives at the 1955 federal election, standing in the Division of Wannon. He was 25 at the time, making him one of the youngest people ever elected to parliament. When Harold Holt became prime minister in 1966, Fraser was made Minister for the Army. He was later Minister for Defence (1968–1969) and Minister for Education and Science (1969–1971) under John Gorton. In 1971, Fraser resigned from cabinet and denounced Gorton as "unfit to hold the great office of prime minister"; this precipitated the replacement of Gorton with William McMahon. He subsequently returned to his old education and science portfolio.

After the Coalition was defeated at the 1972 election, Fraser unsuccessfully stood for the Liberal leadership, losing to Billy Snedden. When the party lost the 1974 election, he began to move against Snedden, eventually mounting a successful challenge in March 1975. As Leader of the Opposition, Fraser used the Coalition's control of the Senate to block supply to the Whitlam Government, precipitating a constitutional crisis. This culminated with Gough Whitlam being dismissed as prime minister by Governor-General John Kerr, a unique occurrence in Australian history. The correctness of Fraser's actions in the crisis and the exact nature of his involvement in Kerr's decision have since been a topic of debate.

After Whitlam's dismissal, Fraser was sworn in as prime minister on an initial caretaker basis. The Coalition won a landslide victory at the 1975 election, and was re-elected in 1977 and 1980. Fraser took a keen interest in foreign affairs as prime minister, and was more active in the international sphere than many of his predecessors. He was a strong supporter of multiculturalism, and during his term in office Australia admitted significant numbers of non-white immigrants (including Vietnamese boat people) for the first time. His government also established the Special Broadcasting Service (SBS). Particularly in his final years in office, Fraser came into conflict with the economic rationalist faction of his party. His government made few major changes to economic policy.

Fraser and the Coalition lost power at the 1983 election, and he left politics a short time later. In retirement, he held advisory positions with the UN and Commonwealth of Nations, and was president of the aid agency CARE from 1990 to 1995. He resigned his membership of the Liberal Party in 2009, having been a critic of its policy direction for a number of years. Evaluations of Fraser's prime ministership have been mixed. He is generally credited with restoring stability to the country after a series of short-term leaders, but some have seen his government as a lost opportunity for economic reform. Only three Australian prime ministers have served longer terms in office – Menzies, Bob Hawke, and John Howard.

John Malcolm Fraser was born in Toorak, Melbourne, Victoria, on 21 May 1930. He was the second of two children born to Una Arnold (née Woolf) and John Neville Fraser; his older sister Lorraine had been born in 1928. Both he and his father were known exclusively by their middle names. His paternal grandfather, Sir Simon Fraser, was born in Nova Scotia, Canada, and arrived in Australia in 1853. He made his fortune as a railway contractor, and later acquired significant pastoral holdings, becoming a member of the "squattocracy". Fraser's maternal grandfather, Louis Woolf, was born in Dunedin, New Zealand, and arrived in Australia as a child. He was of Jewish origin, a fact which his grandson did not learn until he was an adult. A chartered accountant by trade, he married Amy Booth, who was related to the wealthy Hordern family of Sydney and was a first cousin of Sir Samuel Hordern.

Fraser had a political background on both sides of his family. His father served on the Wakool Shire Council, including as president for two years, and was an admirer of Billy Hughes and a friend of Richard Casey. Simon Fraser served in both houses of the colonial Parliament of Victoria, and represented Victoria at several of the constitutional conventions of the 1890s. He eventually become one of the inaugural members of the new federal Senate, serving from 1901 to 1913 as a member of the early conservative parties. Louis Woolf also ran for the Senate in 1901, standing as a Free Trader in Western Australia. He polled only 400 votes across the whole state, and was never again a candidate for public office.

Fraser spent most of his early life at "Balpool-Nyang", a sheep station of on the Edward River near Moulamein, New South Wales. His father had a law degree from Magdalen College, Oxford, but never practised law and preferred the life of a grazier. Fraser contracted a severe case of pneumonia when he was eight years old, which nearly proved fatal. He was home-schooled until the age of ten, when he was sent to board at Tudor House School in the Southern Highlands. He attended Tudor House from 1940 to 1943, and then completed his secondary education at Melbourne Grammar School from 1944 to 1948 where he was a member of Rusden House. While at Melbourne Grammar, he lived in a flat that his parents owned on Collins Street. In 1943, Fraser's father sold "Balpool-Nyang" – which had been prone to drought – and bought "Nareen", in the Western District of Victoria. He was devastated by the sale of his childhood home, and regarded the day he found out about it as the worst of his life.

In 1949, Fraser moved to England to study at Magdalen College, Oxford, which his father had also attended. He read Philosophy, Politics and Economics (PPE), graduating in 1952 with third-class honours. Although Fraser did not excel academically, he regarded his time at Oxford as his intellectual awakening, where he learned "how to think". His college tutor was Harry Weldon, who was a strong influence. His circle of friends at Oxford included Raymond Bonham Carter, Nicolas Browne-Wilkinson, and John Turner. In his second year, he had a relationship with Anne Reid, who as Anne Fairbairn later became a prominent poet. After graduating, Fraser considered taking a law degree or joining the British Army, but eventually decided to return to Australia and take over the running of the family property.

Fraser returned to Australia in mid-1952. He began attending meetings of the Young Liberals in Hamilton, and became acquainted with many of the local party officials. In November 1953, aged 23, Fraser unexpectedly won Liberal preselection for the Division of Wannon. The previous Liberal member, Dan Mackinnon, had been defeated in 1951 and moved to a different electorate. He was expected to be succeeded by Magnus Cormack, who had recently lost his place in the Senate. Fraser had put his name forward as a way of building a profile for future candidacies, but mounted a strong campaign and in the end won a narrow victory. In January 1954, he made the first of a series of weekly radio broadcasts on 3HA Hamilton and 3YB Warrnambool, titled "One Australia". His program – consisting of a pre-recorded 15-minute monologue – covered a wide range of topics, and was often reprinted in newspapers. It continued more or less uninterrupted until his retirement from politics in 1983, and helped him build a substantial personal following in his electorate.

At the 1954 election, Fraser lost to the sitting Labor member Don McLeod by just 17 votes (out of over 37,000 cast). However, he reprised his candidacy at the early 1955 election and won a landslide victory, assisted by a favourable redistribution, McLeod's retirement, and the 1955 Labor Party split. Fraser took his seat in parliament at the age of 25 – the youngest sitting MP by four years, and the first who had been too young to serve in World War II. He was re-elected at the 1958 election despite being restricted in his campaigning by a bout of hepatitis. Fraser was soon being touted as a future member of cabinet, but despite good relations with Robert Menzies never served in any of his ministries. This was probably due to a combination of his youth and the fact that the ministry already contained a disproportionately high number of Victorians.

Fraser spoke on a wide range of topics during his early years in parliament, but took a particular interest in foreign affairs. In 1964, he and Gough Whitlam were both awarded Leader Grants by the United States Department of State, allowing them to spend two months in Washington, D.C., getting to know American political and military leaders. The Vietnam War was the main topic of conversation, and on his return trip to Australia he spent two days in Saigon. Early in 1965, he also made a private seven-day visit to Jakarta, and with assistance from Ambassador Mick Shann secured meetings with various high-ranking officials.

After more than a decade on the backbench, Fraser was appointed to the Cabinet by the prime minister, Harold Holt, in 1966. As Minister for the Army he presided over the controversial Vietnam War conscription program.

Under the new prime minister, John Gorton, he became Minister for Education and Science and in 1969 was promoted to Minister for Defence, a particularly challenging post at the time, given the height of Australia's involvement in the Vietnam War and the protests against it.

In March 1971 Fraser abruptly resigned from the Cabinet in protest at what he called Gorton's "interference in (his) ministerial responsibilities".

This precipitated a series of events which eventually led to the downfall of Gorton and his replacement as prime minister by William McMahon. Gorton never forgave Fraser for the role he played in his downfall; to the day Gorton died in 2002, he could not bear to be in the same room with Fraser.

McMahon immediately reappointed Fraser to the Cabinet, returning him to his old position of Minister for Education and Science. When the Liberals were defeated at the 1972 election by the Labor Party under Gough Whitlam, McMahon resigned and Fraser became Shadow Minister for Labour under Billy Snedden.

After Coalition lost the 1972 election, Fraser was one of five candidates for the Liberal leadership that had been vacated by McMahon. He outpolled John Gorton and James Killen, but was eliminated on the third ballot. Billy Snedden eventually defeated Nigel Bowen by a single vote on the fifth ballot. In the new shadow cabinet – which featured only Liberals – Fraser was given responsibility for primary industry. This was widely seen as a snub, as the new portfolio kept him mostly out of the public eye and was likely to be given to a member of the Country Party when the Coalition returned to government. In an August 1973 reshuffle, Snedden instead made him the Liberals' spokesman for industrial relations. He had hoped to be given responsibility for foreign affairs (in place of the retiring Nigel Bowen), but that role was given to Andrew Peacock. Fraser oversaw the development of the party's new industrial relations policy, which was released in April 1974. It was seen as more flexible and even-handed than the policy that the Coalition had pursued in government, and was received well by the media. According to Fraser's biographer Philip Ayres, by "putting a new policy in place, he managed to modify his public image and emerge as an excellent communicator across a traditionally hostile divide".

After the Liberals lost the 1974 election, Fraser unsuccessfully challenged Snedden for the leadership in November. Despite surviving the challenge, Snedden's position in opinion polls continued to decline and he was unable to get the better of Whitlam in the Parliament. Fraser again challenged Snedden on 21 March 1975, this time succeeding and becoming Leader of the Liberal Party and Leader of the Opposition.

Following a series of ministerial scandals engulfing the Whitlam Government later that year, Fraser began to instruct Coalition senators to delay the government's budget bills, with the objective of forcing an early election that he believed he would win. After several months of political deadlock, during which time the government secretly explored methods of obtaining supply funding outside the Parliament, the Governor-General, Sir John Kerr, controversially dismissed Whitlam as prime minister on 11 November 1975.

Fraser was immediately sworn in as caretaker prime minister on the condition that he end the political deadlock and call an immediate election.

On 19 November 1975, shortly after the election had been called, a letter bomb was sent to Fraser, but it was intercepted and defused before it reached him. Similar devices were sent to the governor-general and the Premier of Queensland, Joh Bjelke-Petersen.

At the 1975 election, Fraser led the Liberal-Country Party Coalition to a landslide victory. The Coalition won 30 seats from Labor to gain a 55-seat majority, which remains to date the largest in Australian history. Fraser subsequently led the Coalition to a second victory in 1977, with only a very small decrease in their vote. The Liberals actually won a majority in their own right in both of these elections, something that even Holt and Robert Menzies had not been able to achieve. Although Fraser thus had no need for the support of the (National) Country Party to govern, he retained the formal Coalition between the two parties.

Fraser quickly dismantled some of the programs of the Whitlam Government, such as the Ministry of the Media, and made major changes to the universal health insurance system Medibank. He initially maintained Whitlam's levels of tax and spending, but real per-person tax and spending soon began to increase. He did manage to rein in inflation, which had soared under Whitlam. His so-called "Razor Gang" implemented stringent budget cuts across many areas of the Commonwealth Public Sector, including the Australian Broadcasting Corporation (ABC).

Fraser practised Keynesian economics during his time as Prime Minister, in part demonstrated by running budget deficits throughout his term as Prime Minister. He was the Liberal Party's last Keynesian Prime Minister. Though he had long been identified with the Liberal Party's right wing, he did not carry out the radically conservative program that his political enemies had predicted, and that some of his followers wanted. Fraser's relatively moderate policies particularly disappointed the Treasurer, John Howard, as well as other ministers who were strong adherents of emerging free market neo-liberal economics, and therefore detractors of Keynesian economics. The government's economic record was marred by rising double-digit unemployment and double-digit inflation, creating "stagflation", caused in part by the ongoing effects of the 1973 oil crisis.

Fraser was particularly active in foreign policy as prime minister. He supported the Commonwealth in campaigning to abolish apartheid in South Africa and refused permission for the aircraft carrying the Springbok rugby team to refuel on Australian territory en route to their controversial 1981 tour of New Zealand. However, an earlier tour by the South African ski boat angling team was allowed to pass through Australia on the way to New Zealand in 1977 and the transit records were suppressed by Cabinet order.

Fraser also strongly opposed white minority rule in Rhodesia. During the 1979 Commonwealth Conference, Fraser, together with his Nigerian counterpart, convinced the newly elected British prime minister, Margaret Thatcher, to withhold recognition of the internal settlement Zimbabwe Rhodesia government; Thatcher had earlier promised to recognise it. Subsequently, the Lancaster House Agreement was signed and Robert Mugabe was elected leader of an independent Zimbabwe at the inaugural 1980 election. Duncan Campbell, a former deputy secretary of the Department of Foreign Affairs and Trade has stated that Fraser was "the principal architect" in the ending of white minority rule. The President of Tanzania, Julius Nyerere, said that he considered Fraser's role "crucial in many parts" and the President of Zambia, Kenneth Kaunda, called his contribution "vital".

Under Fraser, Australia recognised Indonesia's annexation of East Timor, although many East Timorese refugees were granted asylum in Australia. Fraser was also a strong supporter of the United States and supported the boycott of the 1980 Summer Olympics in Moscow. However, although he persuaded some sporting bodies not to compete, Fraser did not try to prevent the Australian Olympic Committee sending a team to the Moscow Games.

Fraser also surprised his critics over immigration policy; according to 1977 Cabinet documents, the Fraser Government adopted a formal policy for "a humanitarian commitment to admit refugees for resettlement". Fraser's aim was to expand immigration from Asian countries and allow more refugees to enter Australia. He was a firm supporter of multiculturalism and established a government-funded multilingual radio and television network, the Special Broadcasting Service (SBS), building on their first radio stations which had been established under the Whitlam Government.

Despite Fraser's support for SBS, his government imposed stringent budget cuts on the national broadcaster, the ABC, which came under repeated attack from the Coalition for alleged "left-wing bias" and "unfair" coverage on their TV programs, including "This Day Tonight" and "Four Corners", and on the ABC's new youth-oriented radio station Double Jay. One result of the cuts was a plan to establish a national youth radio network, of which Double Jay was the first station. The network was delayed for many years and did not come to fruition until the 1990s. Fraser also legislated to give Indigenous Australians control of their traditional lands in the Northern Territory, but resisted imposing land rights laws on conservative state governments.
At the 1980 election, Fraser saw his majority more than halved, from 48 seats to 21. The Coalition also lost control of the Senate. Despite this, Fraser remained ahead of Labor leader Bill Hayden in opinion polls. However, the economy was hit by the early 1980s recession, and a protracted scandal over tax-avoidance schemes run by some high-profile Liberals also began to hurt the Government.

In April 1981, the Minister for Industrial Relations, Andrew Peacock, resigned from the Cabinet, accusing Fraser of "constant interference in his portfolio". Fraser, however, had accused former prime minister John Gorton of the same thing a decade earlier. Peacock subsequently challenged Fraser for the leadership; although Fraser defeated Peacock, these events left him politically weakened.

By early 1982, the popular former ACTU President, Bob Hawke, who had entered Parliament in 1980, was polling well ahead of both Fraser and the Labor Leader, Bill Hayden, on the question of who voters would rather see as prime minister. Fraser was well aware of the infighting this caused between Hayden and Hawke and had planned to call a snap election in autumn 1982, preventing the Labor Party changing leaders. These plans were derailed when Fraser suffered a severe back injury. Shortly after recovering from his injury, the Liberal Party narrowly won a by-election in the marginal seat of Flinders in January 1983. The failure of the Labor Party to win the seat convinced Fraser that he would be able to win an election against Hayden. As leadership tensions began to grow in the Labor Party throughout January, Fraser subsequently resolved to call a double dissolution election at the earliest opportunity, hoping to capitalise on Labor's disunity and prevent them from replacing Hayden.

On 3 February 1983, Fraser arranged to visit the Governor-General of Australia, Ninian Stephen, intending to call a surprise election. Without any knowledge of Fraser's plans, Hayden resigned as Labor Leader just two hours before Fraser travelled to Government House. This meant that the considerably more popular Bob Hawke was able to replace him at almost exactly the same time that the writs were issued for the election. Although Fraser reacted to the move by saying he looked forward to "knock[ing] two Labor Leaders off in one go" at the forthcoming election, Labor immediately surged in the opinion polls.

At the election on 5 March the Coalition was heavily defeated, suffering a 24-seat swing, the worst defeat of a non-Labor government since Federation. Fraser immediately announced his resignation as Liberal leader and formally resigned as prime minister on 11 March 1983; he retired from Parliament two months later. To date, he is the last non-interim prime minister from a rural seat.

In retirement Fraser served as Chairman of the UN Panel of Eminent Persons on the Role of Transnational Corporations in South Africa 1985, as Co-Chairman of the Commonwealth Group of Eminent Persons on South Africa in 1985–86 (appointed by Prime Minister Hawke), and as Chairman of the UN Secretary-General's Expert Group on African Commodity Issues in 1989–90. He was a distinguished international fellow at the American Enterprise Institute from 1984 to 1986. Fraser helped to establish the foreign aid group CARE organisation in Australia and became the agency's international president in 1991, and worked with a number of other charitable organisations. In 2006, he was appointed Professorial Fellow at the Asia Pacific Centre for Military Law, and in October 2007 he presented his inaugural professorial lecture, "Finding Security in Terrorism's Shadow: The importance of the rule of law".

On 14 October 1986, Fraser, then the Chairman of the Commonwealth Eminent Persons Group, was found in the foyer of the Admiral Benbow Inn, a seedy Memphis hotel, wearing only a pair of underpants and confused as to where his trousers were. The hotel was an establishment popular with prostitutes and drug dealers. Though it was rumoured at the time that the former Prime Minister had been with a prostitute, his wife stated that Fraser had no recollection of the events and that she believes it more likely that he was the victim of a practical joke by his fellow delegates.

In 1993, Fraser made a bid for the Liberal Party presidency but withdrew at the last minute following opposition to his bid, which was raised due to Fraser being critical of then Liberal leader John Hewson for losing the election earlier that year.

After 1996, Fraser was critical of the Howard Coalition government over foreign policy issues, particularly John Howard's alignment with the foreign policy of the Bush administration, which Fraser saw as damaging Australian relationships in Asia. He opposed Howard's policy on asylum-seekers, campaigned in support of an Australian Republic and attacked what he perceived as a lack of integrity in Australian politics, together with former Labor prime minister Gough Whitlam, finding much common ground with his predecessor and his successor Bob Hawke, another republican.

The 2001 election continued his estrangement from the Liberal Party. Many Liberals criticised the Fraser years as "a decade of lost opportunity" on deregulation of the Australian economy and other issues. In early 2004, a Young Liberal convention in Hobart called for Fraser's life membership of the Liberal Party to be ended.

In 2006, Fraser criticised Howard Liberal government policies on areas such as refugees, terrorism and civil liberties, and that "if Australia continues to follow United States policies, it runs the risk of being embroiled in the conflict in Iraq for decades, and a fear of Islam in the Australian community will take years to eradicate". Fraser claimed that the way the Howard government handled the David Hicks, Cornelia Rau and Vivian Solon cases was questionable.

On 20 July 2007, Fraser sent an open letter to members of the large activist group GetUp!, encouraging members to support GetUp's campaign for a change in policy on Iraq including a clearly defined exit strategy. Fraser stated: "One of the things we should say to the Americans, quite simply, is that if the United States is not prepared to involve itself in high-level diplomacy concerning Iraq and other Middle East questions, our forces will be withdrawn before Christmas."

After the defeat of the Howard government at the 2007 federal election, Fraser claimed Howard approached him in a corridor, following a cabinet meeting in May 1977 regarding Vietnamese refugees, and said: "We don't want too many of these people. We're doing this just for show, aren't we?" The claims were made by Fraser in an interview to mark the release of the 1977 cabinet papers. Howard, through a spokesman, denied making the comment.

In October 2007 Fraser gave a speech to Melbourne Law School on terrorism and "the importance of the rule of law," which Liberal MP Sophie Mirabella
condemned in January 2008, claiming errors and "either intellectual sloppiness or deliberate dishonesty", and claimed that he tacitly supported Islamic fundamentalism, that he should have no influence on foreign policy, and claimed his stance on the war on terror had left him open to caricature as a "frothing-at-the-mouth leftie".

Shortly after Tony Abbott won the 2009 Liberal Party leadership spill, Fraser ended his Liberal Party membership, stating the party was "no longer a liberal party but a conservative party".

In December 2011, Fraser was highly critical of the Australian government's decision (also supported by the Liberal Party Opposition) to permit the export of uranium to India, relaxing the Fraser government's policy of banning sales of uranium to countries that are not signatories of the Nuclear Non-Proliferation Treaty.

In 2012, Fraser criticised the basing of US military forces in Australia. In 2014, speaking on the Russian RT television network, he criticised the concept of American exceptionalism and US foreign policy.

In late 2012, Fraser wrote a foreword for the journal "Jurisprudence" where he openly criticised the current state of human rights in Australia and the Western World. "It is a sobering thought that in recent times, freedoms hard won through centuries of struggle, in the United Kingdom and elsewhere have been whittled away. In Australia alone we have laws that allow the secret detention of the innocent. We have had a vast expansion of the power of intelligence agencies. In many cases the onus of proof has been reversed and the justice that once prevailed has been gravely diminished."

In July 2013, Fraser endorsed Australian Greens Senator Sarah Hanson-Young for re-election in a television advertisement, stating she had been a "reasonable and fair-minded voice".

Fraser's books include "Malcolm Fraser: The Political Memoirs" (with Margaret Simons – The Miegunyah Press, 2010) and "Dangerous Allies" (Melbourne University Press, 2014), which warns of "strategic dependence" on the United States.

On 20 March 2015, his office announced that Fraser had died in the early hours of the morning, noting that he had suffered a brief illness. An obituary noted that there had been "greater appreciation of the constructive and positive nature of his post-prime ministerial contribution" as his retirement years progressed.

Fraser was given a state funeral at Scots' Church in Melbourne on 27 March 2015. His ashes are interred within the 'Prime Ministers Garden' of Melbourne General Cemetery.

On 9 December 1956, Fraser married Tamara "Tamie" Beggs, who was almost six years his junior. They had met at a New Year's Eve party, and bonded over similar personal backgrounds and political views. The couple would have four children together: Mark (b. 1958), Angela (b. 1959), Hugh (b. 1963), and Phoebe (b. 1966). Tamie frequently assisted her husband in campaigning, and her outgoingness was seen as complementing his more shy and reserved nature. She advised him on most of the important decisions in his career, and in retirement he observed that "if she had been prime minister in 1983, we would have won".

Fraser attended Anglican schools, although his parents were Presbyterian. In university he was inclined towards atheism, once writing that "the idea that God exists is a nonsense". However, his beliefs became less definite over time and tended towards agnosticism. During his political career, he occasionally self-described as Christian, such as in a 1975 interview with "The Catholic Weekly". Margaret Simons, the co-author of Fraser's memoirs, thought that he was "not religious, and yet thinks religion is a necessary thing". In a 2010 interview with her, he said: "I would probably like to be less logical and, you know, really able to believe there is a god, whether it is Allah, or the Christian god, or some other – but I think I studied too much philosophy ... you can never know".


Orders

Foreign honours

Organisations

Personal

Fellowships

Academic degrees


In 2004, Fraser designated the University of Melbourne the official custodian of his personal papers and library to create the Malcolm Fraser Collection at the university.

Upon his death, Fraser's 1983 nemesis and often bitter opponent Hawke fondly described him as a "very significant figure in the history of Australian politics" who, in his post-Prime Ministerial years, "became an outstanding figure in the advancement of human rights issues in all respects", praised him for being "extraordinarily generous and welcoming to refugees from Indochina" and concluded that Fraser had "moved so far to the left he was almost out of sight".

In June 2018, he was honoured with the proposed naming of the Australian Electoral Division of Fraser in the inner north-western suburbs of Melbourne.






</doc>
<doc id="19735" url="https://en.wikipedia.org/wiki?curid=19735" title="Macquarie University">
Macquarie University

Macquarie University () is a public research university based in Sydney, Australia, in the suburb of Macquarie Park. Founded in 1964 by the New South Wales Government, it was the third university to be established in the metropolitan area of Sydney.

Established as a verdant university, Macquarie has five faculties, as well as the Macquarie University Hospital and the Macquarie Graduate School of Management, which are located on the university's main campus in suburban Sydney.

The university is the first in Australia to fully align its degree system with the Bologna Accord.

The idea of founding a third university in Sydney was flagged in the early 1960s when the New South Wales Government formed a committee of enquiry into higher education to deal with a perceived emergency in university enrollments in New South Wales. During this enquiry, the Senate of the University of Sydney put in a submission which highlighted 'the immediate need to establish a third university in the metropolitan area'. After much debate a future campus location was selected in what was then a semi-rural part of North Ryde, and it was decided that the future university be named after Lachlan Macquarie, an important early governor of the colony of New South Wales.

Macquarie University was formally established in 1964 with the passage of the Macquarie University Act 1964 by the New South Wales parliament.

The initial concept of the campus was to create a new high technology corridor, similar to the area surrounding Stanford University in Palo Alto, California, the goal being to provide for interaction between industry and the new university. The academic core was designed in the Brutalist style and developed by the renowned town planner Walter Abraham who also oversaw the next 20 years of planning and development for the university. A committee appointed to advise the state government on the establishment of the new university at North Ryde nominated Abraham as the architect-planner. The fledgling Macquarie University Council decided that planning for the campus would be done within the university, rather than by consultants, and this led to the establishment of the architect-planners office.

The first Vice-Chancellor of Macquarie University, Alexander George Mitchell, was selected by the University Council which met for the first time on 17 June 1964. Members of the first university council included: Colonel Sir Edward Ford OBE, David Paver Mellor, Rae Else-Mitchell QC and Sir Walter Scott.

The university first opened to students on 6 March 1967 with more students than anticipated. The Australian Universities Commission had allowed for 510 effective full-time students (EFTS) but Macquarie had 956 enrolments and 622 EFTS. Between 1968 and 1969, enrolment at Macquarie increased dramatically with an extra 1200 EFTS, with 100 new academic staff employed. 1969 also saw the establishment of the Macquarie Graduate School of Management (MGSM).

Macquarie grew during the seventies and eighties with rapid expansion in courses offered, student numbers and development of the site. In 1972, the university established the Macquarie Law School, the third law school in Sydney. In their book "Liberality of Opportunity", Bruce Mansfield and Mark Hutchinson describe the founding of Macquarie University as 'an act of faith and a great experiment'. An additional topic considered in this book is the science reform movement of the late 1970s that resulted in the introduction of a named science degree, thus facilitating the subsequent inclusion of other named degrees in addition to the traditional BA. An alternative 
view on this topic is given by theoretical physicist John Ward.

After over a decade of service, the first Vice Chancellor Professor Mitchell was succeeded by Professor Edwin Webb in December 1975. Professor Webb was required to steer the university through one of its most difficult periods as the value of universities were debated and the governments introduced significant funding cuts.

Professor Webb left the university in 1986 and was succeeded by Di Yerbury, the first female Vice-Chancellor in Australia. Professor Yerbury would go on to hold the position of Vice-Chancellor for nearly 20 years.

In 1990 the university absorbed the Institute of Early Childhood Studies of the Sydney College of Advanced Education, under the terms of the Higher Education (Amalgamation) Act 1989.l

Professor Steven Schwartz replaced Di Yerbury at the beginning of 2006. Yerbury's departure was attended with much controversy, including a "bitter dispute" with Schwartz, disputed ownership of university artworks worth $13 million and Yerbury's salary package. In August 2006, Professor Schwartz expressed concern about the actions of Yerbury in a letter to university auditors. Yerbury strongly denied any wrongdoing and claimed the artworks were hers.

During 2007, Macquarie University restructured its student organisation after an audit raised questions about management of hundreds of thousands of dollars in funds by student organisations At the centre of the investigation was Victor Ma, president of the Macquarie University Students' Council, who was previously involved in a high-profile case of student election fixing at the University of Sydney.
The university Council resolved to immediately remove Ma from his position. Vice-Chancellor Schwartz cited an urgent need to reform Macquarie's main student bodies.
However, Ma strongly denied any wrongdoing and labelled the controversy a case of 'character assassination'.
The Federal Court ordered on 23 May 2007 that Macquarie University Union Ltd be wound up.

Following the dissolution of Macquarie University Union Ltd, the outgoing student organisation was replaced with a new wholly owned subsidiary company of the university, known as U@MQ Ltd. The new student organisation originally lacked a true student representative union; however, following a complete review and authorisation from the university Council, a new student union known as Macquarie University Students Association (MUSRA) was established in 2009.

Within the first few hundred days of Schwartz's instatement as Vice-Chancellor, the 'Macquarie@50' strategic plan was launched, which positioned the university to enhance research, teaching, infrastructure and academic rankings by the university's 50th anniversary in 2014. Included in the university's plans for the future was the establishment of a sustainability office in order to more effectively manage environmental and social development at Macquarie. As part of this campaign, in 2009 Macquarie became the first Fair Trade accredited university in Australia. The beginning of 2009 also saw the introduction of a new logo for the university which retained the Sirius Star, present on both the old logo and the university crest, but now 'embedded in a stylised lotus flower'. In accordance with the university by-law, the crest continues to be used for formal purposes and is displayed on university testamurs. The by-law also prescribes the university's motto, taken from Chaucer: 'And gladly teche'.

In 2013, the university became the first in Australia to fully align its degree system with the Bologna Accord.

Macquarie’s arms was assumed through a 1967 amendment of the Macquarie University Act 1964 (Confirmed by Letters Patent of the College of Arms, 16 August 1969). The escutcheon displays the Macquarie Lighthouse tower, the first major public building in the colony, as well as the Sirius star, the name of the flagship of the First Fleet. The university’s founders originally wanted to base the university’s arms on Lachlan Macquarie’s family crest, however they decided to go for a more radical approach that represented Lachlan Macquarie as a builder and administrator. The motto chosen for the university was "And Glady Teche." This is taken from the general Prologue of The Canterbury Tales, Geoffrey Chaucer c.1400 and symbolises the university's commitment to both learning and teaching. The coat of arms and the motto are used in a very limited number of formal communications.

Macquarie has had a number of logos in its history. In 2014, the university launched a new logo as part of its Shared Identity Project. The logo reintroduced the Macquarie Lighthouse, a popular symbol of the University within the University community and maintained the Sirus Star.

Macquarie University's main campus is located about north-west of the Sydney CBD and is set on 126 hectares of rolling lawns and natural bushland. Located within the high-technology corridor of Sydney's north-west and in close proximity to Macquarie Park and its surrounding industries, Macquarie's location has been crucial in its development as a relatively research intensive university.

Prior to the development of the campus, most of the site was cultivated with peach orchards, market gardens and poultry farms. The university's first architect-planner was Walter Abraham, one of the first six administrators appointed to Macquarie University. As the site adapted from its former rural use to a busy collegiate environment, he implemented carefully designed planting programs across the campus. Abraham established a grid design comprising lots of running north-south, with the aim of creating a compact academic core. The measure of was seen as one minute's walk, and grid design reflected the aim of having a maximum walk of 10 minutes between any two parts of the university. The main east-west walkway that runs from the Macquarie University Research Park through to the arts faculty buildings, was named Wally's Walk in recognition of Walter Abraham's contribution to the development of the university.

Apart from its centres of learning, the campus features the Macquarie University Research Park, museums, art galleries, a sculpture park, an observatory, a sport and aquatic centre and also the private Macquarie University Hospital. The campus has its own postcode, 2109.

Macquarie became the first university in Australia to own and operate a private medical facility in 2010 when it opened a $300 million hospital on its campus. The hospital is the first and only private not-for-profit teaching hospital on an Australian university campus. The Macquarie University Hospital is located to the north of the main campus area towards the university sports grounds. It comprises 183 beds, 12 operating theatres, 2 cardiac and vascular angiography suites. The hospital is co-located with the university's Australian School of Advanced Medicine.

The university hosts a number of high technology companies on its campus. Primarily designed to encourage interaction between the university and industry, commercialisation of its campus has also given the institution an additional revenue stream. Tenants are selected based off their potential to collaborate with the universities researches or their ability to provide opportunities for its students and graduates. Cochlear Limited, has its headquarters in close proximity to the Australian Hearing Hub on the southern edge of campus. Other companies that have office space at the campus include Dow Corning, Goodman Fielder, Nortel Networks, OPSM and Siemens.

The Macquarie University Observatory was originally constructed in 1978 as a research facility but, since 1997, has been accessible to the public through its Public Observing Program.

The library houses over 1.8 million items and uses the Library of Congress Classification System. The library features several collections including a Rare Book Collection, a Palaeontology Collection and the Brunner Collection of Egyptological materials. Macquarie University operated two libraries during the transition. The old library in building C7A closed at the end of July 2011 (which has since been repurposed as a student support and study space), and the new library in building C3C became fully operational on 1 August 2011. The new library was the first university library in Australia to possess an Automated Storage and Retrieval System (ASRS). The ASRS consists of an environmentally controlled vault with metal bins storing the items; robotic cranes retrieve an item on request and deliver it to the service desk for collection.

The Macquarie University Incubator is a space to research and develop ideas that can be commercialised.

Macquarie University has two residential colleges on its campus, Dunmore Lang College and Robert Menzies College, both founded in 1972. The colleges offer academic support and a wide range of social and sporting activities in a communal environment.

Separate to the colleges is the Macquarie University Village. The village has over 900 rooms in mostly town house style buildings to the north of the campus. The village encourages its students to interact in its communal spaces and has a number of social events throughout the year.

The museums and collections of Macquarie University are extensive and include nine museums and galleries. Each collection focuses on various historical, scientific or artistic interests. The most visible collection on campus is the sculpture park which is exhibited across the entire campus. At close to 100 sculptures on display, it is the largest park of its kind in the Southern Hemisphere. All museums and galleries are open to the public and offer educational programs for students at primary, secondary and tertiary levels.

Located on the western side of the campus is the Macquarie University Sport and Aquatic Centre. Previously a sports hall facility, the complex was renovated and reopened in 2007 with the addition of the new gym and aquatic centre. It houses a 50-metre FINA-compliant outdoor pool and a 25-metre indoor pool. The complex also contains a gymnasium and squash, badminton, basketball, volleyball and netball courts.

Macquarie also has seven hectares of high quality playing fields for football, cricket and tennis. Situated to the north of the campus, the playing fields are used by the university as well as a number of elite sporting teams such as Sydney FC and the Westfield Matildas.

Macquarie University is served by the Macquarie University railway station, which opened in 2009. Macquarie is the only university in Australia with a railway station on campus. The underground station is served by eight trains per hour for most of the day and is on the Sydney Trains network. In 2018, Macquarie University station will close for six months for conversion to a Sydney Metro station on the Sydney Metro Northwest line.

There is also a major bus interchange within the campus that provides close to 800 bus services daily. The M2 Motorway runs parallel to the northern boundary of the campus and is accessible to traffic from the university.

The university currently comprises 35 departments within five faculties:

Research centres, schools and institutes that are affiliated with the university:

Macquarie University's Australian Hearing Hub is partnered with Cochlear. Cochlear Headquarters are on campus. The Australian Hearing Hub includes the head office of Australian Hearing.

The Australian Research Institute for Environment and Sustainability is a research centre that promotes change for environmental sustainability, is affiliated with the University and is located on its campus.

Access Macquarie Limited was established in 1989 as the commercial arm of the university. It facilitates and supports the commercial needs of industry, business and government organisations seeking to utilise the academic expertise of the broader University community.

The university is governed by a 17-member Council.

The University Council is the governing authority of the university under the "Macquarie University Act 1989". The Council takes primary responsibility for the control and management of the affairs of the University, and is empowered to make by-laws and rules relating to how the University is managed. Members of the Council include the University Vice-Chancellor, Academic and non-academic staff, the Vice President of the Academic Senate and a student representative. The Council is chaired by The Chancellor of the University.

The Academic Senate is the primary academic body of the university. It has certain powers delegated to it by Council, such as the approving of examination results and the completion of requirements for the award of degrees. At the same time, it makes recommendations to the Council concerning all changes to degree rules, and all proposals for new awards. While the Academic Senate is an independent body, it is required to make recommendations to the university Council in relation to matters outside its delegated authority.

Macquarie's current Vice-Chancellor, Bruce Dowton, took over from Schwartz in September 2012. Prior to his appointment Dowton served as a senior medical executive having held a range of positions in university, healthcare and consulting organisations. He also served as a pediatrician at the Massachusetts General Hospital for Children, and as Clinical Professor of Pediatrics at Harvard Medical School. There have been five Vice-Chancellors in the university's history.

The Macquarie University International College offers Foundation Studies (Pre-University) and University-level Diplomas. Upon successful completion of a MUIC Diploma, students enter the appropriate bachelor's degree as a second year student.

The Centre for Macquarie English is the English-language centre that offers a range of specialised, direct entry English programmes that are approved by Macquarie University.

The university positions itself as being research intensive. In 2012, 85% of Macquarie's broad fields of research was rated 'at or above world standard' in the Excellence in Research for Australia 2012 National report. The university is within the top 3 universities in Australia for the number of peer reviewed publications produced per academic staff member.

Researchers at Macquarie University, David Skellern and Neil Weste, and the Commonwealth Scientific and Industrial Research Organisation helped develop Wi-Fi. David Skellern has been a major donor to the University through the Skellern Family Trust. Macquarie physicists Frank Duarte and Jim Piper pioneered the laser designs adopted by researchers worldwide, in various major national programs, for atomic vapor laser isotope separation.

Macquarie University's linguistics department developed the Macquarie Dictionary. The dictionary is regarded as the standard reference on Australian English.

Macquarie University has a research partnership with the University of Hamburg in Germany and Fudan University in China. They offer dual and joint degree programs and engage in joint research.

Macquarie University (MQ) world rankings includes it being number 240 on the QS rankings, number 251+ on Times (THE), number 151+ on ARWU, and number 267= with US News. This contributes to Macquarie being the number 8 ranked Australian university overall in the world ranking systems. Macquarie University rankings within Australia include being placed at number 8 on the ERA scale (2012) and being a 4 1/2 Star AEN rated university. Macquarie also has a student survey satisfaction rating of 77.4% for business, 90.3% for health, 91.4% for arts, and 93.8% for science. Macquarie is ranked in the top 40 universities in the Asia-Pacific region and within Australia's top 12 universities according to the Academic Ranking of World Universities, the U.S. News & World Report Rankings and the QS World University Rankings. Macquarie was the highest ranked university in Australia under the age of 50 and was ranked 18th in the world (prior to its golden jubilee in 2014), according to the QS World University Rankings.

Internationally, Macquarie was ranked 239th in the world (9th in Australia) in the Academic Ranking of World Universities of 2014. Macquarie University was ranked among the top 50 universities in the world for linguistics (43rd), psychology (48th) and earth and marine sciences (48th), and was ranked in the top 5 nationally for philosophy and earth and marine sciences, according to the 2014 QS World University Rankings.

Macquarie ranked 67th in the world for Arts and Humanities (equal 5th in Australia), according to the 2015 Times Higher Education rankings by subject and 54th in the world for arts and humanities, according to the 2017 USNWR rankings by subject. Arts and Humanities is Macquarie's best discipline area in rankings. Macquarie was one of four non-Group of Eight universities ranked in the top 100 universities in the world in particular discipline areas.

The Macquarie Graduate School of Management is one of the oldest business schools in Australia. In 2014, "The Economist" ranked MGSM 5th in the Asia-Pacific, 3rd in Australia, 1st in Sydney/New South Wales and 49th in the world. It was the highest ranked business school in Australia and was ranked 68th in the world in the 2015 "Financial Times" MBA ranking.

Macquarie is the fourth largest university in Sydney (38,753 students in 2013). The university has the largest student exchange programme in Australia.

In 2012, 9,802 students from Asia were enrolled at Macquarie University (Sydney campuses and offshore programs in China, Hong Kong, Korea and Singapore).

Campus Life manages the university's non-academic services: food and retail, sport and recreation, student groups, child care, and entertainment. From late 2017 onward its Campus Hub facility has been closed for reconstruction; a 'pop-up'-style replacement, the Campus Common, has been opened for the duration.

The Global Leadership Program (GLP) is a University-funded extracurricular program that is open to all students, and can be undertaken alongside any degree at Macquarie University. The GLP aims to instil leadership skills, cross cultural understanding and international awareness in its graduates, and rewards student engagement in practical activities such as volunteering, internships, and overseas study. Upon completion of the GLP, students receive a formal notation on their academic transcript. The GLP is the proud recipient of the Institute for International Education's 2017 Heiskell award for Innovation in International Education - Internationalising the Campus. Macquarie University is the first Southern Hemisphere university to receive the award in its 17 year history. 

Macquarie University has its own community radio station on campus, 2SER FM. The station is jointly owned by Macquarie University and University of Technology, Sydney.

Macquarie University students celebrate Conception Day each year since 1969 to – according to legend – commemorate the date of conception of Lachlan Macquarie, as his birthday fell at the wrong time of year for a celebration. Conception Day is traditionally held on the last day of classes before the September mid-semester break.

Alumni include Rhodes and John Monash Scholars and several Fulbright Scholars.

Notable alumni include: Australian politician and former Lord Mayor of Brisbane, Jim Soorley; Australian politician, Tanya Plibersek; Australian basketball player, Lauren Jackson; Australian swimmer, Ian Thorpe; Australian water polo player, Holly Lincoln-Smith; three founding members of the Australian children's musical group The Wiggles (Murray Cook, Anthony Field, Greg Page); New Zealand conservationist, Pete Bethune.

Notable alumni in science include: Australian scientist Barry Brook, American physicist Frank Duarte, and Australian physicist Cathy Foley. Alumni notable in the business world include: Australian hedge fund manager Greg Coffey, Australian businesswoman Catherine Livingstone, founder of Freelancer.com Matt Barrie, businessman Napoleon Perdis and Australian venture capitalist Larry R. Marshall.

Notable faculty members include: Australian writer and four time Miles Franklin Award winner, Thea Astley; Hungarian Australian mathematician, Esther Szekeres; Australian mathematician, Neil Trudinger; Australian environmentalist and activist, Tim Flannery; British physicist and author, Paul Davies; British-Australian physicist, John Clive Ward; Israeli-Australian mathematician, José Enrique Moyal; Australian linguist, Geoffrey Hull; Australian geologist, Fellow of the Australian Academy of Science, John Veevers; Australian climatologist, Ann Henderson-Sellers; Australian sociologist, Raewyn Connell.

Four Macquarie University academics were included in The World's Most Influential Minds 2014 report by Thomson Reuters, which identified the most highly cited researchers of the last 11 years.





</doc>
<doc id="19736" url="https://en.wikipedia.org/wiki?curid=19736" title="Muspelheim">
Muspelheim

In Norse mythology, Muspelheim (), also called Muspell (), is a realm of fire. This realm is one of the Nine Worlds, ruled by Surtr with his consort Sinmara in some accounts. The denizens of Muspelheim were usually referred to as the Eldjötnar (or Eldthursar, "Eldþursar"—"fire giants") in Norse tradition, though they were also identified by other epithets in Eddic poetry, such as the Múspellssynir (or Múspellsmegir—"sons of Muspell"; altn. "Múspellmegir", "Múspellsynir") and the Rjúfendr (from "rjúfa"—"to break, tear asunder", "Destroyers of Doomsday"). Both of these terms sometimes described an entirely separate mythological species that dwelled alongside or in place of the eldjötnar within this fiery realm. 

Muspelheim was the first elemental world to emanate from the primordial void of potentiality Ginnungagap; and the land to the North, Niflheim was the second. The two mixed and gave rise to Ymir.

According to the Ragnarök prophecies in Snorri Sturluson's Gylfaginning, the first part of his Prose Edda, the sons of Muspell will break the Bifröst bridge, signaling the end of times:

The etymology of "Muspelheim" is uncertain, but may come from "Mund-spilli", "world-destroyers", "wreck of the world".



</doc>
<doc id="19737" url="https://en.wikipedia.org/wiki?curid=19737" title="Maxwell's equations">
Maxwell's equations

Maxwell's equations are a set of partial differential equations that, together with the Lorentz force law, form the foundation of classical electromagnetism, classical optics, and electric circuits. The equations provide a mathematical model for electric, optical and radio technologies, such as power generation, electric motors, wireless communication, lenses, radar etc. Maxwell's equations describe how electric and magnetic fields are generated by charges, currents, and changes of the fields. One important consequence of the equations is that they demonstrate how fluctuating electric and magnetic fields propagate at the speed of light. Known as electromagnetic radiation, these waves may occur at various wavelengths to produce a spectrum from radio waves to γ-rays. The equations are named after the physicist and mathematician James Clerk Maxwell, who between 1861 and 1862 published an early form of the equations that included the Lorentz force law. He also first used the equations to propose that light is an electromagnetic phenomenon.

The equations have two major variants. The microscopic Maxwell equations have universal applicability, but are unwieldy for common calculations. They relate the electric and magnetic fields to total charge and total current, including the complicated charges and currents in materials at the atomic scale. The "macroscopic" Maxwell equations define two new auxiliary fields that describe the large-scale behaviour of matter without having to consider atomic scale charges and quantum phenomena like spins. However, their use requires experimentally determining parameters for a phenomenological description of the electromagnetic response of materials.

The term "Maxwell's equations" is often also used for equivalent alternative formulations. Versions of Maxwell's equations based on the electric and magnetic potentials are preferred for explicitly solving the equations as a boundary value problem, analytical mechanics, or for use in quantum mechanics. The spacetime formulations (i.e., on spacetime rather than space and time separately), are commonly used in high energy and gravitational physics because they make the compatibility of the equations with special and general relativity manifest. In fact, Einstein developed special and general relativity to accommodate the absolute speed of light that drops out of the Maxwell equations with the principle that only relative movement has physical consequences.

Since the mid-20th century, it has been understood that Maxwell's equations are not exact, but a classical limit of the fundamental theory of quantum electrodynamics.

Gauss's law describes the relationship between a static electric field and the electric charges that cause it: The static electric field points away from positive charges and towards negative charges, and the net outflow of the electric field through any closed surface is proportional to the charge enclosed by the surface. Picturing the electric field by its field lines, this means the field lines begin at positive electric charges and end at negative electric charges. 'Counting' the number of field lines passing through a closed surface yields the total charge (including bound charge due to polarization of material) enclosed by that surface, divided by dielectricity of free space (the vacuum permittivity).

Gauss's law for magnetism states that there are no "magnetic charges" (also called magnetic monopoles), analogous to electric charges. Instead, the magnetic field due to materials is generated by a configuration called a dipole, and the net outflow of the magnetic field through any closed surface is zero. Magnetic dipoles are best represented as loops of current but resemble positive and negative 'magnetic charges', inseparably bound together, having no net 'magnetic charge'. In terms of field lines, this equation states that magnetic field lines neither begin nor end but make loops or extend to infinity and back. In other words, any magnetic field line that enters a given volume must somewhere exit that volume. Equivalent technical statements are that the sum total magnetic flux through any Gaussian surface is zero, or that the magnetic field is a solenoidal vector field.

The Maxwell–Faraday version of Faraday's law of induction describes how a time varying magnetic field creates ("induces") an electric field. In integral form, it states that the work per unit charge required to move a charge around a closed loop equals the rate of decrease of the magnetic flux through the enclosed surface.

The dynamically induced electric field has closed field lines similar to a magnetic field, unless superposed by a static (charge induced) electric field. This aspect of electromagnetic induction is the operating principle behind many electric generators: for example, a rotating bar magnet creates a changing magnetic field, which in turn generates an electric field in a nearby wire.

Ampère's law with Maxwell's addition states that magnetic fields can be generated in two ways: by electric current (this was the original "Ampère's law") and by changing electric fields (this was "Maxwell's addition", which he called displacement current). In integral form, the magnetic field induced around any closed loop is proportional to the electric current plus displacement current (proportional to the rate of change of electric flux) through the enclosed surface.

Maxwell's addition to Ampère's law is particularly important: it makes the set of equations mathematically consistent for non static fields, without changing the laws of Ampere and Gauss for static fields. However, as a consequence, it predicts that a changing magnetic field induces an electric field and vice versa. Therefore, these equations allow self-sustaining "electromagnetic waves" to travel through empty space (see electromagnetic wave equation).

The speed calculated for electromagnetic waves, which could be predicted from experiments on charges and currents, exactly matches the speed of light; indeed, light "is" one form of electromagnetic radiation (as are X-rays, radio waves, and others). Maxwell understood the connection between electromagnetic waves and light in 1861, thereby unifying the theories of electromagnetism and optics.

In the electric and magnetic field formulation there are four equations that determine the fields for given charge and current distribution. A separate law of nature, the Lorentz force law, describes how, conversely, the electric and magnetic field act on charged particles and currents. A version of this law was included in the original equations by Maxwell but, by convention, is included no longer. The vector calculus formalism below, due to Oliver Heaviside, has become standard. It is manifestly rotation invariant, and therefore mathematically much more transparent than Maxwell's original 20 equations in x,y,z components. The relativistic formulations are even more symmetric and manifestly Lorentz invariant. For the same equations expressed using tensor calculus or differential forms, see alternative formulations.

The differential and integral equations formulations are mathematically equivalent and are both useful. The integral formulation relates fields within a region of space to fields on the boundary and can often be used to simplify and directly calculate fields from symmetric distributions of charges and currents. On the other hand, the differential equations are purely "local" and are a more natural starting point for calculating the fields in more complicated (less symmetric) situations, for example using finite element analysis.

The definitions of charge, electric field, and magnetic field can be altered to simplify theoretical calculation, by absorbing dimensioned factors of and into the units of calculation, by convention. With a corresponding change in convention for the Lorentz force law this yields the same physics, i.e. trajectories of charged particles, or work done by an electric motor. These definitions are often preferred in theoretical and high energy physics where it is natural to take the electric and magnetic field with the same units, to simplify the appearance of the electromagnetic tensor: the Lorentz covariant object unifying electric and magnetic field would then contain components with uniform unit and dimension. Such modified definitions are conventionally used with the Gaussian (CGS) units. Using these definitions and conventions, colloquially "in Gaussian units",
the Maxwell equations become:

Note that the equations are particularly readable when length and time are measured in compatible units like seconds and lightseconds i.e. in units such that c = 1 unit of length/unit of time. Ever since 1983, metres and seconds are compatible except for historical legacy since "by definition" c = 299 792 458 m/s (≈ 1.0 feet/nanosecond).

Further cosmetic changes, called rationalisations, are possible by absorbing factors of depending on whether we want Coulomb's law or Gauss law to come out nicely, see Lorentz-Heaviside units (used mainly in particle physics). In theoretical physics it is often useful to choose units such that Plancks constant, the elementary charge, and even Newton's constant are 1. See Planck units.

Symbols in bold represent vector quantities, and symbols in "italics" represent scalar quantities, unless otherwise indicated.

The equations introduce the electric field, , a vector field, and the magnetic field, , a pseudovector field, each generally having a time and location dependence.
The sources are

The universal constants appearing in the equations are

In the differential equations, 

In the integral equations, 
Here a "fixed" volume or surface means that it does not change over time.
The equations are correct, complete and a little easier to interpret with time-independent surfaces. For example, since the surface is time-independent, we can bring the differentiation under the integral sign in Faraday's law:
Maxwell's equations can be formulated with possibly time-dependent surfaces and volumes by using the differential version and using Gauss and Stokes formula appropriately. 

The equivalence of the differential and integral formulations are a consequence of the Gauss divergence theorem and the Kelvin–Stokes theorem.

According to the (purely mathematical) Gauss divergence theorem the electric flux through the 
boundary surface can be rewritten as
The integral version of Gauss's equation can thus be rewritten as 
Since is arbitrary (e.g. an arbitrary small ball with arbitrary center), this is satisfied iff the integrand is zero. This is 
the differential equations formulation of Gauss equation up to a trivial rearrangement.

Similarly rewriting the magnetic flux in Gauss's law for magnetism in integral form gives 
which is satisfied for all iff formula_7.

By the Kelvin–Stokes theorem we can rewrite the line integrals of the fields around the closed boundary curve to an integral of the "circulation of the fields" (i.e. their curls) over a surface it bounds, i.e.

Hence the modified Ampere law in integral form can be rewritten as 
Since can be chosen arbitrarily, e.g. as an arbitrary small, arbitrary oriented, and arbitrary centered disk, we conclude that the integrand is zero iff Ampere's modified law in differential equations form is satisfied.
The equivalence of Faraday's law in differential and integral form follows likewise.

The line integrals and curls are analogous to quantities in classical fluid dynamics: the circulation of a fluid is the line integral of the fluid's flow velocity field around a closed loop, and the vorticity of the fluid is the curl of the velocity field.

The invariance of charge can be derived as a corollary of Maxwell's equations. The left hand side of the modified Ampere's Law has zero divergence by the div–curl identity. Combining the right hand side, Gauss's law, and interchange of derivatives gives:

i.e.

By the Gauss Divergence Theorem, this means the rate of change of charge in a fixed volume equals the net current flowing through the boundary:
In particular, in an isolated system the total charge is conserved.

In a region with no charges () and no currents (), such as in a vacuum, Maxwell's equations reduce to:

Taking the curl of the curl equations, and using the curl of the curl identity we obtain

The quantity formula_14 has the dimension of (time/length). Defining
formula_15, the equations above have the form of the standard wave equations

Already during Maxwell's lifetime, it was found that the known values for formula_17 and formula_18 give formula_19, then already known to be the speed of light in free space. This let him to propose that light and radio waves were propagating electromagnetic waves, since amply confirmed. In the old SI system of units, the values of formula_20 and formula_21 are defined constants, (which means that by definition formula_22) that define the ampere and the metre. In the new SI system, only "c" keeps its defined value, and the electron charge gets a defined value. 
In materials with relative permittivity, , and relative permeability, , the phase velocity of light becomes

which is usually less than .

In addition, and are perpendicular to each other and to the direction of wave propagation, and are in phase with each other. A sinusoidal plane wave is one special solution of these equations. Maxwell's equations explain how these waves can physically propagate through space. The changing magnetic field creates a changing electric field through Faraday's law. In turn, that electric field creates a changing magnetic field through Maxwell's addition to Ampère's law. This perpetual cycle allows these waves, now known as electromagnetic radiation, to move through space at velocity .

The above equations are the "microscopic" version of Maxwell's equations, expressing the electric and the magnetic fields in terms of the (possibly atomic-level) charges and currents present. This is sometimes called the "general" form, but the macroscopic version below is equally general, the difference being one of bookkeeping.

The microscopic version is sometimes called "Maxwell's equations in a vacuum": this refers to the fact that the material medium is not built into the structure of the equations, but appears only in the charge and current terms. The microscopic version was introduced by Lorentz, who tried to use it to derive the macroscopic properties of bulk matter from its microscopic constituents.

"Maxwell's macroscopic equations", also known as Maxwell's equations in matter, are more similar to those that Maxwell introduced himself.

In the "macroscopic" equations, the influence of bound charge and bound current is incorporated into the displacement field and the magnetizing field , while the equations depend only on the free charges and free currents . This reflects a splitting of the total electric charge "Q" and current "I" (and their densities "ρ" and J) into free and bound parts:

The cost of this splitting is that the additional fields and need to be determined through phenomenological constituent equations relating these fields to the electric field and the magnetic field , together with the bound charge and current.

See below for a detailed description of the differences between the microscopic equations, dealing with "total" charge and current including material contributions, useful in air/vacuum;
and the macroscopic equations, dealing with "free" charge and current, practical to use within materials.

When an electric field is applied to a dielectric material its molecules respond by forming microscopic electric dipoles – their atomic nuclei move a tiny distance in the direction of the field, while their electrons move a tiny distance in the opposite direction. This produces a "macroscopic" "bound charge" in the material even though all of the charges involved are bound to individual molecules. For example, if every molecule responds the same, similar to that shown in the figure, these tiny movements of charge combine to produce a layer of positive bound charge on one side of the material and a layer of negative charge on the other side. The bound charge is most conveniently described in terms of the polarization of the material, its dipole moment per unit volume. If is uniform, a macroscopic separation of charge is produced only at the surfaces where enters and leaves the material. For non-uniform , a charge is also produced in the bulk.

Somewhat similarly, in all materials the constituent atoms exhibit magnetic moments that are intrinsically linked to the angular momentum of the components of the atoms, most notably their electrons. The connection to angular momentum suggests the picture of an assembly of microscopic current loops. Outside the material, an assembly of such microscopic current loops is not different from a macroscopic current circulating around the material's surface, despite the fact that no individual charge is traveling a large distance. These "bound currents" can be described using the magnetization .

The very complicated and granular bound charges and bound currents, therefore, can be represented on the macroscopic scale in terms of and , which average these charges and currents on a sufficiently large scale so as not to see the granularity of individual atoms, but also sufficiently small that they vary with location in the material. As such, "Maxwell's macroscopic equations" ignore many details on a fine scale that can be unimportant to understanding matters on a gross scale by calculating fields that are averaged over some suitable volume.

The "definitions" (not constitutive relations) of the auxiliary fields are:

where is the polarization field and is the magnetization field, which are defined in terms of microscopic bound charges and bound currents respectively. The macroscopic bound charge density and bound current density in terms of polarization and magnetization are then defined as

If we define the total, bound, and free charge and current density by

and use the defining relations above to eliminate , and , the "macroscopic" Maxwell's equations reproduce the "microscopic" equations.

In order to apply 'Maxwell's macroscopic equations', it is necessary to specify the relations between displacement field and the electric field , as well as the magnetizing field and the magnetic field . Equivalently, we have to specify the dependence of the polarisation (hence the bound charge) and the magnetisation (hence the bound current) on the applied electric and magnetic field. The equations specifying this response are called constitutive relations. For real-world materials, the constitutive relations are rarely simple, except approximately, and usually determined by experiment. See the main article on constitutive relations for a fuller description.

For materials without polarization and magnetisation, the constitutive relations are (by definition)
where is the permittivity of free space and the permeability of free space. Since there is no bound charge, the total and the free charge and current are equal.

An alternative viewpoint on the microscopic equations is that they are the macroscopic equations "together" with the statement that vacuum behaves like a perfect linear "material" without additional polarisation and magnetisation.
More generally, for linear materials the constitutive relations are
where is the permittivity and the permeability of the material. For the displacement field the linear approximation is usually excellent because for all but the most extreme electric fields or temperatures obtainable in the laboratory (high power pulsed lasers) the interatomic electric fields of materials of the order of 10 V/m are much higher than the external field. For the magnetizing field formula_30, however, the linear approximation can break down in common materials like iron leading to phenomena like hysteresis. Even the linear case can have various complications, however.

Even more generally, in the case of non-linear materials (see for example nonlinear optics), and are not necessarily proportional to , similarly or is not necessarily proportional to . In general and depend on both and , on location and time, and possibly other physical quantities.

In applications one also has to describe how the free currents and charge density behave in terms of and possibly coupled to other physical quantities like pressure, and the mass, number density, and velocity of charge-carrying particles. E.g., the original equations given by Maxwell (see History of Maxwell's equations) included Ohms law in the form

Following is a summary of some of the numerous other mathematical formalisms to write the microscopic Maxwell's equations, with the columns separating the two homogeneous Maxwell equations from the two inhomogeneous ones involving charge and current. Each formulation has versions directly in terms of the electric and magnetic fields, and indirectly in terms of the electrical potential and the vector potential . Potentials were introduced as a convenient way to solve the homogeneous equations, but it was thought that all observable physics was contained in the electric and magnetic fields (or relativistically, the Faraday tensor). The potentials play a central role in quantum mechanics, however, and act quantum mechanically with observable consequences even when the electric and magnetic fields vanish (Aharonov–Bohm effect).

Each table describes one formalism. See the main article for details of each formulation. SI units are used throughout.

The Maxwell equations can also be formulated on a spacetime-like Minkowski space where space and time are treated on equal footing. The direct spacetime formulations make manifest that the Maxwell equations are relativistically invariant. Because of this symmetry electric and magnetic field are treated on equal footing and are recognised as components of the Faraday tensor. This reduces the four Maxwell equations to two, which simplifies the equations, although we can no longer use the familiar vector formulation. In fact the Maxwell equations in the space + time formulation are not Galileo invariant and have Lorentz invariance as a hidden symmetry. This was a major source of inspiration for the development of relativity theory. To repeat: the space + time formulation is not a non-relativistic approximation and it describes the same physics by simply renaming variables. For this reason the relativistic invariant equations are usually called the Maxwell equations as well.

Each table describes one formalism.


Other formalisms include the geometric algebra formulation and a matrix representation of Maxwell's equations. Historically, a quaternionic formulation was used.

Maxwell's equations are partial differential equations that relate the electric and magnetic fields to each other and to the electric charges and currents. Often, the charges and currents are themselves dependent on the electric and magnetic fields via the Lorentz force equation and the constitutive relations. These all form a set of coupled partial differential equations which are often very difficult to solve: the solutions encompass all the diverse phenomena of classical electromagnetism. Some general remarks follow.

As for any differential equation, boundary conditions and initial conditions are necessary for a unique solution. For example, even with no charges and no currents anywhere in spacetime, there are the obvious solutions for which E and B are zero or constant, but there are also non-trivial solutions corresponding to electromagnetic waves. In some cases, Maxwell's equations are solved over the whole of space, and boundary conditions are given as asymptotic limits at infinity. In other cases, Maxwell's equations are solved in a finite region of space, with appropriate conditions on the boundary of that region, for example an artificial absorbing boundary representing the rest of the universe, or periodic boundary conditions, or walls that isolate a small region from the outside world (as with a waveguide or cavity resonator).

Jefimenko's equations (or the closely related Liénard–Wiechert potentials) are the explicit solution to Maxwell's equations for the electric and magnetic fields created by any given distribution of charges and currents. It assumes specific initial conditions to obtain the so-called "retarded solution", where the only fields present are the ones created by the charges. However, Jefimenko's equations are unhelpful in situations when the charges and currents are themselves affected by the fields they create.

Numerical methods for differential equations can be used to compute approximate solutions of Maxwell's equations when exact solutions are impossible. These include the finite element method and finite-difference time-domain method. For more details, see Computational electromagnetics.

Maxwell's equations "seem" overdetermined, in that they involve six unknowns (the three components of and ) but eight equations (one for each of the two Gauss's laws, three vector components each for Faraday's and Ampere's laws). (The currents and charges are not unknowns, being freely specifiable subject to charge conservation.) This is related to a certain limited kind of redundancy in Maxwell's equations: It can be proven that any system satisfying Faraday's law and Ampere's law "automatically" also satisfies the two Gauss's laws, as long as the system's initial condition does. This explanation was first introduced by Julius Adams Stratton in 1941. Although it is possible to simply ignore the two Gauss's laws in a numerical algorithm (apart from the initial conditions), the imperfect precision of the calculations can lead to ever-increasing violations of those laws. By introducing dummy variables characterizing these violations, the four equations become not overdetermined after all. The resulting formulation can lead to more accurate algorithms that take all four laws into account.

Both identities formula_35, which reduce eight equations to six independent ones, are the true reason of overdetermination.

Maxwell's equations and the Lorentz force law (along with the rest of classical electromagnetism) are extraordinarily successful at explaining and predicting a variety of phenomena; however they are not exact, but a classical limit of quantum electrodynamics (QED).

Some observed electromagnetic phenomena are incompatible with Maxwell's equations. These include photon–photon scattering and many other phenomena related to photons or virtual photons, "nonclassical light" and quantum entanglement of electromagnetic fields (see quantum optics). E.g. quantum cryptography cannot be described by Maxwell theory, not even approximately. The approximate nature of Maxwell's equations becomes more and more apparent when going into the extremely strong field regime (see Euler–Heisenberg Lagrangian) or to extremely small distances.

Finally, Maxwell's equations cannot explain any phenomenon involving individual photons interacting with quantum matter, such as the photoelectric effect, Planck's law, the Duane–Hunt law, and single-photon light detectors. However, many such phenomena may be approximated using a halfway theory of quantum matter coupled to a classical electromagnetic field, either as external field or with the expected value of the charge current and density on the right hand side of Maxwell's equations.

Popular variations on the Maxwell equations as a classical theory of electromagnetic fields are relatively scarce because the standard equations have stood the test of time remarkably well.

Maxwell's equations posit that there is electric charge, but no magnetic charge (also called magnetic monopoles), in the universe. Indeed, magnetic charge has never been observed (despite extensive searches) and may not exist. If they did exist, both Gauss's law for magnetism and Faraday's law would need to be modified, and the resulting four equations would be fully symmetric under the interchange of electric and magnetic fields.


The developments before relativity:






</doc>
<doc id="19738" url="https://en.wikipedia.org/wiki?curid=19738" title="Metrization theorem">
Metrization theorem

In topology and related areas of mathematics, a metrizable space is a topological space that is homeomorphic to a metric space. That is, a topological space formula_1 is said to be metrizable if there is a metric 

such that the topology induced by "d" is formula_3. Metrization theorems are theorems that give sufficient conditions for a topological space to be metrizable.

Metrizable spaces inherit all topological properties from metric spaces. For example, they are Hausdorff paracompact spaces (and hence normal and Tychonoff) and first-countable. However, some properties of the metric, such as completeness, cannot be said to be inherited. This is also true of other structures linked to the metric. A metrizable uniform space, for example, may have a different set of contraction maps than a metric space to which it is homeomorphic.

One of the first widely recognized metrization theorems was Urysohn's metrization theorem. This states that every Hausdorff second-countable regular space is metrizable. So, for example, every second-countable manifold is metrizable. (Historical note: The form of the theorem shown here was in fact proved by Tychonoff in 1926. What Urysohn had shown, in a paper published posthumously in 1925, was that every second-countable "normal" Hausdorff space is metrizable). The converse does not hold: there exist metric spaces that are not second countable, for example, an uncountable set endowed with the discrete metric. The Nagata–Smirnov metrization theorem, described below, provides a more specific theorem where the converse does hold.

Several other metrization theorems follow as simple corollaries to Urysohn's Theorem. For example, a compact Hausdorff space is metrizable if and only if it is second-countable.

Urysohn's Theorem can be restated as: A topological space is separable and metrizable if and only if it is regular, Hausdorff and second-countable. The Nagata–Smirnov metrization theorem extends this to the non-separable case. It states that a topological space is metrizable if and only if it is regular, Hausdorff and has a σ-locally finite base. A σ-locally finite base is a base which is a union of countably many locally finite collections of open sets. For a closely related theorem see the Bing metrization theorem.

Separable metrizable spaces can also be characterized as those spaces which are homeomorphic to a subspace of the Hilbert cube formula_4, i.e. the countably infinite product of the unit interval (with its natural subspace topology from the reals) with itself, endowed with the product topology.

A space is said to be locally metrizable if every point has a metrizable neighbourhood. Smirnov proved that a locally metrizable space is metrizable if and only if it is Hausdorff and paracompact. In particular, a manifold is metrizable if and only if it is paracompact.

The group of unitary operators formula_5 on a separable Hilbert space formula_6 endowed
with the strong operator topology is metrizable (see Proposition II.1 in ).

Non-normal spaces cannot be metrizable; important examples include

The real line with the lower limit topology is not metrizable. The usual distance function is not a metric on this space because the topology it determines is the usual topology, not the lower limit topology. This space is Hausdorff, paracompact and first countable.

The long line is locally metrizable but not metrizable; in a sense it is "too long".



</doc>
<doc id="19739" url="https://en.wikipedia.org/wiki?curid=19739" title="Martin Agricola">
Martin Agricola

Martin Agricola (6 January 1486 – 10 June 1556) was a German composer of Renaissance music and a music theorist.

Agricola was born in Schwiebus in Lebusz.

From 1524 until his death he lived at Magdeburg, where he occupied the post of teacher or cantor in the Protestant school. The senator and music-printer Georg Rhau, of Wittenberg, was a close friend of Agricola, whose theoretical works, providing valuable material concerning the change from the old to the new system of notation, he published.

Among Agricola's other theoretical works is "Musica instrumentalis deudsch" (1528 and 1545), a study of musical instruments, and one of the most important works in early organology; and one of the earliest books on the Rudiments of music.

Agricola was also the first to harmonize in four parts Martin Luther's chorale, "Ein feste Burg".




</doc>
<doc id="19740" url="https://en.wikipedia.org/wiki?curid=19740" title="Max August Zorn">
Max August Zorn

Max August Zorn (; June 6, 1906 – March 9, 1993) was a German mathematician. He was an algebraist, group theorist, and numerical analyst. He is best known for Zorn's lemma, a method used in set theory that is applicable to a wide range of mathematical constructs such as vector spaces, ordered sets and the like. Zorn's lemma was first postulated by Kazimierz Kuratowski in 1922, and then independently by Zorn in 1935.

Zorn was born in Krefeld, Germany. He attended the University of Hamburg. He received his Ph.D. in April 1930 for a thesis on alternative algebras. He published his findings in "Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg". Zorn showed that split-octonions could be represented by a mixed-style of matrices called Zorn's vector-matrix algebra.

Max Zorn was appointed as an assistant at the University of Halle. However, he did not have the opportunity to work there for long since he was forced to leave Germany in 1933 because of the Nazi policies. According to grandson Eric, "[Max] spoke with a raspy, airy voice most of his life. Few people knew why, because he only told the story after significant prodding, but he talked that way because pro-Hitler thugs who objected to his politics, had battered his throat in a 1933 street fight."

Zorn emigrated to the U.S. and was appointed a Sterling Fellow at Yale University. While at Yale, Zorn wrote his paper "A Remark on Method in Transfinite Algebra" that stated his Maximum Principle, later called Zorn's lemma. It requires a set that contains the union of any chain of subsets to have one chain not contained in any other, called the maximal element. He illustrated the principle with applications in ring theory and field extensions. Zorn’s lemma is an alternative expression of the axiom of choice, and thus a subject of interest in axiomatic set theory.

In 1936 he moved to UCLA and remained until 1946. While at UCLA Zorn revisited his study of alternative rings and proved the existence of the nilradical of certain alternative rings. According to Angus E. Taylor, Max was his most stimulating colleague at UCLA.

In 1946 Zorn became a professor at Indiana University where he taught until retiring in 1971. He was thesis advisor for Israel Nathan Herstein.

Zorn died in Bloomington, Indiana, United States, in March 1993, of congestive heart failure, according to his obituary in "The New York Times".

Max Zorn married Alice Schlottau and they had one son, Jens, and one daughter, Liz. Jens (born June 19, 1931) is an emeritus professor of physics at the University of Michigan and an accomplished sculptor. Max Zorn's grandson Eric Zorn is a columnist for the "Chicago Tribune".





</doc>
<doc id="19745" url="https://en.wikipedia.org/wiki?curid=19745" title="Main (river)">
Main (river)

The Main ( is a river in Germany. With a length of (including its 52 km long source river White Main), it is the longest right tributary of the Rhine. It is also the longest river lying entirely in Germany (if the Weser and the Werra are considered as two separate rivers; together they are longer). The largest cities along the Main are Frankfurt am Main and Würzburg.

The mainspring of the Main River flows through the German states of Bavaria, Baden-Württemberg (forming the border with Bavaria for some distance) and Hesse. Its basin competes with the Danube for water; as a result, many of its boundaries are identical with those of the European Watershed.

The Main begins near Kulmbach in Franconia at the joining of its two headstreams, the Red Main ("Roter Main") and the White Main ("Weißer Main"). The Red Main originates in the Franconian Jura mountain range, in length, and runs through Creussen and Bayreuth. The White Main originates in the mountains of the Fichtelgebirge; it is long. In its upper and middle section, the Main runs through the valleys of the German Highlands. Its lower section crosses the Lower Main Lowlands (Hanau-Seligenstadt Basin and northern Upper Rhine Plain) to Wiesbaden, where it discharges into the Rhine. Major tributaries of the Main are the Regnitz, the Franconian Saale, the Tauber, and the Nidda.

The name ""Main"" derives from the Latin "Moenus" or "Menus". It is not related to the name of the city Mainz (Latin: "Moguntiacum").

The Main is navigable for shipping from its mouth at the Rhine close to Mainz for to Bamberg. Since 1992, the Main has been connected to the Danube via the Rhine-Main-Danube Canal and the highly regulated Altmühl river. The Main has been canalized with 34 large locks () to allow CEMT class V vessels () to navigate the total length of the river. The 16 locks in the adjacent Rhine-Main-Danube Canal and the Danube itself are of the same dimensions.

There are 34 dams and locks along the 380 km navigable portion of the Main, from the confluence with the Regnitz near Bamberg, to the Rhine.


Most of the dams along the Main also have turbines for power generation.

Tributaries from source to mouth:
Left

Right

Around Frankfurt are several large inland ports. Because the river is rather narrow on many of the upper reaches, navigation with larger vessels and push convoys requires great skill.

The largest cities along the Main are Frankfurt am Main and Würzburg. The Main also passes the following towns and cities: Burgkunstadt, Lichtenfels, Bad Staffelstein, Eltmann, Haßfurt, Schweinfurt, Volkach, Kitzingen, Marktbreit, Ochsenfurt, Karlstadt, Gemünden, Lohr, Marktheidenfeld, Wertheim, Miltenberg, Obernburg, Erlenbach/Main, Aschaffenburg, Seligenstadt, Hainburg, Hanau, Offenbach, Hattersheim, Flörsheim, and Rüsselsheim.

The river has gained enormous importance as a vital part of European "Corridor VII", the inland waterway link from the North Sea to the Black Sea.

In a historical and political sense, the Main line is referred to as the northern border of Southern Germany, with its predominantly Catholic population. The river roughly marked the southern border of the North German Federation, established in 1867 under Prussian leadership as the predecessor of the German Empire.

The river course also corresponds with the Speyer line isogloss between Central and Upper German dialects, sometimes mocked as "Weißwurstäquator".

The Main-Radweg is a major German bicycle path running along the Main River. It is approximately and was the first long-distance bicycle path to be awarded 5 stars by the General German Bicycle Club ADFC in 2008. It starts from either Creußen or Bischofsgrün and ends in Mainz.





</doc>
<doc id="19747" url="https://en.wikipedia.org/wiki?curid=19747" title="Marcus Vipsanius Agrippa">
Marcus Vipsanius Agrippa

Marcus Vipsanius Agrippa (; 64/62 BC – 12 BC) was a Roman consul, statesman, general and architect. He was a close friend, son-in-law, and lieutenant to Gaius Julius Caesar Octavianus and was responsible for the construction of some of the most notable buildings in the history of Rome and for important military victories, most notably at the Battle of Actium in 31 BC against the forces of Mark Antony and Cleopatra. As a result of these victories, Octavianus became the first Roman Emperor, adopting the name of Augustus. Agrippa assisted Augustus in making Rome 'a city of marble" and renovating aqueducts to give all Romans, from every social class, access to the highest quality public services. He was responsible for the creation of many baths, porticoes and gardens, as well as the original Pantheon. Agrippa was also husband to Julia the Elder (who later married the second Emperor Tiberius), maternal grandfather to Caligula, and maternal great-grandfather to the Emperor Nero.

Agrippa was born between 64–62 BC, in an uncertain location. His father was perhaps called Lucius Vipsanius Agrippa. He had an elder brother whose name was also Lucius Vipsanius Agrippa, and a sister named Vipsania Polla. His family originated in the Italian countryside and was of humble and plebeian origins. They had not been prominent in Roman public life. However, Agrippa was about the same age as Octavian (the future emperor Augustus), and the two were educated together and became close friends. Despite Agrippa's association with the family of Julius Caesar, his elder brother chose another side in the civil wars of the 40s BC, fighting under Cato against Caesar in Africa. When Cato's forces were defeated, Agrippa's brother was taken prisoner but freed after Octavian interceded on his behalf.

It is not known whether Agrippa fought against his brother in Africa, but he probably served in Caesar's campaign of 46–45 BC against Gnaeus Pompeius, which culminated in the Battle of Munda. Caesar regarded him highly enough to send him with Octavius in 45 BC to study in Apollonia (on the Illyrian coast) with the Macedonian legions, while Caesar consolidated his power in Rome. In the fourth month of their stay in Apollonia the news of Julius Caesar's assassination in March 44 BC reached them. Agrippa and another friend, Quintus Salvidienus Rufus, advised Octavius to march on Rome with the troops from Macedonia, but Octavius decided to sail to Italy with a small retinue. After his arrival, he learned that Caesar had adopted him as his legal heir. Octavius at this time took Caesar's name, but modern historians refer to him as "Octavian" during this period.

After Octavian's return to Rome, he and his supporters realised they needed the support of legions. Agrippa helped Octavian to levy troops in Campania. Once Octavian had his legions, he made a pact with Mark Antony and Lepidus, legally established in 43 BC as the Second Triumvirate. Octavian and his consular colleague Quintus Pedius arranged for Caesar's assassins to be prosecuted in their absence, and Agrippa was entrusted with the case against Gaius Cassius Longinus. It may have been in the same year that Agrippa began his political career, holding the position of Tribune of the Plebs, which granted him entry to the Senate.
In 42 BC, Agrippa probably fought alongside Octavian and Antony in the Battle of Philippi. After their return to Rome, he played a major role in Octavian's war against Lucius Antonius and Fulvia Antonia, respectively the brother and wife of Mark Antony, which began in 41 BC and ended in the capture of Perusia in 40 BC. However, Salvidienus remained Octavian's main general at this time. After the Perusine war, Octavian departed for Gaul, leaving Agrippa as urban praetor in Rome with instructions to defend Italy against Sextus Pompeius, an opponent of the Triumvirate who was now occupying Sicily. In July 40, while Agrippa was occupied with the Ludi Apollinares that were the praetor's responsibility, Sextus began a raid in southern Italy. Agrippa advanced on him, forcing him to withdraw. However, the Triumvirate proved unstable, and in August 40 both Sextus and Antony invaded Italy (but not in an organized alliance). Agrippa's success in retaking Sipontum from Antony helped bring an end to the conflict. Agrippa was among the intermediaries through whom Antony and Octavian agreed once more upon peace. During the discussions Octavian learned that Salvidienus had offered to betray him to Antony, with the result that Salvidienus was prosecuted and either executed or committed suicide. Agrippa was now Octavian's leading general.
In 39 or 38 BC, Octavian appointed Agrippa governor of Transalpine Gaul, where in 38 BC he put down a rising of the Aquitanians. He also fought the Germanic tribes, becoming the next Roman general to cross the Rhine after Julius Caesar. He was summoned back to Rome by Octavian to assume the consulship for 37 BC. He was well below the usual minimum age of 43, but Octavian had suffered a humiliating naval defeat against Sextus Pompey and needed his friend to oversee the preparations for further warfare. Agrippa refused the offer of a triumph for his exploits in Gaul – on the grounds, says Dio, that he thought it improper to celebrate during a time of trouble for Octavian. Since Sextus Pompeius had command of the sea on the coasts of Italy, Agrippa's first care was to provide a safe harbour for Octavian's ships. He accomplished this by cutting through the strips of land which separated the Lacus Lucrinus from the sea, thus forming an outer harbour, while joining the lake Avernus to the Lucrinus to serve as an inner harbor. The new harbor-complex was named Portus Julius in Octavian's honour. Agrippa was also responsible for technological improvements, including larger ships and an improved form of grappling hook. About this time, he married Caecilia Pomponia Attica, daughter of Cicero's friend Titus Pomponius Atticus.

In 36 BC, Octavian and Agrippa set sail against Sextus. The fleet was badly damaged by storms and had to withdraw; Agrippa was left in charge of the second attempt. Thanks to superior technology and training, Agrippa and his men won decisive victories at Mylae and Naulochus, destroying all but seventeen of Sextus' ships and compelling most of his forces to surrender. Octavian, with his power increased, forced the triumvir Lepidus into retirement and entered Rome in triumph. Agrippa received the unprecedented honour of a naval crown decorated with the beaks of ships; as Dio remarks, this was "a decoration given to nobody before or since".

Agrippa participated in smaller military campaigns in 35 and 34 BC, but by the autumn of 34 he had returned to Rome. He rapidly set out on a campaign of public repairs and improvements, including renovation of the aqueduct known as the Aqua Marcia and an extension of its pipes to cover more of the city. He became the first water commissioner of Rome in 33 BC. Through his actions after being elected in 33 BC as one of the aediles (officials responsible for Rome's buildings and festivals), the streets were repaired and the sewers were cleaned out, while lavish public spectacles were put on. Agrippa signalled his tenure of office by effecting great improvements in the city of Rome, restoring and building aqueducts, enlarging and cleansing the Cloaca Maxima, constructing baths and porticos, and laying out gardens. He also gave a stimulus to the public exhibition of works of art. It was unusual for an ex-consul to hold the lower-ranking position of aedile, but Agrippa's success bore out this break with tradition. As emperor, Augustus would later boast that "he had found the city of brick but left it of marble", thanks in part to the great services provided by Agrippa under his reign.

Agrippa was again called away to take command of the fleet when the war with Antony and Cleopatra broke out. He captured the strategically important city of Methone at the southwest of the Peloponnese, then sailed north, raiding the Greek coast and capturing Corcyra (modern Corfu). Octavian then brought his forces to Corcyra, occupying it as a naval base. Antony drew up his ships and troops at Actium, where Octavian moved to meet him. Agrippa meanwhile defeated Antony's supporter Quintus Nasidius in a naval battle at Patrae. Dio relates that as Agrippa moved to join Octavian near Actium, he encountered Gaius Sosius, one of Antony's lieutenants, who was making a surprise attack on the squadron of Lucius Tarius, a supporter of Octavian. Agrippa's unexpected arrival turned the battle around.

As the decisive battle approached, according to Dio, Octavian received intelligence that Antony and Cleopatra planned to break past his naval blockade and escape. At first he wished to allow the flagships past, arguing that he could overtake them with his lighter vessels and that the other opposing ships would surrender when they saw their leaders' cowardice. Agrippa objected, saying that Antony's ships, although larger, could outrun Octavian's if they hoisted sails, and that Octavian ought to fight now because Antony's fleet had just been struck by storms. Octavian followed his friend's advice.

On September 2, 31 BC, the Battle of Actium was fought. Octavian's victory, which gave him the mastery of Rome and the empire, was mainly due to Agrippa. Octavian then bestowed upon him the hand of his niece Claudia Marcella Major in 28 BC. He also served a second consulship with Octavian the same year. In 27 BC, Agrippa held a third consulship with Octavian, and in that year, the senate also bestowed upon Octavian the imperial title of Augustus.

In commemoration of the Battle of Actium, Agrippa built and dedicated the building that served as the Roman Pantheon before its destruction in 80 AD. Emperor Hadrian used Agrippa's design to build his own Pantheon, which survives in Rome. The inscription of the later building, which was built around 125, preserves the text of the inscription from Agrippa's building during his third consulship. The years following his third consulship, Agrippa spent in Gaul, reforming the provincial administration and taxation system, along with building an effective road system and aqueducts.

Agrippa's friendship with Augustus seems to have been clouded by the jealousy of Augustus' nephew Marcus Claudius Marcellus, which was probably instigated by the intrigues of Livia, the third wife of Augustus, who feared his influence over her husband. Traditionally it is said the result of such jealousy was that Agrippa left Rome, ostensibly to take over the governorship of eastern provinces – a sort of honourable exile, but he only sent his legate to Syria, while he himself remained at Lesbos and governed by proxy, though he may have been on a secret mission to negotiate with the Parthians about the return of the Roman legions' standards which they held. On the death of Marcellus, which took place within a year of his exile, he was recalled to Rome by Augustus, who found he could not dispense with his services. However, if one places the events in the context of the crisis in 23 BC it seems unlikely that, when facing significant opposition and about to make a major political climb down, the emperor Augustus would place a man in exile in charge of the largest body of Roman troops. What is far more likely is that Agrippa's 'exile' was actually the careful political positioning of a loyal lieutenant in command of a significant army as a backup plan in case the settlement plans of 23 BC failed and Augustus needed military support. Moreover, after 23 BC as part of what became known as Augustus' "Second Constitutional Settlement", Agrippa's constitutional powers were greatly increased to provide the Principate of Augustus with greater constitutional stability by providing for a political heir or replacement for Augustus if he were to succumb to his habitual ill health or was assassinated. In the course of the year, proconsular imperium, similar to Augustus' power, was conferred upon Agrippa for five years. The exact nature of the grant is uncertain but it probably covered Augustus' imperial provinces, east and west, perhaps lacking authority over the provinces of the Senate. That was to come later, as was the jealously guarded tribunicia potestas, or powers of a tribune of the plebeians. These great powers of state are not usually heaped upon a former exile.
It is said that Maecenas advised Augustus to attach Agrippa still more closely to him by making him his son-in-law. He accordingly induced him to divorce Marcella and marry his daughter, Julia the Elder--the widow of Marcellus, equally celebrated for her beauty, abilities, and her shameless extravagance--by 21 BC. In 19 BC, Agrippa was employed in putting down a rising of the Cantabrians in Hispania (Cantabrian Wars).

In 18 BC, Agrippa's powers were even further increased to almost match those of Augustus. That year his proconsular imperium was augmented to cover the provinces of the Senate. More than that, he was finally granted tribunicia potestas, or powers of a tribune of the plebeians. As was the case with Augustus, Agrippa’s grant of tribunician powers was conferred without his having to actually hold that office. These powers were considerable, giving him veto power over the acts of the Senate or other magistracies, including those of other tribunes, and the power to present laws for approval by the People. Just as important, a tribune’s person was sacred, meaning that any person who harmfully touched them or impeded their actions, including political acts, could lawfully be killed. After the grant of these powers Agrippa was, on paper, almost as powerful as Augustus was. However, there was no doubt that Augustus was the man in charge.

Agrippa was appointed governor of the eastern provinces a second time in 17 BC, where his just and prudent administration won him the respect and good-will of the provincials, especially from the Jewish population. Agrippa also restored effective Roman control over the Cimmerian Chersonnese (Crimean Peninsula) during his governorship.

Agrippa’s last public service was his beginning of the conquest of the upper Danube River region, which would become the Roman province of Pannonia in 13 BC. He died at Campania in 12 BC at the age of 51. His posthumous son, Marcus Vipsanius Agrippa Postumus, was named in his honor. Augustus honoured his memory by a magnificent funeral and spent over a month in mourning. Augustus personally oversaw all of Agrippa's children’s educations. Although Agrippa had built a tomb for himself, Augustus had Agrippa's remains placed in Augustus' own mausoleum.

Agrippa was also known as a writer, especially on the subject of geography. Under his supervision, Julius Caesar's dream of having a complete survey of the Empire made was carried out. Agrippa constructed a circular chart, which was later engraved on marble by Augustus, and afterwards placed in the colonnade built by his sister Polla. Amongst his writings, an autobiography, now lost, is referenced.

Agrippa established a standard for Roman foot (Agrippa's own) in 29 BC, and thus a definition of a pace as 5 feet. An imperial Roman mile denotes 5,000 Roman feet.

The term Via Agrippa is used for any part of the network of roadways in Gaul built by Agrippa. Some of these still exist as paths or even as highways.

Agrippa had several children through his three marriages:

Through his numerous children, Agrippa would become ancestor to many subsequent members of the Julio-Claudian dynasty, whose position he helped to attain, as well as many other reputed Romans.


There have been some attempts to assign further descendants to a number of the aforementioned figures, including two lines of Asinii descended from Gaius Asinius Pollio and Marcus Asinius Agrippa respectively. A daughter (and further descendants) named Rubellia Bassa to Julia, who may have been a daughter of Gaius Rubellius Blandus by an earlier marriage. And, finally, a series of descendants from Junia Lepida and her husband, Gaius Cassius Longinus. However, all of these lines of descent are extremely hypothetical and lack any evidence to support a connection to the descendants of Agrippa.
Agrippa is a character in William Shakespeare's play "Antony and Cleopatra".

A fictional version of Agrippa in his later life played a prominent role in the 1976 BBC Television series "I, Claudius". Agrippa was portrayed as a much older man, though he would have only been 39 years old at the time of the first episode (24/23 BC). He was played by John Paul.

Agrippa is the main character in Paul Naschy's 1980 film "Los cántabros", played by Naschy himself. It is a highly fictionalized version of the Cantabrian Wars in which Agrippa is depicted as the lover of the sister of Cantabrian leader Corocotta.

Agrippa appears in several film versions of the life of Cleopatra. He is normally portrayed as an old man rather than a young one. Among the people to portray him are Philip Locke, Alan Rowe and Andrew Keir.

Agrippa is also one of the principal characters in the British/Italian joint project "" (2003) featuring flashbacks between Augustus and Julia about Agrippa, which shows him in his youth on serving in Caesar's army up until his victory at Actium and the defeat of Cleopatra. He is portrayed by Ken Duken. In the 2005 series "Empire" the young Agrippa (played by Christopher Egan) becomes Octavian's sidekick after saving him from an attempted poisoning.

Marcus Agrippa, a highly fictional character based on Marcus Vipsanius Agrippa's early life, is part of the BBC-HBO-RAI television series "Rome". He is played by Allen Leech. He describes himself as the grandson of a slave. The series creates a romantic relationship between Agrippa and Octavian's sister Octavia Minor, for which there is no historical evidence.

Agrippa is mentioned by name in book VIII of Virgil's "The Aeneid", where Aeneas sees an image of Agrippa leading ships in the Battle of Actium on the shield forged for him by Vulcan and given to him by his mother, Venus.

Agrippa is a main character in the early part of Robert Graves' novel "I, Claudius". He is a main character in the later two novels of Colleen McCullough's Masters of Rome series. He is a featured character of prominence and importance in the historical fiction novel "Cleopatra's Daughter" by Michelle Moran. He also features prominently in John Edward Williams' historical novel "Augustus". In the backstory of "Gunpowder Empire", the first volume in Harry Turtledove's Crosstime Traffic series, Agrippa lived until AD 26, conquering all of Germania for the Empire and becoming the second Emperor when Augustus died in AD 14.

A heavily fictionalized version of Agrippa is one of the playable characters (the other being an equally fictionalized Augustus) in the video game "Shadow of Rome". There, Agrippa is sentenced to become a gladiator after his father was wrongly sentenced for assassinating Caesar. Agrippa's goal is to stay alive as a gladiator for as long as possible, while Augustus acts as an infiltrator who slowly exposes the conspiracy against Caesar. Eventually, Augustus is able to prove Vipsanius' innocence and both of them are pardoned. Then a civil war breaks out, because the direct successor was outraged by exposure of the conspiracy. Agrippa and Augustus fight against Antonius. Agrippa appears as a Great Admiral in the computer game Sid Meier's Civilization V. A fictionalized version of Agrippa also appears in the video game Assassin's Creed Origins as the commander of the Roman Citadel in the province of Kyrenaika where the player character has to kill him and retrieve a document from his body.





</doc>
<doc id="19757" url="https://en.wikipedia.org/wiki?curid=19757" title="Mariotto Albertinelli">
Mariotto Albertinelli

Mariotto Albertinelli, in full Mariotto di Bigio di Bindo Albertinelli (13 October 1474 – 5 November 1515) was an Italian Renaissance painter of the Florentine school. He was a close friend and collaborator of Fra Bartolomeo and their joint works appear as if they have been painted by one hand.

His work shows the influence of Perugino, Piero di Cosimo and Lorenzo di Credi as well as of Flemish painting. Some of Albertinelli's works reveal an eccentrically archaic tendency while others show a return to conventions of the early Renaissance.

Mariotto Albertinelli was born in Florence as the son of a gold beater. He was an only child and his mother died when he was just five years old. He was himself trained as a gold beater until the age of 12 when he became a pupil of Cosimo Rosselli and a fellow-pupil with Fra Bartolomeo. The two pupils formed such a close friendship that in 1494 they started a joint studio in Florence. After a while Albertinelli had mastered Fra Bartolomeo’s technique to such extent that he could paint in a style that blended with that of his partner. The closeness in style was such that for many years some doubts remained over who had painted certain works. For example, the Kress tondo, now in the Columbia Museum of Art was previously attributed to Fra Bartolomeo but is now thought to be the work of Albertinelli using the former's cartoon.
According to the early Italian biographer Vasari, Albertinelli was in the beginning of his career exclusively working for Alfonsina Orsini, the wife of Piero II de’ Medici and mother of Lorenzo II de' Medici. His early works were small paintings destined for the homes of sophisticated patrons. He produced these works independently of Fra Bartolommeo and as a result they are stylistically distinguishable. Piero di Cosimo who worked in Cosimo Rosselli’s workshop introduced Albertinelli to Flemish techniques.

From around 1500 Fra Bartolomeo renounced painting for a few years in the wake of Savonarola's morality campaign. He joined the Dominican order as Fra Bartolomeo. Albertinelli then worked as an independent painter. He received various commissions including in 1503 for the high altarpiece for the Chapel of Congregazione di San Martino (later Church of Santa Elisabetta) in Florence (now in the Uffizi). The central panel depicts the Visitation and the predella the Circumcision, the Adoration of the Child and the Annunciation. The subject depicts the story in the Gospel of Luke when Elizabeth, the cousin of the Virgin Mary visits Mary, who is, like herself, pregnant. Elizabeth features prominently in this episode and this subject was therefore chosen for a church dedicated to her. While the figural composition of the Visitation was derived from designs by Fra Bartolomeo, Albertinelli changed elements in these designs by replacing Fra Bartolomeo’s strong contrasts in light and dark with smoother gradations of tone. The work also shows the influence of Perugino in its use of soft highlights and its inclusion of a classical arcade.
In 1509 Albertinelli and Fra Bartolomeo, who had by then resumed painting, entered again into a partnership. Their partnership was on an equal footing and entitled each to half the profit of a shared commission. The partnership was dissolved in January 1513, as is evidenced by a surviving document.

According to Vasari, Albertinelli lived as a libertine and was fond of good living and women. He experienced financial problems and was unable to repay some of his loans, including one to Raphael, before he died. Albertinelli’s wife Antonia whom he had married in 1506 repaid some loans. Among his many students were Jacopo da Pontormo, Innocenzo di Pietro Francucci da Imola and Giuliano Bugiardini.

Albertinelli's paintings bear the imprint of varied influences: Perugino's sense of volume in space and perspective, Fra Bartolomeo's coloring, the landscape portrayal of Flemish masters like Memling and Leonardo's Sfumato technique. His chief paintings are in Florence, notably his masterpiece, the "Visitation" (1503), which is in the collection of the Uffizi.


</doc>
<doc id="19758" url="https://en.wikipedia.org/wiki?curid=19758" title="Beijing cuisine">
Beijing cuisine

Beijing cuisine, also known as Jing cuisine and Mandarin cuisine, and as Beiping cuisine in Taiwan, is the local cuisine of Beijing, the national capital of China.

As Beijing has been the capital of China for centuries, its cuisine is influenced by culinary traditions from all over China, but the style that has the greatest influence on Beijing cuisine is that of the eastern coastal province of Shandong. Beijing cuisine has itself, in turn, also greatly influenced other Chinese cuisines, particularly the cuisine of Liaoning, the Chinese imperial cuisine, and the Chinese aristocrat cuisine.

Another tradition that influenced Beijing cuisine (as well as influenced by the latter itself) is the Chinese imperial cuisine that originated from the "Emperor's Kitchen" (), which referred to the cooking facilities inside the Forbidden City, where thousands of cooks from different parts of China showed their best culinary skills to please the imperial family and officials. Therefore, it is sometimes difficult to determine the actual origin of a dish as the term "Mandarin" is generalised and refers not only to Beijing, but other provinces as well. However, some generalisation of Beijing cuisine can be characterised as follows: Foods that originated in Beijing are often snacks rather than main courses, and they are typically sold by small shops or street vendors. There is emphasis on dark soy paste, sesame paste, sesame oil and scallions, and fermented tofu is often served as a condiment. In terms of cooking techniques, methods relating to different ways of frying are often used. There is less emphasis on rice as an accompaniment as compared to many other regions in China, as local rice production in Beijing is limited by the relatively dry climate.

Many dishes in Beijing cuisine that are served as main courses are derived from a variety of Chinese Halal foods, particularly lamb and beef dishes, as well as from Huaiyang cuisine.

Huaiyang cuisine has been praised since ancient times in China, and it was a general practice for an official travelling to Beijing to take up a new post to bring along with him a chef specialising in Huaiyang cuisine. When these officials had completed their terms in the capital and returned to their native provinces, most of the chefs they brought along often remained in Beijing. They opened their own restaurants or were hired by wealthy locals. The imperial clan of the Ming dynasty, the House of Zhu, who had ancestry from Jiangsu Province, also contributed greatly in introducing Huaiyang cuisine to Beijing when the capital was moved from Nanjing to Beijing in the 15th century, because the imperial kitchen was mainly Huaiyang style. The element of traditional Beijing culinary and gastronomical cultures of enjoying artistic performances such as Beijing opera while dining directly developed from the similar practice in the culture of Jiangsu and Huaiyang cuisines.

Chinese Islamic cuisine is another important component of Beijing cuisine, and was first prominently introduced when Beijing became the capital of the Yuan dynasty. However, the most significant contribution to the formation of Beijing cuisine came from Shandong cuisine, as most chefs from Shandong Province came to Beijing en masse during the Qing dynasty. Unlike the earlier two cuisines, which were brought by the ruling class such as nobles, aristocrats and bureaucrats, and then spread to the general populace, the introduction of Shandong cuisine begun with serving the general populace, with much wider market segment, from wealthy merchants to the working class.

The Qing dynasty was a major period in the formation of Beijing cuisine. Before the Boxer Rebellion, the foodservice establishments in Beijing were strictly stratified by the foodservice guild. Each category of the establishment was specifically based on its ability to provide for a particular segment of the market. The top ranking foodservice establishments served nobles, aristocrats, and wealthy merchants and landlords, while lower ranking foodservice establishments served the populace of lower financial and social status. It was during this period when Beijing cuisine gained fame and became recognised by the Chinese culinary society, and the stratification of the foodservice was one of its most obvious characteristics as part of its culinary and gastronomic cultures during this first peak of its formation.

The official stratification was an integral part of the local culture of Beijing and it was not finally abolished officially after the end of the Qing dynasty, which resulted in the second peak in the formation of Beijing cuisine. Meals previously offered to nobles and aristocrats were made available to anyone who could afford them instead of being restricted only to the upper class. As chefs freely switched between jobs offered by different foodservice establishments, they brought their skills that further enriched and developed Beijing cuisine. Though the stratification of food services in Beijing was no longer effected by imperial laws, the structure more or less remained despite continuous weakening due to the financial background of the local clientele. The different classes are listed in the following subsections.

Foodservice establishments with names ending with the Chinese character "zhuang" (), or "zhuang zihao" (), were the top-ranking foodservice establishments, not only in providing foods, but entertainment as well. The form of entertainment provided was usually Beijing opera, and foodservice establishments of this class always had long-term contracts with a Beijing opera troupe to perform onsite. Moreover, foodservice establishments of this class would always have long-term contracts with famous performers, such as national-treasure-class performers, to perform onsite, though not on a daily basis. Foodservice establishments of this category did not accept any different customers on a walk-in basis, but instead, only accepted customers who came as a group and ordered banquets by appointment, and the banquets provided by foodservice establishments of this category often included most, if not all tables, at the site. The bulk of the business of foodservice of this category, however, was catering at customers' homes or other locations, and such catering was often for birthdays, marriages, funerals, promotions and other important celebrations and festivals. When catering, these foodservice establishments not only provided what was on the menu, but fulfilled customers' requests.

Foodservice establishments categorised as "leng zhuangzi" () lacked any rooms to host banquets, and thus their business was purely catering.

Foodservice establishments with names ending with the Chinese character "tang" (), or "tang zihao" (), are similar to foodservice establishments with names ending with the Chinese character "zhuang", but the business of these second-class foodservice establishments were generally evenly divided among onsite banquet hosting and catering (at customers' homes). Foodservice establishments of this class would also have long-term contracts with Beijing opera troupes to perform onsite, but they did not have long-term contracts with famous performers, such as national-treasure-class performers, to perform onsite on regular basis; however these top performers would still perform at foodservice establishments of this category occasionally. In terms of catering at the customers' sites, foodservice establishments of this category often only provided dishes strictly according to their menu, and would not provide any dishes that were not on the menu.

Foodservice establishments with names ending with the Chinese character "ting" (), or "ting zihao" () are foodservice establishments which had more business in onsite banquet hosting than catering at customers' homes. For onsite banquet hosting, entertainment was still provided, but foodservice establishments of this category did not have long-term contracts with Beijing opera troupes, so that performers varied from time to time, and top performers usually did not perform here or at any lower-ranking foodservice establishments. For catering, different foodservice establishments of this category were incapable of handling significant catering on their own, but generally had to combine resources with other foodservice establishments of the same ranking (or lower) to do the job.

Foodservice establishments with names ending with the Chinese character "yuan" (), or "yuan zihao" () did nearly all their business in hosting banquets onsite. Entertainment was not provided on a regular basis, but there were stages built onsite for Beijing opera performers. Instead of being hired by the foodservice establishments like in the previous three categories, performers at foodservice establishments of this category were usually contractors who paid the foodservice establishment to perform and split the earnings according to a certain percentage. Occasionally, foodservice establishments of this category would be called upon to help cater at customers' homes, and like foodservice establishments with names ending with the Chinese character "ting", they could not do the job on their own but had to work with others, never taking the lead as foodservice establishments with names ending with the Chinese character "ting" could.

Foodservice establishments with names ending with the Chinese character "lou" (), or "lou zihao" () did the bulk of their business hosting banquets onsite by appointment. In addition, a smaller portion of the business was in serving different customers onsite on a walk-in basis. Occasionally, when catering at customers' homes, foodservice establishments of this category would only provide the few specialty dishes they were famous for.

Foodservice establishments with names ending with the Chinese character "ju" (), or "ju zihao" () generally divided their business evenly into two areas: serving different customers onsite on a walk-in basis, and hosting banquets by appointment for customers who came as one group. Occasionally, when catering at the customers' homes, foodservice establishments of this category would only provide the few specialty dishes they were famous for, just like foodservice establishments with names ending with the Chinese character "lou". However, unlike those establishments, which always cooked their specialty dishes on location, foodservice establishment of this category would either cook on location or simply bring the already-cooked food to the location.

Foodservice establishments with names ending with the Chinese character "zhai" (), or "zhai zihao" () were mainly in the business of serving different customers onsite on a walk-in basis, but a small portion of their income did come from hosting banquets by appointment for customers who came as one group. Just like foodservice establishments with names ending with the Chinese character "ju", when catering at customers’ homes, foodservice establishments of this category would also only provide the few specialty dishes they are famous for, but they would mostly bring the already-cooked dishes to the location, and would only cook on location occasionally.

Foodservice establishments with names ending with the Chinese character "fang" (), or "fang zihao" (). Foodservice establishments of this category generally did not offer the service of hosting banquets made by appointment for customers who came as one group, but instead, often only offered to serve different customers onsite on a walk-in basis. Foodservice establishments of this category or lower would not be called upon to perform catering at the customers' homes for special events.

Foodservice establishments with names ending with the Chinese character "guan" (), or "guan zihao" (). Foodservice establishments of this category mainly served different customers onsite on a walk-in basis, and in addition, a portion of the income would be earned from selling to-goes.

Foodservice establishments with names ending with the Chinese character "dian" (), or "dian zihao" (). Foodservice establishments of this category had their own place, like all previous categories, but serving different customers to dine onsite on a walk-in basis only provided half of the overall income, while the other half came from selling to-goes.

Foodservice establishments with name ending with the Chinese character "pu" (), or "pu zihao" (). Foodservice establishments of this category ranked next to the last, and they were often named after the owners' last names. Foodservice establishments of this category had fixed spots of business for having their own places, but not as large as those belonging to the category of "dian", and thus did not have tables, but only seats for customers. As a result, the bulk of the income of foodservice establishments of this category was from selling to-goes, while income earned from customers dining onsite only provided a small portion of the overall income.

Foodservice establishments with names ending with the Chinese character "tan" (), or "tan zihao" (). The lowest ranking foodservice establishments without any tables, and selling to-goes was the only form of business. In addition to name the food stand after the owners' last name or the food sold, these food stands were also often named after the owners' nicknames.

Numerous traditional restaurants in Beijing are credited with great contributions in the formation of Beijing cuisine, but many of them have gone out of business as time went by. However, some of them managed to survive until today, and some of them are:



</doc>
<doc id="19760" url="https://en.wikipedia.org/wiki?curid=19760" title="Manichaeism">
Manichaeism

Manichaeism (;
in Modern Persian "Āyin-e Māni"; ) was a major religious movement that was founded by the Iranian prophet Mani (in , Syriac: ܡܐܢܝ , Latin: "Manichaeus" or "Manes" from ; 216–276) in the Sasanian Empire.

Manichaeism taught an elaborate dualistic cosmology describing the struggle between a good, spiritual world of light, and an evil, material world of darkness. Through an ongoing process that takes place in human history, light is gradually removed from the world of matter and returned to the world of light, whence it came. Its beliefs were based on local Mesopotamian religious movements and Gnosticism.

Manichaeism was quickly successful and spread far through the Aramaic-speaking regions. It thrived between the third and seventh centuries, and at its height was one of the most widespread religions in the world. Manichaean churches and scriptures existed as far east as China and as far west as the Roman Empire. It was briefly the main rival to Christianity in the competition to replace classical paganism. Manichaeism survived longer in the east than in the west, and it appears to have finally faded away after the 14th century in south China, contemporary to the decline of the Church of the East in Ming China. While most of Manichaeism's original writings have been lost, numerous translations and fragmentary texts have survived.

An adherent of Manichaeism is called a "Manichaean" or "Manichean", or "Manichee", especially in older sources.

Mani was an Iranian born in 216 in or near Seleucia-Ctesiphon (now al-Mada'in) in the Parthian Empire. According to the "Cologne Mani-Codex", Mani's parents were members of the Jewish Christian Gnostic sect known as the Elcesaites.

Mani composed seven writings, six of which were written in the Syriac language, a late variety of Aramaic. The seventh, the "Shabuhragan", was written by Mani in Middle Persian and presented by him to the Sasanian emperor, Shapur I. Although there is no proof Shapur I was a Manichaean, he tolerated the spread of Manichaeism and refrained from persecuting it within his empire's boundaries.

According to one tradition, it was Mani himself who invented the unique version of the Syriac script known as the Manichaean alphabet, which was used in all of the Manichaean works written within the Sasanian Empire, whether they were in Syriac or Middle Persian, and also for most of the works written within the Uyghur Khaganate. The primary language of Babylon (and the administrative and cultural language of the Sassanid Empire) at that time was Eastern Middle Aramaic, which included three main dialects: Jewish Babylonian Aramaic (the language of the Babylonian Talmud), Mandaean (the language of Mandaeism), and Syriac, which was the language of Mani, as well as of the Syriac Christians.

While Manichaeism was spreading, existing religions such as Zoroastrianism were still popular and Christianity was gaining social and political influence. Although having fewer adherents, Manichaeism won the support of many high-ranking political figures. With the assistance of the Sasanian Empire, Mani began missionary expeditions. After failing to win the favour of the next generation of Persian royalty, and incurring the disapproval of the Zoroastrian clergy, Mani is reported to have died in prison awaiting execution by the Persian Emperor Bahram I. The date of his death is estimated at 276–277.

Mani believed that the teachings of Gautama Buddha, Zoroaster, and Jesus were incomplete, and that his revelations were for the entire world, calling his teachings the "Religion of Light". Manichaean writings indicate that Mani received revelations when he was 12 and again when he was 24, and over this time period he grew dissatisfied with the Elcesaite sect he was born into. Mani began preaching at an early age and was possibly influenced by contemporary Babylonian-Aramaic movements such as Mandaeism, and Aramaic translations of Jewish apocalyptic writings similar to those found at Qumran (such as the book of Enoch literature), and by the Syriac dualist-gnostic writer Bardaisan (who lived a generation before Mani). With the discovery of the Mani-Codex, it also became clear that he was raised in a Jewish-Christian baptism sect, the Elcesaites, and was influenced by their writings, as well. According to biographies preserved by Ibn al-Nadim and the Persian polymath al-Biruni, he received a revelation as a youth from a spirit, whom he would later call his Twin ( , from which is also derived the name of the Thomas the Apostle, the "twin"), his "Syzygos" ( "spouse, partner", in the "Cologne Mani-Codex"), his Double, his Protective Angel or Divine Self. It taught him truths that he developed into a religion. His divine Twin or true Self brought Mani to self-realization. He claimed to be the "Paraclete of the Truth", as promised by Jesus in the New Testament.
Manichaeism's views on Jesus are described by historians:
Historians also note that Mani declared himself to be an "apostle of Jesus Christ". Manichaean tradition is also noted to have claimed that Mani was the reincarnation of different religious figures such as Buddha, Krishna, Zoroaster, and Jesus.

Academics also note that since much of what is known about Manichaeism comes from later 10th- and 11th-century Muslim historians like Al-Biruni and especially ibn al-Nadim (and his "Fihrist"), "Islamic authors ascribed to Mani the claim to be the Seal of the Prophets." In reality, for Mani the expression "seal of prophecy" refers to his disciples, who testify for the veracity of his message, as a seal does.
Another source of Mani's scriptures was original Aramaic writings relating to the "Book of Enoch" literature (see the Book of Enoch and the Second Book of Enoch), as well as an otherwise unknown section of the Book of Enoch called "The Book of Giants". This book was quoted directly, and expanded on by Mani, becoming one of the original six Syriac writings of the Manichaean Church. Besides brief references by non-Manichaean authors through the centuries, no original sources of "The Book of Giants" (which is actually part six of the Book of Enoch) were available until the 20th century.

Scattered fragments of both the original Aramaic "Book of Giants" (which were analyzed and published by Józef Milik in 1976) and of the Manichaean version of the same name (analyzed and published by Walter Bruno Henning in 1943) were found with the discovery in the twentieth century of the Dead Sea Scrolls in the Judaean Desert and the Manichaean writings of the Uyghur Manichaean kingdom in Turpan. Henning wrote in his analysis of them:

By comparing the cosmology in the Book of Enoch literature and the Book of Giants, alongside the description of the Manichaean myth, scholars have observed that the Manichaean cosmology can be described as being based, in part, on the description of the cosmology developed in detail in the Book of Enoch literature. This literature describes the being that the prophets saw in their ascent to heaven, as a king who sits on a throne at the highest of the heavens. In the Manichaean description, this being, the "Great King of Honor", becomes a deity who guards the entrance to the world of light, placed at the seventh of ten heavens. In the Aramaic Book of Enoch, in the Qumran writings in general, and in the original Syriac section of Manichaean scriptures quoted by Theodore bar Konai, he is called "malka raba de-ikara" (the Great King of Honor).

Mani was also influenced by writings of the Assyrian gnostic Bardaisan (154–222), who, like Mani, wrote in Syriac, and presented a dualistic interpretation of the world in terms of light and darkness, in combination with elements from Christianity.
Noting Mani's travels to the Kushan Empire (several religious paintings in Bamyan are attributed to him) at the beginning of his proselytizing career, Richard Foltz postulates Buddhist influences in Manichaeism:
The Kushan monk Lokakṣema began translating Pure Land Buddhist texts into Chinese in the century prior to Mani arriving there, and the Chinese texts of Manichaeism are full of uniquely Buddhist terms taken directly from these Chinese Pure Land scriptures, including the term "pure land" (淨土 Jìngtǔ) itself. However, the central object of veneration in Pure Land Buddhism, Amitābha, the Buddha of Infinite Light, does not appear in Chinese Manichaeism, and seems to have been replaced by another deity.

Manichaeism spread with extraordinary speed through both the East and West. It reached Rome through the apostle Psattiq by 280, who was also in Egypt in 244 and 251. It was flourishing in the Faiyum in 290.

Manichaean monasteries existed in Rome in 312 during the time of Pope Miltiades.

In 291, persecution arose in the Sasanian Empire with the murder of the apostle Sisin by Emperor Bahram II, and the slaughter of many Manichaeans. In 296, Diocletian decreed against the Manichaeans: "We order that their organizers and leaders be subject to the final penalties and condemned to the fire with their abominable scriptures." This resulted in martyrdom for many in Egypt and North Africa (see Diocletian Persecution). By 354, Hilary of Poitiers wrote that Manichaeism was a significant force in Roman Gaul. In 381, Christians requested Theodosius I to strip Manichaeans of their civil rights. Starting in 382, the emperor issued a series of edicts to suppress Manichaeism and punish its followers.
Augustine of Hippo (354–430) converted to Christianity from Manichaeism in the year 387. This was shortly after the Roman emperor Theodosius I had issued a decree of death for all Manichaean monks in 382 and shortly before he declared Christianity to be the only legitimate religion for the Roman Empire in 391. Due to the heavy persecution, the religion almost disappeared from western Europe in the 5th century and from the eastern portion of the empire in the 6th century. According to his "Confessions", after nine or ten years of adhering to the Manichaean faith as a member of the group of "hearers", Augustine became a Christian and a potent adversary of Manichaeism (which he expressed in writing against his Manichaean opponent Faustus of Mileve), seeing their beliefs that knowledge was the key to salvation as too passive and not able to effect any change in one's life.

Some modern scholars have suggested that Manichaean ways of thinking influenced the development of some of Augustine's ideas, such as the nature of good and evil, the idea of hell, the separation of groups into elect, hearers, and sinners, and the hostility to the flesh and sexual activity, and his dualistic theology. These influences of Manichaeism in Augustine's Christian thinking may well have been part of the conflict between Augustine and Pelagius, a British monk whose theology, being less influenced by the Latin Church, was non-dualistic, and one that saw the created order, and mankind in particular, as having a Divine core, rather than a 'darkness' at its core.
How Manichaeism might have influenced Christianity continues to be debated. Manichaeism could have influenced the Bogomils, Paulicians, and Cathars. However, these groups left few records, and the link between them and Manichaeans is tenuous. Regardless of its accuracy, the charge of Manichaeism was leveled at them by contemporary orthodox opponents, who often tried to make contemporary heresies conform to those combatted by the church fathers. Whether the dualism of the Paulicians, Bogomils, and Cathars and their belief that the world was created by a Satanic demiurge were due to influence from Manichaeism is impossible to determine. The Cathars apparently adopted the Manichaean principles of church organization. Priscillian and his followers may also have been influenced by Manichaeism. The Manichaeans preserved many apocryphal Christian works, such as the Acts of Thomas, that would otherwise have been lost.

Manichaeism maintained a sporadic and intermittent existence in the west (Mesopotamia, Africa, Spain, France, North Italy, the Balkans) for a thousand years, and flourished for a time in Persia and even further east in Northern India, Western China, and Tibet. While it had long been thought that Manichaeism arrived in China only at the end of the 7th century, a recent archaeological discovery demonstrated that it was already known there in the second half of the 6th century.
Some Sogdians in Central Asia believed in the religion. Uyghur khagan Boku Tekin (759–780) converted to the religion in 763 after a 3 days discussion with its preachers, the Babylonian headquarters sent high rank clerics to Uyghur, and Manichaeism remained the state religion for about a century before the collapse of the Uyghur Khaganate in 840. In the east it spread along trade routes as far as Chang'an, the capital of Tang China. After the Tang Dynasty, some Manichaens groups participated in peasant movements. The religion was used by many rebel leaders to mobilise followers. In the Song and Yuan dynasties of China remnants of Manichaeism continued to leave a legacy contributing to sects such as the Red Turbans. During the Song Dynasty, the Manichaeans were derogatorily referred by the Chinese as "chicai simo" (meaning that they "abstain from meat and worship demons"). An account in "Fozu Tongji", an important historiography of Buddhism in China compiled by Buddhist scholars during 1258-1269, says that the Manichaens worshipped the "white Buddha" and their leader wore a violet headgear, while the followers wore white costumes. Many Manichaeans took part in rebellions against the Song government and were eventually quelled. After that, all governments were suppressive against Manichaeism and its followers and the religion was banned by the Ming Dynasty in 1370.

The Manichaeans tried to assimilate their religion along with Islam in the Muslim caliphates. Relatively little is known about the religion during the first century of Islamic rule. During the early caliphates, Manichaeism attracted many followers. It had a significant appeal among the Muslim society, especially among the elites. Due to the appeal of its teachings, many Muslims adopted the ideas of its theology and some even became dualists. An apologia for Manichaeism ascribed to ibn al-Muqaffa' defended its phantasmagorical cosmogony and attacked the fideism of Islam and other monotheistic religions. According to some accounts, even the Umayyad caliph al-Walid II was a follower of Mani. The Manichaeans had sufficient structure to have a head of their community.

Under the eighth-century Abbasid Caliphate, Arabic "zindīq" and the adjectival term "zandaqa" could denote many different things, though it seems primarily (or at least initially) to have signified a follower of Manichaeism however its true meaning is not known. In the ninth century, it is reported that Caliph al-Ma'mun tolerated a community of Manichaeans.

During the early Abbasid period, the Manichaeans underwent persecution. The third Abbasid caliph, al-Mahdi, persecuted the Manichaeans, establishing an inquisition against dualists who if being found guilty of heresy refused to renounce their beliefs, were executed. Their persecution was finally ended in 780s by Harun al-Rashid. During the reign of the Caliph al-Muqtadir, many Manichaeans fled from Mesopotamia to Khorasan from fear of persecution and the base of the religion was later shifted to Samarkand.

Manichaeism claimed to present the complete version of teachings that were corrupted and misinterpreted by the followers of its predecessors Adam, Zoroaster, Buddha and Jesus. Accordingly, as it spread, it adapted new deities from other religions into forms it could use for its scriptures. Its original Aramaic texts already contained stories of Jesus. When they moved eastward and were translated into Iranian languages, the names of the Manichaean deities (or angels) were often transformed into the names of Zoroastrian yazatas. Thus "Abbā dəRabbūṯā" ("The Father of Greatness", the highest Manichaean deity of Light), in Middle Persian texts might either be translated literally as "pīd ī wuzurgīh", or substituted with the name of the deity "Zurwān". Similarly, the Manichaean primal figure "Nāšā Qaḏmāyā" "The Original Man" was rendered "Ohrmazd Bay", after the Zoroastrian god Ohrmazd. This process continued in Manichaeism's meeting with Chinese Buddhism, where, for example, the original Aramaic "qaryā" (the "call" from the World of Light to those seeking rescue from the World of Darkness), becomes identified in the Chinese scriptures with Guanyin ( or Avalokiteśvara in Sanskrit, literally, "watching/perceiving sounds [of the world]", the bodhisattva of Compassion).

Manichaeism was repressed by the Sasanian Empire. In 291, persecution arose in the Persian empire with the murder of the apostle Sisin by Bahram II, and the slaughter of many Manichaeans. In 296, the Roman emperor Diocletian decreed all the Manichaean leaders to be burnt alive along with the Manichaean scriptures and many Manichaeans in Europe and North Africa were killed. This policy of persecution was also followed by his successors. Theodosius I issued a decree of death for all Manichaean monks in 382 AD. The religion was vigorously attacked and persecuted by both the Christian Church and the Roman state. Augustine of Hippo, one of the early Doctors of the Catholic Church was a Manichaean until his conversion to Christianity in 386. He was never persecuted for this and he freely converted. Due to the heavy persecution upon its followers in the Roman Empire, the religion almost disappeared from western Europe in the 5th century and from the eastern portion of the empire in the sixth century.

In 732, Emperor Xuanzong of Tang banned any Chinese from converting to the religion, saying it was a heretic religion that was confusing people by claiming to be Buddhism. However the foreigners who followed the religion were allowed to practice it without punishment. After the fall of the Uyghur Khaganate in 840, which was the chief patron of Manichaeism (which was also the state religion of the Khaganate) in China, all Manichaean temples in China except in the two capitals and Taiyuan were closed down and never reopened since these temples were viewed as a symbol of foreign arrogance by the Chinese (see Cao'an.) Even those that were allowed to remain open did not for long. The Manichaean temples were attacked by Chinese people who burned the images and idols of these temples. Manichaean priests were ordered to wear hanfu instead of their traditional clothing, which was viewed as un-Chinese. In 843, Emperor Wuzong of Tang gave the order to kill all Manichaean clerics as part of his Great Anti-Buddhist Persecution, and over half died. They were made to look like Buddhists by the authorities, their heads were shaved, they were made to dress like Buddhist monks and then killed. Although the religion was mostly forbidden and its followers persecuted thereafter in China, it survived till the 14th century in the country. Under the Song dynasty, its followers were derogatorily referred to with the chengyu () "vegetarian demon-worshippers".

Many Manichaeans took part in rebellions against the Song dynasty. They were quelled by Song China and were suppressed and persecuted by all successive governments before the Mongol Yuan dynasty. In 1370, the religion was banned through an edict of the Ming dynasty, whose Hongwu Emperor had a personal dislike for the religion. Its core teaching influences many religious sects in China, including the White Lotus movement.

According to Wendy Doniger, Manichaeism may have continued to exist in the in the modern-East Turkestan region until the Mongol conquest in the 13th century.

Manicheans also suffered persecution for some time under the Abbasid Caliphate of Baghdad. In 780, the third Abbasid Caliph, al-Mahdi, started a campaign of inquisition against those who were "dualist heretics" or "Manichaeans" called the "zindīq". He appointed a "master of the heretics" ( "ṣāhib al-zanādiqa"), an official whose task was to pursue and investigate suspected dualists, who were then examined by the Caliph. Those found guilty who refused to abjure their beliefs were executed. This persecution continued under his successor, Caliph al-Hadi, and continued for some time during reign of Harun al-Rashid, who finally abolished it and ended it. During the reign of the 18th Abbassid Caliph al-Muqtadir, many Manichaeans fled from Mesopotamia to Khorasan from fear of persecution by him and about 500 of them assembled in Samarkand. The base of the religion was later shifted to this city, which became their new Patriarchate.

During the Middle Ages, several movements emerged that were collectively described as "Manichaean" by the Catholic Church, and persecuted as Christian heresies through the establishment, in 1184, of the Inquisition. They included the Cathar churches of Western Europe. Other groups sometimes referred to as "neo-Manichaean" were the Paulician movement, which arose in Armenia, and the Bogomils in Bulgaria. An example of this usage can be found in the published edition of the Latin Cathar text, the "Liber de duobus principiis" ("Book of the Two Principles"), which was described as "Neo-Manichaean" by its publishers. As there is no presence of Manichaean mythology or church terminology in the writings of these groups, there has been some dispute among historians as to whether these groups were descendants of Manichaeism.

Some sites are preserved in Xinjiang and Fujian in China. The Cao'an temple is the only fully intact Manichaean building, though it later became associated with Buddhism. Several small groups claim to continue to practice this faith.

Mani's teaching dealt with the origin of evil, by addressing a theoretical part of the problem of evil by denying the omnipotence of God and postulating two opposite powers. Manichaean theology taught a dualistic view of good and evil. A key belief in Manichaeism is that the powerful, though not omnipotent good power (God), was opposed by the semi-eternal evil power (Satan). Humanity, the world and the soul are seen as the byproduct of the battle between God's proxy, Primal Man, and Satan. The human person is seen as a battleground for these powers: the soul defines the person, but it is under the influence of both light and dark. This contention plays out over the world as well as the human body—neither the Earth nor the flesh were seen as intrinsically evil, but rather possessed portions of both light and dark. Natural phenomena (such as rain) were seen as the physical manifestation of this spiritual contention. Therefore, the Manichaean worldview explained the existence of evil by positing a flawed creation in the formation of which God took no part and which constituted rather the product of a rebellion by Satan against God.

Manichaeism presented an elaborate description of the conflict between the spiritual world of light and the material world of darkness. The beings of both the world of darkness and the world of light have names. There are numerous sources for the details of the Manichaean belief. There are two portions of Manichaean scriptures that are probably the closest thing to the original Manichaean writings in their original languages that will ever be available. These are the Syriac-Aramaic quotation by the Nestorian Christian Theodore bar Konai, in his Syriac "Book of Scholia" ("Ketba de-Skolion"z, 8th century), and the Middle Persian sections of Mani's Shabuhragan discovered at Turpan (a summary of Mani's teachings prepared for Shapur I). These two sections are probably the original Syriac and Middle Persian written by Mani.

From these and other sources, it is possible to derive an almost complete description of the detailed Manichaean vision (a complete list of Manichaean deities is outlined below). According to Mani, the unfolding of the universe takes place with three "creations":

The First Creation: Originally, good and evil existed in two completely separate realms, one the "World of Light", ruled by the "Father of Greatness" together with his five "Shekhinas" (divine attributes of light), and the other the "World of Darkness", ruled by the "King of Darkness". At a certain point, the "Kingdom of Darkness" notices the "World of Light", becomes greedy for it and attacks it. The "Father of Greatness", in the first of three "creations" (or "calls"), calls to the "Mother of Life", who sends her son "Original Man" ("Nāšā Qaḏmāyā" in Aramaic), to battle with the attacking powers of Darkness, which include the "Demon of Greed". The "Original Man" is armed with five different shields of light (reflections of the five "Shekhinas"), which he loses to the forces of darkness in the ensuing battle, described as a kind of "bait" to trick the forces of darkness, as the forces of darkness greedily consume as much light as they can. When the "Original Man" comes to, he is trapped among the forces of darkness.

The Second Creation: Then the "Father of Greatness" begins the "Second Creation", calling to the "Living Spirit", who calls to his five sons, and sends a call to the "Original Man" ("Call" then becomes a Manichaean deity). An answer ("Answer" becomes another Manichaean deity) then returns from the "Original Man" to the "World of Light". The "Mother of Life", the "Living Spirit", and his five sons begin to create the universe from the bodies of the evil beings of the "World of Darkness", together with the light that they have swallowed. Ten heavens and eight earths are created, all consisting of various mixtures of the evil material beings from the "World of Darkness" and the swallowed light. The sun, moon, and stars are all created from light recovered from the "World of Darkness". The waxing and waning of the moon is described as the moon filling with light, which passes to the sun, then through the Milky Way, and eventually back to the "World of Light".
The Third Creation: Great demons (called "archons" in bar-Khonai's account) are hung out over the heavens, and then the "Father of Greatness" begins the "Third Creation". Light is recovered from out of the material bodies of the male and female evil beings and demons, by causing them to become sexually aroused in greed, towards beautiful images of the beings of light, such as the "Third Messenger" and the "Virgins of Light". However, as soon as the light is expelled from their bodies and falls to the earth (some in the form of abortions – the source of fallen angels in the Manichaean myth), the evil beings continue to swallow up as much of it as they can to keep the light inside of them. This results eventually in the evil beings swallowing huge quantities of light, copulating, and producing Adam and Eve. The "Father of Greatness" then sends the "Radiant Jesus" to awaken Adam, and to enlighten him to the true source of the light that is trapped in his material body. Adam and Eve, however, eventually copulate, and produce more human beings, trapping the light in bodies of mankind throughout human history. The appearance of the Prophet Mani was another attempt by the "World of Light" to reveal to mankind the true source of the spiritual light imprisoned within their material bodies.

Beginning with the time of its creation by Mani, the Manichaean religion had a detailed description of deities and events that took place within the Manichaean scheme of the universe. In every language and region that Manichaeism spread to, these same deities reappear, whether it is in the original Syriac quoted by Theodore bar Konai, or the Latin terminology given by Saint Augustine from Mani's "Epistola Fundamenti", or the Persian and Chinese translations found as Manichaeism spread eastward. While the original Syriac retained the original description that Mani created, the transformation of the deities through other languages and cultures produced incarnations of the deities not implied in the original Syriac writings. This process began in Mani's lifetime, with "The Father of Greatness", for example, being translated into Middle Persian as Zurvan, a Zoroastrian supreme being.







The Manichaean Church was divided into the Elect, who had taken upon themselves the vows of Manicheaism, and the Hearers, those who had not, but still participated in the Church. The terms for these divisions were already common since the days of early Christianity. In Chinese writings, the Middle Persian and Parthian terms are transcribed phonetically (instead of being translated into Chinese). These were recorded by Augustine of Hippo.

The most important religious observance of the Manichaeans was the Bema Fest, observed annually:
The Bema was originally, in the Syriac Christian churches, a seat placed in the middle of the nave on which the bishop would preside and from which the Gospel would be read. In the Manichaean places of worship, the throne was a five-stepped altar, covered by precious cloths, symbolizing the five classes of the hierarchy. The top of the Bema was always empty, as it was the seat of Mani. The Bema was celebrated at the vernal equinox, was preceded by fasts, and symbolized the passion of Mani, thus it was strictly parallel to the Christian Easter.

While it is often presumed that the Bema seat was empty, there is some evidence from the Coptic Manichaean "Bema Psalms", that the Bema seat may have actually contained a copy of Mani's picture book, the Arzhang.

Mani wrote either seven or eight books, which contained the teachings of the religion. Only scattered fragments and translations of the originals remain.

The original six Syriac writings are not preserved, although their Syriac names have been. There are also fragments and quotations from them. A long quotation, preserved by the eighth-century Nestorian Christian author Theodore Bar Konai, shows that in the original Syriac Aramaic writings of Mani there was no influence of Iranian or Zoroastrian terms. The terms for the Manichaean deities in the original Syriac writings are in Aramaic. The adaptation of Manichaeism to the Zoroastrian religion appears to have begun in Mani's lifetime however, with his writing of the Middle Persian "Shabuhragan", his book dedicated to the Sasanian emperor, Shapur I. In it, there are mentions of Zoroastrian divinities such as Ahura Mazda, Angra Mainyu, and Āz. Manichaeism is often presented as a Persian religion, mostly due to the vast number of Middle Persian, Parthian, and Sogdian (as well as Turkish) texts discovered by German researchers near Turpan in what is now Xinjiang, China, during the early 1900s. However, from the vantage point of its original Syriac descriptions (as quoted by Theodore Bar Khonai and outlined above), Manichaeism may be better described as a unique phenomenon of Aramaic Babylonia, occurring in proximity to two other new Aramaic religious phenomena, Talmudic Judaism and Mandaeism, which also appeared in Babylonia in roughly the third century.

The original, but now lost, six sacred books of Manichaeism were composed in Syriac Aramaic, and translated into other languages to help spread the religion. As they spread to the east, the Manichaean writings passed through Middle Persian, Parthian, Sogdian, Tocharian, and ultimately Uyghur and Chinese translations. As they spread to the west, they were translated into Greek, Coptic, and Latin.
Henning describes how this translation process evolved and influenced the Manichaeans of Central Asia:




In later centuries, as Manichaeism passed through eastern Persian-speaking lands and arrived at the Uyghur Khaganate (回鶻帝國), and eventually the Uyghur kingdom of Turpan (destroyed around 1335), Middle Persian and Parthian prayers ("āfrīwan" or "āfurišn") and the Parthian hymn-cycles (the "Huwīdagmān" and "Angad Rōšnan" created by Mar Ammo) were added to the Manichaean writings. A translation of a collection of these produced the "Manichaean Chinese Hymnscroll" (, which Lieu translates as "Hymns for the Lower Section [i.e. the Hearers] of the Manichaean Religion"). In addition to containing hymns attributed to Mani, it contains prayers attributed to Mani's earliest disciples, including Mār Zaku, Mār Ammo and Mār Sīsin. Another Chinese work is a complete translation of the "Sermon of the Light Nous", presented as a discussion between Mani and his disciple Adda.

Until discoveries in the 1900s of original sources, the only sources for Manichaeism were descriptions and quotations from non-Manichaean authors, either Christian, Muslim, Buddhist, or Zoroastrian. While often criticizing Manichaeism, they also quoted directly from Manichaean scriptures. This enabled Isaac de Beausobre, writing in the 18th century, to create a comprehensive work on Manichaeism, relying solely on anti-Manichaean sources. Thus quotations and descriptions in Greek and Arabic have long been known to scholars, as have the long quotations in Latin by Saint Augustine, and the extremely important quotation in Syriac by Theodore Bar Konai.

Eusebius commented as follows:
An example of how inaccurate some of these accounts could be is seen in the account of the origins of Manichaeism contained in the "Acta Archelai". This was a Greek anti-manichaean work written before 348, most well known in its Latin version, which was regarded as an accurate account of Manichaeism until refuted by Isaac de Beausobre in the 18th century:

In the time of the Apostles there lived a man named Scythianus, who is described as coming "from Scythia", and also as being "a Saracen by race" ("ex genere Saracenorum"). He settled in Egypt, where he became acquainted with "the wisdom of the Egyptians", and invented the religious system that was afterwards known as Manichaeism. Finally he emigrated to Palestine, and, when he died, his writings passed into the hands of his sole disciple, a certain Terebinthus. The latter betook himself to Babylonia, assumed the name of Budda, and endeavoured to propagate his master's teaching. But he, like Scythianus, gained only one disciple, who was an old woman. After a while he died, in consequence of a fall from the roof of a house, and the books that he had inherited from Scythianus became the property of the old woman, who, on her death, bequeathed them to a young man named Corbicius, who had been her slave. Corbicius thereupon changed his name to Manes, studied the writings of Scythianus, and began to teach the doctrines that they contained, with many additions of his own. He gained three disciples, named Thomas, Addas, and Hermas. About this time the son of the Persian king fell ill, and Manes undertook to cure him; the prince, however, died, whereupon Manes was thrown into prison. He succeeded in escaping, but eventually fell into the hands of the king, by whose order he was flayed, and his corpse was hung up at the city gate.

A. A. Bevan, who quoted this story, commented that it "has no claim to be considered historical".

According to Hegemonius' portrayal of Mani, the evil demiurge who created the world was the Jewish Jehovah. Hegemonius reports that Mani said,
In the early 1900s, original Manichaean writings started to come to light when German scholars led by Albert Grünwedel, and then by Albert von Le Coq, began excavating at Gaochang, the ancient site of the Manichaean Uyghur Kingdom near Turpan, in Chinese Turkestan (destroyed around AD 1300). While most of the writings they uncovered were in very poor condition, there were still hundreds of pages of Manichaean scriptures, written in three Iranian languages (Middle Persian, Parthian, and Sogdian) and old Uyghur. These writings were taken back to Germany, and were analyzed and published at the Preußische Akademie der Wissenschaften in Berlin, by Le Coq and others, such as Friedrich W. K. Müller and Walter Bruno Henning. While the vast majority of these writings were written in a version of the Syriac script known as Manichaean script, the German researchers, perhaps for lack of suitable fonts, published most of them using the Hebrew alphabet (which could easily be substituted for the 22 Syriac letters).

Perhaps the most comprehensive of these publications was "Manichaeische Dogmatik aus chinesischen und iranischen Texten" ("Manichaean Dogma from Chinese and Iranian texts"), by Ernst Waldschmidt and Wolfgang Lentz, published in Berlin in 1933. More than any other research work published before or since, this work printed, and then discussed, the original key Manichaean texts in the original scripts, and consists chiefly of sections from Chinese texts, and Middle Persian and Parthian texts transcribed with the Hebrew alphabet. After the Nazi party gained power in Germany, the Manichaean writings continued to be published during the 1930s, but the publishers no longer used Hebrew letters, instead transliterating the texts into Latin letters.

Additionally, in 1930, German researchers in Egypt found a large body of Manichaean works in Coptic. Though these were also damaged, hundreds of complete pages survived and, beginning in 1933, were analyzed and published in Berlin before World War II, by German scholars such as Hans Jakob Polotsky. Some of these Coptic Manichaean writings were lost during the war.

After the success of the German researchers, French scholars visited China and discovered what is perhaps the most complete set of Manichaean writings, written in Chinese. These three Chinese writings, all found at the Caves of the Thousand Buddhas among the Dunhuang manuscripts, and all written before the 9th century, are today kept in London, Paris, and Beijing. Some of the scholars involved with their initial discovery and publication were Édouard Chavannes, Paul Pelliot, and Aurel Stein. The original studies and analyses of these writings, along with their translations, first appeared in French, English, and German, before and after World War II. The complete Chinese texts themselves were first published in Tokyo, Japan in 1927, in the Taisho Tripitaka, volume 54. While in the last thirty years or so they have been republished in both Germany (with a complete translation into German, alongside the 1927 Japanese edition), and China, the Japanese publication remains the standard reference for the Chinese texts.

In Egypt, a small codex was found and became known through antique dealers in Cairo. It was purchased by the University of Cologne in 1969. Two of its scientists, Henrichs and Koenen, produced the first edition known since as the Cologne Mani-Codex, which was published in four articles in the "Zeitschrift für Papyrologie und Epigraphik". The ancient papyrus manuscript contained a Greek text describing the life of Mani. Thanks to this discovery, much more is known about the man who founded one of the most influential world religions of the past.

The terms "Manichaean" and "Manichaeism" are sometimes used figuratively as a synonym of the more general term "dualist" with respect to a philosophy, outlook or worldview. The terms are often used to suggest that the world view in question simplistically reduces the world to a struggle between good and evil. For example, Zbigniew Brzezinski used the phrase "Manichaean paranoia" in reference to U.S. President George W. Bush's world view (in "The Daily Show with Jon Stewart", March 14, 2007); Brzezinski elaborated that he meant "the notion that he [Bush] is leading the forces of good against the empire of evil". Philosopher Frantz Fanon frequently invoked the concept of Manicheanism in his discussions of violence between colonizers and the colonized. Saint Augustine of Hippo was deeply influenced by Manicheanism prior to his conversion to Christianity.

Author and journalist Glenn Greenwald followed up on the theme in describing Bush in his book "A Tragic Legacy" (2007).

In "The Man Who Shot Snapping Turtles" ("Memoirs of Hecate County"), Edmund Wilson's narrator refers to Asa Stryker's argument as "the Manichaean heresy".

The attitudes and foreign policies of the present-day United States and its leaders have been described as reflecting a Manichaean worldview, though this is a criticism easily applied to any country or culture that propagandizes their own intrinsic good and their enemy's intrinsic evil.








</doc>
<doc id="19761" url="https://en.wikipedia.org/wiki?curid=19761" title="Moroccan cuisine">
Moroccan cuisine

Moroccan cuisine is influenced by Morocco's interactions and exchanges with other cultures and nations over the centuries. Moroccan cuisine is typically a mix of Arabic, Andalusian, and Mediterranean cuisines with a slight European and Subsaharan influence.

Morocco produces a large range of Mediterranean fruits, and vegetables and even some tropical ones. Common meats include beef, goat, mutton and lamb, chicken and seafood, which serve as a base for the cuisine. Characteristic flavorings include lemon pickle, argan oil, cold-pressed, unrefined olive oil and dried fruits. As in Mediterranean cuisine in general, the staple ingredients include wheat, used for bread and couscous, and olive oil; the third Mediterranean staple, the grape, is eaten as a dessert, though a certain amount of wine is made in the country.

Spices are used extensively in Moroccan food. Although some spices have been imported to Morocco through the Arabs for thousands of years, many ingredients—like saffron from Talaouine, mint and olives from Meknes, and oranges and lemons from Fes—are home-grown, and are being exported internationally. Common spices include cinnamon, cumin, turmeric, ginger, paprika, coriander, saffron, mace, cloves, fennel, anise, nutmeg, cayenne pepper, fenugreek, caraway, black pepper and sesame seeds. Twenty-seven spices are combined for the famous Moroccan spice mixture "ras el hanout".

Common herbs in Moroccan cuisine include mint, parsley, coriander, oregano, peppermint, marjoram, verbena, sage and bay laurel.

A typical lunch meal begins with a series of hot and cold salads, followed by a tagine or Dwaz. Often, for a formal meal, a lamb or chicken dish is next, or couscous topped with meat and vegetables. Moroccans either eat with fork, knife and spoon or with their hands using bread as a utensil depending on the dish served. The consumption of pork and alcohol is uncommon due to religious restrictions.

The main Moroccan dish most people are familiar with is couscous, the old national delicacy. Beef is the most commonly eaten red meat in Morocco, usually eaten in a tagine with a wide selection of vegetables. Chicken is also very commonly used in tagines, or roasted.

Lamb is also heavily consumed, and since Moroccan sheep breeds store most of their fat in their tails, Moroccan lamb does not have the pungent flavour that Western lamb and mutton have.

Since Morocco lies on two coasts, the Atlantic and the Mediterranean, Moroccan cuisine has ample seafood dishes. European pilchard is caught in large but declining quantities. Other fish species include mackerel, anchovy, sardinella, and horse mackerel.

Other famous Moroccan dishes are Pastilla (also spelled Basteeya or Bestilla), Tanjia and Harira, a typical heavy soup, eaten during winter to warm up and is usually served for dinner, it is typical eaten with plain bread or with dates during the month of Ramadan. Bissara is a broad bean-based soup that is also consumed during the colder months of the year.

A big part of the daily meal is bread. Bread in Morocco is principally made from durum wheat semolina known as khobz. Bakeries are very common throughout Morocco and fresh bread is a staple in every city, town and village. The most common is whole grain coarse ground or white flour bread or baguettes. There are also a number of flat breads and pulled unleavened pan-fried breads.

In addition, there are dried salted meats and salted preserved meats such as khlea and g'did (basically sheep bacon), which are used to flavor tagines or used in "el rghaif", a folded savory Moroccan pancake.

Salads include both raw and cooked vegetables, served either hot or cold. Cold salads include "zaalouk," an aubergine and tomato mixture, and taktouka (a mixture of tomatoes, smoked green peppers, garlic and spices) characteristic of the cities of Taza and Fes, in the Atlas.

Usually, seasonal fruits rather than cooked desserts are served at the close of a meal. A common dessert is "kaab el ghzal" ("gazelle's horns"), a pastry stuffed with almond paste and topped with sugar. Another is "Halwa chebakia", pretzel-shaped dough deep-fried, soaked in honey and sprinkled with sesame seeds; it is eaten during the month of Ramadan. Coconut fudge cakes, 'Zucre Coco', are popular also.

Morocco is endowed with over 3000 km of coastline. There is an abundance of fish in these coastal waters with the sardine being commercially significant as Morocco is the world's largest exporter. At Moroccan fish markets one can find sole, swordfish, tuna, tarbot, mackerel, shrimp, congre eel, skate, red snapper, spider crab, lobster and a variety of mollusks.
The most popular drink is green tea with mint. Traditionally, making good mint tea in Morocco is considered an art form and the drinking of it with friends and family is often a daily tradition. The pouring technique is as crucial as the quality of the tea itself. Moroccan tea pots have long, curved pouring spouts and this allows the tea to be poured evenly into tiny glasses from a height. For the best taste, glasses are filled in two stages. The Moroccans traditionally like tea with bubbles, so while pouring they hold the teapot high above the glasses. Finally, the tea is accompanied with hard sugar cones or lumps. Morocco has an abundance of oranges and tangerines, so fresh orange juice is easily found freshly squeezed and is cheap.

Selling fast food in the street has long been a tradition, and the best example is Djemaa el Fna square in Marrakech. Starting in the 1980s, new snack restaurants started serving "Bocadillo" (a Spanish word for a sandwich). Though the composition of a bocadillo varies by region, it is usually a baguette filled with salad and a choice of meats, Mozarella, fish (usually tuna), or omelette.

Dairy product shops locally called Mhlaba, are very prevalent all around the country. Those dairy stores generally offer all types of dairy products, juices, and local delicacies such as (Bocadillos, Msemen and Harcha).

In the late 1990s, several multinational fast-food franchises opened restaurants in major cities.




</doc>
<doc id="19763" url="https://en.wikipedia.org/wiki?curid=19763" title="Martin Van Buren">
Martin Van Buren

Martin Van Buren (; December 5, 1782 – July 24, 1862) was an American statesman who served as the eighth President of the United States from 1837 to 1841. A founder of the Democratic Party, he previously served as the ninth Governor of New York, the tenth U.S. Secretary of State and the eighth Vice President of the United States. Van Buren won the 1836 presidential election because of the endorsement of popular outgoing President Andrew Jackson and the organizational strength of the Democratic Party. He lost his 1840 reelection bid to Whig Party nominee William Henry Harrison due in part to the poor economic conditions of the Panic of 1837. Later in his life, Van Buren emerged as an important anti-slavery leader and led the Free Soil ticket in the 1848 presidential election.

Van Buren was born in Kinderhook, New York, to a family of Dutch Americans. He was raised speaking Dutch and learned English at school, making him the only U.S. President who spoke English as a second language. Van Buren trained as a lawyer and quickly became involved in politics as a member of the Democratic-Republican Party. He won election to the New York State Senate and became the leader of the Bucktails, the faction of Democratic-Republicans opposed to Governor DeWitt Clinton. Van Buren established a political machine known as the Albany Regency and emerged as the most influential politician in his home state in the 1820s. He was elected to the United States Senate in 1821, and supported William H. Crawford in the 1824 presidential election. After John Quincy Adams won the 1824 election, Van Buren opposed Adams's proposals for federally-funded internal improvements and other measures. Van Buren's major political goal was to re-establish a two-party system with partisan differences based on ideology rather than personalities or sectional differences. With this goal in mind, he supported Jackson's candidacy against Adams in the 1828 presidential election. To support Jackson's candidacy, Van Buren ran for Governor of New York. After Jackson took office in 1829, Van Buren resigned as governor to accept appointment as secretary of state.

During Jackson's eight years as president, Van Buren was a key advisor, and built the organizational structure for the coalescing Democratic Party, particularly in New York. After resigning from his position in order to help resolve the Petticoat affair, Van Buren briefly served as the American ambassador to Britain. At Jackson's behest, the 1832 Democratic National Convention nominated Van Buren for vice president, and Van Buren took office after the Democratic ticket won the 1832 presidential election. With Jackson's strong support, Van Buren faced little opposition for the presidential nomination at the 1835 Democratic National Convention, and he defeated several Whig opponents in the 1836 presidential election. Van Buren's response to the Panic of 1837 centered on his Independent Treasury system, a plan under which the federal government would store its funds in vaults rather than in banks. He also continued Jackson's policy of Indian removal. In foreign affairs, he maintained peaceful relations with Britain and, seeking to avoid heightened sectional tensions, denied the application of Texas for admission to the Union. In the 1840 election, the Whigs rallied around Harrison's military record and ridiculed Van Buren as "Martin Van Ruin," and a surge of new voters helped turn Van Buren out of office.

At the opening of the Democratic convention in 1844, Van Buren was the leading candidate for the party's nomination for President, but his continued opposition to the annexation of Texas aroused the opposition of Southern Democrats and the party nominated James K. Polk. Van Buren grew increasingly opposed to slavery after he left office, and, motivated additionally by intra-party differences at the state and national level, he agreed to lead a third party ticket in the 1848 presidential election. Though he finished in a distant third nationally, Van Buren's presence in the race most likely helped Whig nominee Zachary Taylor defeat Democrat Lewis Cass. Van Buren returned to the Democratic fold after the 1848 election, but he supported Abraham Lincoln's policies during the American Civil War. Van Buren's health began to fail in 1861, and he died in July 1862 at age 79. He has been generally ranked as a slightly-below-average U.S. President by historians and political scientists.

Van Buren was born on December 5, 1782, in the village of Kinderhook, New York, about south of Albany on the Hudson River. He was the first U.S. President not born a British subject, nor of British ancestry. His father, Abraham Van Buren, was a descendant of Cornelis Maessen of the village of Buurmalsen, Netherlands, who had come to North America in 1631 and purchased a plot of land on Manhattan Island. Abraham Van Buren had been a Patriot during the American Revolution, and he later joined the Democratic-Republican Party. He owned and operated an inn and tavern in Kinderhook and served as Kinderhook's town clerk for several years. In 1776, he married Maria Hoes (or Goes) Van Alen, the widow of Johannes Van Alen. Like Abraham, Maria was of Dutch extraction. She had had three children with her first husband, including future Congressman James I. Van Alen. Her second marriage produced five children, including Martin. Uniquely among U.S. presidents, Van Buren spoke English as a second language, with his primary language in his youth being Dutch.

Van Buren received a basic education at the village schoolhouse and briefly studied Latin at the Kinderhook Academy and at Washington Seminary in Claverack. His formal education ended in 1796, when he began reading law at the office of Peter Silvester and his son Francis, prominent Federalist attorneys in Kinderhook. At his father's inn, Van Buren learned early to interact with people from varied ethnic, income, and societal groups, which he would later use to his advantage as a political organizer. Van Buren was small in stature; as an adult he was 5 feet 6 inches tall, and was often referred to as "Little Van." When he first began his legal studies, he often presented an unkempt appearance in rough, homespun clothing. The Silvesters admonished Van Buren about how crucial a lawyer's clothing and personal appearance were to the success of his practice. He accepted their advice and patterned his clothing, appearance, bearing and conduct after theirs.

Despite his association with the Silvesters and Kinderhook's strong affiliation with the Federalist Party, Van Buren adopted the Democratic-Republican political leanings of his father. After six years under the Silvesters, the elder Silvester and Democratic-Republican political figure John Peter Van Ness suggested that Van Buren's political leanings constrained him to complete his education with a Democratic-Republican attorney. Accepting this advice, he spent a final year of apprenticeship in the New York City office of John Van Ness's brother William P. Van Ness, a political lieutenant of Aaron Burr. Van Ness introduced Van Buren to the intricacies of New York state politics, and Van Buren observed Burr's battles for control of the state Democratic-Republican party against George Clinton and Robert R. Livingston. In 1803, after being admitted to the New York bar, Van Buren returned to Kinderhook.

Van Buren married Hannah Hoes, his childhood sweetheart and a daughter of his first cousin, on February 21, 1807, in Catskill, New York. Like Van Buren, she was raised in a Dutch home; she spoke primarily Dutch, and spoke English with a distinct accent. The couple had five children, four of whom lived to adulthood: Abraham (1807–1873), John (1810–1866), Martin Jr. (1812–1855), Winfield Scott (born and died in 1814), and Smith Thompson (1817–1876). Hannah Van Buren contracted tuberculosis and died on February 5, 1819, at the age of 35. Martin Van Buren never remarried.

Upon returning to Kinderhook in 1803, Van Buren formed a law partnership with his half-brother, James Van Alen, and became financially secure enough to increase his focus on politics. Van Buren had been active in politics from age 18 if not before; he had attended a Democratic-Republican Party convention in Troy, New York where he worked successfully to secure for John Peter Van Ness the party nomination in a special election for the 6th Congressional District seat. Upon returning to Kinderhook, Van Buren broke with the Burr faction, becoming an ally of both DeWitt Clinton and Daniel D. Tompkins. After the faction led by Clinton and Tompkins dominated the 1807 elections, Van Buren was appointed Surrogate of Columbia County, New York. Seeking to find a better base for his political and legal career, Van Buren and his family moved to the town of Hudson, the seat of Columbia County, in 1808. Van Buren's legal practice continued to flourish, and he traveled all over the state to represent various clients.

In 1812, Van Buren won his party's nomination for a seat in the New York State Senate. Though several Democratic-Republicans, including John Peter Van Ness, joined with the Federalists to oppose his candidacy, Van Buren won election to the state senate in mid-1812. Later in the year, the United States entered the War of 1812 against Great Britain, while Clinton launched an unsuccessful bid to defeat President James Madison in the 1812 presidential election. After the election, Van Buren became suspicious that Clinton was working with the Federalist Party, and he broke from his former political ally.

During the War of 1812, Van Buren worked with Clinton, Governor Tompkins, and Ambrose Spencer to support the Madison administration's prosecution of the war. In addition, he was a special judge advocate appointed to serve as a prosecutor of William Hull during Hull's court-martial following the surrender of Detroit. In the winter of 1814–15 he collaborated with Winfield Scott on ways to reorganize the New York Militia in anticipation of another military campaign, but their work was halted by the end of the war in early 1815. Van Buren was so favorably impressed by Scott that he named his fourth son after him. Van Buren's strong support for the war boosted his standing, and in 1815 he was elected to the position of New York Attorney General. Van Buren moved from Hudson to the state capital of Albany, where he established a legal partnership with Benjamin Butler, and shared a house with political ally Roger Skinner. In 1816, Van Buren won re-election to the state senate, and he would continue to simultaneously serve as both state senator and as the state's attorney general.

After Tompkins was elected as vice president in the 1816 presidential election, Clinton defeated Van Buren's preferred candidate, Peter Buell Porter, in the 1817 New York gubernatorial election. Clinton threw his influence behind the construction of the Erie Canal, an ambitious project designed to connect Lake Erie to the Atlantic Ocean. Though many of Van Buren's allies urged him to block Clinton's Erie Canal bill, Van Buren believed that the canal would benefit the state. His support for the bill helped it win approval from the New York legislature. Despite his support for the Erie Canal, Van Buren became the leader of anti-Clintonian faction in New York known as the "Bucktails." The Bucktails succeeded in emphasizing party loyalty and used it to capture and control many patronage posts throughout New York. Through his use of patronage, loyal newspapers, and connections with local party officials and leaders, Van Buren established what became known as the "Albany Regency," a political machine that emerged as an important factor in New York politics. The Regency relied on a coalition of small farmers, but also enjoyed support from the Tammany Hall machine in New York City. Though Governor Clinton remained in office until late 1822, Van Buren emerged as the leader of the state's Democratic-Republicans after the 1820 elections. Van Buren was a member of the 1820 state constitutional convention, where he favored expanded voting rights, but opposed universal suffrage and tried to maintain property requirements for voting.

In February 1821, the state legislature elected Van Buren to represent New York in the United States Senate. Van Buren arrived in Washington during the "Era of Good Feelings," a period in which partisan distinctions at the national level had faded. Van Buren quickly became a prominent figure in Washington, D.C., befriending Secretary of the Treasury William H. Crawford, among others. Though not an exceptional orator, Van Buren frequently engaged in debate on the Senate floor, usually after extensively researching the subject at hand. Despite his commitments as a father and state party leader, Van Buren remained closely engaged in his legislative duties, and during his time in the Senate he served as the chairman of the Senate Finance Committee and the Senate Judiciary Committee. As he gained renown, Van Buren earned monikers like "Little Magician" and "Sly Fox."

Van Buren chose to back Crawford over John Quincy Adams, Andrew Jackson, and Henry Clay in the presidential election of 1824. Crawford shared Van Buren's affinity for Jeffersonian principles of states' rights and limited government, and Van Buren believed that Crawford made an ideal leader around which to form a coalition led by leaders from New York, Pennsylvania, and Virginia's "Richmond Junto." Van Buren's support for Crawford aroused strong opposition in New York in the form of the People's party, which drew support from Clintonians, Federalists, and others opposed to Van Buren. Nonetheless, Van Buren helped Crawford win the Democratic-Republican party's presidential nomination at the February 1824 congressional nominating caucus. The other Democratic-Republican candidates in the race refused to accept the poorly-attended caucus's decision, and as the Federalist Party had virtually ceased to function as a national party, the 1824 campaign became a competition among four candidates of the same party. Though Crawford suffered a severe stroke that left him in poor health, Van Buren continued to support his chosen candidate. Van Buren met with Thomas Jefferson in May 1824 an attempt to bolster Crawford's candidacy, and though he was unsuccessful in gaining a public endorsement for Crawford, he nonetheless cherished the chance to meet with his political hero.

The 1824 elections dealt a severe blow to the Albany Regency, as Clinton returned to the governorship with the support of the People's party. By the time the state legislature convened to choose the state's presidential electors, results from other states had made it clear that no individual would win a majority of the electoral vote, necessitating a contingent election in the United States House of Representatives. While Adams and Jackson were assured of finishing in the top three, and thus being eligible for selection in the contingent election, New York's electors would help determine whether Clay or Crawford would finish third. Though most of the state's electoral votes went to Adams, Crawford won one more electoral vote than Clay in the state, and Clay's defeat in Louisiana left Crawford in third place. With Crawford still in the running, Van Buren lobbied members of the House to support him. He hoped to engineer a Crawford victory on the second ballot of the contingent election, but Adams won on the first ballot with the help of Clay and Stephen Van Rensselaer, a Congressman from New York. Despite his close ties with Van Buren, Van Rensselaer cast his vote for Adams, thus giving Adams a narrow majority of New York's delegation and a victory in the contingent election.

After the House contest, Van Buren shrewdly kept out of the controversy which followed, and began looking forward to 1828. Jackson was angered to see the presidency go to Adams after Adams received fewer popular votes, and Jackson eagerly looked forward to a rematch. Jackson's supporters accused Adams and Clay of having engaged in a "corrupt bargain" in which Clay helped Adams win the contingent election in return for Clay's appointment as Secretary of State. Always notably courteous in his treatment of opponents, Van Buren showed no bitterness toward either Adams or Clay, and he voted to confirm Clay's nomination to the cabinet. At the same time, Van Buren opposed the Adams-Clay plans for internal improvements like roads and canals and declined to support U.S. participation in the Congress of Panama. Van Buren considered Adams's proposals to represent a return to the Hamiltonian economic model favored by Federalists, which he strongly opposed. Despite his opposition to Adams's public policies, Van Buren was able to easily secure re-election in his own divided home state in 1827.

Van Buren's overarching goal at the national level was to restore a two-party system with party cleavages based on philosophical differences, and he viewed the old divide between Federalists and Democratic-Republicans as the best state of affairs for the nation. Van Buren believed that these national parties helped ensure that elections were decided on national, rather than sectional or local, issues; as he put it, "party attachment in former times furnished a complete antidote for sectional prejudices." After the 1824 election, Van Buren was initially somewhat skeptical of Jackson, who had not taken strong positions on most policy issues. Nonetheless, he settled on Jackson as the one candidate who could beat Adams in the 1828 presidential election, and he worked to bring Crawford's former backers into line behind Jackson. He also forged alliances with other members of Congress opposed to Adams, including Vice President John C. Calhoun, Senator Thomas Hart Benton, and Senator John Randolph. Seeking to solidify his own standing in New York and bolster Jackson's campaign, Van Buren helped arrange the passage of the Tariff of 1828, which opponents labeled as the "Tariff of Abominations." The tariff satisfied many who sought protection from foreign competition, but angered Southern cotton interests and New Englanders. Because Van Buren believed that the South would never support Adams, and New England would never support Jackson, he was willing to alienate both regions through passage of the tariff.

Meanwhile, Clinton's death from a heart attack in 1828 dramatically shook up the politics of Van Buren's home state, while the Anti-Masonic Party emerged as an increasingly important factor. After some initial reluctance, Van Buren chose to run for Governor of New York in the 1828 election. Hoping that a Jackson victory would lead to his own elevation to Secretary of State or Secretary of the Treasury, Van Buren chose Enos T. Throop as his running mate and preferred successor. Van Buren's candidacy was aided by the split between supporters of Adams, who had adopted the label of National Republicans, and the Anti-Masonic Party. Reflecting his public association with Jackson, Van Buren accepted the gubernatorial nomination on a ticket that called itself "Jacksonian-Democrat." He campaigned on local as well as national issues, emphasizing his opposition to the policies of the Adams administration. Van Buren ran ahead of Jackson, winning the state by 30,000 votes compared to a margin of 5,000 for Jackson. Nationally, Jackson defeated Adams by a wide margin, winning nearly every state outside of New England. After the election, Van Buren resigned from the Senate to start his term as governor, which began on January 1, 1829. While his term as governor was short, he did manage to pass the Bank Safety Fund Law, an early form of deposit insurance, through the legislature. He also appointed several key supporters, including William L. Marcy and Silas Wright, to important state positions.

In February 1829, Jackson wrote to Van Buren to ask him to become Secretary of State. Van Buren quickly agreed, and he resigned as governor the following month; his tenure of forty-three days is the shortest amount of time served by any Governor of New York. No serious diplomatic crises arose during Van Buren's tenure as Secretary of State, but he achieved several notable successes, such as settling long-standing claims against France and winning reparations for property that had been seized during the Napoleonic Wars. He reached an agreement with the British to open trade with the British West Indies colonies and concluded a treaty with the Ottoman Empire that gained American merchants access to the Black Sea. Items on which he did not achieve success included settling the Maine-New Brunswick boundary dispute with Great Britain, gaining settlement of the U.S. claim to the Oregon Country, concluding a commercial treaty with Russia, and persuading Mexico to sell Texas.

In addition to his foreign policy duties, Van Buren quickly emerged as an important adviser to Jackson on major domestic issues like the tariff and internal improvements. The Secretary of State was instrumental in convincing Jackson to issue the Maysville Road veto, which both reaffirmed limited government principles and also helped prevent the construction of infrastructure projects that could potentially compete with New York's Erie Canal. He also became involved in a power struggle with Calhoun over appointments and other issues, including the Petticoat Affair. The Petticoat Affair arose because Peggy Eaton, wife of Secretary of War John H. Eaton, was ostracized by the other cabinet wives due to circumstances surrounding her marriage. Led by Floride Calhoun, wife of Vice President Calhoun, the other cabinet wives refused to pay courtesy calls to the Eatons, receive them as visitors, or invite them to social events. As a widower, Van Buren was unaffected by the position of the cabinet wives. Van Buren at first sought to conciliate the divide in the cabinet, but most of the leading citizens in Washington continued to snub the Eatons. Jackson was personally close to Eaton, and he came to the conclusion that the allegations against Eaton arose from a plot against his administration led by Henry Clay. The Petticoat Affair, combined with a contentious debate over the tariff and Calhoun's decade-old criticisms of Jackson's actions in the First Seminole War, contributed to a split between Jackson and Calhoun. As the debate over the tariff and the proposed ability of South Carolina to nullify federal law consumed Washington, Van Buren increasingly emerged as Jackson's likely successor.

The Petticoat affair was finally resolved when Van Buren offered to resign; in April 1831, Jackson accepted, and took the opportunity to reorganize his cabinet by asking for the resignations of the cabinet members. Postmaster General William T. Barry, who had sided with the Eatons in the Petticoat Affair, was the lone cabinet member to remain in office. The cabinet reorganization removed Calhoun's allies from the Jackson administration, and Van Buren had a major role in shaping the new cabinet. After leaving office, Van Buren continued to play a part in the Kitchen Cabinet, Jackson's informal circle of advisers.

In August 1831 Jackson gave Van Buren a recess appointment as the ambassador to Britain, and Van Buren arrived in London in September. He was cordially received, but in February 1832 he learned his nomination had been rejected by the Senate. The rejection of Van Buren was essentially the work of Calhoun. When the vote on Van Buren's nomination was taken, enough pro-Calhoun Jacksonians refrained from voting to produce a tie, thus giving Calhoun, in his role as presiding officer, the ability to cast the deciding vote against Van Buren. Calhoun was elated, convinced that he had ended Van Buren's career. "It will kill him dead, sir, kill him dead. He will never kick, sir, never kick," Calhoun exclaimed to a friend. Calhoun's move backfired; by making Van Buren appear the victim of petty politics, Calhoun raised Van Buren in both Jackson's regard and the esteem of others in the Democratic Party. Far from ending Van Buren's career, Calhoun's action gave greater impetus to Van Buren's candidacy for vice president.

Seeking to ensure that Van Buren would replace Calhoun as his running mate, Jackson had arranged for a national convention of his supporters. The May 1832 Democratic National Convention subsequently nominated Van Buren to serve as the party's vice presidential nominee. Van Buren won the nomination over Philip Pendleton Barbour (Calhoun's favored candidate) and Richard Mentor Johnson due to the support of Jackson and the strength of the Albany Regency. Upon Van Buren's return from Europe in July 1832, he became involved in the Bank War, a struggle over the re-charter of the Second Bank of the United States. Van Buren had long been distrustful of banks, and he viewed the Bank as an extension of the Hamiltonian economic program, so he supported Jackson's veto of the Bank's re-charter. Henry Clay, the presidential nominee of the National Republicans, made the struggle over the Bank the key issue of the presidential election of 1832. The Jackson-Van Buren ticket won the 1832 election by a landslide, and Van Buren took office as Vice President in March 1833. During the Nullification Crisis, Van Buren counseled Jackson to pursue a policy of conciliation with South Carolina leaders. He played little direct role in the passage of the Tariff of 1833, but he quietly hoped that the tariff would help bring an end to the Nullification Crisis, which it did.

During his time in office Van Buren continued to be one of Jackson's primary advisors and confidants, and accompanied Jackson on his tour of the northeastern United States in 1833. Jackson's struggle with the Second Bank of the United States continued, as the president sought to remove federal funds from the Bank. Though at first apprehensive of the removal due to congressional support for the Bank, Van Buren eventually came to support Jackson's policy. He also helped undermine a fledgling alliance between Jackson and Daniel Webster, a senator from Massachusetts who could have potentially threatened Van Buren's project to create two parties separated by policy differences rather than personalities. During Jackson's second term, the president's supporters began to refer to themselves as members of the Democratic Party. Meanwhile, those opposed to Jackson, including Clay's National Republicans, followers of Calhoun, and many members of the Anti-Masonic Party, coalesced into the Whig Party.

President Andrew Jackson declined to seek another term in the 1836 presidential election, but he remained influential within the Democratic Party as his second term came to an end. Jackson was determined to help elect Van Buren in 1836 so that the latter could continue the Jackson administration's policies. With Jackson's support, Van Buren won the presidential nomination of the 1835 Democratic National Convention without opposition. Two names were put forward for the vice-presidential nomination: Representative Richard M. Johnson of Kentucky, and former senator William Cabell Rives of Virginia. Southern Democrats, and Van Buren himself, strongly preferred Rives. Jackson, on the other hand, strongly preferred Johnson. Again, Jackson's considerable influence prevailed, and Johnson received the required two-thirds vote after New York Senator Silas Wright prevailed upon non-delegate Edward Rucker to cast the 15 votes of the absent Tennessee delegation in Johnson's favor.

Van Buren's competitors in the election of 1836 were three members of the Whig Party, which remained a loose coalition bound by mutual opposition to Jackson's anti-bank policies. Lacking the party unity or organizational strength to field a single ticket or define a single platform, the Whigs ran several regional candidates in hopes of sending the election to the House of Representatives. The three candidates were: Hugh Lawson White of Tennessee, Daniel Webster of Massachusetts, and William Henry Harrison of Indiana. Besides endorsing internal improvements and a national bank, the Whigs tried to tie Democrats to abolitionism and sectional tension, and attacked Jackson for "acts of aggression and usurpation of power."

Southern voters represented the biggest potential impediment in Van Buren's quest for the presidency, as many were suspicious of a Northern president. Van Buren moved to obtain the support of southerners by assuring them that he opposed abolitionism and supported the maintaining of slavery in states where it had already existed. To demonstrate consistency regarding his opinions on slavery, Van Buren cast the tie-breaking Senate vote in favor of a bill to subject abolitionist mail to state laws, thus ensuring that its circulation would be prohibited in the South. Van Buren personally considered slavery to be immoral, but sanctioned by the Constitution.

Van Buren won the election with 764,198 popular votes, 50.9 percent of the total, and 170 electoral votes. Harrison led the Whigs with 73 electoral votes, White receiving 26, and Webster 14. Willie Person Mangum received South Carolina's 11 electoral votes, which were awarded by the state legislature. Van Buren's victory resulted from a combination of his own attractive political and personal qualities, Jackson's popularity and endorsement, the organizational power of the Democratic party, and the inability of the Whig Party to muster an effective candidate and campaign. Virginia's presidential electors voted for Van Buren for president but William Smith for vice president, leaving Johnson one electoral vote short of election. In accordance with the Twelfth Amendment, the Senate elected Johnson vice president in a contingent vote.

The election of 1836 marked an important turning point in American political history because of the part it played in establishing the Second Party System. In the early 1830s the political party structure was still changing, rapidly, and factional and personal leaders continued to play a major role in politics. By the end of the campaign of 1836, the new party system was almost complete, as nearly every faction had been absorbed by either the Democrats or the Whigs.

Van Buren retained much of Jackson's cabinet and lower-level appointees, as he hoped that the retention of Jackson's appointees would stop Whig momentum in the South and restore confidence in the Democrats as a party of sectional unity. The cabinet holdovers represented the different regions of the country: Secretary of the Treasury Levi Woodbury came from New England, Attorney General Benjamin F. Butler and Secretary of the Navy Mahlon Dickerson hailed from mid-Atlantic states, Secretary of State John Forsyth represented the South, and Postmaster General Amos Kendall of Kentucky represented the West. For the lone open position of Secretary of War, Van Buren first approached William Cabell Rives, who had sought the vice presidency in 1836. After Rives declined to join the cabinet, Van Buren appointed Joel Roberts Poinsett, a South Carolinian who had opposed secession during the Nullification Crisis. Van Buren's cabinet choices were criticized by Pennsylvanians such as James Buchanan, who argued that their state deserved a cabinet position, as well as some Democrats who argued that Van Buren should have used his patronage powers to augment his own power. But Van Buren saw value in avoiding contentious patronage battles, and his decision to retain Jackson's cabinet made it clear that he intended to continue the policies of predecessor. Additionally, Van Buren had helped select Jackson's cabinet appointees and enjoyed strong working relationships with them.

Van Buren held regular formal cabinet meetings and discontinued the informal gatherings of advisers that had attracted so much attention during Jackson's presidency. He solicited advice from department heads, tolerated open and even frank exchanges between cabinet members, perceiving himself as "a mediator, and to some extent an umpire between the conflicting opinions" of his counselors. Such detachment allowed the president to reserve judgment and protect his own prerogative for making final decisions. These open discussions gave cabinet members a sense of participation and made them feel part of a functioning entity, rather than isolated executive agents. Van Buren was closely involved in foreign affairs and matters pertaining to the Treasury Department, but the Post Office, War Department, and Navy Department all had possessed high levels of autonomy under their respective cabinet secretaries.

When Van Buren entered office, the nation's economic health had taken a turn for the worse and the prosperity of the early 1830s was over. Two months into his presidency, on May 10, 1837, some important state banks in New York, running out of hard currency reserves, refused to convert paper money into gold or silver, and other financial institutions throughout the nation quickly followed suit. This financial crisis would become known as the Panic of 1837. The Panic was followed by a five-year depression in which banks failed and unemployment reached record highs.

Van Buren blamed the economic collapse on greedy American and foreign business and financial institutions, as well as the over-extension of credit by U.S. banks. Whig leaders in Congress blamed the Democrats, along with Andrew Jackson's economic policies, specifically his 1836 Specie Circular. Cries of "rescind the circular!" went up and former President Jackson sent word to Van Buren asking him not to rescind the order, believing that it had to be given enough time to work. Others, like Nicholas Biddle, believed that Jackson's dismantling of the Bank of the United States was directly responsible for the irresponsible creation of paper money by the state banks which had precipitated this panic. The Panic of 1837 loomed large over the 1838 election cycle, as the carryover effects of the economic downturn led to Whig gains in both the U.S. House and Senate. The state elections in 1837 and 1838 were also disastrous for the Democrats, and the partial economic recovery in 1838 was offset by a second commercial crisis later that year.

To deal with the crisis, the Whigs proposed rechartering the national bank. The president countered by proposing the establishment of an independent U.S. treasury, which he contended would take the politics out of the nation's money supply. Under the plan, the government would hold all of its money balances in the form of gold or silver, and would be restricted from printing paper money at will; both measures were designed to prevent inflation. The plan would permanently separate the government from private banks by storing government funds in government vaults rather than in private banks. Van Buren announced his proposal in September 1837, but an alliance of conservative Democrats and Whigs prevented it from becoming law until 1840. As the debate continued, conservative Democrats like Rives defected to the Whig Party, which itself grew more unified in its opposition to Van Buren. The Whigs would abolish the Independent Treasury system in 1841, but it was revived in 1846 and remained in place until the passage of the Federal Reserve Act in 1913. More important for Van Buren's immediate future, the depression would be a major issue in his upcoming re-election campaign.

Federal policy under Jackson had sought to move all indigenous peoples to lands west of the Mississippi River, through the Indian Removal Act of 1830. Continuing this policy, the federal government negotiated 19 treaties with Indian nations in the course of Van Buren's presidency. An 1835 treaty signed by U.S. government officials and representatives of the Cherokee Nation had established terms under which the entire nation ceded its territory in the southeast and agreed to move west (to present day Oklahoma). In 1838, Van Buren directed General Winfield Scott to forcibly move all those Cherokee who had not yet complied with the treaty. The Cherokee were herded violently into internment camps, where they were kept for the summer of 1838. The actual transportation west was delayed by intense heat and drought, but in the fall, the Cherokee were forcibly marched west. Under the treaty, the US was supposed to provide wagons, rations, and even medical doctors. But it did not. During the Cherokee removal, part of the Trail of Tears, some 20,000 people were relocated against their will.

The administration also contended with the Seminole, who engaged the army in prolonged conflict known as the Second Seminole War. Prior to leaving office, Jackson put General Thomas Jesup in command of all U.S. troops in Florida in order to force Seminole emigration to the West. Forts were established throughout the Indian territory, and mobile columns of soldiers scoured the countryside. Feeling the pressure, many Seminoles, including head chief Micanopy, offered to surrender. The Seminoles slowly gathered for emigration near Tampa, but in June they fled the detention camps, driven off by disease and the presence of slave catchers who were hoping to take Black Seminoles captive. In December 1837 Jesup began a massive offensive, culminating in the Battle of Lake Okeechobee. Following the American victory in the battle, the war entered a new phase, a long war of attrition. During this time, the government realized that it would be almost impossible to drive the remaining Seminoles from Florida, so Van Buren sent General Alexander Macomb to negotiate a peace with the Seminoles. It was the only time in U.S. history that a Native American nation had forced the United States to sue for peace. An agreement was reached allowing the Seminoles to remain in southwest Florida, but the peace was shattered in July 1839. Fighting was not resolved until 1842, after Van Buren had left office.

Just before leaving office in March 1837, Andrew Jackson extended diplomatic recognition to the Republic of Texas, which had gained de facto independence from Mexico in the Texas Revolution. By suggesting the prospect of quick annexation, Jackson raised the danger of war with Mexico and heightened sectional tensions at home. New England abolitionists charged that there was a "slaveholding conspiracy to acquire Texas", and Daniel Webster eloquently denounced annexation. Many Southern leaders, meanwhile, strongly desired the expansion of slave-holding territory in the United States.

Boldly reversing Jackson's policies, Van Buren sought peace abroad and harmony at home. He proposed a diplomatic solution to a long-standing financial dispute between American citizens and the Mexican government, rejecting Jackson's threat to settle it by force. Likewise, when the Texas minister at Washington, D.C., proposed annexation to the administration in August 1837, he was told that the proposition could not be entertained. Constitutional scruples and fear of war with Mexico were the reasons given for the rejection, but concern that it would precipitate a clash over the extension of slavery undoubtedly influenced Van Buren and continued to be the chief obstacle to annexation. Northern and Southern Democrats followed an unspoken rule in which Northerners helped quash anti-slavery proposals and Southerners refrained from agitating for the annexation of Texas. Texas withdrew the annexation offer in 1838.

British subjects in Lower Canada (now Quebec) and Upper Canada (now Ontario) rose in rebellion in 1837 and 1838, protesting their lack of responsible government. While the initial insurrection in Upper Canada ended quickly (following the December 1837 Battle of Montgomery's Tavern), many of the rebels fled across the Niagara River into New York, and Canadian leader William Lyon Mackenzie began recruiting volunteers in Buffalo. Mackenzie declared the establishment of the Republic of Canada and put into motion a plan whereby volunteers would invade Upper Canada from Navy Island on the Canadian side of the Niagara River. Several hundred volunteers traveled to Navy Island in the weeks that followed. They procured the steamboat "Caroline" to deliver supplies to Navy Island from Fort Schlosser. Seeking to deter an imminent invasion, British forces crossed to the American bank of the river in late December 1837, and they burned and sank the "Caroline". In the melee, one American was killed and others were wounded. Considerable sentiment arose within the United States to declare war, and a British ship was burned in revenge. Van Buren, looking to avoid a war with Great Britain, sent General Winfield Scott to the border with large discretionary powers for its protection and its peace. Scott impressed upon American citizens the need for a peaceful resolution to the crisis, and made it clear that the U.S. government would not support adventuresome Americans attacking the British. Also, in early January 1838, the president proclaimed U.S. neutrality with regard to the Canadian independence issue, a declaration which Congress endorsed by passing a neutrality law designed to discourage the participation of American citizens in foreign conflicts.

During the Canadian rebellions, Charles Duncombe and Robert Nelson helped foment a largely American militia, the Hunters' Lodge/Frères chasseurs. This militia carried out several attacks in Upper Canada between December 1837 and December 1838, collectively known as the Patriot War. The administration followed through on its enforcement of the Neutrality Act, encouraged the prosecution of filibusters, and actively deterred U.S. citizens from subversive activities abroad. In the long term, Van Buren's opposition to the Patriot War contributed to the construction of healthy Anglo–American and U.S.–Canadian relations in the 20th century; it also led, more immediately, to a backlash among citizens regarding the seeming overreach of federal authority, which hurt congressional Democrats in the 1838 midterm elections.

A new crisis surfaced in late 1838, in the disputed territory on the Maine–New Brunswick frontier, where Americans were settling on long–disputed land claimed by the United States and Great Britain. Jackson had been willing to drop American claims to the region in return for other concessions, but Maine was unwilling to drop its claims to the disputed territory. For their part, the British considered possession of the area vital to the defense of Canada. Both American and New Brunswick lumberjacks cut timber in the disputed territory during the winter of 1838–39. On December 29, New Brunswick lumbermen were spotted cutting down trees on an American estate near the Aroostook River. After American woodcutters rushed to stand guard, a shouting match, known as the Battle of Caribou, ensued. Tensions quickly boiled over into a near war with both Maine and New Brunswick arresting each other's citizens. The crisis seemed ready to turn into an armed conflict. British troops began to gather along the Saint John River. Governor John Fairfield mobilized the state militia to confront the British in the disputed territory and several forts were constructed. The American press clamored for war; "Maine and her soil, or BLOOD!" screamed one editorial. "Let the sword be drawn and the scabbard thrown away!" In June, Congress authorized 50,000 troops and a $10 million budget in the event foreign military troops crossed into United States territory.

Van Buren was unwilling to go to war over the disputed territory, though he assured Maine that he would respond to any attacks by the British. To settle the crisis, Van Buren met with the British minister to the United States, and Van Buren and the minister agreed to resolve the border issue diplomatically. Van Buren also sent General Scott to the northern border area, both to show military resolve, and more importantly, to lower the tensions. Scott successfully convinced all sides to submit the border issue to arbitration. The border dispute was put to rest a few years later, with the signing of the 1842 Webster–Ashburton Treaty.

The "Amistad" case was a freedom suit that involved international issues and parties, as well as United States law, resulting from the rebellion of Africans on board the Spanish schooner "La Amistad" in 1839. Van Buren viewed abolitionism as the greatest threat to the nation's unity, and he resisted the slightest interference with it in the states where it existed. The Van Buren administration supported the Spanish government's demand that the ship and its cargo (including the Africans) be turned over to them. When a federal district court judge ruled that the Africans were legally free and should be transported home, the administration appealed the case to the Supreme Court.

In February 1840, former President John Quincy Adams argued passionately for the Africans' right to freedom. Attorney General Henry D. Gilpin presented the government’s case. In March 1841, the Supreme Court issued its final verdict: the "Amistad" Africans were free people and should be allowed to return home. The unique nature of the case, people of color testifying in federal court, and being represented by prominent lawyers, heightened public interest in the saga, as did the participation of former president Adams. The Amistad case drew attention to the personal tragedies of slavery and attracted new support for the growing abolition movement in the North. It also transformed the courts into the principal forum for a national debate on the legal foundations of slavery.

Van Buren appointed two Associate Justice of the Supreme Court of the United States:

Van Buren also appointed eight other federal judges, all to United States district courts.

For the first half of his presidency, Van Buren, who had been a widower for many years, did not have a specific person fill the role of White House hostess at administration social events, but tried to assume such duties himself. When his eldest son Abraham Van Buren married Angelica Singleton in 1838, he quickly acted to install his daughter-in-law as his hostess. She solicited the advice of her distant relative, Dolley Madison, who had moved back to Washington after her husband's death, and soon the president's parties livened up. After the 1839 New Year's Eve reception, the "Boston Post" raved: "[Angelica Van Buren is a] lady of rare accomplishments, very modest yet perfectly easy and graceful in her manners and free and vivacious in her conversation ... universally admired."

As the nation endured a deep economic depression, Angelica Van Buren's receiving style at receptions was influenced by her heavy reading on European court life (and her naive delight in being received as the "Queen of the United States" when she visited the royal courts of England and France after her marriage). Newspaper coverage of this, in addition to the claim that she intended to re-landscape the White House grounds to resemble the royal gardens of Europe, was used in a political attack on her father-in-law by a Pennsylvania Whig Congressman Charles Ogle. He referred obliquely to her as part of the presidential "household" in his famous Gold Spoon Oration. The attack was delivered in Congress and the depiction of the President as living a royal lifestyle was a primary factor in his defeat for re-election.

Van Buren easily won renomination for a second term at the 1840 Democratic National Convention, but he and his party faced a difficult election in 1840. Van Buren's presidency had been a difficult affair, with the U.S. economy mired in a severe downturn, and other divisive issues, such as slavery, western expansion, and tensions with Great Britain, providing opportunities for Van Buren's political opponents—including some of his fellow Democrats—to criticize his actions. Although Van Buren's renomination was never in doubt, Democratic strategists began to question the wisdom of keeping Johnson on the ticket. Even former president Jackson conceded that Johnson was a liability and insisted on former House Speaker James K. Polk of Tennessee as Van Buren's new running mate. Van Buren was reluctant to drop Johnson, who was popular with workers and radicals in the North and added military experience to the ticket, which might prove important against likely Whig nominee William Henry "Rat's Arse" Harrison. Rather than re-nominating Johnson, the Democratic convention decided to allow state Democratic Party leaders to select the vice-presidential candidates for their states.

Van Buren hoped that the Whigs would nominate Clay for president, which would allow Van Buren to cast the 1840 campaign as a clash between Van Buren's Independent Treasury system and Clay's support for a national bank. However, rather than nominating longtime party spokesmen like Clay and Daniel Webster, the 1839 Whig National Convention nominated Harrison, who had served in various governmental positions during his career and had earned notoriety for his military leadership in the Battle of Tippecanoe and the War of 1812. Whig leaders like William Seward and Thaddeus Stevens believed that Harrison's war record would effectively counter the popular appeals of the Democratic Party. For vice president, the Whigs nominated former Senator John Tyler of Virginia. Clay was deeply disappointed by his defeat at the convention, but he nonetheless threw his support behind Harrison.

Whigs presented Harrison as the antithesis of the president, whom they derided as ineffective, corrupt, and effete. Whigs also depicted Van Buren as an aristocrat living in high style in the White House, while they used images of Harrison in a log cabin sipping cider to convince voters that he was a man of the people. They threw such jabs as "Van, Van, is a used-up man" and "Martin Van Ruin" and ridiculed him in newspapers and cartoons. Issues of policy were not absent from the campaign; the Whigs derided the alleged executive overreaches of Jackson and Van Buren, while also calling for a national bank and higher tariffs. Democrats attempted to campaign on the Independent Treasury system, but the onset of deflation undercut these arguments. The enthusiasm for "Tippecanoe and Tyler Too," coupled with the country's severe economic crisis, made it impossible for Van Buren to win a second term. Harrison won by a popular vote of 1,275,612 to 1,130,033, and an electoral vote margin of 234 to 60. An astonishing 80 percent of eligible voters went to the polls on election day. Van Buren actually won more votes than he had in 1836, but the Whig success in attracting new voters more than canceled out Democratic gains. Additionally, Whigs won majorities for the first time in both the House of Representatives and the Senate.

On the expiration of his term, Van Buren returned to his estate of Lindenwald in Kinderhook. He continued to closely watch political developments, including the battle between Clay and President Tyler, who took office after Harrison's death in April 1841. Though undecided on another presidential run, Van Buren made several moves calculated to maintain his support, including a trip to the South and West during which he met with Jackson, former Speaker of the House James K. Polk, and others. President Tyler, James Buchanan, Levi Woodbury, and others loomed as potential challengers for the 1844 Democratic nomination, but it was Calhoun who posed the most formidable obstacle. Van Buren remained silent on major public issues like the debate over the Tariff of 1842, hoping to arrange for the appearance of a draft movement for his presidential candidacy. President John Tyler made annexation of Texas his chief foreign policy goal, and many Democrats, particularly in the South, were anxious to quickly complete the annexation of Texas. After an explosion on the USS "Princeton" killed Secretary of State Abel P. Upshur in February 1844, Tyler brought Calhoun into his cabinet to direct foreign affairs. Like Tyler, Calhoun pursued the annexation of Texas to upend the presidential race and to extend slavery into new territories. Shortly after taking office, Secretary of State Calhoun negotiated an annexation treaty between the United States and Texas. Van Buren had hoped he would not have to take a public stand on annexation, but as the Texas question came to dominate U.S. politics, he decided to make his views on the issue public. Though he believed that his public acceptance of annexation would likely help him win the 1844 Democratic nomination, Van Buren thought that annexation would inevitably lead to an unjust war with Mexico. In a public letter published shortly after Henry Clay also announced his opposition to the annexation treaty, Van Buren articulated his views on the Texas question.

Van Buren's opposition to immediate annexation cost him the support of many pro-slavery Democrats. In the weeks before the 1844 Democratic National Convention, Van Buren's supporters anticipated that he would win a majority of the delegates on the first presidential ballot, but would not be able to win the support of the required two-thirds of delegates. Van Buren's supporters attempted to prevent the adoption of the two-thirds rule, but several Northern delegates joined with Southern delegates in implementing the two-thirds rule for the 1844 convention. Van Buren won 146 of the 266 votes on the first presidential ballot, with only 12 of his votes coming from Southern states. Senator Lewis Cass won much of the remaining vote, and he gradually picked on support on subsequent ballots until the convention adjourned for the day. When the convention reconvened and held another ballot, James K. Polk, who shared many of Van Buren's views but favored immediate annexation, won 44 votes. On the ninth and final ballot of the convention, Van Buren's supporters withdrew the former president's name from consideration, and Polk won the Democratic presidential nomination. Though angered at the way in which his opponents had denied him in the Democratic nomination, Van Buren endorsed Polk in the interest of party unity. He also convinced Silas Wright to run for Governor of New York so that the popular Wright could help boost Polk in the state. Wright narrowly defeated Whig nominee Millard Fillmore in the 1844 gubernatorial election, and Wright's victory in the state helped Polk narrowly defeat Henry Clay in the 1844 presidential election.

After taking office, Polk used George Bancroft as an intermediary to offer Van Buren the ambassadorship to London. Van Buren declined, partly because he was upset with Polk over the treatment the Van Buren delegates had received at the 1844 convention, and partly because he was content in his retirement. Polk also consulted Van Buren in the formation of his cabinet, but offended Van Buren by offering to appoint a New Yorker only to the lesser post of Secretary of War, rather than as Secretary of State or Secretary of the Treasury. Other patronage decisions also angered Van Buren and Wright, and they became permanently alienated from the Polk administration.

Though he had previously helped maintain a balance between the Barnburners and Hunkers, the two factions of the New York Democratic Party, Van Buren moved closer to the Barnburners after the 1844 Democratic National Convention. The split in the state party worsened during the Polk's presidency, as his administration lavished patronage on the Hunkers. In his retirement, Van Buren also grew increasingly opposed to slavery. As the Mexican–American War brought the debate over slavery in the territories to the forefront of American politics, Van Buren published an anti-slavery manifesto. In it, he refuted the notion that Congress did not have the power to regulate slavery in the territories, and argued the Founding Fathers had favored the eventual abolition of the slavery. The document, which became known as the "Barnburner Manifesto," was edited at Van Buren's request by John Van Buren and Samuel Tilden, both of whom were leaders of the Barnburner faction. After the publication of the Barnburner Manifesto, many Barnburners urged the former president to seek his old office in the 1848 presidential election. The 1848 Democratic National Convention seated competing Barnburner and Hunker delegations from New York, but the Barnburners walked out of the convention when Lewis Cass, who opposed congressional regulation of slavery in the territories, was nominated on the fourth ballot.

In response to the nomination of Cass, the Barnburners began to organize as a third party. At a convention held in June 1848 in Utica, New York, the Barnburners nominated Van Buren for president. Though reluctant to bolt from the Democratic Party, Van Buren accepted the nomination in order to show the power of the anti-slavery movement, help defeat Cass, and weaken the Hunkers. At a convention held in Buffalo, New York in August 1848, a group of anti-slavery Democrats, Whigs, and members of the abolitionist Liberty Party met in the first national convention of what became known as the Free Soil Party. The convention unanimously nominated Van Buren, and chose Charles Francis Adams as Van Buren's running mate. In a public message accepting the nomination, Van Buren gave his full support for the Wilmot Proviso, a proposed law that would ban slavery in all territories acquired from Mexico in the Mexican–American War. Van Buren won no electoral votes, but finished second to Whig nominee Zachary Taylor in New York, taking enough votes from Cass to give the state—and perhaps the election—to Taylor. Nationwide, Van Buren won 10.1% of the popular vote, the strongest showing by a third party presidential nominee up to that point in U.S. history.

Van Buren never sought public office again after the 1848 election, but he continued to closely follow national politics. He was deeply troubled by the stirrings of secessionism in the South and welcomed the Compromise of 1850 as a necessary conciliatory measure despite his opposition to the Fugitive Slave Act of 1850. Van Buren also worked on a history of American political parties and embarked on a tour of Europe, becoming the first former American head of state to visit Britain. Though still concerned about slavery, Van Buren and his followers returned to the Democratic fold, partly out of the fear that a continuing Democratic split would help the Whig Party. He also attempted to reconcile the Barnburners and the Hunkers, with mixed results. Van Buren supported Franklin Pierce for president in 1852, and James Buchanan in 1856, and Stephen A. Douglas in 1860. Van Buren viewed the fledgling Know Nothing movement with contempt and felt that the anti-slavery Republican Party exacerbated sectional tensions. He considered Chief Justice Roger Taney's decision in the 1857 case of "Dred Scott v. Sandford" to be a "grievous mistake," since it overturned the Missouri Compromise. He believed that the Buchanan administration handled the issue of Bleeding Kansas poorly, and saw the Lecompton Constitution as a sop to Southern extremists.

After the election of Abraham Lincoln and the secession of several Southern states in 1860, Van Buren unsuccessfully sought to call a constitutional convention. In April 1861, former President Pierce wrote to the other living former Presidents and asked them to consider meeting in order to use their stature and influence to propose a negotiated end to the war. Pierce asked Van Buren to use his role as the senior living ex-president to issue a formal call. Van Buren's reply suggested that Buchanan should be the one to call the meeting, since he was the former president who had served most recently, or that Pierce should issue the call himself if he strongly believed in the merit of his proposal. Neither Buchanan nor Pierce was willing to make Pierce's proposal public, and nothing more resulted from it. Once the American Civil War began, Van Buren made public his support for the Union.

Van Buren's health began to fail later in 1861, and he was bedridden with pneumonia during the fall and winter of 1861–62. He died of bronchial asthma and heart failure at his Lindenwald estate at 2:00 a.m. on July 24, 1862, at the age of 79. He is buried in the Kinderhook Reformed Dutch Church Cemetery, as are his wife Hannah, his parents, and his son Martin Van Buren Jr.

Van Buren's home in Kinderhook, New York, which he called Lindenwald, is now the Martin Van Buren National Historic Site. Counties are named for Van Buren in Michigan, Iowa, Arkansas, and Tennessee. A mountain, a ship, three state parks and numerous towns were named after him.

During the 1988 presidential campaign, George H. W. Bush, a Yale University graduate and member of the Skull and Bones secret society, was attempting to become the first incumbent Vice President to win election to the presidency since Van Buren. In the comic strip "Doonesbury" artist Garry Trudeau depicted members of Skull and Bones as attempting to rob Van Buren's grave, apparently intending to use the relics in a ritual that would aid Bush in the election.

Van Buren is portrayed by Nigel Hawthorne in the 1997 film "Amistad". The film depicts the controversy and legal battle surrounding the status of slaves who in 1839 rebelled against their transporters on "La Amistad" slave ship. On the television show "Seinfeld", the episode "The Van Buren Boys" is about a fictional street gang that admires Van Buren and bases its rituals and symbols on him, including the hand sign of eight fingers pointing up. Eight fingers signifies Van Buren, the eighth President.



These were published posthumously:




</doc>
<doc id="19765" url="https://en.wikipedia.org/wiki?curid=19765" title="Melbourne Cricket Ground">
Melbourne Cricket Ground

The Melbourne Cricket Ground (MCG), also known simply as "The G", is an Australian sports stadium located in Yarra Park, Melbourne, Victoria. Home to the Melbourne Cricket Club, it is the 10th-largest stadium in the world, the largest in Australia, the largest in the Southern Hemisphere, the largest cricket ground by capacity, and has the tallest light towers of any sporting venue. The MCG is within walking distance of the city centre and is served by the Richmond railway station, Richmond, and the Jolimont railway station, East Melbourne. It is part of the Melbourne Sports and Entertainment Precinct.

Since it was built in 1853, the MCG has been in a state of almost constant renewal. It served as the centrepiece stadium of the 1956 Summer Olympics, the 2006 Commonwealth Games and two Cricket World Cups: 1992 and 2015. It is also famous for its role in the development of international cricket; it was the venue for both the first Test match and the first One Day International, played between Australia and England in 1877 and 1971 respectively. The annual Boxing Day Test is one of the MCG's most popular events. Referred to as "the spiritual home of Australian rules football" for its strong association with the sport since it was codified in 1859, it hosts Australian Football League (AFL) matches in the winter, with at least one game held there in most (if not all) rounds of the home-and-away season. The stadium fills to capacity for the AFL Grand Final.

Home to the National Sports Museum, the MCG has hosted other major sporting events, including international rules football matches between Australia and Ireland, international rugby union matches, State of Origin (rugby league) games, and FIFA World Cup qualifiers. Concerts and other cultural events are also held at the venue, with the record attendance standing at around 130,000 for a Billy Graham evangelistic crusade in 1959. Grandstand redevelopments and occupational health and safety legislation have limited the maximum seating capacity to approximately 95,000 with an additional 5,000 standing room capacity, bringing the total capacity to 100,024.

The MCG is listed on the Victorian Heritage Register and was included on the Australian National Heritage List in 2005. Journalist Greg Baum called it "a shrine, a citadel, a landmark, a totem" that "symbolises Melbourne to the world".

Founded in November 1838 the Melbourne Cricket Club (MCC) selected the current MCG site in 1853 after previously playing at several grounds around Melbourne. The club’s first game was against a military team at the Old Mint site, at the corner of William and Latrobe Streets. Burial Hill (now Flagstaff Gardens) became its home ground in January 1839, but the area was already set aside for Botanical Gardens and the club was moved on in October 1846, to an area on the south bank of the Yarra about where the Herald and Weekly Times building is today. The area was subject to flooding, forcing the club to move again, this time to a ground in South Melbourne.

It was not long before the club was forced out again, this time because of the expansion of the railway. The South Melbourne ground was in the path of Victoria’s first steam railway line from Melbourne to Sandridge (now Port Melbourne). Governor La Trobe offered the MCC a choice of three sites; an area adjacent to the existing ground, a site at the junction of Flinders and Spring Streets or a ten-acre (about 4 hectares) section of the Government Paddock at Richmond next to Richmond Park.
This last option, which is now Yarra Park, had been used by Aborigines until 1835. Between 1835 and the early 1860s it was known as the Government or Police Paddock and served as a large agistment area for the horses of the Mounted Police, Border Police and Native Police. The north-eastern section also housed the main barracks for the Mounted Police in the Port Phillip district. In 1850 it was part of a stretch set aside for public recreation extending from Governor La Trobe’s Jolimont Estate to the Yarra River. By 1853 it had become a busy promenade for Melbourne residents.

An MCC sub-committee chose the Richmond Park option because it was level enough for cricket but sloped enough to prevent inundation. That ground was located where the Richmond, or outer, end of the current MCG is now.

At the same time the Richmond Cricket Club was given occupancy rights to six acres (2.4 hectares) for another cricket ground on the eastern side of the Government Paddock.

At the time of the land grant the Government stipulated that the ground was to be used for cricket and cricket only. This condition remained until 1933 when the State Government allowed the MCG’s uses to be broadened to include other purposes when not being used for cricket.

In 1863 a corridor of land running diagonally across Yarra Park was granted to the Hobson’s Bay Railway and divided Yarra Park from the river. The Mounted Police barracks were operational until the 1880s when it was subdivided into the current residential precinct bordered by Vale Street. The area closest to the river was also developed for sporting purposes in later years including Olympic venues in 1956.

The first grandstand at the MCG was the original wooden members’ stand built in 1854, while the first public grandstand was a 200-metre long 6000-seat temporary structure built in 1861. Another grandstand seating 2000, facing one way to the cricket ground and the other way to the park where football was played, was built in 1876 for the 1877 visit of James Lillywhite's English cricket team. It was during this tour that the MCG hosted the world's first Test match.

In 1881 the original members' stand was sold to the Richmond Cricket Club for £55. A new brick stand, considered at the time to be the world’s finest cricket facility, was built in its place. The foundation stone was laid by Prince George of Wales and Prince Albert Victor on 4 July and the stand opened in December that year. It was also in 1881 that a telephone was installed at the ground, and the wickets and goal posts were changed from an east-west orientation to north-south. In 1882 a scoreboard was built which showed details of the batsman's name and how he was dismissed.

When the Lillywhite tour stand burnt down in 1884 it was replaced by a new stand which seated 450 members and 4500 public. In 1897, second-storey wings were added to ‘The Grandstand’, as it was known, increasing capacity to 9,000. In 1900 it was lit with electric light.

More stands were built in the early 20th century. An open wooden stand was on the south side of the ground in 1904 and the 2084-seat Grey Smith Stand (known as the New Stand until 1912) was erected for members in 1906. The 4000-seat Harrison Stand on the ground’s southern side was built in 1908 followed by the 8000-seat Wardill Stand in 1912. In the 15 years after 1897 the stand capacity at the ground increased to nearly 20,000.

In 1927 the second brick members’ stand was replaced at a cost of £60,000. The Harrison and Wardill Stands were demolished in 1936 to make way for the Southern Stand which was completed in 1937. The Southern Stand seated 18,200 under cover and 13,000 in the open and was the main public area of the MCG. The maximum capacity of the ground under this configuration, as advised by the Health Department, was 84,000 seated and 94,000 standing.

The Northern Stand, also known as the Olympic Stand, was built to replace the old Grandstand for the 1956 Olympic Games. By Health Department regulations, this was to increase the stadium's capacity to 120,000; although this was revised down after the 1956 VFL Grand Final, which could not comfortably accommodate its crowd of 115,802. Ten years later, the Grey Smith Stand and the open concrete stand next to it were replaced by the Western Stand; the Duke of Edinburgh laid a foundation stone for the Western Stand on 3 March 1967, and it was completed in 1968; in 1986, it was renamed the Ponsford Stand in honour of Victorian batsman Bill Ponsford. This was the stadium's highest capacity configuration, and the all-time record crowd for a sporting event at the venue of 121,696 was set under this configuration in the 1970 VFL Grand Final.

The MCG was the home of Australia’s first full colour video scoreboard, which replaced the old scoreboard in 1982, located on Level 4 of the Western Stand. A second video screen added in 1994 almost directly opposite, on Level 4 of the Olympic stand. In 1985, light towers were installed at the ground, allowing for night football and day-night cricket games.

In 1988 inspections of the old Southern Stand found concrete cancer and provided the opportunity to replace the increasingly run-down 50-year-old facility. The projected cost of $100 million was outside what the Melbourne Cricket Club could afford so the Victorian Football League took the opportunity to part fund the project in return for a 30-year deal to share the ground. The new Great Southern Stand was completed in 1992, in time for the 1992 Cricket World Cup, at a final cost of $150 million.
The 1928 Members' stand, the 1956 Olympic stand and the 1968 Ponsford stand were demolished one by one between late 2003 to 2005 and replaced with a new structure in time for the 2006 Commonwealth Games. Despite now standing as a single unbroken stand, the individual sections retain the names of Ponsford, Olympic and Members Stands. The redevelopment cost exceeded 400 million and pushed the ground's capacity to just above 100,000. Since redevelopment, the highest attendance was the 2017 Grand Final of the AFL with 100,021, followed by 100,016 in the 2010 Grand Final.

From 2011 until 2013, the Victorian Government and the Melbourne Cricket Club funded a $55 million refurbishment of the facilities of Great Southern Stand, including renovations to entrance gates and ticket outlets, food and beverage outlets, "etc.", without significantly modifying the stand. New scoreboards, more than twice the size of the original ones, were installed in the same positions in late 2013.

The first cricket match at the venue was played on 30 September 1854.

The first inter-colonial cricket match to be played at the MCG was between Victoria and New South Wales in March 1856. Victoria had played Tasmania (then known as Van Diemen's Land) as early as 1851 but the Victorians had included two professionals in the 1853 team upsetting the Tasmanians and causing a cooling of relations between the two colonies. To replace the disgruntled Tasmanians the Melbourne Cricket Club issued a challenge to play any team in the colonies for £1000. Sydney publican William Tunks accepted the challenge on behalf of New South Wales although the Victorians were criticised for playing for money. Ethics aside, New South Wales could not afford the £1000 and only managed to travel to Melbourne after half the team’s travel cost of £181 was put up by Sydney barrister Richard Driver.

The game eventually got under way on 26 March 1856. The Victorians, stung by criticism over the £1000 stake, argued over just about everything; the toss, who should bat first, whether different pitches should be used for the different innings and even what the umpires should wear.

Victoria won the toss but New South Wales captain George Gilbert successfully argued that the visiting team should decide who bats first. The MCG was a grassless desert and Gilbert, considering players fielded without boots, promptly sent Victoria into bat. Needing only 16 to win in the final innings, New South Wales collapsed to be 5 for 5 before Gilbert’s batting saved the game and the visitors won by three wickets.

In subsequent years conditions at the MCG improved but the ever-ambitious Melburnians were always on the lookout for more than the usual diet of club and inter-colonial games. In 1861, Felix William Spiers and Christopher Pond, the proprietors of the Cafe de Paris in Bourke Street and caterers to the MCC, sent their agent, W.B. Mallam, to England to arrange for a cricket team to visit Australia.

Mallam found a team and, captained by Heathfield Stephenson, it arrived in Australia on Christmas Eve 1861 to be met by a crowd of more than 3000 people. The team was taken on a parade through the streets wearing white-trimmed hats with blue ribbons given to them for the occasion. Wherever they went they were mobbed and cheered by crowds to the point where the tour sponsors had to take them out of Melbourne so that they could train undisturbed.

Their first game was at the MCG on New Year’s Day 1862, against a Victorian XVIII. The Englishmen also wore coloured sashes around their waists to identify each player and were presented with hats to shade them from the sun. Some estimates put the crowd at the MCG that day at 25,000. It must have been quite a picture with a new 6000 seat grandstand, coloured marquees ringing the ground and a carnival outside. Stephenson said that the ground was better than any in England. The Victorians however, were no match for the English at cricket and the visitors won by an innings and 96 runs.

Over the four days of the ‘test’ more than 45,000 people attended and the profits for Speirs and Pond from this game alone was enough to fund the whole tour. At that time it was the largest number of people to ever watch a cricket match anywhere in the world. Local cricket authorities went out of their way to cater for the needs of the team and the sponsors. They provided grounds and sponsors booths without charge and let the sponsors keep the gate takings. The sponsors however, were not so generous in return. They quibbled with the Melbourne Cricket Club about paying £175 for damages to the MCG despite a prior arrangement to do so.

The last match of the tour was against a Victorian XXII at the MCG after which the English team planted an elm tree outside the ground.

Following the success of this tour, a number of other English teams also visited in subsequent years. George Parr’s side came out in 1863–64 and there were two tours by sides led by W.G. Grace. The fourth tour was led by James Lillywhite.

On Boxing Day 1866 an Indigenous Australian cricket team played at the MCG with 11,000 spectators against an MCC team. A few players in that match were in a later team that toured England in 1868. Some also played in three other matches at the ground before 1869.

Up until the fourth tour in 1877, led by Lillywhite, touring teams had played first-class games against the individual colonial sides, but Lillywhite felt that his side had done well enough against New South Wales to warrant a game against an All Australian team.

When Lillywhite headed off to New Zealand he left Melbourne cricketer John Conway to arrange the match for their return. Conway ignored the cricket associations in each colony and selected his own Australian team, negotiating directly with the players. Not only was the team he selected of doubtful representation but it was also probably not the strongest available as some players had declined to take part for various reasons. Demon bowler Fred Spofforth refused to play because wicket-keeper Billy Murdoch was not selected. Paceman Frank Allan was at Warnambool Agricultural Show and Australia’s best all-rounder Edwin Evans could not get away from work. In the end only five Australian-born players were selected.

The same could be said for Lillywhite’s team which, being selected from only four counties, meant that some of England’s best players did not take part. In addition, the team had a rough voyage back across the Tasman Sea and many members had been seasick. The game was due to be played on 15 March, the day after their arrival, but most had not yet fully recovered. On top of that, wicket-keeper Ted Pooley was still in a New Zealand prison after a brawl in a Christchurch pub.

England was nonetheless favourite to win the game and the first ever Test match began with a crowd of only 1000 watching. The Australians elected Dave Gregory from New South Wales as Australia’s first ever captain and on winning the toss he decided to bat.

Charles Bannerman scored an unbeaten 165 before retiring hurt. Sydney Cricket Ground curator, Ned Gregory, playing in his one and only Test for Australia, scored Test cricket’s first duck. Australia racked up 245 and 104 while England scored 196 and 108 giving Australia victory by 45 runs. The win hinged on Bannerman’s century and a superb bowling performance by Tom Kendall who took 7 for 55 in England’s second innings.

A fortnight later there was a return game, although it was really more of a benefit for the English team. Australia included Spofforth, Murdoch and T.J.D. Cooper in the side but this time the honours went to England who won by four wickets.

Two years later Lord Harris brought another England team out and during England’s first innings in the Test at the MCG, Fred Spofforth took the first hat-trick in Test cricket. He bagged two hauls of 6 for 48 and 7 for 62 in Australia’s ten wicket win.

Through most of the 20th century, the Melbourne Cricket Ground was one of the two major Test venues in Australia (along with the Sydney Cricket Ground), and it would host one or two Tests in each summer in which Tests were played; since 1982, the Melbourne Cricket Ground has hosted one Test match each summer. Until 1979, the ground almost always hosted its match or one of its matches over the New Year, with the first day's play falling somewhere between 29 December and 1 January; in most years since 1980 and every year since 1995, its test has begun on Boxing Day, and it is now a standard fixture in the Australian cricket calendar and is known as the Boxing Day Test. The venue also hosts one-day international matches each year, and Twenty20 international matches most years. No other venue in Melbourne has hosted a Test, and Docklands Stadium is the only other venue to have hosted a limited-overs international.

The Victorian first-class team plays Sheffield Shield cricket at the venue during the season. Prior to Test cricket being played on Boxing Day, it was a long-standing tradition for Victoria to host New South Wales in a first-class match on Boxing Day. Victoria also played its limited overs matches at the ground. Since the introduction of the domestic Twenty20 Big Bash League (BBL) in 2011, the Melbourne Stars club has played its home matches at the ground. It is also the home ground of the Melbourne Stars Women team, which plays in the Women's Big Bash League (WBBL).

By the 1980s, the integral MCG pitch – grown from Merri Creek black soil – was considered the worst in Australia, in some matches exhibiting wildly inconsistent bounce which could see balls pass through as grubbers or rear dangerously high – a phenomenon which was put down to damage caused by footballers in winter and increased use for cricket during the summers of the 1970s. The integral pitch has since been removed and drop-in pitches have been used since 1996, generally offering consistent bounce and a fair balance between bat and ball.

The highest first class team score in history was posted at the MCG in the Boxing Day match against New South Wales in 1926–27. Victoria scored 1107 in two days, with Bill Ponsford scoring 352 and Jack Ryder scoring 295.

One of the most sensational incidents in Test cricket occurred at the MCG during the Melbourne test of the 1954–55 England tour of Australia. Big cracks had appeared in the pitch during a very hot Saturday’s play and on the rest day Sunday, groundsman Jack House watered the pitch to close them up. This was illegal and the story was leaked by The Age newspaper. The teams agreed to finish the match and England won by 128 runs after Frank Tyson took 7 for 27 in the final innings.

An incident in the second Test of the 1960–61 series involved the West Indies player Joe Solomon being given out after his hat fell on the stumps after being bowled at by Richie Benaud. The crowd sided with the West Indies over the Australians.

Not only was the first Test match played at the MCG, the first One Day International match was also played there, on 5 January 1971, between Australia and England. The match was played on what was originally scheduled to have been the fifth day of a Test match, but the Test was abandoned after the first three days were washed out. Australia won the 40-over match by 5 wickets. The next ODI was played on August 1972, some 19 months later.

In March 1977, the Australian Cricket Board assembled 218 of the surviving 224 Australia-England players for a Test match to celebrate 100 years of Test cricket between the two nations. The match was the idea of former Australian bowler and MCC committee member Hans Ebeling who had been responsible for developing the cricket museum at the MCG. The match had everything. England’s Derek Randall scored 174, Australia’s Rod Marsh also got a century, Lillee took 11 wickets, and David Hookes, in his first test, smacked five fours in a row off England captain Tony Greig’s bowling. Rick McCosker who opened for Australia suffered a fractured jaw after being hit by a sharply rising delivery. He left the field but came back in the second innings with his head swathed in bandages. Australia won the match by 45 runs, exactly the same margin as the first Test in 1877.

Another incident occurred on 1 February 1981 at the end of a one-day match between Australia and New Zealand. New Zealand, batting second, needed six runs off the last ball of the day to tie the game. Australian captain, Greg Chappell instructed his brother Trevor, who was bowling the last over, to send the last ball down underarm to prevent the New Zealand batsman, Brian McKechnie, from hitting the ball for six. Although not entirely in the spirit of the game, an underarm delivery was quite legal, so long as the arm was kept straight. The Laws of cricket have since been changed to prevent such a thing happening again. The incident has long been a sore point between Australia and New Zealand.

In February and March 1985 the Benson & Hedges World Championship of Cricket was played at the MCG, a One Day International tournament involving all of the then Test match playing countries to celebrate 150 years of the Australian state of Victoria. Some matches were also played at Sydney Cricket Ground.

The MCG hosted the 1992 Cricket World Cup Final between Pakistan and England with a crowd of more than 87,000. Pakistan won the match after an all-round performance by Wasim Akram who scored 33 runs and picked up 3 crucial wickets to make Pakistan cricket world champions for the first and as yet only time.

During the 1995 Boxing Day Test at the MCG, Australian umpire Darrell Hair called Sri Lankan spin bowler Muttiah Muralitharan for throwing the ball, rather than bowling it, seven times during the match. The other umpires did not call him once and this caused a controversy, although he was later called for throwing by other umpires seven other times in different matches.

The MCG is known for its great atmosphere, much of which is generated in the infamous Bay 13, situated almost directly opposite to the members stand. In the late 1980s, the crowd at Bay 13 would often mimic the warm up stretches performed by Merv Hughes. In a 1999 One-Day International, the behaviour of Bay 13 was so bad that Shane Warne, donning a helmet for protection, had to enter the ground from his dressing rooms and tell the crowd to settle down at the request of opposing England captain Alec Stewart.

The MCG hosted three pool games as part of the 2015 ICC Cricket World Cup as well as a quarter-final, and then the final on 29 March. Australia comfortably defeated New Zealand by seven wickets in front of an Australian record cricket crowd of 93,013.

In 2017-18 Ashes series, Alastair Cook scored the highest score by an English batsman and second double century since Wally Hammond at the ground. Steve Smith scored his 4th consecutive century at the ground (2014-2017) in reply, being the only player since Don Bradman (1928–31) to do so. Smith also lasted 1093 days, or scored 455 runs, between two wickets fallen. The match ended in a draw, dashing hopes of Australia achieving the third Ashes sweep in the 21st Century. The wicket used for the Boxing Day test was the first Australian wicket ever to be rated 'poor' by the ICC.

Despite being called the Melbourne Cricket Ground, the stadium has been and continues to be used much more often for Australian rules football. Spectator numbers for football are larger than for any other sport in Australia, and it makes more money for the MCG than any of the other sports played there.

Although the Melbourne Cricket Club members were instrumental in founding Australian Rules Football, there were understandable concerns in the early days about the damage that might be done to the playing surface if football was allowed to be played at the MCG. Therefore, football games were often played in the parklands next to the cricket ground, and this was the case for the first documented football match to be played at the ground. The match which today is considered to be the first Australian rules football, played between Scotch College and Melbourne Grammar over three Saturdays beginning 7 August 1858 was played in this area.

It wasn’t until 1869 that football was played on the MCG proper, a trial game involving a police team. It was not for another ten years, in 1879, after the formation of the Victorian Football Association, that the first official match was played on the MCG and the cricket ground itself became a regular venue for football. Two night matches were played on the ground during the year under the newly invented electric light.

In the early years, the MCG was the home ground of Melbourne Football Club, Australia’s oldest club, established in 1858 by the founder of the game itself, Thomas Wills. Melbourne won five premierships during the 1870s using the MCG as its home ground.

The first of nearly 3000 Victorian Football League/Australian Football League games to be played at the MCG was on 15 May 1897, with beating 64 to 19.

Several Australian Football League (AFL) clubs later joined Melbourne in using the MCG as their home ground for matches: (1965), (1985), (1992), (started moving in 1994, became a full-time tenant in 2000) and (2000). Melbourne used the venue as its training base until 1984, before being required to move to preserve the venue's surface when North Melbourne began playing there.

The VFL/AFL Grand Final has been played at the MCG every season since 1902, except in 1924 when no Grand Final was held because of the season's round-robin finals format (it hosted three of the six games in the Finals Series) 1942–1945, when the ground was used by the military during World War II; and in 1991, as the construction of the Great Southern Stand had temporarily reduced the ground’s capacity below that of Waverley Park. All three Grand Final Replays have been played at the MCG.

Before the ground was fully seated, the Grand Final could draw attendances above 110,000. The record for the highest attendance in the history of the sport was set in the 1970 VFL Grand Final, with 121,696 in attendance.

Since being fully seated, Grand Final attendances are typically between 95,000 and 100,000, with the record of 100,021 for the 2017 AFL Grand Final, followed by 100,016 attending the first 2010 AFL Grand Final (which ended in a draw, requiring a replay).

In the modern era, most finals games held in Melbourne have been played at the MCG. Under the current contract, ten finals (excluding the Grand Final) must be played at the MCG over a five-year period. Under previous contracts, the MCG was entitled to host at least one match in each week of the finals, which on several occasions required non-Victorian clubs to play "home" finals in Victoria. On 12 April 2018, the AFL, Victorian Government and Melbourne Cricket Club (MCC) announced that the MCG would continue to host the Grand Final until at least 2057.

All Melbourne based teams (and most of the time Geelong) play their 'home' finals at the MCG unless if 4 Victorian teams win the right to host a final in the first week of the finals.

For many years the VFL had an uneasy relationship with the MCG trustees and the Melbourne Cricket Club. Both needed the other, but resented the dependence. The VFL made the first move which brought things to a head by beginning the development of VFL Park at Mulgrave in the 1960s as its own home ground and as a potential venue for future grand finals. Then in 1983, president of the VFL, Allen Aylett started to pressure the MCG Trust to give the VFL a greater share of the money it made from using the ground for football.

In March 1983 the MCG trustees met to consider a submission from Aylett. Aylett said he wanted the Melbourne Cricket Club’s share of revenue cut from 15 per cent to 10 per cent. He threatened to take the following day’s opening game of the season, Collingwood vs Melbourne, away from the MCG. The money was held aside until an agreement could be reached.

Different deals, half deals and possible deals were done over the years, with the Premier of Victoria, John Cain, Jr., even becoming involved. Cain was said to have promised the VFL it could use the MCG for six months of the year and then hand it back to the MCC, but this never eventuated, as the MCG Trust did not approve it. In the mid-1980s, a deal was done where the VFL was given its own members area in the Southern Stand.

Against this background of political manoeuvring, in 1985 became the third club to make the MCG its home ground. In the same year, North played in the first night football match at the MCG for almost 110 years, against Collingwood on 29 March 1985.

In 1986, only a month after Ross Oakley had taken over as VFL Commissioner, VFL executives met with the MCC and took a big step towards resolving their differences. Changes in the personnel at the MCC also helped. In 1983 John Lill was appointed secretary and Don Cordner its president.

Shortly after the Southern Stand opened in 1992, the Australian Football League moved its headquarters into the complex. The AFL assisted with financing the new stand and came to an agreement that ensures at least 45 AFL games are played at the MCG each year, including the Grand Final in September. Another 45 days of cricket are also played there each year and more than 3.5 million spectators come to watch every year.

As of the end of 2011, Matthew Richardson holds the records for having scored the most goals on the MCG, and Kevin Bartlett holds the record for playing the most matches at the MCG. Two players have scored 14 goals for an AFL or VFL game in one match at the MCG: Gary Ablett, Sr. in 1989 and 1993, and John Longmire in 1990.

Before an AFL match between and on 27 August 1999, the city end scoreboard caught on fire due to an electrical fault, causing the start of play to be delayed by half an hour.

During World War II, the government requisitioned the MCG for military use. From 1942 until 1945 it was occupied by (in order): the United States Army Air Forces, the Royal Australian Air Force, the United States Marine Corps and again the RAAF. Over the course of the war, more than 200,000 personnel were barracked at the MCG. From April to October 1942, the US Army’s Fifth Air Force occupied the ground, naming it "Camp Murphy", in honor of officer Colonel William Murphy, a senior USAAF officer killed in Java. In 1943 the MCG was home to the legendary First Regiment of the First Division of the United States Marine Corps. The First Marine Division were the heroes of the Guadalcanal campaign and used the "cricket grounds", as the marines referred to it, to rest and recuperate. On 14 March 1943 the marines hosted a giant "get together" of American and Australian troops on the arena.

In 1977, Melbourne Cricket Club president Sir Albert Chadwick and Medal of Honor recipient, Colonel Mitchell Paige, unveiled a commemorative plaque recognizing the Americans' time at the ground.

In episode 3 of the 2010 TV miniseries, "The Pacific", members of the US Marines are shown to be camped in the war-era MCG.

The MCG’s most famous moment in history was as the main stadium for the 1956 Olympic Games. The MCG was only one of seven possible venues, including the Melbourne Showgrounds, for the Games’ main arena. The MCG was the Federal Government’s preferred venue but there was resistance from the MCC. The inability to decide on the central venue nearly caused the Games to be moved from Melbourne. Prime Minister Robert Menzies recognised the potential embarrassment to Australia if this happened and organised a three-day summit meeting to thrash things out. Attending was Victorian Premier John Cain, Sr., the Prime Minister, deputy opposition leader Arthur Calwell, all State political leaders, civic leaders, Olympic officials and trustees and officials of the MCC. Convening the meeting was no small effort considering the calibre of those attending and that many of the sports officials were only part-time amateurs.

As 22 November, the date of the opening ceremony, drew closer, Melbourne was gripped ever more tightly by Olympic fever. At 3 pm the day before the opening ceremony, people began to line up outside the MCG gates. That night the city was paralysed by a quarter of a million people who had come to celebrate.

The MCG's capacity was increased by the new Olympic (or Northern) Stand, and on the day itself 103,000 people filled the stadium to capacity. A young up and coming distance runner was chosen to carry the Olympic torch into the stadium for the opening ceremony.

Although Ron Clarke had a number of junior world records for distances of 1500 m, one mile (1.6 km) and two miles (3 km), he was relatively unknown in 1956. Perhaps the opportunity to carry the torch inspired him because he went on to have a career of exceptional brilliance and was without doubt the most outstanding runner of his day. At one stage he held the world record for every distance from two miles (3 km) to 20 km. His few failures came in Olympic and Commonwealth Games competition. Although favourite for the gold at Tokyo in 1964 he was placed ninth in the 5,000 metres race and the marathon and third in the 10,000 metres. He lost again in the 1966 Commonwealth Games and in 1968 at altitude in Mexico he collapsed at the end of the 10 km race.

On that famous day in Melbourne in 1956 the torch spluttered and sparked, showering Clarke with hot magnesium, burning holes in his shirt. When he dipped the torch into the cauldron it burst into flame singeing him further. In the centre of the ground, John Landy, the fastest miler in the world, took the Olympic oath and sculler Merv Wood carried the Australian flag.

The Melbourne Games also saw the high point of Australian female sprinting with Betty Cuthbert winning three gold medals at the MCG. She won the 100 m and 200 m and anchored the winning 4 x 100 m team. Born in Merrylands in Sydney’s west she was a champion schoolgirl athlete and had already broken the world record for the 200 m just before the 1956 Games. She was to be overshadowed by her Western Suburbs club member, the Marlene Matthews. When they got to the Games, Matthews was the overwhelming favourite especially for the 100 m a distance over which Cuthbert had beaten her just once.

Both Matthews and Cuthbert won their heats with Matthews setting an Olympic record of 11.5 seconds in hers. Cuthbert broke that record in the following heat with a time of 11.4 seconds. The world record of 11.3 was held by another Australian, Shirley Strickland who was eliminated in her heat. In the final Matthews felt she got a bad start and was last at the 50 metre mark. Cuthbert sensed Isabella Daniels from the USA close behind her and pulled out a little extra to win Australia’s first gold at the Games in a time of 11.5 seconds, Matthews was third. The result was repeated in the 200 m final. Cuthbert won her second gold breaking Marjorie Jackson’s Olympic record. Mathews was third again.

By the time the 1956 Olympics came around, Shirley Strickland was a mother of 31 years of age but managed to defend her 80 m title, which she had won in Helsinki four years before, winning gold and setting a new Olympic record.

The sensational incident of the track events was the non-selection of Marlene Matthews in the 4 x 100 m relay. Matthews trained with the relay team up until the selection was made but Cuthbert, Strickland, Fleur Mellor and Norma Croker were picked for the team. There was outrage at the selection which increased when Matthews went on to run third in both the 100 m and 200 m finals. Personally she was devastated and felt that she had been overlooked for her poor baton change. Strickland was disappointed with the way Matthews was treated and maintained it was an opinion held in New South Wales that she had baton problems. One of the selectors, Doris Magee from NSW, said that selecting Matthews increased the risk of disqualification at the change. But Cuthbert maintained that the selectors made the right choice saying that Fleur Mellor was fresh, a specialist relay runner and was better around the curves than Matthews.

The men did not fare so well. The 4 x 400 m relay team, including later IOC Committee member Kevan Gosper, won silver. Charles Porter also won silver in the high jump. Hec Hogan won bronze in the 100 m to become the first Australian man to win a medal in a sprint since the turn of the century and despite injury John Landy won bronze in the 1500 m. Allan Lawrence won bronze in the 10,000 m event.

Apart from athletics, the stadium was also used for the soccer finals, the hockey finals, the Opening and Closing Ceremonies, and an exhibition game of baseball between the Australian National Team and a US armed services team at which an estimated crowd of 114,000 attended. This was the Guinness World Record for the largest attendance for any baseball game, which stood until a 29 March 2008 exhibition game between the Boston Red Sox and Los Angeles Dodgers at the Los Angeles Coliseum (also a former Olympic venue in 1932 and 1984) drawing 115,300.

The MCG was also used for another demonstration sport, Australian Rules. The Olympics being an amateur competition meant that only amateurs could play in the demonstration game. A combined team of amateurs from the VFL and VFA were selected to play a state team from the Victorian Amateur Football Association (VAFA). The game was played 7 December 1956 with the VAFA side, wearing white jumpers, green collars and the Olympic rings on their chests, winning easily 81 to 55. One of the players chosen for the VFA side was Lindsay Gaze (although he never got off the bench) who would go on to make his mark in another sport, basketball, rather than Australian Rules.

The MCG’s link with its Olympic past continues to this day. Within its walls is the IOC-endorsed Australian Gallery of Sport and Olympic Museum.

Forty-four years later at the 2000 Summer Olympics in Sydney, the ground hosted several soccer preliminaries, making it one of a few venues ever used for more than one Olympics.

The Opening and Closing Ceremonies of the 2006 Commonwealth Games were held at the MCG, as well as athletics events during the games. The games began on 15 March and ended on 26 March.

The seating capacity of the stadium during the games was 80,000. A total of 47 events were contested, of which 24 by male and 23 by female athletes. Furthermore, three men's and three women's disability events were held within the programme. All athletics events took place within the Melbourne Cricket Ground, while the marathon and racewalking events took place on the streets of Melbourne and finished at the main stadium.

The hosts Australia easily won the medals table with 16 golds and 41 medals in total. Jamaica came second with 10 golds and 22 medals, while Kenya and England were the next best performers. A total of eleven Games records were broken over the course of the seven-day competition. Six of the records were broken by Australian athletes.

The first game of Rugby Union to be played on the ground was on Saturday, 29 June 1878, when the Waratah Club of Sydney played Carlton Football Club in a return of the previous year’s contests in Sydney where the clubs had competed in both codes of football. The match, watched by a crowd of between 6,000 and 7,000 resulted in a draw; one goal and one try being awarded to each team.

The next Rugby match was held on Wednesday 29 June 1881, when the Wanderers, a team organised under the auspices of the Melbourne Cricket Club, played a team representing a detached Royal Navy squadron then visiting Melbourne. The squadron team won by one goal and one try to nil.
It was not until 19 August 1899 that the MCG was again the venue for a Union match, this time Victoria v the British Lions (as they were later to be called). During the preceding week the Victorians had held several trial and practice matches there, as well as several training sessions, despite which they were defeated
30–0 on the day before a crowd of some 7,000.

Nine years later, on Monday, 10 August 1908, Victoria was again the host, this time to the Australian team en route to Great Britain and soon to be dubbed the First Wallabies. Despite being held on a working day some 1,500 spectators attended to see the visitors win by 26–6.

On Saturday, 6 July 1912 the MCG was the venue, for the only time ever, of a match between two Victorian Rugby Union clubs, Melbourne and East Melbourne, the former winning 9–5 in what was reported to be ‘... one of the finest exhibitions of the Rugby game ever seen in Victoria.’ It was played before a large crowd as a curtain raiser to a State Rules match against South Australia.

On Saturday 18 June 1921, in another curtain raiser, this time to a Melbourne-Fitzroy League game, a team representing Victoria was soundly beaten 51–0 by the South African Springboks in front of a crowd of 11,214.

It was nine years later, on Saturday 13 September 1930, that the British Lions returned to play Victoria, again before a crowd of 7,000, this time defeating the home side 41–36, a surprisingly narrow winning margin. 
The first post war match at the MCG was on 21 May 1949 when the NZ Maoris outclassed a Southern States side 35–8 before a crowd of close to 10,000. A year later, on 29 July 1950, for the first and only time, Queensland travelled to Victoria to play an interstate match, defeating their hosts 31–12 before a crowd of 7,479. 
In the following year the MCG was the venue for a contest between the New Zealand All Blacks and an Australian XV . This was on 30 June 1951 before some 9,000 spectators and resulted in a convincing 56–11 win for the visitors.

Union did not return the MCG until the late 1990s, for several night time Test matches, both Australia v New Zealand All Blacks as part of the Tri Nations Series. The first, on Saturday 26 July 1997, being notable for an attendance of 90,119, the visitors winning 33–18 and the second, on Saturday 11 July 1998, for a decisive victory to Australia of 24–16. Australia and New Zealand met again at the MCG during the 2007 Tri Nations Series on 30 June, the hosts again winning, this time by 20 points to 15 in front of a crowd of 79,322.

Rugby league was first played at the ground on 15 August 1914, with the New South Wales team losing to England 15–21.

The first ever State of Origin match at the MCG (and second in Melbourne) was Game II of the 1994 series, and the attendance of 87,161 set a new record rugby league crowd in Australia. The MCG was also the venue for Game II of the 1995 State of Origin series and drew 52,994, the most of any game that series. The second game of the 1997 State of Origin series, which, due to the Super League war only featured Australian Rugby League-signed players, was played there too, but only attracted 25,105, the lowest in a series that failed to attract over 35,000 to any game.

The Melbourne Storm played two marquee games at the MCG in 2000. This was the first time that they had played outside of their normal home ground of Olympic Park Stadium which held 18,500 people. Their first game was held on 3 March 2000 against the St. George Illawarra Dragons in a rematch of the infamous 1999 NRL Grand Final. Dragons player Anthony Mundine said the Storm were 'not worthy premiers' and they responded by running in 12 tries to two, winning 70–10 in front of 23,239 fans. This was their biggest crowd they had played against until 33,427 turned up to the 2007 Preliminary Final at Docklands Stadium which saw Melbourne defeat the Parramatta Eels 26–10. The record home and away crowd record has also been overhauled, when a match at Docklands in 2010 against St George attracted 25,480 spectators. Their second game attracted only 15,535 spectators and was up against the Cronulla Sharks on 24 June 2000. Once again, the Storm won 22–16.

It was announced in June 2014 that the ground would host its first State of Origin match since 1997. Game II of the 2015 series was played at the venue, with an all-time record State of Origin crowd of 91,513 attending the match. The attendance is 19th on the all time rugby league attendance list and the 4th highest rugby league attendance in Australia.
On 9 February 2006 Victorian premier Steve Bracks and Football Federation Australia chairman Frank Lowy announced that the MCG would host a world class soccer event each year from 2006 until 2009 inclusive. 

The agreement sees an annual fixture at the MCG, beginning with a clash between Australia and European champions Greece on 25 May 2006 in front of a sell-out crowd of 95,103, before Australia left to contest in the World Cup finals. Australia beat Greece 1–0. The Socceroos also hosted a match in 2007 against Argentina, losing 1–0, as well as 2010 FIFA World Cup qualification matches in 2009 against Japan, which attracted 81,872 fans as Australia beat Japan 2–1 via 2 Tim Cahill headers after falling behind 1–0 late in the 1st half. In 2010 it was announced that as a warm up to the 2010 FIFA World Cup which the Australians had qualified for, they would play fellow qualified nation New Zealand on 24 May at the MCG.
Other matches played at the MCG include the following:

In 1878 the Melbourne Cricket Club’s Lawn Tennis Committee laid an asphalt court at the MCG and Victoria’s first game of tennis was played there. A second court of grass was laid in 1879 and the first Victorian Championship played on it in 1880. The first inter-colonial championship was played in 1883 and the first formal inter-state match between NSW and Victoria played in 1884 with Victoria winning.

In 1889 the MCC arranged for tennis to be played at the Warehousemen’s Cricket Ground (now known as the Albert Cricket Ground), at Albert Park, rather than at the MCG.

It was at the MCG in 1869 that one of Australia’s first bicycle races was held. The event was for velocipedes, crude wooden machines with pedals on the front wheels. In 1898 the Austral Wheel Race was held at the MCG attracting a crowd of 30,000 to see cyclists race for a total of £200 in prize money.









The Tattersall’s Parade of the Champions undertaking is a gift to the people of Australia by Tattersall's and is a focal point of the Yarra Park precinct.

The MCG is a magnet for tourists worldwide and the statues reinforce the association between the elite sportsmen and women who have competed here and the stadium that rejoiced in their performances.
In 2010, the Melbourne Cricket Club (MCC) announced an expansion to the list of sporting statues placed around the MCG precinct in partnership with Australia Post.

The Australia Post Avenue of Legends project aimed to place a minimum of five statues in Yarra Park, extending from the gate 2 MCC members entrance up the avenue towards Wellington Parade. The most recent addition of Kevin Bartlett was unveiled in March 2017.


 

 


</doc>
<doc id="19766" url="https://en.wikipedia.org/wiki?curid=19766" title="Marshall Plan">
Marshall Plan

The Marshall Plan (officially the European Recovery Program, ERP) was an American initiative to aid Western Europe, in which the United States gave over $12 billion (nearly $ billion in US dollars) in economic assistance to help rebuild Western European economies after the end of World War II. The plan was in operation for four years beginning on April 3, 1948. The goals of the United States were to rebuild war-torn regions, remove trade barriers, modernize industry, improve European prosperity, and prevent the spread of Communism. The Marshall Plan required a lessening of interstate barriers, a dropping of many regulations, and encouraged an increase in productivity, trade union membership, as well as the adoption of modern business procedures.

The Marshall Plan aid was divided amongst the participant states roughly on a per capita basis. A larger amount was given to the major industrial powers, as the prevailing opinion was that their resuscitation was essential for general European revival. Somewhat more aid per capita was also directed towards the Allied nations, with less for those that had been part of the Axis or remained neutral. The largest recipient of Marshall Plan money was the United Kingdom (receiving about 26% of the total), followed by France (18%) and West Germany (11%). Some eighteen European countries received Plan benefits. Although offered participation, the Soviet Union refused Plan benefits, and also blocked benefits to Eastern Bloc countries, such as Hungary and Poland. The United States provided similar aid programs in Asia, but they were not part of the Marshall Plan.

However, its role in the rapid recovery has been debated. Most reject the idea that it alone miraculously revived Europe, since the evidence shows that a general recovery was already under way. The Marshall Plan's accounting reflects that aid accounted for less than 3% of the combined national income of the recipient countries between 1948 and 1951, which means an increase in GDP growth of only 0.3%.

After World War II, in 1947, industrialist Lewis H. Brown wrote at the request of General Lucius D. Clay, "A Report on Germany", which served as a detailed recommendation for the reconstruction of post-war Germany, and served as a basis for the Marshall Plan. The initiative was named after United States Secretary of State George Marshall. The plan had bipartisan support in Washington, where the Republicans controlled Congress and the Democrats controlled the White House with Harry S. Truman as President. The Plan was largely the creation of State Department officials, especially William L. Clayton and George F. Kennan, with help from the Brookings Institution, as requested by Senator Arthur H. Vandenberg, chairman of the Senate Foreign Relations Committee. Marshall spoke of an urgent need to help the European recovery in his address at Harvard University in June 1947. The purpose of the Marshall Plan was to aid in the economic recovery of nations after WWII and to reduce the influence of Communist parties within them. To combat the effects of the Marshall Plan, the USSR developed its own economic plan, known as the Molotov Plan, in spite of the fact that large amounts of resources from the Eastern Bloc countries to the USSR were paid as reparations, for countries participating in the Axis Power during the war.

The phrase "equivalent of the Marshall Plan" is often used to describe a proposed large-scale economic rescue program.

The reconstruction plan, developed at a meeting of the participating European states, was drafted on June 5, 1947. It offered the same aid to the Soviet Union and its allies, but they refused to accept it, as doing so would allow a degree of US control over the communist economies. In fact, the Soviet Union prevented its satellite states (i.e., East Germany, Poland, etc.) from accepting. Secretary Marshall became convinced Stalin had no interest in helping restore economic health in Western Europe.
President Harry Truman signed the Marshall Plan on April 3, 1948, granting $5 billion in aid to 16 European nations. During the four years the plan was in effect, the United States donated $17 billion (equivalent to $ billion in ) in economic and technical assistance to help the recovery of the European countries that joined the Organisation for European Economic Co-operation. The $17 billion was in the context of a US GDP of $258 billion in 1948, and on top of $17 billion in American aid to Europe between the end of the war and the start of the Plan that is counted separately from the Marshall Plan. The Marshall Plan was replaced by the Mutual Security Plan at the end of 1951; that new plan gave away about $7 billion annually until 1961 when it was replaced by another program.

The ERP addressed each of the obstacles to postwar recovery. The plan looked to the future and did not focus on the destruction caused by the war. Much more important were efforts to modernize European industrial and business practices using high-efficiency American models, reducing artificial trade barriers, and instilling a sense of hope and self-reliance.

By 1952, as the funding ended, the economy of every participant state had surpassed pre-war levels; for all Marshall Plan recipients, output in 1951 was at least 35% higher than in 1938. Over the next two decades, Western Europe enjoyed unprecedented growth and prosperity, but economists are not sure what proportion was due directly to the ERP, what proportion indirectly, and how much would have happened without it.
A common American interpretation of the program's role in European recovery was expressed by Paul Hoffman, head of the Economic Cooperation Administration, in 1949, when he told Congress Marshall aid had provided the "critical margin" on which other investment needed for European recovery depended. The Marshall Plan was one of the first elements of European integration, as it erased trade barriers and set up institutions to coordinate the economy on a continental level—that is, it stimulated the total political reconstruction of western Europe.

Belgian economic historian Herman Van der Wee concludes the Marshall Plan was a "great success"

By the end of World War II, much of Europe was devastated. Sustained aerial bombardment during the war had badly damaged most major cities, and industrial facilities were especially hard-hit. The region's trade flows had been thoroughly disrupted; millions were in refugee camps living on aid from United Nations Relief and Rehabilitation Administration and other agencies. Food shortages were severe, especially in the harsh winter of 1946–47. From July 1945 through June 1946, the United States shipped 16.5 million tons of food, primarily wheat, to Europe and Japan. It amounted to one-sixth of the American food supply and provided 35 trillion calories, enough to provide 400 calories a day for one year to 300 million people.

Especially damaged was transportation infrastructure, as railways, bridges, and docks had been specifically targeted by airstrikes, while much merchant shipping had been sunk. Although most small towns and villages had not suffered as much damage, the destruction of transportation left them economically isolated. None of these problems could be easily remedied, as most nations engaged in the war had exhausted their treasuries in the process.

The only major powers whose infrastructure had not been significantly harmed in World War II were the United States and Canada. They were much more prosperous than before the war but exports were a small factor in their economy. Much of the Marshall Plan aid would be used by the Europeans to buy manufactured goods and raw materials from the United States and Canada

Europe's economies were recovering slowly, as unemployment and food shortages led to strikes and unrest in several nations. In 1947 the European economies were still well below their pre-war levels and were showing few signs of growth. Agricultural production was 83% of 1938 levels, industrial production was 88%, and exports only 59%. In Britain the situation was not as severe.

In Germany in 1945–46 housing and food conditions were bad, as the disruption of transport, markets, and finances slowed a return to normality. In the West, bombing had destroyed 5,000,000 houses and apartments, and 12,000,000 refugees from the east had crowded in. Food production was only two-thirds of the pre-war level in 1946–48, while normal grain and meat shipments no longer arrived from the East. The drop in food production can be attributed to a drought that killed a major portion of the wheat crop while a severe winter destroyed the majority of the wheat crop the following year. This caused most Europeans to rely on a 1,500 calorie per day diet. Furthermore, the large shipments of food stolen from occupied nations during the war no longer reached Germany. Industrial production fell more than half and reached pre-war levels only at the end of 1949.

While Germany struggled to recover from the destruction of the War, the recovery effort began in June 1948, moving on from emergency relief. The currency reform in 1948 was headed by the military government and helped Germany to restore stability by encouraging production. The reform revalued old currency and deposits and introduced new currency. Taxes were also reduced and Germany prepared to remove economic barriers.

During the first three years of occupation of Germany, the UK and US vigorously pursued a military disarmament program in Germany, partly by removal of equipment but mainly through an import embargo on raw materials, part of the Morgenthau Plan approved by President Franklin D. Roosevelt.

Nicholas Balabkins concludes that "as long as German industrial capacity was kept idle the economic recovery of Europe was delayed." By July 1947 Washington realized that economic recovery in Europe could not go forward without the reconstruction of the German industrial base, deciding that an "orderly, prosperous Europe requires the economic contributions of a stable and productive Germany." In addition, the strength of Moscow-controlled communist parties in France and Italy worried Washington.

In the view of the State Department under President Harry S Truman, the United States needed to adopt a definite position on the world scene or fear losing credibility. The emerging doctrine of containment (as opposed to rollback) argued that the United States needed to substantially aid non-communist countries to stop the spread of Soviet influence. There was also some hope that the Eastern Bloc nations would join the plan, and thus be pulled out of the emerging Soviet bloc, but that did not happen.
In January 1947, Truman appointed retired General George Marshall as Secretary of State. In July 1947 Marshall scrapped Joint Chiefs of Staff Directive 1067 implemented as part of the Morgenthau Plan under the personal supervision of Roosevelt's treasury secretary Henry Morgenthau, Jr., which had decreed "take no steps looking toward the economic rehabilitation of Germany [or] designed to maintain or strengthen the German economy." Thereafter, JCS 1067 was supplanted by JCS 1779, stating that "an orderly and prosperous Europe requires the economic contributions of a stable and productive Germany." The restrictions placed on German heavy industry production were partly ameliorated; permitted steel production levels were raised from 25% of pre-war capacity to a new limit placed at 50% of pre-war capacity.

With a communist insurgency threatening Greece, and Britain financially unable to continue its aid, the President announced his Truman Doctrine on March 12, 1947, "to support free peoples who are resisting attempted subjugation by armed minorities or by outside pressures", with an aid request for consideration and decision, concerning Greece and Turkey. Also in March 1947, former US President Herbert Hoover, in one of his reports from Germany, argued for a change in US occupation policy, among other things stating:

There is the illusion that the New Germany left after the annexations can be reduced to a 'pastoral state' (Morgenthau's vision). It cannot be done unless we exterminate or move 25,000,000 people out of it.

Hoover further noted that, "The whole economy of Europe is interlinked with German economy through the exchange of raw materials and manufactured goods. The productivity of Europe cannot be restored without the restoration of Germany as a contributor to that productivity." Hoover's report led to a realization in Washington that a new policy was needed; "almost any action would be an improvement on current policy." In Washington, the Joint Chiefs declared that the "complete revival of German industry, particularly coal mining" was now of "primary importance" to American security.

The United States was already spending a great deal to help Europe recover. Over $14 billion was spent or loaned during the postwar period through the end of 1947 and is not counted as part of the Marshall Plan. Much of this aid was designed to restore infrastructure and help refugees. Britain, for example, received an emergency loan of $3.75 billion.

The United Nations also launched a series of humanitarian and relief efforts almost wholly funded by the United States. These efforts had important effects, but they lacked any central organization and planning, and failed to meet many of Europe's more fundamental needs. Already in 1943, the United Nations Relief and Rehabilitation Administration (UNRRA) was founded to provide relief to areas liberated from Germany. UNRRA provided billions of dollars of rehabilitation aid and helped about 8 million refugees. It ceased operation of displaced persons camps in Europe in 1947; many of its functions were transferred to several UN agencies.

After Marshall's appointment in January 1947, administration officials met with Soviet Foreign Minister Vyacheslav Molotov and others to press for an economically self-sufficient Germany, including a detailed accounting of the industrial plants, goods and infrastructure already removed by the Soviets in their occupied zone. Molotov refrained from supplying accounts of Soviet assets. The Soviets took a punitive approach, pressing for a delay rather than an acceleration in economic rehabilitation, demanding unconditional fulfillment of all prior reparation claims, and pressing for progress toward nationwide socioeconomic transformation.

After six weeks of negotiations, Molotov rejected all of the American and British proposals. Molotov also rejected the counter-offer to scrap the British-American "Bizonia" and to include the Soviet zone within the newly constructed Germany. Marshall was particularly discouraged after personally meeting with Stalin to explain that the United States could not possibly abandon its position on Germany, while Stalin expressed little interest in a solution to German economic problems.

After the adjournment of the Moscow conference following six weeks of failed discussions with the Soviets regarding a potential German reconstruction, the United States concluded that a solution could not wait any longer.

To clarify the US's position, a major address by Secretary of State George Marshall was planned. Marshall gave the address to the graduating class of Harvard University on June 5, 1947. Standing on the steps of Memorial Church in Harvard Yard, he offered American aid to promote European recovery and reconstruction. The speech described the dysfunction of the European economy and presented a rationale for US aid.

The modern system of the division of labor upon which the exchange of products is based is in danger of breaking down. ... Aside from the demoralizing effect on the world at large and the possibilities of disturbances arising as a result of the desperation of the people concerned, the consequences to the economy of the United States should be apparent to all. It is logical that the United States should do whatever it is able to do to assist in the return of normal economic health to the world, without which there can be no political stability and no assured peace. Our policy is not directed against any country, but against hunger, poverty, desperation and chaos. Any government that is willing to assist in recovery will find full co-operation on the part of the USA. Its purpose should be the revival of a working economy in the world so as to permit the emergence of political and social conditions in which free institutions can exist.

Marshall was convinced that economic stability would provide political stability in Europe. He offered aid, but the European countries had to organize the program themselves.

The speech, written by Charles Bohlen, contained virtually no details and no numbers. More a proposal than a plan, it was a challenge to European leaders to cooperate and coordinate. It asked Europeans to create their own plan for rebuilding Europe, indicating the United States would then fund this plan. The administration felt that the plan would likely be unpopular among many Americans, and the speech was mainly directed at a European audience. In an attempt to keep the speech out of American papers, journalists were not contacted, and on the same day, Truman called a press conference to take away headlines. In contrast, Dean Acheson, an Under Secretary of State, was dispatched to contact the European media, especially the British media, and the speech was read in its entirety on the BBC.

British Foreign Secretary Ernest Bevin heard Marshall's radio broadcast speech and immediately contacted French Foreign Minister Georges Bidault to begin preparing a quick European response to (and acceptance of) the offer, which led to the creation of the Committee of European Economic Co-operation. The two agreed that it would be necessary to invite the Soviets as the other major allied power. Marshall's speech had explicitly included an invitation to the Soviets, feeling that excluding them would have been a sign of distrust. State Department officials, however, knew that Stalin would almost certainly not participate and that any plan that would send large amounts of aid to the Soviets was unlikely to get Congressional approval.

Speaking at the Paris Peace Conference on October 10, 1946 Molotov had already stated Soviet fears: " If American capital was given a free hand in the small states ruined and enfeebled by the war [it] would buy up the local industries, appropriate the more attractive Rumanian, Yugoslav […] enterprises and would become the master in these small states." While the Soviet ambassador in Washington suspected that the Marshall Plan could lead to the creation of an anti-Soviet bloc, Stalin was open to the offer. He directed that—in negotiations to be held in Paris regarding the aid—countries in the Eastern Bloc should not reject economic conditions being placed upon them. Stalin only changed his outlook when he learned that (a) credit would only be extended under conditions of economic cooperation and, (b) aid would also be extended to Germany in total, an eventuality which Stalin thought would hamper the Soviets' ability to exercise influence in western Germany.

Initially, Stalin maneuvered to kill the Plan, or at least hamper it by means of destructive participation in the Paris talks regarding conditions. He quickly realized, however, that this would be impossible after Molotov reported—following his arrival in Paris in July 1947—that conditions for the credit were non-negotiable. Looming as just as large a concern was the Czechoslovak eagerness to accept the aid, as well as indications of a similar Polish attitude.

Stalin suspected a possibility that these Eastern Bloc countries might defy Soviet directives not to accept the aid, potentially causing a loss of control of the Eastern Bloc. In addition, the most important condition was that every country choosing to take advantage of the plan would need to have its economic situation independently assessed—a level of scrutiny to which the Soviets could not agree. Bevin and Bidault also insisted that any aid be accompanied by the creation of a unified European economy, something incompatible with the strict Soviet command economy.

Soviet Foreign Minister Vyacheslav Molotov left Paris, rejecting the plan. Thereafter, statements were made suggesting a future confrontation with the West, calling the United States both a "fascizing" power and the "center of worldwide reaction and anti-Soviet activity," with all U.S.-aligned countries branded as enemies. The Soviets also then blamed the United States for communist losses in elections in Belgium, France and Italy months earlier, in the spring of 1947. It claimed that "marshallization" must be resisted and prevented by any means, and that French and Italian communist parties were to take maximum efforts to sabotage the implementation of the Plan. In addition, Western embassies in Moscow were isolated, with their personnel being denied contact with Soviet officials.

On July 12, a larger meeting was convened in Paris. Every country of Europe was invited, with the exceptions of Spain (a World War II neutral that had sympathized with Axis powers) and the small states of Andorra, San Marino, Monaco, and Liechtenstein. The Soviet Union was invited with the understanding that it would likely refuse. The states of the future Eastern Bloc were also approached, and Czechoslovakia and Poland agreed to attend. In one of the clearest signs and reflections of tight Soviet control and domination over the region, Jan Masaryk, the foreign minister of Czechoslovakia, was summoned to Moscow and berated by Stalin for considering Czechoslovakia's possible involvement with and joining of the Marshall Plan. The prime minister of Poland, Józef Cyrankiewicz, was rewarded by Stalin for his country's rejection of the Plan, which came in the form of the Soviet Union's offer of a lucrative trade agreement lasting for a period of five years, a grant amounting to the approximate equivalent of $450 million (in 1948; the sum would have been $4.4 billion in 2014) in the form of long-term credit and loans and the provision of 200,000 tonnes of grain, heavy and manufacturing machinery and factories and heavy industries to Poland.

The Marshall Plan participants were not surprised when the Czechoslovakian and Polish delegations were prevented from attending the Paris meeting. The other Eastern Bloc states immediately rejected the offer. Finland also declined, to avoid antagonizing the Soviets (see also Finlandization). The Soviet Union's "alternative" to the Marshall plan, which was purported to involve Soviet subsidies and trade with western Europe, became known as the Molotov Plan, and later, the Comecon. In a 1947 speech to the United Nations, Soviet deputy foreign minister Andrei Vyshinsky said that the Marshall Plan violated the principles of the United Nations. He accused the United States of attempting to impose its will on other independent states, while at the same time using economic resources distributed as relief to needy nations as an instrument of political pressure.

Although all other Communist European Countries had deferred to Stalin and rejected the aid, the Yugoslavs, led by Josip Broz (Tito), at first went along and rejected the Marshall Plan. However, in 1948 Tito broke decisively with Stalin on other issues, making Yugoslavia an independent communist state. Yugoslavia requested American aid. American leaders were internally divided, but finally agreed and began sending money on a small scale in 1949, and on a much larger scale in 1950-53. The American aid was not part of the Marshall Plan.

In late September, the Soviet Union called a meeting of nine European Communist parties in southwest Poland. A Communist Party of the Soviet Union (CPSU) report was read at the outset to set the heavily anti-Western tone, stating now that "international politics is dominated by the ruling clique of the American imperialists" which have embarked upon the "enslavement of the weakened capitalist countries of Europe". Communist parties were to struggle against the US presence in Europe by any means necessary, including sabotage. The report further claimed that "reactionary imperialist elements throughout the world, particularly in the U.S.A., in Britain and France, had put particular hope on Germany and Japan, primarily on Hitlerite Germany—first as a force most capable of striking a blow at the Soviet Union".

Referring to the Eastern Bloc, the report stated that "the Red Army's liberating role was complemented by an upsurge of the freedom-loving peoples' liberation struggle against the fascist predators and their hirelings." It argued that "the bosses of Wall Street" were "tak[ing] the place of Germany, Japan and Italy". The Marshall Plan was described as "the American plan for the enslavement of Europe". It described the world now breaking down "into basically two camps—the imperialist and antidemocratic camp on the one hand, and the antiimperialist and democratic camp on the other".

Although the Eastern Bloc countries except Czechoslovakia had immediately rejected Marshall Plan aid, Eastern Bloc communist parties were blamed for permitting even minor influence by non-communists in their respective countries during the run up to the Marshall Plan. The meeting's chair, Andrei Zhdanov, who was in permanent radio contact with the Kremlin from whom he received instructions, also castigated communist parties in France and Italy for collaboration with those countries' domestic agendas. Zhdanov warned that if they continued to fail to maintain international contact with Moscow to consult on all matters, "extremely harmful consequences for the development of the brother parties' work" would result.

Italian and French communist leaders were prevented by party rules from pointing out that it was actually Stalin who had directed them not to take opposition stances in 1944. The French communist party, as others, was then to redirect its mission to "destroy capitalist economy" and that the Soviet Communist Information Bureau (Cominform) would take control of the French Communist Party's activities to oppose the Marshall Plan. When they asked Zhdanov if they should prepare for armed revolt when they returned home, he did not answer. In a follow-up conversation with Stalin, he explained that an armed struggle would be impossible and that the struggle against the Marshall Plan was to be waged under the slogan of national independence.

Congress, under the control of conservative Republicans, agreed to the program for multiple reasons. The 20-member conservative isolationist Senate wing of the party, based in the rural Midwest and led by Senator Kenneth S. Wherry (R-Nebraska), was outmaneuvered by the emerging internationalist wing, led by Senator Arthur H. Vandenberg (R-Michigan). The opposition argued that it would be "a wasteful 'operation rat-hole'"; that it made no sense to oppose communism by supporting the socialist governments in Western Europe; and that American goods would reach Russia and increase its war potential. Vandenberg, assisted by Senator Henry Cabot Lodge, Jr. (R-Massachusetts) admitted there was no certainty that the plan would succeed, but said it would halt economic chaos, sustain Western civilization, and stop further Soviet expansion. Senator Robert A. Taft (R-Ohio) hedged on the issue. He said it was without economic justification; however, it was "absolutely necessary" in "the world battle against communism." In the end, only 17 senators voted against it on March 13, 1948 A bill granting an initial $5 billion passed Congress with strong bipartisan support. Congress would eventually allocate $12.4 billion in aid over the four years of the plan.

Congress reflected public opinion, which resonated with the ideological argument that communism flourishes in poverty. Truman's own prestige and power had been greatly enhanced by his stunning victory in the 1948 election. Across America, multiple interest groups, including business, labor, farming, philanthropy, ethnic groups, and religious groups, saw the Marshall Plan as an inexpensive solution to a massive problem, noting it would also help American exports and stimulate the American economy as well. Major newspapers were highly supportive, including such conservative outlets as Time Magazine. Vandenberg made sure of bipartisan support on the Senate Foreign Relations Committee. The Solid Democratic South was highly supportive, the upper Midwest was dubious, but heavily outnumbered. The plan was opposed by conservatives in the rural Midwest, who opposed any major government spending program and were highly suspicious of Europeans. The plan also had some opponents on the left, led by Henry A. Wallace, the former Vice President. He said the Plan was hostile to the Soviet Union, a subsidy for American exporters, and sure to polarize the world between East and West. However, opposition against the Marshall Plan was greatly reduced by the shock of the Communist coup in Czechoslovakia in February 1948. The appointment of the prominent businessman Paul G. Hoffman as director reassured conservative businessmen that the gigantic sums of money would be handled efficiently.

Turning the plan into reality required negotiations among the participating nations. Sixteen nations met in Paris to determine what form the American aid would take, and how it would be divided. The negotiations were long and complex, with each nation having its own interests. France's major concern was that Germany not be rebuilt to its previous threatening power. The Benelux countries (Belgium, Netherlands, and Luxemburg), despite also suffering under the Nazis, had long been closely linked to the German economy and felt their prosperity depended on its revival. The Scandinavian nations, especially Sweden, insisted that their long-standing trading relationships with the Eastern Bloc nations not be disrupted and that their neutrality not be infringed.

The United Kingdom insisted on special status as a longstanding belligerent during the war, concerned that if it were treated equally with the devastated continental powers it would receive virtually no aid. The Americans were pushing the importance of free trade and European unity to form a bulwark against communism. The Truman administration, represented by William L. Clayton, promised the Europeans that they would be free to structure the plan themselves, but the administration also reminded the Europeans that implementation depended on the plan's passage through Congress. A majority of Congress members were committed to free trade and European integration, and were hesitant to spend too much of the money on Germany. However, before the Marshall Plan was in effect, France, Austria, and Italy needed immediate aid. On December 17, 1947, the United States agreed to give $40 million to France, Austria, China, and Italy.

Agreement was eventually reached and the Europeans sent a reconstruction plan to Washington, which was formulated and agreed upon by the Committee of European Economic Co-operation in 1947. In the document, the Europeans asked for $22 billion in aid. Truman cut this to $17 billion in the bill he put to Congress.
On March 17, 1948, Truman addressed European security and condemned the Soviet Union before a hastily convened Joint Session of Congress. Attempting to contain spreading Soviet influence in Eastern Bloc, Truman asked Congress to restore a peacetime military draft and to swiftly pass the Economic Cooperation Act, the name given to the Marshall Plan. Of the Soviet Union Truman said, "The situation in the world today is not primarily the result of the natural difficulties which follow a great war. It is chiefly due to the fact that one nation has not only refused to cooperate in the establishment of a just and honorable peace but—even worse—has actively sought to prevent it."

Members of the Republican-controlled 80th Congress (1947–1949) were skeptical. "In effect, he told the Nation that we have lost the peace, that our whole war effort was in vain.", noted Representative Frederick Smith of Ohio. Others thought he had not been forceful enough to contain the USSR. "What [Truman] said fell short of being tough", noted Representative Eugene Cox, a Democrat from Georgia, "there is no prospect of ever winning Russian cooperation." Despite its reservations, the 80th Congress implemented Truman's requests, further escalating the Cold War with the USSR.

Truman signed the Economic Cooperation Act into law on April 3, 1948; the Act established the Economic Cooperation Administration (ECA) to administer the program. ECA was headed by economic cooperation administrator Paul G. Hoffman. In the same year, the participating countries (Austria, Belgium, Denmark, France, West Germany, the United Kingdom, Greece, Iceland, Ireland, Italy, Luxembourg, the Netherlands, Norway, Sweden, Switzerland, Turkey, and the United States) signed an accord establishing a master financial-aid-coordinating agency, the Organization for European Economic Cooperation (later called the Organization for Economic Cooperation and Development or OECD), which was headed by Frenchman Robert Marjolin.

The first substantial aid went to Greece and Turkey in January 1947, which were seen as the front line of the battle against communist expansion, and were already receiving aid under the Truman Doctrine. Initially, Britain had supported the anti-communist factions in those countries, but due to its dire economic condition it decided to pull out and in February 1947 requested the US to continue its efforts. The ECA formally began operation in July 1948.

The ECA's official mission statement was to give a boost to the European economy: to promote European production, to bolster European currency, and to facilitate international trade, especially with the United States, whose economic interest required Europe to become wealthy enough to import US goods. Another unofficial goal of ECA (and of the Marshall Plan) was the containment of growing Soviet influence in Europe, evident especially in the growing strength of communist parties in Czechoslovakia, France, and Italy.

The Marshall Plan money was transferred to the governments of the European nations. The funds were jointly administered by the local governments and the ECA. Each European capital had an ECA envoy, generally a prominent American businessman, who would advise on the process. The cooperative allocation of funds was encouraged, and panels of government, business, and labor leaders were convened to examine the economy and see where aid was needed.

The Marshall Plan aid was mostly used for the purchase of goods from the United States. The European nations had all but exhausted their foreign exchange reserves during the war, and the Marshall Plan aid represented almost their sole means of importing goods from abroad. At the start of the plan, these imports were mainly much-needed staples such as food and fuel, but later the purchases turned towards reconstruction needs as was originally intended. In the latter years, under pressure from the United States Congress and with the outbreak of the Korean War, an increasing amount of the aid was spent on rebuilding the militaries of Western Europe. Of the some $13 billion allotted by mid-1951, $3.4 billion had been spent on imports of raw materials and semi-manufactured products; $3.2 billion on food, feed, and fertilizer; $1.9 billion on machines, vehicles, and equipment; and $1.6 billion on fuel.

Also established were counterpart funds, which used Marshall Plan aid to establish funds in the local currency. According to ECA rules, recipients had to invest 60% of these funds in industry. This was prominent in Germany, where these government-administered funds played a crucial role in lending money to private enterprises which would spend the money rebuilding. These funds played a central role in the reindustrialization of Germany. In 1949–50, for instance, 40% of the investment in the German coal industry was by these funds.

The companies were obligated to repay the loans to the government, and the money would then be lent out to another group of businesses. This process has continued to this day in the guise of the state-owned KfW bank, (Kreditanstalt für Wiederaufbau, meaning Reconstruction Credit Institute). The Special Fund, then supervised by the Federal Economics Ministry, was worth over DM 10 billion in 1971. In 1997 it was worth DM 23 billion. Through the revolving loan system, the Fund had by the end of 1995 made low-interest loans to German citizens amounting to around DM 140 billion. The other 40% of the counterpart funds were used to pay down the debt, stabilize the currency, or invest in non-industrial projects. France made the most extensive use of counterpart funds, using them to reduce the budget deficit. In France, and most other countries, the counterpart fund money was absorbed into general government revenues, and not recycled as in Germany.

The Netherlands received US aid for economic recovery in the Netherlands Indies. However, in January 1949, the American government suspended this aid in response to the Dutch efforts to restore colonial rule in Indonesia during the Indonesian National Revolution, and it implicitly threatened to suspend Marshall aid to the Netherlands if the Dutch government continued to oppose the independence of Indonesia.

At the time the United States was a significant oil producing nation — one of the goals of the Marshall Plan was for Europe to use oil in place of coal, but the Europeans wanted to buy crude oil and use the Marshall Plan funds to build refineries instead. However, when independent American oil companies complained, the ECA denied funds for European refinery construction.

The US Bureau of Labor Statistics (BLS) contributed heavily to the success of the Technical Assistance Program. The United States Congress passed a law on June 7, 1940 that allowed the BLS to "make continuing studies of labor productivity" and appropriated funds for the creation of a Productivity and Technological Development Division. The BLS could then use its expertise in the field of productive efficiency to implement a productivity drive in each Western European country receiving Marshall Plan aid. Counterpart funds were used to finance large-scale tours of American industry. France, for example, sent 500 missions with 4700 businessmen and experts to tour American factories, farms, stores, and offices. They were especially impressed with the prosperity of American workers, and how they could purchase an inexpensive new automobile for nine months work, compared to 30 months in France.

By implementing technological literature surveys and organized plant visits, American economists, statisticians, and engineers were able to educate European manufacturers in statistical measurement. The goal of the statistical and technical assistance from the Americans was to increase productive efficiency of European manufacturers in all industries.

To conduct this analysis, the BLS performed two types of productivity calculations. First, they used existing data to calculate how much a worker produces per hour of work—the average output rate. Second, they compared the existing output rates in a particular country to output rates in other nations. By performing these calculations across all industries, the BLS was able to identify the strengths and weaknesses of each country's manufacturing and industrial production. From that, the BLS could recommend technologies (especially statistical) that each individual nation could implement. Often, these technologies came from the United States; by the time the Technical Assistance Program began, the United States used statistical technologies "more than a generation ahead of what [the Europeans] were using".

The BLS used these statistical technologies to create Factory Performance Reports for Western European nations. The American government sent hundreds of technical advisers to Europe to observe workers in the field. This on-site analysis made the Factory Performance Reports especially helpful to the manufacturers. In addition, the Technical Assistance Program funded 24,000 European engineers, leaders, and industrialists to visit America and tour America's factories, mines, and manufacturing plants. This way, the European visitors would be able to return to their home countries and implement the technologies used in the United States. The analyses in the Factory Performance Reports and the "hands-on" experience had by the European productivity teams effectively identified productivity deficiencies in European industries; from there, it became clearer how to make European production more effective.

Before the Technical Assistance Program even went into effect, United States Secretary of Labor Maurice Tobin expressed his confidence in American productivity and technology to both American and European economic leaders. He urged that the United States play a large role in improving European productive efficiency by providing four recommendations for the program's administrators:

The effects of the Technical Assistance Program were not limited to improvements in productive efficiency. While the thousands of European leaders took their work/study trips to the United States, they were able to observe a number of aspects of American society as well. The Europeans could watch local, state, and federal governments work together with citizens in a pluralist society. They observed a democratic society with open universities and civic societies in addition to more advanced factories and manufacturing plants. The Technical Assistance Program allowed Europeans to bring home many types of American ideas.

Another important aspect of the Technical Assistance Program was its low cost. While $19.4 billion was allocated for capital costs in the Marshall Plan, the Technical Assistance Program only required $300 million. Only one-third of that $300 million cost was paid by the United States.

Even while the Marshall Plan was being implemented, the dismantling of ostensibly German industry continued; and in 1949 Konrad Adenauer, an opponent to Hitler's regime and the head of the Christian Democratic Union, wrote to the Allies requesting the end of industrial dismantling, citing the inherent contradiction between encouraging industrial growth and removing factories, and also the unpopularity of the policy. Adenauer had been released from prison, only to discover that the Soviets had effectively divided Europe with Germany divided even further. Support for dismantling was by this time coming predominantly from the French, and the Petersberg Agreement of November 1949 greatly reduced the levels of deindustrialization, though dismantling of minor factories continued until 1951. The first "level of industry" plan, signed by the Allies on March 29, 1946, had stated that German heavy industry was to be lowered to 50% of its 1938 levels by the destruction of 1,500 listed manufacturing plants. Marshall Plan played a huge role in post-war recovery for Europe in general. 1948, conditions were improving, European workers exceeded by 20 percent from the earning from the west side. Thanks to the Plan, during 1952, it went up 35 percent of the industrial and agricultural.

In January 1946 the Allied Control Council set the foundation of the future German economy by putting a cap on German steel production. The maximum allowed was set at about 5,800,000 tons of steel a year, equivalent to 25% of the pre-war production level. The UK, in whose occupation zone most of the steel production was located, had argued for a more limited capacity reduction by placing the production ceiling at 12 million tons of steel per year, but had to submit to the will of the US, France and the Soviet Union (which had argued for a 3 million ton limit). Steel plants thus made redundant were to be dismantled. Germany was to be reduced to the standard of life it had known at the height of the Great Depression (1932). Consequently, car production was set to 10% of pre-war levels, and the manufacture of other commodities was reduced as well.

The first "German level of industry" plan was subsequently followed by a number of new ones, the last signed in 1949. By 1950, after the virtual completion of the by then much watered-down "level of industry" plans, equipment had been removed from 706 manufacturing plants in western Germany and steel production capacity had been reduced by 6,700,000 tons. Vladimir Petrov concludes that the Allies "delayed by several years the economic reconstruction of the war-torn continent, a reconstruction which subsequently cost the United States billions of dollars." In 1951 West Germany agreed to join the European Coal and Steel Community (ECSC) the following year. This meant that some of the economic restrictions on production capacity and on actual production that were imposed by the International Authority for the Ruhr were lifted, and that its role was taken over by the ECSC.

The Marshall Plan aid was divided among the participant states on a roughly per capita basis. A larger amount was given to the major industrial powers, as the prevailing opinion was that their resuscitation was essential for general European revival. Somewhat more aid per capita was also directed towards the Allied nations, with less for those that had been part of the Axis or remained neutral. The exception was Iceland, which had been neutral during the war, but received far more on a per capita basis than the second highest recipient. The table below shows Marshall Plan aid by country and year (in millions of dollars) from "The Marshall Plan Fifty Years Later." There is no clear consensus on exact amounts, as different scholars differ on exactly what elements of American aid during this period were part of the Marshall Plan.

The Marshall Plan, just as GARIOA, consisted of aid both in the form of grants and in the form of loans. Out of the total, 1.2 billion USD were loan-aid.

Ireland which received 146.2 million USD through the Marshall Plan, received 128.2 million USD as loans, and the remaining 18 million USD as grants. By 1969 the Irish Marshall Plan debt, which was still being repaid, amounted to 31 million pounds, out of a total Irish foreign debt of 50 million pounds.

The UK received 385 million USD of its Marshall Plan aid in the form of loans. Unconnected to the Marshall Plan the UK also received direct loans from the US amounting to 4.6 billion USD. The proportion of Marshall Plan loans versus Marshall Plan grants was roughly 15% to 85% for both the UK and France.

Germany, which up until the 1953 Debt agreement had to work on the assumption that all the Marshall Plan aid was to be repaid, spent its funds very carefully. Payment for Marshall Plan goods, "counterpart funds", were administered by the Reconstruction Credit Institute, which used the funds for loans inside Germany. In the 1953 Debt agreement, the amount of Marshall plan aid that Germany was to repay was reduced to less than 1 billion USD. This made the proportion of loans versus grants to Germany similar to that of France and the UK. The final German loan repayment was made in 1971. Since Germany chose to repay the aid debt out of the German Federal budget, leaving the German ERP fund intact, the fund was able to continue its reconstruction work. By 1996 it had accumulated a value of 23 billion Deutsche Mark.

The Central Intelligence Agency received 5% of the Marshall Plan funds (cca 685 million USD), which it used to finance secret operations abroad. Through the Office of Policy Coordination, headed by Frank Wisner, the money was directed towards covert operations that entailed the creation of false front organizations to manipulate the affairs of foreign countries, including by initiating propaganda campaigns against governments, creating illegal underground opposition groups, and infiltrating local organizations, such as labor unions.

Marshall Plan funds were also used to support a Ukrainian right-wing guerilla group called Nightingale. The Nightingale battalion (Nachtigall Battalion) had been established by the invading Nazis in Spring 1941 to create a common front with Ukrainian fascists and other right-wing radical Ukrainian nationalists of the Ukrainian National Organization against the Soviets. The US point-person for Nightingale was Mykola Lebed (sometimes spelled Mikola Lebed), founder of the terrorist arm of the Ukrainian Insurgent Army, which had murdered thousands of Jewish people, Russians and Poles during WWII. Starting in September 1949, the US began parachuting Ukrainian militants into Ukraine, an operation that continued for five years. The aim of the CIA infiltration operation, ultimately unsuccessful, was to dislodge Moscow's control over Ukraine.

The Marshall Plan was originally scheduled to end in 1953. Any effort to extend it was halted by the growing cost of the Korean War and rearmament. American Republicans hostile to the plan had also gained seats in the 1950 Congressional elections, and conservative opposition to the plan was revived. Thus the plan ended in 1951, though various other forms of American aid to Europe continued afterwards.

The years 1948 to 1952 saw the fastest period of growth in European history. Industrial production increased by 35%. Agricultural production substantially surpassed pre-war levels. The poverty and starvation of the immediate postwar years disappeared, and Western Europe embarked upon an unprecedented two decades of growth that saw standards of living increase dramatically. Additionally, the long-term effect of economic integration raised European income levels substantially, by nearly 20 percent by the mid-1970s. There is some debate among historians over how much this should be credited to the Marshall Plan. Most reject the idea that it alone miraculously revived Europe, as evidence shows that a general recovery was already underway. Most believe that the Marshall Plan sped this recovery, but did not initiate it. Many argue that the structural adjustments that it forced were of great importance. Economic historians J. Bradford DeLong and Barry Eichengreen call it "history's most successful structural adjustment program." One effect of the plan was that it subtly "Americanized" European countries, especially Austria, through new exposure to American popular culture, including the growth in influence of Hollywood movies and rock n' roll.

The political effects of the Marshall Plan may have been just as important as the economic ones. Marshall Plan aid allowed the nations of Western Europe to relax austerity measures and rationing, reducing discontent and bringing political stability. The communist influence on Western Europe was greatly reduced, and throughout the region, communist parties faded in popularity in the years after the Marshall Plan. The trade relations fostered by the Marshall Plan helped forge the North Atlantic alliance that would persist throughout the Cold War in the form of NATO. At the same time, the nonparticipation of the states of the Eastern Bloc was one of the first clear signs that the continent was now divided.

The Marshall Plan also played an important role in European integration. Both the Americans and many of the European leaders felt that European integration was necessary to secure the peace and prosperity of Europe, and thus used Marshall Plan guidelines to foster integration. In some ways, this effort failed, as the OEEC never grew to be more than an agent of economic cooperation. Rather, it was the separate European Coal and Steel Community, which notably excluded Britain, that would eventually grow into the European Union. However, the OEEC served as both a testing and training ground for the structures that would later be used by the European Economic Community. The Marshall Plan, linked into the Bretton Woods system, also mandated free trade throughout the region.

While some historians today feel some of the praise for the Marshall Plan is exaggerated, it is still viewed favorably and many thus feel that a similar project would help other areas of the world. After the fall of communism, several proposed a "Marshall Plan for Eastern Europe" that would help revive that region. Others have proposed a Marshall Plan for Africa to help that continent, and US Vice President Al Gore suggested a Global Marshall Plan. "Marshall Plan" has become a metaphor for any very large-scale government program that is designed to solve a specific social problem. It is usually used when calling for federal spending to correct a perceived failure of the private sector.

The Marshall Plan money was in the form of grants from the U.S. Treasury that did not have to be repaid. The Organisation for European Economic Co-operation took the leading role in allocating funds, and the OEEC arranged for the transfer of the goods. The American supplier was paid in dollars, which were credited against the appropriate European Recovery Program funds. The European recipient, however, was not given the goods as a gift but had to pay for them (usually on credit) in local currency. These payments were kept by the European government involved in a special counterpart fund. This counterpart money, in turn, could be used by the government for further investment projects. Five percent of the counterpart money was paid to the US to cover the administrative costs of the ERP. In addition to ERP grants, the Export-Import Bank (an agency of the US government) at the same time made long-term loans at low interest rates to finance major purchases in the US, all of which were repaid.

In the case of Germany, there also were 16 billion marks of debts from the 1920s which had defaulted in the 1930s, but which Germany decided to repay to restore its reputation. This money was owed to government and private banks in the US, France, and Britain. Another 16 billion marks represented postwar loans by the US. Under the London Debts Agreement of 1953, the repayable amount was reduced by 50% to about 15 billion marks and stretched out over 30 years, and compared to the fast-growing German economy were of minor impact.

Large parts of the world devastated by World War II did not benefit from the Marshall Plan. The only major Western European nation excluded was Francisco Franco's Spain, which was highly unpopular in Washington. With the escalation of the Cold War, the United States reconsidered its position, and in 1951 embraced Spain as an ally, encouraged by Franco's aggressive anti-communist policies. Over the next decade, a considerable amount of American aid would go to Spain, but less than its neighbors had received under the Marshall Plan.

The Soviet Union had been as badly affected as any part of the world by the war. The Soviets imposed large reparations payments on the Axis allies that were in its sphere of influence. Austria, Finland, Hungary, Romania, and especially East Germany were forced to pay vast sums and ship large amounts of supplies to the USSR. These reparation payments meant the Soviet Union itself received about the same as 16 European countries received in total from Marshall Plan aid.

In accordance with the agreements with the USSR, shipment of dismantled German industrial installations from the west began on March 31, 1946. Under the terms of the agreement, the Soviet Union would in return ship raw materials such as food and timber to the western zones. In view of the Soviet failure to do so, the western zones halted the shipments east, ostensibly on a temporary basis, although they were never resumed. It was later shown that the main reason for halting shipments east was not the behavior of the USSR but rather the recalcitrant behavior of France. Examples of material received by the USSR were equipment from the Kugel-Fischer ballbearing plant at Schweinfurt, the Daimler-Benz underground aircraft-engine plant at Obrigheim, the Deschimag shipyards at Bremen-Weser, and the Gendorf powerplant.

The USSR did establish COMECON as a riposte to the Marshall Plan to deliver aid for Eastern Bloc countries, but this was complicated by the Soviet efforts to manage their own recovery from the war. The members of Comecon looked to the Soviet Union for oil; in turn, they provided machinery, equipment, agricultural goods, industrial goods, and consumer goods to the Soviet Union. Economic recovery in the East was much slower than in the West, resulting in the formation of the shortage economies and a gap in wealth between East and West. Finland, which the USSR forbade to join the Marshall Plan and which was required to give large reparations to the USSR, saw its economy recover to pre-war levels in 1947. France, which received billions of dollars through the Marshall Plan, similarly saw its average income per person return to almost pre-war level by 1949. By mid-1948 industrial production in Poland, Hungary, Bulgaria, and Czechoslovakia had recovered to a level somewhat above pre-war level.

From the end of the war to the end of 1953, the US provided grants and credits amounting to $5.9 billion to Asian countries, especially China/Taiwan ($1.051 billion), India ($255 million), Indonesia ($215 million), Japan ($2.44 billion), South Korea ($894 million), Pakistan ($98 million) and the Philippines ($803 million). In addition, another $282 million went to Israel and $196 million to the rest of the Middle East. All this aid was separate from the Marshall Plan.

Canada, like the United States, was damaged little by the war and in 1945 was one of the world's richest economies. It operated its own aid program. In 1948, the US allowed ERP aid to be used in purchasing goods from Canada. Canada made over a billion dollars in sales in the first two years of operation.

The total of American grants and loans to the world from 1945 to 1953 came to $44.3 billion.

Bradford DeLong and Barry Eichengreen conclude it was "History's Most Successful Structural Adjustment Program." They state:

It was not large enough to have significantly accelerated recovery by financing investment, aiding the reconstruction of damaged infrastructure, or easing commodity bottlenecks. We argue, however, that the Marshall Plan did play a major role in setting the stage for post-World War II Western Europe's rapid growth. The conditions attached to Marshall Plan aid pushed European political economy in a direction that left its post World War II "mixed economies" with more "market" and less "controls" in the mix.

Jacob Magid argues:

There is little evidence that direct economic effects account for the Marshall Plan's success. Instead, the indirect economic effects, particularly in the implementation of liberal capitalistic policies, and the political effects, particularly the ideal of European integration and government-business partnerships, are the major reasons for Europe's unsurpassed growth.

Prior to passing and enacting the Marshall Plan, President Truman and George Marshall started a domestic overhaul of public opinion from coast to coast. The purpose of this campaign was to sway public opinion in their direction and to inform the common person of what the Marshall Plan was and what the Plan would ultimately do. They spent months attempting to convince Americans that their cause was just and that they should embrace the higher taxes that would come in the foreseeable future.

A copious amount of propaganda ended up being highly effective in swaying public opinion towards supporting the Marshall Plan. During the nationwide campaign for support, "more than a million pieces of pro-Marshall Plan publications-booklets, leaflets, reprints, and fact sheets," were disseminated. Truman and Marshall's efforts proved to be effective. In a Gallup Poll taken between the months of July and December 1947, it shows the percentage of Americans unaware of the Marshall Plan fell from 51% to 36% nationwide. By the time the Marshall Plan was ready to be implemented, there was a general consensus throughout the American public that this was the right policy for both America, and the countries who would be receiving aid.

During the period leading up to World War II, Americans were highly isolationist, and many called The Marshall Plan a "milestone" for American ideology. By looking at polling data over time from pre-World War II to post-World War II, one would find that there was a change in public opinion in regards to ideology. Americans swapped their isolationist ideals for a much more global internationalists ideology after World War II.

In a National Opinion Research Center (NORC) poll taken in April 1945, a cross-section of Americans were asked, "If our government keeps on sending lendlease materials, which we may not get paid for, to friendly countries for about three years after the war, do you think this will mean more jobs or fewer jobs for most Americans, or won't it make any difference?" 75% said the same or more jobs; 10% said fewer.

Before proposing anything to Congress in 1947, the Truman administration made an elaborate effort to organize public opinion in favor of the Marshall Plan spending, reaching out to numerous national organizations representing business, labor, farmers, women, and other interest groups. Political scientist Ralph Levering points out that:

Mounting large public relations campaigns and supporting private groups such as the Citizens Committee for the Marshall Plan, the administration carefully built public and bipartisan Congressional support before bringing these measures to a vote.

Public opinion polls in 1947 consistently showed strong support for the Marshall plan among Americans. Furthermore, Gallup polls in England, France, and Italy showed favorable majorities over 60% 

Initial criticism of the Marshall Plan came from a number of economists. Wilhelm Röpke, who influenced German Minister for Economy Ludwig Erhard in his economic recovery program, believed recovery would be found in eliminating central planning and restoring a market economy in Europe, especially in those countries which had adopted more fascist and corporatist economic policies. Röpke criticized the Marshall Plan for forestalling the transition to the free market by subsidizing the current, failing systems. Erhard put Röpke's theory into practice and would later credit Röpke's influence for West Germany's preeminent success.

Henry Hazlitt criticized the Marshall Plan in his 1947 book "Will Dollars Save the World?", arguing that economic recovery comes through savings, capital accumulation, and private enterprise, and not through large cash subsidies. Ludwig von Mises criticized the Marshall Plan in 1951, believing that "the American subsidies make it possible for [Europe's] governments to conceal partially the disastrous effects of the various socialist measures they have adopted". Some critics and Congressmen at the time believed that America was giving too much aid to Europe. America had already given Europe $9 billion in other forms of help in previous years. The Marshall Plan gave another $13 billion, equivalent to about $100 billion in 2010 value.

However, its role in the rapid recovery has been debated. Most reject the idea that it alone miraculously revived Europe since the evidence shows that a general recovery was already underway. The Marshall Plan grants were provided at a rate that was not much higher in terms of flow than the previous UNRRA aid and represented less than 3% of the combined national income of the recipient countries between 1948 and 1951, which would mean an increase in GDP growth of only 0.3%. In addition, there is no correlation between the amount of aid received and the speed of recovery: both France and the United Kingdom received more aid, but West Germany recovered significantly faster.

Criticism of the Marshall Plan became prominent among historians of the revisionist school, such as Walter LaFeber, during the 1960s and 1970s. They argued that the plan was American economic imperialism and that it was an attempt to gain control over Western Europe just as the Soviets controlled the Eastern Bloc. In a review of West Germany's economy from 1945 to 1951, German analyst Werner Abelshauser concluded that "foreign aid was not crucial in starting the recovery or in keeping it going". The economic recoveries of France, Italy, and Belgium, Cowen argues, began a few months before the flow of US money. Belgium, the country that relied earliest and most heavily on free-market economic policies after its liberation in 1944, experienced swift recovery and avoided the severe housing and food shortages seen in the rest of continental Europe.

Former US Chairman of the Federal Reserve Bank Alan Greenspan gives most credit to German Chancellor Ludwig Erhard for Europe's economic recovery. Greenspan writes in his memoir "The Age of Turbulence" that Erhard's economic policies were the most important aspect of postwar Western European recovery, even outweighing the contributions of the Marshall Plan. He states that it was Erhard's reductions in economic regulations that permitted Germany's miraculous recovery, and that these policies also contributed to the recoveries of many other European countries. Its recovery is attributed to traditional economic stimuli, such as increases in investment, fueled by a high savings rate and low taxes. Japan saw a large infusion of US investment during the Korean War.
Noam Chomsky said the Marshall Plan "set the stage for large amounts of private U.S. investment in Europe, establishing the basis for modern transnational corporations".

Alfred Friendly, press aide to the US Secretary of Commerce W. Averell Harriman, wrote a humorous operetta about the Marshall Plan during its first year; one of the lines in the operetta was: "Wines for Sale; will you swap / A little bit of steel for Chateau Neuf du Pape?"






</doc>
<doc id="19769" url="https://en.wikipedia.org/wiki?curid=19769" title="Mariculture">
Mariculture

Mariculture is a specialized branch of aquaculture involving the cultivation of marine organisms for food and other products in the open ocean, an enclosed section of the ocean, or in tanks, ponds or raceways which are filled with seawater. An example of the latter is the farming of marine fish, including finfish and shellfish like prawns, or oysters and seaweed in saltwater ponds. Non-food products produced by mariculture include: fish meal, nutrient agar, jewellery (e.g. cultured pearls), and cosmetics.

Similar to algae cultivation, shellfish can be farmed in multiple ways: on ropes, in bags or cages, or directly on (or within) the intertidal substrate. Shellfish mariculture does not require feed or fertilizer inputs, nor insecticides or antibiotics, making shellfish aquaculture (or 'mariculture') a self-supporting system. Shellfish can also be used in multi-species cultivation techniques, where shellfish can utilize waste generated by higher trophic level organisms.

After trials in 2012, a commercial "sea ranch" was set up in Flinders Bay, Western Australia to raise abalone. The ranch is based on an artificial reef made up of 5000 () separate concrete units called "abitats" (abalone habitats). The abitats can host 400 abalone each. The reef is seeded with young abalone from an onshore hatchery.

The abalone feed on seaweed that has grown naturally on the abitats; with the ecosystem enrichment of the bay also resulting in growing numbers of dhufish, pink snapper, wrasse, Samson fish among other species.

Brad Adams, from the company, has emphasised the similarity to wild abalone and the difference from shore based aquaculture. "We're not aquaculture, we're ranching, because once they're in the water they look after themselves."

Raising marine organisms under controlled conditions in exposed, high-energy ocean environments beyond significant coastal influence, is a relatively new approach to mariculture. Open ocean aquaculture (OOA) uses cages, nets, or long-line arrays that are moored, towed or float freely. Research and commercial open ocean aquaculture facilities are in operation or under development in Panama, Australia, Chile, China, France, Ireland, Italy, Japan, Mexico, and Norway. As of 2004, two commercial open ocean facilities were operating in U.S. waters, raising Threadfin near Hawaii and cobia near Puerto Rico. An operation targeting bigeye tuna recently received final approval. All U.S. commercial facilities are currently sited in waters under state or territorial jurisdiction. The largest deep water open ocean farm in the world is raising cobia 12 km off the northern coast of Panama in highly exposed sites.

Enhanced Stocking (also known as sea ranching) is a Japanese principle based on operant conditioning and the migratory nature of certain species. The fishermen raise hatchlings in a closely knitted net in a harbor, sounding an underwater horn before each feeding. When the fish are old enough they are freed from the net to mature in the open sea. During spawning season, about 80% of these fish return to their birthplace. The fishermen sound the horn and then net those fish that respond.

In seawater pond mariculture, fish are raised in ponds which receive water from the sea. This has the benefit that the nutrition (e.g. microorganisms) present in the seawater can be used. This is a great advantage over traditional fish farms (e.g. sweet water farms) for which the farmers buy feed (which is expensive). Other advantages are that water purification plants may be planted in the ponds to eliminate the buildup of nitrogen, from fecal and other contamination. Also, the ponds can be left unprotected from natural predators, providing another kind of filtering.

Mariculture has rapidly expanded over the last two decades due to new technology, improvements in formulated feeds, greater biological understanding of farmed species, increased water quality within closed farm systems, greater demand for seafood products, site expansion and government interest. As a consequence, mariculture has been subject to some controversy regarding its social and environmental impacts. Commonly identified environmental impacts from marine farms are:


As with most farming practices, the degree of environmental impact depends on the size of the farm, the cultured species, stock density, type of feed, hydrography of the site, and husbandry methods. The adjacent diagram connects these causes and effects.

Mariculture of finfish can require a significant amount of fishmeal or other high protein food sources. Originally, a lot of fishmeal went to waste due to inefficient feeding regimes and poor digestibility of formulated feeds which resulted in poor feed conversion ratios.

In cage culture, several different methods are used for feeding farmed fish – from simple hand feeding to sophisticated computer-controlled systems with automated food dispensers coupled with "in situ" uptake sensors that detect consumption rates. In coastal fish farms, overfeeding primarily leads to increased disposition of detritus on the seafloor (potentially smothering seafloor dwelling invertebrates and altering the physical environment), while in hatcheries and land-based farms, excess food goes to waste and can potentially impact the surrounding catchment and local coastal environment. This impact is usually highly local, and depends significantly on the settling velocity of waste feed and the current velocity (which varies both spatially and temporally) and depth.

The impact of escapees from aquaculture operations depends on whether or not there are wild conspecifics or close relatives in the receiving environment, and whether or not the escapee is reproductively capable. Several different mitigation/prevention strategies are currently employed, from the development of infertile triploids to land-based farms which are completely isolated from any marine environment. Escapees can adversely impact local ecosystems through hybridization and loss of genetic diversity in native stocks, increase negative interactions within an ecosystem (such as predation and competition), disease transmission and habitat changes (from trophic cascades and ecosystem shifts to varying sediment regimes and thus turbidity).

The accidental introduction of invasive species is also of concern. Aquaculture is one of the main vectors for invasives following accidental releases of farmed stocks into the wild. One example is the Siberian sturgeon ("Acipenser baerii") which accidentally escaped from a fish farm into the Gironde Estuary (Southwest France) following a severe storm in December 1999 (5,000 individual fish escaped into the estuary which had never hosted this species before). Molluscan farming is another example whereby species can be introduced to new environments by ‘hitchhiking’ on farmed molluscs. Also, farmed molluscs themselves can become dominate predators and/or competitors, as well as potentially spread pathogens and parasites.

One of the primary concerns with mariculture is the potential for disease and parasite transfer. Farmed stocks are often selectively bred to increase disease and parasite resistance, as well as improving growth rates and quality of products. As a consequence, the genetic diversity within reared stocks decreases with every generation – meaning they can potentially reduce the genetic diversity within wild populations if they escape into those wild populations. Such genetic pollution from escaped aquaculture stock can reduce the wild population’s ability to adjust to the changing natural environment. Also, maricultured species can harbour diseases and parasites (e.g., lice) which can be introduced to wild populations upon their escape. An example of this is the parasitic sea lice on wild and farmed Atlantic salmon in Canada. Also, non-indigenous species which are farmed may have resistance to, or carry, particular diseases (which they picked up in their native habitats) which could be spread through wild populations if they escape into those wild populations. Such ‘new’ diseases would be devastating for those wild populations because they would have no immunity to them.

With the exception of benthic habitats directly beneath marine farms, most mariculture causes minimal destruction to habitats. However, the destruction of mangrove forests from the farming of shrimps is of concern. Globally, shrimp farming activity is a small contributor to the destruction of mangrove forests; however, locally it can be devastating. Mangrove forests provide rich matrices which support a great deal of biodiversity – predominately juvenile fish and crustaceans. Furthermore, they act as buffering systems whereby they reduce coastal erosion, and improve water quality for in situ animals by processing material and ‘filtering’ sediments.

In addition, nitrogen and phosphorus compounds from food and waste may lead to blooms of phytoplankton, whose subsequent degradation can drastically reduce oxygen levels. If the algae are toxic, fish are killed and shellfish contaminated.

Mariculture development must be sustained by basic and applied research and development in major fields such as nutrition, genetics, system management, product handling, and socioeconomics. One approach is closed systems that have no direct interaction with the local environment. However, investment and operational cost are currently significantly higher than open cages, limiting them to their current role as hatcheries.

Sustainable mariculture promises economic and environmental benefits. Economies of scale imply that ranching can produce fish at lower cost than industrial fishing, leading to better human diets and the gradual elimination of unsustainable fisheries. Maricultured fish are also perceived to be of higher quality than fish raised in ponds or tanks, and offer more diverse choice of species. Consistent supply and quality control has enabled integration in food market channels.






Scientific literature on mariculture can be found in the following journals:



</doc>
<doc id="19770" url="https://en.wikipedia.org/wiki?curid=19770" title="Memetics">
Memetics

Memetics (also referred to colloquially as memeology) is the study of information and culture based on an analogy with Darwinian evolution. Proponents describe memetics as an approach to evolutionary models of cultural information transfer. Critics regard memetics as a pseudoscience. Memetics describes how an idea can propagate successfully, but doesn't necessarily imply a concept is factual.

The term meme was coined in Richard Dawkins' 1976 book "The Selfish Gene," but Dawkins later distanced himself from the resulting field of study. Analogous to a gene, the meme was conceived as a "unit of culture" (an idea, belief, pattern of behaviour, etc.) which is "hosted" in the minds of one or more individuals, and which can reproduce itself in the sense of jumping from the mind of one person to the mind of another. Thus what would otherwise be regarded as one individual influencing another to adopt a belief is seen as an idea-replicator reproducing itself in a new host. As with genetics, particularly under a Dawkinsian interpretation, a meme's success may be due to its contribution to the effectiveness of its host.

The Usenet newsgroup alt.memetics started in 1993 with peak posting years in the mid to late 1990s. The "Journal of Memetics" was published electronically from 1997 to 2005.

In his book "The Selfish Gene" (1976), the evolutionary biologist Richard Dawkins used the term "meme" to describe a unit of human cultural transmission analogous to the gene, arguing that replication also happens in culture, albeit in a different sense. Bella Hiscock outlined a similar hypothesis in 1975, which Dawkins referenced. Cultural evolution itself is a much older topic, with a history that dates back at least as far as Darwin's era.

Dawkins (1976) proposed that the meme is a unit of information residing in the brain and is the mutating replicator in human cultural evolution. It is a pattern that can influence its surroundings – that is, it has causal agency – and can propagate. This proposal resulted in debate among sociologists, biologists, and scientists of other disciplines. Dawkins himself did not provide a sufficient explanation of how the replication of units of information in the brain controls human behaviour and ultimately culture, and the principal topic of the book was genetics. Dawkins apparently did not intend to present a comprehensive theory of "memetics" in "The Selfish Gene", but rather coined the term "meme" in a speculative spirit. Accordingly, different researchers came to define the term "unit of information" in different ways.

The modern memetics movement dates from the mid-1980s. A January 1983 "Metamagical Themas" column by Douglas Hofstadter, in "Scientific American", was influential – as was his 1985 book of the same name. "Memeticist" was coined as analogous to "geneticist" – originally in "The Selfish Gene." Later Arel Lucas suggested that the discipline that studies memes and their connections to human and other carriers of them be known as "memetics" by analogy with "genetics". Dawkins' "The Selfish Gene" has been a factor in attracting the attention of people of disparate intellectual backgrounds. Another stimulus was the publication in 1991 of "Consciousness Explained" by Tufts University philosopher Daniel Dennett, which incorporated the meme concept into a theory of the mind. In his 1991 essay "Viruses of the Mind", Richard Dawkins used memetics to explain the phenomenon of religious belief and the various characteristics of organised religions. By then, memetics had also become a theme appearing in fiction (e.g. Neal Stephenson's "Snow Crash").

The idea of "language as a virus" had already been introduced by William S. Burroughs as early as 1962 in his book "The Ticket That Exploded", and later in "The Electronic Revolution", published in 1970 in "". Douglas Rushkoff explored the same concept in "Media Virus: Hidden Agendas in Popular Culture" in 1995.

However, the foundation of memetics in its full modern incarnation originated in the publication in 1996 of two books by authors outside the academic mainstream: "Virus of the Mind: The New Science of the Meme" by former Microsoft executive turned motivational speaker and professional poker-player, Richard Brodie, and "Thought Contagion: How Belief Spreads Through Society" by Aaron Lynch, a mathematician and philosopher who worked for many years as an engineer at Fermilab. Lynch claimed to have conceived his theory totally independently of any contact with academics in the cultural evolutionary sphere, and apparently was not even aware of Dawkins' "The Selfish Gene" until his book was very close to publication.

Around the same time as the publication of the books by Lynch and Brodie the e-journal Journal of Memetics – "Evolutionary Models of Information Transmission" appeared on the web. It was first hosted by the Centre for Policy Modelling at Manchester Metropolitan University but later taken over by Francis Heylighen of the CLEA research institute at the Vrije Universiteit Brussel. The e-journal soon became the central point for publication and debate within the nascent memeticist community. (There had been a short-lived paper-based memetics publication starting in 1990, the "Journal of Ideas" edited by Elan Moritz.) In 1999, Susan Blackmore, a psychologist at the University of the West of England, published "The Meme Machine", which more fully worked out the ideas of Dennett, Lynch, and Brodie and attempted to compare and contrast them with various approaches from the cultural evolutionary mainstream, as well as providing novel, and controversial, memetics-based theories for the evolution of language and the human sense of individual selfhood.

The term "meme" derives from the Ancient Greek μιμητής ("mimētḗs"), meaning "imitator, pretender". The similar term "mneme" was used in 1904, by the German evolutionary biologist Richard Semon, best known for his development of the engram theory of memory, in his work "Die mnemischen Empfindungen in ihren Beziehungen zu den Originalempfindungen", translated into English in 1921 as "The Mneme" . Until Daniel Schacter published "Forgotten Ideas, Neglected Pioneers: Richard Semon and the Story of Memory" in 2000, Semon's work had little influence, though it was quoted extensively in Erwin Schrödinger’s prescient 1956 Tarner Lecture “Mind and Matter”. Richard Dawkins (1976) apparently coined the word "meme" independently of Semon, writing this:
"'Mimeme' comes from a suitable Greek root, but I want a monosyllable that sounds a bit like 'gene'. I hope my classicist friends will forgive me if I abbreviate mimeme to meme. If it is any consolation, it could alternatively be thought of as being related to 'memory', or to the French word même." 

In 2005, the "Journal of Memetics – Evolutionary Models of Information Transmission" ceased publication and published a set of articles on the future of memetics. The website states that although "there was to be a relaunch...after several years nothing has happened". Susan Blackmore has left the University of the West of England to become a freelance science-writer and now concentrates more on the field of consciousness and cognitive science. Derek Gatherer moved to work as a computer programmer in the pharmaceutical industry, although he still occasionally publishes on memetics-related matters. Richard Brodie is now climbing the world professional poker rankings. Aaron Lynch disowned the memetics community and the words "meme" and "memetics" (without disowning the ideas in his book), adopting the self-description "thought contagionist". He died in 2005.

Susan Blackmore (2002) re-stated the definition of meme as: whatever is copied from one person to another person, whether habits, skills, songs, stories, or any other kind of information. Further she said that memes, like genes, are replicators in the sense as defined by Dawkins.
That is, they are information that is copied. Memes are copied by imitation, teaching and other methods. The copies are not perfect: memes are copied with variation; moreover, they compete for space in our memories and for the chance to be copied again. Only some of the variants can survive. The combination of these three elements (copies; variation; competition for survival) forms precisely the condition for Darwinian evolution, and so memes (and hence human cultures) evolve. Large groups of memes that are copied and passed on together are called co-adapted meme complexes, or "memeplexes". In Blackmore's definition, the way that a meme replicates is through imitation. This requires brain capacity to generally imitate a model or selectively imitate the model. Since the process of social learning varies from one person to another, the imitation process cannot be said to be completely imitated. The sameness of an idea may be expressed with different memes supporting it. This is to say that the mutation rate in memetic evolution is extremely high, and mutations are even possible within each and every iteration of the imitation process. It becomes very interesting when we see that a social system composed of a complex network of microinteractions exists, but at the macro level an order emerges to create culture.

The memetics movement split almost immediately into two. The first group were those who wanted to stick to Dawkins' definition of a meme as "a unit of cultural transmission". Gibron Burchett, another memeticist responsible for helping to research and co-coin the term memetic engineering, along with Leveious Rolando and Larry Lottman, has stated that a meme can be defined, more precisely, as "a unit of cultural information that can be copied, located in the brain". This thinking is more in line with Dawkins' second definition of the meme in his book "The Extended Phenotype". The second group wants to redefine memes as observable cultural artifacts and behaviors. However, in contrast to those two positions, Blackmore does not reject either concept of external or internal memes.

These two schools became known as the "internalists" and the "externalists." Prominent internalists included both Lynch and Brodie; the most vocal externalists included Derek Gatherer, a geneticist from Liverpool John Moores University, and William Benzon, a writer on cultural evolution and music. The main rationale for externalism was that internal brain entities are not observable, and memetics cannot advance as a science, especially a quantitative science, unless it moves its emphasis onto the directly quantifiable aspects of culture. Internalists countered with various arguments: that brain states will eventually be directly observable with advanced technology, that most cultural anthropologists agree that culture is about beliefs and not artifacts, or that artifacts cannot be replicators in the same sense as mental entities (or DNA) are replicators. The debate became so heated that a 1998 Symposium on Memetics, organised as part of the 15th International Conference on Cybernetics, passed a motion calling for an end to definitional debates. McNamara demonstrated in 2011 that functional connectivity profiling using neuroimaging tools enables the observation of the processing of internal memes, "i-memes", in response to external "e-memes".

An advanced statement of the internalist school came in 2002 with the publication of "The Electric Meme", by Robert Aunger, an anthropologist from the University of Cambridge. Aunger also organised a conference in Cambridge in 1999, at which prominent sociologists and anthropologists were able to give their assessment of the progress made in memetics to that date. This resulted in the publication of "Darwinizing Culture: The Status of Memetics as a Science", edited by Aunger and with a foreword by Dennett, in 2001.

This evolutionary model of cultural information transfer is based on the concept that units of information, or "memes", have an independent existence, are self-replicating, and are subject to selective evolution through environmental forces. Starting from a proposition put forward in the writings of Richard Dawkins, this model has formed the basis of a new area of study, one that looks at the self-replicating units of culture. It has been proposed that just as memes are analogous to genes, memetics is analogous to genetics.

Critics contend that some proponents' assertions are "untested, unsupported or incorrect." Luis Benitez-Bribiesca, a critic of memetics, calls it "a pseudoscientific dogma" and "a dangerous idea that poses a threat to the serious study of consciousness and cultural evolution" among other things. As factual criticism, he refers to the lack of a "code script" for memes, as the DNA is for genes, and to the fact that the meme mutation mechanism (i.e., an idea going from one brain to another) is too unstable (low replication accuracy and high mutation rate), which would render the evolutionary process chaotic. This, however, has been demonstrated (e.g. by Daniel C. Dennett, in "Darwin's Dangerous Idea") to not be the case, in fact, due to the existence of self-regulating correction mechanisms (vaguely resembling those of gene transcription) enabled by the redundancy and other properties of most meme expression languages, which do stabilize information transfer. (E.g. spiritual narratives—including music and dance forms—can survive in full detail across any number of generations even in cultures with oral tradition only.) Memes for which stable copying methods are available will inevitably get selected for survival more often than those which can only have unstable mutations, therefore going extinct. (Notably, Benitez-Bribiesca's claim of "no code script" is also irrelevant, considering the fact that there is nothing preventing the information contents of memes from being coded, encoded, expressed, preserved or copied in all sorts of different ways throughout their life-cycles.)

Another criticism comes from semiotics, (e.g., Deacon, Kull) stating that the concept of meme is a primitivized concept of Sign. Meme is thus described in memetics as a sign without its triadic nature. In other words, meme is a degenerate sign, which includes only its ability of being copied. Accordingly, in the broadest sense, the objects of copying are memes, whereas the objects of translation and interpretation are signs.
Mary Midgley criticises memetics for at least two reasons: "One, culture is not best understood by examining its smallest parts, as culture is pattern-like, comparable to an ocean current. Many more factors, historical and others, should be taken into account than only whatever particle culture is built from. Two, if memes are not thoughts (and thus not cognitive phenomena), as Daniel C. Dennett insists in "Darwin's Dangerous Idea", then their ontological status is open to question, and memeticists (who are also reductionists) may be challenged whether memes even exist. Questions can extend to whether the idea of "meme" is itself a meme, or is a true concept. Fundamentally, memetics is an attempt to produce knowledge through organic metaphors, which as such is a questionable research approach, as the application of metaphors has the effect of hiding that which does not fit within the realm of the metaphor. Rather than study actual reality, without preconceptions, memetics, as so many of the socio-biological explanations of society, believe that saying that the apple is like an orange is a valid analysis of the apple."

Henry Jenkins, Joshua Green, and Sam Ford, in their book "Spreadable Media" (2013), criticize Dawkins' idea of the meme, writing that "while the idea of the meme is a compelling one, it may not adequately account for how content circulates through participatory culture." The three authors also criticize other interpretations of memetics, especially those which describe memes as "self-replicating", because they ignore the fact that "culture is a human product and replicates through human agency."

Like other critics, Maria Kronfeldner has criticized memetics for being based on an allegedly inaccurate analogy with the gene; alternately, she claims it is "heuristically trivial", being a mere redescription of what is already known without offering any useful novelty.

Dawkins in "A Devil's Chaplain" responded that there are actually two different types of memetic processes (controversial and informative). The first is a type of cultural idea, action, or expression, which does have high variance; for instance, a student of his who had inherited some of the mannerisms of Wittgenstein. However, he also describes a self-correcting meme, highly resistant to mutation. As an example of this, he gives origami patterns in elementary schools – except in rare cases, the meme is either passed on in the exact sequence of instructions, or (in the case of a forgetful child) terminates. This type of meme tends not to evolve, and to experience profound mutations in the rare event that it does.

Another definition, given by Hokky Situngkir, tried to offer a more rigorous formalism for the meme, "memeplexes", and the "deme", seeing the meme as a cultural unit in a cultural complex system. It is based on the Darwinian genetic algorithm with some modifications to account for the different patterns of evolution seen in genes and memes. In the method of memetics as the way to see culture as a complex adaptive system, he describes a way to see memetics as an alternative methodology of cultural evolution. However, there are as many possible definitions that are credited to the word "meme". For example, in the sense of computer simulation the term "memetic algorithm" is used to define a particular computational viewpoint.

The possibility of quantitative analysis of memes using neuroimaging tools and the suggestion that such studies have already been done was given by McNamara (2011). This author proposes hyperscanning (concurrent scanning of two communicating individuals in two separate MRI machines) as a key tool in the future for investigating memetics.

Velikovsky (2013) proposed the "holon" as the structure of the meme, synthesizing the major theories on memes of Richard Dawkins, Mihaly Csikszentmihalyi, E. O. Wilson, Frederick Turner (poet) and Arthur Koestler.

Proponents of memetics as described in the Journal of Memetics (out of print since 2005 ) – "Evolutionary Models of Information Transmission" believe that 'memetics' has the potential to be an important and promising analysis of culture using the framework of evolutionary concepts. 
Keith Henson in "Memetics and the Modular-Mind" (Analog Aug. 1987) makes the case that memetics needs to incorporate evolutionary psychology to understand the psychological traits of a meme's host. This is especially true of time-varying, meme-amplification host-traits, such as those leading to wars.

DiCarlo () has developed the idea of 'memetic equilibrium' to describe a cultural compatible state with biological equilibrium. In "Problem Solving and Neurotransmission in the Upper Paleolithic" (in press), diCarlo argues that as human consciousness evolved and developed, so too did our ancestors' capacity to consider and attempt to solve environmental problems in more conceptually sophisticated ways. Understood in this way, problem solving amongst a particular group, when considered satisfactory, often produces a feeling of environmental control, stability, in short—memetic equilibrium. 
But the pay-off is not merely practical, providing purely functional utility—it is biochemical and it comes in the form of neurotransmitters. The relationship between a gradually emerging conscious awareness and sophisticated languages in which to formulate representations combined with the desire to maintain biological equilibrium, generated the necessity for equilibrium to fill in conceptual gaps in terms of understanding three very important aspects in the Upper Paleolithic: causality, morality, and mortality. The desire to explain phenomena in relation to maintaining survival and reproductive stasis, generated a normative stance in the minds of our ancestors—Survival/Reproductive Value (or S-R Value).

Houben (2014) has argued on several occasions that the exceptional resilience of Vedic ritual and its interaction with a changing ecological and economic environment over several millennia can be profitably dealt with in a ‘cultural evolution’ perspective in which the Vedic mantra is the ‘meme’ or unit of cultural replication.
This renders superfluous attempts to explain the phenomenon of Vedic tradition in genetic terms. The domain of Vedic ritual should be able to fulfil to a large extent the three challenges posed to memetics by B. Edmonds (2002 and 2005).

Research methodologies that apply memetics go by many names: Viral marketing, cultural evolution, the history of ideas, social analytics, and more. Many of these applications do not make reference to the literature on memes directly but are built upon the evolutionary lens of idea propagation that treats semantic units of culture as self-replicating and mutating patterns of information that are assumed to be relevant for scientific study. For example, the field of public relations is filled with attempts to introduce new ideas and alter social discourse. One means of doing this is to design a meme and deploy it through various media channels. One historic example of applied memetics is the PR campaign conducted in 1991 as part of the build-up to the first Gulf War in the United States.

The application of memetics to a difficult complex social system problem, environmental sustainability, has recently been attempted at thwink.org Using meme types and memetic infection in several stock and flow simulation models, Jack Harich has demonstrated several interesting phenomena that are best, and perhaps only, explained by memes. One model, The Dueling Loops of the Political Powerplace, argues that the fundamental reason corruption is the norm in politics is due to an inherent structural advantage of one feedback loop pitted against another. Another model, The Memetic Evolution of Solutions to Difficult Problems, uses memes, the evolutionary algorithm, and the scientific method to show how complex solutions evolve over time and how that process can be improved. The insights gained from these models are being used to engineer memetic solution elements to the sustainability problem.

Another application of memetics in the sustainability space is the crowdfunded Climate Meme Project conducted by Joe Brewer and Balazs Laszlo Karafiath in the spring of 2013. This study was based on a collection of 1000 unique text-based expressions gathered from Twitter, Facebook, and structured interviews with climate activists. The major finding was that the global warming meme is not effective at spreading because it causes emotional duress in the minds of people who learn about it. Five central tensions were revealed in the discourse about [climate change], each of which represents a resonance point through which dialogue can be engaged. The tensions were Harmony/Disharmony (whether or not humans are part of the natural world), Survival/Extinction (envisioning the future as either apocalyptic collapse of civilization or total extinction of the human race), Cooperation/Conflict (regarding whether or not humanity can come together to solve global problems), Momentum/Hesitation (about whether or not we are making progress at the collective scale to address climate change), and Elitism/Heretic (a general sentiment that each side of the debate considers the experts of its opposition to be untrustworthy).

Ben Cullen, in his book "Contagious Ideas", brought the idea of the meme into the discipline of archaeology. He coined the term "Cultural Virus Theory", and used it to try to anchor archaeological theory in a neo-Darwinian paradigm. Archaeological memetics could assist the application of the meme concept to material culture in particular.

Francis Heylighen of the Center Leo Apostel for Interdisciplinary Studies has postulated what he calls "memetic selection criteria". These criteria opened the way to a specialized field of "applied memetics" to find out if these selection criteria could stand the test of quantitative analyses. In 2003 Klaas Chielens carried out these tests in a Masters thesis project on the testability of the selection criteria.

In "Selfish Sounds and Linguistic Evolution", Austrian linguist Nikolaus Ritt has attempted to operationalise memetic concepts and use them for the explanation of long term sound changes and change conspiracies in early English. It is argued that a generalised Darwinian framework for handling cultural change can provide explanations where established, speaker centred approaches fail to do so. The book makes comparatively concrete suggestions about the possible material structure of memes, and provides two empirically rich case studies.

Australian academic S.J. Whitty has argued that project management is a memeplex with the language and stories of its practitioners at its core. This radical approach sees a project and its management as an illusion; a human construct about a collection of feelings, expectations, and sensations, which are created, fashioned, and labeled by the human brain. Whitty's approach requires project managers to consider that the reasons for using project management are not consciously driven to maximize profit, and are encouraged to consider project management as naturally occurring, self-serving, evolving process which shapes organizations for its own purpose.

Swedish political scientist Mikael Sandberg argues against "Lamarckian" interpretations of institutional and technological evolution and studies creative innovation of information technologies in governmental and private organizations in Sweden in the 1990s from a memetic perspective. Comparing the effects of active ("Lamarckian") IT strategy versus user–producer interactivity (Darwinian co-evolution), evidence from Swedish organizations shows that co-evolutionary interactivity is almost four times as strong a factor behind IT creativity as the "Lamarckian" IT strategy.








</doc>
<doc id="19773" url="https://en.wikipedia.org/wiki?curid=19773" title="March 25">
March 25





</doc>
<doc id="19780" url="https://en.wikipedia.org/wiki?curid=19780" title="List of islands of Michigan">
List of islands of Michigan

The following is a list of islands of Michigan. Michigan has the second longest coastline of any state after Alaska. Being bordered by four of the five Great Lakes—Erie, Huron, Michigan, and Superior—Michigan also has 64,980 inland lakes and ponds, as well as innumerable rivers, that may contain their own islands included in this list. The majority of the islands are within the Great Lakes. Other islands can also be found within other waterways of the Great Lake system, including Lake St. Clair, St. Clair River, Detroit River, and St. Marys River.

The largest of all the islands is Isle Royale in Lake Superior, which, in addition to its waters and other surrounding islands, is organized as Isle Royale National Park. Isle Royale itself is . The most populated island is Grosse Ile with approximately 10,000 residents, located in the Detroit River about south of Detroit. The majority of Michigan's islands are uninhabited and very small. Some of these otherwise unusable islands have been used for the large number of Michigan's lighthouses to aid in shipping throughout the Great Lakes, while others have been set aside as nature reserves. Many islands in Michigan have the same name, even some that are in the same municipality and body of water, such as Gull, Long, or Round islands.

Only Monroe County has territory in the westernmost portion of Lake Erie, which has a surface elevation of . The islands in the southern portion of the county are part of the North Maumee Bay Archeological District of the Detroit River International Wildlife Refuge, in which Turtle Island is the only island in the state of Michigan that is shared by another state. This remote and tiny island is cut in half and shared with Ohio. While not distinct named islands in their own right, Sterling State Park and Pointe Mouillee are situated on several islands.

Lake Huron is the second largest of the Great Lakes (after Lake Superior) with a surface area of . Michigan is the only U.S. state to border Lake Huron, while the portion of the lake on the other side of the international border belongs to the Canadian province of Ontario. The vast majority of Michigan's islands in Lake Huron are centered around Drummond Island in the northernmost portion of the state's lake territory. Drummond Island is the largest of Michigan's islands in Lake Huron and is the second largest Michigan island after Lake Superior's Isle Royale. Another large group of islands is the Les Cheneaux Islands archipelago, which itself contains dozens of small islands. Many of the lake's islands are very small and uninhabited.

As the most popular tourist destination in the state, Mackinac Island is the most well known of Lake Huron's islands. Drummond Island is the most populous of Michigan's islands in Lake Huron, with a population of 992 at the 2000 census. While Mackinac Island had a population of only 553, there are thousands more seasonal workers and tourists during the summer months.

Michigan only has islands in Lake Michigan in the northern portion of the lake. There are no islands in the southern half of Lake Michigan. The largest and most populated of Michigan's islands in Lake Michigan is Beaver Island at and 551 residents. Some of the smaller islands surrounding Beaver Island are part of the larger Michigan Islands National Wildlife Refuge.

Lake Superior is the largest of the Great Lakes, and the coastline is sparsely populated. At , Isle Royale is the largest Michigan island and is the center of Isle Royale National Park, which itself contains over 450 islands. The following is a list of islands in Lake Superior that are "not" part of Isle Royale National Park. For those islands, see the list of islands in Isle Royale National Park.

Lake St. Clair connects Lake Huron and Lake Erie through the St. Clair River in the north and the Detroit River in the south. At , it is one of the largest non-Great Lakes in the United States, but it only contains a small number of islands near the mouth of the St. Clair River, where all of the following islands are located. The largest of these islands is Harsens Island, and all the islands are in Clay Township in St. Clair County.

The Detroit River runs for and connects Lake St. Clair to Lake Erie. For its entire length, it carries the international border between the United States and Canada. Some islands belong to Ontario in Canada and are not included in the list below. All islands on the American side belong to Wayne County. Portions of the southern portion of the river serve as wildlife refuges as part of the Detroit River International Wildlife Refuge. The largest and most populous island is Grosse Ile at and a population of around 10,000. Most of the islands are around and closely connected to Grosse Ile.

The St. Marys River connects Lake Superior and Lake Huron at the easternmost point of the Upper Peninsula. It carries the international border throughout its length, and some of the islands belong to neighboring Ontario. The largest of Michigan's islands in the river are Sugar Island and Neebish Island. Wider portions of the river are designated as Lake George, Lake Nicolet, and the Munuscong Lake. The whole length of the Michigan portion of the river is part of Chippewa County.

Michigan has numerous inland lakes and rivers that also contain their own islands. The following also lists the body of water in which these islands are located. Five islands below (<nowiki>*</nowiki> and highlighted in green) are actually islands within an island; they are contained within inland lakes in Isle Royale.

Grand Lake is a large lake in Presque Isle County. While it is not the largest inland lake in Michigan, it does contain the most inland islands. At its shortest distance, it is located less than from Lake Huron, but the two are not connected. Grand Lake contains 14 islands, of which Grand Island is by far the largest.




</doc>
<doc id="19808" url="https://en.wikipedia.org/wiki?curid=19808" title="List of Governors of Michigan">
List of Governors of Michigan

The Governor of Michigan is the head of the executive branch of Michigan's state government and serves as the commander-in-chief of the state's military forces. The governor has a duty to enforce state laws; the power to either approve or veto appropriation bills passed by the Michigan Legislature; the power to convene the legislature; and the power to grant pardons, except in cases of impeachment. He or she is also empowered to reorganize the executive branch of the state government.

Michigan was originally part of French and British holdings, and administered by their colonial governors. After becoming part of the United States, numerous areas of what is today Michigan were originally part of the Northwest Territory, Indiana Territory and Illinois Territory, and administered by territorial governors. In 1805, the Michigan Territory was created, and five men served as territorial governors, until Michigan was granted statehood in 1837. Forty-eight individuals have held the position of state governor. The first female governor, Jennifer Granholm, was elected in 2003.

After Michigan gained statehood, governors held the office for a two-year term, until the 1963 Michigan Constitution changed the term to four years. The number of times an individual could hold the office was unlimited until a 1992 constitutional amendment imposed a lifetime term limit of two four-year governorships. The longest-serving governor in Michigan's history was William Milliken, who was promoted from lieutenant governor after Governor George W. Romney resigned, then was elected to three further successive terms.

Michigan was part of colonial New France until the Treaty of 1763 transferred ownership to the Kingdom of Great Britain. During this time, it was governed by the Lieutenants General of New France until 1627, the Governors of New France from 1627 to 1663, and the Governors General of New France until the transfer to Great Britain. The 1783 Treaty of Paris ceded the territory that is now Michigan to the United States as part of the end of the Revolutionary War, but British troops were not removed from the area until 1796. During the British ownership, their governors administrated the area as part of the Canadian territorial holdings.

Prior to becoming its own territory, parts of Michigan were administered by the governors of the Northwest Territory, the governors of the Indiana Territory and the governors of the Illinois Territory. On June 30, 1805, the Territory of Michigan was created, with General William Hull as the first territorial governor.

Michigan was admitted to the Union on January 26, 1837. The original 1835 Constitution of Michigan provided for the election of a governor and a lieutenant governor every 2 years. The fourth and current constitution of 1963 increased this term to four years. There was no term limit on governors until a constitutional amendment effective in 1993 limited governors to two terms.

Should the office of governor become vacant, the lieutenant governor becomes governor, followed in order of succession by the Secretary of State and the Attorney General. Prior to the current constitution, the duties of the office would devolve upon the lieutenant governor, without that person actually becoming governor. The term begins at noon on January 1 of the year following the election. Prior to the 1963 constitution, the governor and lieutenant governor were elected through separate votes, allowing them to be from different parties. In 1963, this was changed, so that votes are cast jointly for a governor and lieutenant governor of the same party.

 (18)

Several governors also held other high positions within the state and federal governments. Eight governors served as U.S. House of Representatives members, while seven held positions in the U.S. Senate, all representing Michigan. Others have served as ambassadors, U.S. Cabinet members, and state and federal Supreme Court justices.

As of , there are four living former governors of Michigan. The most recent death of a former governor was that of George W. Romney (served 1963–69) on July 26, 1995, 18 days after his 88th birthday. Romney was also the most recently serving governor of Michigan to have died. The state's living former governors are:





</doc>
<doc id="19809" url="https://en.wikipedia.org/wiki?curid=19809" title="Moses Amyraut">
Moses Amyraut

Moïse Amyraut, Latin Moyses Amyraldus (Bourgueil, September 1596 – January 8, 1664), in English texts often Moses Amyraut, was a French Huguenot, Reformed theologian and metaphysician. He is perhaps most noted for his modifications to Calvinist theology regarding the nature of Christ's atonement, which is referred to as Amyraldism or Amyraldianism.

Born at Bourgueil, in the valley of the Changeon in the province of Anjou, his father was a lawyer, and, preparing Moses for his own profession, sent him, on the completion of his study of the humanities at Orléans to the university of Poitiers.

At the university he took the degree of licentiate (BA) of laws. On his way home from the university he passed through Saumur, and, having visited the pastor of the Protestant church there, was introduced by him to Philippe de Mornay, governor of the city. Struck with young Amyraut's ability and culture, they both urged him to change from law to theology. His father advised him to revise his philological and philosophical studies, and read over Calvin's "Institutions," before finally determining a course. He did so, and decided for theology.

He moved to the Academy of Saumur and studied under John Cameron, who ultimately regarded him as his greatest scholar. He had a brilliant course, and was in due time licensed as a minister of the French Protestant Church. The contemporary civil wars and excitements hindered his advancement. His first church was in Saint-Aignan, in the province of Maine. There he remained two years. Jean Daillé, who moved to Paris, advised the church at Saumur to secure Amyraut as his successor, praising him "as above himself." The university of Saumur at the same time had fixed its eyes on him as professor of theology. The great churches of Paris and Rouen also contended for him, and to win him sent their deputies to the provincial synod of Anjou.

Amyraut had left the choice to the synod. He was appointed to Saumur in 1633, and to the professor's chair along with the pastorate. On the occasion of his inauguration he maintained for thesis "De Sacerdotio Christi". His co-professors were Louis Cappel and Josué de la Place, who also were Cameron's pupils and lifelong friends, who collaborated in the "Theses Salmurienses", a collection of theses propounded by candidates in theology prefaced by the inaugural addresses of the three professors. Amyraut soon gave to French Protestantism a new direction.

In 1631 he published his "Traité des religions"; and from this year onward he was a foremost man in the church. Chosen to represent the provincial synod of Anjou, Touraine and Maine at the 1631 , he was appointed as orator to present to the king "The Copy of their Complaints and Grievances for the Infractions and Violations of the Edict of Nantes".

Previous deputies had addressed the king on their bent knees, whereas the representatives of the Catholics had been permitted to stand. Amyraut consented to be orator only if the assembly authorized him to stand. There was intense resistance. Cardinal Richelieu himself, preceded by lesser dignitaries, condescended to visit Amyraut privately, to persuade him to kneel; but Amyraut held resolutely to his point and carried it. His "oration" on this occasion, which was immediately published in the French "Mercure", remains a striking landmark in the history of French Protestantism. During his absence on this matter the assembly debated "whether the Lutherans who desired it, might be admitted into communion with the Reformed Churches of France at the Lord's Table." It was decided in the affirmative previous to his return; but he approved with astonishing eloquence, and thereafter was ever in the front rank in maintaining intercommunion between all churches holding the main doctrines of the Reformation.

Pierre Bayle recounts the title-pages of no fewer than thirty-two books of which Amyraut was the author. These show that he took part in all the great controversies on predestination and Arminianism which then so agitated and harassed all Europe. Substantially he held fast the Calvinism of his preceptor Cameron; but, like Richard Baxter in England, by his breadth and charity he exposed himself to all manner of misconstruction. In 1634 he published his "Traité de la predestination", in which he tried to mitigate the harsh features of predestination by his "Universalismus hypotheticus". God, he taught, predestines all men to happiness on condition of their having faith. This gave rise to a charge of heresy, of which he was acquitted at the national synod held at Alençon in 1637, and presided over by Benjamin Basnage (1580–1652). The charge was brought up again at the national synod of Charenton in 1644, when he was again acquitted. A third attack at the synod of Loudun in 1659 met with no better success. The university of Saumur became the university of French Protestantism.

Amyraut had as many as a hundred students in attendance upon his lectures. One of these was William Penn, who would later go on to found the Pennsylvania Colony in America based in part on Amyraut's notions of religious freedom . Another historic part filled by Amyraut was in the negotiations originated by Pierre le Gouz de la Berchère (1600–1653), first president of the "parlement" of Grenoble, when exiled to Saumur, for a reconciliation and reunion of the Catholics of France with the French Protestants. Very large were the concessions made by Richelieu in his personal interviews with Amyraut; but, as with the Worcester House negotiations in England between the Church of England and nonconformists, they inevitably fell through. On all sides the statesmanship and eloquence of Amyraut were conceded. His "De l'elevation de la foy et de l'abaissement de la raison en la creance des mysteres de la religion" (1641) gave him early a high place as a metaphysician. Exclusive of his controversial writings, he left behind him a very voluminous series of practical evangelical books, which have long remained the "fireside" favourites of the peasantry of French Protestantism. Amongst these are "Estat des fideles apres la mort"; "Sur l'oraison dominicale"; "Du merite des oeuvres"; "Traité de la justification"; and paraphrases of books of the Old and New Testament. His closing years were weakened by a severe fall he met with in 1657. He died on 18 January 1664.

There were a number of theologians who defended Calvinistic orthodoxy against Amyraut and Saumur, including Friedrich Spanheim (1600–1649) and Francis Turretin (1623–1687). Ultimately, the Helvetic Consensus was drafted to counteract the theology of Saumur and Amyraldism.




</doc>
<doc id="19811" url="https://en.wikipedia.org/wiki?curid=19811" title="Murray River">
Murray River

The Murray River (or River Murray) (Ngarrindjeri: "Millewa", Yorta Yorta: "Tongala") is Australia's longest river, at in length. The Murray rises in the Australian Alps, draining the western side of Australia's highest mountains, and then meanders across Australia's inland plains, forming the border between the states of New South Wales and Victoria as it flows to the northwest into South Australia. It turns south at Morgan for its final , reaching the ocean at Lake Alexandrina.

The water of the Murray flows through several terminal lakes that fluctuate in salinity (and were often fresh until recent decades) including Lake Alexandrina and The Coorong before emptying through the Murray Mouth into the southeastern portion of the Indian Ocean, often referenced on Australian maps as the Southern Ocean, near Goolwa. Despite discharging considerable volumes of water at times, particularly before the advent of largescale river regulation, the mouth has always been comparatively small and shallow.

As of 2010, the Murray River system receives 58 percent of its natural flow. It is perhaps Australia's most important irrigated region, and it is widely known as the food bowl of the nation.

The Murray River forms part of the long combined Murray–Darling river system which drains most of inland Victoria, New South Wales, and southern Queensland. Overall the catchment area is one seventh of Australia's total land mass. The Murray carries only a small fraction of the water of comparably-sized rivers in other parts of the world, and with a great annual variability of its flow. In its natural state it has even been known to dry up completely during extreme droughts, although that is extremely rare, with only two or three instances of this occurring since official record keeping began.

The Murray River makes up most of the border between the Australian states of Victoria and New South Wales. Where it does, the border is the top of the bank of the Victorian side of the river (i.e., none of the river itself is actually in Victoria). This was determined in a 1980 ruling by the High Court of Australia, which settled the question as to which state had jurisdiction in the unlawful death of a man who was fishing by the river's edge on the Victorian side of the river. This boundary definition can be ambiguous, since the river changes its course over time, and some of the river banks have been modified.

West of the line of longitude 141°E, the river continues as the border between Victoria and South Australia for approximately , where this is the only stretch where a state border runs down the middle of the river. This was due to a miscalculation during the 1840s, when the border was originally surveyed. Past this point, the Murray River is entirely within the state of South Australia.

The following major settlements are located along the course of the river, with population figures from the 2011 Census:

The Murray River (and associated tributaries) support a variety of river life adapted to its vagaries. This includes a variety of native fish such as the famous Murray cod, trout cod, golden perch, Macquarie perch, silver perch, eel-tailed catfish, Australian smelt, and western carp gudgeon, and other aquatic species like the Murray short-necked turtle, Murray River crayfish, broad-clawed yabbies, and the large clawed "Macrobrachium" shrimp, as well as aquatic species more widely distributed through southeastern Australia such as common longnecked turtles, common yabbies, the small claw-less "paratya" shrimp, water rats, and platypus. The Murray River also supports fringing corridors and forests of the river red gum.

The health of the Murray River has declined significantly since European settlement, particularly due to river regulation, and much of its aquatic life including native fish are now declining, rare or endangered. Recent extreme droughts (2000–07) have put significant stress on river red gum forests, with mounting concern over their long-term survival. The Murray has also flooded on occasion, the most significant of which was the flood of 1956, which inundated many towns on the lower Murray and which lasted for up to six months.

Introduced fish species such as carp, "gambusia", weather loach, redfin perch, brown trout, and rainbow trout have also had serious negative effects on native fish, while carp have contributed to environmental degradation of the Murray River and tributaries by destroying aquatic plants and permanently raising turbidity. In some segments of the Murray River, carp have become the only species found.

Between 2.5 and 0.5 million years ago the Murray River terminated in a vast freshwater lake called Lake Bungunnia. Lake Bungunnia was formed by earth movements that blocked the Murray River near Swan Reach during this period of time. At its maximum extent Lake Bungunnia covered , extending to near the Menindee Lakes in the north and to near Boundary Bend on the Murray in the south. The draining of Lake Bungunnia occurred approximately 600,000 years ago.

Deep clays deposited by the lake are evident in cliffs around Chowilla in South Australia. Considerably higher rainfall would have been required to keep such a lake full; the draining of Lake Bungunnia appears to mark the end of a wet phase in the history of the Murray-Darling Basin and the onset of widespread arid conditions similar to today. A species of "Neoceratodus" lungfish existed in Lake Bungunnia (McKay & Eastburn, 1990); today "Neoceratodus" lungfish are only found in several Queensland rivers.

The noted Barmah Red Gum Forests owe their existence to the Cadell Fault. About 25,000 years ago, displacement occurred along the Cadell fault, raising the eastern edge of the fault, which runs north-south, above the floodplain. This created a complex series of events. A section of the original Murray River channel immediately behind the fault was abandoned, and it exists today as an empty channel known as Green Gully. The Goulburn River was dammed by the southern end of the fault to create a natural lake.

The Murray River flowed to the north around the Cadell Fault, creating the channel of the Edward River which exists today and through which much of the Murray River's waters still flow. Then the natural dam on the Goulburn River failed, the lake drained, and the Murray River avulsed to the south and started to flow through the smaller Goulburn River channel, creating "The Barmah Choke" and "The Narrows" (where the river channel is unusually narrow), before entering into the proper Murray River channel again.

This complex series of events, however, diverts attention from the primary result of the Cadell Fault — that the west-flowing water of the Murray River strikes the north-south fault and diverts both north and south around the fault in the two main channels (Edward and ancestral Goulburn) as well as a fan of small streams, and regularly floods a large amount of low-lying country in the area. These conditions are perfect for River Red Gums, which rapidly formed forests in the area. Thus the displacement of the Cadell Fault 25,000 BP led directly to the formation of the famous Barmah River Red Gum Forests.

The Barmah Choke and The Narrows mean the amount of water that can travel down this part of the Murray River is restricted. In times of flood and high irrigation flows the majority of the water, in addition to flooding the Red Gum forests, actually travels through the Edward River channel. The Murray River has not had enough flow power to naturally enlarge The Barmah Choke and The Narrows to increase the amount of water they can carry.

The Cadell Fault is quite noticeable as a continuous, low, earthen embankment as one drives into Barmah from the west, although to the untrained eye it may appear man-made.
The Murray Mouth is the point at which the Murray River empties into the sea, and the interaction between its shallow, shifting and variable currents and the open sea can be complex and unpredictable. During the peak period of Murray River commerce (roughly 1855 to 1920), it presented a major impediment to the passage of goods and produce between Adelaide and the Murray settlements, and many vessels foundered or were wrecked there.

Since the early 2000s, dredging machines have operated at the Murray Mouth, moving sand from the channel to maintain a minimal flow from the sea and into the Coorong's lagoon system. Without the 24-hour dredging, the Mouth would silt up and close, cutting the supply of fresh sea-water into the Coorong, which would then warm up, stagnate and die.

Being one of the major river systems in one of the driest continents of Earth, the Murray has significant cultural relevance to Indigenous Australians. According to the peoples of Lake Alexandrina, the Murray was created by the tracks of the Great Ancestor, Ngurunderi, as he pursued Pondi, the Murray Cod. The chase originated in the interior of New South Wales. Ngurunderi pursued the fish (who, like many totem animals in Aboriginal myths, is often portrayed as a man) on rafts (or "lala") made from red gums and continually launched spears at his target. But Pondi was a wily prey and carved a weaving path, carving out the river's various tributaries. Ngurunderi was forced to beach his rafts, and often create new ones as he changed from reach to reach of the river.

At Kobathatang, Ngurunderi finally got lucky, and struck Pondi in the tail with a spear. However, the shock to the fish was so great it launched him forward in a straight line to a place called Peindjalang, near Tailem Bend. Eager to rectify his failure to catch his prey, the hunter and his two wives (sometimes the escaped sibling wives of Waku and Kanu) hurried on, and took positions high on the cliff on which Tailem Bend now stands. They sprung an ambush on Pondi only to fail again. Ngurunderi set off in pursuit again, but lost his prey as Pondi dived into Lake Alexandrina. Ngurunderi and his women settled on the shore, only to suffer bad luck with fishing, being plagued by a water fiend known as Muldjewangk. They later moved to a more suitable spot at the site of present-day Ashville. The twin summits of Mount Misery are supposed to be the remnants of his rafts, they are known as "Lalangengall" or "the two watercraft".

Remarkably, this story of a hunter pursuing a Murray cod that carved out the Murray persists in numerous forms in various language groups that inhabit the enormous area spanned by the Murray system. The Wotojobaluk people of Victoria tell of Totyerguil from the area now known as Swan Hill who ran out of spears while chasing Otchtout the cod.

The first Europeans to encounter the river were Hamilton Hume and William Hovell, who crossed the river where Albury now stands in 1824: Hume named it the "Hume River" after his father. In 1830 Captain Charles Sturt reached the river after travelling down its tributary the Murrumbidgee River and named it the "Murray River" in honour of the then British Secretary of State for War and the Colonies, Sir George Murray, not realising it was the same river that Hume and Hovell had encountered further upstream.

Sturt continued down the remaining length of the Murray to finally reach Lake Alexandrina and the river's mouth. The area of the Murray Mouth was explored more thoroughly by Captain Collet Barker in 1831.

The first three settlers on the Murray River are known to have been James Collins Hawker (explorer and surveyor) along with E. J. Eyre (explorer and later Governor of Jamaica) plus E. B. Scott (onetime superintendent of Yatala Labour Prison). Hawker is known to have sold his share in the Bungaree Station which he founded with his brothers, and relocated alongside the Murray at a site near Moorundie

In 1852 Francis Cadell, in preparation for the launch of his steamer service, explored the river in a canvas boat, travelling downstream from Swan Hill.

In 1858 the Government Zoologist, William Blandowski, along with Gerard Krefft, explored the lower reaches of the Murray and Darling rivers, compiling a list of birds and mammals.

The lack of an estuary means that shipping cannot enter the Murray from the sea. However, in the 19th century the river supported a substantial commercial trade using shallow-draft paddle steamers, the first trips being made by two boats from South Australia on the spring flood of 1853. The "Lady Augusta", captained by Francis Cadell, reached Swan Hill while another, "Mary Ann", captained by William Randell, made it as far as Moama (near Echuca). In 1855 a steamer carrying gold-mining supplies reached Albury but Echuca was the usual turn-around point though small boats continued to link with up-river ports such as Tocumwal, Wahgunya and Albury.

The arrival of steamboat transport was welcomed by pastoralists who had been suffering from a shortage of transport due to the demands of the gold fields. By 1860 a dozen steamers were operating in the high water season along the Murray and its tributaries. Once the railway reached Echuca in 1864, the bulk of the woolclip from the Riverina was transported via river to Echuca and then south to Melbourne.

The Murray was plagued by "snags", fallen trees submerged in the water, and considerable efforts were made to clear the river of these threats to shipping by using barges equipped with steam-driven winches. In recent times, efforts have been made to restore many of these "snags" by placing dead gum trees back into the river. The primary purpose of this is to provide habitat for fish species whose breeding grounds and shelter were eradicated by the removal of "snags."
The volume and value of river trade made Echuca Victoria's second port and in the decade from 1874 it underwent considerable expansion. By this time up to thirty steamers and a similar number of barges were working the river in season. River transport began to decline once the railways touched the Murray at numerous points. The unreliable levels made it impossible for boats to compete with the rail and later road transport. However, the river still carries pleasure boats along its entire length.

Today, most traffic on the river is recreational. Small private boats are used for water skiing and fishing. Houseboats are common, both commercial for hire and privately owned. There are a number of both historic paddle steamers and newer boats offering cruises ranging from half an hour to 5 days. In 2009, British Adventurer David Cornthwaite walked and kayaked 2476 km along the Murray River from source to sea.

The Murray River has been a significant barrier to land-based travel and trade. Many of the Ports for transport of goods along the Murray have also developed as places to cross the river, either by bridge or ferry. The first bridge to cross the Murray, which was built in 1869, is in the town of Murray Bridge, formerly called Edwards Crossing.

Small-scale pumping plants began drawing water from the Murray in the 1850s and the first high-volume plant was constructed at Mildura in 1887. The introduction of pumping stations along the river promoted an expansion of farming and led ultimately to the development of irrigation areas (including the Murrumbidgee Irrigation Area).

In 1915, the three Murray states – New South Wales, Victoria, and South Australia – signed the River Murray Agreement which proposed the construction of storage reservoirs in the river's headwaters as well as at Lake Victoria near the South Australian border. Along the intervening stretch of the river a series of locks and weirs were built. These were originally proposed to support navigation even in times of low water, but riverborne transport was already declining due to improved highway and railway systems.

Four large reservoirs were built along the Murray. In addition to Lake Victoria (completed late 1920s), these are Lake Hume near Albury–Wodonga (completed 1936), Lake Mulwala at Yarrawonga (completed 1939), and Lake Dartmouth, which is actually on the Mitta Mitta River upstream of Lake Hume (completed 1979). The Murray also receives water from the complex dam and pipeline system of the Snowy Mountains Scheme. An additional reservoir was proposed in the 1960s at Chowilla Dam which was to have been built in South Australia and would have flooded land mostly in Victoria and New South Wales. This reservoir was cancelled in favour of building Dartmouth Dam due to costs and concerns relating to increased salinity.

From 1935 to 1940 a series of barrages was built near the Murray Mouth to stop seawater egress into the lower part of the river during low flow periods. They are the Goolwa Barrage, located at , Mundoo Channel Barrage at , Boundary Creek Barrage at , Ewe Island Barrage at , and Tauwitchere Barrage at .
These dams inverted the patterns of the river's natural flow from the original winter-spring flood and summer-autumn dry to the present low level through winter and higher during summer. These changes ensured the availability of water for irrigation and made the Murray Valley Australia's most productive agricultural region, but have seriously disrupted the life cycles of many ecosystems both inside and outside the river, and the irrigation has led to dryland salinity that now threatens the agricultural industries.

The disruption of the river's natural flow, runoff from agriculture, and the introduction of pest species like the European carp has led to serious environmental damage along the river's length and to concerns that the river will be unusably salty in the medium to long term – a serious problem given that the Murray supplies 40 percent of the water supply for Adelaide. Efforts to alleviate the problems proceed but disagreement between various groups stalls progress.

In 2006, the state government of South Australia revealed its plan to investigate the construction of the controversial Wellington Weir.

Lock 1 was completed near Blanchetown in 1922. Torrumbarry weir downstream of Echuca began operating in December 1923. Of the numerous locks that were proposed, only thirteen were completed; Locks 1 to 11 on the stretch downstream of Mildura, Lock 15 at Euston and Lock 26 at Torrumbarry. Construction of the remaining weirs purely for navigation purposes was abandoned in 1934. The last lock to be completed was Lock 15, in 1937.
Lock 11, just downstream of Mildura, creates a long lock pool which aided irrigation pumping from Mildura and Red Cliffs.

Each lock has a navigable passage next to it through the weir, which is opened during periods of high river flow, when there is too much water for the lock. The weirs can be completely removed, and the locks completely covered by water during flood conditions. Lock 11 is unique in that the lock was built inside a bend of the river, with the weir in the bend itself. A channel was dug to the lock, creating an island between it and the weir. The weir is also of a different design, being dragged out of the river during high flow, rather than lifted out.

Major tributaries


Population centres




</doc>
<doc id="19812" url="https://en.wikipedia.org/wiki?curid=19812" title="Project Mercury">
Project Mercury

Project Mercury was the first human spaceflight program of the United States, running from 1958 through 1963. An early highlight of the Space Race, its goal was to put a man into Earth orbit and return him safely, ideally before the Soviet Union. Taken over from the U.S. Air Force by the newly created civilian space agency NASA, it conducted twenty unmanned developmental flights (some using animals), and six successful flights by astronauts. The program, which took its name from Roman mythology, cost $277 million in 1965 US dollars, and involved the work of 2 million people. The astronauts were collectively known as the "Mercury Seven", and each spacecraft was given a name ending with a "7" by its pilot.

The Space Race began with the 1957 launch of the Soviet satellite Sputnik 1. This came as a shock to the American public, and led to the creation of NASA to expedite existing U.S. space exploration efforts, and place most of them under civilian control. After the successful launch of the Explorer 1 satellite in 1958, manned spaceflight became the next goal. The Soviet Union put the first human, cosmonaut Yuri Gagarin, into a single orbit aboard Vostok 1 on April 12, 1961. Shortly after this, on May 5, the U.S. launched its first astronaut, Alan Shepard, on a suborbital flight. Soviet Gherman Titov followed with a day-long orbital flight in August 1961. The U.S. reached its orbital goal on February 20, 1962, when John Glenn made three orbits around the Earth. When Mercury ended in May 1963, both nations had sent six people into space, but the Soviets led the U.S. in total time spent in space.

The Mercury space capsule was produced by McDonnell Aircraft, and carried supplies of water, food and oxygen for about one day in a pressurized cabin. Mercury flights were launched from Cape Canaveral Air Force Station in Florida, on launch vehicles modified from the Redstone and Atlas D missiles. The capsule was fitted with a launch escape rocket to carry it safely away from the launch vehicle in case of a failure. The flight was designed to be controlled from the ground via the Manned Space Flight Network, a system of tracking and communications stations; back-up controls were outfitted on board. Small retrorockets were used to bring the spacecraft out of its orbit, after which an ablative heat shield protected it from the heat of atmospheric reentry. Finally, a parachute slowed the craft for a water landing. Both astronaut and capsule were recovered by helicopters deployed from a U.S. Navy ship.

The Mercury project gained popularity, and its missions were followed by millions on radio and TV around the world. Its success laid the groundwork for Project Gemini, which carried two astronauts in each capsule and perfected space docking maneuvers essential for manned lunar landings in the subsequent Apollo program announced a few weeks after the first manned Mercury flight.

Project Mercury was officially approved on October 7, 1958 and publicly announced on December 17. Originally called Project Astronaut, President Dwight Eisenhower felt that gave too much attention to the pilot. Instead, the name "Mercury" was chosen from classical mythology, which had already lent names to rockets like the Greek "Atlas" and Roman "Jupiter" for the SM-65 and PGM-19 missiles. It absorbed military projects with the same aim, such as the Air Force Man In Space Soonest.

Following the end of World War II, a nuclear arms race evolved between the U.S. and the Soviet Union (USSR). Since the USSR did not have bases in the western hemisphere from which to deploy bomber planes, Joseph Stalin decided to develop intercontinental ballistic missiles, which drove a missile race. The rocket technology in turn enabled both sides to develop Earth-orbiting satellites for communications, and gathering weather data and intelligence. Americans were shocked when the Soviet Union placed the first satellite into orbit in October 1957, leading to a growing fear that the U.S. was falling into a "missile gap". A month later, the Soviets launched Sputnik 2, carrying a dog into orbit. Though the animal was not recovered alive, it was obvious their goal was manned spaceflight. Unable to disclose details of military space projects, President Eisenhower ordered the creation of a civilian space agency in charge of civilian and scientific space exploration. Based on the federal research agency National Advisory Committee for Aeronautics (NACA), it was named the National Aeronautics and Space Administration (NASA). It achieved its first goal, an American satellite in space, in 1958. The next goal was to put a man there.

The limit of space was defined at the time as a minimum altitude of , and the only way to reach it was by using rocket-powered boosters. This created risks for the pilot, including explosion, high g-forces and vibrations during lift off through a dense atmosphere, and temperatures of more than from air compression during reentry.

In space, pilots would require pressurized chambers or space suits to supply fresh air. While there, they would experience weightlessness, which could potentially cause disorientation. Further potential risks included radiation and micrometeoroid strikes, both of which would normally be absorbed in the atmosphere. All seemed possible to overcome: experience from satellites suggested micrometeoroid risk was negligible, and experiments in the early 1950s with simulated weightlessness, high g-forces on humans, and sending animals to the limit of space, all suggested potential problems could be overcome by known technologies. Finally, reentry was studied using the nuclear warheads of ballistic missiles, which demonstrated a blunt, forward-facing heat shield could solve the problem of heating.

T. Keith Glennan had been appointed the first Administrator of NASA, with Hugh L. Dryden (last Director of NACA) as his Deputy, at the creation of the agency on October 1, 1958. Glennan would report to the president through the National Aeronautics and Space Council. The group responsible for Project Mercury was NASA's Space Task Group, and the goals of the program were to orbit a manned spacecraft around Earth, investigate the pilot's ability to function in space, and to recover both pilot and spacecraft safely. Existing technology and off-the-shelf equipment would be used wherever practical, the simplest and most reliable approach to system design would be followed, and an existing launch vehicle would be employed, together with a progressive test program. Spacecraft requirements included: a launch escape system to separate the spacecraft and its occupant from the launch vehicle in case of impending failure; attitude control for orientation of the spacecraft in orbit; a retrorocket system to bring the spacecraft out of orbit; drag braking blunt body for atmospheric reentry; and landing on water. To communicate with the spacecraft during an orbital mission, an extensive communications network had to be built. In keeping with his desire to keep from giving the U.S. space program an overly military flavor, President Eisenhower at first hesitated to give the project top national priority (DX rating under the Defense Production Act), which meant that Mercury had to wait in line behind military projects for materials; however, this rating was granted in May 1959, a little more than a year and a half after Sputnik was launched.

Twelve companies bid to build the Mercury spacecraft on a $20 million ($ adjusted for inflation) contract. In January 1959, McDonnell Aircraft Corporation was chosen to be prime contractor for the spacecraft. Two weeks earlier, North American Aviation, based in Los Angeles, was awarded a contract for Little Joe, a small rocket to be used for development of the launch escape system. The World Wide Tracking Network for communication between the ground and spacecraft during a flight was awarded to the Western Electric Company. Redstone rockets for suborbital launches were manufactured in Huntsville, Alabama by the Chrysler Corporation and Atlas rockets by Convair in San Diego, California. For manned launches, the Atlantic Missile Range at Cape Canaveral Air Force Station in Florida was made available by the USAF. This was also the site of the Mercury Control Center while the computing center of the communication network was in Goddard Space Center, Maryland. Little Joe rockets were launched from Wallops Island, Virginia. Astronaut training took place at Langley Research Center in Virginia, Lewis Flight Propulsion Laboratory in Cleveland, Ohio, and Naval Air Development Center Johnsville in Warminster, PA. Langley wind tunnels together with a rocket sled track at Holloman Air Force Base at Alamogordo, New Mexico were used for aerodynamic studies. Both Navy and Air Force aircraft were made available for the development of the spacecraft's landing system, and Navy ships and Navy and Marine Corps helicopters were made available for recovery. South of Cape Canaveral the town of Cocoa Beach boomed. From here, 75,000 people watched the first American orbital flight being launched in 1962.

The Mercury spacecraft's principal designer was Maxime Faget, who started research for manned spaceflight during the time of the NACA. It was long and wide; with the launch escape system added, the overall length was . With of habitable volume, the capsule was just large enough for a single crew member. Inside were 120 controls: 55 electrical switches, 30 fuses and 35 mechanical levers. The heaviest spacecraft, Mercury-Atlas 9, weighed fully loaded. Its outer skin was made of René 41, a nickel alloy able to withstand high temperatures.

The spacecraft was cone shaped, with a neck at the narrow end. It had a convex base, which carried a heat shield (Item 2 in the diagram below) consisting of an aluminum honeycomb covered with multiple layers of fiberglass. Strapped to it was a retropack (1) consisting of three rockets deployed to brake the spacecraft during reentry. Between these were three minor rockets for separating the spacecraft from the launch vehicle at orbital insertion. The straps that held the package could be severed when it was no longer needed. Next to the heat shield was the pressurized crew compartment (3). Inside, an astronaut would be strapped to a form-fitting seat with instruments in front of him and with his back to the heat shield. Underneath the seat was the environmental control system supplying oxygen and heat, scrubbing the air of CO, vapor and odors, and (on orbital flights) collecting urine. The recovery compartment (4) at the narrow end of the spacecraft contained three parachutes: a drogue to stabilize free fall and two main chutes, a primary and reserve. Between the heat shield and inner wall of the crew compartment was a landing skirt, deployed by letting down the heat shield before landing. On top of the recovery compartment was the antenna section (5) containing both antennas for communication and scanners for guiding spacecraft orientation. Attached was a flap used to ensure the spacecraft was faced heat shield first during reentry. A launch escape system (6) was mounted to the narrow end of the spacecraft containing three small solid-fueled rockets which could be fired briefly in a launch failure to separate the capsule safely from its booster. It would deploy the capsule's parachute for a landing nearby at sea. (See also Mission profile for details.)

The Mercury spacecraft did not have an on-board computer, instead relying on all computation for reentry to be calculated by computers on the ground, with their results (retrofire times and firing attitude) then transmitted to the spacecraft by radio while in flight. All computer systems used in the Mercury space program were housed in NASA facilities on Earth. The computer systems were IBM 701 computers. (See also Ground control for details.)

The astronaut lay in a sitting position with his back to the heat shield, which was found to be the position that best enabled a human to withstand the high g-forces of launch and reentry. A fiberglass seat was custom-molded from each astronaut's space-suited body for maximum support. Near his left hand was a manual abort handle to activate the launch escape system if necessary prior to or during liftoff, in case the automatic trigger failed.

To supplement the onboard environmental control system, he wore a pressure suit with its own oxygen supply, which would also cool him. A cabin atmosphere of pure oxygen at a low pressure of 5.5 psi (equivalent to an altitude of ) was chosen, rather than one with the same composition as air (nitrogen/oxygen) at sea level. This was easier to control, avoided the risk of decompression sickness ("the bends"), and also saved on spacecraft weight. Fires (which never occurred) would have to be extinguished by emptying the cabin of oxygen. In such case, or failure of the cabin pressure for any reason, the astronaut could make an emergency return to Earth, relying on his suit for survival. The astronauts normally flew with their visor up, which meant that the suit was not inflated. With the visor down and the suit inflated, the astronaut could only reach the side and bottom panels, where vital buttons and handles were placed.

The astronaut also wore electrodes on his chest to record his heart rhythm, a cuff that could take his blood pressure, and a rectal thermometer to record his temperature (this was replaced by an oral thermometer on the last flight). Data from these was sent to the ground during the flight. The astronaut normally drank water and ate food pellets.

Once in orbit, the spacecraft could be rotated in three directions: along its longitudinal axis (roll), left to right from the astronaut's point of view (yaw), and up or down (pitch). Movement was created by rocket-propelled thrusters which used hydrogen peroxide as a fuel. For orientation, the pilot could look through the window in front of him or he could look at a screen connected to a periscope with a camera which could be turned 360°.

The Mercury astronauts had taken part in the development of their spacecraft, and insisted that manual control, and a window, be elements of its design. As a result, spacecraft movement and other functions could be controlled three ways: remotely from the ground when passing over a ground station, automatically guided by onboard instruments, or manually by the astronaut, who could replace or override the two other methods. Experience validated the astronauts' insistence on manual controls. Without them, Gordon Cooper's manual reentry during the last flight would not have been possible.

The Mercury spacecraft design was modified three times by NASA between 1958 and 1959. After bidding by potential contractors had been completed, NASA selected the design submitted as "C" in November 1958. After it failed a test flight in July 1959, a final configuration, "D", emerged. The heat shield shape had been developed earlier in the 1950s through experiments with ballistic missiles, which had shown a blunt profile would create a shock wave that would lead most of the heat around the spacecraft. To further protect against heat, either a heat sink, or an ablative material, could be added to the shield. The heat sink would remove heat by the flow of the air inside the shock wave, whereas the ablative heat shield would remove heat by a controlled evaporation of the ablative material. After unmanned tests, the latter was chosen for manned flights. Apart from the capsule design, a rocket plane similar to the existing X-15 was considered. This approach was still too far from being able to make a spaceflight, and was consequently dropped. The heat shield and the stability of the spacecraft were tested in wind tunnels, and later in flight. The launch escape system was developed through unmanned flights. During a period of problems with development of the landing parachutes, alternative landing systems such as the Rogallo glider wing were considered, but ultimately scrapped.

The spacecraft were produced at McDonnell Aircraft, St. Louis, Missouri in clean rooms and tested in vacuum chambers at the McDonnell plant. The spacecraft had close to 600 subcontractors, such as Garrett AiResearch which built the spacecraft's environmental control system. Final quality control and preparations of the spacecraft were made at Hangar S at Cape Canaveral. NASA ordered 20 production spacecraft, numbered 1 through 20. Five of the 20, Nos. 10, 12, 15, 17, and 19, were not flown. Spacecraft No. 3 and No. 4 were destroyed during unmanned test flights. Spacecraft No. 11 sank and was recovered from the bottom of the Atlantic Ocean after 38 years. Some spacecraft were modified after initial production (refurbished after launch abort, modified for longer missions, etc.). A number of Mercury boilerplate spacecraft (made from non-flight materials or lacking production spacecraft systems) were also made by NASA and McDonnell. They were designed and used to test spacecraft recovery systems and the escape tower. McDonnell also built the spacecraft simulators used by the astronauts during training.

A small launch vehicle ( long) called Little Joe was used for unmanned tests of the launch escape system, using a Mercury capsule with an escape tower mounted on it. Its main purpose was to test the system at a point called max-q, at which air pressure against the spacecraft peaked, making separation of the launch vehicle and spacecraft most difficult. It was also the point at which the astronaut was subjected to the heaviest vibrations. The Little Joe rocket used solid-fuel propellant and was originally designed in 1958 by the NACA for suborbital manned flights, but was redesigned for Project Mercury to simulate an Atlas-D launch. It was produced by North American Aviation. It was not able to change direction; instead its flight depended on the angle from which it was launched. Its maximum altitude was fully loaded. A Scout launch vehicle was used for a single flight intended to evaluate the tracking network; however, it failed and was destroyed from the ground shortly after launch.

The Mercury-Redstone Launch Vehicle, an tall (with capsule and escape system) single-stage launch vehicle used for suborbital (ballistic) flights. It had a liquid-fueled engine that burned alcohol and liquid oxygen producing about 75,000 pounds of thrust, which was not enough for orbital missions. It was a descendant of the German V-2, and developed for the U.S. Army during the early 1950s. It was modified for Project Mercury by removing the warhead and adding a collar for supporting the spacecraft together with material for damping vibrations during launch. Its rocket motor was produced by North American Aviation and its direction could be altered during flight by its fins. They worked in two ways: by directing the air around them, or by directing the thrust by their inner parts (or both at the same time). Both the Atlas-D and Redstone launch vehicles contained an automatic abort sensing system which allowed them to abort a launch by firing the launch escape system if something went wrong. The Jupiter rocket, also developed by Von Braun's team at the Redstone Arsenal in Huntsville, was considered as well for intermediate Mercury suborbital flights at a higher speed and altitude than Redstone, but this plan was dropped when it turned out that man-rating Jupiter for the Mercury program would actually cost more than flying an Atlas due to economics of scale. Jupiter's only use other than as a missile system was for the short-lived Juno II launch vehicle and keeping a full staff of technical personnel around solely to fly a few Mercury capsules would result in excessively high costs.

Orbital missions required use of the Atlas LV-3B, a man-rated version of the Atlas D which was originally developed as the United States' first operational intercontinental ballistic missile (ICBM) by Convair for the Air Force during the mid-1950s. The Atlas was a "one-and-one-half-stage" rocket fueled by kerosene and liquid oxygen (LOX). The rocket by itself stood high; total height of the Atlas-Mercury space vehicle at launch was .

The Atlas first stage was a booster skirt with two engines burning liquid fuel. This together with the larger sustainer second stage gave it sufficient power to launch a Mercury spacecraft into orbit. Both stages fired from lift-off with the thrust from the second stage sustainer engine passing through an opening in the first stage. After separation from the first stage, the sustainer stage continued alone. The sustainer also steered the rocket by thrusters guided by gyroscopes. Smaller vernier rockets were added on its sides for precise control of maneuvers.

NASA announced the following seven astronauts – known as the Mercury Seven – on April 9, 1959:

Shepard became the first American in space by making a suborbital flight in May 1961. He went on to fly in the Apollo program and became the only Mercury astronaut to walk on the Moon. Gus Grissom, who became the second American in space, also participated in the Gemini and Apollo programs, but died in January 1967 during a pre-launch test for Apollo 1. Glenn became the first American to orbit the Earth in February 1962, then quit NASA and went into politics, serving as a US Senator from 1974 to 1999, and returned to space in 1998 as a Payload Specialist aboard STS-95. Deke Slayton was grounded in 1962, but remained with NASA and was appointed Chief Astronaut at the beginning of Project Gemini. He remained in the position of senior astronaut, in charge of space crew flight assignments among many other responsibilities, until towards the end of Project Apollo, when he resigned and began training to fly on the Apollo-Soyuz Test Project in 1975, which he successfully did. Gordon Cooper became the last to fly in Mercury and made its longest flight, and also flew a Gemini mission. Carpenter's Mercury flight was his only trip into space. Schirra flew the third orbital Mercury mission, and then flew a Gemini mission. Three years later, he commanded the first manned Apollo mission, becoming the only person to fly in all three of those programs.

One of the astronauts' tasks was publicity; they gave interviews to the press and visited project manufacturing facilities to speak with those who worked on Project Mercury. To make their travels easier, they requested and got jet fighters for personal use. The press was especially fond of John Glenn, who was considered the best speaker of the seven. They sold their personal stories to "Life" magazine which portrayed them as patriotic, God-fearing family men. "Life" was also allowed to be at home with the families while the astronauts were in space. During the project, Grissom, Carpenter, Cooper, Schirra and Slayton stayed with their families at or near Langley Air Force Base; Glenn lived at the base and visited his family in Washington DC on weekends. Shepard lived with his family at Naval Air Station Oceana in Virginia.

Other than Grissom, who was killed in the 1967 Apollo 1 fire, the other six survived past retirement and died between 1993 and 2016.

Prior to Project Mercury, there was no protocol for selecting astronauts so NASA would set a far reaching precedent with both their selection process and initial choices for astronaut. At the end of 1958, various ideas for the selection pool were discussed privately within the national government and the civilian space program, and also among the public at large. Initially, there was the idea to issue a widespread public call to volunteers. Thrill seekers such as rock climbers and acrobats would have been allowed to apply but this idea was quickly shot down by NASA officials who understood that an undertaking such as space flight required individuals with professional training and education in flight engineering. By late 1958, NASA officials decided to move forward with test pilots being the heart of their selection pool. On President Eisenhower's insistence, the group was further narrowed down to active duty military test pilots, which set the number of candidates at 508 men. who were either USN or USMC naval aviation pilots (NAPs), or USAF pilots of Senior or Command rating. These men had long military records which would give NASA officials more background information to base their decisions on. Furthermore, these men were adept at flying the most advanced aircraft to date, giving them the best qualifications for the new position of astronaut. However, this selection excluded women since there were no female military test pilots at the time. It also excluded civilian NASA X-15 pilot Neil Armstrong, though he had been selected by the U.S. Air Force in 1958 for its Man In Space Soonest program, which was replaced by Mercury. Although Armstrong had been a combat-experienced NAP during the Korean War, he left active duty in 1952. Armstrong became NASA's first civilian astronaut in 1962 when he was selected for NASA's second group, and became the first man on the Moon in 1969.

It was further stipulated that candidates should be between 25 and 40 years old, no taller than , and hold a college degree in a STEM subject. The college degree requirement excluded the USAF's X-1 pilot, then-Lt Col (later Brig Gen) Chuck Yeager, the first person to exceed the speed of sound. He later became a critic of the project, ridiculing the civilian space program, labeling astronauts as "spam in a can." John Glenn did not have a college degree either, but used influential friends to make the selection committee accept him. USAF Capt (later Col) Joseph Kittinger, a USAF fighter pilot and stratosphere balloonist, met all the requirements but preferred to stay in his contemporary project. Other potential candidates declined because they did not believe that manned spaceflight had a future beyond Project Mercury. From the original 508, 110 candidates were selected for an interview, and from the interviews, 32 were selected for further physical and mental testing. Their health, vision, and hearing were examined, together with their tolerance to noise, vibrations, g-forces, personal isolation, and heat. In a special chamber, they were tested to see if they could perform their tasks under confusing conditions. The candidates had to answer more than 500 questions about themselves and describe what they saw in different images. Navy Lt (later Capt) Jim Lovell, a NAP who was later an astronaut in the Gemini and Apollo programs, did not pass the physical tests. After these tests it was intended to narrow the group down to six astronauts, but in the end it was decided to keep seven.

The astronauts went through a training program covering some of the same exercises that were used in their selection. They simulated the g-force profiles of launch and reentry in a centrifuge at the Naval Air Development Center, and were taught special breathing techniques necessary when subjected to more than 6 g. Weightlessness training took place in aircraft, first on the rear seat of a two-seater fighter and later inside converted and padded cargo aircraft. They practiced gaining control of a spinning spacecraft in a machine at the Lewis Flight Propulsion Laboratory called the Multi-Axis Spin-Test Inertia Facility (MASTIF), by using an attitude controller handle simulating the one in the spacecraft. A further measure for finding the right attitude in orbit was star and Earth recognition training in planetaria and simulators. Communication and flight procedures were practiced in flight simulators, first together with a single person assisting them and later with the Mission Control Center. Recovery was practiced in pools at Langley, and later at sea with frogmen and helicopter crews.

A Redstone rocket was used to boost the capsule for 2 minutes and 30 seconds to an altitude of ; the capsule continued ascending on a ballistic curve after booster separation. The launch escape system was jettisoned at the same time. At the top of the curve, the spacecraft's retrorockets were fired for testing purposes; they were not necessary for reentry because orbital speed had not been attained. The spacecraft landed in the Atlantic Ocean. The suborbital mission took about 15 minutes, had an apogee altitude of , and a downrange distance of . From the time of booster-spacecraft separation until reentry where air started to slow down the spacecraft, the pilot would experience weightlessness as shown on the image. The recovery procedure would be the same as an orbital mission.

Preparations for a mission started a month in advance with the selection of the primary and back-up astronaut; they would practice together for the mission. For three days prior to launch, the astronaut went through a special diet to minimize his need for defecating during the flight. On the morning of the trip he typically ate a steak breakfast. After having sensors applied to his body and being dressed in the pressure suit, he started breathing pure oxygen to prepare him for the atmosphere of the spacecraft. He arrived at the launch pad, took the elevator up the launch tower and entered the spacecraft two hours before launch. Once the astronaut was secured inside, the hatch was bolted, the launch area evacuated and the mobile tower rolled back. After this, the launch vehicle was filled with liquid oxygen. The entire procedure of preparing for launch and launching the spacecraft followed a time table called the countdown. It started a day in advance with a pre-count, in which all systems of the launch vehicle and spacecraft were checked. After that followed a 15-hour hold, during which pyrotechnics were installed. Then came the main countdown which for orbital flights started 6½ hours before launch (T – 390 min), counted backwards to launch (T = 0) and then forward until orbital insertion (T + 5 min).

On an orbital mission, the Atlas' rocket engines were ignited four seconds before lift-off. The launch vehicle was held to the ground by clamps and then released when sufficient thrust was built up at lift-off (A). After 30 seconds of flight, the point of maximum dynamic pressure against the vehicle was reached, at which the astronaut felt heavy vibrations. After 2 minutes and 10 seconds, the two outboard booster engines shut down and were released with the aft skirt, leaving the center sustainer engine running (B). At this point, the launch escape system was no longer needed, and was separated from the spacecraft by its jettison rocket (C). The space vehicle moved gradually to a horizontal attitude until, at an altitude of , the sustainer engine shut down and the spacecraft was inserted into orbit (D). This happened after 5 minutes and 10 seconds in a direction pointing east, whereby the spacecraft would gain speed from the rotation of the Earth. Here the spacecraft fired the three posigrade rockets for a second to separate it from the launch vehicle. Just before orbital insertion and sustainer engine cutoff, g-loads peaked at 8 g (6 g for a suborbital flight). In orbit, the spacecraft automatically turned 180°, pointed the retropackage forward and its nose 14.5° downward and kept this attitude for the rest of the orbital phase of the mission, as it was necessary for communication with the ground.

Once in orbit, it was not possible for the spacecraft to change its trajectory except by initiating reentry. Each orbit would typically take 88 minutes to complete. The lowest point of the orbit, called perigee, was at the point where the spacecraft entered orbit and was about , and the highest point, called apogee, was on the opposite side of Earth and was about . When leaving orbit (E) the angle downward was increased to 34°, which was the angle of retrofire. Retrorockets fired for 10 seconds each (F) in a sequence where one started 5 seconds after the other. During reentry (G), the astronaut would experience about 8 g (11–12 g on a suborbital mission). The temperature around the heat shield rose to and at the same time, there was a two-minute radio blackout due to ionization of the air around the spacecraft.

After reentry, a small, drogue parachute (H) was deployed at for stabilizing the spacecraft's descent. The main parachute (I) was deployed at starting with a narrow opening that opened fully in a few seconds to lessen the strain on the lines. Just before hitting the water, the landing bag inflated from behind the heat shield to reduce the force of impact (J). Upon landing the parachutes were released. An antenna (K) was raised and sent out signals that could be traced by ships and helicopters. Further, a green marker dye was spread around the spacecraft to make its location more visible from the air. Frogmen brought in by helicopters inflated a collar around the craft to keep it upright in the water. The recovery helicopter hooked onto the spacecraft and the astronaut blew the escape hatch to exit the capsule. He was then hoisted aboard the helicopter that finally brought both him and the spacecraft to the ship.

The number of personnel supporting a Mercury mission was typically around 18,000, with about 15,000 people associated with recovery. Most of the others followed the spacecraft from the World Wide Tracking Network, a chain of 18 stations placed around the equator, which was based on a network used for satellites and made ready in 1960. It collected data from the spacecraft and provided two-way communication between the astronaut and the ground. Each station had a range of and a pass typically lasted 7 minutes. Mercury astronauts on the ground would take part of the Capsule Communicator or CAPCOM who communicated with the astronaut in orbit. Data from the spacecraft was sent to the ground, processed at the Goddard Space Center and relayed to the Mercury Control Center at Cape Canaveral. In the Control Center, the data was displayed on boards on each side of a world map, which showed the position of the spacecraft, its ground track and the place it could land in an emergency within the next 30 minutes.

The World Wide Tracking Network went on to serve subsequent space programs, until it was replaced by a satellite relay system in the 1980s Mission Control Center was moved from Cape Canaveral to Houston in 1965.

On April 12, 1961 the Soviet cosmonaut Yuri Gagarin became the first person in space on an orbital flight. Alan Shepard became the first American in space on a suborbital flight three weeks later, on May 5, 1961. John Glenn, the third Mercury astronaut to fly, became the first American to reach orbit on February 20, 1962, but only after the Soviets had launched a second cosmonaut, Gherman Titov, into a day-long flight in August 1961. Three more Mercury orbital flights were made, ending on May 16, 1963 with a day-long, 22 orbit flight. However, the Soviet Union ended its Vostok program the next month, with the human spaceflight endurance record set by the 82-orbit, almost 5-day Vostok 5 flight.

All of the six manned Mercury flights were successful, though some planned flights were cancelled during the project (see below). The main medical problems encountered were simple personal hygiene, and post-flight symptoms of low blood pressure. The launch vehicles had been tested through unmanned flights, therefore the numbering of manned missions did not start with 1. Also, there were two separately numbered series: MR for "Mercury-Redstone" (suborbital flights), and MA for "Mercury-Atlas" (orbital flights). These names were not popularly used, since the astronauts followed a pilot tradition, each giving their spacecraft a name. They selected names ending with a "7" to commemorate the seven astronauts (Mercury Seven). Times given are Universal Coordinated Time, local time + 5 hours.

The 20 unmanned flights used Little Joe, Redstone, and Atlas launch vehicles. They were used to develop the launch vehicles, launch escape system, spacecraft and tracking network. One flight of a Scout rocket attempted to launch an unmanned satellite for testing the ground tracking network, but failed to reach orbit. The Little Joe program used seven airframes for eight flights, of which three were successful. The second Little Joe flight was named Little Joe 6, because it was inserted into the program after the first 5 airframes had been allocated.

Nine of the planned flights were cancelled. Suborbital flights were planned for four other astronauts but the number of flights was cut down gradually and finally all remaining were cancelled after Titov's flight. Mercury-Atlas 9 was intended to be followed by more one-day flights and even a three-day flight but with the coming of the Gemini Project it seemed unnecessary. The Jupiter booster was, as mentioned above, intended to be used for different purposes.

The project was delayed by 22 months, counting from the beginning until the first orbital mission. It had a dozen prime contractors, 75 major subcontractors, and about 7200 third-tier subcontractors, who together employed two million people. An estimate of its cost made by NASA in 1969 gave $392.6 million ($ adjusted for inflation), broken down as follows: Spacecraft: $135.3 million, launch vehicles: $82.9 million, operations: $49.3 million, tracking operations and equipment: $71.9 million and facilities: $53.2 million.

Today the Mercury program is commemorated as the first manned American space program. It did not win the race against the Soviet Union, but gave back national prestige and was scientifically a successful precursor of later programs such as Gemini, Apollo and Skylab. During the 1950s, some experts doubted that manned spaceflight was possible. Still when John F. Kennedy was elected president, many including he had doubts about the project. As president he chose to support the programs a few months before the launch of "Freedom 7", which became a great public success. Afterwards, a majority of the American public supported manned spaceflight, and within a few weeks, Kennedy announced a plan for a manned mission to land on the Moon and return safely to Earth before the end of the 1960s. The six astronauts who flew were awarded medals, driven in parades and two of them were invited to address a joint session of the U.S. Congress. As a response to the selection criteria, which ruled out women, a private project was founded in which 13 women pilots successfully underwent the same tests as the men in Project Mercury. It was named Mercury 13 by the media Despite this effort, NASA did not select female astronauts until 1978 for the Space Shuttle.

In 1964, a monument commemorating Project Mercury was unveiled near Launch Complex 14 at Cape Canaveral, featuring a metal logo combining the symbol of Mercury with the number 7. In 1962, the United States Postal Service honored the Mercury-Atlas 6 flight with a Project Mercury commemorative stamp, the first U.S. postal issue to depict a manned spacecraft. On film, the program was portrayed in "The Right Stuff" a 1983 adaptation of Tom Wolfe's 1979 book of the same name. On February 25, 2011, the Institute of Electrical and Electronic Engineers, the world's largest technical professional society, awarded Boeing (the successor company to McDonnell Aircraft) a Milestone Award for important inventions which debuted on the Mercury spacecraft.

The spacecraft that flew, together with some that did not are on display in the United States. "Friendship 7" (capsule No. 13) went on a global tour, popularly known as its "fourth orbit". 

Commemorative patches were designed by entrepreneurs after the Mercury program to satisfy collectors.



</doc>
<doc id="19813" url="https://en.wikipedia.org/wiki?curid=19813" title="Gaius Maecenas">
Gaius Maecenas

Gaius Cilnius Maecenas (; 15 April 68 BC – 8 BC) was an ally, friend and political advisor to Octavian (who was to become the first Emperor of Rome as Caesar Augustus) as well as an important patron for the new generation of Augustan poets, including both Horace and Virgil. During the reign of Augustus, Maecenas served as a quasi-culture minister to the Emperor but in spite of his wealth and power he chose not to enter the Senate, remaining of equestrian rank.

His name has become a byword for a wealthy, generous and enlightened patron of the arts.

Expressions in Propertius seem to imply that Maecenas had taken some part in the campaigns of Mutina, Philippi and Perugia. He prided himself on his ancient Etruscan lineage, and claimed descent from the princely house of the Cilnii, who excited the jealousy of their townsmen by their preponderant wealth and influence at Arretium in the 4th century BC. Horace makes reference to this in his address to Maecenas at the opening of his first books of "Odes" with the expression "atavis edite regibus" (descendant of kings). Tacitus refers to him as "Cilnius Maecenas"; it is possible that "Cilnius" was his mother's nomen – or that Maecenas was in fact a cognomen.
The Gaius Maecenas mentioned in Cicero as an influential member of the equestrian order in 91 BC may have been his grandfather, or even his father. The testimony of Horace and Maecenas's own literary tastes imply that he had profited from the highest education of his time.

His great wealth may have been in part hereditary, but he owed his position and influence to his close connection with the Emperor Augustus. He first appears in history in 40 BC, when he was employed by Octavian in arranging his marriage with Scribonia, and afterwards in assisting to negotiate the Treaty of Brundisium and the reconciliation with Mark Antony. As a close friend and advisor he had even acted as deputy for Augustus when he was abroad.

It was in 38 BC that Horace was introduced to Maecenas, who had before this received Lucius Varius Rufus and Virgil into his intimacy. In the "Journey to Brundisium," in 37, Maecenas and Marcus Cocceius Nerva – great-grandfather of the future emperor Nerva – are described as having been sent on an important mission, and they were successful in patching up, by the Treaty of Tarentum, a reconciliation between the two claimants for supreme power. During the Sicilian war against Sextus Pompeius in 36, Maecenas was sent back to Rome, and was entrusted with supreme administrative control in the city and in Italy. He was vicegerent of Octavian during the campaign that led to the Battle of Actium, when, with great promptness and secrecy, he crushed the conspiracy of Lepidus the Younger; during the subsequent absences of his chief in the provinces he again held the same position.
During the latter years of his life he fell somewhat out of favour with his master. Suetonius attributes the loss of the imperial favour to Maecenas' having indiscreetly revealed to Terentia, his beautiful but difficult wife, the discovery of the conspiracy in which her brother Lucius Licinius Varro Murena was implicated, but according to Cassius Dio (writing in the early 3rd century AD) it was due to the emperor's relations with Terentia. Maecenas died in 8 BC, leaving the emperor sole heir to his wealth.

Opinions were much divided in ancient times as to his personal character; but the testimony as to his administrative and diplomatic ability was unanimous. He enjoyed the credit of sharing largely in the establishment of the new order of things, of reconciling parties, and of carrying the new empire safely through many dangers. To his influence especially was attributed the more humane policy of Octavian after his first alliance with Antony and Lepidus. The best summary of his character as a man and a statesman, by Marcus Velleius Paterculus, describes him as "of sleepless vigilance in critical emergencies, far-seeing and knowing how to act, but in his relaxation from business more luxurious and effeminate than a woman." Expressions in the "Odes of Horace" seem to imply that Maecenas was deficient in the robustness of fibre which Romans liked to imagine was characteristic of their city.

Maecenas is most famous for his support of young poets, hence his name has become the eponym for a ""patron of arts"". He supported Virgil who wrote the "Georgics" in his honour. It was Virgil, impressed with examples of Horace's poetry, who introduced Horace to Maecenas. Indeed, Horace begins the first poem of his "Odes" ("Odes" I.i) by addressing his new patron. Maecenas gave him full financial support as well as an estate in the Sabine mountains. Propertius and the minor poets Varius Rufus, Plotius Tucca, Valgius Rufus and Domitius Marsus also were his protégés.

His character as a munificent patron of literature - which has made his name a household word - is gratefully acknowledged by the recipients of it and attested by the regrets of the men of letters of a later age, expressed by Martial and Juvenal. His patronage was exercised, not from vanity or a mere dilettante love of letters, but with a view to the higher interest of the state. He recognized in the genius of the poets of that time not only the truest ornament of the court, but the power of reconciling men's minds to the new order of things, and of investing the actual state of affairs with an ideal glory and majesty. The change in seriousness of purpose between the "Eclogues" and the "Georgics" of Virgil was in a great measure the result of the direction given by the statesman to the poet's genius. A similar change between the earlier odes of Horace, in which he declares his epicurean indifference to affairs of state, and the great national odes of the third book has been ascribed by some to the same guidance. However, since the organization of the Odes is not entirely chronological, and their composition followed both books of Satires and the Epodes, this argument is plainly specious; but doubtless the milieu of Maecenas' circle influenced the writing of the Roman Odes (III.1-6) and others such as the ode to Pollio, Motum ex Metello (II.1).

Maecenas endeavoured also to divert the less masculine genius of Propertius from harping continually on his love to themes of public interest, an effort which to some extent backfired in the ironic elegies of Book III. But if the motive of his patronage had been merely political, it never could have inspired the affection which it did in its recipients. The great charm of Maecenas in his relation to the men of genius who formed his circle was his simplicity, cordiality and sincerity. Although not particular in the choice of some of the associates of his pleasures, he admitted none but men of worth to his intimacy, and when once admitted they were treated like equals. Much of the wisdom of Maecenas probably lives in the "Satires" and "Epistles" of Horace. It has fallen to the lot of no other patron of literature to have his name associated with works of such lasting interest as the "Georgics" of Virgil, the first three books of Horace's "Odes," and the first book of his "Epistles."

Maecenas also wrote literature himself in both prose and verse. The some twenty fragments that remain show that he was less successful as an author than as a judge and patron of literature.
His prose works on various subjects – "Prometheus," dialogues like "Symposium" (a banquet at which Virgil, Horace and Messalla were present), "De cultu suo " (on his manner of life) and a poem "In Octaviam" ("Against Octavia") of which the content is unclear - were ridiculed by Augustus, Seneca and Quintilian for their strange style, the use of rare words and awkward transpositions.
According to Dio Cassius, Maecenas was also the inventor of a system of shorthand.

Maecenas sited his famous gardens, the first gardens in the Hellenistic-Persian garden style in Rome, on the Esquiline Hill, atop the Servian Wall and its adjoining necropolis, near the gardens of Lamia. It contained terraces, libraries and other aspects of Roman culture. Maecenas is said to have been the first to construct a swimming bath of hot water in Rome, which may have been in the gardens. The luxury of his gardens and villas incurred the displeasure of Seneca the Younger.
Though the approximate site is known, it is not easy to reconcile literary indications to determine the gardens' exact location, whether or not they lay on both sides of the Servian "ager" and both north and south of the porta Esquilina. Common graves of the archaic Esquiline necropolis have been found near the north-west corner of the modern Piazza Vittorio Emanuele, that is, outside the Esquiline gate of antiquity and north of the "via Tiburtina vetus"; most probably the "horti Maecenatiani" extended north from this gate and road on both sides of the "ager". The "Auditorium of Maecenas", a probable venue for dining and entertainment, may still be visited (upon reservation) on Largo Leopardi near Via Merulana.

The gardens became imperial property after Maecenas's death, and Tiberius lived there after his return to Rome in 2 AD. Nero connected them with the Palatine Hill via his Domus Transitoria, and viewed the burning of that from the turris Maecenatiana. This turris was probably the "molem propinquam nubibus arduis" ("the pile, among the clouds") mentioned by Horace.

Whether the "horti Maecenatiani" bought by Fronto actually were the former gardens of Maecenas is unknown, and the "domus Frontoniana" mentioned in the twelfth century by Magister Gregorius may also refer to the gardens of Maecenas.

His name has become a byword in many languages for a well-connected and wealthy patron, a role which he is celebrated for in the two poems, the "Elegiae in Maecenatem", which were written after his death and collected in the "Appendix Vergiliana". In various languages, it has even been coined into a word for (private) patronage (mainly cultural, but sometimes wider, usually perceived as more altruistic than sponsorship). A verse of the student song "Gaudeamus igitur" wishes longevity upon the charity of the students' benefactors ("Maecenatum", genitive plural of "Maecenas").

In Poland and Western Ukraine, a lawyer would customarily be addressed with the honorific "Pan Mecenas", as lawyers were considered to be philanthropists and patrons of the arts.

In "The Great Gatsby", along with Midas and J. P. Morgan, Maecenas is one of the three famous wealthy men whose secrets narrator Nick Carraway hopes to find in the books he buys for his home library.

The word "Maecenas", in the sense of cultural benefactor, was the penultimate word used in the 2009 Scripps National Spelling Bee, on May 28, 2009. It was spelled incorrectly.

Maecenas was portrayed by Alex Wyndham in the second season of the 2005 HBO television series "Rome". He was portrayed by Russell Barr in the made-for-TV movie . He is also featured in one episode of the second series of Plebs on ITV.





</doc>
<doc id="19814" url="https://en.wikipedia.org/wiki?curid=19814" title="Meander (disambiguation)">
Meander (disambiguation)

A meander is a bend in a river.

Meander may also refer to:






</doc>
<doc id="19818" url="https://en.wikipedia.org/wiki?curid=19818" title="March 16">
March 16






</doc>
<doc id="19820" url="https://en.wikipedia.org/wiki?curid=19820" title="Magick (Thelema)">
Magick (Thelema)

Magick, in the context of Aleister Crowley's Thelema, is a term used to show and differentiate the occult from performance magic and is defined as "the Science and Art of causing Change to occur in conformity with Will", including both "mundane" acts of will as well as ritual magic. Crowley wrote that "it is theoretically possible to cause in any object any change of which that object is capable by nature". John Symonds and Kenneth Grant attach a deeper occult significance to this preference.

Crowley saw Magick as the essential method for a person to reach true understanding of the self and to act according to one's true will, which he saw as the reconciliation "between freewill and destiny." Crowley describes this process in his "Magick, Book 4":

The term itself is an Early Modern English spelling for "magic", used in works such as the 1651 translation of Heinrich Cornelius Agrippa's "De Occulta Philosophia", "Three Books of Occult Philosophy, or Of Magick". Aleister Crowley chose the spelling to differentiate his practices and rituals from stage magic and the term has since been re-popularised by those who have adopted elements of his teachings.

Crowley defined Magick as "the science and art of causing change to occur in conformity with will." He goes on to elaborate on this, in one postulate, and twenty eight theorems. His first clarification on the matter is that of a postulate, in which he states "ANY required change may be effected by the application of the proper kind and degree of Force in the proper manner, through the proper medium to the proper object." He goes on further to state:

Crowley made many theories for the paranormal effects of Magick; however, as magicians and mystics had done before him and continue to do after him, Crowley dismissed such effects as useless:

Even so, Crowley recognized that paranormal effects and magical powers have some level of value for the individual:

There are several ways to view what Magick is. Again, at its most broad, it can be defined as any willed action leading to intended change. It can also be seen as the general set of methods used to accomplish the Great Work of mystical attainment. At the practical level, Magick most often takes several practices and forms of ritual, including banishing, invocation and evocation, eucharistic ritual, consecration and purification, astral travel, yoga, sex magic, and divination.

The professed purpose of banishing rituals is to eliminate forces that might interfere with a magical operation, and they are often performed at the beginning of an important event or ceremony (although they can be performed for their own sake as well). The area of effect can be a magick circle, a room, or the magician himself. The general theory of Magick proposes that there are various forces which are represented by the classical elements (air, earth, fire, and water), the planets, the signs of the Zodiac, and adjacent spaces in the astral world. Magick also proposes that various spirits and non-corporeal intelligences can be present. Banishings are performed in order to "clean out" these forces and presences. It is not uncommon to believe that banishings are more psychological than anything else, used to calm and balance the mind, but that the effect is ultimately the same—a sense of cleanliness within the self and the environment. There are many banishing rituals, but most are some variation on two of the most common—"The Star Ruby" and the Lesser Banishing Ritual of the Pentagram.

Crowley describes banishing in his "Magick, Book 4" (ch.13):

However, he further asserts:

Purification is similar in theme to banishing, but is a more rigorous process of preparing the self and her temple for serious spiritual work. Crowley mentions that ancient magicians would purify themselves through arduous programs, such as through special diets, fasting, sexual abstinence, keeping the body meticulously tidy, and undergoing a complicated series of prayers. He goes on to say that purification no longer requires such activity, since the magician can purify the self via willed intention. Specifically, the magician labors to purify the mind and body of all influences which may interfere with the Great Work:

Crowley recommended symbolically ritual practices, such as bathing and robing before a main ceremony: "The bath signifies the removal of all things extraneous or antagonistic to the one thought. The putting on of the robe is the positive side of the same operation. It is the assumption of the frame of mind suitable to that one thought."

Consecration is an equally important magical operation. It is essentially the dedication, usually of a ritual instrument or space, to a specific purpose. In "Magick, Book 4" (ch.13), Crowley writes:

Invocation is the bringing in or identifying with a particular deity or spirit. Crowley wrote of two keys to success in this arena: to "inflame thyself in praying" and to "invoke often". For Crowley, the single most important invocation, or any act of Magick for that matter, was the invocation of one's Holy Guardian Angel, or "secret self", which allows the adept to know his or her True Will.

Crowley describes the experience of invocation:

Crowley ("Magick, Book 4") discusses three main categories of invocation, although "in the great essentials these three methods are one. In each case the magician identifies himself with the Deity invoked."


Another invocatory technique that the magician can employ is called the "assumption of godforms"—where with "concentrated imagination of oneself in the symbolic shape of any God, one should be able to identify oneself with the idea which [the god] represents." A general method involves positioning the body in a position that is typical for a given god, imagining that the image of the god is coinciding with or enveloping the body, accompanied by the practice of "vibration" of the appropriate god-name(s).

There is a distinct difference between invocation and evocation, as Crowley explains:

Generally, evocation is used for two main purposes: to gather information and to obtain the services or obedience of a spirit or demon. Crowley believed that the most effective form of evocation was found in the grimoire on Goetia (see below), which instructs the magician in how to safely summon forth and command 72 infernal spirits. However, it is equally possible to evoke angelic beings, gods, and other intelligences related to planets, elements, and the Zodiac.

Unlike with invocation, which involves a calling in, evocation involves a calling forth, most commonly into what is called the "triangle of art."


The word "eucharist" originally comes from the Greek word for thanksgiving. However, within Magick, it takes on a special meaning—the transmutation of ordinary things (usually food and drink) into divine sacraments, which are then consumed. The object is to infuse the food and drink with certain properties, usually embodied by various deities, so that the adept takes in those properties upon consumption. Crowley describes the process of the regular practice of eucharistic ritual:

There are several eucharistic rituals within the magical canon. Two of the most well known are The Mass of the Phoenix and The Gnostic Mass. The first is a ritual designed for the individual, which involves sacrificing a "Cake of Light" (a type of bread that serves as the host) to Ra (i.e. the Sun) and infusing a second Cake with the adept's own blood (either real or symbolic, in a gesture reflecting the myth of the Pelican cutting its own breast to feed its young) and then consuming it with the words, "There is no grace: there is no guilt: This is the Law: Do what thou wilt!" The other ritual, The Gnostic Mass, is a very popular public ritual (although it can be practiced privately) that involves a team of participants, including a Priest and Priestess. This ritual is an enactment of the mystical journey that culminates with the Mystic Marriage and the consumption of a Cake of Light and a goblet of wine (a process termed "communication"). Afterwards, each Communicant declares, "There is no part of me that is not of the gods!"

Yoga, as Crowley interprets it, involves several key components. The first is Asana, which is the assumption (after eventual success) of any easy, steady and comfortable posture so as to maintain a good physique which complements the high level of enlightenment that meditation is accompanied with. Next is Pranayama, which is the control of breath. Yogis believe that the number of breaths a human takes are counted before one is even born and thus, by controlling the intake one may also be able to control the life. Mantram, the use of mantras enables the subject to use the knowledge of the Vedas "Atharva Veda" in this context adequately. Yama and Niyama are the adopted moral or behavioral codes (of the adept's choosing) that will be least likely to excite the mind. Pratyahara is the stilling of the thoughts so that the mind becomes quiet. Dharana is the beginning of concentration, usually on a single shape, like a triangle, which eventually leads to Dhyana, the loss of distinction between object and subject, which can be described as the annihilation of the ego (or sense of a separate self). The final stage is Samādhi—Union with the All; it is considered to be the utmost level of awareness that one could possibly achieve. According to Hindu mythology, one of their main three deities, Shiva, had mastered this and thus was bestowed upon with stupendous power and control.

The art of divination is generally employed for the purpose of obtaining information that can guide the adept in his Great Work. The underlying theory states that there exists intelligences (either outside of or inside the mind of the diviner) that can offer accurate information within certain limits using a language of symbols. Normally, divination within Magick is not the same as fortune telling, which is more interested in predicting future events. Rather, divination tends to be more about discovering information about the nature and condition of things that can help the magician gain insight and to make better decisions.

There are literally hundreds of different divinatory techniques in the world. However, Western occult practice mostly includes the use of astrology (calculating the influence of heavenly bodies), bibliomancy (reading random passages from a book, such as Liber Legis or the I Ching), tarot (a deck of 78 cards, each with symbolic meaning, usually laid out in a meaningful pattern), and geomancy (a method of making random marks on paper or in earth that results in a combination of sixteen patterns).

It is an accepted truism within Magick that divination is imperfect. As Crowley writes, "In estimating the ultimate value of a divinatory judgment, one must allow for more than the numerous sources of error inherent in the process itself. The judgment can do no more than the facts presented to it warrant. It is naturally impossible in most cases to make sure that some important factor has not been omitted [...] One must not assume that the oracle is omniscient."

The Tree of Life is a tool used to categorize and organize various mystical concepts. At its most simple level, it is composed of ten spheres, or emanations, called sephiroth (sing. "sephira") which are connected by twenty two paths. The sephiroth are represented by the planets and the paths by the characters of the Hebrew alphabet, which are subdivided by the four classical elements, the seven classical planets, and the twelve signs of the Zodiac. Within the western magical tradition, the Tree is used as a kind of conceptual filing cabinet. Each sephira and path is assigned various ideas, such as gods, cards of the Tarot, astrological planets and signs, elements, etc.

Crowley considered a deep understanding of the Tree of Life to be essential to the magician:

Similar to yoga, learning the Tree of Life is not so much Magick as it is a way to map out one's spiritual universe. As such, the adept may use the Tree to determine a destination for astral travel, to choose which gods to invoke for what purposes, et cetera. It also plays an important role in modeling the spiritual journey, where the adept begins in Malkuth, which is the every-day material world of phenomena, with the ultimate goal being at Kether, the sphere of Unity with the All.

A magical record is a journal or other source of documentation containing magical events, experiences, ideas, and any other information that the magician may see fit to add. There can be many purposes for such a record, such as recording evidence to verify the effectiveness of specific procedures (per the scientific method that Aleister Crowley claimed should be applied to the practice of Magick) or to ensure that data may propagate beyond the lifetime of the magician. Benefits of this process vary, but usually include future analysis and further education by the individual and/or associates with whom the magician feels comfortable in revealing such intrinsically private information.

Crowley was highly insistent upon the importance of this practice. As he writes in Liber E, "It is absolutely necessary that all experiments should be recorded in detail during, or immediately after, their performance ... The more scientific the record is, the better. Yet the emotions should be noted, as being some of the conditions. Let then the record be written with sincerity and care; thus with practice it will be found more and more to approximate to the ideal." Other items he suggests for inclusion include the physical and mental condition of the experimenter, the time and place, and environmental conditions, including the weather.

As with Magick itself, a "magical weapon" is any instrument used to bring about intentional change. As Crowley writes, "Illustration: It is my Will to inform the World of certain facts within my knowledge. I therefore take "magical weapons", pen, ink, and paper ... The composition and distribution of this book is thus an act of Magick by which I cause Changes to take place in conformity with my Will." With that said, in practice, magical weapons are usually specific, consecrated items used within ceremonial magic. There is no hard and fast rule for what is or isn't a magical weapon—if a magician considers it such a weapon, then it is. However, there does exist a set of magical weapons that have particular uses and symbolic meanings. Common weapons include the dagger (or athame in neopagan parlance), sword, wand, holy oil, cup (or graal), disk (or pentacle), oil lamp, bell, and thurible (or censer).

A magical formula is generally a name, word, or series of letters whose meaning illustrates principles and degrees of understanding that are often difficult to relay using other forms of speech or writing. It is a concise means to communicate very abstract information through the medium of a word or phrase, usually regarding a process of spiritual or mystical change. Common formulae include INRI, IAO, ShT, AUMGN, NOX, and LVX.

These words often have no intrinsic meaning in and of themselves. However, when deconstructed, each individual letter may refer to some universal concept found in the system that the formula appears. Additionally, in grouping certain letters together one is able to display meaningful sequences that are considered to be of value to the spiritual system that utilizes them (e.g. spiritual hierarchies, historiographic data, psychological stages, etc.)

In magical rituals involving the invocation of deities, a vocal technique called "vibration" is commonly used. This was a basic aspect of magical training for Crowley, who described it in "Liber O." According to that text, vibration involves a physical set of steps, starting in a standing position, breathing in through the nose while imagining the name of the god entering with the breath, imagining that breath travelling through the entire body, stepping forward with the left foot while throwing the body forward with arms outstretched, visualizing the name rushing out when spoken, ending in an upright stance, with the right forefinger placed upon the lips. According to Crowley in "Liber O", success in this technique is signaled by physical exhaustion and "though only by the student himself is it perceived, when he hears the name of the God vehemently roared forth, as if by the concourse of ten thousand thunders; and it should appear to him as if that Great Voice proceeded from the Universe, and not from himself."

In general ritual practice, "vibration" can also refer to a technique of saying a god-name or a magical formula in a long, drawn-out fashion (i.e. with a full, deep breath) that employs the nasal passages, such that the sound feels and sounds "vibrated'. This is known as Galdering.







</doc>
<doc id="19821" url="https://en.wikipedia.org/wiki?curid=19821" title="Marcus Claudius Tacitus">
Marcus Claudius Tacitus

Tacitus (; ; c. 200 – June 276), was Roman Emperor from 275 to 276. During his short reign he campaigned against the Goths and the Heruli, for which he received the title "Gothicus Maximus".

Tacitus was born in Interamna (Terni), in Italia. He circulated copies of the historian Gaius Cornelius Tacitus' work, which was barely read at the time, perhaps contributing to the partial survival of the historian's work; however, modern historiography rejects his claimed descent from the historian as a fabrication. In the course of his long life he discharged the duties of various civil offices, holding the consulship twice, once under Valerian and again in 273, earning universal respect.

After the assassination of Aurelian, the army, apparently in remorse at the effects of the previous centuries' military license, which had brought about the death of the well-liked emperor, relinquished the right of choosing his successor to the senate. Initially, the Senate hesitated to accept the responsibility, but when the delay had gone on eight months from Aurelian's death it at last determined to settle the matter and offered the throne to the aged "Princeps Senatus", Tacitus. 

Tacitus, after ascertaining the sincerity of the Senate's regard for him, accepted their nomination, and the choice was cordially ratified by the army. This was the last time the Senate elected a Roman Emperor. The interregnum between Aurelian and Tacitus had been quite long, and there is substantial evidence that Aurelian's wife, Ulpia Severina, ruled in her own right before the election of Tacitus. Tacitus had been living in Campania before his election, and returned only reluctantly to the assembly of the senate in Rome, where he was elected.
He immediately asked the Senate to deify Aurelian, before arresting and executing Aurelian's murderers.

Amongst the highest concerns of the new reign was the restoration of the ancient powers of the senate. He granted substantial prerogatives to the senate, securing to them by law the appointment of the emperor, of the consuls, and the provincial governors, as well as supreme right of appeal from every court in the empire in its judicial function, and the direction of certain branches of the revenue in its long-abeyant administrative capacity However, after the reforms of Diocletian in the succeeding decades not a vestige would be left of these rights to the senate, though Tacitus' immediate successor, Probus, respected them.

Next he moved against the barbarian mercenaries that had been gathered by Aurelian to supplement Roman forces for his Eastern campaign. These mercenaries had plundered several towns in the Eastern Roman provinces after Aurelian had been murdered and the campaign cancelled. His half-brother, the Praetorian Prefect Florianus, and Tacitus himself won a victory against these tribes, among which were the Heruli, gaining the emperor the title "Gothicus Maximus".

On his way back to the west to deal with a Frankish and Alamannic invasion of Gaul, according to Aurelius Victor, Eutropius and the Historia Augusta, Tacitus died of fever at Tyana in Cappadocia in June 276. It was reported that he began acting strangely, declaring that he would alter the names of the months to honor himself, before succumbing to a fever. In a contrary account, Zosimus claims he was assassinated, after appointing one of his relatives to an important command in Syria.

He appears in Harry Sidebottom's historical fiction novel series "Warrior Of Rome".





</doc>
<doc id="19822" url="https://en.wikipedia.org/wiki?curid=19822" title="MV Tampa">
MV Tampa

MV "Tampa" is a roll-on/roll-off container ship completed in 1984 by Hyundai Heavy Industries Co., Ltd. in South Korea for the Norway-based firm, Wilhelmsen Lines Shipowning.

In August 2001, under Captain Arne Rinnan, a diplomatic dispute brewed between Australia, Norway, and Indonesia after "Tampa" rescued 438 Afghans from a distressed fishing vessel in international waters. The Afghans wanted passage to nearby Christmas Island. The Australian government sought to prevent this by refusing "Tampa" entry into Australian waters, insisting on their disembarkment elsewhere, and deploying the Special Air Service Regiment to board the ship. At the time of the incident, "Tampa" carried cargo worth , and 27 crew.

The crew of "Tampa" received the Nansen Refugee Award for 2002 from the United Nations High Commissioner for Refugees (UNHCR) for their efforts to follow international principles of saving people in distress at sea.

In October 2006, MV "Tampa" was one of two Wilhelmsen ships involved in a cocaine-smuggling operation intercepted by the New Zealand Customs Service and the Australian Federal Police. Twenty-seven kilograms of cocaine was allegedly attached to the side of the two cargo ships bound for Australia in purpose-built metal pods, although New Zealand authorities stated they did not believe the ship's crew or owners were involved.

In the 2006 episode of the TV show "MythBusters", a 1:550 scale model of MV "Tampa" was assembled, weighted with lead shot to simulate a full load of cargo, and used as a scale scientific test bed vehicle for determining whether ocean whirlpools are capable of sinking a large container ship.





</doc>
<doc id="19823" url="https://en.wikipedia.org/wiki?curid=19823" title="Maya numerals">
Maya numerals

The Mayan numeral system was the system to represent numbers and calendar dates in the Maya civilization. It was a vigesimal (base-20) positional numeral system. The numerals are made up of three symbols; zero (shell shape, with the plastron uppermost), one (a dot) and five (a bar). For example, thirteen is written as three dots in a horizontal row above two horizontal bars; sometimes it is also written as three vertical dots to the left of two vertical bars. With these three symbols each of the twenty vigesimal digits could be written.
Numbers after 19 were written vertically in powers of twenty. The Mayan used powers of twenty, just as our Hindu–Arabic numeral system uses powers of tens. For example, thirty-three would be written as one dot, above three dots atop two bars. The first dot represents "one twenty" or "1×20", which is added to three dots and two bars, or thirteen. Therefore, (1×20) + 13 = 33. Upon reaching 20 or 400, another row is started (20 or 8000, then 20 or 160,000, and so on). The number 429 would be written as one dot above one dot above four dots and a bar, or (1×20) + (1×20) + 9 = 429. 

Other than the bar and dot notation, Maya numerals were sometimes illustrated by face type glyphs or pictures. The face glyph for a number represents the deity associated with the number. These face number glyphs were rarely used, and are mostly seen on some of the most elaborate monumental carving.

Adding and subtracting numbers below 20 using Maya numerals is very simple.
Addition is performed by combining the numeric symbols at each level:<br>
If five or more dots result from the combination, five dots are removed and replaced by a bar. If four or more bars result, four bars are removed and a dot is added to the next higher row.

Similarly with subtraction, remove the elements of the subtrahend symbol from the minuend symbol:<br>
If there are not enough dots in a minuend position, a bar is replaced by five dots. If there are not enough bars, a dot is removed from the next higher minuend symbol in the column and four bars are added to the minuend symbol which is being worked on.

The "Long Count" portion of the Maya calendar uses a variation on the strictly vigesimal numbering. In the second position, only the digits up to 17 are used, and the place value of the third position is not 20×20 = 400, as would otherwise be expected, but 18×20 = 360, so that one dot over two zeros signifies 360. Presumably, this is because 360 is roughly the number of days in a year. (The Maya had however a quite accurate estimation of 365.2422 days for the solar year at least since the early Classic era.) Subsequent positions use all twenty digits and the place values continue as 18×20×20 = 7,200 and 18×20×20×20 = 144,000, etc.

Every known example of large numbers in the Maya system uses this 'modified vigesimal' system, with the third position representing multiples of 18×20. It is reasonable to assume, but not proven by any evidence, that the normal system in use was a pure base-20 system.

Several Mesoamerican cultures used similar numerals and base-twenty systems and the Mesoamerican Long Count calendar requiring the use of zero as a place-holder.The earliest use of a zero (on Stela 2 at Chiapa de Corzo, Chiapas) has a date of 36 BC.

Since the eight earliest Long Count dates appear outside the Maya homeland, it is assumed that the use of zero and the Long Count calendar predated the Maya, and was possibly the invention of the Olmec. Indeed, many of the earliest Long Count dates were found within the Olmec heartland. However, the Olmec civilization had come to an end by the 4th century BC, several centuries before the earliest known Long Count dates—which suggests that zero was "not" an Olmec discovery.

Mayan numerals were added to the Unicode Standard in June, 2018 with the release of version 11.0.

The Unicode block for Mayan Numerals is U+1D2E0–U+1D2FF:




</doc>
<doc id="19826" url="https://en.wikipedia.org/wiki?curid=19826" title="Michael Foot">
Michael Foot

Michael Mackintosh Foot (23 July 1913 – 3 March 2010) was a British Labour Party politician and man of letters. Foot began his career as a journalist, on "Tribune" and the "Evening Standard". He co-wrote the classic polemic against appeasement of Adolf Hitler, "Guilty Men", under a pseudonym.

Foot served as a Member of Parliament (MP) from 1945 to 1955 and again from 1960 until he retired in 1992. A passionate orator and associated with the left-wing of the Labour Party for most of his career, Foot was an ardent supporter of the Campaign for Nuclear Disarmament and British withdrawal from the European Economic Community. He was appointed to the Cabinet as Secretary of State for Employment under Harold Wilson in 1974, and he later served as Leader of the House of Commons under James Callaghan. He was also Deputy Leader of the Labour Party under Callaghan from 1976 to 1980.

Elected as a compromise candidate, Foot served as the Leader of the Labour Party, and Leader of the Opposition, from 1980 to 1983. His strongly left-wing political positions and criticisms of vacillating leadership made him an unpopular leader. Not particularly telegenic, he was also nicknamed "Worzel Gummidge" for his rumpled appearance. A centrist faction of the party broke away in 1981 to form the SDP. Foot led Labour into the 1983 general election, when the party obtained its lowest share of the vote since the 1918 general election and the fewest parliamentary seats it had had at any time since before 1945. He resigned after the election, and was succeeded as leader by Neil Kinnock.

Among the books he authored are "Guilty Men" (1940); "The Pen and the Sword" (1957), a biography of Jonathan Swift; and a biography of Aneurin Bevan.

Foot was born in Lipson Terrace, Plymouth, Devon, the fifth of seven children of Isaac Foot (1880–1960) and Eva (née Mackintosh, died 17 May 1946), who was Scottish. 
Isaac Foot was a solicitor and founder of the Plymouth law firm Foot and Bowden (which amalgamated with another firm to become Foot Anstey). Isaac Foot was an active member of the Liberal Party and was the Liberal Member of Parliament for Bodmin in Cornwall from 1922–24 and again from 1929–35, and a Lord Mayor of Plymouth.

Michael Foot was the brother of Sir Dingle Foot MP (1905–78), a Liberal and subsequently Labour MP; Hugh Foot, Baron Caradon (1907–90), Governor of Cyprus (1957–60) and representative of the United Kingdom at the United Nations from 1964–70; Liberal politician John Foot, Baron Foot (1909–99); Margaret Elizabeth Foot (1911–65), Jennifer Mackintosh Highet (born 1916) and Christopher Isaac Foot (1917–84). He was the uncle of campaigning journalist Paul Foot (1937–2004) and charity worker Oliver Foot (1946–2008).

Foot was educated at Plymouth College Preparatory School, Forres School in Swanage, and Leighton Park School in Reading. When he left Forres School, the headmaster sent a letter to his father in which he said “he has been the leading boy in the school in every way”. He then went on to read Philosophy, Politics and Economics at Wadham College, Oxford. Foot was a president of the Oxford Union. He also took part in the ESU USA Tour (the debating tour of the United States run by the English-Speaking Union). Upon graduating with a second-class degree in 1934, he took a job as a shipping clerk in Birkenhead. Foot was profoundly influenced by the poverty and unemployment that he witnessed in Liverpool, which was on a different scale from anything he had seen in Plymouth. A Liberal up to this time, Foot was converted to socialism by Oxford University Labour Club president David Lewis, a Canadian Rhodes scholar, and others: "... I knew him [at Oxford] when I was a Liberal [and Lewis] played a part in converting me to socialism." Foot joined the Labour Party and first stood for parliament, aged 22, at the 1935 general election, where he contested Monmouth. During the election, Foot criticised the Prime Minister, Stanley Baldwin, for seeking rearmament. In his election address, Foot contended that "the armaments race in Europe must be stopped now". Foot also supported unilateral disarmament, after multilateral disarmament talks at Geneva had broken down in 1933.

Foot became a journalist, working briefly on the "New Statesman", before joining the left-wing weekly "Tribune" when it was set up in early 1937 to support the Unity Campaign, an attempt to secure an anti-fascist United Front between Labour and other left-wing parties. The campaign's members were Stafford Cripps's (Labour-affiliated) Socialist League, the Independent Labour Party and the Communist Party of Great Britain (CP). Foot resigned in 1938 after the paper's first editor, William Mellor, was sacked for refusing to adopt a new CP policy of backing a Popular Front, including non-socialist parties, against fascism and appeasement. In a 1955 interview, Foot ideologically identified as a libertarian socialist.

On the recommendation of Aneurin Bevan, Foot was soon hired by Lord Beaverbrook to work as a writer on his "Evening Standard". (Bevan is supposed to have told Beaverbrook on the phone: "I've got a young bloody knight-errant here. They sacked his boss, so he resigned. Have a look at him.") At the outbreak of the Second World War, Foot volunteered for military service, but was rejected because of his chronic asthma. It was suggested in 2011 that he became a member of the secret Auxiliary Units.

In 1940, under the pen-name "Cato" he and two other Beaverbrook journalists (Frank Owen, editor of the "Standard", and Peter Howard of the "Daily Express") published "Guilty Men", which attacked the appeasement policy of the Chamberlain government; it became a runaway bestseller. (In so doing, Foot reversed his position of the 1935 election—when he had attacked the Conservatives as militaristic and demanded disarmament in the face of Nazi Germany.) Beaverbrook made Foot editor of the "Evening Standard" in 1942, when he was aged 28. During the war, Foot made a speech that was later featured in the documentary TV series "The World at War" broadcast in February 1974. Foot was speaking in defence of the "Daily Mirror", which had criticised the conduct of the war by the Churchill government. He mocked the notion that the Government would make no more territorial demands of other newspapers if they allowed the "Mirror" to be censored.

Foot left the "Standard" in 1945 to join the "Daily Herald" as a columnist. The "Daily Herald" was jointly owned by the TUC and Odhams Press, and was effectively an official Labour Party paper. He rejoined "Tribune" as editor from 1948 to 1952, and was again the paper's editor from 1955 to 1960. Throughout his political career he railed against the increasing corporate domination of the press.

Foot fought the Plymouth Devonport constituency in the 1945 general election. His election agent was Labour activist and lifelong friend Ron Lemin. He won the seat for Labour for the first time, holding it until his surprise defeat by Dame Joan Vickers at the 1955 general election. Until 1957, he was the most prominent ally of Aneurin Bevan, who had taken Cripps's place as leader of the Labour left, though Foot and Bevan fell out after Bevan renounced unilateral nuclear disarmament at the 1957 Labour Party conference.

Before the Cold War began in the late 1940s, Foot favoured a 'third way' foreign policy for Europe (he was joint author with Richard Crossman and Ian Mikardo of the pamphlet "Keep Left" in 1947), but in the wake of the communist seizure of power in Hungary and Czechoslovakia he and "Tribune" took a strongly anti-communist position, eventually embracing NATO.

Foot was however a critic of the West's handling of the Korean War, an opponent of West German rearmament in the early 1950s and a founder member of the Campaign for Nuclear Disarmament. Under his editorship, "Tribune" opposed both the British government's Suez adventure and the Soviet crushing of the Hungarian Revolution in 1956. In this period he made regular television appearances on the current affairs programmes "In The News" (BBC) and subsequently "Free Speech" (ITV). "There was certainly nothing wrong with his television technique in those days", reflected Anthony Howard shortly after Foot's death.

Foot returned to parliament at a by-election in Ebbw Vale, Monmouthshire in 1960, the seat having been left vacant by Bevan's death. He had the Labour whip withdrawn in March 1961 after rebelling against the Labour leadership over air force estimates. He only returned to the Parliamentary Labour Group in 1963, when Harold Wilson became Labour leader after the sudden death of Hugh Gaitskell.

Harold Wilson—the subject of an enthusiastic campaign biography by Foot published by Robert Maxwell's Pergamon Press in 1964—offered Foot a place in his first government, but Foot turned it down, instead becoming the leader of Labour's left opposition from the back benches. He opposed the government's moves to restrict immigration, join the European Communities (or "Common Market" as they were referred to) and reform the trade unions, was against the Vietnam War and Rhodesia's unilateral declaration of independence, and denounced the Soviet suppression of "socialism with a human face" in Czechoslovakia in 1968. He also famously allied with the Tory right-winger Enoch Powell to scupper the government's plan to abolish the voting rights of hereditary peers and create a House of Lords comprising only life peers—a "seraglio of eunuchs" as Foot put it.

Foot challenged James Callaghan for the post of Treasurer of the Labour Party in 1967, but failed.

After 1970, Labour moved to the left and Wilson came to an accommodation with Foot. Foot served in the Second Shadow Cabinet of Harold Wilson in various roles between 1970 and 1974. In April 1972, he stood for the Deputy Leadership of the party, along with Edward Short and Anthony Crosland. Short defeated Foot in the second ballot after Crosland had been eliminated in the first.

When, in 1974, Labour returned to office under Wilson, Foot became Secretary of State for Employment. According to Ben Pimlott, his appointment was intended to please the left of the party and the Trade Unions. In this role, he played the major part in the government's efforts to maintain the trade unions' support. He was also responsible for the Health and Safety at Work Act. Foot was one of the mainstays of the "no" campaign in the 1975 referendum on British membership of the European Communities. When Wilson retired in 1976, Foot contested the party leadership and led in the first ballot, but was ultimately defeated by James Callaghan. Later that year Foot was elected Deputy Leader, and served as Leader of the House of Commons, which gave him the unenviable task of trying to maintain the survival of the Callaghan government as its majority evaporated.

In 1975, Foot, along with Jennie Lee and others, courted controversy when they supported Indira Gandhi, the Prime Minister of India, after she prompted the declaration of a state of emergency. In December 1975, "The Times" ran an editorial titled 'Is Mr Foot a Fascist?'—their answer was that he was—after Norman Tebbit accused him of 'undiluted fascism' when Foot said that the Ferrybridge Six deserved dismissal for defying a closed shop.

Following Labour's 1979 general election defeat by Margaret Thatcher, James Callaghan remained as party leader for the next 18 months before he resigned. Foot was elected Labour leader on 10 November 1980, beating Denis Healey in the second round of the leadership election (the last leadership contest to involve only Labour MPs). Foot presented himself as a compromise candidate, capable—unlike Healey—of uniting the party, which at the time was riven by the grassroots left-wing insurgency centred around Tony Benn.

The Bennites were demanding revenge for what they considered to be the betrayals of the Callaghan government. They called for MPs who had acquiesced in Callaghan's policies to be replaced by left-wingers who would support unilateral nuclear disarmament, withdrawal from the European Communities, and widespread nationalisation. (Benn did not stand for the leadership: apart from Foot and Healey, the other candidates—both eliminated in the first round—were John Silkin, a Tribunite like Foot, and Peter Shore, a Eurosceptic.)

When he became leader, Foot was already 67 years old; and frail. After the 1979 energy crisis, Britain went into recession in 1980, which was blamed on the Conservative government's controversial monetarist policy against inflation, which had the effect of increasing unemployment. As a result, Labour had moved ahead of the Conservatives in the opinion polls. After Foot's election as leader, opinion polls showed a double-digit lead for Labour, boosting his hopes of becoming Prime Minister at the next general election, which had to be held by May 1984.

When Foot became leader, the Conservative politician Kenneth Baker commented: "Labour was led by Dixon of Dock Green under Jim Callaghan. Now it is led by Worzel Gummidge." Foot's nickname in the press gradually became "Worzel Gummidge", or "Worzel". This became particularly common after Remembrance Day 1981. After his tenure as leader, Foot would be "depicted as a scarecrow on ITV’s satirical puppet show "Spitting Image"."

Almost immediately after his election as leader, he was faced with a serious crisis. On 25 January 1981, four senior politicians on the right-wing of the Labour Party (Roy Jenkins, Shirley Williams, David Owen and William Rodgers, the so-called "Gang of Four") left Labour and formed the SDP, which was launched on 26 March 1981. This was largely seen as the consequence of the Labour Party's swing to the left, polarising divisions in an already divided party.

The SDP won the support of large sections of the media. For most of 1981 and early-1982 its opinion poll ratings suggested that it could at least overtake Labour and possibly win a general election. The Conservatives were then unpopular because of the economic policies of Margaret Thatcher, which had seen unemployment reach a postwar high.

The Labour left was still strong. In 1981, Benn decided to challenge Healey for the Deputy Leadership of the Labour Party, a contest Healey won, albeit narrowly. Foot struggled to make an impact, and was widely criticised for his ineffectiveness, though his performances in the Commons—most notably on the Falklands War of 1982—won him widespread respect from other parliamentarians. He was criticised by some on the left for supporting Thatcher's immediate resort to military action. The right-wing newspapers nevertheless lambasted him consistently for what they saw as his bohemian eccentricity, attacking him for wearing what they described as a "donkey jacket" (actually he wore a type of duffel coat) at the wreath-laying ceremony at the Cenotaph on Remembrance Day in November 1981, for which he was likened to an "out-of-work navvy" by a fellow Labour MP. Foot did not make it generally known that the Queen Mother had described it as a "sensible coat for a day like this", which could be considered a slight or a compliment depending on whether irony was intended. He later donated the coat to the People's History Museum in Manchester, which holds a collection that spans Foot's entire political career from 1938 to 1990, and his personal papers dating back to 1926.

The formation of the SDP—which formed an alliance with the Liberal Party in June 1981—contributed to a fall in Labour support. The double-digit lead which had still been intact in opinion polls at the start of 1981 was swiftly wiped out, and by the end of October the opinion polls were showing the Alliance ahead of Labour. Labour briefly regained their lead of most opinion polls in early 1982, but when the Falklands conflict ended on 14 June 1982 with a British victory over Argentina, opinion polls showed the Conservatives firmly in the lead. Their position at the top of the polls was strengthened by the return to economic growth later in the year. It was looking certain that the Conservatives would be re-elected, and the only key issue that the media were still speculating by the end of 1982 was whether it would be Labour or the Alliance who formed the next opposition.

Through late 1982 and early 1983, there was constant speculation that Labour MPs would replace Foot with Healey as leader. Such speculation increased after Labour lost the 1983 Bermondsey by-election, in which Peter Tatchell was Labour candidate, standing against a Conservative, a Liberal (eventual winner Simon Hughes) and John O'Grady, who had declared himself the Real Bermondsey Labour candidate. Critically, Labour held on in a subsequent by-election in Darlington, and Foot remained leader for the 1983 general election.

The 1983 Labour manifesto, strongly socialist in tone, advocated unilateral nuclear disarmament, higher personal taxation and a return to a more interventionist industrial policy. The manifesto also pledged that a Labour government would abolish the House of Lords, nationalise banks and immediately withdraw from the then-European Economic Community. Gerald Kaufman, once Harold Wilson's press officer and during the 1980s; a prominent figure on the Labour right-wing, described the 1983 Labour manifesto as "the longest suicide note in history."

As a statement on internal democracy, Foot passed the edict that the manifesto would consist of all resolutions arrived at conference. The party also failed to master the medium of television, while Foot addressed public meetings around the country, and made some radio broadcasts, in the same manner as Clement Attlee did in 1945. Members joked that they had not expected Foot to allow the slogan "Think positive, Act positive, Vote Labour" on grammatical grounds.

Foot's involvement in the nuclear disarmament movement gave rise to the beloved story that "The Times" ran the headline "Foot Heads Arms Body" over an article about his leadership of a nuclear-disarmament committee. Some decades later, Martyn Cornell recalled the story as true, saying he had written the headline himself as a "Times" subeditor around 1986. The headline does not, however, appear in The Times Digital Archive, which includes every day's newspaper for the years 1785–2010.

The "Daily Mirror" was the only major newspaper to back Foot and the Labour Party at the 1983 general election, urging its readers to vote Labour and "Stop the waste of our nation, for your job your children and your future" in response to the mass unemployment which followed Conservative Prime Minister Margaret Thatcher's monetarist economic policies to reduce inflation. Most other newspapers had urged their readers to vote Conservative.

The Labour Party led by Foot, lost to the Conservatives in a landslide – a result which had been widely predicted by the opinion polls since the previous summer. The only consolation for Foot and Labour was that they did not lose their place in opposition to the SDP-Liberal Alliance, who came close to them in terms of votes but were still a long way behind in terms of seats. Despite this, Foot was very critical of the Alliance, accusing them of "siphoning" Labour support and enabling the Tories to win more seats.

Foot resigned days after the bitter election defeat, and was succeeded as leader on 2 October by Neil Kinnock; who had been tipped from the outset to be Labour's choice of new leader.

Foot took a back seat in Labour politics after 1983 and retired from the House of Commons at the 1992 general election, when Labour lost to the Tories (led by John Major) for the fourth election in succession, but remained politically active. From 1987 to 1992, he was the oldest sitting British MP (preceding former Prime Minister Sir Edward Heath). He defended Salman Rushdie, after Ayatollah Khomeini advocated killing the novelist in a fatwā, and took a strongly pro-interventionist position against Serbia during its conflict with Croatia and Bosnia, supporting NATO forces whilst citing defence of civilian populations in the latter countries. In addition he was among the Patrons of the British-Croatian Society. "The Guardian"'s political editor Michael White criticised Foot's "overgenerous" support for Croatian leader Franjo Tuđman.

Foot remained a high-profile member of the Campaign for Nuclear Disarmament. He wrote several books, including highly regarded biographies of Aneurin Bevan and H. G. Wells. Indeed, he was a distinguished Vice-president of the H. G. Wells Society. Many of his friends have said publicly that they regret that he ever gave up literature for politics.

Michael Foot became a supporter of pro-Europeanism in the 1990s.

Foot was an Honorary Associate of the National Secular Society and a Distinguished Supporter of the British Humanist Association. In 1988, he was elected a Fellow of the Royal Society of Literature. 

In a poll of Labour party activists he was voted the worst post-war Labour party leader. Though Foot is considered by many a failure as Labour leader, his biographer Mervyn Jones strongly makes the case that no one else could have held Labour together at the time, particularly in the face of the controversy over the infiltration of the party by Militant. Foot is remembered with affection in Westminster as a great parliamentarian. He was widely liked, and admired for his integrity, habitual courtesy, and generosity of spirit, by both his colleagues and opponents.

A portrait of Foot by the artist Robert Lenkiewicz now permanently hangs in Portcullis House, Westminster.

Oleg Gordievsky, a high-ranking KGB officer who defected from the Soviet Union to Britain in 1985, made allegations against Foot in his 1995 memoirs. "The Sunday Times", which serialised Gordievsky's book under the headline "KGB: Michael Foot was our agent", claimed in an article of 19 February that the Soviet intelligence services regarded Foot as an "agent of influence", codenamed "Agent BOOT", and in the pay of the KGB for many years. Crucially, the newspaper used material from the original manuscript of the book which had not been included in the published version. At the time a leading article in "The Independent" newspaper asserted: "It seems extraordinary that such an unreliable figure should now be allowed, given the lack of supporting evidence, to damage the reputation of figures such as Mr Foot." In a February 1992 interview, Gordievsky had claimed that he had no further Labour Party revelations to make. Foot successfully sued the "Sunday Times", winning "substantial" damages.

However, in the "Daily Telegraph" in 2010, Charles Moore gave a "full account", which he said had been provided to him by Gordievsky shortly after Foot's death, of the extent of Foot's alleged KGB involvement. The account provides additional information concerning the allegations, but no new evidence. The evidence against Foot consists solely of Gordievsky's testimony. Moore wrote that, although the claims are difficult to corroborate without MI6 and KGB files, Gordievsky's past record in revealing KGB contacts in Britain had been shown to be reliable. However Moore did not think that Foot would have known that he was considered an agent, and he probably considered that he was simply keeping the Soviet Union well informed in the interests of peace. There is no evidence Foot gave away secrets.

Foot was a passionate supporter of Plymouth Argyle Football Club from his childhood and once remarked that he wasn't going to die until he had seen them play in the Premier League.
He served for several years as a director of the club, seeing two promotions under his tenure.

For his 90th birthday, Foot was registered with the Football League as an honorary player and given the shirt number 90. This made him the oldest registered professional player in the history of football.

Foot was married to the film-maker, author and feminist historian Jill Craigie (1911–99) from 1949 until her death fifty years later. He had no children.

He was a great music lover 

In February 2007, it was revealed that Foot had an extramarital affair with a woman around 35 years his junior in the early-1970s. The affair, which lasted nearly a year, put a considerable strain on his marriage. The affair is detailed in Foot's official biography, published in March 2007.

On 23 July 2006, his 93rd birthday, Michael Foot became the longest-lived leader of a major British political party, passing Lord Callaghan's record of 92 years, 364 days.

A staunch republican (though well liked by the Royal Family on a personal level), Foot rejected honours from the Queen and the government, including a knighthood and a peerage, on more than one occasion.

He was also an atheist. He was one of three leaders of the Labour Party to positively declare that they disbelieved.

Foot suffered from asthma until 1963 (which disqualified him from service in the Second World War) and eczema until middle age.

In October 1963, he was involved in a car crash, suffering pierced lungs, broken ribs, and a broken left leg. Foot used a walking stick for the rest of his life. According to former MP Tam Dalyell, Foot had up until the accident, been a chain-smoker; but gave up the habit thereafter.

In 1976, Foot became blind in one eye following an attack of shingles.

Foot died at his Hampstead, north London home in the morning of 3 March 2010 at the age of 96. The House of Commons was informed of the news later that day by Justice Secretary Jack Straw, who told the House: "I am sure that this news will be received with great sadness not only in my own party but across the country as a whole." Foot's funeral was a non-religious service, held on 15 March 2010 at Golders Green Crematorium in North-West London.

A memorial to Foot in Plymouth was vandalised with Nazi symbols in the wake of the 2016 United Kingdom European Union membership referendum in July 2016.

Foot was portrayed by Patrick Godfrey in the 2002 BBC production of Ian Curteis's long unproduced "The Falklands Play" and by Michael Pennington in the film "The Iron Lady".
Foot was likened to the scarecrow Worzel Gummidge in "Dear Bill", a long-running series of fictional letters which appeared in the British satirical magazine "Private Eye", purportedly written by Denis Thatcher, husband of Margaret Thatcher, to his friend and golfing partner Bill Deedes, former editor of the "Daily Telegraph".





</doc>
<doc id="19828" url="https://en.wikipedia.org/wiki?curid=19828" title="Max and Moritz">
Max and Moritz

Max and Moritz (A Story of Seven Boyish Pranks) (original: Max und Moritz – Eine Bubengeschichte in sieben Streichen) is a German language illustrated story in verse. This highly inventive, blackly humorous tale, told entirely in rhymed couplets, was written and illustrated by Wilhelm Busch and published in 1865. It is among the early works of Busch, nevertheless it already features many substantial, effectually aesthetic and formal regularities, procedures and basic patterns of Busch's later works. Many familiar with comic strip history consider it to have been the direct inspiration for the "Katzenjammer Kids" and "Quick & Flupke". The German title satirizes the German custom of giving a subtitle to the name of dramas in the form of "Ein Drama in ... Akten" ("A Drama in ... Acts"), which became dictum in colloquial usage for any event with an unpleasant or dramatic course, e.g. "Bundespräsidentenwahl - Drama in drei Akten" ("Federal Presidential Elections - Drama in Three Acts").

Busch's classic tale of the terrible duo (now in the public domain) has since become a proud part of the culture in German-speaking countries. Even today, parents usually read these tales to their not-yet-literate children. To this day in Germany, Austria, and Switzerland, a certain familiarity with the story and its rhymes is still presumed, as it is often referenced in mass communication. The two leering faces are synonymous with mischief, and appear almost logo-like in advertising and even graffiti.

During World War 1, the Red Baron, Manfred von Richthofen, named his dog Moritz, giving the name Max to another animal given to his friend.

"Max and Moritz" is the first published original foreign children’s book in Japan which was translated into rōmaji by Shinjirō Shibutani and Kaname Oyaizu in 1887 as "" ("Naughty stories").

Max and Moritz became the forerunners to the comic strip. The story inspired Rudolph Dirks to create The Katzenjammer Kids.

After World War 2, German-U.S. composer Richard Mohaupt created together with choreographer Alfredo Bortoluzzi the dance burlesque ("Tanzburleske") "Max und Moritz", which premiered at Badisches Staatstheater Karlsruhe on December 18, 1949.

There have been several English translations of the original German verses over the years, but all have maintained the original trochaic tetrameter:

Ah, how oft we read or hear of <br>
Boys we almost stand in fear of!<br>
For example, take these stories<br>
Of two youths, named Max and Moritz,<br>
Who, instead of early turning<br>
Their young minds to useful learning,<br>
Often leered with horrid features<br>
At their lessons and their teachers.

Look now at the empty head: he<br>
Is for mischief always ready.<br>
Teasing creatures - climbing fences,<br>
Stealing apples, pears, and quinces,<br>
Is, of course, a deal more pleasant,<br>
And far easier for the present,<br>
Than to sit in schools or churches,<br>
Fixed like roosters on their perches

But O dear, O dear, O deary,<br>
When the end comes sad and dreary!<br>
'Tis a dreadful thing to tell<br>
That on Max and Moritz fell!<br>
All they did this book rehearses,<br>
Both in pictures and in verses.

The boys tie several crusts of bread together with thread, and lay this trap in the chicken yard of Bolte, an old widow, causing all the chickens to become fatally entangled.

This prank is remarkably similar to the eighth history of the classic German prankster tales of Till Eulenspiegel.

As the widow cooks her chickens, the boys sneak onto her roof. When she leaves her kitchen momentarily, the boys steal the chickens using a fishing pole down the chimney. The widow hears her dog barking and hurries upstairs, finds the hearth empty and beats the dog.

The boys torment Böck, a well-liked tailor who has a fast stream flowing in front of his house. They saw through the planks of his wooden bridge, making a precarious gap, then taunt him by making goat noises (a pun on his name being similar to the zoological expression 'buck'), until he runs outside. The bridge breaks; the tailor is swept away and nearly drowns (but for two geese, which he grabs a hold of and which fly high to safety).

Although Till removes the planks of the bridge instead of sawing them there are some similarities to Till Eulenspiegel (32nd History).

While their devout teacher, Lämpel, is busy at church, the boys invade his home and fill his favorite pipe with gunpowder. When he lights the pipe, the blast knocks him unconscious, blackens his skin and burns away all his hair. But: "Time that comes will quick repair; yet the pipe retains its share."

The boys collect bags full of May bugs, which they promptly deposit in their Uncle Fritz's bed. Uncle is nearly asleep when he feels the bugs walking on his nose. Horrified, he goes into a frenzy, killing them with a shoe.

The boys invade a bakery which they believe is closed. Attempting to steal pretzels, they fall into a vat of dough. The baker returns, catches the breaded pair, and bakes them. But they survive, and escape by gnawing through their crusts.

Hiding out in the grain storage area of a farmer, Mecke, the boys slit some grain sacks. Carrying away one of the sacks, farmer Mecke immediately notices the problem. He puts the boys in the sack instead, then takes it to the mill. The boys are ground to bits and devoured by the miller’s ducks. Later, no one expresses regret.

"Max und Moritz" was adapted into a ballet by Richard Mohaupt and Alfredo Bortuluzzi. In 1956 Norbert Schultze adapted it into a straightforward children's film, "Max und Moritz" (1956), while Thomas Frydetzki and Annette Stefan made a more loose, satirical adaptation in 2005 named "Max und Moritz Reloaded". The comic was also adapted into two 1978 animated TV specials.

 in German for a single work


</doc>
<doc id="19829" url="https://en.wikipedia.org/wiki?curid=19829" title="May Day">
May Day

May Day is a public holiday usually celebrated on 1 May. It is an ancient northern hemisphere spring festival and a traditional spring holiday in many cultures. Dances, singing, and cake are usually part of the festivities. In the late 19th century, May Day was chosen as the date for International Workers' Day by the Socialists and Communists of the Second International to commemorate the Haymarket affair in Chicago. International Workers' Day can also be referred to as "May Day", but it is a different celebration from the traditional May Day.

The earliest May Day celebrations appeared with the "Floralia", festival of Flora, the Roman goddess of flowers, held on 27 April during the Roman Republic era, and with the Walpurgis Night celebrations of the Germanic countries. It is also associated with the Gaelic Beltane, most commonly held on 30 April. The day is a traditional summer holiday in many European pagan cultures. While 1 February is the first day of spring, 1 May is the first day of summer; hence, the summer solstice on 25 June (now 21 June) is Midsummer.

As Europe became Christianised, the pagan holidays lost their religious character and May Day changed into a popular secular celebration. A significant celebration of May Day occurs in Germany where it is one of several days on which St. Walburga, credited with bringing Christianity to Germany, is celebrated. The secular versions of May Day, observed in Europe and North America, may be best known for their traditions of dancing around the maypole and crowning the Queen of May. Fading in popularity since the late 20th century is the giving of "May baskets," small baskets of sweets or flowers, usually left anonymously on neighbours' doorsteps.

Since the 18th century, many Roman Catholics have observed May – and May Day – with various May devotions to the Blessed Virgin Mary In works of art, school skits, and so forth, Mary's head will often be adorned with flowers in a May crowning. 1 May is also one of two feast days of the Catholic patron saint of workers St Joseph the Worker, a carpenter, husband to Mother Mary, and surrogate father of Jesus. Replacing another feast to St. Joseph, this date was chosen by Pope Pius XII in 1955 as a counterpoint to the communist International Workers Day celebrations on May Day.

In the late 20th century, many neopagans began reconstructing traditions and celebrating May Day as a pagan religious festival.

Traditional English May Day rites and celebrations include crowning a May Queen and celebrations involving a maypole. Historically, Morris dancing has been linked to May Day celebrations. Much of this tradition derives from the pagan Anglo-Saxon customs held during "Þrimilci-mōnaþ" (the Old English name for the month of May meaning "Month of Three Milkings") along with many Celtic traditions.

May Day has been a traditional day of festivities throughout the centuries, most associated with towns and villages celebrating springtime fertility (of the soil, livestock, and people) and revelry with village fetes and community gatherings. Seeding has been completed by this date and it was convenient to give farm labourers a day off. Perhaps the most significant of the traditions is the maypole, around which traditional dancers circle with ribbons.

The spring bank holiday on the first Monday in May was created in 1978; May Day itself May 1 is not a public holiday in England (unless it falls on a Monday). In February 2011, the UK Parliament was reported to be considering scrapping the bank holiday associated with May Day, replacing it with a bank holiday in October, possibly coinciding with Trafalgar Day (celebrated on October 21), to create a "United Kingdom Day".

Unlike the other Bank Holidays and common law holidays, the first Monday in May is taken off from (state) schools by itself, and not as part of a half term or end of term holiday. This is because it has no Christian significance and does not otherwise fit into the usual school holiday pattern. (By contrast, the Easter Holiday can start as late - relative to Easter - as Good Friday, if Easter falls early in the year; or finish as early - relative to Easter - as Easter Monday, if Easter falls late in the year, because of the supreme significance of Good Friday and Easter Day to Christianity.)

May Day was abolished and its celebration banned by Puritan parliaments during the Interregnum, but reinstated with the restoration of Charles II in 1660. 1 May 1707, was the day the Act of Union came into effect, joining England and Scotland to form the Kingdom of Great Britain.
In Oxford, it is a centuries-old tradition for May Morning revellers to gather below the Great Tower of Magdalen College at 6am to listen to the college choir sing traditional madrigals as a conclusion to the previous night's celebrations. Since the 1980s some people then jump off Magdalen Bridge into the River Cherwell. For some years, the bridge has been closed on 1 May to prevent people from jumping, as the water under the bridge is only deep and jumping from the bridge has resulted in serious injury in the past. There are still people who climb the barriers and leap into the water, causing themselves injury.

In Durham, students of the University of Durham gather on Prebend's Bridge to see the sunrise and enjoy festivities, folk music, dancing, madrigal singing and a barbecue breakfast. This is an emerging Durham tradition, with patchy observance since 2001.

Kingsbury Episcopi, Somerset, has seen its yearly May Day Festival celebrations on the May bank holiday Monday burgeon in popularity in the recent years. Since it was reinstated 21 years ago it has grown in size, and on 5 May, 2014 thousands of revellers were attracted from all over the south west to enjoy the festivities, with BBC Somerset covering the celebrations. These include traditional maypole dancing and morris dancing, as well as contemporary music acts..

Whitstable, Kent, hosts a good example of more traditional May Day festivities, where the Jack in the Green festival was revived in 1976 and continues to lead an annual procession of morris dancers through the town on the May bank holiday. A separate revival occurred in Hastings in 1983 and has become a major event in the town calendar. A traditional sweeps festival is performed over the May bank holiday in Rochester, Kent, where the Jack in the Green is woken at dawn on 1 May by Morris dancers.

At 7:15 p.m. on 1 May each year, the Kettle Bridge Clogs morris dancing side dance across Barming Bridge (otherwise known as the Kettle Bridge), which spans the River Medway near Maidstone, to mark the official start of their morris dancing season.

Also known as Ashtoria Day in northern parts of rural Cumbria. A celebration of unity and female bonding. Although not very well known, it is often cause for huge celebration.

The Maydayrun involves thousands of motorbikes taking a trip from London (Locksbottom) to the Hastings seafront, East Sussex. The event has been taking place for almost 30 years now and has grown in interest from around the country, both commercially and publicly. The event is not officially organised; the police only manage the traffic, and volunteers manage the parking.

Padstow in Cornwall holds its annual Obby-Oss (Hobby Horse) day of festivities. This is believed to be one of the oldest fertility rites in the UK; revellers dance with the Oss through the streets of the town and even through the private gardens of the citizens, accompanied by accordion players and followers dressed in white with red or blue sashes who sing the traditional "May Day" song. The whole town is decorated with springtime greenery, and every year thousands of onlookers attend. Prior to the 19th-century, distinctive May Day celebrations were widespread throughout west Cornwall, and are being revived in St. Ives and Penzance.

Kingsand, Cawsand and Millbrook in Cornwall celebrate Flower Boat Ritual on the May Day bank holiday. A model of the ship The Black Prince is covered in flowers and is taken in procession from the Quay at Millbrook to the beach at Cawsand where it is cast adrift. The houses in the villages are decorated with flowers and people traditionally wear red and white clothes. There are further celebrations in Cawsand Square with Morris dancing and May pole dancing.

May Day has been celebrated in Scotland for centuries. It was previously closely associated with the Beltane festival. Reference to this earlier celebration is found in poem 'Peblis to the Play', contained in the Maitland Manuscripts of fifteenth- and sixteenth-century Scots poetry:

<poem style="margin-left: 2em;">At Beltane, quhen ilk bodie bownis
To Peblis to the Play,
To heir the singin and the soundis;
The solace, suth to say,
Be firth and forrest furth they found
Thay graythis tham full gay;
God wait that wald they do that stound,
For it was their feist day,
Thay said, [...]</poem>

The poem describes the celebration in the town of Peebles in the Scottish Borders, which continues to stage a parade and pageant each year, including the annual ‘Common Riding’, which takes place in many towns throughout the Borders. As well as the crowning of a Beltane Queen each year, it is custom to sing ‘The Beltane Song’.

John Jamieson, in his "Etymological Dictionary of the Scottish Language" (1808) describes some of the May Day/Beltane customs which persisted in the eighteenth and early nineteenth centuries in parts of Scotland, which he noted were beginning to die out. In the nineteenth century, folklorist Alexander Carmichael (1832–1912), collected the song "Am Beannachadh Bealltain" ("The Beltane Blessing") in his "Carmina Gadelica", which he heard from a crofter in South Uist.

Scottish May Day/Beltane celebrations have been somewhat revived since the late twentieth century. Both Edinburgh and Glasgow organise May Day festivals and rallies. In Edinburgh, the Beltane Fire Festival is held on the evening of May eve and into the early hours of May Day on the city's Calton Hill. An older Edinburgh tradition has it that young women who climb Arthur's Seat and wash their faces in the morning dew will have lifelong beauty. At the University of St Andrews, some of the students gather on the beach late on April 30 and run into the North Sea at sunrise on May Day, occasionally naked. This is accompanied by torchlit processions and much elated celebration.

In Wales the first day of May is known as "Calan Mai" or "Calan Haf", and parallels the festival of Beltane and other May Day traditions in Europe. 

Traditions would start the night before ("Nos Galan Haf") with bonfires, and is considered a "Ysbrydnos" or "spirit night" when people would gather hawthorn ("draenen wen") and flowers to decorate their houses, celebrating new growth and fertility. While on May Day celebrations would include summer dancing ("dawnsio haf") and May carols ("carolau mai" or "carolau haf") othertimes referred to as "singing under the wall" ("canu dan y pared)," May Day was also a time for officially opening a village green (twmpath chwarae).

In Finland, Walpurgis night (') ("") is one of the four biggest holidays along with Christmas Eve, New Year's Eve, and Midsummer ('). Walpurgis witnesses the biggest carnival-style festival held in Finland's cities and towns. The celebrations, which begin on the evening of 30 April and continue on 1 May, typically centre on the consumption of sima, sparkling wine and other alcoholic beverages. Student traditions, particularly those of engineering students, are one of the main characteristics of '. Since the end of the 19th century, this traditional upper-class feast has been appropriated by university students. Many ' (university-preparatory high school) alumni wear the black and white student cap and many higher education students wear student coveralls. One tradition is to drink sima, a home-made low-alcohol mead, along with freshly cooked funnel cakes.

May Day or "Spring Day" ("Kevadpüha") is a national holiday in Estonia celebrating the arrival of spring.

More traditional festivities take place throughout the night before and into the early hours of 1 May, on the Walpurgis Night ("Volbriöö").

On 1 May 1561, King Charles IX of France received a lily of the valley as a lucky charm. He decided to offer a lily of the valley each year to the ladies of the court. At the beginning of the 20th century, it became custom to give a sprig of lily of the valley, a symbol of springtime, on 1 May. The government permits individuals and workers' organisations to sell them tax-free on that single day. Nowadays, people may present loved ones either with bunches of lily of the valley or dog rose flowers.

In rural regions of Germany, especially the Harz Mountains, Walpurgisnacht celebrations of pagan origin are traditionally held on the night before May Day, including bonfires and the wrapping of a "Maibaum" (maypole). Young people use this opportunity to party, while the day itself is used by many families to get some fresh air. Motto: "Tanz in den Mai" (""Dance into May"").

In the Rhineland, 1 May is also celebrated by the delivery of a maypole, a tree covered in streamers to the house of a girl the night before. The tree is typically from a love interest, though a tree wrapped only in white streamers is a sign of dislike. Women usually place roses or rice in the form of a heart at the house of their beloved one. It is common to stick the heart to a window or place it in front of the doormat. In leap years, it is the responsibility of the women to place the maypole. All the action is usually done secretly and it is an individual's choice whether to give a hint of their identity or stay anonymous.

May Day was not established as a public holiday until 1933. As Labour Day, many political parties and unions host activities related to work and employment.

May Day has been celebrated in Ireland since pagan times as the feast of Beltane (Bealtaine) and in latter times as Mary's day. Traditionally, bonfires were lit to mark the coming of summer and to banish the long nights of winter. Officially Irish May Day holiday is the first Monday in May. Old traditions such as bonfires are no longer widely observed, though the practice still persists in some places across the country. Limerick, Clare and many other people in other counties still keep on this tradition, including areas in Dublin city such as Ringsend.

In Italy it is called "Calendimaggio" or "cantar maggio" a seasonal feast held to celebrate the arrival of spring. The event takes its name from the period in which it takes place, that is, the beginning of May, from the Latin "calenda maia". The Calendimaggio is a tradition still alive today in many regions of Italy as an allegory of the return to life and rebirth: among these Piedmont, Liguria, Lombardy, Emilia-Romagna (for example, is celebrated in the area of the "Quattro Province" or Piacenza, Pavia, Alessandria and Genoa), Tuscany and Umbria. This magical-propitiatory ritual is often performed during an almsgiving in which, in exchange for gifts (traditionally eggs, wine, food or sweets), the Maggi (or maggerini) sing auspicious verses to the inhabitants of the houses they visit. Throughout the Italian peninsula these "Il Maggio" couplets are very diverse—most are love songs with a strong romantic theme, that young people sang to celebrate the arrival of spring. Symbols of spring revival are the trees (alder, golden rain) and flowers (violets, roses), mentioned in the verses of the songs, and with which the maggerini adorn themselves. In particular the plant alder, which grows along the rivers, is considered the symbol of life and that's why it is often present in the ritual.

Calendimaggio can be historically noted in Tuscany as a mythical character who had a predominant role and met many of the attributes of the god Belenus. In Lucania, the "Maggi" have a clear auspicious character of pagan origin. In Syracuse, Sicily, the "Albero della Cuccagna" (cf. "Greasy pole") is held during the month of May, a feast celebrated to commemorate the victory over the Athenians led by Nicias. However, Angelo de Gubernatis, in his work "Mythology of Plants", believes that without doubt the festival was previous to that of said victory.

It is a celebration that dates back to ancient peoples, and is very integrated with the rhythms of nature, such as the Celts (celebrating Beltane), Etruscans and Ligures, in which the arrival of summer was of great importance.

1 May is a day that celebrates Spring.

Maios (Latin Maius), the month of May, took its name from the goddess Maia (Gr ), a Greek and Roman goddess of fertility. The day of Maios (Modern Greek Πρωτομαγιά) celebrates the final victory of the summer against winter as the victory of life against death. The celebration is similar to an ancient ritual associated with another minor demi-god Adonis which also celebrated the revival of nature. There is today some conflation with yet another tradition, the revival or marriage of Dionysus (the Greek God of theatre and wine-making). This event, however, was celebrated in ancient times not in May but in association with the Anthesteria, a festival held in February and dedicated to the goddess of agriculture Demeter and her daughter Persephone. Persephone emerged every year at the end of Winter from the Underworld. The Anthesteria was a festival of souls, plants and flowers, and Persephone's coming to earth from Hades marked the rebirth of nature, a common theme in all these traditions.

What remains of the customs today, echoes these traditions of antiquity. A common, until recently, May Day custom involved the annual revival of a youth called Adonis, or alternatively of Dionysus, or of Maios (in Modern Greek Μαγιόπουλο, the Son of Maia). In a simple theatrical ritual, the significance of which has long been forgotten, a chorus of young girls sang a song over a youth lying on the ground, representing Adonis, Dionysus or Maios. At the end of the song, the youth rose up and a flower wreath was placed on his head.

The most common aspect of modern May Day celebrations is the preparation of a flower wreath from wild flowers, although as a result of urbanisation there is an increasing trend to buy wreaths from flower shops. The flowers are placed on the wreath against a background of green leaves and the wreath is hung either on the entrance to the family house/apartment or on a balcony. It remains there until midsummer night. On that night, the flower wreaths are set alight in bonfires known as St John’s fires. Youths leap over the flames consuming the flower wreaths. This custom has also practically disappeared, like the theatrical revival of Adonis/Dionysus/Maios, as a result of rising urban traffic and with no alternative public grounds in most Greek city neighbourhoods, not to mention potential conflicts with demonstrating workers.

On May Day, Bulgarians celebrate Irminden (or Yeremiya, Eremiya, Irima, Zamski den). The holiday is associated with snakes and lizards and rituals are made in order to protect people from them. The name of the holiday comes from the prophet Jeremiah, but its origins are most probably pagan.

It is said that on the days of the Holy Forty or Annunciation snakes come out of their burrows, and on Irminden their king comes out. Old people believe that those working in the fields on this day will be bitten by a snake in summer.

In western Bulgaria people light fires, jump over them and make noises to scare snakes. Another custom is to prepare "podnici" (special clay pots made for baking bread).

This day is especially observed by pregnant women so that their offspring do not catch "yeremiya" — an illness due to evil powers.

On May Day, the Romanians celebrate the "arminden" (or "armindeni"), the beginning of summer, symbolically tied with the protection of crops and farm animals. The name comes from Slavonic "Jeremiinŭ dĭnĭ", meaning prophet Jeremiah's day, but the celebration rites and habits of this day are apotropaic and pagan (possibly originating in the cult of the god Pan).

The day is also called "ziua pelinului" ("mugwort day") or "ziua bețivilor" ("drunkards' day") and it is celebrated to ensure good wine in autumn and, for people and farm animals alike, good health and protection from the elements of nature (storms, hail, illness, pests). People would have parties in natural surroundings, with "lăutari" (fiddlers) for those who could afford it. Then it is customary to roast and eat lamb, along with new mutton cheese, and to drink mugwort-flavoured wine, or just red wine, to refresh the blood and get protection from diseases. On the way back, the men wear lilac or mugwort flowers on their hats.

Other apotropaic rites include, in some areas of the country, people washing their faces with the morning dew (for good health) and adorning the gates for good luck and abundance with green branches or with birch saplings (for the houses with maiden girls). The entries to the animals' shelters are also adorned with green branches. All branches are left in place until the wheat harvest when they are used in the fire which will bake the first bread from the new wheat.

On May Day eve, country women do not work in the field as well as in the house to avoid devastating storms and hail coming down on the village.

"Arminden" is also "ziua boilor" (oxen day) and thus the animals are not to be used for work, or else they could die or their owners could get ill.

It is said that the weather is always good on May Day to allow people to celebrate.

"Maias" is a superstition throughout Portugal, with special focus on the northern territories and rarely elsewhere. Maias is the dominant naming in Northern Portugal, but it may be referred to by other names, including Witches' day ("Dia das Bruxas"), the Donkey ("O Burro", referring to a evil spirit) or the last of April, as the local traditions preserved to this day occur on that evening only. People put the yellow flowers of Portuguese brooms, the bushes are known as "giestas". The flowers of the bush are known as Maias, which are placed on doors or gates and every doorway of houses, windows, granaries, currently also cars, which the populace collect on the evening of the 30th of April when the Portuguese brooms are blooming, to defend those places from bad spirits, witches and the evil eye. The placement of the May flower or bush in the doorway must be done before midnight.

These festivities are a continuum of the "Os Maios" of Galiza. In ancient times, this was done while playing traditional night-music. In some places, children were dressed in these flowers and went from place to place begging for money or bread. On the 1st of May, people also used to sing "Cantigas de Maio", traditional songs related to this day and the whole month of May.

May Day is celebrated throughout the country as "Los Mayos" (lit. "the Mays") often in a similar way to "Fiesta de las Cruces" in many parts of Hispanic America. By way of example, in Galicia, the festival ("os maios", in the local language) consists in different representations around a decorated tree or sculpture. People sing popular songs (also called "maios",) making mentions to social and political events during the past year, sometimes under the form of a converse, while they walk around the sculpture with the percussion of two sticks. In Lugo and in the village of Vilagarcía de Arousa it was usual to ask a tip to the attendees, which used to be a handful of dry chestnuts ("castañas maiolas"), walnuts or hazelnuts. Today the tradition became a competition where the best sculptures and songs receive a prize.

In the Galician city of Ourense this day is celebrated traditionally on 3 May, the day of the Holy Cross, that in the Christian tradition replaced the tree "where the health, life and resurrection are," according to the introit of that day's mass.

The more traditional festivities have moved to the day before, Walpurgis Night ("Valborgsmässoafton"), known in some locales as simply "Last of April". The first of May is instead celebrated as International Workers' Day.

In Poland, there is a state holiday on 1 May. It is currently celebrated without a specific connotation, and as such it is May Day. However, due to historical connotations, most of the celebrations are focused around Labour Day festivities. It is customary for labour activists and left-wing political parties to organize parades in cities and towns across Poland on this day. The holiday is also commonly referred to as "Labour Day" ("Święto Pracy").

In Poland, May Day is closely followed by May 3rd Constitution Day. These two dates combined often result in a long weekend called "Majówka". People often travel, and "Majówka" is unofficially considered the start of the barbecuing season in Poland. Between these two, on 2 May, though formerly a working day, there is now a patriotic holiday, the Day of the Polish Flag (), introduced by a Parliamentary Act of February 20, 2004. May Day has a public holiday too.

In Czech Republic, May Day is traditionally considered as a holiday of love and May as a month of love. The celebrations of spring are held on April 30th when a maypole ("májka" in Czech) is lifted--a tradition possibly connected to Beltane, since bonfires are also lit on that day. The event is similar to German Walpurgisnacht. It's public holiday on April 30th.
On May 31st, the maypole is taken down in an event called Maypole Felling.

On 1 May, couples in love are kissing under a blooming tree. According to the ethnographer Klára Posekaná, this is not an old habit, it could originate around the beginning of the 20th century and probably in the urban environment, perhaps in connection with Karel Hynek Mácha and Petřín. A cherry, an apple or a birch is most often considered a suitable tree.

May Day is celebrated in some parts of the provinces of British Columbia, New Brunswick and Ontario.

Toronto

In Toronto, on the morning of 1 May, various Morris Dancing troops from Toronto and Hamilton gather on the road by Grenadier Cafe, in High Park to "dance in the May". The dancers and crowd then gather together and sing traditional May Day songs such as Hal-An-Tow and Padstow.

British Columbia

Celebrations often take place not on 1 May but during the Victoria Day long weekend, later in the month and when the weather is likely to be better. The longest continually observed May Day in the British Commonwealth is held in the city of New Westminster, BC. There, the first May Day celebration was held on 4 May 1870.

May Day was also celebrated by some early European settlers of the American continent. In some parts of the United States, May baskets are made. These are small baskets usually filled with flowers or treats and left at someone's doorstep. The giver rings the bell and runs away.

Modern May Day ceremonies in the U.S. vary greatly from region to region and many unite both the holiday's "Green Root" (pagan) and "Red Root" (labour) traditions.

May Day celebrations were common at women's colleges and academic institutions in the late nineteenth and early twentieth century, a tradition that continues at Bryn Mawr College and Brenau University to this day.

In Minneapolis, the May Day Parade and Festival is presented annually by In the Heart of the Beast Puppet and Mask Theatre on the first Sunday in May, and draws around 50,000 people to Powderhorn Park.

In Hawaii, May Day is also known as Lei Day, and it is normally set aside as a day to celebrate island culture in general and the culture of the Native Hawaiians in particular. Invented by poet and local newspaper columnist Don Blanding, the first Lei Day was celebrated on 1 May 1927 in Honolulu. Leonard "Red" and Ruth Hawk composed "May Day Is Lei Day in Hawai'i," the traditional holiday song.




</doc>
<doc id="19830" url="https://en.wikipedia.org/wiki?curid=19830" title="Maxwell–Boltzmann distribution">
Maxwell–Boltzmann distribution

In physics (in particular in statistical mechanics), the Maxwell–Boltzmann distribution is a particular probability distribution named after James Clerk Maxwell and Ludwig Boltzmann. 

It was first defined and used for describing particle speeds in idealized gases, where the particles move freely inside a stationary container without interacting with one another, except for very brief collisions in which they exchange energy and momentum with each other or with their thermal environment. 
The term "particle" in this context refers to gaseous particles (atoms or molecules), and the system of particles is assumed to have reached thermodynamic equilibrium.
The energies of such particles follow what is known as Maxwell-Boltzmann statistics, and the statistical distribution of speeds is derived by equating particle energies with kinetic energy.

Mathematically, the Maxwell–Boltzmann distribution is the chi distribution with three degrees of freedom (the components of the velocity vector in Euclidean space), with a scale parameter measuring speeds in units proportional to the square root of formula_8 (the ratio of temperature and particle mass).

The Maxwell–Boltzmann distribution is a result of the kinetic theory of gases, which provides a simplified explanation of many fundamental gaseous properties, including pressure and diffusion. 
The Maxwell–Boltzmann distribution applies fundamentally to particle velocities in three dimensions, but turns out to depend only on the speed (the magnitude of the velocity) of the particles. 
A particle speed probability distribution indicates which speeds are more likely: a particle will have a speed selected randomly from the distribution, and is more likely to be within one range of speeds than another.
The kinetic theory of gases applies to the classical ideal gas, which is an idealization of real gases. In real gases, there are various effects (e.g., van der Waals interactions, vortical flow, relativistic speed limits, and quantum exchange interactions) that can make their speed distribution different from the Maxwell–Boltzmann form. 
However, rarefied gases at ordinary temperatures behave very nearly like an ideal gas and the Maxwell speed distribution is an excellent approximation for such gases. 
Ideal plasmas, which are ionized gases of sufficiently low density, frequently also have particle distributions that are partially or entirely Maxwellian.

The distribution was first derived by Maxwell in 1860 on heuristic grounds. 
Boltzmann later, in the 1870s, carried out significant investigations into the physical origins of this distribution.

Assuming the system of interest contains a large number of particles, the fraction of the particles within an infinitesimal element of three-dimensional velocity space, dformula_9, centered on a velocity vector of magnitude formula_10, is formula_11dformula_9, in which 

where formula_14 is the particle mass and formula_15 is the product of Boltzmann's constant and thermodynamic temperature.
One can write the element of velocity space as dformula_9 = dformula_17dformula_18dformula_19, for velocities in a standard Cartesian coordinate system, or as dformula_9 = formula_21dformula_10dformula_23 in a standard spherical coordinate system, where dformula_23 is an element of solid angle. Here formula_11 is given as a probability distribution function, properly normalized so that formula_26dformula_9 over all velocities equals one. In plasma physics, the probability distribution is often multipled by the particle density, so that the integral of the resulting distribution function equals the density. 

Undergraduate students are likely to work with the Maxwellian distribution function for particles moving in only one direction. If this direction is formula_28, then one has 
which can be obtained by integrating the three-dimensional form given above over formula_18 and formula_19.

Recognizing the symmetry of formula_11, one can integrate over solid angle and write a probability distribution of speeds as the function

This probability density function gives the probability, per unit speed, of finding the particle with a speed near formula_10. This equation is simply the Maxwell-Boltzmann distribution (given in the infobox) with distribution parameter formula_35. 
The Maxwell–Boltzmann distribution is equivalent to the chi distribution with three degrees of freedom and scale parameter formula_35.

The simplest ordinary differential equation satisfied by the distribution is:

or in unitless presentation:

With the Darwin–Fowler method of mean values the Maxwell–Boltzmann distribution is obtained as an exact result.

The mean speed formula_41, 
most probable speed (mode) , 
and root-mean-square speed formula_42 
can be obtained from properties of the Maxwell distribution.

This works well for nearly ideal, monatomic gases like helium, but also for molecular gases like diatomic oxygen. 
This is because despite the larger heat capacity (larger internal energy at the same temperature) due to their larger number of degrees of freedom, their translational kinetic energy (and thus their speed) is unchanged.

with the solution:
(the second solution formula_45 representing the "least probable speed").

For diatomic nitrogen (N, the primary component of air) 
at room temperature (), this gives 

In summary, the typical speeds are related as follows:

The root mean square speed is directly related to the speed of sound in the gas, by
where formula_62 is the adiabatic index, 
For the example above, diatomic nitrogen (approximating air) at , formula_63 and
the true value for air can be approximated by using the average molar weight of air (), yielding 

The original derivation in 1860 by James Clerk Maxwell was an argument based on molecular collisions of the Kinetic theory of gases as well as certain symmetries in the speed distribution function; Maxwell also gave an early argument that these molecular collisions entail a tendency towards equilibrium. After Maxwell, Ludwig Boltzmann in 1872 also derived the distribution on mechanical grounds and argued that gases should over time tend toward this distribution, due to collisions (see H-theorem). He later (1877) derived the distribution again under the framework of statistical thermodynamics. The derivations in this section are along the lines of Boltzmann's 1877 derivation, starting with result known as Maxwell–Boltzmann statistics (from statistical thermodynamics). Maxwell–Boltzmann statistics gives the average number of particles found in a given single-particle microstate, under certain assumptions, the logarithm of the fraction of particles in a given microstate is proportional to the ratio of the energy of that state to the temperature of the system:
The assumptions of this equation are that the particles do not interact, and that they are classical; this means that each particle's state can be considered independently from the other particles' states. Additionally, the particles are assumed to be in thermal equilibrium.

This relation can be written as an equation by introducing a normalizing factor:
where:
The denominator in Equation () is simply a normalizing factor so that the ratios formula_66 add up to unity — in other words it is a kind of partition function (for the single-particle system, not the usual partition function of the entire system).

Because velocity and speed are related to energy, Equation () can be used to derive relationships between temperature and the speeds of gas particles. All that is needed is to discover the density of microstates in energy, which is determined by dividing up momentum space into equal sized regions.

The potential energy is taken to be zero, so that all energy is in the form of kinetic energy.
The relationship between kinetic energy and momentum for massive non-relativistic particles is

where "p" is the square of the momentum vector 
p = ["p", "p", "p"]. We may therefore rewrite Equation () as:

where "Z" is the partition function, corresponding to the denominator in Equation (). Here "m" is the molecular mass of the gas, "T" is the thermodynamic temperature and "k" is the Boltzmann constant. This distribution of formula_66 is proportional to the probability density function "f" for finding a molecule with these values of momentum components, so:

The normalizing constant can be determined by recognizing that the probability of a molecule having "some" momentum must be 1. 
Integrating the exponential in () over all "p", "p", and "p" yields a factor of 

So that the normalized distribution function is:
The distribution is seen to be the product of three independent normally distributed variables formula_69, formula_70, and formula_71, with variance formula_72. Additionally, it can be seen that the magnitude of momentum will be distributed as a Maxwell–Boltzmann distribution, with formula_73.
The Maxwell–Boltzmann distribution for the momentum (or equally for the velocities) can be obtained more fundamentally using the H-theorem at equilibrium within the Kinetic theory of gases framework.

The energy distribution is found imposing

where formula_74 is the infinitesimal phase-space volume of momenta corresponding to the energy interval formula_75.
Making use of the spherical symmetry of the energy-momentum dispersion relation formula_76,
this can be expressed in terms of formula_75 as

Using then () in (), and expressing everything in terms of the energy formula_78, we get
and finally

Since the energy is proportional to the sum of the squares of the three normally distributed momentum components, this distribution is a gamma distribution; in particular, it is a chi-squared distribution with three degrees of freedom.

By the equipartition theorem, this energy is evenly distributed among all three degrees of freedom, so that the energy per degree of freedom is distributed as a chi-squared distribution with one degree of freedom:

where formula_81 is the energy per degree of freedom. At equilibrium, this distribution will hold true for any number of degrees of freedom. For example, if the particles are rigid mass dipoles of fixed dipole moment, they will have three translational degrees of freedom and two additional rotational degrees of freedom. The energy in each degree of freedom will be described according to the above chi-squared distribution with one degree of freedom, and the total energy will be distributed according to a chi-squared distribution with five degrees of freedom. This has implications in the theory of the specific heat of a gas.

The Maxwell–Boltzmann distribution can also be obtained by considering the gas to be a type of quantum gas for which the approximation "ε » k T" may be made.

Recognizing that the velocity probability density "f" is proportional to the momentum probability density function by

and using p = mv we get

which is the Maxwell–Boltzmann velocity distribution. The probability of finding a particle with velocity in the infinitesimal element ["dv", "dv", "dv"] about velocity v = ["v", "v", "v"] is

Like the momentum, this distribution is seen to be the product of three independent normally distributed variables formula_17, formula_18, and formula_19, but with variance formula_87. 
It can also be seen that the Maxwell–Boltzmann velocity distribution for the vector velocity
["v", "v", "v"] is the product of the distributions for each of the three directions:

where the distribution for a single direction is

Each component of the velocity vector has a normal distribution with mean formula_90 and standard deviation formula_91, so the vector has a 3-dimensional normal distribution, a particular kind of multivariate normal distribution, with mean formula_92 and standard deviation formula_93.

The Maxwell–Boltzmann distribution for the speed follows immediately from the distribution of the velocity vector, above. Note that the speed is

and the volume element in spherical coordinates

where formula_96 and formula_97 are the spherical coordinate angles of the velocity vector. Integration of the probability density function of the velocity over the solid angles formula_98 yields an additional factor of formula_99.
The speed distribution with substitution of the speed for the sum of the squares of the vector components:




</doc>
<doc id="19831" url="https://en.wikipedia.org/wiki?curid=19831" title="Margaret Thatcher">
Margaret Thatcher

Margaret Hilda Thatcher, Baroness Thatcher, (; 13 October 19258 April 2013) was a British stateswoman who served as Prime Minister of the United Kingdom from 1979 to 1990 and Leader of the Conservative Party from 1975 to 1990. She was the longest-serving British prime minister of the 20th century and the first woman to hold that office. A Soviet journalist dubbed her the Iron Lady, a nickname that became associated with her uncompromising politics and leadership style. As Prime Minister, she implemented policies that came to be known as Thatcherism.

A research chemist at Somerville College, Oxford, before becoming a barrister, Thatcher was elected Member of Parliament for Finchley in 1959. Edward Heath appointed her Secretary of State for Education and Science in his Conservative government. In 1975, Thatcher defeated Heath in the Conservative Party leadership election to become Leader of the Opposition, the first woman to lead a major political party in the United Kingdom. She became Prime Minister after winning the 1979 general election.

Thatcher introduced a series of economic policies intended to reverse high unemployment and Britain's struggles in the wake of the Winter of Discontent and an ongoing recession. Her political philosophy and economic policies emphasised deregulation (particularly of the financial sector), flexible labour markets, the privatisation of state-owned companies, and reducing the power and influence of trade unions. Thatcher's popularity in her first years in office waned amid recession and increasing unemployment, until victory in the 1982 Falklands War and the recovering economy brought a resurgence of support, resulting in her decisive re-election in 1983. She survived an assassination attempt in the Brighton hotel bombing in 1984.

Thatcher was re-elected for a third term in 1987, but her subsequent support for the Community Charge ("poll tax") was widely unpopular, and her views on the European Community were not shared by others in her Cabinet. She resigned as Prime Minister and party leader in November 1990, after Michael Heseltine launched a challenge to her leadership. After retiring from the Commons in 1992, she was given a life peerage as Baroness Thatcher (of Kesteven in the County of Lincolnshire) which entitled her to sit in the House of Lords. In 2013, she died of a stroke in London at the age of 87. Always a controversial figure, she has nonetheless been lauded as one of the greatest, most influential and most widely known politicians in British history, even as arguments over Thatcherism persist.
Thatcher was born Margaret Hilda Roberts on 13 October 1925, in Grantham, Lincolnshire. Her parents were Alfred Roberts (1892–1970), from Northamptonshire, and Beatrice Ethel (née Stephenson, 1888–1960), from Lincolnshire. She spent her childhood in Grantham, where her father owned two grocery shops. Prior to the Second World War, in 1938 the Roberts family briefly gave sanctuary to a teenage Jewish girl who had escaped Nazi Germany. Margaret and her pen-friending sister Muriel saved pocket money to help pay for the teenager's journey.
Alfred Roberts was an alderman and a Methodist local preacher, and brought up his daughter as a strict Wesleyan Methodist attending the Finkin Street Methodist Church. He came from a Liberal family but stood (as was then customary in local government) as an Independent. He was Mayor of Grantham in 1945–46 and lost his position as alderman in 1952 after the Labour Party won its first majority on Grantham Council in 1950.
Margaret Roberts attended Huntingtower Road Primary School and won a scholarship to Kesteven and Grantham Girls' School, a grammar school. Her school reports showed hard work and continual improvement; her extracurricular activities included the piano, field hockey, poetry recitals, swimming and walking. She was head girl in 1942–43. In her upper sixth year she applied for a scholarship to study chemistry at University of Oxford's Somerville College, a women's college at the time, but she was initially rejected and was offered a place only after another candidate withdrew.

Roberts arrived at Oxford in 1943 and graduated in 1947 with Second-Class Honours, in the four-year Chemistry Bachelor of Science degree, specialising in X-ray crystallography under the supervision of Dorothy Hodgkin. Her dissertation was on the structure of the antibiotic gramicidin. Thatcher did not devote herself entirely to studying chemistry as she only intended to be a chemist for a short period of time. Even when working on the subject, she was already thinking towards law and politics. She was reportedly more proud of becoming the first Prime Minister with a science degree than the first female Prime Minister, and as Prime Minister attempted to preserve Somerville as a women's college.

During her time at Oxford, she was noted for her isolated and serious attitude. Her first boyfriend, (1926–2014), recalled that she was "very thoughtful and a very good conversationalist. That's probably what interested me. She was good at general subjects". Her enthusiasm for politics as a girl made him think of her as "unusual". Bray met Roberts' parents and described her father as "slightly austere" and "totally correct" and her mother as "very proper" and "motherly".

At the end of the term at Oxford, Bray gradually became more distant and hoped for their relationship to "fizzle out". Bray later recalled that he thought Roberts had taken the relationship more seriously than he had done. When asked about Bray in later life, Thatcher prevaricated but acknowledged the circumstances between herself and Bray.

Roberts became President of the Oxford University Conservative Association in 1946. She was influenced at university by political works such as Friedrich Hayek's "The Road to Serfdom" (1944), which condemned economic intervention by government as a precursor to an authoritarian state.

After graduating, Roberts moved to Colchester in Essex to work as a research chemist for BX Plastics near Manningtree. In 1948 she applied for a job at ICI, but was rejected after the personnel department assessed her as "headstrong, obstinate and dangerously self-opinionated". Professor Jon Agar argued that her understanding of modern scientific research impacted her views as Prime Minister.

Roberts joined the local Conservative Association and attended the party conference at Llandudno, Wales, in 1948, as a representative of the University Graduate Conservative Association. Meanwhile, she became a high-ranking affiliate of the Vermin Club, a group of grassroots Conservatives formed in response to a derogatory comment made by Aneurin Bevan. One of her Oxford friends was also a friend of the Chair of the Dartford Conservative Association in Kent, who were looking for candidates. Officials of the association were so impressed by her that they asked her to apply, even though she was not on the party's approved list; she was selected in January 1950 (aged 24) and added to the approved list "post ante".

At a dinner following her formal adoption as Conservative candidate for Dartford in February 1949 she met divorcé Denis Thatcher, a successful and wealthy businessman, who drove her to her Essex train. After their first meeting she described him to Muriel as "not a very attractive creaturevery reserved but quite nice". In preparation for the election Roberts moved to Dartford, where she supported herself by working as a research chemist for J. Lyons and Co. in Hammersmith, part of a team developing emulsifiers for ice cream. Shortly after her marriage, she and her husband began attending Anglican services and would later convert to Anglicanism.

In the 1950 and 1951 general elections, Roberts was the Conservative candidate for the safe Labour seat of Dartford. The local party selected her as its candidate because, though not a dynamic public speaker, Roberts was well-prepared and fearless in her answers; prospective candidate Bill Deedes recalled that "Once she opened her mouth, the rest of us began to look rather second-rate". She attracted media attention as the youngest and the only female candidate. She lost on both occasions to Norman Dodds, but reduced the Labour majority by 6,000, and then a further 1,000. During the campaigns, she was supported by her parents and by future husband Denis Thatcher, whom she married in December 1951. Denis funded his wife's studies for the bar; she qualified as a barrister in 1953 and specialised in taxation. Later that same year their twins Carol and Mark were born, delivered prematurely by Caesarean section.

In 1954, Thatcher was defeated when she sought selection to be the Conservative party candidate for the Orpington by-election of January 1955. She chose not to stand as a candidate in the 1955 general election, in later years stating: "I really just felt the twins were ... only two, I really felt that it was too soon. I couldn't do that." Afterwards, Thatcher began looking for a Conservative safe seat and was selected as the candidate for Finchley in April 1958 (narrowly beating Ian Montagu Fraser). She was elected as MP for the seat after a hard campaign in the 1959 election. Benefiting from her fortunate result in a lottery for backbenchers to propose new legislation, Thatcher's maiden speech was, unusually, in support of her private member's bill (the Public Bodies (Admission to Meetings) Act 1960), requiring local authorities to hold their council meetings in public; the bill was successful and became law. In 1961 she went against the Conservative Party's official position by voting for the restoration of birching as a judicial corporal punishment.

Thatcher's talent and drive caused her to be mentioned as a future Prime Minister in her early 20s although she herself was more pessimistic, stating as late as 1970: "There will not be a woman prime minister in my lifetimethe male population is too prejudiced." In October 1961 she was promoted to the frontbench as Parliamentary Undersecretary at the Ministry of Pensions and National Insurance by Harold Macmillan. Thatcher was the youngest woman in history to receive such a post, and among the first MPs elected in 1959 to be promoted. After the Conservatives lost the 1964 election she became spokesman on Housing and Land, in which position she advocated her party's policy of allowing tenants to buy their council houses. She moved to the Shadow Treasury team in 1966 and, as Treasury spokesman, opposed Labour's mandatory price and income controls, arguing they would unintentionally produce effects that would distort the economy.

By 1966, party leaders viewed Thatcher as a potential Shadow Cabinet member. Jim Prior suggested her as a member after the Conservatives' 1966 defeat, but party leader Edward Heath and Chief Whip William Whitelaw eventually settled on Mervyn Pike as the Shadow Cabinet's sole woman member.

At the 1966 Conservative Party conference she criticised the high-tax policies of the Labour government as being steps "not only towards Socialism, but towards Communism", arguing that lower taxes served as an incentive to hard work. Thatcher was one of the few Conservative MPs to support Leo Abse's bill to decriminalise male homosexuality. She voted in favour of David Steel's bill to legalise abortion, as well as a ban on hare coursing. She supported the retention of capital punishment and voted against the relaxation of divorce laws.

In 1967, the United States Embassy in London chose Thatcher to take part in the International Visitor Leadership Program (then called the Foreign Leader Program), a professional exchange programme that gave her the opportunity to spend about six weeks visiting various US cities and political figures as well as institutions such as the International Monetary Fund. Although she was not yet a Shadow Cabinet member, the embassy reportedly described her to the State Department as a possible future Prime Minister. The description helped Thatcher meet with prominent persons during a busy itinerary focused on economic issues, including Paul Samuelson, Walt Rostow, Pierre-Paul Schweitzer and Nelson Rockefeller. Following the visit, Heath appointed Thatcher to the Shadow Cabinet as Fuel and Power spokesman. Prior to the 1970 general election, she was promoted to Shadow Transport spokesman and later to Education.

In 1968 Enoch Powell delivered his "Rivers of Blood" speech in which he strongly criticised Commonwealth immigration to the United Kingdom and the then-proposed Race Relations Bill. When Heath telephoned Thatcher to inform her that he was going to sack Powell from the Shadow Cabinet, she recalled that she "really thought that it was better to let things cool down for the present rather than heighten the crisis". She believed that his main points about Commonwealth immigration were correct and that his selected quotations from his speech had been taken out of context. In a 1991 interview for "Today", Thatcher stated that she thought Powell had "made a valid argument, if in sometimes regrettable terms".

Around this time she gave her first Commons speech as a Shadow Transport minister and highlighted the need for investment in British Rail. She argued: "... if we build bigger and better roads, they would soon be saturated with more vehicles and we would be no nearer solving the problem." Thatcher made her first visit to the Soviet Union in the summer of 1969 as the Opposition Transport spokeswoman, and in October delivered a speech celebrating her ten years in Parliament. A couple of months later, in early 1970, she told "The Finchley Press" that she would like to see a "reversal of the permissive society".

The Conservative Party led by Edward Heath won the 1970 general election, and Thatcher was subsequently appointed to the Cabinet as Secretary of State for Education and Science. Thatcher caused controversy when after only a few days in office she withdrew Labour's Circular 10/65 which attempted to force comprehensivisation, without going through a consultation progress. She was highly criticized for the speed in which she carried this out. Consequently, she drafted her own new policy (Circular 10/70) which ensured that a local authority was not forced to go comprehensive. Her new policy was not meant to stop the development of new comprehensives; she said: "We shall ... expect plans to be based on educational considerations rather than on the comprehensive principle."

Thatcher supported Lord Rothschild's 1971 proposal for market forces to affect government funding of research. Although many scientists opposed the proposal, her research background probably made her sceptical of their claim that outsiders should not interfere with funding. The department evaluated proposals for more local education authorities to close grammar schools and to adopt comprehensive secondary education. Although Thatcher was committed to a tiered secondary modern-grammar school system of education and attempted to preserve grammar schools, during her tenure as Education Secretary she turned down only 326 of 3,612 proposals (roughly 9 per cent) for schools to become comprehensives; the proportion of pupils attending comprehensive schools consequently rose from 32 per cent to 62 per cent. Nevertheless, she managed to save 94 grammar schools.
During her first months in office she attracted public attention as a consequence of the government's attempts to cut spending. She gave priority to academic needs in schools, while administering public expenditure cuts on the state education system, resulting in the abolition of free milk for schoolchildren aged seven to eleven. She held that few children would suffer if schools were charged for milk, but agreed to provide younger children with ⅓ pint daily for nutritional purposes. She also argued that she was simply carrying on with what the Labour government had started since they had stopped giving free milk to secondary schools. Milk would still be provided to those children that required it on medical grounds and schools could still sell milk. The aftermath of the milk row hardened her determination, she told the editor-proprietor Harold Creighton of "The Spectator": "Don't underestimate me, I saw how they broke Keith, but they won't break me."

Cabinet papers later revealed that she opposed the policy but had been forced into it by the Treasury. Her decision provoked a storm of protest from Labour and the press, leading to her being notoriously nicknamed "Margaret Thatcher, Milk Snatcher". She reportedly considered leaving politics in the aftermath and later wrote in her autobiography: "I learned a valuable lesson . I had incurred the maximum of political odium for the minimum of political benefit."

The Heath ministry continued to experience difficulties with oil embargoes and union demands for wage increases in 1973, subsequently losing the February 1974 general election. Labour formed a minority government and went on to win a narrow majority in the October 1974 general election. Heath's leadership of the Conservative Party looked increasingly in doubt. Thatcher was not initially seen as the obvious replacement, but she eventually became the main challenger, promising a fresh start. Her main support came from the parliamentary 1922 Committee and "The Spectator", but Thatcher's time in office gave her the reputation of a pragmatist rather than that of an ideologue. She defeated Heath on the first ballot and he resigned the leadership. In the second ballot she defeated Whitelaw, Heath's preferred successor. Thatcher's election had a polarising effect on the party, as her support was stronger among MPs on the right, and also among those from southern England, and those who had not attended public schools or Oxbridge.
Thatcher became Conservative Party leader and Leader of the Opposition on 11 February 1975; she appointed Whitelaw as her deputy. Heath was never reconciled to Thatcher's leadership of the party.

Television critic Clive James, writing in "The Observer" two days prior to the second leadership ballot, compared her voice of 1973 to a cat sliding down a blackboard. Thatcher had already begun to work on her presentation on the advice of Gordon Reece, a former television producer. By chance, Reece met the actor Laurence Olivier, who arranged lessons with the National Theatre's voice coach.

Thatcher began attending lunches regularly at the Institute of Economic Affairs (IEA), a think tank founded by Hayekian poultry magnate Antony Fisher; she had been visiting the IEA and reading its publications since the early 1960s. There she was influenced by the ideas of Ralph Harris and Arthur Seldon, and became the face of the ideological movement opposing the British welfare state. Keynesian economics, they believed, was weakening Britain. The institute's pamphlets proposed less government, lower taxes, and more freedom for business and consumers.
Thatcher intended to promote neoliberal economic ideas at home and abroad. Despite setting the direction of her foreign policy for a Conservative government, Thatcher was distressed by her repeated failure to shine in the House of Commons. Consequently, Thatcher decided that as "her voice was carrying little weight at home", she would "be heard in the wider world". Thatcher undertook visits across the Atlantic, establishing an international profile and promoting her economic and foreign policies. She toured the United States in 1975, and visited again in 1977, when she met US President Jimmy Carter. Among other foreign trips, she met Shah Mohammad Reza Pahlavi during a visit to Iran in April 1978. Thatcher chose to travel without her Shadow Foreign Secretary, Reginald Maudling, in an attempt to make a bolder personal impact.

In domestic affairs, Thatcher opposed Scottish devolution (home rule) and the creation of a Scottish Assembly. She instructed Conservative MPs to vote against the Scotland and Wales Bill in December 1976, which was successfully defeated, and then when new Bills were proposed she supported amending the legislation to allow the English to vote in the 1979 referendum on Scottish devolution.

Britain's economy during the 1970s was so weak that Foreign Secretary James Callaghan warned his fellow Labour Cabinet members in 1974 of the possibility of "a breakdown of democracy", telling them: "If I were a young man, I would emigrate." In mid-1978, the economy began to recover and opinion polls showed Labour in the lead, with a general election being expected later that year and a Labour win a serious possibility. Now Prime Minister, Callaghan surprised many by announcing on 7 September that there would be no general election that year and he would wait until 1979 before going to the polls. Thatcher reacted to this by branding the Labour government "chickens", and Liberal Party leader David Steel joined in, criticising Labour for "running scared".

The Labour government then faced fresh public unease about the direction of the country and a damaging series of strikes during the winter of 1978–79, dubbed the "Winter of Discontent". The Conservatives attacked the Labour government's unemployment record, using advertising with the slogan "Labour Isn't Working". A general election was called after the Callaghan ministry lost a motion of no confidence in early 1979. The Conservatives won a 44-seat majority in the House of Commons and Thatcher became the first female British prime minister.

In 1976, Thatcher gave her "Britain Awake" foreign policy speech which lambasted the Soviet Union for seeking world dominance. The Soviet Army journal "Krasnaya Zvezda" ("Red Star") rebutted her stance in a piece entitled "Iron Lady Raises Fears" by Captain Yuri Gavrilov (alluding to "Iron Chancellor" Bismarck of imperial Germany). "The Sunday Times" covered the "Red Star" article the next day, and Thatcher embraced the epithet a week later; in a speech to Finchley Conservatives she compared it to the Duke of Wellington's nickname "The Iron Duke". The metaphorical sobriquet followed her throughout her political career, and has since become a generic descriptor for strong-willed female politicians.

Thatcher became Prime Minister on 4 May 1979. Arriving at Downing Street she said, paraphrasing the misattributed Prayer of Saint Francis:

Thatcher was to remain in office throughout the 1980s. For most of her premiership, she was described as the most powerful woman in the world.

Thatcher was Leader of the Opposition and Prime Minister at a time of increased racial tension in Britain. Commenting on the local elections of 1977, "The Economist" noted "The Tory tide swamped the smaller parties. That specifically includes the National Front (NF), which suffered a clear decline from last year". Her standing in the polls rose by 11% after a January 1978 interview for "World in Action" in which she said "the British character has done so much for democracy, for law and done so much throughout the world that if there is any fear that it might be swamped people are going to react and be rather hostile to those coming in", as well as "in many ways add to the richness and variety of this country. The moment the minority threatens to become a big one, people get frightened". In the 1979 general election, the Conservatives attracted voters from the NF, whose support almost collapsed. In a meeting in July 1979 with Foreign Secretary Lord Carrington and Home Secretary William Whitelaw she objected to the number of Asian immigrants, in the context of limiting the total of Vietnamese boat people allowed to settle in the UK to fewer than 10,000 over two years.

As Prime Minister, Thatcher met weekly with Queen Elizabeth II to discuss government business, and their relationship came under close scrutiny. Her biographer John Campbell writes: Michael Shea, the Queen's press secretary, had reportedly leaked anonymous rumours of a rift, which were officially denied by William Heseltine, the Private Secretary to the Sovereign. Thatcher later wrote: "I always found the Queen's attitude towards the work of the Government absolutely correct ... stories of clashes between 'two powerful women' were just too good not to make up."

Thatcher's economic policy was influenced by monetarist thinking and economists such as Milton Friedman and Alan Walters. Together with Chancellor of the Exchequer Geoffrey Howe, she lowered direct taxes on income and increased indirect taxes. She increased interest rates to slow the growth of the money supply and thereby lower inflation, introduced cash limits on public spending, and reduced expenditure on social services such as education and housing. Cuts to higher education resulted in her becoming the first Oxford-educated post-war Prime Minister without an honorary doctorate from Oxford University, after a 738–319 vote of the governing assembly and a student petition.

Her new centrally-funded City Technology Colleges did not achieve much success, and the Funding Agency for Schools was set up to control expenditure by opening and closing schools; a right-wing think tank described it as having "an extraordinary range of dictatorial powers".

Some Heathite Conservatives in the Cabinet, the so-called "wets", expressed doubt over Thatcher's policies. The 1981 England riots resulted in the British media discussing the need for a policy U-turn. At the 1980 Conservative Party conference, Thatcher addressed the issue directly, with a speech written by the playwright Ronald Millar that included the lines: "You turn if you want to. The lady's not for turning!"

Thatcher's job approval rating fell to 23% by December 1980, lower than recorded for any previous Prime Minister. As the recession of the early 1980s deepened, she increased taxes, despite concerns expressed in a March 1981 statement signed by 364 leading economists:

By 1982, the UK began to experience signs of economic recovery; inflation was down to 8.6% from a high of 18%, but unemployment was over 3 million for the first time since the 1930s. By 1983, overall economic growth was stronger, and inflation and mortgage rates had fallen to their lowest levels in 13 years, although manufacturing employment as a share of total employment fell to just over 30%, with total unemployment remaining high, peaking at 3.3 million in 1984.

During the 1982 Conservative Party Conference, Thatcher said: "We have done more to roll back the frontiers of socialism than any previous Conservative Government." She claimed at the Party Conference the following year that the British people had completely rejected state socialism and understood:

By 1987, unemployment was falling, the economy was stable and strong and inflation was low. Opinion polls showed a comfortable Conservative lead, and local council election results had also been successful, prompting Thatcher to call a general election for 11 June that year, despite the deadline for an election still being 12 months away. The election saw Thatcher re-elected for a third successive term.

Thatcher had been firmly opposed to British membership of the Exchange Rate Mechanism (ERM, a precursor to European Monetary Union), believing that it would constrain the British economy, despite the urging of both her Chancellor of the Exchequer Nigel Lawson and Foreign Secretary Geoffrey Howe; in October 1990 she was persuaded by John Major, Lawson's successor as Chancellor, to join the ERM at what proved to be too high a rate.

Thatcher reformed local government taxes by replacing domestic rates (a tax based on the nominal rental value of a home) with the Community Charge (or poll tax) in which the same amount was charged to each adult resident. The new tax was introduced in Scotland in 1989 and in England and Wales the following year, and proved to be among the most unpopular policies of her premiership. Public disquiet culminated in a 70,000 to 200,000-strong demonstration in London in March 1990; the demonstration around Trafalgar Square deteriorated into riots, leaving 113 people injured and 340 under arrest. The Community Charge was abolished in 1991 by her successor, John Major. It has since transpired that Thatcher herself had failed to register for the tax, and was threatened with financial penalties if she did not return her form.

Thatcher believed that the trade unions were harmful to both ordinary trade unionists and the public. She was committed to reducing the power of the unions, whose leadership she accused of undermining parliamentary democracy and economic performance through strike action. Several unions launched strikes in response to legislation introduced to limit their power, but resistance eventually collapsed. Only 39% of union members voted Labour in the 1983 general election. According to the BBC in 2004, Thatcher "managed to destroy the power of the trade unions for almost a generation". The miners' strike of 1984–85 was the biggest confrontation between the unions and the government under Thatcher.

In March 1984, the National Coal Board (NCB) proposed to close 20 of the 174 state-owned mines and cut 20,000 jobs out of 187,000. Two-thirds of the country's miners, led by the National Union of Mineworkers (NUM) under Arthur Scargill, downed tools in protest. However, Scargill refused to hold a ballot on the strike, having previously lost three ballots on a national strike (in January and October 1982, and March 1983). This led to the strike being declared illegal by the High Court of Justice.

Thatcher refused to meet the union's demands and compared the miners' dispute to the Falklands War, declaring in a speech in 1984: "We had to fight the enemy without in the Falklands. We always have to be aware of the enemy within, which is much more difficult to fight and more dangerous to liberty." Although Thatcher had only described the miners' leaders and left-wing authorities as the "enemy within", her opponents quickly misrepresented it as a reference to all miners and as a sign that she showed contempt for the organised working class. Consequently, the phrase was forever used against her in the future.

After a year out on strike, in March 1985 the NUM leadership conceded without a deal. The cost to the economy was estimated to be at least £1.5 billion, and the strike was blamed for much of the pound's fall against the US dollar. Thatcher reflected on the end of the strike and said that "if anyone has won" it was "the miners who stayed at work" and all those "that have kept Britain going".

The government closed 25 unprofitable coal mines in 1985, and by 1992 a total of 97 mines had been closed; those that remained were privatised in 1994. The resulting closure of 150 coal mines, some of which were not losing money, resulted in the loss of tens of thousands of jobs and had the effect of devastating entire communities. Strikes had helped bring down Heath's government, and Thatcher was determined to succeed where he failed. Her strategy of preparing fuel stocks, appointing hardliner Ian MacGregor as NCB leader, and ensuring that police were adequately trained and equipped with riot gear, contributed to her triumph over the striking miners.

The number of stoppages across the UK peaked at 4,583 in 1979, when more than 29 million working days had been lost. In 1984, the year of the miners' strike, there were 1,221, resulting in the loss of more than 27 million working days. Stoppages then fell steadily throughout the rest of Thatcher's premiership; in 1990 there were 630 and fewer than 2 million working days lost, and they continued to fall thereafter. Thatcher's tenure also witnessed a sharp decline in trade union density, with the percentage of workers belonging to a trade union falling from 57.3% in 1979 to 49.5% in 1985. In 1979 up until Thatcher's final year in office, trade union membership also fell, from 13.5 million in 1979 to fewer than 10 million.

The policy of privatisation has been called "a crucial ingredient of Thatcherism". After the 1983 election the sale of state utilities accelerated; more than £29 billion was raised from the sale of nationalised industries, and another £18 billion from the sale of council houses. The process of privatisation, especially the preparation of nationalised industries for privatisation, was associated with marked improvements in performance, particularly in terms of labour productivity.

Some of the privatised industries, including gas, water, and electricity, were natural monopolies for which privatisation involved little increase in competition. The privatised industries that demonstrated improvement sometimes did so while still under state ownership. British Steel Corporation had made great gains in profitability while still a nationalised industry under the government-appointed MacGregor chairmanship, which faced down trade-union opposition to close plants and halve the workforce. Regulation was also significantly expanded to compensate for the loss of direct government control, with the foundation of regulatory bodies such as Oftel (1984), Ofgas (1986), and the National Rivers Authority (1989). There was no clear pattern to the degree of competition, regulation, and performance among the privatised industries.

In most cases privatisation benefited consumers in terms of lower prices and improved efficiency, but results overall have been mixed. Not all privatised companies have had successful share price trajectories in the longer term. A 2010 review by the Institute of Economic Affairs in London states:

Thatcher always resisted privatising British Rail and was said to have told Transport Secretary Nicholas Ridley that "Railway privatisation will be the Waterloo of this government. Please never mention the railways to me again". Shortly before her resignation in 1990, she accepted the arguments for privatisation, which her successor John Major implemented in 1994.

The privatisation of public assets was combined with financial deregulation in an attempt to fuel economic growth. Chancellor Geoffrey Howe abolished the UK's exchange controls in 1979, which allowed more capital to be invested in foreign markets, and the Big Bang of 1986 removed many restrictions on the London Stock Exchange.

In 1980 and 1981, Provisional Irish Republican Army (PIRA) and Irish National Liberation Army (INLA) prisoners in Northern Ireland's Maze Prison carried out hunger strikes in an effort to regain the status of political prisoners that had been removed in 1976 by the preceding Labour government.

Bobby Sands began the 1981 strike, saying that he would fast until death unless prison inmates won concessions over their living conditions. Thatcher refused to countenance a return to political status for the prisoners, having declared "Crime is crime is crime; it is not political", Nevertheless, the British government privately contacted republican leaders in a bid to bring the hunger strikes to an end. After the deaths of Sands and nine others, the strike ended. Some rights were restored to paramilitary prisoners, but not official recognition of political status. Violence in Northern Ireland escalated significantly during the hunger strikes.

Thatcher narrowly escaped injury in an IRA assassination attempt at a Brighton hotel early in the morning on 12 October 1984. Five people were killed, including the wife of minister John Wakeham. Thatcher was staying at the hotel to prepare for the Conservative Party conference, which she insisted should open as scheduled the following day. She delivered her speech as planned, though rewritten from her original draft, in a move that was widely supported across the political spectrum and enhanced her popularity with the public.

On 6 November 1981, Thatcher and Irish Taoiseach Garret FitzGerald established the Anglo-Irish Inter-Governmental Council, a forum for meetings between the two governments. On 15 November 1985, Thatcher and FitzGerald signed the Hillsborough Anglo-Irish Agreement, which marked the first time a British government had given the Republic of Ireland an advisory role in the governance of Northern Ireland. In protest, the Ulster Says No movement led by Ian Paisley attracted 100,000 to a rally in Belfast, Ian Gow, later assassinated by the PIRA, resigned as Minister of State in the HM Treasury, and all 15 Unionist MPs resigned their parliamentary seats; only one was not returned in the subsequent by-elections on 23 January 1986.

Thatcher supported an active climate protection policy and was instrumental in the passing of the Environmental Protection Act 1990, the establishment of the Intergovernmental Panel on Climate Change, and in founding the Hadley Centre for Climate Research and Prediction. She helped to put climate change, acid rain and general pollution in the British mainstream in the late 1980s, and in 1989 she called for a global treaty on climate change. Her speeches included one to the Royal Society on 27 September 1988 and to the UN General Assembly in November 1989. However, following her retirement as Prime Minister in 1990, she became sceptical about climate change policy and rejected climate alarmism.

Thatcher appointed Lord Carrington, a senior member of the party and former Minister of Defence, as Foreign Minister in 1979. Although he was considered a "wet", he avoided domestic affairs and got along well with Thatcher. The first issue was what to do with Rhodesia, where the five-percent white population was determined to rule the prosperous, largely-black ex-colony in the face of overwhelming international disapproval. After the collapse of the Portuguese Empire in Africa in 1975, South Africa, which had been Rhodesia's chief supporter, realised that country was a liability. Black rule was inevitable, and Carrington brokered a peaceful solution at the Lancaster House conference in December 1979, attended by Rhodesian Prime Minister Ian Smith, as well as the key black leaders: Abel Muzorewa, Robert Mugabe, Joshua Nkomo and Josiah Tongogara. The conference ended the Rhodesian Bush War. The end result was the new nation of Zimbabwe under black rule in 1980.

Thatcher's first foreign policy crisis came with the 1979 Soviet invasion of Afghanistan. She condemned the invasion, said it showed the bankruptcy of a détente policy, and helped convince some British athletes to boycott the 1980 Moscow Olympics. She gave weak support to US President Jimmy Carter who tried to punish the USSR with economic sanctions. Britain's economic situation was precarious, and most of NATO was reluctant to cut trade ties. The "Financial Times" reported that her government had secretly supplied Saddam Hussein with military equipment since 1981.
Thatcher became closely aligned with the Cold War policies of US President Ronald Reagan, based on their shared distrust of Communism. A disagreement came in 1983 when Reagan did not consult with her on the invasion of Grenada. During her first year as Prime Minister she supported NATO's decision to deploy US nuclear cruise and Pershing II missiles in Western Europe, permitting the US to station more than 160 cruise missiles at RAF Greenham Common, starting on 14 November 1983 and triggering mass protests by the Campaign for Nuclear Disarmament. She bought the Trident nuclear missile submarine system from the US to replace Polaris, tripling the UK's nuclear forces at an eventual cost of more than £12 billion (at 1996–97 prices). Thatcher's preference for defence ties with the US was demonstrated in the Westland affair of 1985–86, when she acted with colleagues to allow the struggling helicopter manufacturer Westland to refuse a takeover offer from the Italian firm Agusta in favour of the management's preferred option, a link with Sikorsky Aircraft. Defence Secretary Michael Heseltine, who had supported the Agusta deal, resigned from the government in protest.
On 2 April 1982 the ruling military junta in Argentina ordered the invasion of the British possessions of the Falkland Islands and South Georgia, triggering the Falklands War. The subsequent crisis was "a defining moment of her premiership". At the suggestion of Harold Macmillan and Robert Armstrong, she set up and chaired a small War Cabinet (formally called ODSA, Overseas and Defence committee, South Atlantic) to oversee the conduct of the war, which by 5–6 April had authorised and dispatched a naval task force to retake the islands. Argentina surrendered on 14 June and "Operation Corporate" was hailed a success, notwithstanding the deaths of 255 British servicemen and 3 Falkland Islanders. Argentine fatalities totalled 649, half of them after the nuclear-powered submarine torpedoed and sank the cruiser on 2 May. Thatcher was criticised for the neglect of the Falklands' defence that led to the war, and especially by Tam Dalyell in Parliament for the decision to torpedo the "General Belgrano", but overall she was considered a highly capable and committed war leader. The "Falklands factor", an economic recovery beginning early in 1982, and a bitterly divided opposition all contributed to Thatcher's second election victory in 1983. Thatcher frequently referred after the war to the "Falklands spirit"; journalists Max Hastings and Simon Jenkins suggested in 1983 that this reflected her preference for the streamlined decision-making of her War Cabinet over the painstaking deal-making of peacetime cabinet government.

In September 1982 she visited China to discuss with Deng Xiaoping the sovereignty of Hong Kong after 1997. China was the first communist state Thatcher had visited and she was the first British prime minister to visit China. Throughout their meeting, she sought the PRC's agreement to a continued British presence in the territory. Deng insisted that the PRC's sovereignty on Hong Kong was non-negotiable, but stated his willingness to settle the sovereignty issue with the British government through formal negotiations, and both governments promised to maintain Hong Kong's stability and prosperity. After the two-year negotiations, Thatcher conceded to the PRC government and signed the Sino-British Joint Declaration in Beijing in 1984, agreeing to hand over Hong Kong's sovereignty in 1997.

In April 1986 she permitted US F-111s to use Royal Air Force bases for the bombing of Libya in retaliation for the alleged Libyan bombing of a Berlin discothèque, citing the right of self-defence under Article 51 of the UN Charter. Polls suggested that fewer than one in three British citizens approved of her decision.

Although saying that she was in favour of "peaceful negotiations" to end apartheid, Thatcher opposed sanctions imposed on South Africa by the Commonwealth and the European Economic Community (EEC). She attempted to preserve trade with South Africa while persuading the government there to abandon apartheid. This included "[c]asting herself as President Botha's candid friend", and inviting him to visit the UK in 1984, in spite of the "inevitable demonstrations" against his government. Thatcher dismissed the African National Congress (ANC) in October 1987 as "a typical terrorist organisation". During his visit to Britain five months after his release from prison, Nelson Mandela praised Thatcher: "She is an enemy of apartheid ... We have much to thank her for."
Thatcher and her government backed the Khmer Rouge keeping their seat in the UN after they were ousted from power in Cambodia by the Cambodian–Vietnamese War. Although Thatcher denied it at the time, it was revealed in 1991 that from 1983 the SAS was sent to secretly train the "non-Communist" members of the CGDK to fight against the Vietnamese-backed Kampuchea (PRK) government. The "non-communist members", such as the Sihanoukists and the Khmer People's National Liberation Front, "were dominated, diplomatically and militarily, by the Khmer Rouge". It was reported that the SAS had taught "the use of improvised explosive devices, booby traps and the manufacture and use of time-delay devices", in what activist Rae McGrath denounced as "a criminally irresponsible and cynical policy".
Thatcher and her party supported British membership of the EEC in the 1975 national referendum, but she believed that the role of the organisation should be limited to ensuring free trade and effective competition, and feared that the EEC's approach was at odds with her views on smaller government and deregulation. Her opposition to further European integration became more pronounced during her premiership and particularly after her third election victory in 1987. During a 1988 speech in Bruges she outlined her opposition to proposals from the EEC, forerunner of the European Union, for a federal structure and increased centralisation of decision making. She said: "We have not successfully rolled back the frontiers of the state in Britain, only to see them re-imposed at a European level, with a European super-state exercising a new dominance from Brussels."

Thatcher was one of the first Western leaders to respond warmly to reformist Soviet leader Mikhail Gorbachev. Following Reagan–Gorbachev summit meetings and reforms enacted by Gorbachev in the USSR, she declared in November 1988 that "We're not in a Cold War now", but rather in a "new relationship much wider than the Cold War ever was". She went on a state visit to the Soviet Union in 1984 and met with Gorbachev and Council of Ministers Chairman Nikolai Ryzhkov.

Thatcher was in the US on a state visit when Iraqi leader Saddam Hussein invaded neighbouring Kuwait in August 1990. During her talks with President George H. W. Bush, who succeeded Reagan in 1989, she recommended intervention, and put pressure on Bush to deploy troops in the Middle East to drive the Iraqi Army out of Kuwait. Bush was apprehensive about the plan, prompting Thatcher to remark to him during a telephone conversation that "This was no time to go wobbly!" Thatcher's government supplied military forces to the international coalition in the build-up to the Gulf War, but she had resigned by the time hostilities began on 17 January 1991. She applauded the coalition victory as a backbencher, but warned that "the victories of peace will take longer than the battles of war". It was later disclosed that Thatcher suggested threatening Saddam with chemical weapons after the invasion of Kuwait.

Thatcher, sharing the concerns of French President François Mitterrand, was initially opposed to German reunification, telling Gorbachev that it "would lead to a change to postwar borders, and we cannot allow that because such a development would undermine the stability of the whole international situation and could endanger our security". She expressed concern that a united Germany would align itself more closely with the Soviet Union and move away from NATO. In March 1990, West German Chancellor Helmut Kohl reassured Thatcher that he would keep her "informed of all his intentions about unification", and that he was prepared to disclose "matters which even his cabinet would not know". In November 1989, Thatcher hailed the fall of the Berlin Wall as "a great day for freedom".

Thatcher was challenged for the leadership of the Conservative Party by the little-known backbench MP Sir Anthony Meyer in the 1989 leadership election. Of the 374 Conservative MPs eligible to vote, 314 voted for Thatcher and 33 for Meyer. Her supporters in the party viewed the result as a success, and rejected suggestions that there was discontent within the party.

During her premiership Thatcher had the second-lowest average approval rating (40%) of any post-war Prime Minister. Since the resignation of Nigel Lawson as Chancellor in October 1989, polls consistently showed that she was less popular than her party. A self-described conviction politician, Thatcher always insisted that she did not care about her poll ratings and pointed instead to her unbeaten election record.

Opinion polls in September 1990 reported that Labour had established a 14% lead over the Conservatives, and by November the Conservatives had been trailing Labour for 18 months. These ratings, together with Thatcher's combative personality and tendency to override collegiate opinion, contributed to discontent within her party.

Thatcher removed Geoffrey Howe as Foreign Secretary in July 1989 after he and Lawson had forced her to agree to a plan for Britain to join the European Exchange Rate Mechanism (ERM). Britain joined the ERM in October 1990. On 1 November 1990, Howe, by then the last remaining member of Thatcher's original 1979 cabinet, resigned from his position as Deputy Prime Minister, ostensibly over her open hostility to moves towards European Monetary Union. In his resignation speech on 13 November, Howe commented on Thatcher's openly dismissive attitude to the government's proposal for a new European currency competing against existing currencies (a "hard ECU"): Howe's resignation hastened the end to Thatcher's premiership.

On 14 November, Michael Heseltine mounted a challenge for the leadership of the Conservative Party. Opinion polls had indicated that he would give the Conservatives a national lead over Labour. Although Thatcher led on the first ballot with the votes of 204 Conservative MPs (54.8%) to 152 votes (40.9%) for Heseltine and 16 abstentions, she was four votes short of the required 15% majority. A second ballot was therefore necessary. Thatcher initially declared her intention to "fight on and fight to win" the second ballot, but consultation with her Cabinet persuaded her to withdraw. After holding an audience with the Queen, calling other world leaders, and making one final Commons speech, on 28 November she left Downing Street in tears. She reportedly regarded her ousting as a betrayal. Her resignation was a shock to many outside Britain, with foreign observers such as Henry Kissinger and Gorbachev privately expressing consternation.

Thatcher was replaced as Prime Minister and party leader by Chancellor John Major, who prevailed over Heseltine in the subsequent ballot. Major oversaw an upturn in Conservative support in the 17 months leading to the 1992 general election and led the party to a fourth successive victory on 9 April 1992. Thatcher favoured Major in the leadership contest, but her support for him waned in later years.

Thatcher returned to the backbenches as a constituency parliamentarian after leaving the premiership. Her domestic approval rating recovered after her resignation; the balance of public opinion was that her government had been good for the country. Aged 66, she retired from the House at the 1992 general election, saying that leaving the Commons would allow her more freedom to speak her mind.

Upon leaving the House of Commons, Thatcher became the first former Prime Minister to set up a foundation; the British wing of the Margaret Thatcher Foundation was dissolved in 2005 due to financial difficulties. She wrote two volumes of memoirs, "The Downing Street Years" (1993) and "The Path to Power" (1995). In 1991 she and her husband Denis moved to a house in Chester Square, a residential garden square in central London's Belgravia district.

Thatcher was hired by the tobacco company Philip Morris as a "geopolitical consultant" in July 1992, for $250,000 per year and an annual contribution of $250,000 to her foundation. Thatcher earned $50,000 for each speech she delivered.

In August 1992 she called for NATO to stop the Serbian assault on Goražde and Sarajevo to end ethnic cleansing during the Bosnian War. In an op-ed, she compared the situation in Bosnia–Herzegovina to "the worst excesses of the Nazis", and warned that there could be a "holocaust". She was an advocate of Croatian and Slovenian independence. In a 1991 interview for Croatian Radiotelevision, Thatcher commented on the Yugoslav Wars; she was critical of Western governments for not recognising the breakaway republics of Croatia and Slovenia as independent states and for not supplying them with armaments after the Serbian-led Yugoslav Army attacked.

She made a series of speeches in the Lords criticising the Maastricht Treaty, describing it as "a treaty too far" and stated: "I could never have signed this treaty." She cited A. V. Dicey when arguing that, as all three main parties were in favour of the treaty, the people should have their say in a referendum.

Thatcher served as honorary chancellor of the College of William & Mary in Virginia from 1993 to 2000, while also serving as chancellor of the private University of Buckingham from 1992 to 1998, a university she had formally opened in 1976 as the former Education Secretary.

After Tony Blair's election as Labour Party leader in 1994, Thatcher praised Blair as "probably the most formidable Labour leader since Hugh Gaitskell", adding: "I see a lot of socialism behind their front bench, but not in Mr Blair. I think he genuinely has moved." Blair responded in kind: "She was a thoroughly determined person, and that is an admirable quality".

In 1998, Thatcher called for the release of former Chilean dictator Augusto Pinochet when Spain had him arrested and sought to try him for human rights violations. She cited the help he gave Britain during the Falklands War. In 1999, she visited him while he was under house arrest near London. Pinochet was released in March 2000 on medical grounds by Home Secretary Jack Straw.
At the 2001 general election, Thatcher supported the Conservative campaign, as she had done in 1992 and 1997, and in the Conservative leadership election following its defeat, she endorsed Iain Duncan Smith over Kenneth Clarke. In 2002 she encouraged George W. Bush to aggressively tackle the "unfinished business" of Iraq under Saddam Hussein, and praised Blair for his "strong, bold leadership" in standing with Bush in the Iraq War.

She broached the same subject in her "", which was published in April 2002 and dedicated to Ronald Reagan, writing that there would be no peace in the Middle East until Saddam Hussein was toppled. Her book also said that Israel must trade land for peace, and that the European Union (EU) was a "fundamentally unreformable", "classic utopian project, a monument to the vanity of intellectuals, a programme whose inevitable destiny is failure". She argued that Britain should renegotiate its terms of membership or else leave the EU and join the North American Free Trade Area.

Following several small strokes she was advised by her doctors not to engage in further public speaking. In March 2002 she announced that on doctors' advice she would cancel all planned speaking engagements and accept no more.

On 26 June 2003, Thatcher's husband Sir Denis died of pancreatic cancer, and was cremated on 3 July.

On 11 June 2004, Thatcher (against doctor's orders) attended the state funeral service for Ronald Reagan. She delivered her eulogy via videotape; in view of her health, the message had been pre-recorded several months earlier. Thatcher flew to California with the Reagan entourage, and attended the memorial service and interment ceremony for the president at the Ronald Reagan Presidential Library.

In 2005, Thatcher criticised the way the decision to invade Iraq had been made two years previously. Although she still supported the intervention to topple Saddam Hussein, she said that (as a scientist) she would always look for "facts, evidence and proof", before committing the armed forces. She celebrated her 80th birthday on 13 October at the Mandarin Oriental Hotel in Hyde Park, London; guests included the Queen, the Duke of Edinburgh, Princess Alexandra and Tony Blair. Lord (Geoffrey) Howe of Aberavon was also in attendance and said of Thatcher: "Her real triumph was to have transformed not just one party but two, so that when Labour did eventually return, the great bulk of Thatcherism was accepted as irreversible."

Thatcher's daughter Carol that her mother had dementia in 2005, saying "Mum doesn't read much any more because of her memory loss". In her 2008 memoir, Carol wrote that her mother "could hardly remember the beginning of a sentence by the time she got to the end". She later recounted how she was first struck by her mother's dementia when, in conversation, Thatcher confused the Falklands and Yugoslav conflicts; she recalled the pain of needing to tell her mother repeatedly that her husband Denis was dead.
In 2006, Thatcher attended the official Washington, D.C. memorial service to commemorate the fifth anniversary of the 11 September attacks on the US. She was a guest of Vice President Dick Cheney, and met Secretary of State Condoleezza Rice during her visit. In February 2007 Thatcher became the first living British prime minister to be honoured with a statue in the Houses of Parliament. The bronze statue stands opposite that of her political hero, Sir Winston Churchill, and was unveiled on 21 February 2007 with Thatcher in attendance; she remarked in the Members' Lobby of the Commons: "I might have preferred ironbut bronze will do ... It won't rust."

Thatcher was a public supporter of the Prague Declaration on European Conscience and Communism and the resulting Prague Process, and sent a public letter of support to its preceding conference.

After collapsing at a House of Lords dinner, Thatcher, suffering low blood pressure, was admitted to St Thomas' Hospital in central London on 7 March 2008 for tests. In 2009 she was hospitalised again when she fell and broke her arm. Thatcher returned to 10 Downing Street in late November 2009 for the unveiling of an official portrait by artist Richard Stone, an unusual honour for a living former Prime Minister. Stone was previously commissioned to paint portraits of the Queen and Queen Mother.

On 4 July 2011, Thatcher was to attend a ceremony for the unveiling of a statue to Ronald Reagan, outside the US Embassy in London, but was unable to attend due to her frail health. She last attended a sitting of the House of Lords on 19 July 2010, and on 30 July 2011 it was announced that her office in the Lords had been closed. Earlier that month, Thatcher was named the most competent Prime Minister of the past 30 years in an Ipsos MORI poll.

Baroness Thatcher died on 8 April 2013, at the age of 87, after suffering a stroke. She had been staying at a suite in the Ritz Hotel in London since December 2012 after having difficulty with stairs at her Chester Square home in Belgravia. Her death certificate listed the primary causes of death as a "cerebrovascular accident" and "repeated transient ischaemic attack"; secondary causes were listed as a "carcinoma of the bladder" and dementia.

Details of Thatcher's funeral had been agreed with her in advance. She received a ceremonial funeral, including full military honours, with a church service at St Paul's Cathedral on 17 April.

Queen Elizabeth II and the Duke of Edinburgh attended her funeral, marking only the second time in the Queen's reign that she attended the funeral of any of her former prime ministers; the first and only precedent being that of Winston Churchill, who received a state funeral in 1965.

After the service at St Paul's Cathedral, Thatcher's body was cremated at Mortlake Crematorium, where her husband had been cremated. On 28 September, a service for Thatcher was held in the All Saints Chapel of the Royal Hospital Chelsea's Margaret Thatcher Infirmary. In a private ceremony, Thatcher's ashes were interred in the grounds of the hospital, next to those of her husband.

Thatcherism represented a systematic and decisive overhaul of the post-war consensus, whereby the major political parties largely agreed on the central themes of Keynesianism, the welfare state, nationalised industry, and close regulation of the economy, and high taxes. Thatcher generally supported the welfare state, while proposing to rid it of abuses.

She promised in 1982 that the highly popular National Health Service was "safe in our hands". At first she ignored the question of privatising nationalised industries. Heavily influenced by right-wing think tanks, and especially by Keith Joseph, Thatcher broadened her attack. Thatcherism came to refer to her policies as well as aspects of her ethical outlook and personal style, including moral absolutism, nationalism, interest in the individual, and an uncompromising approach to achieving political goals.

Thatcher defined her own political philosophy in a major and controversial break with the one-nation conservatism of Edward Heath and her Conservative predecessors in an interview published in "Woman's Own" magazine, three months after her victory in the 1987 general election:I think we have gone through a period when too many children and people have been given to understand "I have a problem, it is the Government's job to cope with it!" or "I have a problem, I will go and get a grant to cope with it!" "I am homeless, the Government must house me!" and so they are casting their problems on society and who is society? There is no such thing! There are individual men and women and there are families and no government can do anything except through people and people look to themselves first. It is our duty to look after ourselves and then also to help look after our neighbour and life is a reciprocal business and people have got the entitlements too much in mind without the obligations.

The number of adults owning shares rose from 7 per cent to 25 per cent during her tenure, and more than a million families bought their council houses, giving an increase from 55 per cent to 67 per cent in owner occupiers from 1979 to 1990. The houses were sold at a discount of 33–55 per cent, leading to large profits for some new owners. Personal wealth rose by 80 per cent in real terms during the 1980s, mainly due to rising house prices and increased earnings. Shares in the privatised utilities were sold below their market value to ensure quick and wide sales, rather than maximise national income.

Thatcher's premiership was also marked by periods of high unemployment and social unrest, and many critics on the left of the political spectrum fault her economic policies for the unemployment level; many of the areas affected by mass unemployment as well as her monetarist economic policies remained blighted for decades, by such social problems as drug abuse and family breakdown. Unemployment did not fall below its 1979 level during her tenure, although in June 1990 the recorded rate (5.4%) was lower than the rate in April 1979 (5.5%). The long-term effects of her policies on manufacturing remain contentious.

Conversing in Scotland in April 2009, Thatcher insisted she had no regrets and was right to introduce the "poll tax" and withdraw subsidies from "outdated industries, whose markets were in terminal decline", subsidies that created "the culture of dependency, which had done such damage to Britain". Political economist Susan Strange termed the new financial growth model "casino capitalism", reflecting her view that speculation and financial trading were becoming more important to the economy than industry.

Critics on the left describe her as divisive and claim she condoned greed and selfishness. Welsh politician Rhodri Morgan and others have characterised Thatcher as a "Marmite" figure. Michael White, writing in the "New Statesman", challenged the view that her reforms had brought a net benefit. Others depict her approach as having been "a mixed bag" or "Curate's egg".

Thatcher did "little to advance the political cause of women" either within her party or the government. Burns states that some British feminists regarded her as "an enemy". June Purvis claims that although Thatcher had struggled laboriously against the sexist prejudices of her day to rise to the top, she made no effort to ease the path for other women. Thatcher did not regard women's rights as requiring particular attention as she did not, especially during her premiership, consider that women were being deprived of their rights. She suggested that women should be shortlisted by default for all public appointments but had once proposed that those with young children ought to leave the work force.
Thatcher's stance on immigration in the late 1970s was perceived as part of a rising racist public discourse, which film critic Martin Barker termed "new racism". As Leader of the Opposition, Thatcher believed that the National Front was winning over large numbers of Conservative voters with warnings against floods of immigrants. Her strategy was to undermine the Front narrative by acknowledging that many of their voters had serious concerns in need of addressing. In January 1978, Thatcher criticised Labour immigration policy with the goal of attracting voters away from the Front and to the Conservatives. Her rhetoric was followed by an increase in Conservative support at the expense of the Front. Critics on the left reacted in accusing her of pandering to racism. Sociologists Mark Mitchell and Dave Russell responded that Thatcher had been badly misinterpreted, arguing that race was never an important focus of Thatcherism. Throughout her premiership, both major parties took similar positions on immigration policy, having in 1981 passed the British Nationality Act with bipartisan support. There were no policies passed or proposed by her government aimed at restricting immigration, and the subject of race was never highlighted by Thatcher in any of her major speeches as Prime Minister.

Many of Thatcher's policies had an influence on the Labour Party, which had returned to power in 1997 under Tony Blair. Blair rebranded the party "New Labour" in 1994 with an aim of increasing its appeal beyond its traditional supporters, and to attract those who had supported Thatcher, such as the "Essex man". She is said to have regarded New Labour as her greatest achievement.
Shortly after Thatcher's death, Scottish First Minister Alex Salmond argued that her policies had the "unintended consequence" of encouraging Scottish devolution. Lord Foulkes of Cumnock agreed on "Scotland Tonight" that she had provided "the impetus" for devolution. Writing for "The Scotsman", Thatcher argued against devolution on the basis that it would eventually lead to Scottish independence.

Thatcher's tenure of 11 years and 209 days as Prime Minister was the longest since Lord Salisbury (13 years and 252 days, in three spells) and the longest continuous period in office since Lord Liverpool (14 years and 305 days). She remains the longest-serving Prime Minister officially referred to as such, as the post was only officially given recognition in the order of precedence in 1905.

Having led her party to general election victories three times in a row (twice in landslide), she ranks as the most popular party leader in British history in terms of votes cast for the winning party, with over 40 million ballots cast for the Conservatives in total between 1979 and 1987. Her final election win was hailed as a "historic hat trick" by "The Independent" and other newspapers.

Thatcher was voted the fourth-greatest British prime minister of the 20th century in a poll of 139 academics organised by MORI, and in 2002 she ranked highest among living persons in the BBC poll "100 Greatest Britons". In 1999, "Time" deemed Thatcher one of the . In 2015 she topped a poll by "Scottish Widows", a major financial services company, as the most influential woman of the past 200 years; and in 2016 topped BBC Radio 4's "Woman's Hour Power List" of women judged to have had the biggest impact on female lives over the past 70 years.

According to theatre critic Michael Billington, Thatcher left an "emphatic mark" on the arts while Prime Minister. One of the earliest satires of Thatcher as Prime Minister involved satirist John Wells (as writer and performer), actress Janet Brown (voicing Thatcher) and future "Spitting Image" producer John Lloyd (as co-producer), who in 1979 were teamed up by producer Martin Lewis for the satirical audio album "The Iron Lady", which consisted of skits and songs satirising Thatcher's rise to power. The album was released in September 1979.

Thatcher was the subject or the inspiration for 1980s protest songs. Musicians Billy Bragg and Paul Weller helped to form the Red Wedge collective to support Labour in opposition to Thatcher. Known simply as "Maggie" by both supporters and opponents, the chant song "Maggie Out" became a signature rallying cry among the left during the latter half of her premiership.

Thatcher was parodied by Wells in several media. He collaborated with Richard Ingrams on the spoof "Dear Bill" letters, which ran as a column in "Private Eye" magazine; they were also published in book form and became a West End stage revue titled "Anyone for Denis?", with Wells in the role of Denis Thatcher. It was followed by a 1982 TV special directed by Dick Clement, in which Thatcher was played by Angela Thorne.

Since her resignation as Prime Minister in 1990, Thatcher has been portrayed in a number of television programmes, documentaries, films and plays. She was portrayed by Patricia Hodge in Ian Curteis's long unproduced "The Falklands Play" (2002) and by Andrea Riseborough in the TV film "The Long Walk to Finchley" (2008). She is the protagonist in two films, played by Lindsay Duncan in "Margaret" (2009) and by Meryl Streep in "The Iron Lady" (2011), in which she is depicted as suffering from dementia or Alzheimer's disease.

Thatcher became a Privy Councillor (PC) upon becoming Secretary of State for Education and Science in 1970. She was the first woman entitled to full membership rights as an honorary member of the Carlton Club on becoming Leader of the Conservative Party in 1975.

As Prime Minister, Thatcher received two honorary distinctions:

Within two weeks of her resignation, Thatcher was appointed a Member of the Order of Merit (OM) by Queen Elizabeth II in December 1990. Her husband Denis was honoured with a hereditary baronetcy at the same time. As the spouse of a knight, Thatcher was entitled to use the honorific style "Lady", an automatically conferred title that she declined to use. She became Lady Thatcher in her own right in 1992, upon her ennoblement in the House of Lords.

Thatcher was awarded twice in 1991 with the highest civilian awards of the United States and South Africa respectively:

She was appointed a Dame of the Order of St John (DStJ) in July 1991.

In the Falklands, Margaret Thatcher Day has been marked each 10 January since 1992, commemorating her first visit to the Islands in January 1983, six months after the end of the Falklands War in June 1982.

Thatcher became a member of the House of Lords in 1992 with a life peerage as Baroness Thatcher, of Kesteven in the County of Lincolnshire. As a peer, Thatcher was entitled to use a personal coat of arms. A second coat of arms was created for Thatcher following her appointment as a Lady Companion of the Order of the Garter (LG) in 1995, the highest order of chivalry for women. Despite having received her own arms, Thatcher often used the Royal Arms instead of her own contrary to protocol.

In the US, Thatcher received the Ronald Reagan Freedom Award, and was designated Patron of The Heritage Foundation in 2006, where she established the Margaret Thatcher Center for Freedom.




</doc>
<doc id="19833" url="https://en.wikipedia.org/wiki?curid=19833" title="Metastability">
Metastability

In physics, metastability is a stable state of a dynamical system other than the system's state of least energy.
A ball resting in a hollow on a slope is a simple example of metastability. If the ball is only slightly pushed, it will settle back into its hollow, but a stronger push may start the ball rolling down the slope. Bowling pins show similar metastability by either merely wobbling for a moment or tipping over completely. A common example of metastability in science is isomerisation. Higher energy isomers are long lived as they are prevented from rearranging to their preferred ground state by (possibly large) barriers in the potential energy.

During a metastable state of finite lifetime, all state-describing parameters reach and hold stationary values. In isolation:

The metastability concept originated in the physics of first-order phase transitions. It then acquired new meaning in the study of aggregated subatomic particles (in atomic nuclei or in atoms) or in molecules, macromolecules or clusters of atoms and molecules. Later, it was borrowed for the study of decision-making and information transmission systems.

Many complex natural and man-made systems can demonstrate metastability.

Non-equilibrium thermodynamics is a branch of physics that studies the dynamics of statistical ensembles of molecules via unstable states. Being "stuck" in a thermodynamic trough without being at the lowest energy state is known as having kinetic stability or being "kinetically persistent". The particular motion or kinetics of the atoms involved has resulted in getting stuck, despite there being preferable (lower-energy) alternatives.

Metastable states of matter range from melting solids (or freezing liquids), boiling liquids (or condensing gases) and sublimating solids to supercooled liquids or superheated liquid-gas mixtures. Extremely pure, supercooled water stays liquid below 0 °C and remains so until applied vibrations or condensing seed doping initiates crystallization centers. This is a common situation for the droplets of atmospheric clouds.

Metastable phases are common in condensed matter. For example, diamond is a metastable form of carbon at standard temperature and pressure. It can be converted to graphite (plus leftover kinetic energy), but only after overcoming an activation energy – an intervening hill. Martensite is a metastable phase used to control the hardness of most steel. The bonds between the building blocks of polymers such as DNA, RNA, and proteins are also metastable. Adenosine triphosphate is a highly metastable molecule, colloquially described as being "full of energy" that can be used in many ways in biology. Metastable polymorphs of silica are commonly observed. In some cases, such as in the allotropes of solid boron, acquiring a sample of the stable phase is difficult. Generally speaking, emulsions/colloidal systems and glasses are metastable.

Sandpiles are one system which can exhibit metastability if a steep slope or tunnel is present. Sand grains form a pile due to friction. It is possible for an entire large sand pile to reach a point where it is stable, but the addition of a single grain causes large parts of it to collapse.

The avalanche is a well-known problem with large piles of snow and ice crystals on steep slopes. In dry conditions, snow slopes act similarly to sandpiles. An entire mountainside of snow can suddenly slide due to the presence of a skier, or even a loud noise or vibration.

Aggregated systems of subatomic particles described by quantum mechanics (quarks inside nucleons, nucleons inside atomic nuclei, electrons inside atoms, molecules, or atomic clusters) are found to have many distinguishable states. Of these, one (or a small degenerate set) is indefinitely stable: the ground state or global minimum.

All other states besides the ground state (or those degenerate with it) have higher energies. Of all these other states, the metastable states are the ones having lifetimes lasting at least 10 to 10 times longer than the shortest lived states of the set.

A "metastable state" is then long-lived (locally stable with respect to configurations of 'neighbouring' energies) but not eternal (as the global minimum is). Being excited – of an energy above the ground state – it will eventually decay to a more stable state, releasing energy. Indeed, above absolute zero, all states of a system have a non-zero probability to decay; that is, to spontaneously fall into another state (usually lower in energy). One mechanism for this to happen is through tunnelling.

Some energetic states of an atomic nucleus (having distinct spatial mass, charge, spin, isospin distributions) are much longer-lived than other (nuclear isomers of the same isotope), e.g. technetium-99m. The isotope tantalum-180m, although being a metastable excited state, is long-lived enough that it has never been observed to decay, with a half-life calculated to be least years, over 3 million times the current age of the universe.

Some atomic energy levels are metastable. Rydberg atoms are an example of metastable excited atomic states. Transitions from metastable excited levels are typically those forbidden by electric dipole selection rules. This means that any transitions from this level are relatively unlikely to occur. In a sense, an electron that happens to find itself in a metastable configuration is trapped there. Of course, since transitions from a metastable state are not impossible (merely less likely), the electron will eventually decay to a less energetic state, typically by an electric quadrupole transition, or often by non-radiative de-excitation (e.g., collisional de-excitation).

This slow-decay property of a metastable state is apparent in phosphorescence, the kind of photoluminescence seen in glow-in-the-dark toys that can be charged by first being exposed to bright light. Whereas spontaneous emission in atoms has a typical timescale on the order of 10 seconds, the decay of metastable states can typically take milliseconds to minutes, and so light emitted in phosphorescence is usually both weak and long-lasting.

In chemical systems, a system of atoms or molecules involving a change in chemical bond can be in a metastable state, which lasts for a relatively long period of time. Molecular vibrations and thermal motion make chemical species at the energetic equivalent of the top of a round hill very short-lived. Metastable states that persist for many seconds (or years) are found in energetic "valleys" which are not the lowest possible valley (point 1 in illustration). A common type of metastability is isomerism.

The stability or metastability of a given chemical system depends on its environment, particularly temperature and pressure. The difference between producing a stable vs. metastable entity can have important consequences. For instances, having the wrong crystal polymorph can result in failure of a drug while in storage between manufacture and administration. The map of which state is the most stable as a function of pressure, temperature and/or composition is known as a phase diagram. In regions where a particular state is not the most stable, it may still be metastable.

Reaction intermediates are relatively short-lived, and are usually thermodynamically unstable rather than metastable. The IUPAC recommends referring to these as "transient" rather than metastable.

Metastability is also used to refer to specific situations in mass spectrometry and spectrochemistry.

The evolution of a many-body quantum system between its characteristic set of states may be influenced by the following external actions:

Metastability in electronics is usually seen as a problem. A changing circuit is supposed to settle into one of a small number of desired states, but if the circuit is vulnerable to metastability, it can get stuck in an undesirable state.

Metastability in the brain is a phenomenon studied in computational neuroscience to elucidate how the human brain recognizes patterns. Here, the term metastability is used rather loosely. There is no lower-energy state, but there are semi-transient signals in the brain that persist for a while and are different than the usual equilibrium state.



</doc>
<doc id="19834" url="https://en.wikipedia.org/wiki?curid=19834" title="Mary Wollstonecraft">
Mary Wollstonecraft

Mary Wollstonecraft (; 27 April 1759 – 10 September 1797) was an English writer, philosopher, and advocate of women's rights. During her brief career, she wrote novels, treatises, a travel narrative, a history of the French Revolution, a conduct book, and a children's book. Wollstonecraft is best known for "A Vindication of the Rights of Woman" (1792), in which she argues that women are not naturally inferior to men, but appear to be only because they lack education. She suggests that both men and women should be treated as rational beings and imagines a social order founded on reason.

Until the late 20th century, Wollstonecraft's life, which encompassed several unconventional personal relationships, received more attention than her writing. After two ill-fated affairs, with Henry Fuseli and Gilbert Imlay (by whom she had a daughter, Fanny Imlay), Wollstonecraft married the philosopher William Godwin, one of the forefathers of the anarchist movement. Wollstonecraft died at the age of 38, eleven days after giving birth to her second daughter, leaving behind several unfinished manuscripts. This daughter, Mary Wollstonecraft Godwin, became an accomplished writer herself, as Mary Shelley, whose best known work was "Frankenstein".

After Wollstonecraft's death, her widower published a "Memoir" (1798) of her life, revealing her unorthodox lifestyle, which inadvertently destroyed her reputation for almost a century. However, with the emergence of the feminist movement at the turn of the twentieth century, Wollstonecraft's advocacy of women's equality and critiques of conventional femininity became increasingly important. Today Wollstonecraft is regarded as one of the founding feminist philosophers, and feminists often cite both her life and work as important influences.

Wollstonecraft was born on 27 April 1759 in Spitalfields, London. She was the second of the seven children of Elizabeth Dixon and Edward John Wollstonecraft. Although her family had a comfortable income when she was a child, her father gradually squandered it on speculative projects. Consequently, the family became financially unstable and they were frequently forced to move during Wollstonecraft's youth. The family's financial situation eventually became so dire that Wollstonecraft's father compelled her to turn over money that she would have inherited at her maturity. Moreover, he was apparently a violent man who would beat his wife in drunken rages. As a teenager, Wollstonecraft used to lie outside the door of her mother's bedroom to protect her. Wollstonecraft played a similar maternal role for her sisters, Everina and Eliza, throughout her life. For example, in a defining moment in 1784, she convinced Eliza, who was suffering from what was probably postpartum depression, to leave her husband and infant; Wollstonecraft made all of the arrangements for Eliza to flee, demonstrating her willingness to challenge social norms. The human costs, however, were severe: her sister suffered social condemnation and, because she could not remarry, was doomed to a life of poverty and hard work.

Two friendships shaped Wollstonecraft's early life. The first was with Jane Arden in Beverley. The two frequently read books together and attended lectures presented by Arden's father, a self-styled philosopher and scientist. Wollstonecraft revelled in the intellectual atmosphere of the Arden household and valued her friendship with Arden greatly, sometimes to the point of being emotionally possessive. Wollstonecraft wrote to her: "I have formed romantic notions of friendship ... I am a little singular in my thoughts of love and friendship; I must have the first place or none." In some of Wollstonecraft's letters to Arden, she reveals the volatile and depressive emotions that would haunt her throughout her life. The second and more important friendship was with Fanny (Frances) Blood, introduced to Wollstonecraft by the Clares, a couple in Hoxton who became parental figures to her; Wollstonecraft credited Blood with opening her mind.

Unhappy with her home life, Wollstonecraft struck out on her own in 1778 and accepted a job as a lady's companion to Sarah Dawson, a widow living in Bath. However, Wollstonecraft had trouble getting along with the irascible woman (an experience she drew on when describing the drawbacks of such a position in "Thoughts on the Education of Daughters", 1787). In 1780 she returned home, called back to care for her dying mother. Rather than return to Dawson's employ after the death of her mother, Wollstonecraft moved in with the Bloods. She realized during the two years she spent with the family that she had idealized Blood, who was more invested in traditional feminine values than was Wollstonecraft. But Wollstonecraft remained dedicated to her and her family throughout her life (she frequently gave pecuniary assistance to Blood's brother, for example).

Wollstonecraft had envisioned living in a female utopia with Blood; they made plans to rent rooms together and support each other emotionally and financially, but this dream collapsed under economic realities. In order to make a living, Wollstonecraft, her sisters, and Blood set up a school together in Newington Green, a Dissenting community. Blood soon became engaged and after their marriage her husband, Hugh Skeys, took her to Lisbon, Portugal, to improve her health, which had always been precarious. Despite the change of surroundings Blood's health further deteriorated when she became pregnant, and in 1785 Wollstonecraft left the school and followed Blood to nurse her, but to no avail. Moreover, her abandonment of the school led to its failure. Blood's death devastated Wollstonecraft and was part of the inspiration for her first novel, "" (1788).

After Blood's death, Wollstonecraft's friends helped her obtain a position as governess to the daughters of the Anglo-Irish Kingsborough family in Ireland. Although she could not get along with Lady Kingsborough, the children found her an inspiring instructor; Margaret King would later say she "had freed her mind from all superstitions". Some of Wollstonecraft's experiences during this year would make their way into her only children's book, "Original Stories from Real Life" (1788).

Frustrated by the limited career options open to respectable yet poor women—an impediment which Wollstonecraft eloquently describes in the chapter of "Thoughts on the Education of Daughters" entitled "Unfortunate Situation of Females, Fashionably Educated, and Left Without a Fortune"—she decided, after only a year as a governess, to embark upon a career as an author. This was a radical choice, since, at the time, few women could support themselves by writing. As she wrote to her sister Everina in 1787, she was trying to become "the first of a new genus". She moved to London and, assisted by the liberal publisher Joseph Johnson, found a place to live and work to support herself. She learned French and German and translated texts, most notably "Of the Importance of Religious Opinions" by Jacques Necker and "Elements of Morality, for the Use of Children" by Christian Gotthilf Salzmann. She also wrote reviews, primarily of novels, for Johnson's periodical, the "Analytical Review". Wollstonecraft's intellectual universe expanded during this time, not only from the reading that she did for her reviews but also from the company she kept: she attended Johnson's famous dinners and met such luminaries as the radical pamphleteer Thomas Paine and the philosopher William Godwin. The first time Godwin and Wollstonecraft met, they were both disappointed in each other. Godwin had come to hear Paine, but Wollstonecraft assailed him all night long, disagreeing with him on nearly every subject. Johnson himself, however, became much more than a friend; she described him in her letters as a father and a brother.

While in London, Wollstonecraft pursued a relationship with the artist Henry Fuseli, even though he was already married. She was, she wrote, enraptured by his genius, "the grandeur of his soul, that quickness of comprehension, and lovely sympathy". She proposed a platonic living arrangement with Fuseli and his wife, but Fuseli's wife was appalled, and he broke off the relationship with Wollstonecraft. After Fuseli's rejection, Wollstonecraft decided to travel to France to escape the humiliation of the incident, and to participate in the revolutionary events that she had just celebrated in her recent "Vindication of the Rights of Men" (1790). She had written the "Rights of Men" in response to the Whig MP Edmund Burke's conservative critique of the French Revolution in "Reflections on the Revolution in France" (1790) and it made her famous overnight. "Reflections on the Revolution in France" was published on 1 November 1790, which so angered Wollstonecraft that she spent the rest of the month writing her rebuttal, and "The Vindication of the Rights of Man, in a Letter to the Right Honorable Edmund Burke" was published on 29 November 1790, initially anonymously. The second edition of "The Vindication of the Rights of Man" was published on 18 December, and this time the publisher revealed Wollstonecraft as the author.

Wollstonecraft called the French Revolution a "glorious "chance" to obtain more virtue and happiness than hitherto blessed our globe". Against Burke's dismissal of the Third Estate as men of no account, Wollstonecraft wrote: "Time many show, that this obscure throng knew more of the human heart and of legislation than the profligates of rank, emasculated by hereditary effeminacy". About the events of 5–6 October 1789, when the royal family was marched from Versailles to Paris by a group of angry housewives, Burke praised Queen Marie Antoinette as a symbol of the refined elegance of the "ancien régime", who was surrounded by "furies from hell, in the abused shape of the vilest of women". Wollstonecraft by contrast wrote of the same event: "Probably you [Burke] mean women who gained a livelihood by selling vegetables or fish, who never had any advantages of education".

Wollstonecraft was compared with such leading lights as the theologian and controversialist Joseph Priestley and Paine, whose "Rights of Man" (1791) would prove to be the most popular of the responses to Burke. She pursued the ideas she had outlined in "Rights of Men" in "A Vindication of the Rights of Woman" (1792), her most famous and influential work. Wollstonecraft's fame extended across the English channel, for when the French statesmen Charles Maurice de Talleyrand-Périgord visited London in 1792, he visited her, during which she asked that French girls be given the same right to an education that French boys were being offered by the new regime in France.

Wollstonecraft left for Paris in December 1792 and arrived about a month before Louis XVI was guillotined. Britain and France were on the brink of war when she left for Paris, and many advised her not to go. France was in turmoil. She sought out other British visitors such as Helen Maria Williams and joined the circle of expatriates then in the city. On 26 December 1792, Wollstonecraft saw the former king, Louis XVI, being taken to be tried before the National Assembly, and much to her own surprise, found the sight of Louis riding down the streets as a prisoner in a wagon "made the tears flow insensibly from my eyes, when I saw Louis sitting, with more dignity than I expected from his character, in a hackney coach going to meet death, where so many of his race have triumphed". During her time in Paris, Wollstonecraft associated mostly with the moderate Girondins rather than the more radical Jacobins.

In February 1793, France declared war on Britain, and Wollstonecraft attempted to leave France for Switzerland, but was declined permission. In March 1793, the Jacobin-dominated Committee of Public Safety came to power in France, instituting a totalitarian regime meant to mobilise France for the first "total war", and life become very difficult for foreigners in France. All foreigners living in France were put under police surveillance, had to produce six written statements from Frenchmen testifying to their loyalty to the republic in order to be granted residency permits, and on 12 April 1793 all foreigners were forbidden to leave France. Despite her sympathy for the revolution, life for Wollstonecraft become very uncomfortable, all the more so as the Girondins had lost out to the Jacobins. Some of Wollstonecraft's French friends lost their heads to the guillotine as the Jacobins set out to annihilate all of their enemies.

Having just written the "Rights of Woman", Wollstonecraft was determined to put her ideas to the test, and in the stimulating intellectual atmosphere of the French revolution she attempted her most experimental romantic attachment yet: she met and fell passionately in love with Gilbert Imlay, an American adventurer. Wollstonecraft put her own principles in practice by sleeping with Imlay despite not being married, which was not something that was considered acceptable behavior from a "respectable" British woman at the time. Whether or not she was interested in marriage, he was not, and she appears to have fallen in love with an idealized portrait of the man. While Wollstonecraft had rejected the sexual component of relationships in the "Rights of Woman", Imlay awakened her passions and her interest in sex. Wollstonecraft was to a certain extent disillusioned by what she saw in France, writing that the people under the republic were still behaved slavishly to those who held power while the government remained "venal" and "brutal". Despite her disenchantment, Wollstonecraft wrote: "I cannot yet give up the hope, that a fairer day is dawning on Europe, though I must hesitatingly observe, that little is to be expected from the narrow principle of commerce, which seems everywhere to be shoving aside "the principle of honor" of the "noblesse". For the same pride of office, the same desire of power are still visible; with this aggravation, that, fearing to return to obscurity, after having but just acquired a relish for distinction, each hero, or philosopher, for all are dubbed with these new titles, endeavors to make hay while the sun shines." Wollstonecraft was offended by the way that the Jacobins refused to grant Frenchwomen equal rights, denounced "Amazons", and made it clear the role of women was to conform to Rousseau's ideal of as a helper to men. On 16 October 1793, Marie Antoinette was guillotined with one of the charges that she been convicted of being she had committed incest with her son. Though Wollstonecraft disliked the former queen, she was troubled by the way that the Jacobins had made Marie Antoinette's alleged perverted sexuality that had caused her to engage in an incestuous relationship with the Dauphin one of the central reasons for why the French people should hate her. On 31 October 1793, most of the Girondin leaders were guillotined, which caused Wollstonecraft to faint when Imlay broke the news to her.

To protect Wollstonecraft, Imlay registered her as his wife in 1793, even though they were not married. After declaring war on Britain, France was blockaded by the Royal Navy, which caused shortages that worsened the problem of inflation. Imlay engaged in blockade-running, chartering ships to bring in food and soap from America into France, which explained why both he and Wollstonecraft were not arrested during the Reign of Terror. As the Terror began in France with arrests and executions occurring daily, Wollstonecraft came under suspicion as someone from a nation that was at war with France and who was known to be a friend of leading Girondins, which led Imlay to make a false statement to the U.S. embassy in Paris that he had married her, automatically making her into an American citizen, in order to protect her from arrest. Some of her friends were not so lucky; many, like Thomas Paine, were arrested, and some were even guillotined (Wollstonecraft's sisters believed she had been imprisoned). Wollstonecraft called life under the Jacobins "nightmarish" with gigantic parades in the day where everyone had to cheer, lest they fall under suspicion of not being committed to the republic, and police raids at night to arrest "enemies of the republic". In a letter to her sister Everina, written in March 1794, Wollstonecraft wrote: It is impossible for you to have any idea of the impression the sad scenes I have been a witness to have left on my mind...death and misery, in every shape of terrour, haunts this devoted country – I certainly am glad that I came to France, because I never could have had else a just opinion of the most extraordinary event that has ever been recorded.

Wollstonecraft soon became pregnant by Imlay, and on 14 May 1794 she gave birth to her first child, Fanny, naming her after perhaps her closest friend. Wollstonecraft was overjoyed; she wrote to a friend: "My little Girl begins to suck so MANFULLY that her father reckons saucily on her writing the second part of the R[igh]ts of Woman" (emphasis hers). She continued to write avidly, despite not only her pregnancy and the burdens of being a new mother alone in a foreign country, but also the growing tumult of the French Revolution. While at Le Havre in northern France, she wrote a history of the early revolution, "An Historical and Moral View of the French Revolution", which was published in London in December 1794. Imlay, unhappy with the domestic-minded and maternal Wollstonecraft, eventually left her. He promised that he would return to Le Havre where she went to give birth to her child, but his delays in writing to her and his long absences convinced Wollstonecraft that he had found another woman. Her letters to him are full of needy expostulations, explained by most critics as the expressions of a deeply depressed woman but by some as a result of her circumstances—alone with an infant in the middle of a revolution.

In July 1794, Wollstonecraft welcomed the fall of the Jacobins, predicting this would be followed with a restoration of freedom of press in France, which led her to return to Paris. In August 1794, Imlay departed for London and promised to return soon. The winter of 1794–95 was the coldest winter in Europe for over a century, which reduced Wollstonecraft and her daughter Fanny down to desperate circumstances. The river Seine froze that winter, which made it impossible for ships to bring food and coal to Paris, leading to widespread starvation and deaths from the cold in that city. Wollstonecraft continued to write to Imlay, asking him to return to France at once, declaring she still had faith in the revolution and did not wish to return to Britain. In 1793, the British government began a crackdown on radicals, suspending civil liberties, imposing drastic censorship, and trying for treason all suspected of sympathy with the revolution, which led Wollstonecraft to fear she would be imprisoned if she returned. After she left France on 7 April 1795, she continued to refer to herself as "Mrs Imlay", even to her sisters, in order to bestow legitimacy upon her child.

The British historian Tom Furniss called "An Historical and Moral View of the French Revolution" the most neglected of Wollstonecraft's books, which was first published in London in 1794, and a second edition did not appear until 1989. The reason for this neglect is later generations were more interested in her feminist writings rather her account of the French Revolution, which Furniss regretted as he called "An Historical and Moral View of the French Revolution" easily Wollstonecraft's "best work". Wollstonecraft was not trained as a historian, but she used all sorts of journals, letters and documents recounting how ordinary people in France reacted to the Revolution, and attempted to counter-act what Furniss called the "hysterical" anti-revolutionary mood in Britain, which depicted the Revolution as due to the entire French nation apparently all going mad. Wollstonecraft argued that the revolution was not due to the French people all going insane in 1789 as popular opinion in Britain held, but was due to a set of social, economic and political conditions that left no other way out of the crisis that gripped France in 1789. "An Historical and Moral View of the French Revolution" presented a difficult balancing act for Wollstonecraft as she condemned the Jacobin regime and the Reign of Terror, but at same time, she continued to argue that revolution was a great achievement, which led her to stop her history in late 1789 rather than write about the Terror of 1793–94. Burke had ended the "Reflections" with reference to the events of 5–6 October 1789, when a group of women from Paris forced the French royal family from the Palace of Versailles to Paris. Burke called the women "furies from hell" while Wollstonecraft called the women just merely ordinary housewives angry about a lack of bread to feed their families. Against Burke's idealised portrait of Marie Antoinette as a noble victim of a mob, Wollstonecraft portrayed the queen as a "femme fatale", a seductive, scheming and dangerous woman. Wollstonecraft argued that the values of the aristocracy were corrupting ones for women since in a monarchy, the main purpose of a woman was to bear sons to continue the house, which essentially reduced a woman's value down to her womb. Moreover, Wollstonecraft argued unless a queen was a queen regnant, most queens were queen consorts, which meant a woman to exercise influence via her husband or son, which encouraged manipulative behavior. Wollstonecraft argued that aristocratic values by emphasising the value of a woman's body and her ability to be charming over the value of her mind and character had encouraged women like Marie Antoinette to be manipulative and ruthless, making the queen into a corrupted and corrupting product of the "ancien régime".

Seeking Imlay, Wollstonecraft returned to London in April 1795, but he rejected her. In May 1795 she attempted to commit suicide, probably with laudanum, but Imlay saved her life (although it is unclear how). In a last attempt to win back Imlay, she embarked upon some business negotiations for him in Scandinavia, trying to recoup some of his losses. Wollstonecraft undertook this hazardous trip with only her young daughter and a maid. She recounted her travels and thoughts in letters to Imlay, many of which were eventually published as "Letters Written During a Short Residence in Sweden, Norway, and Denmark" in 1796. When she returned to England and came to the full realization that her relationship with Imlay was over, she attempted suicide for the second time, leaving a note for Imlay:

She then went out on a rainy night and "to make her clothes heavy with water, she walked up and down about half an hour" before jumping into the River Thames, but a stranger saw her jump and rescued her. Wollstonecraft considered her suicide attempt deeply rational, writing after her rescue, I have only to lament, that, when the bitterness of death was past, I was inhumanly brought back to life and misery. But a fixed determination is not to be baffled by disappointment; nor will I allow that to be a frantic attempt, which was one of the calmest acts of reason. In this respect, I am only accountable to myself. Did I care for what is termed reputation, it is by other circumstances that I should be dishonoured.

Gradually, Wollstonecraft returned to her literary life, becoming involved with Joseph Johnson's circle again, in particular with Mary Hays, Elizabeth Inchbald, and Sarah Siddons through William Godwin. Godwin and Wollstonecraft's unique courtship began slowly, but it eventually became a passionate love affair. Godwin had read her "Letters Written in Sweden, Norway, and Denmark" and later wrote that "If ever there was a book calculated to make a man in love with its author, this appears to me to be the book. She speaks of her sorrows, in a way that fills us with melancholy, and dissolves us in tenderness, at the same time that she displays a genius which commands all our admiration." Once Wollstonecraft became pregnant, they decided to marry so that their child would be legitimate. Their marriage revealed the fact that Wollstonecraft had never been married to Imlay, and as a result she and Godwin lost many friends. Godwin received further criticism because he had advocated the abolition of marriage in his philosophical treatise "Political Justice". After their marriage on 29 March 1797, Godwin and Wollstonecraft moved to 29 The Polygon, Somers Town. Godwin rented an apartment 20 doors away at 17 Evesham Buildings in Chalton Street as a study, so that they could both still retain their independence; they often communicated by letter. By all accounts, theirs was a happy and stable, though brief, relationship.

On 30 August 1797, Wollstonecraft gave birth to her second daughter, Mary. Although the delivery seemed to go well initially, the placenta broke apart during the birth and became infected; puerperal (childbed) fever was a common and often fatal occurrence in the eighteenth century. After several days of agony, Wollstonecraft died of septicaemia on 10 September. Godwin was devastated: he wrote to his friend Thomas Holcroft, "I firmly believe there does not exist her equal in the world. I know from experience we were formed to make each other happy. I have not the least expectation that I can now ever know happiness again." She was buried at Old Saint Pancras Churchyard, where her tombstone reads, "Mary Wollstonecraft Godwin, Author of "A Vindication of the Rights of Woman": Born 27 April 1759: Died 10 September 1797." (In 1851, her remains were moved by her grandson Percy Florence Shelley to his family tomb in St Peter's Church, Bournemouth.) Her monument in the churchyard lies to the north-east of the church just north of Sir John Soane's grave. Her husband was buried with her on his death in 1836, as was his second wife, Mary Jane Godwin (1766–1841).

In January 1798 Godwin published his "Memoirs of the Author of A Vindication of the Rights of Woman". Although Godwin felt that he was portraying his wife with love, compassion, and sincerity, many readers were shocked that he would reveal Wollstonecraft's illegitimate children, love affairs, and suicide attempts. The Romantic poet Robert Southey accused him of "the want of all feeling in stripping his dead wife naked" and vicious satires such as "The Unsex'd Females" were published. Godwin's "Memoirs" portrays Wollstonecraft as a woman deeply invested in feeling who was balanced by his reason and as more of a religious sceptic than her own writings suggest. Godwin's views of Wollstonecraft were perpetuated throughout the nineteenth century and resulted in poems such as "Wollstonecraft and Fuseli" by British poet Robert Browning and that by William Roscoe which includes the lines:
<poem>
</poem>

Wollstonecraft has what scholar Cora Kaplan labelled in 2002 a "curious" legacy that has evolved over time: "for an author-activist adept in many genres ... up until the last quarter-century Wollstonecraft's life has been read much more closely than her writing". After the devastating effect of Godwin's "Memoirs", Wollstonecraft's reputation lay in tatters for nearly a century; she was pilloried by such writers as Maria Edgeworth, who patterned the "freakish" Harriet Freke in "Belinda" (1801) after her. Other novelists such as Mary Hays, Charlotte Turner Smith, Fanny Burney, and Jane West created similar figures, all to teach a "moral lesson" to their readers. (Hays had been a close friend, and helped nurse her in her dying days.) 

In contrast, there was one writer of the generation after Wollstonecraft who apparently did not share the judgmental views of her contemporaries. Jane Austen never mentioned the earlier woman by name, but several of her novels contain positive allusions to Wollstonecraft’s work. The American literary scholar Anne K. Mellor notes several examples. In "Pride and Prejudice", Mr Wickham seems to be based upon the sort of man Wollstonecraft claimed that standing armies produce, while the sarcastic remarks of protagonist Elizabeth Bennet about "female accomplishments" closely echo Wollstonecraft's condemnation of these activities. The balance a woman must strike between feelings and reason in "Sense and Sensibility" follows what Wollstonecraft recommended in her novel "Mary", while the moral equivalence Austen drew in "Mansfield Park" between slavery and the treatment of women in British society tracks one of Wollstonecraft's favorite arguments. In "Persuasion", Austen's characterization of Anne Eliot (as well as her late mother before her) as better qualified than her father to manage the family estate also echoes a Wollstonecraft thesis.

Scholar Virginia Sapiro states that few read Wollstonecraft's works during the nineteenth century as "her attackers implied or stated that no self-respecting woman would read her work". (Still, as Craciun points out, new editions of "Rights of Woman" appeared in the UK in the 1840s and in the US in the 1830s, 1840s, and 1850s.) If readers were few, then "many" were inspired; one such reader was Elizabeth Barrett Browning, who read "Rights of Woman" at age 12 and whose poem "Aurora Leigh" reflected Wollstonecraft's unwavering focus on education. Lucretia Mott, a Quaker minister, and Elizabeth Cady Stanton, Americans who met in 1840 at the World Anti-Slavery Convention in London, discovered they both had read Wollstonecraft, and they agreed upon the need for (what became) the Seneca Falls Convention, an influential women's rights meeting held in 1848. Another woman who read Wollstonecraft was George Eliot, a prolific writer of reviews, articles, novels, and translations. In 1855, she devoted an essay to the roles and rights of women, comparing Wollstonecraft and Margaret Fuller. Fuller was an American journalist, critic, and women's rights activist who, like Wollstonecraft, had traveled to the Continent and had been involved in the struggle for reform (in this case the Roman Republic)—and she had a child by a man without marrying him. Wollstonecraft's children's tales were adapted by Charlotte Mary Yonge in 1870.

Wollstonecraft's work was exhumed with the rise of the movement to give women a political voice. First was an attempt at rehabilitation in 1879 with the publication of Wollstonecraft's "Letters to Imlay, with prefatory memoir" by Charles Kegan Paul. Then followed the first full-length biography, which was by Elizabeth Robins Pennell; it appeared in 1884 as part of a series by the Roberts Brothers on famous women. Millicent Garrett Fawcett, a suffragist and later president of the National Union of Women's Suffrage Societies, wrote the introduction to the centenary edition (i.e. 1892) of the "Rights of Woman"; it cleansed the memory of Wollstonecraft and claimed her as the foremother of the struggle for the vote. By 1898, Wollstonecraft was the subject of a first doctoral thesis and its resulting book.

With the advent of the modern feminist movement, women as politically dissimilar from each other as Virginia Woolf and Emma Goldman embraced Wollstonecraft's life story. By 1929 Woolf described Wollstonecraft—her writing, arguments, and "experiments in living"—as immortal: "she is alive and active, she argues and experiments, we hear her voice and trace her influence even now among the living". Others, however, continued to decry Wollstonecraft's lifestyle. A biography published in 1932 refers to recent reprints of her works, incorporating new research, and to a "study" in 1911, a play in 1922, and another biography in 1924. Interest in her never completely died, with full-length biographies in 1937 and 1951

With the emergence of feminist criticism in academia in the 1960s and 1970s, Wollstonecraft's works returned to prominence. Their fortunes reflected that of the second wave of the North American feminist movement itself; for example, in the early 1970s, six major biographies of Wollstonecraft were published that presented her "passionate life in apposition to [her] radical and rationalist agenda". The feminist artwork "The Dinner Party", first exhibited in 1979, features a place setting for Wollstonecraft. In the 1980s and 1990s, yet another image of Wollstonecraft emerged, one which described her as much more a creature of her time; scholars such as Claudia Johnson, Gary Kelly, and Virginia Sapiro demonstrated the continuity between Wollstonecraft's thought and other important eighteenth-century ideas regarding topics such as sensibility, economics, and political theory.

Wollstonecraft's work has also had an effect on feminism outside the academy in recent years. Ayaan Hirsi Ali, a political writer and former Muslim who is critical of Islam in general and its dictates regarding women in particular, cited the "Rights of Woman" in her autobiography "Infidel" and wrote that she was "inspired by Mary Wollstonecraft, the pioneering feminist thinker who told women they had the same ability to reason as men did and deserved the same rights". British writer Caitlin Moran, author of the best-selling "How to Be a Woman", described herself as "half Wollstonecraft" to the "New Yorker". She has also inspired more widely. Nobel Laureate Amartya Sen, the Indian economist and philosopher who first identified the missing women of Asia, draws repeatedly on Wollstonecraft as a political philosopher in "The Idea of Justice".

Several plaques have been erected to honor Wollstonecraft.

The majority of Wollstonecraft's early productions are about education; she assembled an anthology of literary extracts "for the improvement of young women" entitled "The Female Reader" and she translated two children's works, Maria Geertruida van de Werken de Cambon's "Young Grandison" and Christian Gotthilf Salzmann's "Elements of Morality". Her own writings also addressed the topic. In both her conduct book "Thoughts on the Education of Daughters" (1787) and her children's book "Original Stories from Real Life" (1788), Wollstonecraft advocates educating children into the emerging middle-class ethos: self-discipline, honesty, frugality, and social contentment. Both books also emphasize the importance of teaching children to reason, revealing Wollstonecraft's intellectual debt to the important seventeenth-century educational philosopher John Locke. However, the prominence she affords religious faith and innate feeling distinguishes her work from his and links it to the discourse of sensibility popular at the end of the eighteenth century. Both texts also advocate the education of women, a controversial topic at the time and one which she would return to throughout her career, most notably in "A Vindication of the Rights of Woman". Wollstonecraft argues that well-educated women will be good wives and mothers and ultimately contribute positively to the nation.

Published in response to Edmund Burke's "Reflections on the Revolution in France" (1790), which was a defence of constitutional monarchy, aristocracy, and the Church of England, and an attack on Wollstonecraft's friend, the Rev Richard Price at the Newington Green Unitarian Church, Wollstonecraft's "A Vindication of the Rights of Men" (1790) attacks aristocracy and advocates republicanism. Hers was the first response in a pamphlet war that subsequently became known as the "Revolution Controversy", in which Thomas Paine's "Rights of Man" (1792) became the rallying cry for reformers and radicals.

Wollstonecraft attacked not only monarchy and hereditary privilege but also the language that Burke used to defend and elevate it. In a famous passage in the "Reflections", Burke had lamented: "I had thought ten thousand swords must have leaped from their scabbards to avenge even a look that threatened her <nowiki>[</nowiki>Marie Antoinette<nowiki>]</nowiki> with insult.—But the age of chivalry is gone." Most of Burke's detractors deplored what they viewed as theatrical pity for the French queen—a pity they felt was at the expense of the people. Wollstonecraft was unique in her attack on Burke's gendered language. By redefining the sublime and the beautiful, terms first established by Burke himself in "A Philosophical Enquiry into the Origin of Our Ideas of the Sublime and Beautiful" (1756), she undermined his rhetoric as well as his argument. Burke had associated the beautiful with weakness and femininity and the sublime with strength and masculinity; Wollstonecraft turns these definitions against him, arguing that his theatrical "tableaux" turn Burke's readers—the citizens—into weak women who are swayed by show. In her first unabashedly feminist critique, which Wollstonecraft scholar Claudia L. Johnson argues remains unsurpassed in its argumentative force, Wollstonecraft indicts Burke's defence of an unequal society founded on the passivity of women.

In her arguments for republican virtue, Wollstonecraft invokes an emerging middle-class ethos in opposition to what she views as the vice-ridden aristocratic code of manners. Influenced by Enlightenment thinkers, she believed in progress and derides Burke for relying on tradition and custom. She argues for rationality, pointing out that Burke's system would lead to the continuation of slavery, simply because it had been an ancestral tradition. She describes an idyllic country life in which each family can have a farm that will just suit its needs. Wollstonecraft contrasts her utopian picture of society, drawn with what she says is genuine feeling, to Burke's false feeling.

The "Rights of Men" was Wollstonecraft's first overtly political work, as well as her first feminist work; as Johnson contends, "it seems that in the act of writing the later portions of "Rights of Men" she discovered the subject that would preoccupy her for the rest of her career." It was this text that made her a well-known writer.

"A Vindication of the Rights of Woman" is one of the earliest works of feminist philosophy. In it, Wollstonecraft argues that women ought to have an education commensurate with their position in society and then proceeds to redefine that position, claiming that women are essential to the nation because they educate its children and because they could be "companions" to their husbands rather than mere wives. Instead of viewing women as ornaments to society or property to be traded in marriage, Wollstonecraft maintains that they are human beings deserving of the same fundamental rights as men. Large sections of the "Rights of Woman" respond vitriolically to conduct book writers such as James Fordyce and John Gregory and educational philosophers such as Jean-Jacques Rousseau, who wanted to deny women an education. (Rousseau famously argues in "" (1762) that women should be educated for the pleasure of men.)

Wollstonecraft states that currently many women are silly and superficial (she refers to them, for example, as "spaniels" and "toys"), but argues that this is not because of an innate deficiency of mind but rather because men have denied them access to education. Wollstonecraft is intent on illustrating the limitations that women's deficient educations have placed on them; she writes: "Taught from their infancy that beauty is woman's sceptre, the mind shapes itself to the body, and, roaming round its gilt cage, only seeks to adorn its prison." She implies that, without the encouragement young women receive from an early age to focus their attention on beauty and outward accomplishments, women could achieve much more.

While Wollstonecraft does call for equality between the sexes in particular areas of life, such as morality, she does not explicitly state that men and women are equal. What she does claim is that men and women are equal in the eyes of God. However, such claims of equality stand in contrast to her statements respecting the superiority of masculine strength and valour. Wollstonecraft famously and ambiguously writes: "Let it not be concluded that I wish to invert the order of things; I have already granted, that, from the constitution of their bodies, men seem to be designed by Providence to attain a greater degree of virtue. I speak collectively of the whole sex; but I see not the shadow of a reason to conclude that their virtues should differ in respect to their nature. In fact, how can they, if virtue has only one eternal standard? I must therefore, if I reason consequently, as strenuously maintain that they have the same simple direction, as that there is a God." Her ambiguous statements regarding the equality of the sexes have since made it difficult to classify Wollstonecraft as a modern feminist, particularly since the word and the concept were unavailable to her.

One of Wollstonecraft's most scathing critiques in the "Rights of Woman" is of false and excessive sensibility, particularly in women. She argues that women who succumb to sensibility are "blown about by every momentary gust of feeling" and because they are "the prey of their senses" they cannot think rationally. In fact, she claims, they do harm not only to themselves but to the entire civilization: these are not women who can help refine a civilization—a popular eighteenth-century idea—but women who will destroy it. Wollstonecraft does not argue that reason and feeling should act independently of each other; rather, she believes that they should inform each other.

In addition to her larger philosophical arguments, Wollstonecraft also lays out a specific educational plan. In the twelfth chapter of the "Rights of Woman", "On National Education", she argues that all children should be sent to a "country day school" as well as given some education at home "to inspire a love of home and domestic pleasures." She also maintains that schooling should be co-educational, arguing that men and women, whose marriages are "the cement of society", should be "educated after the same model."

Wollstonecraft addresses her text to the middle-class, which she describes as the "most natural state", and in many ways the "Rights of Woman" is inflected by a bourgeois view of the world. It encourages modesty and industry in its readers and attacks the uselessness of the aristocracy. But Wollstonecraft is not necessarily a friend to the poor; for example, in her national plan for education, she suggests that, after the age of nine, the poor, except for those who are brilliant, should be separated from the rich and taught in another school.

Both of Wollstonecraft's novels criticize what she viewed as the patriarchal institution of marriage and its deleterious effects on women. In her first novel, "Mary: A Fiction" (1788), the eponymous heroine is forced into a loveless marriage for economic reasons; she fulfils her desire for love and affection outside of marriage with two passionate romantic friendships, one with a woman and one with a man. "Maria: or, The Wrongs of Woman" (1798), an unfinished novel published posthumously and often considered Wollstonecraft's most radical feminist work, revolves around the story of a woman imprisoned in an insane asylum by her husband; like Mary, Maria also finds fulfilment outside of marriage, in an affair with a fellow inmate and a friendship with one of her keepers. Neither of Wollstonecraft's novels depict successful marriages, although she posits such relationships in the "Rights of Woman". At the end of "Mary", the heroine believes she is going "to that world where there is neither marrying, nor giving in marriage", presumably a positive state of affairs.

Both of Wollstonecraft's novels also critique the discourse of sensibility, a moral philosophy and aesthetic that had become popular at the end of the eighteenth century. "Mary" is itself a novel of sensibility and Wollstonecraft attempts to use the tropes of that genre to undermine sentimentalism itself, a philosophy she believed was damaging to women because it encouraged them to rely overmuch on their emotions. In "The Wrongs of Woman" the heroine's indulgence on romantic fantasies fostered by novels themselves is depicted as particularly detrimental.

Female friendships are central to both of Wollstonecraft's novels, but it is the friendship between Maria and Jemima, the servant charged with watching over her in the insane asylum, that is the most historically significant. This friendship, based on a sympathetic bond of motherhood, between an upper-class woman and a lower-class woman is one of the first moments in the history of feminist literature that hints at a cross-class argument, that is, that women of different economic positions have the same interests because they are women.

Wollstonecraft's "Letters Written in Sweden, Norway, and Denmark" is a deeply personal travel narrative. The twenty-five letters cover a wide range of topics, from sociological reflections on Scandinavia and its peoples to philosophical questions regarding identity to musings on her relationship with Imlay (although he is not referred to by name in the text). Using the rhetoric of the sublime, Wollstonecraft explores the relationship between the self and society. Reflecting the strong influence of Rousseau, "Letters Written in Sweden, Norway, and Denmark" shares the themes of the French philosopher's "Reveries of a Solitary Walker" (1782): "the search for the source of human happiness, the stoic rejection of material goods, the ecstatic embrace of nature, and the essential role of sentiment in understanding". While Rousseau ultimately rejects society, however, Wollstonecraft celebrates domestic scenes and industrial progress in her text.
Wollstonecraft promotes subjective experience, particularly in relation to nature, exploring the connections between the sublime and sensibility. Many of the letters describe the breathtaking scenery of Scandinavia and Wollstonecraft's desire to create an emotional connection to that natural world. In so doing, she gives greater value to the imagination than she had in previous works. As in her previous writings, she champions the liberation and education of women. In a change from her earlier works, however, she illustrates the detrimental effects of commerce on society, contrasting the imaginative connection to the world with a commercial and mercenary one, an attitude she associates with Imlay.

"Letters Written in Sweden, Norway, and Denmark" was Wollstonecraft's most popular book in the 1790s. It sold well and was reviewed positively by most critics. Godwin wrote "if ever there was a book calculated to make a man in love with its author, this appears to me to be the book." It influenced Romantic poets such as William Wordsworth and Samuel Taylor Coleridge, who drew on its themes and its aesthetic.
This is a complete list of Mary Wollstonecraft's works; all works are the first edition and were authored by Wollstonecraft unless otherwise noted.







</doc>
<doc id="19836" url="https://en.wikipedia.org/wiki?curid=19836" title="Molecular mass">
Molecular mass

Relative Molecular mass or molecular weight is the mass of a molecule. It is calculated as the sum of the atomic weights of each constituent element multiplied by the number of atoms of that element in the molecular formula. The molecular mass of small to medium size molecules, measured by mass spectrometry, determines stoichiometry. For large molecules such as proteins, methods based on viscosity and light-scattering can be used to determine molecular mass when crystallographic data are not available.

Both atomic and molecular masses are usually obtained relative to the mass of the isotope C (carbon 12), which by definition is equal to 12. For example, the molecular weight of methane, whose molecular formula is CH, is calculated as follows:
A more proper term would be "relative molecular mass". However the adjective 'relative' is omitted as it is universally assumed that atomic and molecular masses are relative to the mass of C. Relative atomic and molecular mass values are dimensionless but are given the "unit" Dalton (formerly atomic mass unit) to indicate that the number is equal to the mass of one molecule divided by of the mass of one atom of C. The mass of 1 mol of substance is designated as molar mass. By definition, it has the unit gram.

In the example above the atomic weight of carbon is given as 12.011, not 12. This is because naturally occurring carbon is a mixture of the isotopes C, C and C which have relative atomic masses of 12, 13 and 14 respectively. Moreover, the proportion of the isotopes varies between samples, so 12.011 is an average value. By contrast, there is less variation in naturally occurring hydrogen so the average atomic weight is known more precisely. The precision of the molecular mass is determined by precision of the least precise atomic mass value, in this case that of carbon. In high-resolution mass spectrometry the isotopomers CH and CH are observed as distinct molecules, with molecular weights of 16 and 17, respectively. The intensity of the mass-spectrometry peaks is proportional to the isotopic abundances in the molecular species. C H H can also be observed with molecular weight of 17.

In mass spectrometry, the molecular mass of a small molecule is usually reported as the monoisotopic mass, that is, the mass of the molecule containing only the most common isotope of each element. Note that this also differs subtly from the molecular mass in that the choice of isotopes is defined and thus is a single specific molecular mass of the many possible. The masses used to compute the monoisotopic molecular mass are found on a table of isotopic masses and are not found on a typical periodic table. The average molecular mass is often used for larger molecules since molecules with many atoms are unlikely to be composed exclusively of the most abundant isotope of each element. A theoretical average molecular mass can be calculated using the standard atomic weights found on a typical periodic table, since there is likely to be a statistical distribution of atoms representing the isotopes throughout the molecule. This however may differ from the true average molecular mass of the sample due to natural (or artificial) variations in the isotopic distributions.

To a first approximation, the basis for determination of molecular weight according to Mark–Houwink relations is the fact that the intrinsic viscosity of solutions (or suspensions) of macromolecules depends on volumetric proportion of the dispersed particles in a particular solvent. Specifically, the hydrodynamic size as related to molecular weight depends on a conversion factor, describing the shape of a particular molecule. This allows the apparent molecular weight to be described from a range of techniques sensitive to hydrodynamic effects, including DLS, SEC (also known as GPC), viscometry and diffusion ordered nuclear magnetic resonance spectroscopy (DOSY). The apparent hydrodynamic size can then be used to approximate molecular weight using a series of macromolecule-specific standards. As this requires calibration, it's frequently described as a "relative" molecular weight determination method.

It is also possible to determine absolute molecular weight directly from light scattering, traditionally using the Zimm method. This can be accomplished either via classical static light scattering or via multi-angle light scattering detectors. Molecular weights determined by this method do not require calibration, hence the term "absolute". The only external measurement required is refractive index increment, which describes the change in refractive index with concentration.




</doc>
<doc id="19838" url="https://en.wikipedia.org/wiki?curid=19838" title="Metallic bonding">
Metallic bonding

Metallic bonding is a type of chemical bonding that arises from the electrostatic attractive force between conduction electrons (in the form of an electron cloud of delocalized electrons) and positively charged metal ions. It may be described as the sharing of "free" electrons among a lattice of positively charged ions (cations). Metallic bonding accounts for many physical properties of metals, such as strength, ductility, thermal and electrical resistivity and conductivity, opacity, and luster.

Metallic bonding is not the only type of chemical bonding a metal can exhibit, even as a pure substance. For example, elemental gallium consists of covalently-bound pairs of atoms in both liquid and solid state—these pairs form a crystal lattice with metallic bonding between them. Another example of a metal–metal covalent bond is mercurous ion ().

As chemistry developed into a science it became clear that metals formed the large majority of the periodic table of the elements and great progress was made in the description of the salts that can be formed in reactions with acids. With the advent of electrochemistry, it became clear that metals generally go into solution as positively charged ions and the oxidation reactions of the metals became well understood in the electrochemical series. A picture emerged of metals as positive ions held together by an ocean of negative electrons.

With the advent of quantum mechanics, this picture was given more formal interpretation in the form of the free electron model and its further extension, the nearly free electron model. In both of these models, the electrons are seen as a gas traveling through the lattice of the solid with an energy that is essentially isotropic in that it depends on the square of the magnitude, "not" the direction of the momentum vector k. In three-dimensional k-space, the set of points of the highest filled levels (the Fermi surface) should therefore be a sphere. In the nearly free correction of the model, box-like Brillouin zones are added to k-space by the periodic potential experienced from the (ionic) lattice, thus mildly breaking the isotropy.

The advent of X-ray diffraction and thermal analysis made it possible to study the structure of crystalline solids, including metals and their alloys, and the construction of phase diagrams became accessible. Despite all this progress, the nature of intermetallic compounds and alloys largely remained a mystery and their study was often empirical. Chemists generally steered away from anything that did not seem to follow Dalton's and the problem was considered the domain of a different science, metallurgy.

The almost-free electron model was eagerly taken up by some researchers in this field, notably Hume-Rothery, in an attempt to explain why certain intermetallic alloys with certain compositions would form and others would not. Initially his attempts were quite successful. His idea was to add electrons to inflate the spherical Fermi-balloon inside the series of Brillouin-boxes and determine when a certain box would be full. This indeed predicted a fairly large number of observed alloy compositions. Unfortunately, as soon as cyclotron resonance became available and the shape of the balloon could be determined, it was found that the assumption that the balloon was spherical did not hold at all, except perhaps in the case of caesium. This reduced many of the conclusions to examples of how a model can sometimes give a whole series of correct predictions, yet still be wrong.
The free-electron debacle showed researchers that the model assuming that the ions were in a sea of free electrons needed modification, and so a number of quantum mechanical models such as band structure calculations based on molecular orbitals or the density functional theory were developed. In these models, one either departs from the atomic orbitals of neutral atoms that share their electrons or (in the case of density functional theory) departs from the total electron density. The free-electron picture has, nevertheless, remained a dominant one in education.

The electronic band structure model became a major focus not only for the study of metals but even more so for the study of semiconductors. Together with the electronic states, the vibrational states were also shown to form bands. Rudolf Peierls showed that, in the case of a one-dimensional row of metallic atoms, say hydrogen, an instability had to arise that would lead to the breakup of such a chain into individual molecules. This sparked an interest in the general question: When is collective metallic bonding stable and when will a more localized form of bonding take its place? Much research went into the study of clustering of metal atoms.

As powerful as the concept of the band structure proved to be in the description of metallic bonding, it does have a drawback. It remains a one-electron approximation to a multitudinous many-body problem. In other words, the energy states of each electron are described as if all the other electrons simply form a homogeneous background. Researchers like Mott and Hubbard realized that this was perhaps appropriate for strongly delocalized s- and p-electrons but for d-electrons, and even more for f-electrons the interaction with electrons (and atomic displacements) in the local environment may become stronger than the delocalization that leads to broad bands. Thus, the transition from localized unpaired electrons to itinerant ones partaking in metallic bonding became more comprehensible.

The combination of two phenomena gives rise to metallic bonding: delocalization of electrons and the availability of a far larger number of delocalized energy states than of delocalized electrons. The latter could be called electron deficiency.

Graphene is an example of two-dimensional metallic bonding. Its metallic bonds are similar to aromatic bonding in benzene, naphthalene, anthracene, ovalene, and so on.

Metal aromaticity in metal clusters is another example of delocalization, this time often in three-dimensional entities. Metals take the delocalization principle to its extreme and one could say that a crystal of a metal represents a single molecule over which all conduction electrons are delocalized in all three dimensions. This means that inside the metal one can generally not distinguish molecules, so that the metallic bonding is neither intra- nor intermolecular. 'Nonmolecular' would perhaps be a better term. Metallic bonding is mostly non-polar, because even in alloys there is little difference among the electronegativities of the atoms participating in the bonding interaction (and, in pure elemental metals, none at all). Thus, metallic bonding is an extremely delocalized communal form of covalent bonding. In a sense, metallic bonding is not a 'new' type of bonding at all, therefore, and it describes the bonding only as present in a "chunk" of condensed matter, be it crystalline solid, liquid, or even glass. Metallic vapors by contrast are often atomic (Hg) or at times contain molecules like Na held together by a more conventional covalent bond. This is why it is not correct to speak of a single 'metallic bond'.

The delocalization is most pronounced for - and -electrons. For caesium it is so strong that the electrons are virtually free from the caesium atoms to form a gas constrained only by the surface of the metal. For caesium, therefore, the picture of Cs ions held together by a negatively charged electron gas is not too inaccurate. For other elements the electrons are less free, in that they still experience the potential of the metal atoms, sometimes quite strongly. They require a more intricate quantum mechanical treatment (e.g., tight binding) in which the atoms are viewed as neutral, much like the carbon atoms in benzene. For - and especially -electrons the delocalization is not strong at all and this explains why these electrons are able to continue behaving as unpaired electrons that retain their spin, adding interesting magnetic properties to these metals.

Metal atoms contain few electrons in their valence shells relative to their periods or energy levels. They are electron deficient elements and the communal sharing does not change that. There remain far more available energy states than there are shared electrons. Both requirements for conductivity are therefore fulfilled: strong delocalization and partly filled energy bands. Such electrons can therefore easily change from one energy state into a slightly different one. Thus, not only do they become delocalized, forming a sea of electrons permeating the lattice, but they are also able to migrate through the lattice when an external electrical field is imposed, leading to electrical conductivity. Without the field, there are electrons moving equally in all directions. Under the field, some will adjust their state slightly, adopting a different wave vector. As a consequence, there will be more moving one way than the other and a net current will result.

The freedom of conduction electrons to migrate also give metal atoms, or layers of them, the capacity to slide past each other. Locally, bonds can easily be broken and replaced by new ones after the deformation. This process does not affect the communal metallic bonding very much. This gives rise to metals' typical characteristic phenomena of malleability and ductility. This is particularly true for pure elements. In the presence of dissolved impurities, the defects in the lattice that function as cleavage points may get blocked and the material becomes harder. Gold, for example, is very soft in pure form (24-karat), which is why alloys of 18-karat or lower are preferred in jewelry.

Metals are typically also good conductors of heat, but the conduction electrons only contribute partly to this phenomenon. Collective (i.e., delocalized) vibrations of the atoms known as phonons that travel through the solid as a wave, contribute strongly.

However, the latter also holds for a substance like diamond. It conducts heat quite well but "not" electricity. The latter is "not" a consequence of the fact that delocalization is absent in diamond, but simply that carbon is not electron deficient.
The electron deficiency is an important point in distinguishing metallic from more conventional covalent bonding. Thus, we should amend the expression given above into: "Metallic bonding is an extremely delocalized communal form of electron deficient covalent bonding".

Metallic radius is defined as one-half of the distance between the two adjacent metal ions in the metallic lattice. This radius depends on the nature of the atom as well as its environment—specifically, on the coordination number (CN), which in turn depends on the temperature and applied pressure.

When comparing periodic trends in the size of atoms it is often desirable to apply so-called Goldschmidt correction, which converts the radii to the values the atoms would have if they were 12-coordinated. Since metallic radii are always biggest for the highest coordination number, correction for less dense coordinations involves multiplying by x, where 0 < x < 1. Specifically, for CN = 4, x = 0.88; for CN = 6, x = 0.96, and for CN = 8, x = 0.97. The correction is named after Victor Goldschmidt who obtained the numerical values quoted above.

The radii follow general periodic trends: they decrease across the period due to increase in the effective nuclear charge, which is not offset by the increased number of valence electrons. The radii also increase down the group due to increase in principal quantum number. Between rows 3 and 4, the lanthanide contraction is observed – there is very little increase of the radius down the group due to the presence of poorly shielding f orbitals.

The atoms in metals have a strong attractive force between them. Much energy is required to overcome it. Therefore, metals often have high boiling points, with tungsten (5828 K) being extremely high. A remarkable exception are the elements of the zinc group: Zn, Cd, and Hg. Their electron configuration ends in ...ns and this comes to resemble a noble gas configuration like that of helium more and more when going down in the periodic table because the energy distance to the empty np orbitals becomes larger. These metals are therefore relatively volatile, and are avoided in ultra-high vacuum systems.

Otherwise, metallic bonding can be very strong, even in molten metals, such as Gallium. Even though gallium will melt from the heat of one's hand just above room temperature, its boiling point is not far from that of copper. Molten gallium is therefore a very nonvolatile liquid thanks to its strong metallic bonding.

The strong bonding of metals in the liquid form demonstrates that the energy of a metallic bond is not a strong function of the direction of the metallic bond; this lack of bond directionality is a direct consequence of electron delocalization, and is best understood in contrast to the directional bonding of covalent bonds. The energy of a metallic bond is thus mostly a function of the number of electrons which surround the metallic atom, as exemplified by the Embedded atom model. This typically results in metals assuming relatively simple, close-packed crystal structures, such as FCC, BCC, and HCP.

Given high enough cooling rates and appropriate alloy composition, metallic bonding can occur even in glasses with an amorphous structure.

Much biochemistry is mediated by the weak interaction of metal ions and biomolecules. Such interactions and their associated conformational change has been measured using dual polarisation interferometry.

Metals are insoluble in water or organic solvents unless they undergo a reaction with them. Typically this is an oxidation reaction that robs the metal atoms of their itinerant electrons, destroying the metallic bonding. However metals are often readily soluble in each other while retaining the metallic character of their bonding. Gold, for example, dissolves easily in mercury, even at room temperature. Even in solid metals, the solubility can be extensive. If the structures of the two metals are the same, there can even be complete solid solubility, as in the case of electrum, the alloys of silver and gold. At times, however, two metals will form alloys with different structures than either of the two parents. One could call these materials metal compounds, but, because materials with metallic bonding are typically not molecular, Dalton's law of integral proportions is not valid and often a range of stoichiometric ratios can be achieved. It is better to abandon such concepts as 'pure substance' or 'solute' is such cases and speak of phases instead. The study of such phases has traditionally been more the domain of metallurgy than of chemistry, although the two fields overlap considerably.

The metallic bonding in complicated compounds does not necessarily involve all constituent elements equally. It is quite possible to have an element or more that do not partake at all. One could picture the conduction electrons flowing around them like a river around an island or a big rock. It is possible to observe which elements do partake, e.g., by looking at the core levels in an X-ray photoelectron spectroscopy (XPS) spectrum. If an element partakes, its peaks tend to be skewed.

Some intermetallic materials e.g. do exhibit metal clusters, reminiscent of molecules and these compounds are more a topic of chemistry than of metallurgy. The formation of the clusters could be seen as a way to 'condense out' (localize) the electron deficient bonding into bonds of a more localized nature. Hydrogen is an extreme example of this form of condensation. At high pressures it is a metal. The core of the planet Jupiter could be said to be held together by a combination of metallic bonding and high pressure induced by gravity. At lower pressures however the bonding becomes entirely localized into a regular covalent bond. The localization is so complete that the (more familiar) H gas results. A similar argument holds for an element like boron. Though it is electron deficient compared to carbon, it does not form a metal. Instead it has a number of complicated structures in which icosahedral B clusters dominate. Charge density waves are a related phenomenon.

As these phenomena involve the movement of the atoms towards or away from each other, they can be interpreted as the coupling between the electronic and the vibrational states (i.e. the phonons) of the material. A different such electron-phonon interaction is thought to cause a very different result at low temperatures, that of superconductivity. Rather than blocking the mobility of the charge carriers by forming electron pairs in localized bonds, Cooper-pairs are formed that no longer experience any resistance to their mobility.

The presence of an ocean of mobile charge carriers has profound effects on the optical properties of metals. They can only be understood by considering the electrons as a "collective" rather than considering the states of individual electrons involved in more conventional covalent bonds.

Light consists of a combination of an electrical and a magnetic field. The electrical field is usually able to excite an elastic response from the electrons involved in the metallic bonding. The result is that photons are not able to penetrate very far into the metal and are typically reflected. They bounce off, although some may also be absorbed. This holds equally for all photons of the visible spectrum, which is why metals are often silvery white or grayish with the characteristic specular reflection of metallic luster. The balance between reflection and absorption determines how white or how gray they are, although surface tarnish can obscure such observations. Silver, a very good metal with high conductivity is one of the whitest.

Notable exceptions are reddish copper and yellowish gold. The reason for their color is that there is an upper limit to the frequency of the light that metallic electrons can readily respond to, the plasmon frequency. At the plasmon frequency, the frequency-dependent dielectric function of the free electron gas goes from negative (reflecting) to positive (transmitting); higher frequency photons are not reflected at the surface, and do not contribute to the color of the metal. There are some materials like indium tin oxide (ITO) that are metallic conductors (actually degenerate semiconductors) for which this threshold is in the infrared, which is why they are transparent in the visible, but good mirrors in the IR.

For silver the limiting frequency is in the far UV, but for copper and gold it is closer to the visible. This explains the colors of these two metals. At the surface of a metal resonance effects known as surface plasmons can result. They are collective oscillations of the conduction electrons like a ripple in the electronic ocean. However, even if photons have enough energy they usually do not have enough momentum to set the ripple in motion. Therefore, plasmons are hard to excite on a bulk metal. This is why gold and copper still look like lustrous metals albeit with a dash of color. However, in colloidal gold the metallic bonding is confined to a tiny metallic particle, preventing the oscillation wave of the plasmon from 'running away'. The momentum selection rule is therefore broken, and the plasmon resonance causes an extremely intense absorption in the green with a resulting beautiful purple-red color. Such colors are orders of magnitude more intense than ordinary absorptions seen in dyes and the like that involve individual electrons and their energy states.



</doc>
<doc id="19839" url="https://en.wikipedia.org/wiki?curid=19839" title="Methyl group">
Methyl group

A methyl group is an alkyl derived from methane, containing one carbon atom bonded to three hydrogen atoms — CH. In formulas, the group is often abbreviated Me. Such hydrocarbon groups occur in many organic compounds. It is a very stable group in most molecules. While the methyl group is usually part of a larger molecule, it can be found on its own in any of three forms: anion, cation or radical. The anion has eight valence electrons, the radical seven and the cation six. All three forms are highly reactive and rarely observed.

The methylium cation (CH) exists in the gas phase, but is otherwise not encountered. Some compounds are considered to be sources of the CH cation, and this simplification is used pervasively in organic chemistry. For example, protonation of methanol gives a strongly electrophilic methylating reagent:

Similarly, methyl iodide and methyl triflate are viewed as the equivalent of the methyl cation because they readily undergo S2 reactions by weak nucleophiles.

The methanide anion (CH) exists only in rarefied gas phase or under exotic conditions. It can be produced by electrical discharge in ketene at low pressure (less than one torr) and its enthalpy of reaction is determined to be about 252.2±3.3 kJ/mol.

In discussions mechanisms of organic reactions, methyl lithium and related Grignard reagents are often considered to be salts of "CH"; and though the model may be useful for description and analysis, it is only a useful fiction. Such reagents are generally prepared from the methyl halides:
where M is an alkali metal.

The methyl radical has the formula CH. It exists in dilute gases, but in more concentrated form it readily dimerizes to ethane. It can be produced by thermal decomposition of only certain compounds, especially those with an -N=N- linkage.

The reactivity of a methyl group depends on the adjacent substituents. Methyl groups can be quite unreactive. For example, in organic compounds, the methyl group resists attack by even the strongest acids.

The oxidation of a methyl group occurs widely in nature and industry. The oxidation products derived from methyl are CHOH, CHO, and COH. For example, permanganate often converts a methyl group to a carboxyl (-COOH) group, e.g. the conversion of toluene to benzoic acid. Ultimately oxidation of methyl groups gives protons and carbon dioxide, as seen in combustion.

Demethylation (the transfer of the methyl group to another compound) is a common process, and reagents that undergo this reaction are called methylating agents. Common methylating agents are dimethyl sulfate, methyl iodide, and methyl triflate. Methanogenesis, the source of natural gas, arises via a demethylation reaction.

Certain methyl groups can be deprotonated. For example, the acidity of the methyl groups in acetone ((CH)CO) is about 10 more acidic than methane. The resulting carbanions are key intermediates in many reactions in organic synthesis and biosynthesis. Fatty acids are produced in this way.

When placed in benzylic or allylic positions, the strength of the C-H bond is decreased, and the reactivity of the methyl group increases. One manifestation of this enhanced reactivity is the photochemical chlorination of the methyl group in toluene to give benzyl chloride.

In the special case where one hydrogen is replaced by deuterium (D) and another hydrogen by tritium (T), the methyl substituent becomes chiral. Methods exist to produce optically pure methyl compounds, e.g., chiral acetic acid (CHDTCOH). Through the use of chiral methyl groups, the stereochemical course of several biochemical transformations have been analyzed.

French chemists Jean-Baptiste Dumas and Eugene Peligot, after determining methanol's chemical structure, introduced "methylene" from the Greek "methy" "wine" and "hȳlē" "wood, patch of trees" with the intention of highlighting its origins, "alcohol made from wood (substance)". The term "methyl" was derived in about 1840 by back-formation from "methylene", and was then applied to describe "methyl alcohol".

"Methyl" is the IUPAC nomenclature of organic chemistry term for an alkane (or alkyl) molecule, using the prefix "meth-" to indicate the presence of a single carbon.



</doc>
<doc id="19842" url="https://en.wikipedia.org/wiki?curid=19842" title="Mild ale">
Mild ale

The term "mild" originally meant young beer or ale, as opposed to "stale" aged beer or ale with its resulting "tang". In more recent times, it has been interpreted as having a low gravity or being "mildly hopped".

This style of beer, originated in Britain in the 17th century or earlier, has a predominantly malty palate. Modern mild ales are mainly dark coloured with an alcohol by volume (ABV) of 3% to 3.6%, although there are lighter hued examples as well as stronger examples reaching 6% abv and higher.

Light mild is generally similar, but pale in colour, for instance Harveys Brewery Knots of May. There is some overlap between the weakest styles of bitter and light mild, with the term AK being used to refer to both. The designation of such beers as "bitter" or "mild" has tended to change with fashion. A good example is McMullen's AK, which was re-badged as a bitter after decades as a light mild. AK (a very common beer name in the 19th century) was often referred to as a "mild bitter beer" interpreting "mild" as "unaged".

Once sold in every pub, mild experienced a sharp decline in popularity after the 1960s and was in danger of completely disappearing. However, in recent years the increase of microbreweries has led to a modest renaissance and an increasing number of milds (sometimes labelled "Dark") are now being brewed.

The Campaign for Real Ale has designated May as Mild Month. In the United States, a group of beer bloggers organised the first American Mild Month for May 2015, with forty-five participating breweries across the country.

"Mild" was originally used to designate any beer which was young, fresh or unaged and did not refer to a specific style of beer. Thus there was Mild Ale but also Mild Porter and even Mild Bitter Beer. These young beers were often blended with aged "stale" beer to improve their flavour. As the 19th century progressed and public taste moved away from the aged taste, unblended young beer, mostly in the form of Mild Ale or Light Bitter Beer, began to dominate the market.

In the 19th century a typical brewery produced three or four mild ales, usually designated by a number of X's, the weakest being X, the strongest XXXX. They were considerably stronger than the milds of today, with the gravity ranging from around 1.055 to 1.072 (about 5.5% to 7% abv). Gravities dropped throughout the late 19th century and by 1914 the weakest milds were down to about 1.045, still considerably stronger than modern versions.

The draconian measures applied to the brewing industry during the First World War had a particularly dramatic effect upon mild. As the biggest-selling beer, it suffered the largest cut in gravity when breweries had to limit the average OG of their beer to 1.030. In order to be able to produce some stronger beer - which was exempt from price controls and thus more profitable - mild was reduced to 1.025 or lower.

Modern dark mild varies from dark amber to near-black in colour and is very light-bodied. Its flavour is dominated by malt, sometimes with roasty notes derived from the use of black malt, with a subdued hop character, though there are some quite bitter examples. Most are in the range 1.030–1.036 (3–3.6% abv).

Light mild is generally similar, but paler in colour. Some dark milds are created by the addition of caramel to a pale beer.

Until the 1960s mild was the most popular beer style in England. Pockets of demand remain, particularly in the West Midlands and North West England, but has been largely ousted by bitter and lager elsewhere. In 2002, only 1.3% of beer sold in pubs was Mild. Mild's popularity in Wales, in particular, persisted as a relatively low-alcohol, sweet drink for coal miners. Some brewers have continued to produce mild, but have found it sells better under a different name: for instance, Brains's mild was renamed Dark. Outside the United Kingdom mild is virtually unknown, with the exception of Old in New South Wales and some microbrewery recreations in North America and Scandinavia. Some notable examples of Milds are: Bank's Mild, Cain's Dark Mild, Highgate Dark Mild, Brain's Dark, Moorehouse Blackcat, Rudgate Ruby Mild, and Theakston Traditional Mild

A popular drink in the West Midlands, "brown and mild" (also known as a "boilermaker") is a half pint of draught mild served mixed with a half pint of bottled brown ale in a pint glass. In the North West of England a mixture of half a pint of mild and half a pint of bitter is known as a "mixed". In Norfolk (UK), the same mixture was called a pint of "twos".

Mild ales are generally based on mild malt or pale malt. Most milds contain, in addition, a quantity of crystal malt; dark milds, meanwhile, make use of chocolate malt, black malt or dark brewing sugars. Milds tend to be lightly hopped compared to pale ale and are usually low in alcohol; strong mild ales used to reach six or seven per cent abv, but very few such beers are still brewed. Sarah Hughes Dark Ruby Mild, brewed to a pre-World War I recipe, is a rare example of a strong Mild (6.0% ABV).

As part of the first American Mild Month, the project organizers challenged participating breweries to create a new variation on the mild ale style by brewing with American malts and hops. They defined American Mild as "a restrained, darkish ale, with gentle hopping and a clean finish so that the malt and what hops are present, shine through".


</doc>
<doc id="19843" url="https://en.wikipedia.org/wiki?curid=19843" title="Mars Society">
Mars Society

The Mars Society is an American worldwide volunteer-driven space-advocacy non-profit organization dedicated to promoting the human exploration and settlement of the planet Mars. Inspired by "The Case for Mars" conferences which were hosted by The Mars Underground at the University of Colorado Boulder, the Mars Society was established by Dr. Robert Zubrin and others in 1998 with the goal of educating the public, the media and government on the benefits of exploring Mars, the importance of planning for a humans-to-Mars mission in the coming decades and the need to create a permanent human presence on the Red Planet.

Mars Society, Inc. was formally established in September 1997 under the Colorado Non-Profit Corporation Act. In August 1998 more than 700 delegates – astronomers, scientists, engineers, astronauts, entrepreneurs, educators, students and space enthusiasts – attended a week-end of talks and presentations from leading Mars exploration advocates. Since then, the Mars Society, guided by its Steering Committee, has grown to over 5,000 members and some 6,000 associate supporters across more than 50 countries around the world. Members of the Mars Society are from all walks of life and actively work to promote the ideals of space exploration and the opportunities for exploring the Red Planet. In 2017 the Marspedia encyclopedia became an official project of the Mars Society.

The Mars Society's goals aren't purely theoretical. Its aim is to show that Mars is an achievable goal through a practical series of technical and other projects, including:


In addition, the Society:


The current board of directors of the Mars Society includes Robert Zubrin (chairman) and James Heiser.

Notable members of its steering committee include Buzz Aldrin and Peter H. Smith.

Notable former members of the board of directors or steering committee of the Mars Society include Kim Stanley Robinson, Michael D. Griffin, Christopher McKay, and Pascal Lee.

The Society is an organization member of the Alliance for Space Development.

The Mars Society has chapters in the U.S. and around the world. Many of these chapters undertake scientific, engineering and political initiatives to further the Mars Society's goals. Some accomplishments of Mars Society chapters are listed below:

Mars Society of Canada:

Northern California Chapter of the Mars Society:

The Mars Society - San Diego:

Dallas Chapter of the Mars Society:

Mars Society Seattle (formerly known as Mars Society Puget Sound):

Österreichisches Weltraum Forum (OeWF)

The ASF (Österreichisches Weltraum Forum, OeWF) is a national network for aerospace and space enthusiasts, being the Austrian chapter of the Mars Society. The Forum serves as a communication platform between the space sector and the public; it is embedded in a global network of specialists from the space industry, research and policy. Hence, the OeWF facilitates a strengthening of the national space sector through enhancing the public visibility of space activities, technical workshops, and conferences as well as Forum-related projects.

Their research focus is Mars Analogue Research, e.g. the AustroMars mission with roughly 130 volunteers supporting a mission simulation at the Mars Desert Research Station (MDRS) and the ongoing PolAres, a multi-year research program which encompass the development of a Mars analogue rover system and a novel spacesuit prototype dubbed "Aouda.X", culminating in an arctic expedition in 2011.

The Forum has a small, but a highly active pool of professional members contributing to space endeavors, mostly in cooperation with other nations as well as international space organizations. The spectrum of their activities ranges from simple classroom presentation to 15.000-visitors space exhibitions, from expert reports for the Austrian Federal Ministry for Technology to space technology transfer activities for terrestrial applications.

Mars Society Belgium
Mars Society Bulgaria

Mars Society Bulgaria was created in 2012.

Association Planète Mars

The Mars Society French chapter was established in 1999 as "Association Planète Mars", a non-profit organization with its headquarters in Paris. Its founder and president is Richard Heidmann, a space propulsion engineer, who participated in the founding convention of the Mars Society in August 1998 and is a member of the Mars Society Steering Committee.

While fully supporting the ideas and actions of the Mars Society, it considers that those must be adapted to the specific cultural and political context of France and Europe.
The main activities of Association Planète Mars are devoted to public communication, through conferences, exhibits, events, media appearances (TV, radio, magazines...). It also acts occasionally as an adviser for journalists or film makers.

Whenever possible, it cooperates with other associations or science outreach organisms, which permits to reinforce its action and reach a wider public.

Association Planète Mars seeks to interest younger people: 25% of its paid members are under the age of 25. It aims to encourage Mars-related projects to be undertaken by engineering students. The association also encourages the formation of working groups on miscellaneous topics. Today, three groups are active, respectively on mission safety, Martian architecture and medical aspects. It has participated in several MDRS and FMARS missions, including a prototype of a "Cliff Exploration Vehicle".

Another major field of action is lobbying, aiming at both political and institutional groups, in France and at the European level (European Council, ESA). In doing so, it relies on the networks established by some of its managers. On the occasion of most critical events, the association publishes political documents to support its views, which are distributed both to opinion formers and to the press. This has been the case in June 2004, in the wake of the US Space Exploration Initiative, and in September 2008 in preparation of the ESA ministerial council.

Mars Society Deutschland

The German Chapter of the Mars Society (Mars Society Deutschland e.V. | "eingetragener Verein" | - MSD) was founded in 2001 based on the Founding Declaration of the Mars Society of the USA from 1998 and has about 230 members. The MSD is registered in Germany as a non-profit association ("gemeinnütziger Verein"). Registered members pay a yearly membership fee of 60 Euro. However, students and firms pay a different fee. The activities of the MSD are focused on technical-scientific projects such as the Mars Balloon Probe ARCHIMEDES as well as on all Mars exploration and general manned space matters. The main means of communication with members and the general public is the MSD Website with information on the ARCHIMEDES project, publications on Mars and other space subjects, the regular news, which can be commented by visitors of the website, the Space Forum and informative meetings.

The MSD Board comprises five members. Since June 2009 its president is the Space Physicist Dr. Michael Danielides.
The development of ARCHIMEDES is led by Dipl.Ing. Hannes Griebel, who is also a member of the MSD Board and prepares his doctorate thesis on ARCHIMEDES.

ARCHIMEDES is presently under development and the major project of the MSD since 2001. Starting in 2006, flight tests have been undertaken for testing the innovative balloon system in the low-gravity environment. Test carriers were so far the Airbus A300 for short duration parabolic flights and the sounding rocket test campaigns REXUS3-REGINA and REXUS4-MIRIAM for longer duration flight tests under free space conditions. Further flights tests are planned for the coming years (e.g. MIRIAM II) with the objective of qualifying ARCHIMEDES for its Mars mission by 2018. ARCHIMEDES will be carried to Mars on board an AMSAT Mars Probe or a similar satellite. ARCHIMEDES is developed by the MSD with the support of the Bundeswehr University Munich, of the IABG in Ottobrunn, the DLR-MORABA for rocket flight opportunities, other universities, and several industrial companies supporting specific technical areas.

Mars Society Hellas

The Mars Society Hellas was founded in 2016. Its headquarters are in Athens and its president is Alexandros Krassakis.
It is the Official Hellenic Chapter of The Mars Society and its Aims/Constitution Pillars are aligned with The Mars Society’s Founding Declaration. The focus is given in: (i). Bring together people with an interest in the planet Mars; (ii). Establish and co-operate in projects relating to Mars exploration; (iii). Promote awareness of The Mars Society principles; (iv). Engagement of academic institutions with The Mars Society Hellas.

Italian Mars Society

The Mars Society Italian chapter (Italian Mars Society) was founded in 2005. Its headquarters are in Curno (Bergamo), near Milan, and its president is Antonio Del Mastro, an entrepreneur and owner of an engineering consulting company.
The Italian Mars Society carries out scientific divulgation activities related to the Red Planet. It cooperates with the other European Mars Society chapters to build a knowledge network whose aim is to develop the interest of European people towards Mars exploration.

Another Mars Society Chapter is also in creation in Luxembourg.

Stichting Mars Society Nederland

The Dutch Chapter of the Mars Society (Mars Society Netherlands) started with its successful website in 1999. The MS NL is registered in the Netherlands as a non-profit organization ("stichting"). Registered members pay a yearly membership fee. The activities of the MS NL are focused on outreach through conferences (national and European), exhibits, events, and media campaigns.

The main means of communication with members and the general public is still the MS NL Website with information on Mars and related space subjects. In cooperation with the Netherlands Society for Aerospace (NVR), MS NL strives to educate youth in 'kids for space', and the 'RuimteWijs' program, giving lectures about Mars all over Netherlands. Lectures on Mars are also given under auspices of the Dutch Astronomical Society. MS NL cooperates with The Planetary Society in mars events.

The MS NL has been involved with international projects of The Mars Society from the starts, it is a strong proponent of European cooperation in space endeavours.

The Mars Society Netherlands chapter ceased to exist since 2011; as its board and members moved over to a new Mars-oriented organisation.

Mars Society Polska (MSP)

Mars Society Polska (MSP) is actively participating in the creation of the Polish space industry. Since this sector is still developing, the organization is taking the opportunity to provide a strong Mars-related element for the years to come. Poland was the last member state of the EU to sign the cooperation agreement with ESA. Most projects in Poland currently focus on satellite technology, so MSP is the only leading organization promoting exploration and manned spaceflight. Besides private sponsors, it relies on resources obtained from the Ministry of Science and Higher Education and local authorities, proposing projects to be undertaken with local communities and thus engaging with the general public.

MSP's first project was the Polish MPV (pressurized rover) design, for which some hardware was produced. This enabled development of the Polish Mars Society itself, together with a number of educational activities for Polish schools. This was followed by the joint organization of the Polish edition of the Red Rover Goes to Mars contest and organization of a Mars colonization negotiation game (Columbia Memorial Negotiations). In 2007 MSP organized the first Mars Festival, a two-day event which drew 600 visitors, with Discovery Channel as the main sponsor. Mars Festival 2008 was smaller due to the efforts being made in other projects, particularly the Polish URC rover, named Skarabeusz.

The flagship MSP project is the Polish Martian habitat, based on a design by Janek Kozicki. It has three inflatable modules attached and a usable surface of 900 m². The habitat is to be located close to a large town, meaning that beyond its role as a test site, largely for materials and design, it will be accessible to the wider public and media.

MSP has established a constant presence in the mainstream Polish media and is working on a documentary about itself. It is also developing software projects, IT systems for the future martian habitat, with a Virtual Mars Base and remote access. Jan Kotlarz of MSP has created RODM software for the modeling of the Martian surface based on high-resolution photographs from Mars Reconnaissance Orbiter. RODM is currently being tested by NASA and ESA.

Mars Society España

The Spanish Mars society acts as a meeting, news, and conference organizer.

Mars Society Switzerland

The Mars Society Switzerland ("MSS") was founded in February 2010. It covers the French and German speaking parts of Switzerland. It keeps close links with the French branch ("association planète Mars", see above).
Its aim is to convince the Swiss public of the interest and feasibility of the Martian exploration with inhabited flights through the Mars direct concept such as described by Robert Zubrin. It wants to gather around the scientists working on Mars in Switzerland, all people who share their interest on the matter.

In November 2010, MSS participated to the 8th Swiss Geoscience Meeting which was the opportunity to discuss the main topics related to Mars geology, the making of the planet, the role of water and the atmosphere.

In 2011 (September 30 until October 2), MSS held the 11th European Mars Convention ("EMC11") in the frame of the University of Neuchâtel. Through 24 presentations and two debates with major Swiss media, this convention covered all subjects related to Mars exploration; from astronautics to architecture, including the study of geology which remains its key objective.

On September 10, 2012, in the Natural History Museum Bern ("NHMB"), it held a conference on the theme "Searching for Life on Mars". 
The conference was centered upon a presentation by Professor André Maeder (a well-known astrophysicist at the University of Geneva) following the publishing of his book "L'unique Terre habitée?" (Favre editions). Another presentation was made by Dr. Beda Hofmann, Head of the Earth Science Dept. of the NHMB. He showed and commented photos of primitive forms of life which he gathered to serve as references for the observations to be made by the ESA ExoMars mission (to be launched in 2018).
Pierre Brisson, president of the Mars Society Switzerland introduced the conference, speaking about the instruments aboard Curiosity and the targets of exploration of the rover.

In October (12th till 14th) The Mars Society Switzerland participated to the 12th EMC ("EMC12") in Neubiberg, Germany (University of the German Armed Forces, near Münich). In this frame, Pierre Brisson discussed the past possibility of an Ocean in the Northern Lowlands of the planet.

A key event of the year 2013 (March 26), was a conference organized with "Club 44" in La Chaux de Fonds, during which Professor Michel Cabane, LATMOS and co-PI of the SAM Instruments aboard Curiosity, presented the findings of his instruments dedicated to the study of the molecular and atomic compositions of the rocks and atmosphere of the planet Mars.

Mars Society UK

The Mars Society UK is the oldest Mars Society outside the United States. It held its first public meeting on July 4, 1998, in London. Professor Colin Pillinger, head of the Beagle 2 project, was the Guest Speaker, and the event marked the first time Beagle 2 had been presented to the general public in the UK. From 1998 through to 2003, the Mars society UK (MSUK) continued to support Beagle 2, providing numerous public events at which members of the Beagle 2 project team could speak, and the Beagle 2 model be displayed.

Highlights of the MSUK's history include:

More recently, the MSUK had been allied with attempts to initiate a formally recognized and fully founded UK Space Conference (UKSC) with the first such event being held in April 2009.

The Mars Society India chapter (MSI) was founded in January 2012 by Dhruv Joshi, an alumnus of the Indian Institute of Technology Bombay. Dhruv Joshi was inspired to set up the chapter in India after he attended a presentation by Mars society Switzerland chapter; during his visit to Switzerland. MSI was launched on March 2, 2012 at Mumbai, with collaboration from Nehru center (Planetarium) and students of Indian Institute of Technology - Bombay (IIT-B). MSI endeavors to set a platform for bringing immense talent pool of Indian students to the forefront and achieve country's ambitious space missions.

Mars Society Bangladesh chapter was found in 2016. A group of 40 students and three teams from Bangladesh participated in 2016 University Rover Challenge (URC 2016) powered by Mars Society, held in June 2016 at Utah, USA.

There is a chapter in Australia, with branches in ACT, NSW,
Northern Territory, Queensland, South Australia, Tasmania, Victoria, Western Australia.

There is a chapter in New Zealand.



</doc>
<doc id="19845" url="https://en.wikipedia.org/wiki?curid=19845" title="Minerva">
Minerva

Minerva (; ; Etruscan: "Menrva") was the Roman goddess of wisdom and strategic warfare and the sponsor of arts, trade, and strategy. The Romans did not stress her relation to battle and warfare as the Greeks did. From the second century BC onward, the Romans equated her with the Greek goddess Athena.

Following the Greek myths around Athena, she was born of Metis, who had been swallowed by Jupiter, and burst from her father's head, fully armed and clad in armor. Jupiter forcefully impregnated the titaness Metis, which resulted in her attempting to change shape (or shapeshift) to escape him. Jupiter then recalled the prophecy that his own child would overthrow him as he had Saturn, and in turn, Saturn had Caelus. 

Fearing that their child would be male, and would grow stronger than he was and rule the Heavens in his place, Jupiter swallowed Metis whole after tricking her into turning herself into a fly. The titaness gave birth to Minerva and forged weapons and armor for her child while within Jupiter's body. In some versions of the story, Metis continued to live inside of Jupiter's mind as the source of his wisdom. Others say she was simply a vessel for the birth of Minerva. The constant pounding and ringing left Jupiter with agonizing pain. To relieve the pain, Vulcan used a hammer to split Jupiter's head and, from the cleft, Minerva emerged, whole, adult, and in full battle armor.

She was the virgin goddess of music, poetry, medicine, wisdom, commerce, weaving, and the crafts. She is often depicted with her sacred creature, an owl usually named as the "owl of Minerva", which symbolised her association with wisdom and knowledge as well as, less frequently, the snake and the olive tree.

Minerva was worshipped at several locations in Rome, most prominently as part of the Capitoline Triad. She was also worshipped at the Temple of Minerva Medica, and at the "Delubrum Minervae", a temple founded around 50 BC by Pompey on the site now occupied by the church of "Santa Maria sopra Minerva".

The Romans celebrated her festival from March 19 to March 23 during the day which is called, in the neuter plural, Quinquatria, the fifth after the Ides of March, the nineteenth, an artisans' holiday. A lesser version, the "Minusculae Quinquatria", was held on the Ides of June, June 13, by the flute-players, who were particularly useful to religion. In 207 BC, a guild of poets and actors was formed to meet and make votive offerings at the temple of Minerva on the Aventine Hill. Among others, its members included Livius Andronicus. The Aventine sanctuary of Minerva continued to be an important center of the arts for much of the middle Roman Republic.

As "Minerva Medica", she was the goddess of medicine and physicians. As "Minerva Achaea", she was worshipped at Lucera in Apulia where votive gifts and arms said to be those of Diomedes were preserved in her temple.

Her worship also was spread throughout the empire. In Britain, for example, she was syncretized with the local goddess Sulis, who often was invoked for restitution for theft.

In "Fasti" III, Ovid called her the "goddess of a thousand works". Minerva was worshipped throughout Italy, and when she eventually became equated with the Greek goddess Athena, she also became a goddess of battle. Unlike Mars, god of war, she was sometimes portrayed with sword lowered, in sympathy for the recent dead, rather than raised in triumph and battle lust. In Rome her bellicose nature was emphasized less than elsewhere. 

Minerva is featured on the coinage of different Roman Emperors. She often is represented on the reverse side of a coin holding an owl and a spear among her attributes.

Stemming from an Italic moon goddess "*Meneswā" ('She who measures'), the Etruscans adopted the inherited Old Latin name, "*Menerwā", thereby calling her Menrva. It is presumed that her Roman name, Minerva, is based on this Etruscan mythology. Minerva was the goddess of wisdom, war, art, schools, and commerce. She was the Etruscan counterpart to Greek Athena. Like Athena, Minerva burst from the head of her father, Jupiter (Greek Zeus), who had devoured her mother (Metis) in an unsuccessful attempt to prevent her birth.

By a process of folk etymology, the Romans could have linked her foreign name to the root "men-" in Latin words such as "mens" meaning "mind", perhaps because one of her aspects as goddess pertained to the intellectual. The word "mens" is built from the Proto-Indo-European root "*men-" 'mind' (linked with memory as in Greek Mnemosyne/μνημοσύνη and "mnestis"/μνῆστις: memory, remembrance, recollection, "manush" in Sanskrit meaning mind).

The Etruscan Menrva was part of a holy triad with Tinia and Uni, equivalent to the Roman Capitoline Triad of Jupiter-Juno-Minerva.

As a patron goddess of wisdom, Minerva frequently features in statuary, as an image on seals, and in other forms at educational institutions.








</doc>
<doc id="19846" url="https://en.wikipedia.org/wiki?curid=19846" title="Mars Direct">
Mars Direct

Mars Direct is a proposal for a human mission to Mars which purports to be both cost-effective and possible with current technology. It was originally detailed in a research paper by Martin Marietta engineers Robert Zubrin and David Baker in 1990, and later expanded upon in Zubrin's 1996 book "The Case for Mars". It now serves as a staple of Zubrin's speaking engagements and general advocacy as head of the Mars Society, an organization devoted to the colonization of Mars.

On July 20, 1989, George H. W. Bush – then President of the United States – announced plans for what came to be known as the Space Exploration Initiative (SEI). In a speech on the steps of the National Air and Space Museum he described long-term plans which would culminate in a manned mission to the surface of Mars.

By December 1990, a study to estimate the project's cost determined that long-term expenditure would total approximately 450 billion dollars spread over 20 to 30 years. The "90 Day Study" as it came to be known, evoked a hostile Congressional reaction towards SEI given that it would have required the largest single government expenditure since World War II. Within a year, all funding requests for SEI had been denied.

Dan Goldin became NASA Administrator on April 1, 1992, officially abandoning plans for near-term human exploration beyond Earth orbit with the shift towards a "faster, better, cheaper" strategy for robotic exploration.

While working at Martin Marietta designing interplanetary mission architectures, Robert Zubrin perceived a fundamental flaw in the SEI program. Zubrin came to understand that if NASA's plan was to fully utilize as many technologies as possible in support of sending the mission to Mars, it would become politically untenable. In his own words:

The exact opposite of the correct way to do engineering.

Zubrin's alternative to this "Battlestar Galactica" mission strategy (dubbed so by its detractors for the large, nuclear powered spaceships that supposedly resembled the science-fiction spaceship of the same name) involved a longer surface stay, a faster flight-path in the form of a conjunction class mission, in situ resource utilization and craft launched directly from the surface of Earth to Mars as opposed to be being assembled in orbit or by a space-based drydock. After receiving approval from management at Marietta, a 12-man team within the company began to work out the details of the mission. While they focused primarily on more traditional mission architectures, Zubrin began to collaborate with colleague David Baker's extremely simple, stripped-down and robust strategy. Their goal to "use local resources, travel light, and live off the land" became the hallmark of Mars Direct.

The first flight of the Ares rocket (not to be confused with the similarly named rocket of the now defunct Constellation program) would take an unmanned Earth Return Vehicle to Mars after a 6-month cruise phase, with a supply of hydrogen, a chemical plant and a small nuclear reactor. Once there, a series of chemical reactions (the Sabatier reaction coupled with electrolysis) would be used to combine a small amount of hydrogen (8 tons) carried by the "Earth Return Vehicle" with the carbon dioxide of the Martian atmosphere to create up to 112 tonnes of methane and oxygen. This relatively simple chemical-engineering procedure was used regularly in the 19th and 20th centuries, and would ensure that only 7% of the return propellant would need to be carried to the surface of Mars.

96 tonnes of methane and oxygen would be needed to send the "Earth Return Vehicle" on a trajectory back home at the conclusion of the surface stay; the rest would be available for Mars rovers. The process of generating fuel is expected to require approximately ten months to complete.

Some 26 months after the "Earth Return Vehicle" is originally launched from Earth, a second vehicle, the Mars Habitat Unit, would be launched on a 6-month long low-energy transfer trajectory to Mars, and would carry a crew of four astronauts (the minimum number required so that the team can be split in two without leaving anyone alone). The Habitat Unit would not be launched until the automated factory aboard the ERV had signaled the successful production of chemicals required for operation on the planet and the return trip to Earth. During the trip, artificial gravity would be generated by tethering the Habitat Unit to the spent upper stage of the booster, and setting them rotating about a common axis. This rotation would produce a comfortable 1 "g" working environment for the astronauts, freeing them of the debilitating effects of long-term exposure to weightlessness.

Upon reaching Mars, the upper stage would be jettisoned, with the Habitat Unit aerobraking into Mars orbit before soft-landing in proximity to the "Earth Return Vehicle". Precise landing would be supported by a radar beacon started by the first lander. Once on Mars, the crew would spend 18 months on the surface, carrying out a range of scientific research, aided by a small rover vehicle carried aboard their Mars Habitat Unit, and powered by the methane produced by the Earth Return Vehicle.

To return, the crew would use the "Earth Return Vehicle", leaving the Mars Habitat Unit for the possible use of subsequent explorers. On the return trip to Earth, the propulsion stage of the Earth Return Vehicle would be used as a counterweight to generate artificial gravity for the trip back.

Follow-up missions would be dispatched at 2 year intervals to Mars to ensure that a redundant ERV would be on the surface at all times, waiting to be used by the next crewed mission or the current crew in an emergency. In such an emergency scenario, the crew would trek hundreds of kilometers to the other ERV in their long-range vehicle.

The Mars Direct proposal includes a component for a Launch Vehicle "Ares", an Earth Return Vehicle (ERV) and a Mars Habitat Unit (MHU).

The plan involves several launches making use of heavy-lift boosters of similar size to the Saturn V used for the Apollo missions, which would potentially be derived from Space Shuttle components. This proposed rocket is dubbed "Ares", which would use space shuttle Advanced Solid Rocket Boosters, a modified shuttle external tank, and a new Lox/LH2 third stage for the trans-Mars injection of the payload. Ares would put 121 tonnes into a 300 km circular orbit, and boost 47 tonnes toward Mars.

The Earth Return Vehicle is a two-stage vehicle. The upper stage comprises the living accommodation for the crew during their six-month return trip to Earth from Mars. The lower stage contains the vehicle's rocket engines and a small chemical production plant.

The Mars Habitat Unit is a 2- or 3-deck vehicle providing a comprehensive living and working environment for a Mars crew. In addition to individual sleeping quarters which provide a degree of privacy for each of the crew and a place for personal effects, the Mars Habitat Unit includes a communal living area, a small galley, exercise area, and hygiene facilities with closed-cycle water purification. The lower deck of the Mars Habitat Unit provides the primary working space for the crew: small laboratory areas for carrying out geology and life science research; storage space for samples, airlocks for reaching the surface of Mars, and a suiting-up area where crew members prepare for surface operations. Protection from harmful radiation while in space and on the surface of Mars (e.g. from solar flares) would be provided by a dedicated "storm shelter" in the core of the vehicle.

The Mars Habitat Unit would also include a small pressurized rover that is stored in the lower deck area and assembled on the surface of Mars. Powered by a methane engine, it is designed to extend the range over which astronauts can explore the surface of Mars out to 320 km.

Since it was first proposed as a part of Mars Direct, the Mars Habitat Unit has been adopted by NASA as a part of their Mars Design Reference Mission, which uses two Mars Habitat Units – one of which flies to Mars unmanned, providing a dedicated laboratory facility on Mars, together with the capacity to carry a larger rover vehicle. The second Mars Habitat Unit flies to Mars with the crew, its interior given over completely to living and storage space.

To prove the viability of the Mars Habitat Unit, the Mars Society has implemented the Mars Analogue Research Station Program (MARS), which has established a number of prototype Mars Habitat Units around the world.

Baker pitched Mars Direct at the Marshall Spaceflight Center in April 1990, where reception was very positive. The engineers flew around the country to present their plan, which generated significant interest. When their tour culminated in a demonstration at the National Space Society they received a standing ovation. The plan gained rapid media attention shortly afterwards.

Resistance to the plan came from teams within NASA working on the Space Station and advanced propulsion concepts. The NASA administration rejected Mars Direct. Zubrin remained committed to the strategy, and after parting with David Baker attempted to convince the new NASA administration of Mars Direct's merits in 1992.

After being granted a small research fund at Martin Marietta, Zubrin and his colleagues successfully demonstrated an in-situ propellant generator which achieved an efficiency of 94%. No chemical engineers partook in the development of the demonstration hardware. After showing the positive results to the Johnson Space Center, the NASA administration still held several reservations about the plan.

In November 2003, Zubrin was invited to speak to the U.S. Senate committee on the future of space exploration. Two months later the Bush administration announced the creation of the Constellation program, a manned spaceflight initiative with the goal of sending humans to the Moon by 2020. While a Mars mission was not specifically detailed, a plan to reach Mars based on utilizing the Orion spacecraft was tentatively developed for implementation in the 2030s. In 2009 the Obama administration began a review of the Constellation program, and after budgetary concerns the program was cancelled in 2010.

There are a variety of psychological and sociological issues that could affect long-duration expeditionary space missions. Early human spaceflight missions to Mars are expected by some to have significant psycho-social problems to overcome, as well as provide considerable data for refining mission design, mission planning, and crew selection for future missions.

Since Mars Direct was initially conceived, it has undergone regular review and development by Zubrin himself, the Mars Society, NASA, Stanford University and others.

Zubrin and Weaver developed a modified version of Mars Direct, called Mars Semi-Direct, in response to some specific criticisms. This mission consists of three spacecraft and includes a "Mars Ascent Vehicle" (MAV). The ERV remains in Mars orbit for the return journey, while the unmanned MAV lands and manufactures propellants for the ascent back up to Mars orbit. The Mars Semi-Direct architecture has been used as the basis of a number of studies, including the NASA Design Reference Missions.

When subjected to the same cost-analysis as the 90-day report, Mars Semi-Direct was predicted to cost 55 billion dollars over 10 years, capable of fitting into the existing NASA budget.

Mars Semi-Direct became the basis of the Design Reference Mission 1.0 of NASA, replacing the Space Exploration Initiative.

The NASA model, referred to as the Design Reference Mission, on version 5.0 as of September 1, 2012, calls for a significant upgrade in hardware (at least three launches per mission, rather than two), and sends the ERV to Mars fully fueled, parking it in orbit above the planet for subsequent rendezvous with the MAV.

With the potentially imminent advent of low-cost heavy lift capability, Zubrin has posited a dramatically lower cost manned Mars mission using hardware developed by space transport company SpaceX. In this simpler plan, a crew of two would be sent to Mars by a single Falcon Heavy launch, the Dragon spacecraft acting as their interplanetary cruise habitat. Additional living space for the journey would be enabled through the use of inflatable add-on modules if required. The problems associated with long-term weightlessness would be addressed in the same manner as the baseline Mars Direct plan, a tether between the Dragon habitat and the TMI (Trans-Mars Injection) stage acting to allow rotation of the craft.

The Dragon's heatshield characteristics could allow for a safe descent if landing rockets of sufficient power were made available. Research at NASA's Ames Research Center has demonstrated that a robotic Dragon would be capable of a fully propulsive landing on the Martian surface. On the surface, the crew would have at their disposal two Dragon spacecraft with inflatable modules as habitats, two ERVs, two Mars ascent vehicles and 8 tonnes of cargo.

The Mars Society and Stanford studies retain the original two-vehicle mission profile of Mars Direct, but increase the crew size to six.

Mars Society Australia developed their own four-person "Mars Oz" reference mission, based on Mars Semi-Direct. This study uses horizontally landing, bent biconic shaped modules, and relies on solar power and chemical propulsion throughout, where Mars Direct and the DRMs used nuclear reactors for surface power and, in the case of the DRMs for propulsion as well. The Mars Oz reference mission also differs in assuming, based on space station experience, that spin gravity will not be required.

The Mars Society has argued the viability of the Mars Habitat Unit concept through their Mars Analogue Research Station program. These are two or three decked vertical cylinders ~8 m in diameter and 8 m high. Mars Society Australia plans to build its own station based on the Mars Oz design. The Mars Oz design features a horizontal cylinder 4.7 m in diameter and 18 m long, with a tapered nose. A second similar module will function as a garage and power and logistics module.

Mars Direct was featured on a Discovery Channel programs "Mars: The Next Frontier" in which issues were discussed surrounding NASA funding of the project, and on "Mars Underground", where the plan is discussed more in-depth.

"Mars to Stay" proposals involve not returning the first immigrant/explorers immediately, or ever. It has been suggested the cost of sending a four or six person team could be one fifth to one tenth the cost of returning that same four or six person team. Depending on the precise approach taken, a quite complete lab could be sent and landed for less than the cost of sending back even 50 kilos of Martian rocks. Twenty or more persons could be sent for the cost of returning four.






</doc>
<doc id="19848" url="https://en.wikipedia.org/wiki?curid=19848" title="Max Planck">
Max Planck

Max Karl Ernst Ludwig Planck, FRS (; 23 April 1858 – 4 October 1947) was a German theoretical physicist whose discovery of energy quanta won him the Nobel Prize in Physics in 1918.

Planck made many contributions to theoretical physics, but his fame as a physicist rests primarily on his role as the originator of quantum theory, which revolutionized human understanding of atomic and subatomic processes. In 1948 the German scientific institution the Kaiser Wilhelm Society (of which Planck was twice president), was renamed the Max Planck Society (MPS). The MPS now includes 83 institutions representing a wide range of scientific directions.

Planck came from a traditional, intellectual family. His paternal great-grandfather and grandfather were both theology professors in Göttingen; his father was a law professor in Kiel and Munich.
Planck was born in Kiel, Holstein, to Johann Julius Wilhelm Planck and his second wife, Emma Patzig. He was baptized with the name of "Karl Ernst Ludwig Marx Planck"; of his given names, "Marx" (a now obsolete variant of "Markus" or maybe simply an error for "Max", which is actually short for "Maximilian") was indicated as the "appellation name". However, by the age of ten he signed with the name "Max" and used this for the rest of his life.

He was the 6th child in the family, though two of his siblings were from his father's first marriage. Among his earliest memories was the marching of Prussian and Austrian troops into Kiel during the Second Schleswig War in 1864. In 1867 the family moved to Munich, and Planck enrolled in the Maximilians gymnasium school, where he came under the tutelage of Hermann Müller, a mathematician who took an interest in the youth, and taught him astronomy and mechanics as well as mathematics. It was from Müller that Planck first learned the principle of conservation of energy. Planck graduated early, at age 17. This is how Planck first came in contact with the field of physics.

Planck was gifted when it came to music. He took singing lessons and played piano, organ and cello, and composed songs and operas. However, instead of music he chose to study physics.
The Munich physics professor Philipp von Jolly advised Planck against going into physics, saying, "in this field, almost everything is already discovered, and all that remains is to fill a few holes." Planck replied that he did not wish to discover new things, but only to understand the known fundamentals of the field, and so began his studies in 1874 at the University of Munich. Under Jolly's supervision, Planck performed the only experiments of his scientific career, studying the diffusion of hydrogen through heated platinum, but transferred to theoretical physics.

In 1877 he went to the Friedrich Wilhelms University in Berlin for a year of study with physicists Hermann von Helmholtz and Gustav Kirchhoff and mathematician Karl Weierstrass. He wrote that Helmholtz was never quite prepared, spoke slowly, miscalculated endlessly, and bored his listeners, while Kirchhoff spoke in carefully prepared lectures which were dry and monotonous. He soon became close friends with Helmholtz. While there he undertook a program of mostly self-study of Clausius's writings, which led him to choose thermodynamics as his field.

In October 1878 Planck passed his qualifying exams and in February 1879 defended his dissertation, "Über den zweiten Hauptsatz der mechanischen Wärmetheorie" ("On the second law of thermodynamics"). He briefly taught mathematics and physics at his former school in Munich.

In June 1880, he presented his habilitation thesis, "Gleichgewichtszustände isotroper Körper in verschiedenen Temperaturen" ("Equilibrium states of isotropic bodies at different temperatures").

With the completion of his habilitation thesis, Planck became an unpaid private lecturer (Privatdozent) in Munich, waiting until he was offered an academic position. Although he was initially ignored by the academic community, he furthered his work on the field of heat theory and discovered one after another the same thermodynamical formalism as Gibbs without realizing it. Clausius's ideas on entropy occupied a central role in his work.

In April 1885 the University of Kiel appointed Planck as associate professor of theoretical physics. Further work on entropy and its treatment, especially as applied in physical chemistry, followed. He published his "Treatise on Thermodynamics" in 1897. He proposed a thermodynamic basis for Svante Arrhenius's theory of electrolytic dissociation.

In 1889 he was named the successor to Kirchhoff's position at the Friedrich-Wilhelms-Universität in Berlin – presumably thanks to Helmholtz's intercession – and by 1892 became a full professor. In 1907 Planck was offered Boltzmann's position in Vienna, but turned it down to stay in Berlin. During 1909, as a University of Berlin professor, he was invited to become the Ernest Kempton Adams Lecturer in Theoretical Physics at Columbia University in New York City. A series of his lectures were translated and co-published by Columbia University professor A. P. Wills. He retired from Berlin on 10 January 1926, and was succeeded by Erwin Schrödinger.

In March 1887 Planck married Marie Merck (1861–1909), sister of a school fellow, and moved with her into a sublet apartment in Kiel. They had four children: Karl (1888–1916), the twins Emma (1889–1919) and Grete (1889–1917), and Erwin (1893–1945).

After the apartment in Berlin, the Planck family lived in a villa in Berlin-Grunewald, Wangenheimstrasse 21. Several other professors of Berlin University lived nearby, among them theologian Adolf von Harnack, who became a close friend of Planck. Soon the Planck home became a social and cultural center. Numerous well-known scientists, such as Albert Einstein, Otto Hahn and Lise Meitner were frequent visitors. The tradition of jointly performing music had already been established in the home of Helmholtz.

After several happy years, in July 1909 Marie Planck died, possibly from tuberculosis. In March 1911 Planck married his second wife, Marga von Hoesslin (1882–1948); in December his fifth child Hermann was born.

During the First World War Planck's second son Erwin was taken prisoner by the French in 1914, while his oldest son Karl was killed in action at Verdun. Grete died in 1917 while giving birth to her first child. Her sister died the same way two years later, after having married Grete's widower. Both granddaughters survived and were named after their mothers. Planck endured these losses stoically.

In January 1945, Erwin, to whom he had been particularly close, was sentenced to death by the Nazi Volksgerichtshof because of his participation in the failed attempt to assassinate Hitler in July 1944. Erwin was executed on 23 January 1945.

As a professor at the Friedrich-Wilhelms-Universität in Berlin, Planck joined the local Physical Society. He later wrote about this time: "In those days I was essentially the only theoretical physicist there, whence things were not so easy for me, because I started mentioning entropy, but this was not quite fashionable, since it was regarded as a mathematical spook". Thanks to his initiative, the various local Physical Societies of Germany merged in 1898 to form the German Physical Society (Deutsche Physikalische Gesellschaft, DPG); from 1905 to 1909 Planck was the president.

Planck started a six-semester course of lectures on theoretical physics, "dry, somewhat impersonal" according to Lise Meitner, "using no notes, never making mistakes, never faltering; the best lecturer I ever heard" according to an English participant, James R. Partington, who continues: "There were always many standing around the room. As the lecture-room was well heated and rather close, some of the listeners would from time to time drop to the floor, but this did not disturb the lecture". Planck did not establish an actual "school"; the number of his graduate students was only about 20, among them:

In 1894 Planck turned his attention to the problem of black-body radiation. He had been commissioned by electric companies to create maximum light from lightbulbs with minimum energy. The problem had been stated by Kirchhoff in 1859: "how does the intensity of the electromagnetic radiation emitted by a black body (a perfect absorber, also known as a cavity radiator) depend on the frequency of the radiation (i.e., the color of the light) and the temperature of the body?". The question had been explored experimentally, but no theoretical treatment agreed with experimental values. Wilhelm Wien proposed Wien's law, which correctly predicted the behaviour at high frequencies, but failed at low frequencies. The Rayleigh–Jeans law, another approach to the problem, created what was later known as the "ultraviolet catastrophe", but contrary to many textbooks this was not a motivation for Planck.

Planck's first proposed solution to the problem in 1899 followed from what Planck called the "principle of elementary disorder", which allowed him to derive Wien's law from a number of assumptions about the entropy of an ideal oscillator, creating what was referred-to as the Wien–Planck law. Soon it was found that experimental evidence did not confirm the new law at all, to Planck's frustration. Planck revised his approach, deriving the first version of the famous Planck black-body radiation law, which described the experimentally observed black-body spectrum well. It was first proposed in a meeting of the DPG on 19 October 1900 and published in 1901. This first derivation did not include energy quantisation, and did not use statistical mechanics, to which he held an aversion. In November 1900, Planck revised this first approach, relying on Boltzmann's statistical interpretation of the second law of thermodynamics as a way of gaining a more fundamental understanding of the principles behind his radiation law. As Planck was deeply suspicious of the philosophical and physical implications of such an interpretation of Boltzmann's approach, his recourse to them was, as he later put it, "an act of despair ... I was ready to sacrifice any of my previous convictions about physics."

The central assumption behind his new derivation, presented to the DPG on 14 December 1900, was the supposition, now known as the Planck postulate, that electromagnetic energy could be emitted only in quantized form, in other words, the energy could only be a multiple of an elementary unit:

where is Planck's constant, also known as Planck's action quantum (introduced already in 1899), and is the frequency of the radiation. Note that the elementary units of energy discussed here are represented by and not simply by . Physicists now call these quanta photons, and a photon of frequency will have its own specific and unique energy. The total energy at that frequency is then equal to multiplied by the number of photons at that frequency.
At first Planck considered that quantisation was only "a purely formal assumption ... actually I did not think much about it..."; nowadays this assumption, incompatible with classical physics, is regarded as the birth of quantum physics and the greatest intellectual accomplishment of Planck's career (Ludwig Boltzmann had been discussing in a theoretical paper in 1877 the possibility that the energy states of a physical system could be discrete). The discovery of Planck's constant enabled him to define a new universal set of physical units (such as the Planck length and the Planck mass), all based on fundamental physical constants upon which much of quantum theory is based. In recognition of Planck's fundamental contribution to a new branch of physics, he was awarded the Nobel Prize in Physics for 1918 (he actually received the award in 1919).

Subsequently, Planck tried to grasp the meaning of energy quanta, but to no avail. "My unavailing attempts to somehow reintegrate the action quantum into classical theory extended over several years and caused me much trouble." Even several years later, other physicists like Rayleigh, Jeans, and Lorentz set Planck's constant to zero in order to align with classical physics, but Planck knew well that this constant had a precise nonzero value. "I am unable to understand Jeans' stubbornness – he is an example of a theoretician as should never be existing, the same as Hegel was for philosophy. So much the worse for the facts if they don't fit."

Max Born wrote about Planck: "He was, by nature, a conservative mind; he had nothing of the revolutionary and was thoroughly skeptical about speculations. Yet his belief in the compelling force of logical reasoning from facts was so strong that he did not flinch from announcing the most revolutionary idea which ever has shaken physics."

In 1905, the three epochal papers by Albert Einstein were published in the journal "Annalen der Physik". Planck was among the few who immediately recognized the significance of the special theory of relativity. Thanks to his influence, this theory was soon widely accepted in Germany. Planck also contributed considerably to extend the special theory of relativity. For example, he recast the theory in terms of classical action.

Einstein's hypothesis of light "quanta" (photons), based on Heinrich Hertz's 1887 discovery (and further investigation by Philipp Lenard) of the photoelectric effect, was initially rejected by Planck. He was unwilling to discard completely Maxwell's theory of electrodynamics. "The theory of light would be thrown back not by decades, but by centuries, into the age when Christiaan Huygens dared to fight against the mighty emission theory of Isaac Newton ..."

In 1910, Einstein pointed out the anomalous behavior of specific heat at low temperatures as another example of a phenomenon which defies explanation by classical physics. Planck and Nernst, seeking to clarify the increasing number of contradictions, organized the First Solvay Conference (Brussels 1911). At this meeting Einstein was able to convince Planck.

Meanwhile, Planck had been appointed dean of Berlin University, whereby it was possible for him to call Einstein to Berlin and establish a new professorship for him (1914). Soon the two scientists became close friends and met frequently to play music together.

At the onset of the First World War Planck endorsed the general excitement of the public, writing that, "Besides much that is horrible, there is also much that is unexpectedly great and beautiful: the smooth solution of the most difficult domestic political problems by the unification of all parties (and) ... the extolling of everything good and noble."

Nonetheless, Planck refrained from the extremes of nationalism. In 1915, at a time when Italy was about to join the Allied Powers, he voted successfully for a scientific paper from Italy, which received a prize from the Prussian Academy of Sciences, where Planck was one of four permanent presidents.

Planck also signed the infamous "Manifesto of the 93 intellectuals", a pamphlet of polemic war propaganda (while Einstein retained a strictly pacifistic attitude which almost led to his imprisonment, being spared by his Swiss citizenship). But in 1915 Planck, after several meetings with Dutch physicist Lorentz, revoked parts of the Manifesto. Then in 1916 he signed a declaration against German annexationism.

In the turbulent post-war years, Planck, now the highest authority of German physics, issued the slogan "persevere and continue working" to his colleagues.

In October 1920 he and Fritz Haber established the "Notgemeinschaft der Deutschen Wissenschaft" (Emergency Organization of German Science), aimed at providing financial support for scientific research. A considerable portion of the money the organization would distribute was raised abroad.

Planck also held leading positions at Berlin University, the Prussian Academy of Sciences, the German Physical Society and the Kaiser Wilhelm Society (which in 1948 became the Max Planck Society). During this time economic conditions in Germany were such that he was hardly able to conduct research. In 1926 Planck became a foreign member of the Royal Netherlands Academy of Arts and Sciences.

During the interwar period, Planck became a member of the Deutsche Volks-Partei (German People's Party), the party of Nobel Peace Prize laureate Gustav Stresemann, which aspired to liberal aims for domestic policy and rather revisionistic aims for politics around the world.

Planck disagreed with the introduction of universal suffrage and later expressed the view that the Nazi dictatorship resulted from "the ascent of the rule of the crowds".

At the end of the 1920s Bohr, Heisenberg and Pauli had worked out the Copenhagen interpretation of quantum mechanics, but it was rejected by Planck, and by Schrödinger, Laue, and Einstein as well. Planck expected that wave mechanics would soon render quantum theory—his own child—unnecessary. This was not to be the case, however. Further work only cemented quantum theory, even against his and Einstein's philosophical revulsions. Planck experienced the truth of his own earlier observation from his struggle with the older views in his younger years: "A new scientific truth does not triumph by convincing its opponents and making them see the light, but rather because its opponents eventually die, and a new generation grows up that is familiar with it."

When the Nazis came to power in 1933, Planck was 74. He witnessed many Jewish friends and colleagues expelled from their positions and humiliated, and hundreds of scientists emigrated from Nazi Germany. Again he tried to "persevere and continue working" and asked scientists who were considering emigration to remain in Germany. Nevertheless, he did help his nephew, the economist Hermann Kranold to emigrate to London after his arrest. He hoped the crisis would abate soon and the political situation would improve.

Otto Hahn asked Planck to gather well-known German professors in order to issue a public proclamation against the treatment of Jewish professors, but Planck replied, "If you are able to gather today 30 such gentlemen, then tomorrow 150 others will come and speak against it, because they are eager to take over the positions of the others." Under Planck's leadership, the Kaiser Wilhelm Society (KWG) avoided open conflict with the Nazi regime, except concerning Fritz Haber. Planck tried to discuss the issue with Adolf Hitler but was unsuccessful. In the following year, 1934, Haber died in exile.

One year later, Planck, having been the president of the KWG since 1930, organized in a somewhat provocative style an official commemorative meeting for Haber. He also succeeded in secretly enabling a number of Jewish scientists to continue working in institutes of the KWG for several years. In 1936, his term as president of the KWG ended, and the Nazi government pressured him to refrain from seeking another term.

As the political climate in Germany gradually became more hostile, Johannes Stark, prominent exponent of Deutsche Physik ("German Physics", also called "Aryan Physics") attacked Planck, Sommerfeld and Heisenberg for continuing to teach the theories of Einstein, calling them "white Jews". The "Hauptamt Wissenschaft" (Nazi government office for science) started an investigation of Planck's ancestry, claiming that he was "1/16 Jewish", but Planck himself denied it.
In 1938, Planck celebrated his 80th birthday. The DPG held a celebration, during which the Max-Planck medal (founded as the highest medal by the DPG in 1928) was awarded to French physicist Louis de Broglie. At the end of 1938, the Prussian Academy lost its remaining independence and was taken over by Nazis ("Gleichschaltung"). Planck protested by resigning his presidency. He continued to travel frequently, giving numerous public talks, such as his talk on Religion and Science, and five years later he was sufficiently fit to climb 3,000-metre peaks in the Alps.

During the Second World War the increasing number of Allied bombing missions against Berlin forced Planck and his wife to temporarily leave the city and live in the countryside. In 1942 he wrote: "In me an ardent desire has grown to persevere this crisis and live long enough to be able to witness the turning point, the beginning of a new rise." In February 1944 his home in Berlin was completely destroyed by an air raid, annihilating all his scientific records and correspondence. His rural retreat was threatened by the rapid advance of the Allied armies from both sides.

In 1945, Planck's son Erwin was arrested following the attempted assassination of Hitler in the 20 July plot. Erwin consequently died at the hands of the Gestapo; his death destroyed much of Max Planck's will to live. After the end of the war Planck, his second wife, and his son by her were brought to a relative in Göttingen, where Planck died on 4 October 1947. His grave is situated in the old Stadtfriedhof (City Cemetery) in Göttingen.

Planck was a member of the Lutheran Church in Germany. However, Planck was very tolerant towards alternative views and religions.
In a lecture in 1937 entitled "Religion und Naturwissenschaft" he suggested the importance of these symbols and rituals related directly with a believer's ability to worship God, but that one must be mindful that the symbols provide an imperfect illustration of divinity. He criticized atheism for being focused on the derision of such symbols, while at the same time warned of the over-estimation of the importance of such symbols by believers.

He was favorable to all religions, but he himself chose Christianity. He did, however, regret the Church's demands for unquestioning belief, which served to repel questioners. For example, he believed "the faith in miracles must yield, step by step, before the steady and firm advance of the facts of science, and its total defeat is undoubtedly a matter of time." 

In his 1937 lecture "Religion and Naturwissenschaft," Planck expressed the view that God is everywhere present, and held that "the holiness of the unintelligible Godhead is conveyed by the holiness of symbols." Atheists, he thought, attach too much importance to what are merely symbols. Planck was a churchwarden from 1920 until his death, and believed in an almighty, all-knowing, beneficent God (though not necessarily a personal one). Both science and religion wage a "tireless battle against skepticism and dogmatism, against unbelief and superstition" with the goal "toward God!"

Max Planck said in 1944, "As a man who has devoted his whole life to the most clear headed science, to the study of matter, I can tell you as a result of my research about atoms this much: There is no matter as such. All matter originates and exists only by virtue of a force which brings the particle of an atom to vibration and holds this most minute solar system of the atom together. We must assume behind this force the existence of a conscious and intelligent mind. This mind is the matrix of all matter."

Planck regarded the scientist as a man of imagination and Christian faith. He said: "Both religion and science require a belief in God. For believers, God is in the beginning, and for physicists He is at the end of all considerations… To the former He is the foundation, to the latter, the crown of the edifice of every generalized world view".

On the other hand, Planck wrote, "...'to believe' means 'to recognize as a truth,' and the knowledge of nature, continually advancing on incontestably safe tracks, has made it utterly impossible for a person possessing some training in natural science to recognize as founded on truth the many reports of extraordinary occurrences contradicting the laws of nature, of miracles which are still commonly regarded as essential supports and confirmations of religious doctrines, and which formerly used to be accepted as facts pure and simple, without doubt or criticism. The belief in miracles must retreat step by step before relentlessly and reliably progressing science and we cannot doubt that sooner or later it must vanish completely."

Later in life, Planck's views on God were that of a deist. For example, six months before his death a rumour started that Planck had converted to Catholicism, but when questioned what had brought him to make this step, he declared that, although he had always been deeply religious, he did not believe "in a personal God, let alone a Christian God."






</doc>
<doc id="19849" url="https://en.wikipedia.org/wiki?curid=19849" title="March 30">
March 30





</doc>
<doc id="19852" url="https://en.wikipedia.org/wiki?curid=19852" title="Madhuri Dixit">
Madhuri Dixit

Madhuri Dixit Nene (born 15 May 1967) is an Indian actress who is known for her work in Hindi cinema. She has been praised by critics for her acting and dancing skills. She has received six Filmfare Awards, four for Best Actress, one for Best Supporting Actress and one special award for her contribution to the film industry. She has been nominated for the Filmfare Award for Best Actress a record fourteen times, and along with Raakhee is the most nominated actress in the acting categories, with 16 nominations. She was awarded the Padma Shri, India's fourth-highest civilian award, by the Government of India in 2008.

Dixit made her film debut in "Abodh" (1984) and received wider public recognition with "Tezaab" (1988). She went on to establish herself as a leading actress of Hindi cinema with the films "Ram Lakhan" (1989), "Dil" (1990), "Saajan" (1991), "Beta" (1992), "Khalnayak" (1993), "Hum Aapke Hain Koun..!" (1994), "Raja" (1995) and "Dil To Pagal Hai" (1997). She garnered critical acclaim for her work in the dramas "Anjaam" (1994), "Mrityudand" (1997), "Pukar" (2000), "Lajja" (2001) and "Devdas" (2002). In 2002, she took a sabbatical from films to raise her children and made a comeback with the musical "Aaja Nachle" in 2007, later starring in the 2014 dramas "Dedh Ishqiya" and "Gulaab Gang". Her first role in Marathi cinema came with the 2018 comedy drama "Bucket List".

In addition to acting in films, Dixit has been featured as a talent judge for four seasons of the dance reality shows "Jhalak Dikhhla Jaa" and "Dance Deewane". She has participated in several stage shows and has engaged in philanthropic activities. Since 1999, she is married to Doctor Shriram Madhav Nene, with whom she has two children.

Dixit is a native of Mumbai, India. She was born on 15 May 1967 in a Maharashtrian family to Shankar and Snehlata Dixit. She had always desired dance since the age of 3 and went on to practice the art of Kathak and later on to become a trained professional Kathak dancer. 

Dixit attended Divine Child High School and joined Sathaye college (Vile Parle) where she studied micro biology as one of her subjects in BSc. However, as she narrated in a 2018 interview with AIB, six months after she had commenced her BSc course, she decided to discontinue studies and work in films full time.

Dixit made her acting debut in Rajshri Productions' movie "Abodh" in 1984 opposite the Bengali actor, Tapas Paul. The film failed to do well but Dixit earned positive reviews from critics. Aakash Barvalia on Gomolo praised her as he wrote "Madhuri excels in her role as a young bride who acquits herself well as the naive village girl and does not realize what marriage actually entails." Her only release of 1985 was "Awara Baap" which turned out to be another failure for Dixit. During this time, a monochrome photograph of hers, shot by Gautam Rajadhyaksha was featured on the cover of the then popular magazine "Debonair". She had already appeared as the cover girl of Film Fare in April 1986.

In 1986, Dixit starred in two movies, "Swati" and "Manav Hatya" but both films flopped at the box office. Dixit had three releases in 1987 with "Mohre", "Hifazat" and "Uttar Dakshin" but her films continued to flop at the box office. Among Dixit's three releases of 1988, the first of them - and "Khatron Ke Khiladi" too failed to do well commercially.

In 1988, Dixit finally attained recognition after a series of consecutive flops when she landed the lead female role of Mohini in N. Chandra's romantic thriller "Tezaab" opposite Anil Kapoor. The film went on to become that year's highest-grossing movie. It was Dixit's first commercial success and her earliest of several collaborations with Kapoor. Akshay Shah of Planet Bollywood praised Dixit for her dancing skills and her acting. He wrote "Mahduri Dixit gives a fine-tuned performance. Though she is more remembered for her crowd pleasing dance act "Ek Do Teen", her acting needs to be noted, especially in the scenes where she is pitted against Anupam Kher. She looks and dances like a dream." Dixit received her first nomination for the Filmfare Best Actress Award for her work in "Tezaab".

In 1989, her first release was Subhash Ghai's drama "Ram Lakhan", in which she was paired once again with Anil Kapoor. The film became a super-hit and went on to become the second highest-grossing film of the year. Dixit's next release, the critically acclaimed romantic drama "Prem Pratigyaa", in which she starred opposite Mithun Chakraborty, failed to do well commercially. However, critics were appreciative of her portrayal and she received her second nomination for the Filmfare Best Actress Award. Thereafter Dixit got success in Rajiv Rai's multi-starrer action thriller "Tridev", in which she starred alongside Naseeruddin Shah, Jackie Shroff, Sunny Deol, Sangeeta Bijlani, Sonam and Amrish Puri. Dixit was paired with Deol. The movie became the year's third highest-grossing film. Dixit continued her success with her fourth pairing with Anil Kapoor in Vidhu Vinod Chopra's drama "Parinda". Although Dixit's role was small, she received praise for her work. Aniket Joshi has praised her as he wrote "Madhuri Dixit, the only female character in the film, has a small role as Paro (Karan´s love interest and Prakash´s sister) yet still does well in the few scenes she has."

In 1990, Dixit starred in the romantic drama "Dil" opposite Aamir Khan. She played the role of a rich and arrogant girl who falls in love with a poorer boy and later leaves her home to marry him. The film became the biggest box office hit of the year in India and Dixit's performance earned her the first Filmfare Best Actress Award of her career. She was also seen in the dramas "Sailaab" opposite Aditya Pancholi and "Kishen Kanhaiya" opposite Anil Kapoor; the former flopped while the latter went on to become the third biggest hit of the year. After the success of "Dil", Dixit became one of the leading actresses of Bollywood. The following year, she starred in the super-hit movie "Saajan" opposite Salman Khan and Sanjay Dutt. This became the year's highest-grossing movie. Dixit's performance earned her the fourth nomination for the Filmfare Best Actress Award.

In 1992, Dixit featured in the film "Beta" opposite Anil Kapoor. Dixit's performance of Saraswati earned her critical acclaim. Upperstall.com praised Dixit's work: "She made Beta her own film totally stealing a march over the film's hero, Anil Kapoor. While the highlight of the film was undoubtedly Dhak Dhak Karne Laga – probably the sexiest and most popular dance of Madhuri's career, her blazing performance – that of a woman married to an illiterate, well-meaning man and who exposes her scheming mother-in-law whom her husband dotes on – had film reviewers gushing with several people jokingly saying the film should have been called Beti instead!" Dixit's performance in "Beta" won her a second Filmfare Best Actress Award. Following the film's success, Dixit became famously known as the "Dhak Dhak Girl" for her performance in the famous song "Dhak Dhak Karne Laga".

In 1993, Dixit starred in the controversial super-hit "Khalnayak" opposite Sanjay Dutt and Jackie Shroff. Her portrayal of the police officer Ganga earned her critical acclaim. Shahid Khan of Planet Bollywood praised Dixit's dancing and acting skills in the movie: "In any other film, the heroine would not have much to do with a plot like this. So immense is the star-power of Madhuri Dixit that Subhash Ghai actually carved out a solid role for her in this macho tale of a bad boy and his shenanigans. She gives real competition to Sanjay Dutt. The audiences ran in droves to witness the Madhuri Dixit magic. In every scene, she holds you captive with her stunning beauty, demanding screen presence, expressive acting and excellent dancing skills. Saroj Khan’s dance choreography is entertaining to watch, particularly in "Choli Ke Peeche Kya Hai". Madhuri’s entrance is a riot with a prison inmate asking her, 'Kaisi aurat hai tu? (What kind of a woman are you?)' and she responds by slapping her and saying, 'Aisi aurat hoon main! (This is the kind of woman I am!)'". Dixit's performance in "Khalnayak" earned her a sixth nomination for the Filmfare Best Actress Award and became the year's second highest grossing film. In 1994, Dixit starred in "Anjaam", being paired with Shahrukh Khan for the first time. Dixit's performance of a revenge-seeking wife and mother earned her a seventh nomination for the Filmfare Best Actress Award. Dixit was praised for her performance, though the film was an average performer at the box office. Her next starring role was in the film "Hum Aapke Hain Koun..!" opposite Salman Khan. The film became one of the biggest hits in the history of Hindi cinema and made 1.35 billion worldwide, breaking the record of the film "Sholay" (1975). It became the highest grossing Bollywood film in Hindi cinema history after its theatrical run and held the record for 7 years till the release of "" (2001). Dixit's role of a modern yet family-oriented young girl fetched her the third Filmfare Best Actress Award of her career. Critics believed the film to be "too sweet" but appreciated Dixit's performance as she mesmerised on the screen. In the Millennium Edition of the "Guinness Book of World Records", "Hum Aapke Hain Kaun" became the Bollywood's highest-grossing film. Upperstall.com wrote "Madhuri Dixit, reunited with the banner that launched her, stands out and is undoubtedly the life of the film as Nisha. Be it the mischievous but strong-minded, independent girl who gives it to Prem as good as she gets or the woman willing to sacrifice her love believing she is doing the right thing, Madhuri is absolutely spot on creating one of the more memorable female characters of Hindi Cinema."

1995 saw Dixit star in the hit "Raja" opposite Sanjay Kapoor. The film became the fourth highest grosser of the year and its success was attributed to her immense popularity. Her next release was David Dhawan's "Yaraana" opposite Rishi Kapoor, in which she played a dancer on the run from her abusive lover. This film was a below average performer at the box office. Both films earned her nominations for the Filmfare Best Actress Award.

Dixit's career went into a slump during 1996 as films like "Prem Granth" and "Rajkumar" flopped at the box office. In 1997, Dixit bounced back with Prakash Jha's "Mrityudand", in which she performed opposite Shabana Azmi and Shilpa Shirodkar. Dixit played the role of Ketki, one of the three troubled women and earned immense critical acclaim for her performance. Upperstall.com wrote "Madhuri gives a wonderful, insightful performance as the self-respecting educated woman married into a family of male chauvinistic upper class landlords and who is humiliated by her husband when she objects to his misdeeds, never hitting a false note. She more than matches up to seasoned artists like Shabana and Om Puri in their scenes together. In a word, she is mesmerizing."

Dixit starred in Yash Chopra's musical romance "Dil To Pagal Hai" (1997), opposite Shahrukh Khan and Karisma Kapoor. Her role of a woman faced with a moral dilemma in a love triangle fetched her the fourth Filmfare Best Actress Award. The film also proved to be a success at the box office. However, her career then went to a decline with films like "Wajood" (1998) and "Aarzoo" (1999) sinking at the box office.

In 2000, Dixit starred in Rajkumar Santoshi's "Pukar" opposite Anil Kapoor. A love story based on the backdrop of the Indian Army, the film was shot over the course of 350 days. Dixit's portrayal of a negative role in it earned her the twelfth nomination for the Filmfare Best Actress Award. She then played the title character in "Gaja Gamini", the first feature film directed by painter M. F. Husain. Hussain was fixated with Dixit, and watched her "Hum Aapke Hain Koun..!" several times, and was certain that he would make a film only with her. The film followed the story of Gaja Gamini, who appears in various incarnations as Mona Lisa, Shakuntala and others. "Pukar" was an average grosser, while the latter bombed at the box office.

In 2001, Dixit starred in Deepak Shivdasani's love triangle "Yeh Raaste Hain Pyaar Ke" opposite Ajay Devgan and Preity Zinta. Dixit followed up by reuniting with Rajkumar Santoshi in the multi-starrer "Lajja" (2001). Dealing with the issue of gender inequality, Dixit played the role of a stage player in it. Anita Bora of Rediff.com wrote: "Madhuri slips into her role as Janaki..with consummate ease..and..dazzles us with a class act." The film was a box-office failure in India but was an overseas success. Dixit was nominated for the Filmfare Award for Best Supporting Actress for "Lajja". In 2002, Dixit starred in a lead role in the love triangle film "Hum Tumhare Hain Sanam" opposite Shahrukh Khan and Salman Khan. A remake of director K. S. Adhiyaman's own Tamil film "Thotta Chinungi" (1995), the film took six years in the making, with huge sabbaticals in between shoots due to several production problems. Few critics noted that the delay made the film look outdated.

Dixit's next release was Sanjay Leela Bhansali's period romance "Devdas", co-starring Shahrukh Khan and Aishwarya Rai. It was based on the Sharat Chandra Chattopadhyay's novel of the same name. She portrayed Chandramukhi, a courtesan who is in love with the title character. Sita Menon of Rediff.com wrote: "The most understated role and perhaps the one that is most lingering, in terms of virtuosity, is that played by Madhuri Dixit. As Chandramukhi, she is simply stunning, lending passion, fire and gentleness with such consummate ease that watching her perform is sheer delight." The film was screened at the 2002 Cannes Film Festival and was featured by "Time" in their listing of the "10 best films of the millennium". The film emerged as a major international success with revenues of over . "Devdas" was chosen as India's official entry for the Academy Award for Best Foreign Language Film and received a nomination for the BAFTA Award for Best Film Not in the English Language. Dixit won the Filmfare Best supporting actress award for the film. It turned out to be her last film for a while as she shifted to Denver with her family.

The following year a film named after her, "Main Madhuri Dixit Banna Chahti Hoon", was released in which a woman (played by Antara Mali) aspires to become the new Madhuri Dixit by trying her luck in Bollywood. Dixit also made her first appearance on television as a host for the India's first reality show "Kahin Na Kahin Koi Hai" on Sony TV.

In 2006, Dixit returned to India with her family, and acted in cinematographer Anil Mehta's dance film "Aaja Nachle" (2007). It was her first release in five years, where she played the role of a choreographer. The film was a box office failure, but Dixit's role was appreciated by critics. Rajeev Masand of CNN-IBN criticised the plot, while he wrote about Dixit's performance: "It’s hard to take your eyes off the screen when she’s up there, dazzling you with her spontaneity, her easy charm and her 100-watt smile." Her performance earned her the nomination for the Filmfare Best Actress Award.

In 2011, Dixit was felicitated with the Filmfare Special Award for completing 25 years in the film industry.

Dixit returned to India in 2012, and made her acting comeback after seven years with the black comedy "Dedh Ishqiya" (2014). It was a sequel to "Ishqiya" (2010), where she was cast as con-woman "Begum Para" opposite Naseeruddin Shah, Arshad Warsi and Huma Qureshi. Dixit expressed that she agreed to do the film because of the "unapologetic way" its director Abhishek Chaubey presented Vidya Balan’s character" in "Ishqiya". The film opened to positive response from critics who called it "one of the year's most important releases". Anupama Chopra called Dixit 
"compelling", while Deepanjana Pal of Firstpost wrote "She's still capable of keeping an audience glued to their seats when the credits start rolling, all because she's dancing on screen.". The film earned her the fourteenth nomination for Filmfare Award for Best Actress.

Her next release of the year was the debutant director Soumik Sen's "Gulaab Gang", alongside Juhi Chawla. Dixit portrayed Rajjo, the leader of a women's activist group. The film and her role were inspired by the real vigilante activist Sampat Pal Devi and her group Gulabi Gang. Pal filed a case against the film claiming that the makers did not take permission to make a film on her life, but the court later lifted the stay from the film. 

To prepare for her role, Dixit practised Shaolin Kung fu, stick training, and close combat. 

"Gulaab Gang" failed at the box office, earning mixed reviews. Subhash K. Jha labelled Dixit's performance and demeanour "inconsistent". However, Sampat Pal claimed that in Dixit's character she finds a "reflection of her own life so stark" that it makes her feel "it was she on screen".

Dixit is not only known for her acting skills, but for her dancing skills as well. Often referred to as the ""Dhak Dhak Girl"", she is famous for her dance in the song "Dhak Dhak Karne Laga" from the film "Beta". She is also known for her dance sequences accompanying Bollywood songs such as "Ek Do Teen" (from "Tezaab"), "Humko Aaj Kal Hai" (from "Sailaab"), "Bada Dukh Deenha" (from "Ram Lakhan"), "Chane Ke Khet Mein" (from "Anjaam"), "Didi Tera Devar Deewana" (from "Hum Aapke Hain Koun..!"), "Choli Ke Peechhe" (from "Khalnayak"), "Akhiyan Milaun" (from "Raja"), "Mera Piya Ghar Aaya" (from "Yaraana"), "Kay Sera Sera" (from "Pukar"), "Badi Mushkil" (from "Lajja"), "Maar Daala" (from "Devdas"), "Aaja Nachle" (from "Aaja Nachle"), and "Ghagra" (from "Yeh Jawaani Hai Deewani").

Pandit Birju Maharaj, a Kathak dancer, who choreographed Dixit in the film "Devdas" for the song "Kaahe Chhed", calls her "the best Bollywood dancer due to her versatility." Film critic Raja Sen describes Dixit as "the industry's numero uno in every sense." Further he elaborates, "She is an exemplary dancer. From Kathak to Dhak Dhak, she's done it all and wowed us every step of the way." Saroj Khan, who choreographed most of Dixit's early hits, after working with her again in "Gulaab Gang" called her the "choreographer's delight".

Dixit is a frequent performer and has participated in several world tours and concerts.

In mid-2008, Madhuri, Aishwarya Rai, Abhishek Bachchan, Amitabh Bachchan, Preity Zinta and Ritesh Deshmukh starred in the "Unforgettable World Tour" stage production. The tour covered the USA, Canada, Trinidad, and London, England.

In 2013, Dixit joined the fourth installment of Shah Rukh Khan's Temptation tour "Temptation Reloaded" where they visited and performed a series of concerts in Auckland, Perth, Sydney and Dubai; it also featured Rani Mukerji, Deepika Padukone, Jacqueline Fernandez, Honey Singh and Meiyang Chang.

Dixit opened her own online dance academy "Dance With Madhuri", where her fans have a one-on-one lesson to learn some of her most famous and well known dance numbers.

In May–June 2015 the Tamil Nadu Consumer's Forum sent her notices for "false representation" in advertisements of Maggi, a noodle brand in which toxic levels of lead were found. She continued endorsing the safety of the product on Twitter, even when food regulators had already found more than 17 times the permissible limits of lead and the product was banned.

On June 5, 2015, Food Safety and Standards Authority of India (FSSAI) orders banned all nine variants of Maggi instant noodles from India, terming them "unsafe and hazardous" for human consumption. It was also found that the "health oats noodles", a product which had not even completed the mandatory "process of risk assessment", was promoted by the actress.

In 2001 Dixit won on Kaun Banega Crorepati, a game show then in its first season on the air. She directed that her winnings be donated for the welfare of victims of an earthquake in Gujarat and to an orphanage in Pune.

In 2009 Dixit performed for NDTV Toyota Greenathon—India's first-ever nationwide campaign for saving the environment and creating awareness about environmental issues. NDTV organised India's first 24-hour live telethon, a fund-raising event that brings in people to donate money to support TERI's initiative—Lighting a Billion Lives which aims at providing solar power to villages without electricity. Dixit became a part of this great social cause, and she performed her hit numbers on the live show.

On 3 February 2011 Dixit spent an evening with 75 orphanage kids of farmers at an ashram in Trimbakeshwar and participated in the birthdays of two children: Hrishikesh and Rani. "We artists are ready to help such children. People from the higher society should come forward and stand firmly behind them," she said on the occasion. Dixit is a Goodwill Ambassador and a patron for "Emeralds for Elephants" — a charity project for the conservation of Asian elephants and other endangered species. This project has been designed to create awareness and raise vital funds for the protection of the critically endangered Asian elephant. A collaborative project between the World Land Trust (a UK based nonprofit environmental organisation) and the Wildlife Trust of India that is creating protected wildlife corridors connecting National Parks and protected areas to others. Speaking about the issue she said: "Elephants are one of my favourite animals and I love them. So what we need to do today is to see how we can preserve our animals. I feel very strongly about this."

On 4 February 2012 Madhuri Dixit interacted with Cancer affected children on World Cancer Day which was organised by Pawan Hans Helicopters Ltd at Juhu, Mumbai.On World Cancer Day Pawan Hans Helicopters Limited offered a Free Joy Ride to Cancer affected children with the support of Cancer Patients Aid Association.

She also participated in the Hiru Golden Film Awards 2016 in Sri Lanka as a special guest along with Bollywood actors such as Sunil Shetty, Neil Nitin Mukesh, Jackie Shroff, and actresses Sridevi, and Karishma Kapoor.

In the year 1985, she appeared in the Rajshri Production's TV show "Paying Guest". She came into the first episode of the show as Neena.

In 2001 on the game show "Kaun Banega Crorepati"s first season, hosted by Amitabh Bachchan, she won that she donated for people affected in natural calamities. She also made an appearance in season 4's Grand Finale that aired in 2010. In 2002, Dixit hosted a TV show "Kahina Kahi Koi Hai" which was a match-making show that aired on Sony Entertainment.

She also made appearances on "Koffee with Karan" in Seasons two and three that aired in 2007 and 2011, respectively. Her Season 2 appearance was a solo appearance and in Season 3 she shared the chat show with Sonakshi Sinha. In 2014, Madhuri made her third appearance on Koffee with Karan and on its fourth season alongside Juhi Chawla.

Dixit has been a guest on "Nach Baliye" Season 1 and 3, a celebrity-couple dancing competition in 2005 (aired on Star One) & 2007 (aired on Star Plus).

In 2011, she appeared as a judge on the dance reality show "Jhalak Dikhhla Jaa 4" along with other co-judges Remo D'souza and Malaika Arora Khan.

In 2012 and 2013, Dixit rejoined "Jhalak Dikhhla Jaa 5" and "Jhalak Dikhhla Jaa 6" along with her co-judges Remo D'souza and Karan Johar.

Madhuri also appeared on Indias Got Talent as a guest judge on two separate occasions for the promotion of her upcoming movies Dedh Ishqiya and Gulaab Gang that released in early 2014.

Later on in 2014, Jhalak Dikhhla Jaa was renewed for a new season, "Jhalak Dikhhla Jaa 7" where Madhuri Dixit returned to the judging panel for her fourth time with her co-judges Remo D'souza and Karan Johar.

During the making of "Saajan" in 1990, Madhuri had an affair with Sanjay Dutt who was married at the time. Despite her father's objections, the relationship continued. However, with Dutt's arrest under TADA in 1993, Madhuri ended the relationship. Talking about this break-up, film journalist Bharati Pradhan reported in "The Telegraph" that "Madhuri had always been a clean player. A Dutt-Dixit alliance would’ve been disastrous and she was too sensible not to see it."

On 17 October 1999, Dixit married Shriram Madhav Nene, a cardiovascular surgeon from Los Angeles, California. They have two sons, Arin (b. March 2003) and Rayaan (b. March 2005). After her marriage, Dixit relocated to Denver, Colorado, for over a decade. She moved back to Mumbai with her family in October 2011.

Dixit has been the Muse for the Indian painter M. F. Husain. He was so fascinated by Dixit in "Hum Aapke Hain Koun..!" that he watched the film 67 times, and in 2007 he booked an entire theatre to see her comeback movie "Aaja Nachle". He made a series of paintings of her, and in 2000 made a film named "Gaja Gamini" starring her, which was intended as a tribute to Dixit herself.



</doc>
