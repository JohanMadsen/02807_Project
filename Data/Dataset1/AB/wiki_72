<doc id="21272" url="https://en.wikipedia.org/wiki?curid=21272" title="Neutron">
Neutron

The neutron is a subatomic particle, symbol or , with no net electric charge and a mass slightly larger than that of a proton. Protons and neutrons constitute the nuclei of atoms. Since protons and neutrons behave similarly within the nucleus, and each has a mass of approximately one atomic mass unit, they are both referred to as nucleons. Their properties and interactions are described by nuclear physics.

The chemical and nuclear properties of the nucleus are determined by the number of protons, called the atomic number, and the number of neutrons, called the neutron number. The atomic mass number is the total number of nucleons. For example, carbon has atomic number 6, and its abundant carbon-12 isotope has 6 neutrons, whereas its rare carbon-13 isotope has 7 neutrons. Some elements occur in nature with only one stable isotope, such as fluorine. Other elements occur with many stable isotopes, such as tin with ten stable isotopes.

Within the nucleus, protons and neutrons are bound together through the nuclear force. Neutrons are required for the stability of nuclei, with the exception of the single-proton hydrogen atom. Neutrons are produced copiously in nuclear fission and fusion. They are a primary contributor to the nucleosynthesis of chemical elements within stars through fission, fusion, and neutron capture processes.

The neutron is essential to the production of nuclear power. In the decade after the neutron was discovered by James Chadwick in 1932, neutrons were used to induce many different types of nuclear transmutations. With the discovery of nuclear fission in 1938, it was quickly realized that, if a fission event produced neutrons, each of these neutrons might cause further fission events, etc., in a cascade known as a nuclear chain reaction. These events and findings led to the first self-sustaining nuclear reactor (Chicago Pile-1, 1942) and the first nuclear weapon (Trinity, 1945).

Free neutrons, while not directly ionizing atoms, cause ionizing radiation. As such they can be a biological hazard, depending upon dose. A small natural "neutron background" flux of free neutrons exists on Earth, caused by cosmic ray showers, and by the natural radioactivity of spontaneously fissionable elements in the Earth's crust. Dedicated neutron sources like neutron generators, research reactors and spallation sources produce free neutrons for use in irradiation and in neutron scattering experiments.

Atomic nuclei are formed by a number of protons, Z the atomic number, and a number of neutrons, N the neutron number, bound together by the nuclear force. The atomic number defines the chemical properties of the atom, and the neutron number determines the isotope or nuclide. The terms isotope and nuclide are often used synonymously, but they refer to chemical and nuclear properties, respectively. Strictly speaking, isotopes are two or more nuclides with the same number of protons; nuclides with the same number of neutrons are called isotones. The atomic mass number, symbol A, equals Z+N. Nuclides with the same atomic mass number are called isobars. The nucleus of the most common isotope of the hydrogen atom (with the chemical symbol H) is a lone proton. The nuclei of the heavy hydrogen isotopes deuterium (D or H) and tritium (T or H) contain one proton bound to one and two neutrons, respectively. All other types of atomic nuclei are composed of two or more protons and various numbers of neutrons. The most common nuclide of the common chemical element lead, Pb, has 82 protons and 126 neutrons, for example. The table of nuclides comprises all the known nuclides. Even though it is not a chemical element, the neutron is included in this table.

The free neutron has a mass of 939,565,413.3 eV/c, or , or . The neutron has a mean square radius of about , or 0.8 fm, and it is a spin-½ fermion.
The neutron has no measurable electric charge. With its positive electric charge, the proton is directly influenced by electric fields, whereas the neutron is unaffected by electric fields. The neutron has a magnetic moment, however, so the neutron is influenced by magnetic fields. The neutron's magnetic moment has a negative value, because its orientation is opposite to the neutron's spin.

A free neutron is unstable, decaying to a proton, electron and antineutrino with a mean lifetime of just under 15 minutes (). This radioactive decay, known as beta decay, is possible because the mass of the neutron is slightly greater than the proton. The free proton is stable. Neutrons or protons bound in a nucleus can be stable or unstable, however, depending on the nuclide. Beta decay, in which neutrons decay to protons, or vice versa, is governed by the weak force, and it requires the emission or absorption of electrons and neutrinos, or their antiparticles.
Protons and neutrons behave almost identically under the influence of the nuclear force within the nucleus. The concept of isospin, in which the proton and neutron are viewed as two quantum states of the same particle, is used to model the interactions of nucleons by the nuclear or weak forces. Because of the strength of the nuclear force at short distances, the binding energy of nucleons is more than seven orders of magnitude larger than the electromagnetic energy binding electrons in atoms. Nuclear reactions (such as nuclear fission) therefore have an energy density that is more than ten million times that of chemical reactions. Because of the mass–energy equivalence, nuclear binding energies add or subtract from the mass of nuclei. Ultimately, the ability of the nuclear force to store energy arising from the electromagnetic repulsion of nuclear components is the basis for most of the energy that makes nuclear reactors or bombs possible. In nuclear fission, the absorption of a neutron by a heavy nuclide (e.g., uranium-235) causes the nuclide to become unstable and break into light nuclides and additional neutrons. The positively charged light nuclides then repel, releasing electromagnetic potential energy.

The neutron is classified as a "hadron", because it is a composite particle made of quarks. The neutron is also classified as a "baryon", because it is composed of three valence quarks. The finite size of the neutron and its magnetic moment indicates that the neutron is a composite particle, as opposed to being an elementary particle. A neutron contains two down quarks with charge − "e" and one up quark with charge + "e".

Like protons, the quarks of the neutron are held together by the strong force, mediated by gluons. The nuclear force results from secondary effects of the more fundamental strong force.

The story of the discovery of the neutron and its properties is central to the extraordinary developments in atomic physics that occurred in the first half of the 20th century, leading ultimately to the atomic bomb in 1945. In the 1911 Rutherford model, the atom consisted of a small positively charged massive nucleus surrounded by a much larger cloud of negatively charged electrons. In 1920, Rutherford suggested that the nucleus consisted of positive protons and neutrally-charged particles, suggested to be a proton and an electron bound in some way. Electrons were assumed to reside within the nucleus because it was known that beta radiation consisted of electrons emitted from the nucleus. Rutherford called these uncharged particles "neutrons", by the Latin root for "neutralis" (neuter) and the Greek suffix "-on" (a suffix used in the names of subatomic particles, i.e. "electron" and "proton"). References to the word "neutron" in connection with the atom can be found in the literature as early as 1899, however.

Throughout the 1920s, physicists assumed that the atomic nucleus was composed of protons and "nuclear electrons" but there were obvious problems. It was difficult to reconcile the proton–electron model for nuclei with the Heisenberg uncertainty relation of quantum mechanics. The Klein paradox, discovered by Oskar Klein in 1928, presented further quantum mechanical objections to the notion of an electron confined within a nucleus. Observed properties of atoms and molecules were inconsistent with the nuclear spin expected from the proton–electron hypothesis. Since both protons and electrons carry an intrinsic spin of ½ "ħ", there is no way to arrange an odd number of spins ±½ "ħ" to give a spin integer multiple of "ħ". Nuclei with integer spin are common, e.g., N.

In 1931, Walther Bothe and Herbert Becker found that if alpha particle radiation from polonium fell on beryllium, boron, or lithium, an unusually penetrating radiation was produced. The radiation was not influenced by an electric field, so Bothe and Becker assumed it was gamma radiation. The following year Irène Joliot-Curie and Frédéric Joliot-Curie in Paris showed that if this "gamma" radiation fell on paraffin, or any other hydrogen-containing compound, it ejected protons of very high energy. Neither Rutherford nor James Chadwick at the Cavendish Laboratory in Cambridge were convinced by the gamma ray interpretation. Chadwick quickly performed a series of experiments that showed that the new radiation consisted of uncharged particles with about the same mass as the proton. These particles were neutrons. Chadwick won the Nobel Prize in Physics for this discovery in 1935.

Models for atomic nucleus consisting of protons and neutrons were quickly developed by Werner Heisenberg and others. The proton–neutron model explained the puzzle of nuclear spins. The origins of beta radiation were explained by Enrico Fermi in 1934 by the process of beta decay, in which the neutron decays to a proton by "creating" an electron and a (as yet undiscovered) neutrino. In 1935 Chadwick and his doctoral student Maurice Goldhaber, reported the first accurate measurement of the mass of the neutron.

By 1934, Fermi had bombarded heavier elements with neutrons to induce radioactivity in elements of high atomic number. In 1938, Fermi received the Nobel Prize in Physics ""for his demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons"". In 1938 Otto Hahn, Lise Meitner, and Fritz Strassmann discovered nuclear fission, or the fractionation of uranium nuclei into light elements, induced by neutron bombardment. In 1945 Hahn received the 1944 Nobel Prize in Chemistry ""for his discovery of the fission of heavy atomic nuclei."" The discovery of nuclear fission would lead to the development of nuclear power and the atomic bomb by the end of World War II.

Under the Standard Model of particle physics, the only possible decay mode for the neutron that conserves baryon number is for one of the neutron's quarks to change flavour via the weak interaction. The decay of one of the neutron's down quarks into a lighter up quark can be achieved by the emission of a W boson. By this process, the Standard Model description of beta decay, the neutron decays into a proton (which contains one down and two up quarks), an electron, and an electron antineutrino.

Since interacting protons have a mutual electromagnetic repulsion that is stronger than their attractive nuclear interaction, neutrons are a necessary constituent of any atomic nucleus that contains more than one proton (see diproton and neutron–proton ratio). Neutrons bind with protons and one another in the nucleus via the nuclear force, effectively moderating the repulsive forces between the protons and stabilizing the nucleus.

Outside the nucleus, free neutrons are unstable and have a mean lifetime of (about 14 minutes, 42 seconds); therefore the half-life for this process (which differs from the mean lifetime by a factor of ) is (about 10 minutes, 11 seconds). Beta decay of the neutron, described above, can be denoted by the radioactive decay:

where , , and denote the proton, electron and electron antineutrino, respectively.
For the free neutron the decay energy for this process (based on the masses of the neutron, proton, and electron) is 0.782343 MeV. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at 0.782 ± 0.013 MeV. The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy) as well as neutrino mass is constrained by many other methods.

A small fraction (about one in 1000) of free neutrons decay with the same products, but add an extra particle in the form of an emitted gamma ray:

This gamma ray may be thought of as a sort of "internal bremsstrahlung" that arises as the emitted beta particle interacts with the charge of the proton in an electromagnetic way. Internal bremsstrahlung gamma ray production is also a minor feature of beta decays of bound neutrons (as discussed below).

A very small minority of neutron decays (about four per million) are so-called "two-body (neutron) decays", in which a proton, electron and antineutrino are produced as usual, but the electron fails to gain the 13.6 eV necessary energy to escape the proton (the ionization energy of hydrogen), and therefore simply remains bound to it, as a neutral hydrogen atom (one of the "two bodies"). In this type of free neutron decay, almost all of the neutron decay energy is carried off by the antineutrino (the other "body"). (The hydrogen atom recoils with a speed of only about (decay energy)/(hydrogen rest energy) times the speed of light, or 250 km/s.)

The transformation of a free proton to a neutron (plus a positron and a neutrino) is energetically impossible, since a free neutron has a greater mass than a free proton. But a high-energy collision of a proton and an electron or neutrino can result in a neutron.

While a free neutron has a half life of about 10.2 min, most neutrons within nuclei are stable. According to the nuclear shell model, the protons and neutrons of a nuclide are a quantum mechanical system organized into discrete energy levels with unique quantum numbers. For a neutron to decay, the resulting proton requires an available state at lower energy than the initial neutron state. In stable nuclei the possible lower energy states are all filled, meaning they are each occupied by two protons with spin up and spin down. The Pauli exclusion principle therefore disallows the decay of a neutron to a proton within stable nuclei. The situation is similar to electrons of an atom, where electrons have distinct atomic orbitals and are prevented from decaying to lower energy states, with the emission of a photon, by the exclusion principle.

Neutrons in unstable nuclei can decay by beta decay as described above. In this case, an energetically allowed quantum state is available for the proton resulting from the decay. One example of this decay is carbon-14 (6 protons, 8 neutrons) that decays to nitrogen-14 (7 protons, 7 neutrons) with a half-life of about 5,730 years.

Inside a nucleus, a proton can transform into a neutron via inverse beta decay, if an energetically allowed quantum state is available for the neutron. This transformation occurs by emission of a positron and an electron neutrino:

The transformation of a proton to a neutron inside of a nucleus is also possible through electron capture:
Positron capture by neutrons in nuclei that contain an excess of neutrons is also possible, but is hindered because positrons are repelled by the positive nucleus, and quickly annihilate when they encounter electrons.

Three types of beta decay in competition are illustrated by the single isotope copper-64 (29 protons, 35 neutrons), which has a half-life of about 12.7 hours. This isotope has one unpaired proton and one unpaired neutron, so either the proton or the neutron can decay. This particular nuclide is almost equally likely to undergo proton decay (by positron emission, 18% or by electron capture, 43%) or neutron decay (by electron emission, 39%).

The mass of a neutron cannot be directly determined by mass spectrometry due to lack of electric charge. However, since the masses of a proton and of a deuteron can be measured with a mass spectrometer, the mass of a neutron can be deduced by subtracting proton mass from deuteron mass, with the difference being the mass of the neutron plus the binding energy of deuterium (expressed as a positive emitted energy). The latter can be directly measured by measuring the energy (formula_1) of the single gamma photon emitted when neutrons are captured by protons (this is exothermic and happens with zero-energy neutrons), plus the small recoil kinetic energy (formula_2) of the deuteron (about 0.06% of the total energy).

The energy of the gamma ray can be measured to high precision by X-ray diffraction techniques, as was first done by Bell and Elliot in 1948. The best modern (1986) values for neutron mass by this technique are provided by Greene, et al. These give a neutron mass of:

The value for the neutron mass in MeV is less accurately known, due to less accuracy in the known conversion of u to MeV:

Another method to determine the mass of a neutron starts from the beta decay of the neutron, when the momenta of the resulting proton and electron are measured.

The total electric charge of the neutron is . This zero value has been tested experimentally, and the present experimental limit for the charge of the neutron is ,   or . This value is consistent with zero, given the experimental uncertainties (indicated in parentheses). By comparison, the charge of the proton is .

Even though the neutron is a neutral particle, the magnetic moment of a neutron is not zero. The neutron is not affected by electric fields, but it is affected by magnetic fields. The magnetic moment of the neutron is an indication of its quark substructure and internal charge distribution.
The value for the neutron's magnetic moment was first directly measured by Luis Alvarez and Felix Bloch at Berkeley, California, in 1940, using an extension of the magnetic resonance methods developed by Rabi. Alvarez and Bloch determined the magnetic moment of the neutron to be , where "μ" is the nuclear magneton.

In the quark model for hadrons, the neutron is composed of one up quark (charge +2/3 "e") and two down quarks (charge −1/3 "e"). The magnetic moment of the neutron can be modeled as a sum of the magnetic moments of the constituent quarks. The calculation assumes that the quarks behave like pointlike Dirac particles, each having their own magnetic moment. Simplistically, the magnetic moment of the neutron can be viewed as resulting from the vector sum of the three quark magnetic moments, plus the orbital magnetic moments caused by the movement of the three charged quarks within the neutron.

In one of the early successes of the Standard Model (SU(6) theory, now understood in terms of quark behavior), in 1964 Mirza A. B. Beg, Benjamin W. Lee, and Abraham Pais theoretically calculated the ratio of proton to neutron magnetic moments to be −3/2, which agrees with the experimental value to within 3%. The measured value for this ratio is . A contradiction of the quantum mechanical basis of this calculation with the Pauli exclusion principle, led to the discovery of the color charge for quarks by Oscar W. Greenberg in 1964.

The above treatment compares neutrons with protons, allowing the complex behavior of quarks to be subtracted out between models, and merely exploring what the effects would be of differing quark charges (or quark type). Such calculations are enough to show that the interior of neutrons is very much like that of protons, save for the difference in quark composition with a down quark in the neutron replacing an up quark in the proton.

Attempts have been made to quantitatively recover the neutron magnetic moment from first principles. From the nonrelativistic, quantum mechanical wavefunction for baryons composed of three quarks, a straightforward calculation gives fairly accurate estimates for the magnetic moments of neutrons, protons, and other baryons. For a neutron, the end result of this calculation is that the magnetic moment of the neutron is given by , where "μ" and "μ" are the magnetic moments for the down and up quarks, respectively. This result combines the intrinsic magnetic moments of the quarks with their orbital magnetic moments, and assumes the three quarks are in a particular, dominant quantum state.

The results of this calculation are encouraging, but the masses of the up or down quarks were assumed to be 1/3 the mass of a nucleon. The masses of the quarks are actually only about 1% that of a nucleon. The discrepancy stems from the complexity of the Standard Model for nucleons, where most of their mass originates in the gluon fields, virtual particles, and their associated energy that are essential aspects of the strong force. Furthermore, the complex system of quarks and gluons that constitute a neutron requires a relativistic treatment. The nucleon magnetic moment has been successfully computed numerically from first principles, however, including all the effects mentioned and using more realistic values for the quark masses. The calculation gave results that were in fair agreement with measurement, but it required significant computing resources.

The neutron is a spin 1/2 particle, that is, it is a fermion with intrinsic angular momentum equal to 1/2 , where is the reduced Planck constant. For many years after the discovery of the neutron, its exact spin was ambiguous. Although it was assumed to be a spin 1/2 Dirac particle, the possibility that the neutron was a spin 3/2 particle lingered. The interactions of the neutron's magnetic moment with an external magnetic field were exploited to finally determine the spin of the neutron. In 1949, Hughes and Burgy measured neutrons reflected from a ferromagnetic mirror and found that the angular distribution of the reflections was consistent with spin 1/2. In 1954, Sherwood, Stephenson, and Bernstein employed neutrons in a Stern–Gerlach experiment that used a magnetic field to separate the neutron spin states. They recorded two such spin states, consistent with a spin 1/2 particle.

As a fermion, the neutron is subject to the Pauli exclusion principle; two neutrons cannot have the same quantum numbers. This is the source of the degeneracy pressure which makes neutron stars possible.
An article published in 2007 featuring a model-independent analysis concluded that the neutron has a negatively charged exterior, a positively charged middle, and a negative core. In a simplified classical view, the negative "skin" of the neutron assists it to be attracted to the protons with which it interacts in the nucleus. (However, the main attraction between neutrons and protons is via the nuclear force, which does not involve charge.)

The simplified classical view of the neutron's charge distribution also "explains" the fact that the neutron magnetic dipole points in the opposite direction from its spin angular momentum vector (as compared to the proton). This gives the neutron, in effect, a magnetic moment which resembles a negatively charged particle. This can be reconciled classically with a neutral neutron composed of a charge distribution in which the negative sub-parts of the neutron have a larger average radius of distribution, and therefore contribute more to the particle's magnetic dipole moment, than do the positive parts that are, on average, nearer the core.

The Standard Model of particle physics predicts a tiny separation of positive and negative charge within the neutron leading to a permanent electric dipole moment. The predicted value is, however, well below the current sensitivity of experiments. From several unsolved puzzles in particle physics, it is clear that the Standard Model is not the final and full description of all particles and their interactions. New theories going beyond the Standard Model generally lead to much larger predictions for the electric dipole moment of the neutron. Currently, there are at least four experiments trying to measure for the first time a finite neutron electric dipole moment, including:

The antineutron is the antiparticle of the neutron. It was discovered by Bruce Cork in the year 1956, a year after the antiproton was discovered. CPT-symmetry puts strong constraints on the relative properties of particles and antiparticles, so studying antineutrons yields provide stringent tests on CPT-symmetry. The fractional difference in the masses of the neutron and antineutron is . Since the difference is only about two standard deviations away from zero, this does not give any convincing evidence of CPT-violation.

The existence of stable clusters of 4 neutrons, or tetraneutrons, has been hypothesised by a team led by Francisco-Miguel Marqués at the CNRS Laboratory for Nuclear Physics based on observations of the disintegration of beryllium-14 nuclei. This is particularly interesting because current theory suggests that these clusters should not be stable.

In February 2016, Japanese physicist Susumu Shimoura of the University of Tokyo and co-workers reported they had observed the purported tetraneutrons for the first time experimentally. Nuclear physicists around the world say this discovery, if confirmed, would be a milestone in the field of nuclear physics and certainly would deepen our understanding of the nuclear forces.

The dineutron is another hypothetical particle. In 2012, Artemis Spyrou from Michigan State University and coworkers reported that they observed, for the first time, the dineutron emission in the decay of Be. The dineutron character is evidenced by a small emission angle between the two neutrons. The authors measured the two-neutron separation energy to be 1.35(10) MeV, in good agreement with shell model calculations, using standard interactions for this mass region.

At extremely high pressures and temperatures, nucleons and electrons are believed to collapse into bulk neutronic matter, called neutronium. This is presumed to happen in neutron stars.

The extreme pressure inside a neutron star may deform the neutrons into a cubic symmetry, allowing tighter packing of neutrons.

The common means of detecting a charged particle by looking for a track of ionization (such as in a cloud chamber) does not work for neutrons directly. Neutrons that elastically scatter off atoms can create an ionization track that is detectable, but the experiments are not as simple to carry out; other means for detecting neutrons, consisting of allowing them to interact with atomic nuclei, are more commonly used. The commonly used methods to detect neutrons can therefore be categorized according to the nuclear processes relied upon, mainly neutron capture or elastic scattering.

A common method for detecting neutrons involves converting the energy released from neutron capture reactions into electrical signals. Certain nuclides have a high neutron capture cross section, which is the probability of absorbing a neutron. Upon neutron capture, the compound nucleus emits more easily detectable radiation, for example an alpha particle, which is then detected. The nuclides , , , , , , and are useful for this purpose. 

Neutrons can elastically scatter off nuclei, causing the struck nucleus to recoil. Kinematically, a neutron can transfer more energy to a light nucleus such as hydrogen or helium than to a heavier nucleus. Detectors relying on elastic scattering are called fast neutron detectors. Recoiling nuclei can ionize and excite further atoms through collisions. Charge and/or scintillation light produced in this way can be collected to produce a detected signal. A major challenge in fast neutron detection is discerning such signals from erroneous signals produced by gamma radiation in the same detector.

Fast neutron detectors have the advantage of not requiring a moderator, and are therefore capable of measuring the neutron's energy, time of arrival, and in certain cases direction of incidence.

Free neutrons are unstable, although they have the longest half-life of any unstable subatomic particle by several orders of magnitude. Their half-life is still only about 10 minutes, however, so they can be obtained only from sources that produce them continuously.

Natural neutron background. A small natural background flux of free neutrons exists everywhere on Earth. In the atmosphere and deep into the ocean, the "neutron background" is caused by muons produced by cosmic ray interaction with the atmosphere. These high-energy muons are capable of penetration to considerable depths in water and soil. There, in striking atomic nuclei, among other reactions they induce spallation reactions in which a neutron is liberated from the nucleus. Within the Earth's crust a second source is neutrons produced primarily by spontaneous fission of uranium and thorium present in crustal minerals. The neutron background is not strong enough to be a biological hazard, but it is of importance to very high resolution particle detectors that are looking for very rare events, such as (hypothesized) interactions that might be caused by particles of dark matter. Recent research has shown that even thunderstorms can produce neutrons with energies of up to several tens of MeV. Recent research has shown that the fluence of these neutrons lies between 10 and 10 per ms and per m depending on the detection altitude. The energy of most of these neutrons, even with initial energies of 20 MeV, decreases down to the keV range within 1 ms.

Even stronger neutron background radiation is produced at the surface of Mars, where the atmosphere is thick enough to generate neutrons from cosmic ray muon production and neutron-spallation, but not thick enough to provide significant protection from the neutrons produced. These neutrons not only produce a Martian surface neutron radiation hazard from direct downward-going neutron radiation but may also produce a significant hazard from reflection of neutrons from the Martian surface, which will produce reflected neutron radiation penetrating upward into a Martian craft or habitat from the floor.

Sources of neutrons for research. These include certain types of radioactive decay (spontaneous fission and neutron emission), and from certain nuclear reactions. Convenient nuclear reactions include tabletop reactions such as natural alpha and gamma bombardment of certain nuclides, often beryllium or deuterium, and induced nuclear fission, such as occurs in nuclear reactors. In addition, high-energy nuclear reactions (such as occur in cosmic radiation showers or accelerator collisions) also produce neutrons from disintegration of target nuclei. Small (tabletop) particle accelerators optimized to produce free neutrons in this way, are called neutron generators.

In practice, the most commonly used small laboratory sources of neutrons use radioactive decay to power neutron production. One noted neutron-producing radioisotope, californium-252 decays (half-life 2.65 years) by spontaneous fission 3% of the time with production of 3.7 neutrons per fission, and is used alone as a neutron source from this process. Nuclear reaction sources (that involve two materials) powered by radioisotopes use an alpha decay source plus a beryllium target, or else a source of high-energy gamma radiation from a source that undergoes beta decay followed by gamma decay, which produces photoneutrons on interaction of the high-energy gamma ray with ordinary stable beryllium, or else with the deuterium in heavy water. A popular source of the latter type is radioactive antimony-124 plus beryllium, a system with a half-life of 60.9 days, which can be constructed from natural antimony (which is 42.8% stable antimony-123) by activating it with neutrons in a nuclear reactor, then transported to where the neutron source is needed.

Nuclear fission reactors naturally produce free neutrons; their role is to sustain the energy-producing chain reaction. The intense neutron radiation can also be used to produce various radioisotopes through the process of neutron activation, which is a type of neutron capture.

Experimental nuclear fusion reactors produce free neutrons as a waste product. However, it is these neutrons that possess most of the energy, and converting that energy to a useful form has proved a difficult engineering challenge. Fusion reactors that generate neutrons are likely to create radioactive waste, but the waste is composed of neutron-activated lighter isotopes, which have relatively short (50–100 years) decay periods as compared to typical half-lives of 10,000 years for fission waste, which is long due primarily to the long half-life of alpha-emitting transuranic actinides.

Free neutron beams are obtained from neutron sources by neutron transport. For access to intense neutron sources, researchers must go to a specialized neutron facility that operates a research reactor or a spallation source.

The neutron's lack of total electric charge makes it difficult to steer or accelerate them. Charged particles can be accelerated, decelerated, or deflected by electric or magnetic fields. These methods have little effect on neutrons. However, some effects may be attained by use of inhomogeneous magnetic fields because of the neutron's magnetic moment. Neutrons can be controlled by methods that include moderation, reflection, and velocity selection. Thermal neutrons can be polarized by transmission through magnetic materials in a method analogous to the Faraday effect for photons. Cold neutrons of wavelengths of 6–7 angstroms can be produced in beams of a high degree of polarization, by use of magnetic mirrors and magnetized interference filters.

The neutron plays an important role in many nuclear reactions. For example, neutron capture often results in neutron activation, inducing radioactivity. In particular, knowledge of neutrons and their behavior has been important in the development of nuclear reactors and nuclear weapons. The fissioning of elements like uranium-235 and plutonium-239 is caused by their absorption of neutrons.

"Cold", "thermal", and "hot" neutron radiation is commonly employed in neutron scattering facilities, where the radiation is used in a similar way one uses X-rays for the analysis of condensed matter. Neutrons are complementary to the latter in terms of atomic contrasts by different scattering cross sections; sensitivity to magnetism; energy range for inelastic neutron spectroscopy; and deep penetration into matter.

The development of "neutron lenses" based on total internal reflection within hollow glass capillary tubes or by reflection from dimpled aluminum plates has driven ongoing research into neutron microscopy and neutron/gamma ray tomography.

A major use of neutrons is to excite delayed and prompt gamma rays from elements in materials. This forms the basis of neutron activation analysis (NAA) and prompt gamma neutron activation analysis (PGNAA). NAA is most often used to analyze small samples of materials in a nuclear reactor whilst PGNAA is most often used to analyze subterranean rocks around bore holes and industrial bulk materials on conveyor belts.

Another use of neutron emitters is the detection of light nuclei, in particular the hydrogen found in water molecules. When a fast neutron collides with a light nucleus, it loses a large fraction of its energy. By measuring the rate at which slow neutrons return to the probe after reflecting off of hydrogen nuclei, a neutron probe may determine the water content in soil.

Because neutron radiation is both penetrating and ionizing, it can be exploited for medical treatments. Neutron radiation can have the unfortunate side-effect of leaving the affected area radioactive, however. Neutron tomography is therefore not a viable medical application.

Fast neutron therapy utilizes high-energy neutrons typically greater than 20 MeV to treat cancer. Radiation therapy of cancers is based upon the biological response of cells to ionizing radiation. If radiation is delivered in small sessions to damage cancerous areas, normal tissue will have time to repair itself, while tumor cells often cannot. Neutron radiation can deliver energy to a cancerous region at a rate an order of magnitude larger than gamma radiation

Beams of low-energy neutrons are used in boron capture therapy to treat cancer. In boron capture therapy, the patient is given a drug that contains boron and that preferentially accumulates in the tumor to be targeted. The tumor is then bombarded with very low-energy neutrons (although often higher than thermal energy) which are captured by the boron-10 isotope in the boron, which produces an excited state of boron-11 that then decays to produce lithium-7 and an alpha particle that have sufficient energy to kill the malignant cell, but insufficient range to damage nearby cells. For such a therapy to be applied to the treatment of cancer, a neutron source having an intensity of the order of a thousand million (10) neutrons per second per cm is preferred. Such fluxes require a research nuclear reactor.

Exposure to free neutrons can be hazardous, since the interaction of neutrons with molecules in the body can cause disruption to molecules and atoms, and can also cause reactions that give rise to other forms of radiation (such as protons). The normal precautions of radiation protection apply: Avoid exposure, stay as far from the source as possible, and keep exposure time to a minimum. Some particular thought must be given to how to protect from neutron exposure, however. For other types of radiation, e.g., alpha particles, beta particles, or gamma rays, material of a high atomic number and with high density make for good shielding; frequently, lead is used. However, this approach will not work with neutrons, since the absorption of neutrons does not increase straightforwardly with atomic number, as it does with alpha, beta, and gamma radiation. Instead one needs to look at the particular interactions neutrons have with matter (see the section on detection above). For example, hydrogen-rich materials are often used to shield against neutrons, since ordinary hydrogen both scatters and slows neutrons. This often means that simple concrete blocks or even paraffin-loaded plastic blocks afford better protection from neutrons than do far more dense materials. After slowing, neutrons may then be absorbed with an isotope that has high affinity for slow neutrons without causing secondary capture radiation, such as lithium-6.

Hydrogen-rich ordinary water affects neutron absorption in nuclear fission reactors: Usually, neutrons are so strongly absorbed by normal water that fuel enrichment with fissionable isotope is required. The deuterium in heavy water has a very much lower absorption affinity for neutrons than does protium (normal light hydrogen). Deuterium is, therefore, used in CANDU-type reactors, in order to slow (moderate) neutron velocity, to increase the probability of nuclear fission compared to neutron capture.

A "thermal neutron" is a free neutron that is Boltzmann distributed with kT= () at room temperature. This gives characteristic (not average, or median) speed of 2.2 km/s. The name 'thermal' comes from their energy being that of the room temperature gas or material they are permeating. (see "kinetic theory" for energies and speeds of molecules). After a number of collisions (often in the range of 10–20) with nuclei, neutrons arrive at this energy level, provided that they are not absorbed.

In many substances, thermal neutron reactions show a much larger effective cross-section than reactions involving faster neutrons, and thermal neutrons can therefore be absorbed more readily (i.e., with higher probability) by any atomic nuclei that they collide with, creating a heavier — and often unstable — isotope of the chemical element as a result.

Most fission reactors use a neutron moderator to slow down, or "thermalize" the neutrons that are emitted by nuclear fission so that they are more easily captured, causing further fission. Others, called fast breeder reactors, use fission energy neutrons directly.

"Cold neutrons" are thermal neutrons that have been equilibrated in a very cold substance such as liquid deuterium. Such a "cold source" is placed in the moderator of a research reactor or spallation source. Cold neutrons are particularly valuable for neutron scattering experiments.

Ultracold neutrons are produced by elastically scattering cold neutrons in substances with a temperature of a few kelvins, such as solid deuterium or superfluid helium. An alternative production method is the mechanical deceleration of cold neutrons.

A "fast neutron" is a free neutron with a kinetic energy level close to (), hence a speed of ~ (~5% of the speed of light). They are named "fission energy" or "fast" neutrons to distinguish them from lower-energy thermal neutrons, and high-energy neutrons produced in cosmic showers or accelerators. Fast neutrons are produced by nuclear processes such as nuclear fission. Neutrons produced in fission, as noted above, have a Maxwell–Boltzmann distribution of kinetic energies from 0 to ~14 MeV, a mean energy of 2 MeV (for U-235 fission neutrons), and a mode of only 0.75 MeV, which means that more than half of them do not qualify as fast (and thus have almost no chance of initiating fission in fertile materials, such as U-238 and Th-232).

Fast neutrons can be made into thermal neutrons via a process called moderation. This is done with a neutron moderator. In reactors, typically heavy water, light water, or graphite are used to moderate neutrons.

D–T (deuterium–tritium) fusion is the fusion reaction that produces the most energetic neutrons, with 14.1 MeV of kinetic energy and traveling at 17% of the speed of light. D–T fusion is also the easiest fusion reaction to ignite, reaching near-peak rates even when the deuterium and tritium nuclei have only a thousandth as much kinetic energy as the 14.1 MeV that will be produced.

14.1 MeV neutrons have about 10 times as much energy as fission neutrons, and are very effective at fissioning even non-fissile heavy nuclei, and these high-energy fissions produce more neutrons on average than fissions by lower-energy neutrons. This makes D–T fusion neutron sources such as proposed tokamak power reactors useful for transmutation of transuranic waste. 14.1 MeV neutrons can also produce neutrons by knocking them loose from nuclei.

On the other hand, these very high-energy neutrons are less likely to simply be captured without causing fission or spallation. For these reasons, nuclear weapon design extensively utilizes D–T fusion 14.1 MeV neutrons to cause more fission. Fusion neutrons are able to cause fission in ordinarily non-fissile materials, such as depleted uranium (uranium-238), and these materials have been used in the jackets of thermonuclear weapons. Fusion neutrons also can cause fission in substances that are unsuitable or difficult to make into primary fission bombs, such as reactor grade plutonium. This physical fact thus causes ordinary non-weapons grade materials to become of concern in certain nuclear proliferation discussions and treaties.

Other fusion reactions produce much less energetic neutrons. D–D fusion produces a 2.45 MeV neutron and helium-3 half of the time, and produces tritium and a proton but no neutron the rest of the time. D–He fusion produces no neutron.

A fission energy neutron that has slowed down but not yet reached thermal energies is called an epithermal neutron.

Cross sections for both capture and fission reactions often have multiple resonance peaks at specific energies in the epithermal energy range.
These are of less significance in a fast neutron reactor, where most neutrons are absorbed before slowing down to this range, or in a well-moderated thermal reactor, where epithermal neutrons interact mostly with moderator nuclei, not with either fissile or fertile actinide nuclides.
However, in a partially moderated reactor with more interactions of epithermal neutrons with heavy metal nuclei, there are greater possibilities for transient changes in reactivity that might make reactor control more difficult.

Ratios of capture reactions to fission reactions are also worse (more captures without fission) in most nuclear fuels such as plutonium-239, making epithermal-spectrum reactors using these fuels less desirable, as captures not only waste the one neutron captured but also usually result in a nuclide that is not fissile with thermal or epithermal neutrons, though still fissionable with fast neutrons. The exception is uranium-233 of the thorium cycle, which has good capture-fission ratios at all neutron energies.

High-energy neutrons have much more energy than fission energy neutrons and are generated as secondary particles by particle accelerators or in the atmosphere from cosmic rays. These high-energy neutrons are extremely efficient at ionization and far more likely to cause cell death than X-rays or protons.






</doc>
<doc id="21273" url="https://en.wikipedia.org/wiki?curid=21273" title="Neon">
Neon

Neon is a chemical element with symbol Ne and atomic number 10. It is a noble gas. Neon is a colorless, odorless, inert monatomic gas under standard conditions, with about two-thirds the density of air. It was discovered (along with krypton and xenon) in 1898 as one of the three residual rare inert elements remaining in dry air, after nitrogen, oxygen, argon and carbon dioxide were removed. Neon was the second of these three rare gases to be discovered and was immediately recognized as a new element from its bright red emission spectrum. The name neon is derived from the Greek word, , neuter singular form of ("neos"), meaning new. Neon is chemically inert, and no uncharged neon compounds are known. The compounds of neon currently known include ionic molecules, molecules held together by van der Waals forces and clathrates.

During cosmic nucleogenesis of the elements, large amounts of neon are built up from the alpha-capture fusion process in stars. Although neon is a very common element in the universe and solar system (it is fifth in cosmic abundance after hydrogen, helium, oxygen and carbon), it is rare on Earth. It composes about 18.2 ppm of air by volume (this is about the same as the molecular or mole fraction) and a smaller fraction in Earth's crust. The reason for neon's relative scarcity on Earth and the inner (terrestrial) planets is that neon is highly volatile and forms no compounds to fix it to solids. As a result, it escaped from the planetesimals under the warmth of the newly ignited Sun in the early Solar System. Even the outer atmosphere of Jupiter is somewhat depleted of neon, although for a different reason. It is also lighter than air, causing it to escape even from Earth's atmosphere.

Neon gives a distinct reddish-orange glow when used in low-voltage neon glow lamps, high-voltage discharge tubes and neon advertising signs. The red emission line from neon also causes the well known red light of helium–neon lasers. Neon is used in some plasma tube and refrigerant applications but has few other commercial uses. It is commercially extracted by the fractional distillation of liquid air. Since air is the only source, it is considerably more expensive than helium.

Neon (Greek ("néon"), neuter singular form of νέος meaning "new"), was discovered in 1898 by the British chemists Sir William Ramsay (1852–1916) and Morris W. Travers (1872–1961) in London. Neon was discovered when Ramsay chilled a sample of air until it became a liquid, then warmed the liquid and captured the gases as they boiled off. The gases nitrogen, oxygen, and argon had been identified, but the remaining gases were isolated in roughly their order of abundance, in a six-week period beginning at the end of May 1898. First to be identified was krypton. The next, after krypton had been removed, was a gas which gave a brilliant red light under spectroscopic discharge. This gas, identified in June, was named neon, the Greek analogue of "novum", (new), suggested by Ramsay's son. The characteristic brilliant red-orange color emitted by gaseous neon when excited electrically was noted immediately; Travers later wrote, "the blaze of crimson light from the tube told its own story and was a sight to dwell upon and never forget."

A second gas was also reported along with neon, having approximately the same density as argon but with a different spectrum – Ramsay and Travers named it "metargon". However, subsequent spectroscopic analysis revealed it to be argon contaminated with carbon monoxide. Finally, the same team discovered xenon by the same process, in September 1898.

Neon's scarcity precluded its prompt application for lighting along the lines of Moore tubes, which used nitrogen and which were commercialized in the early 1900s. After 1902, Georges Claude's company, Air Liquide, produced industrial quantities of neon as a byproduct of his air liquefaction business. In December 1910 Claude demonstrated modern neon lighting based on a sealed tube of neon. Claude tried briefly to sell neon tubes for indoor domestic lighting, due to their intensity, but the market failed because homeowners objected to the color. In 1912, Claude's associate began selling neon discharge tubes as eye-catching advertising signs and was instantly more successful. Neon tubes were introduced to the U.S. in 1923 with two large neon signs bought by a Los Angeles Packard car dealership. The glow and arresting red color made neon advertising completely different from the competition. The intense color and vibrancy of neon equated with American society at the time, suggesting a "century of progress" and transforming cities into sensational new environments filled with radiating advertisements and "electro-graphic architecture."

Neon played a role in the basic understanding of the nature of atoms in 1913, when J. J. Thomson, as part of his exploration into the composition of canal rays, channeled streams of neon ions through a magnetic and an electric field and measured the deflection of the streams with a photographic plate. Thomson observed two separate patches of light on the photographic plate (see image), which suggested two different parabolas of deflection. Thomson eventually concluded that some of the atoms in the neon gas were of higher mass than the rest. Though not understood at the time by Thomson, this was the first discovery of isotopes of stable atoms. Thomson's device was a crude version of the instrument we now term a mass spectrometer.

Neon is the second lightest inert gas. Neon has three stable isotopes: Ne (90.48%), Ne (0.27%) and Ne (9.25%). Ne and Ne are partly primordial and partly nucleogenic (i.e. made by nuclear reactions of other nuclides with neutrons or other particles in the environment) and their variations in natural abundance are well understood. In contrast, Ne (the chief primordial isotope made in stellar nucleosynthesis) is not known to be nucleogenic or radiogenic. The causes of the variation of Ne in the Earth have thus been hotly debated.

The principal nuclear reactions which generate nucleogenic neon isotopes start from Mg and Mg, which produce Ne and Ne, respectively, after neutron capture and immediate emission of an alpha particle. The neutrons that produce the reactions are mostly produced by secondary spallation reactions from alpha particles, in turn derived from uranium-series decay chains. The net result yields a trend towards lower Ne/Ne and higher Ne/Ne ratios observed in uranium-rich rocks such as granites. Ne may also be produced in a nucleogenic reaction, when Ne absorbs a neutron from various natural terrestrial neutron sources.

In addition, isotopic analysis of exposed terrestrial rocks has demonstrated the cosmogenic (cosmic ray) production of Ne. This isotope is generated by spallation reactions on magnesium, sodium, silicon, and aluminium. By analyzing all three isotopes, the cosmogenic component can be resolved from magmatic neon and nucleogenic neon. This suggests that neon will be a useful tool in determining cosmic exposure ages of surface rocks and meteorites.

Similar to xenon, neon content observed in samples of volcanic gases is enriched in Ne, as well as nucleogenic Ne, relative to Ne content. The neon isotopic content of these mantle-derived samples represents a non-atmospheric source of neon. The Ne-enriched components are attributed to exotic primordial rare gas components in the Earth, possibly representing solar neon. Elevated Ne abundances are found in diamonds, further suggesting a solar neon reservoir in the Earth.

Neon is the second-lightest noble gas, after helium. It glows reddish-orange in a vacuum discharge tube. Also, neon has the narrowest liquid range of any element: from 24.55 K to 27.05 K (−248.45 °C to −245.95 °C, or −415.21 °F to −410.71 °F). It has over 40 times the refrigerating capacity of liquid helium and three times that of liquid hydrogen (on a per unit volume basis). In most applications it is a less expensive refrigerant than helium.
Neon plasma has the most intense light discharge at normal voltages and currents of all the noble gases. The average color of this light to the human eye is red-orange due to many lines in this range; it also contains a strong green line which is hidden, unless the visual components are dispersed by a spectroscope.

Two quite different kinds of neon lighting are in common use. Neon glow lamps are generally tiny, with most operating between 100 and 250 volts. They have been widely used as power-on indicators and in circuit-testing equipment, but light-emitting diodes (LEDs) now dominate in those applications. These simple neon devices were the forerunners of plasma displays and plasma television screens. Neon signs typically operate at much higher voltages (2–15 kilovolts), and the luminous tubes are commonly meters long. The glass tubing is often formed into shapes and letters for signage as well as architectural and artistic applications.

Stable isotopes of neon are produced in stars. Ne is created in fusing helium and oxygen in the alpha process. This requires temperatures above 100 megakelvin, which are only available in the cores of stars of more than 3 solar masses.

Neon is abundant on a universal scale; it is the fifth most abundant chemical element in the universe by mass, after hydrogen, helium, oxygen, and carbon (see chemical element). Its relative rarity on Earth, like that of helium, is due to its relative lightness, high vapor pressure at very low temperatures, and chemical inertness, all properties which tend to keep it from being trapped in the condensing gas and dust clouds that formed the smaller and warmer solid planets like Earth.

Neon is monatomic, making it lighter than the molecules of diatomic nitrogen and oxygen which form the bulk of Earth's atmosphere; a balloon filled with neon will rise in air, albeit more slowly than a helium balloon.

Neon's abundance in the universe is about 1 part in 750; in the Sun and presumably in the proto-solar system nebula, about 1 part in 600. The Galileo spacecraft atmospheric entry probe found that even in the upper atmosphere of Jupiter, the abundance of neon is reduced (depleted) by about a factor of 10, to a level of 1 part in 6,000 by mass. This may indicate that even the ice-planetesimals which brought neon into Jupiter from the outer solar system, formed in a region which was too warm to retain the neon atmospheric component (abundances of heavier inert gases on Jupiter are several times that found in the Sun).

Neon comprises 1 part in 55,000 in the Earth's atmosphere, or 18.2 ppm by volume (this is about the same as the molecule or mole fraction), or 1 part in 79,000 of air by mass. It comprises a smaller fraction in the crust. It is industrially produced by cryogenic fractional distillation of liquefied air.

On 17 August 2015, based on studies with the Lunar Atmosphere and Dust Environment Explorer (LADEE) spacecraft, NASA scientists reported the detection of neon in the exosphere of the moon.

Neon is the first p-block noble gas, and the first element with a true octet of electrons. It is inert: as is the case with its lighter analogue, helium, no strongly bound neutral molecules containing neon have been identified. The ions [NeAr], [NeH], and [HeNe] have been observed from optical and mass spectrometric studies. Solid neon clathrate hydrate was produced from water ice and neon gas at pressures 0.35–0.48 GPa and temperatures ca. –30 °C. Ne atoms are not bonded to water and can freely move through this material. They can be extracted by placing the clathrate into a vacuum chamber for several days, yielding ice XVI, the least dense crystalline form of water.

The familiar Pauling electronegativity scale relies upon chemical bond energies, but such values have obviously not been measured for inert helium and neon. The Allen electronegativity scale, which relies only upon (measurable) atomic energies, identifies neon as the most electronegative element, closely followed by fluorine and helium.

Neon is often used in signs and produces an unmistakable bright reddish-orange light. Although tube lights with other colors are often called "neon", they use different noble gases or varied colors of fluorescent lighting.

Neon is used in vacuum tubes, high-voltage indicators, lightning arresters, wave meter tubes, television tubes, and helium–neon lasers. Liquefied neon is commercially used as a cryogenic refrigerant in applications not requiring the lower temperature range attainable with more extreme liquid helium refrigeration.

Neon, as liquid or gas, is relatively expensive – for small quantities, the price of liquid neon can be more than 55 times that of liquid helium. Driving neon's expense is the rarity of neon, which unlike helium, can only be obtained from air.

The triple point temperature of neon (24.5561 K) is a defining fixed point in the International Temperature Scale of 1990.



</doc>
<doc id="21274" url="https://en.wikipedia.org/wiki?curid=21274" title="Nickel">
Nickel

Nickel is a chemical element with symbol Ni and atomic number 28. It is a silvery-white lustrous metal with a slight golden tinge. Nickel belongs to the transition metals and is hard and ductile. Pure nickel, powdered to maximize the reactive surface area, shows a significant chemical activity, but larger pieces are slow to react with air under standard conditions because an oxide layer forms on the surface and prevents further corrosion (passivation). Even so, pure native nickel is found in Earth's crust only in tiny amounts, usually in ultramafic rocks, and in the interiors of larger nickel–iron meteorites that were not exposed to oxygen when outside Earth's atmosphere.

Meteoric nickel is found in combination with iron, a reflection of the origin of those elements as major end products of supernova nucleosynthesis. An iron–nickel mixture is thought to compose Earth's inner core.

Use of nickel (as a natural meteoric nickel–iron alloy) has been traced as far back as 3500 BCE. Nickel was first isolated and classified as a chemical element in 1751 by Axel Fredrik Cronstedt, who initially mistook the ore for a copper mineral, in the cobalt mines of Los, Hälsingland, Sweden. The element's name comes from a mischievous sprite of German miner mythology, Nickel (similar to Old Nick), who personified the fact that copper-nickel ores resisted refinement into copper. An economically important source of nickel is the iron ore limonite, which often contains 1–2% nickel. Nickel's other important ore minerals include pentlandite and a mixture of Ni-rich natural silicates known as garnierite. Major production sites include the Sudbury region in Canada (which is thought to be of meteoric origin), New Caledonia in the Pacific, and Norilsk in Russia.

Nickel is slowly oxidized by air at room temperature and is considered corrosion-resistant. Historically, it has been used for plating iron and brass, coating chemistry equipment, and manufacturing certain alloys that retain a high silvery polish, such as German silver. About 9% of world nickel production is still used for corrosion-resistant nickel plating. Nickel-plated objects sometimes provoke nickel allergy. Nickel has been widely used in coins, though its rising price has led to some replacement with cheaper metals in recent years.

Nickel is one of four elements (the others are iron, cobalt, and gadolinium) that are ferromagnetic at approximately room temperature. Alnico permanent magnets based partly on nickel are of intermediate strength between iron-based permanent magnets and rare-earth magnets. The metal is valuable in modern times chiefly in alloys; about 68% of world production is used in stainless steel. A further 10% is used for nickel-based and copper-based alloys, 7% for alloy steels, 3% in foundries, 9% in plating and 4% in other applications, including the fast-growing battery sector. As a compound, nickel has a number of niche chemical manufacturing uses, such as a catalyst for hydrogenation, cathodes for batteries, pigments and metal surface treatments. Nickel is an essential nutrient for some microorganisms and plants that have enzymes with nickel as an active site.

Nickel is a silvery-white metal with a slight golden tinge that takes a high polish. It is one of only four elements that are magnetic at or near room temperature, the others being iron, cobalt and gadolinium. Its Curie temperature is , meaning that bulk nickel is non-magnetic above this temperature. The unit cell of nickel is a face-centered cube with the lattice parameter of 0.352 nm, giving an atomic radius of 0.124 nm. This crystal structure is stable to pressures of at least 70 GPa. Nickel belongs to the transition metals and is hard and ductile.

The nickel atom has two electron configurations, [Ar] 3d 4s and [Ar] 3d 4s, which are very close in energy – the symbol [Ar] refers to the argon-like core structure. There is some disagreement on which configuration has the lowest energy. Chemistry textbooks quote the electron configuration of nickel as [Ar] 4s 3d, which can also be written [Ar] 3d 4s. This configuration agrees with the Madelung energy ordering rule, which predicts that 4s is filled before 3d. It is supported by the experimental fact that the lowest energy state of the nickel atom is a 3d 4s energy level, specifically the 3d(F) 4s F, "J" = 4 level.

However, each of these two configurations splits into several energy levels due to fine structure, and the two sets of energy levels overlap. The average energy of states with configuration [Ar] 3d 4s is actually lower than the average energy of states with configuration [Ar] 3d 4s. For this reason, the research literature on atomic calculations quotes the ground state configuration of nickel as [Ar] 3d 4s.

The isotopes of nickel range in atomic weight from 48 u () to 78 u ().

Naturally occurring nickel is composed of five stable isotopes; , , , and , with being the most abundant (68.077% natural abundance). Isotopes heavier than cannot be formed by nuclear fusion without losing energy.

Nickel-62 has the highest mean nuclear binding energy per nucleon of any nuclide, at 8.7946 MeV/nucleon. Its binding energy is greater than both and , more abundant elements often incorrectly cited as having the most tightly-bound nuclides. Although this would seem to predict nickel-62 as the most abundant heavy element in the universe, the relatively high rate of photodisintegration of nickel in stellar interiors causes iron to be by far the most abundant.

Stable isotope nickel-60 is the daughter product of the extinct radionuclide , which decays with a half-life of 2.6 million years. Because has such a long half-life, its persistence in materials in the solar system may generate observable variations in the isotopic composition of . Therefore, the abundance of present in extraterrestrial material may provide insight into the origin of the solar system and its early history.

Some 18 nickel radioisotopes have been characterised, the most stable being with a half-life of 76,000 years, with 100 years, and with 6 days. All of the remaining radioactive isotopes have half-lives that are less than 60 hours and the majority of these have half-lives that are less than 30 seconds. This element also has one meta state.

Radioactive nickel-56 is produced by the silicon burning process and later set free in large quantities during type Ia supernovae. The shape of the light curve of these supernovae at intermediate to late-times corresponds to the decay via electron capture of nickel-56 to cobalt-56 and ultimately to iron-56. Nickel-59 is a long-lived cosmogenic radionuclide with a half-life of 76,000 years. has found many applications in isotope geology. has been used to date the terrestrial age of meteorites and to determine abundances of extraterrestrial dust in ice and sediment. Nickel-78's half-life was recently measured at 110 milliseconds, and is believed an important isotope in supernova nucleosynthesis of elements heavier than iron. The nuclide Ni, discovered in 1999, is the most proton-rich heavy element isotope known. With 28 protons and 20 neutrons Ni is "double magic", as is with 28 protons and 50 neutrons. Both are therefore unusually stable for nuclides with so large a proton-neutron imbalance.

On Earth, nickel occurs most often in combination with sulfur and iron in pentlandite, with sulfur in millerite, with arsenic in the mineral nickeline, and with arsenic and sulfur in nickel galena. Nickel is commonly found in iron meteorites as the alloys kamacite and taenite.

The bulk of the nickel is mined from two types of ore deposits. The first is laterite, where the principal ore mineral mixtures are nickeliferous limonite, (Fe,Ni)O(OH), and garnierite (a mixture of various hydrous nickel and nickel-rich silicates). The second is magmatic sulfide deposits, where the principal ore mineral is pentlandite: .

Australia and New Caledonia have the biggest estimate reserves (45% all together).

Identified land-based resources throughout the world averaging 1% nickel or greater comprise at least 130 million tons of nickel (about the double of known reserves). About 60% is in laterites and 40% in sulfide deposits.

On geophysical evidence, most of the nickel on Earth is believed to be in the Earth's outer and inner cores. Kamacite and taenite are naturally occurring alloys of iron and nickel. For kamacite, the alloy is usually in the proportion of 90:10 to 95:5, although impurities (such as cobalt or carbon) may be present, while for taenite the nickel content is between 20% and 65%. Kamacite and taenite are also found in nickel iron meteorites.

The most common oxidation state of nickel is +2, but compounds of Ni, Ni, and Ni are well known, and the exotic oxidation states Ni, Ni, and Ni have been produced and studied.

Nickel tetracarbonyl ), discovered by Ludwig Mond, is a volatile, highly toxic liquid at room temperature. On heating, the complex decomposes back to nickel and carbon monoxide:
This behavior is exploited in the Mond process for purifying nickel, as described above. The related nickel(0) complex bis(cyclooctadiene)nickel(0) is a useful catalyst in organonickel chemistry because the cyclooctadiene (or "cod") ligands are easily displaced.

Nickel(I) complexes are uncommon, but one example is the tetrahedral complex NiBr(PPh). Many nickel(I) complexes feature Ni-Ni bonding, such as the dark red diamagnetic prepared by reduction of with sodium amalgam. This compound is oxidised in water, liberating .

It is thought that the nickel(I) oxidation state is important to nickel-containing enzymes, such as [NiFe]-hydrogenase, which catalyzes the reversible reduction of protons to .
Nickel(II) forms compounds with all common anions, including sulfide, sulfate, carbonate, hydroxide, carboxylates, and halides. Nickel(II) sulfate is produced in large quantities by dissolving nickel metal or oxides in sulfuric acid, forming both a hexa- and heptahydrates useful for electroplating nickel. Common salts of nickel, such as the chloride, nitrate, and sulfate, dissolve in water to give green solutions of the metal aquo complex .

The four halides form nickel compounds, which are solids with molecules that feature octahedral Ni centres. Nickel(II) chloride is most common, and its behavior is illustrative of the other halides. Nickel(II) chloride is produced by dissolving nickel or its oxide in hydrochloric acid. It is usually encountered as the green hexahydrate, the formula of which is usually written NiCl•6HO. When dissolved in water, this salt forms the metal aquo complex . Dehydration of NiCl•6HO gives the yellow anhydrous .

Some tetracoordinate nickel(II) complexes, e.g. bis(triphenylphosphine)nickel chloride, exist both in tetrahedral and square planar geometries. The tetrahedral complexes are paramagnetic, whereas the square planar complexes are diamagnetic. In having properties of magnetic equilibrium and formation of octahedral complexes, they contrast with the divalent complexes of the heavier group 10 metals, palladium(II) and platinum(II), which form only square-planar geometry.

Nickelocene is known; it has an electron count of 20, making it relatively unstable.
Numerous Ni(III) compounds are known, with the first such examples being Nickel(III) trihalophosphines (Ni(PPh)X). Further, Ni(III) forms simple salts with fluoride or oxide ions. Ni(III) can be stabilized by σ-donor ligands such as thiols and phosphines.

Ni(IV) is present in the mixed oxide , while Ni(III) is present in nickel oxide hydroxide, which is used as the cathode in many rechargeable batteries, including nickel-cadmium, nickel-iron, nickel hydrogen, and nickel-metal hydride, and used by certain manufacturers in Li-ion batteries. Ni(IV) remains a rare oxidation state of nickel and very few compounds are known to date.

Because the ores of nickel are easily mistaken for ores of silver, understanding of this metal and its use dates to relatively recent times. However, the unintentional use of nickel is ancient, and can be traced back as far as 3500 BCE. Bronzes from what is now Syria have been found to contain as much as 2% nickel. Some ancient Chinese manuscripts suggest that "white copper" (cupronickel, known as "baitong") was used there between 1700 and 1400 BCE. This Paktong white copper was exported to Britain as early as the 17th century, but the nickel content of this alloy was not discovered until 1822. Coins of nickel-copper alloy were minted by the Bactrian kings Agathocles, Euthydemus II and Pantaleon in the 2nd Century BCE, possibly out of the Chinese cupronickel.

In medieval Germany, a red mineral was found in the Erzgebirge (Ore Mountains) that resembled copper ore. However, when miners were unable to extract any copper from it, they blamed a mischievous sprite of German mythology, Nickel (similar to "Old Nick"), for besetting the copper. They called this ore "Kupfernickel" from the German "Kupfer" for copper. This ore is now known to be nickeline (aka niccolite), a nickel arsenide. In 1751, Baron Axel Fredrik Cronstedt tried to extract copper from kupfernickel at a cobalt mine in the Swedish village of Los, and instead produced a white metal that he named after the spirit that had given its name to the mineral, nickel. In modern German, Kupfernickel or Kupfer-Nickel designates the alloy cupronickel.

Originally, the only source for nickel was the rare Kupfernickel. Beginning in 1824, nickel was obtained as a byproduct of cobalt blue production. The first large-scale smelting of nickel began in Norway in 1848 from nickel-rich pyrrhotite. The introduction of nickel in steel production in 1889 increased the demand for nickel, and the nickel deposits of New Caledonia, discovered in 1865, provided most of the world's supply between 1875 and 1915. The discovery of the large deposits in the Sudbury Basin, Canada in 1883, in Norilsk-Talnakh, Russia in 1920, and in the Merensky Reef, South Africa in 1924, made large-scale production of nickel possible.

Aside from the aforementioned Bactrian coins, nickel was not a component of coins until the mid-19th century.

99.9% nickel five-cent coins were struck in Canada (the world's largest nickel producer at the time) during non-war years from 1922–1981; the metal content made these coins magnetic. During the wartime period 1942–45, most or all nickel was removed from Canadian and U.S. coins to save it for manufacturing armor. Canada used 99.9% nickel from 1968 in its higher-value coins until 2000.

Coins of nearly pure nickel were first used in 1881 in Switzerland.

Birmingham forged nickel coins in about 1833 for trading in Malaya.

In the United States, the term "nickel" or "nick" originally applied to the copper-nickel Flying Eagle cent, which replaced copper with 12% nickel 1857–58, then the Indian Head cent of the same alloy from 1859–1864. Still later, in 1865, the term designated the three-cent nickel, with nickel increased to 25%. In 1866, the five-cent shield nickel (25% nickel, 75% copper) appropriated the designation. Along with the alloy proportion, this term has been used to the present in the United States.

In the 21st century, the high price of nickel has led to some replacement of the metal in coins around the world. Coins still made with nickel alloys include one- and two-euro coins, 5¢, 10¢, 25¢ and 50¢ U.S. coins, and 20p, 50p, £1 and £2 UK coins. Nickel-alloy in 5p and 10p UK coins was replaced with nickel-plated steel began in 2012, causing allergy problems for some people and public controversy.

Around 2 million tonnes of nickel are produced annually worldwide. The Philippines, Indonesia, Russia, Canada and Australia are the world's largest producers of nickel, as reported by the US Geological Survey. The largest deposits of nickel in non-Russian Europe are located in Finland and Greece. Identified land-based resources averaging 1% nickel or greater contain at least 130 million tons of nickel. About 60% is in laterites and 40% is in sulfide deposits. In addition, extensive deep-sea resources of nickel are in manganese crusts and nodules covering large areas of the ocean floor, particularly in the Pacific Ocean.

The one locality in the United States where nickel has been profitably mined is Riddle, Oregon, where several square miles of nickel-bearing garnierite surface deposits are located. The mine closed in 1987. The Eagle mine project is a new nickel mine in Michigan's upper peninsula. Construction was completed in 2013, and operations began in the third quarter of 2014. In the first full year of operation, Eagle Mine produced 18,000 tonnes.

Nickel is obtained through extractive metallurgy: it is extracted from the ore by conventional roasting and reduction processes that yield a metal of greater than 75% purity. In many stainless steel applications, 75% pure nickel can be used without further purification, depending on the impurities.

Traditionally, most sulfide ores have been processed using pyrometallurgical techniques to produce a matte for further refining. Recent advances in hydrometallurgical techniques resulted in significantly purer metallic nickel product. Most sulfide deposits have traditionally been processed by concentration through a froth flotation process followed by pyrometallurgical extraction. In hydrometallurgical processes, nickel sulfide ores are concentrated with flotation (differential flotation if Ni/Fe ratio is too low) and then smelted. The nickel matte is further processed with the Sherritt-Gordon process. First, copper is removed by adding hydrogen sulfide, leaving a concentrate of cobalt and nickel. Then, solvent extraction is used to separate the cobalt and nickel, with the final nickel content greater than 99%.
A second common refining process is leaching the metal matte into a nickel salt solution, followed by the electro-winning of the nickel from solution by plating it onto a cathode as electrolytic nickel.

The purest metal is obtained from nickel oxide by the Mond process, which achieves a purity of greater than 99.99%. The process was patented by Ludwig Mond and has been in industrial use since before the beginning of the 20th century. In this process, nickel is reacted with carbon monoxide in the presence of a sulfur catalyst at around 40–80 °C to form nickel carbonyl. Iron gives iron pentacarbonyl, too, but this reaction is slow. If necessary, the nickel may be separated by distillation. Dicobalt octacarbonyl is also formed in nickel distillation as a by-product, but it decomposes to tetracobalt dodecacarbonyl at the reaction temperature to give a non-volatile solid.

Nickel is obtained from nickel carbonyl by one of two processes. It may be passed through a large chamber at high temperatures in which tens of thousands of nickel spheres, called pellets, are constantly stirred. The carbonyl decomposes and deposits pure nickel onto the nickel spheres. In the alternate process, nickel carbonyl is decomposed in a smaller chamber at 230 °C to create a fine nickel powder. The byproduct carbon monoxide is recirculated and reused. The highly pure nickel product is known as "carbonyl nickel".

The market price of nickel surged throughout 2006 and the early months of 2007; as of April 5, 2007, the metal was trading at US$52,300/tonne or $1.47/oz. The price subsequently fell dramatically, and as of September 2017, the metal was trading at $11,000/tonne, or $0.31/oz.

The US nickel coin contains of nickel, which at the April 2007 price was worth 6.5 cents, along with 3.75 grams of copper worth about 3 cents, with a total metal value of more than 9 cents. Since the face value of a nickel is 5 cents, this made it an attractive target for melting by people wanting to sell the metals at a profit. However, the United States Mint, in anticipation of this practice, implemented new interim rules on December 14, 2006, subject to public comment for 30 days, which criminalized the melting and export of cents and nickels. Violators can be punished with a fine of up to $10,000 and/or imprisoned for a maximum of five years.

As of September 19, 2013, the melt value of a U.S. nickel (copper and nickel included) is $0.045, which is 90% of the face value.

The global production of nickel is presently used as follows: 68% in stainless steel; 10% in nonferrous alloys; 9% in electroplating; 7% in alloy steel; 3% in foundries; and 4% other uses (including batteries).

Nickel is used in many specific and recognizable industrial and consumer products, including stainless steel, alnico magnets, coinage, rechargeable batteries, electric guitar strings, microphone capsules, plating on plumbing fixtures, and special alloys such as permalloy, elinvar, and invar. It is used for plating and as a green tint in glass. Nickel is preeminently an alloy metal, and its chief use is in nickel steels and nickel cast irons, in which it typically increases the tensile strength, toughness, and elastic limit. It is widely used in many other alloys, including nickel brasses and bronzes and alloys with copper, chromium, aluminium, lead, cobalt, silver, and gold (Inconel, Incoloy, Monel, Nimonic).

Because it is resistant to corrosion, nickel was occasionally used as a substitute for decorative silver. Nickel was also occasionally used in some countries after 1859 as a cheap coinage metal (see above), but in the later years of the 20th century was replaced by cheaper stainless steel (i.e., iron) alloys, except in the United States and Canada.

Nickel is an excellent alloying agent for certain precious metals and is used in the fire assay as a collector of platinum group elements (PGE). As such, nickel is capable of fully collecting all six PGE elements from ores, and of partially collecting gold. High-throughput nickel mines may also engage in PGE recovery (primarily platinum and palladium); examples are Norilsk in Russia and the Sudbury Basin in Canada.

Nickel foam or nickel mesh is used in gas diffusion electrodes for alkaline fuel cells.

Nickel and its alloys are frequently used as catalysts for hydrogenation reactions. Raney nickel, a finely divided nickel-aluminium alloy, is one common form, though related catalysts are also used, including Raney-type catalysts.

Nickel is a naturally magnetostrictive material, meaning that, in the presence of a magnetic field, the material undergoes a small change in length. The magnetostriction of nickel is on the order of 50 ppm and is negative, indicating that it contracts.

Nickel is used as a binder in the cemented tungsten carbide or hardmetal industry and used in proportions of 6% to 12% by weight. Nickel makes the tungsten carbide magnetic and adds corrosion-resistance to the cemented parts, although the hardness is less than those with a cobalt binder.

, with its half-life of 100.1 years, is useful in krytron devices as a beta particle (high-speed electron) emitter to make ionization by the keep-alive electrode more reliable.

Around 27% of all nickel production is destined for engineering, 10% for building and construction, 14% for tubular products, 20% for metal goods, 14% for transport, 11% for electronic goods, and 5% for other uses.

Although not recognized until the 1970s, nickel is known to play an important role in the biology of some plants, eubacteria, archaebacteria, and fungi. Nickel enzymes such as urease are considered virulence factors in some organisms. Urease catalyzes the hydrolysis of urea to form ammonia and carbamate. The NiFe hydrogenases can catalyze the oxidation of to form protons and electrons, and can also catalyze the reverse reaction, the reduction of protons to form hydrogen gas. A nickel-tetrapyrrole coenzyme, cofactor F430, is present in methyl coenzyme M reductase, which can catalyze the formation of methane, or the reverse reaction, in methanogenic archaea. One of the carbon monoxide dehydrogenase enzymes consists of an Fe-Ni-S cluster. Other nickel-bearing enzymes include a rare bacterial class of superoxide dismutase and glyoxalase I enzymes in bacteria and several parasitic eukaryotic trypanosomal parasites (in higher organisms, including yeast and mammals, this enzyme contains divalent Zn).

Dietary nickel may affect human health through infections by nickel-dependent bacteria, but it is also possible that nickel is an essential nutrient for bacteria residing in the large intestine, in effect functioning as a prebiotic. The U.S. Institute of Medicine has not confirmed that nickel is an essential nutrient for humans, so neither a Recommended Dietary Allowance (RDA) nor an Adequate Intake have been established. The Tolerable Upper Intake Level of dietary nickel is 1000 µg/day as soluble nickel salts. Dietary intake is estimated at 70 to 100 µg/day, with less than 10% absorbed. What is absorbed is excreted in urine. Relatively large amounts of nickel – comparable to the estimated average ingestion above – leach into food cooked in stainless steel. For example, the amount of nickel leached after 10 cooking cycles into one serving of tomato sauce averages 88 µg.

Nickel released from Siberian Traps volcanic eruptions is suspected of assisting the growth of "Methanosarcina", a genus of euryarchaeote archaea that produced methane during the Permian–Triassic extinction event, the biggest extinction event on record.

The major source of nickel exposure is oral consumption, as nickel is essential to plants. Nickel is found naturally in both food and water, and may be increased by human pollution. For example, nickel-plated faucets may contaminate water and soil; mining and smelting may dump nickel into waste-water; nickel–steel alloy cookware and nickel-pigmented dishes may release nickel into food. The atmosphere may be polluted by nickel ore refining and fossil fuel combustion. Humans may absorb nickel directly from tobacco smoke and skin contact with jewelry, shampoos, detergents, and coins. A less-common form of chronic exposure is through hemodialysis as traces of nickel ions may be absorbed into the plasma from the chelating action of albumin.

The average daily exposure does not pose a threat to human health. Most of the nickel absorbed every day by humans is removed by the kidneys and passed out of the body through urine or is eliminated through the gastrointestinal tract without being absorbed. Nickel is not a cumulative poison, but larger doses or chronic inhalation exposure may be toxic, even carcinogenic, and constitute an occupational hazard.

Nickel compounds are classified as human carcinogens based on increased respiratory cancer risks observed in epidemiological studies of sulfidic ore refinery workers. This is supported by the positive results of the NTP bioassays with Ni sub-sulfide and Ni oxide in rats and mice. The human and animal data consistently indicate a lack of carcinogenicity via the oral route of exposure and limit the carcinogenicity of nickel compounds to respiratory tumours after inhalation. Nickel metal is classified as a suspect carcinogen; there is consistency between the absence of increased respiratory cancer risks in workers predominantly exposed to metallic nickel and the lack of respiratory tumours in a rat lifetime inhalation carcinogenicity study with nickel metal powder. In the rodent inhalation studies with various nickel compounds and nickel metal, increased lung inflammations with and without bronchial lymph node hyperplasia or fibrosis were observed. In rat studies, oral ingestion of water-soluble nickel salts can trigger perinatal mortality effects in pregnant animals. Whether these effects are relevant to humans is unclear as epidemiological studies of highly exposed female workers have not shown adverse developmental toxicity effects.

People can be exposed to nickel in the workplace by inhalation, ingestion, and contact with skin or eye. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for the workplace at 1 mg/m per 8-hour workday, excluding nickel carbonyl. The National Institute for Occupational Safety and Health (NIOSH) specifies the recommended exposure limit (REL) of 0.015 mg/m per 8-hour workday. At 10 mg/m, nickel is immediately dangerous to life and health. Nickel carbonyl [] is an extremely toxic gas. The toxicity of metal carbonyls is a function of both the toxicity of the metal and the off-gassing of carbon monoxide from the carbonyl functional groups; nickel carbonyl is also explosive in air.

Sensitized individuals may show a skin contact allergy to nickel known as a contact dermatitis. Highly sensitized individuals may also react to foods with high nickel content. Sensitivity to nickel may also be present in patients with pompholyx. Nickel is the top confirmed contact allergen worldwide, partly due to its use in jewelry for pierced ears. Nickel allergies affecting pierced ears are often marked by itchy, red skin. Many earrings are now made without nickel or low-release nickel to address this problem. The amount allowed in products that contact human skin is now regulated by the European Union. In 2002, researchers found that the nickel released by 1 and 2 Euro coins was far in excess of those standards. This is believed to be the result of a galvanic reaction. Nickel was voted Allergen of the Year in 2008 by the American Contact Dermatitis Society. In August 2015, the American Academy of Dermatology adopted a position statement on the safety of nickel: "Estimates suggest that contact dermatitis, which includes nickel sensitization, accounts for approximately $1.918 billion and affects nearly 72.29 million people."

Reports show that both the nickel-induced activation of hypoxia-inducible factor (HIF-1) and the up-regulation of hypoxia-inducible genes are caused by depletion of intracellular ascorbate. The addition of ascorbate to the culture medium increased the intracellular ascorbate level and reversed both the metal-induced stabilization of HIF-1- and HIF-1α-dependent gene expression.



</doc>
<doc id="21275" url="https://en.wikipedia.org/wiki?curid=21275" title="Niobium">
Niobium

Niobium, formerly known as columbium, is a chemical element with symbol Nb (formerly Cb) and atomic number 41. It is a soft, grey, crystalline, ductile transition metal, often found in the minerals pyrochlore and columbite, hence the former name "columbium". Its name comes from Greek mythology, specifically Niobe, who was the daughter of Tantalus, the namesake of tantalum. The name reflects the great similarity between the two elements in their physical and chemical properties, making them difficult to distinguish.

The English chemist Charles Hatchett reported a new element similar to tantalum in 1801 and named it columbium. In 1809, the English chemist William Hyde Wollaston wrongly concluded that tantalum and columbium were identical. The German chemist Heinrich Rose determined in 1846 that tantalum ores contain a second element, which he named niobium. In 1864 and 1865, a series of scientific findings clarified that niobium and columbium were the same element (as distinguished from tantalum), and for a century both names were used interchangeably. Niobium was officially adopted as the name of the element in 1949, but the name columbium remains in current use in metallurgy in the United States.

It was not until the early 20th century that niobium was first used commercially. Brazil is the leading producer of niobium and ferroniobium, an alloy of 60–70% niobium with iron. Niobium is used mostly in alloys, the largest part in special steel such as that used in gas pipelines. Although these alloys contain a maximum of 0.1%, the small percentage of niobium enhances the strength of the steel. The temperature stability of niobium-containing superalloys is important for its use in jet and rocket engines.

Niobium is used in various superconducting materials. These superconducting alloys, also containing titanium and tin, are widely used in the superconducting magnets of MRI scanners. Other applications of niobium include welding, nuclear industries, electronics, optics, numismatics, and jewelry. In the last two applications, the low toxicity and iridescence produced by anodization are highly desired properties.

Niobium was identified by English chemist Charles Hatchett in 1801. He found a new element in a mineral sample that had been sent to England from Connecticut, United States in 1734 by John Winthrop F.R.S. (grandson of John Winthrop the Younger) and named the mineral "columbite" and the new element "columbium" after "Columbia", the poetical name for the United States. The "columbium" discovered by Hatchett was probably a mixture of the new element with tantalum.

Subsequently, there was considerable confusion over the difference between columbium (niobium) and the closely related tantalum. In 1809, English chemist William Hyde Wollaston compared the oxides derived from both columbium—columbite, with a density 5.918 g/cm, and tantalum—tantalite, with a density over 8 g/cm, and concluded that the two oxides, despite the significant difference in density, were identical; thus he kept the name tantalum. This conclusion was disputed in 1846 by German chemist Heinrich Rose, who argued that there were two different elements in the tantalite sample, and named them after children of Tantalus: "niobium" (from Niobe) and "pelopium" (from Pelops). This confusion arose from the minimal observed differences between tantalum and niobium. The claimed new elements "pelopium", "ilmenium", and "dianium" were in fact identical to niobium or mixtures of niobium and tantalum.

The differences between tantalum and niobium were unequivocally demonstrated in 1864 by Christian Wilhelm Blomstrand and Henri Etienne Sainte-Claire Deville, as well as Louis J. Troost, who determined the formulas of some of the compounds in 1865 and finally by Swiss chemist Jean Charles Galissard de Marignac in 1866, who all proved that there were only two elements. Articles on "ilmenium" continued to appear until 1871.

De Marignac was the first to prepare the metal in 1864, when he reduced niobium chloride by heating it in an atmosphere of hydrogen. Although de Marignac was able to produce tantalum-free niobium on a larger scale by 1866, it was not until the early 20th century that niobium was used in incandescent lamp filaments, the first commercial application. This use quickly became obsolete through the replacement of niobium with tungsten, which has a higher melting point. That niobium improves the strength of steel was first discovered in the 1920s, and this application remains its predominant use. In 1961, the American physicist Eugene Kunzler and coworkers at Bell Labs discovered that niobium-tin continues to exhibit superconductivity in the presence of strong electric currents and magnetic fields, making it the first material to support the high currents and fields necessary for useful high-power magnets and electrical power machinery. This discovery enabled — two decades later — the production of long multi-strand cables wound into coils to create large, powerful electromagnets for rotating machinery, particle accelerators, and particle detectors.

"Columbium" (symbol "Cb") was the name originally bestowed by Hatchett upon his discovery of the metal in 1801. The name reflected that the type specimen of the ore came from America (Columbia). This name remained in use in American journals—the last paper published by American Chemical Society with "columbium" in its title dates from 1953—while "niobium" was used in Europe. To end this confusion, the name "niobium" was chosen for element 41 at the 15th Conference of the Union of Chemistry in Amsterdam in 1949. A year later this name was officially adopted by the International Union of Pure and Applied Chemistry (IUPAC) after 100 years of controversy, despite the chronological precedence of the name "columbium". This was a compromise of sorts; the IUPAC accepted tungsten instead of wolfram in deference to North American usage; and "niobium" instead of "columbium" in deference to European usage. While many US chemical societies and government organizations typically use the official IUPAC name, some metallurgists and metal societies still use the original American name, ""columbium"".

Niobium is a lustrous, grey, ductile, paramagnetic metal in group 5 of the periodic table (see table), with an electron configuration in the outermost shells atypical for group 5. (This can be observed in the neighborhood of ruthenium (44), rhodium (45), and palladium (46).)

Although it is thought to have a body-centered cubic crystal structure from absolute zero to its melting point, high-resolution measurements of the thermal expansion along the three crystallographic axes reveal anisotropies which are inconsistent with a cubic structure. Therefore, further research and discovery in this area is expected.

Niobium becomes a superconductor at cryogenic temperatures. At atmospheric pressure, it has the highest critical temperature of the elemental superconductors at 9.2 K. Niobium has the greatest magnetic penetration depth of any element. In addition, it is one of the three elemental Type II superconductors, along with vanadium and technetium. The superconductive properties are strongly dependent on the purity of the niobium metal.

When very pure, it is comparatively soft and ductile, but impurities make it harder.

The metal has a low capture cross-section for thermal neutrons; thus it is used in the nuclear industries where neutron transparent structures are desired.

The metal takes on a bluish tinge when exposed to air at room temperature for extended periods. Despite a high melting point in elemental form (2,468 °C), it has a lower density than other refractory metals. Furthermore, it is corrosion-resistant, exhibits superconductivity properties, and forms dielectric oxide layers.

Niobium is slightly less electropositive and more compact than its predecessor in the periodic table, zirconium, whereas it is virtually identical in size to the heavier tantalum atoms, as a result of the lanthanide contraction. As a result, niobium's chemical properties are very similar to those for tantalum, which appears directly below niobium in the periodic table. Although its corrosion resistance is not as outstanding as that of tantalum, the lower price and greater availability make niobium attractive for less demanding applications, such as vat linings in chemical plants.

Niobium in the Earth's crust comprises one stable isotope, Nb. By 2003, at least 32 radioisotopes had been synthesized, ranging in atomic mass from 81 to 113. The most stable of these is Nb with a half-life of 34.7 million years. One of the least stable is Nb, with an estimated half-life of 30 milliseconds. Isotopes that are lighter than the stable Nb tend to decay by β decay, and those that are heavier tend to decay by β decay, with some exceptions. Nb, Nb, and Nb have minor β delayed proton emission decay paths, Nb decays by electron capture and positron emission, and Nb decays by both β and β decay.

At least 25 nuclear isomers have been described, ranging in atomic mass from 84 to 104. Within this range, only Nb, Nb, and Nb do not have isomers. The most stable of niobium's isomers is Nb with a half-life of 16.13 years. The least stable isomer is Nb with a half-life of 103 ns. All of niobium's isomers decay by isomeric transition or beta decay except Nb, which has a minor electron capture branch.

Niobium is estimated to be the 34th most common element in the Earth’s crust, with 20 ppm. Some think that the abundance on Earth is much greater, and that the element's high density has concentrated it in the Earth’s core. The free element is not found in nature, but niobium occurs in combination with other elements in minerals. Minerals that contain niobium often also contain tantalum. Examples include columbite ((Fe,Mn)(Nb,Ta)O) and columbite–tantalite (or "coltan", (Fe,Mn)(Ta,Nb)O). Columbite–tantalite minerals are most usually found as accessory minerals in pegmatite intrusions, and in alkaline intrusive rocks. Less common are the niobates of calcium, uranium, thorium and the rare earth elements. Examples of such niobates are pyrochlore ((Na,Ca)NbO(OH,F)) and euxenite ((Y,Ca,Ce,U,Th)(Nb,Ta,Ti)O). These large deposits of niobium have been found associated with carbonatites (carbonate-silicate igneous rocks) and as a constituent of pyrochlore. 

The three largest currently mined deposits of pyrochlore, two in Brazil and one in Canada, were found in the 1950s, and are still the major producers of niobium mineral concentrates. The largest deposit is hosted within a carbonatite intrusion in Araxá, state of Minas Gerais, Brazil, owned by CBMM (Companhia Brasileira de Metalurgia e Mineração); the other active Brazilian deposit is located near Catalão, state of Goiás, and owned by China Molybdenum, also hosted within a carbonatite intrusion. Together, those two mines produce about 88% of the world's supply. Brazil also has a large but still unexploited deposit near São Gabriel da Cachoeira, state of Amazonas, as well as a few smaller deposits, notably in the state of Roraima.

The third largest producer of niobium is the carbonatite-hosted Niobec mine, in Saint-Honoré, near Chicoutimi, Quebec, Canada, owned by Magris Resources. It produces between 7% and 10% of the world's supply.

After the separation from the other minerals, the mixed oxides of tantalum TaO and niobium NbO are obtained. The first step in the processing is the reaction of the oxides with hydrofluoric acid:

The first industrial scale separation, developed by de Marignac, exploits the differing solubilities of the complex niobium and tantalum fluorides, dipotassium oxypentafluoroniobate monohydrate (K<nowiki>[</nowiki>NbOF<nowiki>]</nowiki>·HO) and dipotassium heptafluorotantalate (K<nowiki>[</nowiki>TaF<nowiki>]</nowiki>) in water. Newer processes use the liquid extraction of the fluorides from aqueous solution by organic solvents like cyclohexanone. The complex niobium and tantalum fluorides are extracted separately from the organic solvent with water and either precipitated by the addition of potassium fluoride to produce a potassium fluoride complex, or precipitated with ammonia as the pentoxide:

Followed by:

Several methods are used for the reduction to metallic niobium. The electrolysis of a molten mixture of K<nowiki>[</nowiki>NbOF<nowiki>]</nowiki> and sodium chloride is one; the other is the reduction of the fluoride with sodium. With this method, a relatively high purity niobium can be obtained. In large scale production, NbO is reduced with hydrogen or carbon. In the aluminothermic reaction, a mixture of iron oxide and niobium oxide is reacted with aluminium:

Small amounts of oxidizers like sodium nitrate are added to enhance the reaction. The result is aluminium oxide and ferroniobium, an alloy of iron and niobium used in the steel production. Ferroniobium contains between 60 and 70% niobium. Without iron oxide, the aluminothermic process is used to produce niobium. Further purification is necessary to reach the grade for superconductive alloys. Electron beam melting under vacuum is the method used by the two major distributors of niobium.

, CBMM from Brazil controlled 85 percent of the world's niobium production. The United States Geological Survey estimates that the production increased from 38,700 tonnes in 2005 to 44,500 tonnes in 2006. Worldwide resources are estimated to be 4,400,000 tonnes. During the ten-year period between 1995 and 2005, the production more than doubled, starting from 17,800 tonnes in 1995. Between 2009 and 2011, production was stable at 63,000 tonnes per year, with a slight decrease in 2012 to only 50,000 tonnes per year.

Lesser amounts are found in Malawi's Kanyika Deposit (Kanyika mine).

In many ways, niobium is similar to tantalum and zirconium. It reacts with most nonmetals at high temperatures; with fluorine at room temperature; with chlorine and hydrogen at 200 °C; and with nitrogen at 400 °C, with products that are frequently interstitial and nonstoichiometric. The metal begins to oxidize in air at 200 °C. It resists corrosion by fused alkalis and by acids, including aqua regia, hydrochloric, sulfuric, nitric and phosphoric acids. Niobium is attacked by hydrofluoric acid and hydrofluoric/nitric acid mixtures.

Although niobium exhibits all of the formal oxidation states from +5 to −1, the most common compounds have niobium in the +5 state. Characteristically, compounds in oxidation states less than 5+ display Nb–Nb bonding.

Niobium forms oxides in the oxidation states +5 (NbO), +4 (NbO), +3 (), and the rarer oxidation state, +2 (NbO). Most common is the pentoxide, precursor to almost all niobium compounds and alloys. Niobates are generated by dissolving the pentoxide in basic hydroxide solutions or by melting it in alkali metal oxides. Examples are lithium niobate (LiNbO) and lanthanum niobate (LaNbO). In the lithium niobate is a trigonally distorted perovskite-like structure, whereas the lanthanum niobate contains lone ions. The layered niobium sulfide (NbS) is also known.

Materials can be coated with a thin film of niobium(V) oxide chemical vapor deposition or atomic layer deposition processes, produced by the thermal decomposition of niobium(V) ethoxide above 350 °C.

Niobium forms halides in the oxidation states of +5 and +4 as well as diverse substoichiometric compounds. The pentahalides () feature octahedral Nb centres. Niobium pentafluoride (NbF) is a white solid with a melting point of 79.0 °C and niobium pentachloride (NbCl) is yellow (see image at left) with a melting point of 203.4 °C. Both are hydrolyzed to give oxides and oxyhalides, such as NbOCl. The pentachloride is a versatile reagent used to generate the organometallic compounds, such as niobocene dichloride (). The tetrahalides () are dark-coloured polymers with Nb-Nb bonds; for example, the black hygroscopic niobium tetrafluoride (NbF) and brown niobium tetrachloride (NbCl).

Anionic halide compounds of niobium are well known, owing in part to the Lewis acidity of the pentahalides. The most important is [NbF], an intermediate in the separation of Nb and Ta from the ores. This heptafluoride tends to form the oxopentafluoride more readily than does the tantalum compound. Other halide complexes include octahedral [NbCl]:

As with other metals with low atomic numbers, a variety of reduced halide cluster ions is known, the prime example being [NbCl].

Other binary compounds of niobium include niobium nitride (NbN), which becomes a superconductor at low temperatures and is used in detectors for infrared light. The main niobium carbide is NbC, an extremely hard, refractory, ceramic material, commercially used in cutting tool bits.

Out of 44,500 tonnes of niobium mined in 2006, an estimated 90% was used in high-grade structural steel. The second largest application is superalloys. Niobium alloy superconductors and electronic components account for a very small share of the world production.

Niobium is an effective microalloying element for steel, within which it forms niobium carbide and niobium nitride. These compounds improve the grain refining, and retard recrystallization and precipitation hardening. These effects in turn increase the toughness, strength, formability, and weldability. Within microalloyed stainless steels, the niobium content is a small (less than 0.1%) but important addition to high strength low alloy steels that are widely used structurally in modern automobiles. Niobium is sometimes used in considerably higher quantities for highly wear-resistant machine components and knives, as high as 3% in Crucible CPM S110V stainless steel.

These same niobium alloys are often used in pipeline construction.

Quantities of niobium are used in nickel-, cobalt-, and iron-based superalloys in proportions as great as 6.5% for such applications as jet engine components, gas turbines, rocket subassemblies, turbo charger systems, heat resisting, and combustion equipment. Niobium precipitates a hardening γ<nowiki>"</nowiki>-phase within the grain structure of the superalloy.

One example superalloy is Inconel 718, consisting of roughly 50% nickel, 18.6% chromium, 18.5% iron, 5% niobium, 3.1% molybdenum, 0.9% titanium, and 0.4% aluminium. These superalloys were used, for example, in advanced air frame systems for the Gemini program. Another niobium alloy was used for the nozzle of the Apollo Service Module. Because niobium is oxidized at temperatures above 400 °C, a protective coating is necessary for these applications to prevent the alloy from becoming brittle.

C-103 alloy was developed in the early 1960s jointly by the Wah Chang Corporation and Boeing Co. DuPont, Union Carbide Corp., General Electric Co. and several other companies were developing Nb-base alloys simultaneously, largely driven by the Cold War and Space Race. It is composed of 89% niobium, 10% hafnium and 1% titanium and is used for liquid rocket thruster nozzles, such as the main engine of the Apollo Lunar Modules.

The nozzle of the Merlin Vacuum series of engines developed by SpaceX for the upper stage of its Falcon 9 rocket is made from a niobium alloy.

The reactivity of niobium with oxygen requires it to be worked in a vacuum or inert atmosphere, which significantly increases the cost and difficulty of production. Vacuum arc remelting (VAR) and electron beam melting (EBM), novel processes at the time, enabled the development of niobium and other reactive metals. The project that yielded C-103 began in 1959 with as many as 256 experimental niobium alloys in the "C-series" (possibly from columbium) that could be melted as buttons and rolled into sheet. Wah Chang had an inventory of hafnium, refined from nuclear-grade zirconium alloys, that it wanted to put to commercial use. The 103rd experimental composition of the C-series alloys, Nb-10Hf-1Ti, had the best combination of formability and high-temperature properties. Wah Chang fabricated the first 500-lb heat of C-103 in 1961, ingot to sheet, using EBM and VAR. The intended applications included turbine engines and liquid metal heat exchangers. Competing niobium alloys from that era included FS85 (Nb-10W-28Ta-1Zr) from Fansteel Metallurgical Corp., Cb129Y (Nb-10W-10Hf-0.2Y) from Wah Chang and Boeing, Cb752 (Nb-10W-2.5Zr) from Union Carbide, and Nb1Zr from Superior Tube Co.

Niobium-germanium (), niobium-tin (), as well as the niobium-titanium alloys are used as a type II superconductor wire for superconducting magnets. These superconducting magnets are used in magnetic resonance imaging and nuclear magnetic resonance instruments as well as in particle accelerators. For example, the Large Hadron Collider uses 600 tons of superconducting strands, while the International Thermonuclear Experimental Reactor uses an estimated 600 tonnes of NbSn strands and 250 tonnes of NbTi strands. In 1992 alone, more than US$1 billion worth of clinical magnetic resonance imaging systems were constructed with niobium-titanium wire.

The superconducting radio frequency (SRF) cavities used in the free-electron lasers FLASH (result of the cancelled TESLA linear accelerator project) and XFEL are made from pure niobium. A cryomodule team at Fermilab used the same SRF technology from the FLASH project to develop 1.3 GHz nine-cell SRF cavities made from pure niobium. The cavities will be used in the linear particle accelerator of the International Linear Collider. The same technology will be used in LCLS-II at SLAC National Accelerator Laboratory and PIP-II at Fermilab.

The high sensitivity of superconducting niobium nitride bolometers make them an ideal detector for electromagnetic radiation in the THz frequency band. These detectors were tested at the Submillimeter Telescope, the South Pole Telescope, the Receiver Lab Telescope, and at APEX, and are now used in the HIFI instrument on board the Herschel Space Observatory.

Lithium niobate, which is a ferroelectric, is used extensively in mobile telephones and optical modulators, and for the manufacture of surface acoustic wave devices. It belongs to the ABO structure ferroelectrics like lithium tantalate and barium titanate. Niobium capacitors are available as alternative to tantalum capacitors, but tantalum capacitors still predominate. Niobium is added to glass to obtain a higher refractive index, making possible thinner and lighter corrective glasses.

Niobium and some niobium alloys are physiologically inert and hypoallergenic. For this reason, niobium is used in prosthetics and implant devices, such as pacemakers. Niobium treated with sodium hydroxide forms a porous layer that aids osseointegration.

Like titanium, tantalum, and aluminium, niobium can be heated and anodized ("reactive metal anodization") to produce a wide array of iridescent colours for jewelry, where its hypoallergenic property is highly desirable.

Niobium is used as a precious metal in commemorative coins, often with silver or gold. For example, Austria produced a series of silver niobium euro coins starting in 2003; the colour in these coins is created by the diffraction of light by a thin anodized oxide layer. In 2012, ten coins are available showing a broad variety of colours in the centre of the coin: blue, green, brown, purple, violet, or yellow. Two more examples are the 2004 Austrian €25 150 Years Semmering Alpine Railway commemorative coin, and the 2006 Austrian €25 European Satellite Navigation commemorative coin.
The Austrian mint produced for Latvia a similar series of coins starting in 2004,
with one following in 2007.
In 2011, the Royal Canadian Mint started production of a $5 sterling silver and niobium coin named "Hunter's Moon"
in which the niobium was selectively oxidized, thus creating unique finishes where no two coins are exactly alike.

The arc-tube seals of high pressure sodium vapor lamps are made from niobium, sometimes alloyed with 1% of zirconium; niobium has a very similar coefficient of thermal expansion, matching the sintered alumina arc tube ceramic, a translucent material which resists chemical attack or reduction by the hot liquid sodium and sodium vapour contained inside the operating lamp.

Niobium is used in arc welding rods for some stabilized grades of stainless steel and in anodes for cathodic protection systems on some water tanks, which are then usually plated with platinum.

Niobium is an important component of high-performance heterogeneous catalysts for the production of acrylic acid by selective oxidation of propane.

Niobium has no known biological role. While niobium dust is an eye and skin irritant and a potential fire hazard, elemental niobium on a larger scale is physiologically inert (and thus hypoallergenic) and harmless. It is frequently used in jewelry and has been tested for use in some medical implants.

Niobium-containing compounds are rarely encountered by most people, but some are toxic and should be treated with care. The short- and long-term exposure to niobates and niobium chloride, two chemicals that are water-soluble, have been tested in rats. Rats treated with a single injection of niobium pentachloride or niobates show a median lethal dose (LD) between 10 and 100 mg/kg. For oral administration the toxicity is lower; a study with rats yielded a LD after seven days of 940 mg/kg.



</doc>
<doc id="21276" url="https://en.wikipedia.org/wiki?curid=21276" title="Neodymium">
Neodymium

Neodymium is a chemical element with symbol Nd and atomic number 60. It is a soft silvery metal that tarnishes in air. Neodymium was discovered in 1885 by the Austrian chemist Carl Auer von Welsbach. It is present in significant quantities in the ore minerals monazite and bastnäsite. Neodymium is not found naturally in metallic form or unmixed with other lanthanides, and it is usually refined for general use. Although neodymium is classed as a rare earth, it is a fairly common element, no rarer than cobalt, nickel, or copper, and is widely distributed in the Earth's crust. Most of the world's commercial neodymium is mined in China.

Neodymium compounds were first commercially used as glass dyes in 1927, and they remain a popular additive in glasses. The color of neodymium compounds—due to the Nd ion—is often a reddish-purple but it changes with the type of lighting, due to the interaction of the sharp light absorption bands of neodymium with ambient light enriched with the sharp visible emission bands of mercury, trivalent europium or terbium. Some neodymium-doped glasses are also used in lasers that emit infrared with wavelengths between 1047 and 1062 nanometers. These have been used in extremely-high-power applications, such as experiments in inertial confinement fusion.

Neodymium is also used with various other substrate crystals, such as yttrium aluminium garnet in the . This laser usually emits infrared at a wavelength of about 1064 nanometers. The Nd:YAG laser is one of the most commonly used solid-state lasers.

Another important use of neodymium is as a component in the alloys used to make high-strength neodymium magnets—powerful permanent magnets. These magnets are widely used in such products as microphones, professional loudspeakers, in-ear headphones, high performance hobby DC electric motors, and computer hard disks, where low magnet mass (or volume) or strong magnetic fields are required. Larger neodymium magnets are used in high-power-versus-weight electric motors (for example in hybrid cars) and generators (for example aircraft and wind turbine electric generators).

Neodymium, a rare-earth metal, was present in the classical mischmetal at a concentration of about 18%. Metallic neodymium has a bright, silvery metallic luster, but as one of the more reactive lanthanide rare-earth metals, it quickly oxidizes in ordinary air. The oxide layer that forms then peels off, exposing the metal to further oxidation. Thus, a centimeter-sized sample of neodymium completely oxidizes within a year.

Neodymium commonly exists in two allotropic forms, with a transformation from a double hexagonal to a body-centered cubic structure taking place at about 863 °C.

Neodymium metal tarnishes slowly in air and it burns readily at about 150 °C to form neodymium(III) oxide:

Neodymium is a quite electropositive element, and it reacts slowly with cold water, but quite quickly with hot water to form neodymium(III) hydroxide:

Neodymium metal reacts vigorously with all the halogens:

Neodymium dissolves readily in dilute sulfuric acid to form solutions that contain the lilac Nd(III) ion. These exist as a [Nd(OH)] complexes:

Neodymium compounds include


Some neodymium compounds have colors that vary based upon the type of lighting.

Naturally occurring neodymium is a mixture of five stable isotopes, Nd, Nd, Nd, Nd and Nd, with Nd being the most abundant (27.2% of the natural abundance), and two radioisotopes, Nd and Nd. In all, 31 radioisotopes of neodymium have been detected , with the most stable radioisotopes being the naturally occurring ones: Nd (alpha decay with a half-life ("t") of 2.29×10 years) and Nd (double beta decay, "t" = 7×10 years, approximately). All of the remaining radioactive isotopes have half-lives that are shorter than eleven days, and the majority of these have half-lives that are shorter than 70 seconds. Neodymium also has 13 known meta states, with the most stable one being Nd ("t" = 5.5 hours), Nd ("t" = 5.5 minutes) and Nd ("t" ~70 seconds).

The primary decay modes before the most abundant stable isotope, Nd, are electron capture and positron decay, and the primary mode after is beta minus decay. The primary decay products before Nd are element Pr (praseodymium) isotopes and the primary products after are element Pm (promethium) isotopes.

Neodymium was discovered by Baron Carl Auer von Welsbach, an Austrian chemist, in Vienna in 1885. He separated neodymium, as well as the element praseodymium, from a material known as didymium by means of fractional crystallization of the double ammonium nitrate tetrahydrates from nitric acid, while following the separation by spectroscopic analysis; however, it was not isolated in relatively pure form until 1925. The name neodymium is derived from the Greek words "neos" (νέος), new, and "didymos" (διδύμος), twin.

Double nitrate crystallization was the means of commercial neodymium purification until the 1950s. Lindsay Chemical Division was the first to commercialize large-scale ion-exchange purification of neodymium. Starting in the 1950s, high purity (above 99%) neodymium was primarily obtained through an ion exchange process from monazite, a mineral rich in rare-earth elements. The metal itself is obtained through electrolysis of its halide salts. Currently, most neodymium is extracted from bastnäsite, (Ce,La,Nd,Pr)COF, and purified by solvent extraction. Ion-exchange purification is reserved for preparing the highest purities (typically >99.99%). The evolving technology, and improved purity of commercially available neodymium oxide, was reflected in the appearance of neodymium glass that resides in collections today. Early neodymium glasses made in the 1930s have a more reddish or orange tinge than modern versions which are more cleanly purple, due to the difficulties in removing the last traces of praseodymium in the era when manufacturing relied upon fractional crystallization technology.

Neodymium is rarely found in nature as a free element, but rather it occurs in ores such as monazite and bastnäsite (these are mineral group names rather than single mineral names) that contain small amounts of all rare-earth metals. In these minerals neodymium is rarely dominant (as in the case of lanthanum), with cerium being the most abundant lanthanide; some exceptions include monazite-(Nd) and kozoite-(Nd). The main mining areas are in China, the United States, Brazil, India, Sri Lanka, and Australia. The reserves of neodymium are estimated at about eight million tonnes. Although it belongs to the rare-earth metals, neodymium is not rare at all. Its abundance in the Earth's crust is about 38 mg/kg, which is the second highest among rare-earth elements, following cerium. The world's production of neodymium was about 7,000 tonnes in 2004. The bulk of current production is from China. the Chinese government has imposed strategic material controls on the element, raising some concerns in consuming countries and causing skyrocketing prices of neodymium and other rare-earth metals. As of late 2011, 99% pure neodymium was traded in world markets for US$300 to US$350 per kilogram, down from the mid-2011 peak of US$500/kg. The price of neodymium oxide fell from $200/kg in 2011 to $40 in 2015, largely due to illegal production in China which circumvented government restrictions. The uncertainty of pricing and availability have caused companies (particularly Japanese ones) to create permanent magnets and associated electric motors with fewer or no rare-earth metals.

Neodymium is typically 10–18% of the rare-earth content of commercial deposits of the light rare-earth-element minerals bastnäsite and monazite. With neodymium compounds being the most strongly colored for the trivalent lanthanides, it can occasionally dominate the coloration of rare-earth minerals when competing chromophores are absent. It usually gives a pink coloration. Outstanding examples of this include monazite crystals from the tin deposits in Llallagua, Bolivia; ancylite from Mont Saint-Hilaire, Quebec, Canada; or lanthanite from the Saucon Valley, Pennsylvania, United States. As with neodymium glasses, such minerals change their colors under the differing lighting conditions. The absorption bands of neodymium interact with the visible emission spectrum of mercury vapor, with the unfiltered shortwave UV light causing neodymium-containing minerals to reflect a distinctive green color. This can be observed with monazite-containing sands or bastnäsite-containing ore.


Neodymium magnets (actually an alloy, NdFeB) are the strongest permanent magnets known. A neodymium magnet of a few grams can lift a thousand times its own weight. These magnets are cheaper, lighter, and stronger than samarium–cobalt magnets. However, they are not superior in every aspect, as neodymium-based magnets lose their magnetism at lower temperatures and tend to rust, while samarium–cobalt magnets do not.

Neodymium magnets appear in products such as microphones, professional loudspeakers, in-ear headphones, guitar and bass guitar pick-ups, and computer hard disks where low mass, small volume, or strong magnetic fields are required. Neodymium magnet electric motors have also been responsible for the development of purely electrical model aircraft within the first decade of the 21st century, to the point that these are displacing internal combustion–powered models internationally. Likewise, due to this high magnetic capacity per weight, neodymium is used in the electric motors of hybrid and electric automobiles, and in the electricity generators of some designs of commercial wind turbines (only wind turbines with "permanent magnet" generators use neodymium). For example, drive electric motors of each Toyota Prius require one kilogram (2.2 pounds) of neodymium per vehicle.

Certain transparent materials with a small concentration of neodymium ions can be used in lasers as gain media for infrared wavelengths (1054–1064 nm), e.g. (yttrium aluminium garnet), Nd:YLF (yttrium lithium fluoride), Nd:YVO (yttrium orthovanadate), and Nd:glass. Neodymium-doped crystals (typically Nd:YVO) generate high-powered infrared laser beams which are converted to green laser light in commercial DPSS hand-held lasers and laser pointers.

The current laser at the UK Atomic Weapons Establishment (AWE), the HELEN (High Energy Laser Embodying Neodymium) 1-terawatt neodymium-glass laser, can access the midpoints of pressure and temperature regions and is used to acquire data for modeling on how density, temperature, and pressure interact inside warheads. HELEN can create plasmas of around 10 K, from which opacity and transmission of radiation are measured.

Neodymium glass solid-state lasers are used in extremely high power (terawatt scale), high energy (megajoules) multiple beam systems for inertial confinement fusion. Nd:glass lasers are usually frequency tripled to the third harmonic at 351 nm in laser fusion devices.

Neodymium glass (Nd:glass) is produced by the inclusion of neodymium oxide (NdO) in the glass melt. Usually in daylight or incandescent light neodymium glass appears lavender, but it appears pale blue under fluorescent lighting. Neodymium may be used to color glass in delicate shades ranging from pure violet through wine-red and warm gray.

The first commercial use of purified neodymium was in glass coloration, starting with experiments by Leo Moser in November 1927. The resulting "Alexandrite" glass remains a signature color of the Moser glassworks to this day. Neodymium glass was widely emulated in the early 1930s by American glasshouses, most notably Heisey, Fostoria ("wisteria"), Cambridge ("heatherbloom"), and Steuben ("wisteria"), and elsewhere (e.g. Lalique, in France, or Murano). Tiffin's "twilight" remained in production from about 1950 to 1980. Current sources include glassmakers in the Czech Republic, the United States, and China.

The sharp absorption bands of neodymium cause the glass color to change under different lighting conditions, being reddish-purple under daylight or yellow incandescent light, but blue under white fluorescent lighting, or greenish under trichromatic lighting. This color-change phenomenon is highly prized by collectors. In combination with gold or selenium, beautiful red colors result. Since neodymium coloration depends upon "forbidden" f-f transitions deep within the atom, there is relatively little influence on the color from the chemical environment, so the color is impervious to the thermal history of the glass. However, for the best color, iron-containing impurities need to be minimized in the silica used to make the glass. The same forbidden nature of the f-f transitions makes rare-earth colorants less intense than those provided by most d-transition elements, so more has to be used in a glass to achieve the desired color intensity. The original Moser recipe used about 5% of neodymium oxide in the glass melt, a sufficient quantity such that Moser referred to these as being "rare-earth doped" glasses. Being a strong base, that level of neodymium would have affected the melting properties of the glass, and the lime content of the glass might have had to be adjusted accordingly.

Light transmitted through neodymium glasses shows unusually sharp absorption bands; the glass is used in astronomical work to produce sharp bands by which spectral lines may be calibrated. Another application is the creation of selective astronomical filters to reduce the effect of light pollution from sodium and fluorescent lighting while passing other colours, especially dark red hydrogen-alpha emission from nebulae. Neodymium is also used to remove the green color caused by iron contaminants from glass.

Neodymium is a component of "didymium" (referring to mixture of salts of neodymium and praseodymium) used for coloring glass to make welder's and glass-blower's goggles; the sharp absorption bands obliterate the strong sodium emission at 589 nm. The similar absorption of the yellow mercury emission line at 578 nm is the principal cause of the blue color observed for neodymium glass under traditional white-fluorescent lighting.

Neodymium and didymium glass are used in color-enhancing filters in indoor photography, particularly in filtering out the yellow hues from incandescent lighting.

Similarly, neodymium glass is becoming widely used more directly in incandescent light bulbs. These lamps contain neodymium in the glass to filter out yellow light, resulting in a whiter light which is more like sunlight.

The use of neodymium in automobile rear-view mirrors, to reduce the glare at night, has been patented.

Similar to its use in glasses, neodymium salts are used as a colorant for enamels.

Neodymium metal dust is combustible and therefore an explosion hazard. Neodymium compounds, as with all rare-earth metals, are of low to moderate toxicity; however, its toxicity has not been thoroughly investigated. Neodymium dust and salts are very irritating to the eyes and mucous membranes, and moderately irritating to skin. Breathing the dust can cause lung embolisms, and accumulated exposure damages the liver. Neodymium also acts as an anticoagulant, especially when given intravenously.

Neodymium magnets have been tested for medical uses such as magnetic braces and bone repair, but biocompatibility issues have prevented widespread application. Commercially available magnets made from neodymium are exceptionally strong, and can attract each other from large distances. If not handled carefully, they come together very quickly and forcefully, causing injuries. For example, there is at least one documented case of a person losing a fingertip when two magnets he was using snapped together from 50 cm away.

Another risk of these powerful magnets is that if more than one magnet is ingested, they can pinch soft tissues in the gastrointestinal tract. This has led to at least 1,700 emergency room visits and necessitated the recall of the Buckyballs line of toys, which were construction sets of small neodymium magnets.




</doc>
<doc id="21277" url="https://en.wikipedia.org/wiki?curid=21277" title="Neptunium">
Neptunium

Neptunium is a chemical element with symbol Np and atomic number 93. A radioactive actinide metal, neptunium is the first transuranic element. Its position in the periodic table just after uranium, named after the planet Uranus, led to it being named after Neptune, the next planet beyond Uranus. A neptunium atom has 93 protons and 93 electrons, of which seven are valence electrons. Neptunium metal is silvery and tarnishes when exposed to air. The element occurs in three allotropic forms and it normally exhibits five oxidation states, ranging from +3 to +7. It is radioactive, poisonous, pyrophoric, and can accumulate in bones, which makes the handling of neptunium dangerous.

Although many false claims of its discovery were made over the years, the element was first synthesized by Edwin McMillan and Philip H. Abelson at the Berkeley Radiation Laboratory in 1940. Since then, most neptunium has been and still is produced by neutron irradiation of uranium in nuclear reactors. The vast majority is generated as a by-product in conventional nuclear power reactors. While neptunium itself has no commercial uses at present, it is used as a precursor for the formation of plutonium-238, used in radioisotope thermal generators to provide electricity for spacecraft. Neptunium has also been used in detectors of high-energy neutrons.

The most stable isotope of neptunium, neptunium-237, is a by-product of nuclear reactors and plutonium production. It, and the isotope neptunium-239, are also found in trace amounts in uranium ores due to neutron capture reactions and beta decay.

Neptunium is a hard, silvery, ductile, radioactive actinide metal. In the periodic table, it is located to the right of the actinide uranium, to the left of the actinide plutonium and below the lanthanide promethium. Neptunium is a hard metal, having a bulk modulus of 118 GPa, comparable to that of manganese. Neptunium metal is similar to uranium in terms of physical workability. When exposed to air at normal temperatures, it forms a thin oxide layer. This reaction proceeds more rapidly as the temperature increases. Neptunium has been determined to melt at 639±3 °C: this low melting point, a property the metal shares with the neighboring element plutonium (which has melting point 639.4 °C), is due to the hybridization of the 5f and 6d orbitals and the formation of directional bonds in the metal. The boiling point of neptunium is not empirically known and the usually given value of 4174 °C is extrapolated from the vapor pressure of the element. If accurate, this would give neptunium the largest liquid range of any element (3535 K passes between its melting and boiling points).

Neptunium is found in at least three allotropes. Some claims of a fourth allotrope have been made, but they are so far not proven. This multiplicity of allotropes is common among the actinides. The crystal structures of neptunium, protactinium, uranium, and plutonium do not have clear analogs among the lanthanides and are more similar to those of the 3d transition metals.

α-neptunium takes on an orthorhombic structure, resembling a highly distorted body-centered cubic structure. Each neptunium atom is coordinated to four others and the Np–Np bond lengths are 260 pm. It is the densest of all the actinides and the fifth-densest of all naturally occurring elements, behind only rhenium, platinum, iridium, and osmium. α-neptunium has semimetallic properties, such as strong covalent bonding and a high electrical resistivity, and its metallic physical properties are closer to those of the metalloids than the true metals. Some allotropes of the other actinides also exhibit similar behaviour, though to a lesser degree. The densities of different isotopes of neptunium in the alpha phase are expected to be observably different: α-Np should have density 20.303 g/cm; α-Np, density 20.389 g/cm; α-Np, density 20.476 g/cm.

β-neptunium takes on a distorted tetragonal close-packed structure. Four atoms of neptunium make up a unit cell, and the Np–Np bond lengths are 276 pm. γ-neptunium has a body-centered cubic structure and has Np–Np bond length of 297 pm. The γ form becomes less stable with increased pressure, though the melting point of neptunium also increases with pressure. The β-Np/γ-Np/liquid triple point occurs at 725 °C and 3200 MPa.

Due to the presence of valence 5f electrons, neptunium and its alloys exhibit very interesting magnetic behavior, like many other actinides. These can range from the itinerant band-like character characteristic of the transition metals to the local moment behavior typical of scandium, yttrium, and the lanthanides. This stems from 5f-orbital hybridization with the orbitals of the metal ligands, and the fact that the 5f orbital is relativistically destabilized and extends outwards. For example, pure neptunium is paramagnetic, NpAl is ferromagnetic, NpGe has no magnetic ordering, and NpSn behaves fermionically. Investigations are underway regarding alloys of neptunium with uranium, americium, plutonium, zirconium, and iron, so as to recycle long-lived waste isotopes such as neptunium-237 into shorter-lived isotopes more useful as nuclear fuel.

One neptunium-based superconductor alloy has been discovered with formula NpPdAl. This occurrence in neptunium compounds is somewhat surprising because they often exhibit strong magnetism, which usually destroys superconductivity. The alloy has a tetragonal structure with a superconductivity transition temperature of −268.3 °C (4.9 K).

Neptunium has five ionic oxidation states ranging from +3 to +7 when forming chemical compounds, which can be simultaneously observed in solutions. It is the heaviest actinide that can lose all its valence electrons in a stable compound. The most stable state in solution is +5, but the valence +4 is preferred in solid neptunium compounds. Neptunium metal is very reactive. Ions of neptunium are prone to hydrolysis and formation of coordination compounds.

A neptunium atom has 93 electrons, arranged in the configuration <nowiki>[</nowiki>Rn<nowiki>]</nowiki>5f6d7s. This differs from the configuration expected by the Aufbau principle in that one electron is in the 6d subshell instead of being as expected in the 5f subshell. This is because of the similarity of the electron energies of the 5f, 6d, and 7s subshells. In forming compounds and ions, all the valence electrons may be lost, leaving behind an inert core of inner electrons with the electron configuration of the noble gas radon; more commonly, only some of the valence electrons will be lost. The electron configuration for the tripositive ion Np is [Rn] 5f, with the outermost 7s and 6d electrons lost first: this is exactly analogous to neptunium's lanthanide homolog promethium, and conforms to the trend set by the other actinides with their [Rn] 5f electron configurations in the tripositive state. The first ionization potential of neptunium was measured to be at most in 1974, based on the assumption that the 7s electrons would ionize before 5f and 6d; more recent measurements have refined this to 6.2657 eV.

20 neptunium radioisotopes have been characterized with the most stable being Np with a half-life of 2.14 million years, Np with a half-life of 154,000 years, and Np with a half-life of 396.1 days. All of the remaining radioactive isotopes have half-lives that are less than 4.5 days, and the majority of these have half-lives that are less than 50 minutes. This element also has at least four meta states, with the most stable being Np with a half-life of 22.5 hours.

The isotopes of neptunium range in atomic weight from 225.0339 u (Np) to 244.068 u (Np). Most of the isotopes that are lighter than the most stable one, Np, decay primarily by electron capture although a sizable number, most notably Np and Np, also exhibit various levels of decay via alpha emission to become protactinium. Np itself, being the beta-stable isobar of mass number 237, decays almost exclusively by alpha emission into Pa, with very rare (occurring only about once in trillions of decays) spontaneous fission and cluster decay (emission of Mg to form Tl). All of the known isotopes except one that are heavier than this decay exclusively via beta emission. The lone exception, Np, exhibits a rare (>0.12%) decay by isomeric transition in addition to the beta emission. Np eventually decays to form bismuth-209 and thallium-205, unlike most other common heavy nuclei which decay into isotopes of lead. This decay chain is known as the neptunium series. This decay chain had long been extinct on Earth due to the short half-lives of all of its isotopes above bismuth-209, but is now being resurrected thanks to artificial production of neptunium on the tonne scale.

The isotopes neptunium-235, -236, and -237 are predicted to be fissile; only neptunium-237's fissionability has been experimentally shown, with the critical mass being about 60 kg, only about 10 kg more than that of the commonly used uranium-235. Calculated values of the critical masses of neptunium-235, -236, and -237 respectively are 66.2 kg, 6.79 kg, and 63.6 kg: the neptunium-236 value is even lower than that of plutonium-239. In particular Np also has a low neutron cross section. Despite this, a neptunium atomic bomb has never been built: uranium and plutonium have lower critical masses than Np and Np, and Np is difficult to purify as it is not found in quantity in spent nuclear fuel and is nearly impossible to separate in any significant quantities from its parent Np.

Since all isotopes of neptunium have half-lives that are many times shorter than the age of the Earth, any primordial neptunium should have decayed by now. After only about 80 million years, the concentration of even the longest lived isotope, Np, would have been reduced to less than one-trillionth (10) of its original amount; and even if the whole Earth had initially been made of pure Np (and ignoring that this would be well over its critical mass of 60 kg), 2100 half-lives would have passed since the formation of the Solar System, and thus all of it would have decayed. Thus neptunium is present in nature only in negligible amounts produced as intermediate decay products of other isotopes.

Trace amounts of the neptunium isotopes neptunium-237 and -239 are found naturally as decay products from transmutation reactions in uranium ores. In particular, Np and Np are the most common of these isotopes; they are directly formed from neutron capture by uranium-238 atoms. These neutrons come from the spontaneous fission of uranium-238, naturally neutron-induced fission of uranium-235, cosmic ray spallation of nuclei, and light elements absorbing alpha particles and emitting a neutron. The half-life of Np is very short, although the detection of its much longer-lived daughter Pu in nature in 1951 definitively established its natural occurrence. In 1952, Np was identified and isolated from concentrates of uranium ore from the Belgian Congo: in these minerals, the ratio of neptunium-237 to uranium is less than or equal to about 10 to 1.

Most neptunium (and plutonium) now encountered in the environment is due to atmospheric nuclear explosions that took place between the detonation of the first atomic bomb in 1945 and the ratification of the Partial Nuclear Test Ban Treaty in 1963. The total amount of neptunium released by these explosions and the few atmospheric tests that have been carried out since 1963 is estimated to be around 2500 kg. The overwhelming majority of this is composed of the long-lived isotopes Np and Np since even the moderately long-lived Np (half-life 396 days) would have decayed to less than one-billionth (10) its original concentration over the intervening decades. An additional very small amount of neptunium, created by neutron irradiation of natural uranium in nuclear reactor cooling water, is released when the water is discharged into rivers or lakes. The concentration of Np in seawater is approximately 6.5 × 10 millibecquerels per liter: this concentration is between 0.1% and 1% that of plutonium.

Once in the environment, neptunium generally oxidizes fairly quickly, usually to the +4 or +5 state. Regardless of its oxidation state, the element exhibits a much greater mobility than the other actinides, largely due to its ability to readily form aqueous solutions with various other elements. In one study comparing the diffusion rates of neptunium(V), plutonium(IV), and americium(III) in sandstone and limestone, neptunium penetrated more than ten times as well as the other elements. Np(V) will also react efficiently in pH levels greater than 5.5 if there are no carbonates present and in these conditions it has also been observed to readily bond with quartz. It has also been observed to bond well with goethite, ferric oxide colloids, and several clays including kaolinite and smectite. Np(V) does not bond as readily to soil particles in mildly acidic conditions as its fellow actinides americium and curium by nearly an order of magnitude. This behavior enables it to migrate rapidly through the soil while in solution without becoming fixed in place, contributing further to its mobility. Np(V) is also readily absorbed by concrete, which because of the element's radioactivity is a consideration that must be addressed when building nuclear waste storage facilities. When absorbed in concrete, it is reduced to Np(IV) in a relatively short period of time. Np(V) is also reduced by humic acid if it is present on the surface of goethite, hematite, and magnetite. Np(IV) is absorbed efficiently by tuff, granodiorite, and bentonite; although uptake by the latter is most pronounced in mildly acidic conditions. It also exhibits a strong tendency to bind to colloidal particulates, an effect that is enhanced when in soil with a high clay content. The behavior provides an additional aid in the element's observed high mobility.

When the first periodic table of the elements was published by Dmitri Mendeleev in the early 1870s, it showed a " — " in place after uranium similar to several other places for then-undiscovered elements. Other subsequent tables of known elements, including a 1913 publication of the known radioactive isotopes by Kasimir Fajans, also show an empty place after uranium, element 92.

Up to and after the discovery of the final component of the atomic nucleus, the neutron in 1932, most scientists did not seriously consider the possibility of elements heavier than uranium. While nuclear theory at the time did not explicitly prohibit their existence, there was little evidence to suggest that they did. However, the discovery of induced radioactivity by Irène and Frédéric Joliot-Curie in late 1933 opened up an entirely new method of researching the elements and inspired a small group of Italian scientists led by Enrico Fermi to begin a series of experiments involving neutron bombardment. Although the Joliot-Curies' experiment involved bombarding a sample of Al with alpha particles to produce the radioactive P, Fermi realized that using neutrons, which have no electrical charge, would most likely produce even better results than the positively charged alpha particles. Accordingly, in March 1934 he began systematically subjecting all of the then-known elements to neutron bombardment to determine whether others could also be induced to radioactivity.

After several months of work, Fermi's group had tentatively determined that lighter elements would disperse the energy of the captured neutron by emitting a proton or alpha particle and heavier elements would generally accomplish the same by emitting a gamma ray. This latter behavior would later result in the beta decay of a neutron into a proton, thus moving the resulting isotope one place up the periodic table. When Fermi's team bombarded uranium, they observed this behavior as well, which strongly suggested that the resulting isotope had an atomic number of 93. Fermi was initially reluctant to publicize such a claim, but after his team observed several unknown half-lives in the uranium bombardment products that did not match those of any known isotope, he published a paper entitled "Possible Production of Elements of Atomic Number Higher than 92" in June 1934. In it he proposed the name ausonium (atomic symbol Ao) for element 93, after the Greek name "Ausonia" (Italy).

Several theoretical objections to the claims of Fermi's paper were quickly raised; in particular, the exact process that took place when an atom captured a neutron was not well understood at the time. This and Fermi's accidental discovery three months later that nuclear reactions could be induced by slow neutrons cast further doubt in the minds of many scientists, notably Aristid von Grosse and Ida Noddack, that the experiment was creating element 93. While von Grosse's claim that Fermi was actually producing protactinium (element 91) was quickly tested and disproved, Noddack's proposal that the uranium had been shattered into two or more much smaller fragments was simply ignored by most because existing nuclear theory did not include a way for this to be possible. Fermi and his team maintained that they were in fact synthesizing a new element, but the issue remained unresolved for several years.
Although the many different and unknown radioactive half-lives in the experiment's results showed that several nuclear reactions were occurring, Fermi's group could not prove that element 93 was being created unless they could isolate it chemically. They and many other scientists attempted to accomplish this, including Otto Hahn and Lise Meitner who were among the best radiochemists in the world at the time and supporters of Fermi's claim, but they all failed. Much later, it was determined that the main reason for this failure was because the predictions of element 93's chemical properties were based on a periodic table which lacked the actinide series. This arrangement placed protactinium below tantalum, uranium below tungsten, and further suggested that element 93, at that point referred to as eka-rhenium, should be similar to the group 7 elements, including manganese and rhenium. Thorium, protactinium, and uranium, with their dominant oxidation states of +4, +5, and +6 respectively, fooled scientists into thinking they belonged below hafnium, tantalum, and tungsten, rather than below the lanthanide series, which was at the time viewed as a fluke, and whose members all have dominant +3 states; neptunium, on the other hand, has a much rarer, more unstable +7 state, with +4 and +5 being the most stable. Upon finding that plutonium and the other transuranic elements also have dominant +3 and +4 states, along with the discovery of the f-block, the actinide series was firmly established.

While the question of whether Fermi's experiment had produced element 93 was stalemated, two additional claims of the discovery of the element appeared, although unlike Fermi, they both claimed to have observed it in nature. The first of these claims was by Czech engineer Odolen Koblic in 1934 when he extracted a small amount of material from the wash water of heated pitchblende. He proposed the name bohemium for the element, but after being analyzed it turned out that the sample was a mixture of tungsten and vanadium. The other claim, in 1938 by Romanian physicist Horia Hulubei and French chemist Yvette Cauchois, claimed to have discovered the new element via spectroscopy in minerals. They named their element sequanium, but the claim was discounted because the prevailing theory at the time was that if it existed at all, element 93 would not exist naturally. However, as neptunium does in fact occur in nature in trace amounts, as demonstrated when it was found in uranium ore in 1952, it is possible that Hulubei and Cauchois did in fact observe neptunium.

Although by 1938 some scientists, including Niels Bohr, were still reluctant to accept that Fermi had actually produced a new element, he was nevertheless awarded the Nobel Prize in Physics in November 1938 ""for his demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons"". A month later, the almost totally unexpected discovery of nuclear fission by Hahn, Meitner, and Otto Frisch put an end to the possibility that Fermi had discovered element 93 because most of the unknown half-lives that had been observed by Fermi's team were rapidly identified as fission products.

Perhaps the closest of all attempts to produce the missing element 93 was that conducted by the Japanese physicist Yoshio Nishina working with chemist Kenjiro Kimura in 1940, just before the outbreak of the Pacific War in 1941: they bombarded U with fast neutrons. However, while slow neutrons tend to induce neutron capture through a (n, γ) reaction, fast neutrons tend to induce a "knock-out" (n, 2n) reaction, where one neutron is added and two more are removed, resulting in the net loss of a neutron. Nishina and Kimura, having tested this technique on Th and successfully produced the known Th and its long-lived beta decay daughter Pa (both occurring in the natural decay chain of U), therefore correctly assigned the new 6.75-day half-life activity they observed to the new isotope U. They confirmed that this isotope was also a beta emitter and must hence decay to the unknown nuclide 93. They attempted to isolate this nuclide by carrying it with its supposed lighter congener rhenium, but no beta or alpha decay was observed from the rhenium-containing fraction: Nishina and Kimura thus correctly speculated that the half-life of 93, like that of Pa, was very long and hence its activity would be so weak as to be unmeasurable by their equipment, thus concluding the last and closest unsuccessful search for transuranic elements.

As research on nuclear fission progressed in early 1939, Edwin McMillan at the Berkeley Radiation Laboratory of the University of California, Berkeley decided to run an experiment bombarding uranium using the powerful 60-inch (1.52 m) cyclotron that had recently been built at the university. The purpose was to separate the various fission products produced by the bombardment by exploiting the enormous force that the fragments gain from their mutual electrical repulsion after fissioning. Although he did not discover anything of note from this, McMillan did observe two new beta decay half-lives in the uranium trioxide target itself, which meant that whatever was producing the radioactivity had not violently repelled each other like normal fission products. He quickly realized that one of the half-lives closely matched the known 23-minute decay period of uranium-239, but the other half-life of 2.3 days was unknown. McMillan took the results of his experiment to chemist and fellow Berkeley professor Emilio Segrè to attempt to isolate the source of the radioactivity. Both scientists began their work using the prevailing theory that element 93 would have similar chemistry to rhenium, but Segrè rapidly determined that McMillan's sample was not at all similar to rhenium. Instead, when he reacted it with hydrogen fluoride (HF) with a strong oxidizing agent present, it behaved much like members of the rare earths. Since these elements comprise a large percentage of fission products, Segrè and McMillan decided that the half-life must have been simply another fission product, titling the paper "An Unsuccessful Search for Transuranium Elements".
However, as more information about fission became available, the possibility that the fragments of nuclear fission could still have been present in the target became more remote. McMillan and several scientists, including Philip H. Abelson, attempted again to determine what was producing the unknown half-life. In early 1940, McMillan realized that his 1939 experiment with Segrè had failed to test the chemical reactions of the radioactive source with sufficient rigor. In a new experiment, McMillan tried subjecting the unknown substance to HF in the presence of a reducing agent, something he had not done before. This reaction resulted in the sample precipitating with the HF, an action that definitively ruled out the possibility that the unknown substance was a rare earth. 
Shortly after this, Abelson, who had received his graduate degree from the university, visited Berkeley for a short vacation and McMillan asked the more able chemist to assist with the separation of the experiment's results. Abelson very quickly observed that whatever was producing the 2.3-day half-life did not have chemistry like any known element and was actually more similar to uranium than a rare earth. This discovery finally allowed the source to be isolated and later, in 1945, led to the classification of the actinide series. As a final step, McMillan and Abelson prepared a much larger sample of bombarded uranium that had a prominent 23-minute half-life from U and demonstrated conclusively that the unknown 2.3-day half-life increased in strength in concert with a decrease in the 23-minute activity through the following reaction:

This proved that the unknown radioactive source originated from the decay of uranium and, coupled with the previous observation that the source was different chemically from all known elements, proved beyond all doubt that a new element had been discovered. McMillan and Abelson published their results in a paper entitled "Radioactive Element 93" in the "Physical Review" on May 27, 1940. They did not propose a name for the element in the paper, but they soon decided on the name "neptunium" since Neptune is the next planet beyond Uranus in our solar system. McMillan and Abelson's success compared to Nishina and Kimura's near miss can be attributed to the favorable half-life of Np for radiochemical analysis and quick decay of U, in contrast to the slower decay of U and extremely long half-life of Np.

It was also realized that the beta decay of Np must produce an isotope of element 94 (now called plutonium), but the quantities involved in McMillan and Abelson's original experiment were too small to isolate and identify plutonium along with neptunium. The discovery of plutonium had to wait until the end of 1940, when Glenn T. Seaborg and his team identified the isotope plutonium-238.

Neptunium's unique radioactive characteristics allowed it to be traced as it moved through various compounds in chemical reactions, at first this was the only method available to prove that its chemistry was different from other elements. As the first isotope of neptunium to be discovered has such a short half-life, McMillan and Abelson were unable to prepare a sample that was large enough to perform chemical analysis of the new element using the technology that was then available. However, after the discovery of the long-lived Np isotope in 1942 by Glenn Seaborg and Arthur Wahl, forming weighable amounts of neptunium became a realistic endeavor. Its half-life was initially determined to be about 3 million years (later revised to 2.144 million years), confirming the predictions of Nishina and Kimura of a very long half-life.

Early research into the element was somewhat limited because most of the nuclear physicists and chemists in the United States at the time were focused on the massive effort to research the properties of plutonium as part of the Manhattan Project. Research into the element did continue as a minor part of the project and the first bulk sample of neptunium was isolated in 1944.

Much of the research into the properties of neptunium since then has been focused on understanding how to confine it as a portion of nuclear waste. Because it has isotopes with very long half-lives, it is of particular concern in the context of designing confinement facilities that can last for thousands of years. It has found some limited uses as a radioactive tracer and a precursor for various nuclear reactions to produce useful plutonium isotopes. However, most of the neptunium that is produced as a reaction byproduct in nuclear power stations is considered to be a waste product.

The vast majority of the neptunium that currently exists on Earth was produced in artificial nuclear reactions. Neptunium-237 is the most commonly synthesized isotope due to it being the only one that both can be created via neutron capture and also has a half-lifelong enough to allow weighable quantities to be easily isolated. As such, it is by far the most common isotope to be utilized in chemical studies of the element.


Heavier isotopes of neptunium decay quickly, and lighter isotopes of neptunium cannot be produced by neutron capture, so chemical separation of neptunium from cooled spent nuclear fuel gives nearly pure Np. The short-lived heavier isotopes Np and Np, useful as radioactive tracers, are produced through neutron irradiation of Np and U respectively, while the longer-lived lighter isotopes Np and Np are produced through irradiation of U with protons and deuterons in a cyclotron.

Artificial Np metal is usually isolated through a reaction of NpF with liquid barium or lithium at around 1200 °C and is most often extracted from spent nuclear fuel rods in kilogram amounts as a by-product in plutonium production.

By weight, neptunium-237 discharges are about 5% as great as plutonium discharges and about 0.05% of spent nuclear fuel discharges. However, even this fraction still amounts to more than fifty tons per year globally.

Recovering uranium and plutonium from spent nuclear fuel for reuse is one of the major processes of the nuclear fuel cycle. As it has a long half-life of just over 2 million years, the alpha emitter Np is one of the major isotopes of the minor actinides separated from spent nuclear fuel. Many separation methods have been used to separate out the neptunium, operating on small and large scales. The small-scale purification operations have the goals of preparing pure neptunium as a precursor of metallic neptunium and its compounds, and also to isolate and preconcentrate neptunium in samples for analysis.

Most methods that separate neptunium ions exploit the differing chemical behaviour of the differing oxidation states of neptunium (from +3 to +6 or sometimes even +7) in solution. Among the methods that are or have been used are: solvent extraction (using various extractants, usually multidentate β-diketone derivatives, organophosphorus compounds, and amine compounds), chromatography using various ion-exchange or chelating resins, coprecipitation (possible matrices include LaF, BiPO, BaSO, Fe(OH), and MnO), electrodeposition, and biotechnological methods. Currently, commercial reprocessing plants use the Purex process, involving the solvent extraction of uranium and plutonium with tributyl phosphate.

When it is in an aqueous solution, neptunium can exist in any of its five possible oxidation states (+3 to +7) and each of these show a characteristic color. The stability of each oxidation state is strongly dependent on various factors, such as the presence of oxidizing or reducing agents, pH of the solution, presence of coordination complex-forming ligands, and even the concentration of neptunium in the solution.

In acidic solutions, the neptunium(III) to neptunium(VII) ions exist as Np, Np, , , and . In basic solutions, they exist as the oxides and hydroxides Np(OH), NpO, NpOOH, NpO(OH), and . Not as much work has been done to characterize neptunium in basic solutions. Np and Np can easily be reduced and oxidized to each other, as can and .

Np(III) or Np exists as hydrated complexes in acidic solutions, . It is a dark blue-purple and is analogous to its lighter congener, the pink rare-earth ion Pm. In the presence of oxygen, it is quickly oxidized to Np(IV) unless strong reducing agents are also present. Nevertheless, it is the second-least easily hydrolyzed neptunium ion in water, forming the NpOH ion. Np is the predominant neptunium ion in solutions of pH 4–5.

Np(IV) or Np is pale yellow-green in acidic solutions, where it exists as hydrated complexes (). It is quite unstable to hydrolysis in acidic aqueous solutions at pH 1 and above, forming NpOH. In basic solutions, Np tends to hydrolyze to form the neutral neptunium(IV) hydroxide (Np(OH)) and neptunium(IV) oxide (NpO).

Np(V) or is green-blue in aqueous solution, in which it behaves as a strong Lewis acid. It is a stable ion and is the most common form of neptunium in aqueous solutions. Unlike its neighboring homologues and , does not spontaneously disproportionate except at very low pH and high concentration:

It hydrolyzes in basic solutions to form NpOOH and .

Np(VI) or , the neptunyl ion, shows a light pink or reddish color in an acidic solution and yellow-green otherwise. It is a strong Lewis acid and is the main neptunium ion encountered in solutions of pH 3–4. Though stable in acidic solutions, it is quite easily reduced to the Np(V) ion, and it is not as stable as the homologous hexavalent ions of its neighbours uranium and plutonium (the uranyl and plutonyl ions). It hydrolyzes in basic solutions to form the oxo and hydroxo ions NpOOH, , and .

Np(VII) is dark green in a strongly basic solution. Though its chemical formula in basic solution is frequently cited as , this is a simplification and the real structure is probably closer to a hydroxo species like . Np(VII) was first prepared in basic solution in 1967. In strongly acidic solution, Np(VII) is found as ; water quickly reduces this to Np(VI). Its hydrolysis products are uncharacterized.

The oxides and hydroxides of neptunium are closely related to its ions. In general, Np hydroxides at various oxidation levels are less stable than the actinides before it on the periodic table such as thorium and uranium and more stable than those after it such as plutonium and americium. This phenomenon is because the stability of an ion increases as the ratio of atomic number to the radius of the ion increases. Thus actinides higher on the periodic table will more readily undergo hydrolysis.

Neptunium(III) hydroxide is quite stable in acidic solutions and in environments that lack oxygen, but it will rapidly oxidize to the IV state in the presence of air. It is not soluble in water. Np(IV) hydroxides exist mainly as the electrically neutral Np(OH) and its mild solubility in water is not affected at all by the pH of the solution. This suggests that the other Np(IV) hydroxide, , does not have a significant presence.

Because the Np(V) ion is very stable, it can only form a hydroxide in high acidity levels. When placed in a 0.1 M sodium perchlorate solution, it does not react significantly for a period of months, although a higher molar concentration of 3.0 M will result in it reacting to the solid hydroxide NpOOH almost immediately. Np(VI) hydroxide is more reactive but it is still fairly stable in acidic solutions. It will form the compound NpO· HO in the presence of ozone under various carbon dioxide pressures. Np(VII) has not been well-studied and no neutral hydroxides have been reported. It probably exists mostly as .

Three anhydrous neptunium oxides have been reported, NpO, NpO, and NpO, though some studies have stated that only the first two of these exist, suggesting that claims of NpO are actually the result of mistaken analysis of NpO. However, as the full extent of the reactions that occur between neptunium and oxygen has yet to be researched, it is not certain which of these claims is accurate. Although neptunium oxides have not been produced with neptunium in oxidation states as high as those possible with the adjacent actinide uranium, neptunium oxides are more stable at lower oxidation states. This behavior is illustrated by the fact that NpO can be produced by simply burning neptunium salts of oxyacids in air.

The greenish-brown NpO is very stable over a large range of pressures and temperatures and does not undergo phase transitions at low temperatures. It does show a phase transition from face-centered cubic to orthorhombic at around 33-37GPa, although it returns to is original phase when pressure is released. It remains stable under oxygen pressures up to 2.84 MPa and temperatures up to 400 °C. 
NpO is black-brown in color and monoclinic with a lattice size of 418×658×409 picometres. It is relatively unstable and decomposes to NpO and O at 420-695 °C. Although NpO was initially subject to several studies that claimed to produce it with mutually contradictory methods, it was eventually prepared successfully by heating neptunium peroxide to 300-350 °C for 2–3 hours or by heating it under a layer of water in an ampoule at 180 °C.

Neptunium also forms a large number of oxide compounds with a wide variety of elements, although the neptunate oxides formed with alkali metals and alkaline earth metals have been by far the most studied. Ternary neptunium oxides are generally formed by reacting NpO with the oxide of another element or by precipitating from an alkaline solution. LiNpO has been prepared by reacting LiO and NpO at 400 °C for 16 hours or by reacting LiO with NpO · HO at 400 °C for 16 hours in a quartz tube and flowing oxygen. Alkali neptunate compounds KNpO, CsNpO, and RbNpO are all created by a similar reaction: 

The oxide compounds KNpO, CsNpO, and RbNpO are formed by reacting Np(VII) () with a compound of the alkali metal nitrate and ozone. Additional compounds have been produced by reacting NpO and water with solid alkali and alkaline peroxides at temperatures of 400 - 600 °C for 15–30 hours. Some of these include Ba(NpO), BaNaNpO, and BaLiNpO. Also, a considerable number of hexavelant neptunium oxides are formed by reacting solid-state NpO with various alkali or alkaline earth oxides in an environment of flowing oxygen. Many of the resulting compounds also have an equivalent compound that substitutes uranium for neptunium. Some compounds that have been characterized include NaNpO, NaNpO, NaNpO, and NaNpO. These can be obtained by heating different combinations of NpO and NaO to various temperature thresholds and further heating will also cause these compounds to exhibit different neptunium allotropes. The lithium neptunate oxides LiNpO and LiNpO can be obtained with similar reactions of NpO and LiO.

A large number of additional alkali and alkaline neptunium oxide compounds such as CsNpO and CsNpO have been characterized with various production methods. Neptunium has also been observed to form ternary oxides with many additional elements in groups 3 through 7, although these compounds are much less well studied.

Although neptunium halide compounds have not been nearly as well studied as its oxides, a fairly large number have been successfully characterized. Of these, neptunium fluorides have been the most extensively researched, largely because of their potential use in separating the element from nuclear waste products. Four binary neptunium fluoride compounds, NpF, NpF, NpF, and NpF, have been reported. The first two are fairly stable and were first prepared in 1947 through the following reactions:
Later, NpF was obtained directly by heating NpO to various temperatures in mixtures of either hydrogen fluoride or pure fluorine gas. NpF is much more difficult to create and most known preparation methods involve reacting NpF or NpF compounds with various other fluoride compounds. NpF will decompose into NpF and NpF when heated to around 320 °C.

NpF or neptunium hexafluoride is extremely volatile, as are its adjacent actinide compounds uranium hexafluoride (UF) and plutonium hexafluoride (PuF). This volatility has attracted a large amount of interest to the compound in an attempt to devise a simple method for extracting neptunium from spent nuclear power station fuel rods. NpF was first prepared in 1943 by reacting NpF and gaseous fluorine at very high temperatures and the first bulk quantities were obtained in 1958 by heating NpF and dripping pure fluorine on it in a specially prepared apparatus. Additional methods that have successfully produced neptunium hexafluoride include reacting BrF and BrF with NpF and by reacting several different neptunium oxide and fluoride compounds with anhydrous hydrogen fluorides.

Four neptunium oxyfluoride compounds, NpOF, NpOF, NpOF, and NpOF, have been reported, although none of them have been extensively studied. NpOF is a pinkish solid and can be prepared by reacting NpO · HO and NpF with pure fluorine at around 330 °C. NpOF and NpOF can be produced by reacting neptunium oxides with anhydrous hydrogen fluoride at various temperatures. Neptunium also forms a wide variety of fluoride compounds with various elements. Some of these that have been characterized include CsNpF, RbNpF, NaNpF, and KNpOF.

Two neptunium chlorides, NpCl and NpCl, have been characterized. Although several attempts to create NpCl have been made, they have not been successful. NpCl is created by reducing neptunium dioxide with hydrogen and carbon tetrachloride (CCl) and NpCl by reacting a neptunium oxide with CCl at around 500 °C. Other neptunium chloride compounds have also been reported, including NpOCl, CsNpCl, CsNpOCl, and CsNaNpCl. Neptunium bromides NpBr and NpBr have also been created; the latter by reacting aluminium bromide with NpO at 350 °C and the former in an almost identical procedure but with zinc present. The neptunium iodide NpI has also been prepared by the same method as NpBr.

Neptunium chalcogen and pnictogen compounds have been well studied primarily as part of research into their electronic and magnetic properties and their interactions in the natural environment. Pnictide and carbide compounds have also attracted interest because of their presence in the fuel of several advanced nuclear reactor designs, although the latter group has not had nearly as much research as the former.

A wide variety of neptunium sulfide compounds have been characterized, including the pure sulfide compounds NpS, NpS, NpS, NpS, NpS, and NpS. Of these, NpS, prepared by reacting NpO with hydrogen sulfide and carbon disulfide at around 1000 °C, is the most well-studied and three allotropic forms are known. The α form exists up to around 1230 °C, the β up to 1530 °C, and the γ form, which can also exist as NpS, at higher temperatures. NpS can be created by reacting NpS and neptunium metal at 1600 °C and NpS can be prepared by the decomposition of NpS at 500 °C or by reacting sulfur and neptunium hydride at 650 °C. NpS is made by heating a mixture of NpS and pure sulfur to 500 °C. All of the neptunium sulfides except for the β and γ forms of NpS are isostructural with the equivalent uranium sulfide and several, including NpS, α−NpS, and β−NpS are also isostructural with the equivalent plutonium sulfide. The oxysulfides NpOS, NpOS, and NpOS have also been created, although the latter three have not been well studied. NpOS was first prepared in 1985 by vacuum sealing NpO, NpS, and pure sulfur in a quartz tube and heating it to 900 °C for one week.

Neptunium selenide compounds that have been reported include NpSe, NpSe, NpSe, NpSe, NpSe, and NpSe. All of these have only been obtained by heating neptunium hydride and selenium metal to various temperatures in a vacuum for an extended period of time and NpSe is only known to exist in the γ allotrope at relatively high temperatures. Two neptunium oxyselenide compounds are known, NpOSe and NpOSe, are formed with similar methods by replacing the neptunium hydride with neptunium dioxide. The known neptunium telluride compounds NpTe, NpTe, NpTe, NpTe, and NpOTe are formed by similar procedures to the selenides and NpOTe is isostructural to the equivalent uranium and plutonium compounds. No neptunium−polonium compounds have been reported.

Neptunium nitride (NpN) was first prepared in 1953 by reacting neptunium hydride and ammonia gas at around 750 °C in a quartz capillary tube. Later, it was produced by reacting different mixtures of nitrogen and hydrogen with neptunium metal at various temperatures. It has also been created by the reduction of neptunium dioxide with diatomic nitrogen gas at 1550 °C. NpN is isomorphous with uranium mononitride (UN) and plutonium mononitride (PuN) and has a melting point of 2830 °C under a nitrogen pressure of around 1 MPa. Two neptunium phosphide compounds have been reported, NpP and NpP. The first has a face centered cubic structure and is prepared by converting neptunium metal to a powder and then reacting it with phosphine gas at 350 °C. NpP can be created by reacting neptunium metal with red phosphorus at 740 °C in a vacuum and then allowing any extra phosphorus to sublimate away. The compound is non-reactive with water but will react with nitric acid to produce Np(IV) solution.

Three neptunium arsenide compounds have been prepared, NpAs, NpAs, and NpAs. The first two were first created by heating arsenic and neptunium hydride in a vacuum-sealed tube for about a week. Later, NpAs was also made by confining neptunium metal and arsenic in a vacuum tube, separating them with a quartz membrane, and heating them to just below neptunium's melting point of 639 °C, which is slightly higher than the arsenic's sublimation point of 615 °C. NpAs is prepared by a similar procedure using iodine as a transporting agent. NpAs crystals are brownish gold and NpAs is black. The neptunium antimonide compound NpSb was created in 1971 by placing equal quantities of both elements in a vacuum tube, heating them to the melting point of antimony, and then heating it further to 1000 °C for sixteen days. This procedure also created trace amounts of an additional antimonide compound NpSb. One neptunium-bismuth compound, NpBi, has also been reported.

The neptunium carbides NpC, NpC, and NpC (tentative) have been reported, but have not characterized in detail despite the high importance and utility of actinide carbides as advanced nuclear reactor fuel. NpC is a non-stoichiometric compound, and could be better labelled as NpC (0.82 ≤ "x" ≤ 0.96). It may be obtained from the reaction of neptunium hydride with graphite at 1400 °C or by heating the constituent elements together in an electric arc furnace using a tungsten electrode. It reacts with excess carbon to form pure NpC. NpC is formed from heating NpO in a graphite crucible at 2660–2800 °C.

Neptunium reacts with hydrogen in a similar manner to its neighbor plutonium, forming the hydrides NpH (face-centered cubic) and NpH (hexagonal). These are isostructural with the corresponding plutonium hydrides, although unlike PuH, the lattice parameters of NpH become greater as the hydrogen content ("x") increases. The hydrides require extreme care in handling as they decompose in a vacuum at 300 °C to form finely divided neptunium metal, which is pyrophoric.

Being chemically stable, neptunium phosphates have been investigated for potential use in immobilizing nuclear waste. Neptunium pyrophosphate (α-NpPO), a green solid, has been produced in the reaction between neptunium dioxide and boron phosphate at 1100 °C, though neptunium(IV) phosphate has so far remained elusive. The series of compounds NpM(PO), where M is an alkali metal (Li, Na, K, Rb, or Cs), are all known. Some neptunium sulfates have been characterized, both aqueous and solid and at various oxidation states of neptunium (IV through VI have been observed). Additionally, neptunium carbonates have been investigated to achieve a better understanding of the behavior of neptunium in geological repositories and the environment, where it may come into contact with carbonate and bicarbonate aqueous solutions and form soluble complexes.

A few organoneptunium compounds are known and chemically characterized, although not as many as for uranium due to neptunium's scarcity and radioactivity. The most well known organoneptunium compounds are the cyclopentadienyl and cyclooctatetraenyl compounds and their derivatives. The trivalent cyclopentadienyl compound Np(CH)·THF was obtained in 1972 from reacting Np(CH)Cl with sodium, although the simpler Np(CH) could not be obtained. Tetravalent neptunium cyclopentadienyl, a reddish-brown complex, was synthesized in 1968 by reacting neptunium(IV) chloride with potassium cyclopentadienide:

It is soluble in benzene and THF, and is less sensitive to oxygen and water than Pu(CH) and Am(CH). Other Np(IV) cyclopentadienyl compounds are known for many ligands: they have the general formula (CH)NpL, where L represents a ligand.
Neptunocene, Np(CH), was synthesized in 1970 by reacting neptunium(IV) chloride with K(CH). It is isomorphous to uranocene and plutonocene, and they behave chemically identically: all three compounds are insensitive to water or dilute bases but are sensitive to air, reacting quickly to form oxides, and are only slightly soluble in benzene and toluene. Other known neptunium cyclooctatetraenyl derivatives include Np(RCH) (R = ethanol, butanol) and KNp(CH)·2THF, which is isostructural to the corresponding plutonium compound. In addition, neptunium hydrocarbyls have been prepared, and solvated triiodide complexes of neptunium are a precursor to many organoneptunium and inorganic neptunium compounds.

There is much interest in the coordination chemistry of neptunium, because its five oxidation states all exhibit their own distinctive chemical behavior, and the coordination chemistry of the actinides is heavily influenced by the actinide contraction (the greater-than-expected decrease in ionic radii across the actinide series, analogous to the lanthanide contraction).

Few neptunium(III) coordination compounds are known, because Np(III) is readily oxidized by atmospheric oxygen while in aqueous solution. However, sodium formaldehyde sulfoxylate can reduce Np(IV) to Np(III), stabilizing the lower oxidation state and forming various sparingly soluble Np(III) coordination complexes, such as ·11HO, ·HO, and .

Many neptunium(IV) coordination compounds have been reported, the first one being , which is isostructural with the analogous uranium(IV) coordination compound. Other Np(IV) coordination compounds are known, some involving other metals such as cobalt (·8HO, formed at 400 K) and copper (·6HO, formed at 600 K). Complex nitrate compounds are also known: the experimenters who produced them in 1986 and 1987 produced single crystals by slow evaporation of the Np(IV) solution at ambient temperature in concentrated nitric acid and excess 2,2′-pyrimidine.

The coordination chemistry of neptunium(V) has been extensively researched due to the presence of cation–cation interactions in the solid state, which had been already known for actinyl ions. Some known such compounds include the neptunyl dimer ·8HO and neptunium glycolate, both of which form green crystals.

Neptunium(VI) compounds range from the simple oxalate (which is unstable, usually becoming Np(IV)) to such complicated compounds as the green . Extensive study has been performed on compounds of the form , where M represents a monovalent cation and An is either uranium, neptunium, or plutonium.

Since 1967, when neptunium(VII) was discovered, some coordination compounds with neptunium in the +7 oxidation state have been prepared and studied. The first reported such compound was initially characterized as ·"n"HO in 1968, but was suggested in 1973 to actually have the formula ·2HO based on the fact that Np(VII) occurs as in aqueous solution. This compound forms dark green prismatic crystals with maximum edge length 0.15–0.4 mm.

Most neptunium coordination complexes known in solution involve the element in the +4, +5, and +6 oxidation states: only a few studies have been done on neptunium(III) and (VII) coordination complexes. For the former, NpX and (X = Cl, Br) were obtained in 1966 in concentrated LiCl and LiBr solutions, respectively: for the latter, 1970 experiments discovered that the ion could form sulfate complexes in acidic solutions, such as and ; these were found to have higher stability constants than the neptunyl ion (). A great many complexes for the other neptunium oxidation states are known: the inorganic ligands involved are the halides, iodate, azide, nitride, nitrate, thiocyanate, sulfate, carbonate, chromate, and phosphate. Many organic ligands are known to be able to be used in neptunium coordination complexes: they include acetate, propionate, glycolate, lactate, oxalate, malonate, phthalate, mellitate, and citrate.

Analogously to its neighbours, uranium and plutonium, the order of the neptunium ions in terms of complex formation ability is Np > ≥ Np > . (The relative order of the middle two neptunium ions depends on the ligands and solvents used.) The stability sequence for Np(IV), Np(V), and Np(VI) complexes with monovalent inorganic ligands is F > > SCN > > Cl > ; the order for divalent inorganic ligands is > > . These follow the strengths of the corresponding acids. The divalent ligands are more strongly complexing than the monovalent ones. can also form the complex ions [] (M = Al, Ga, Sc, In, Fe, Cr, Rh) in perchloric acid solution: the strength of interaction between the two cations follows the order Fe > In > Sc > Ga > Al. The neptunyl and uranyl ions can also form a complex together.

An important of use of Np is as a precursor in plutonium production, where it is irradiated with neutrons to create Pu, an alpha emitter for radioisotope thermal generators for spacecraft and military applications. Np will capture a neutron to form Np and beta decay with a half-life of just over two days to Pu.

Pu also exists in sizable quantities in spent nuclear fuel but would have to be separated from other isotopes of plutonium. Irradiating neptunium-237 with electron beams, provoking bremsstrahlung, also produces quite pure samples of the isotope plutonium-236, useful as a tracer to determine plutonium concentration in the environment.

Neptunium is fissionable, and could theoretically be used as fuel in a fast neutron reactor or a nuclear weapon, with a critical mass of around 60 kilograms. In 1992, the U.S. Department of Energy declassified the statement that neptunium-237 "can be used for a nuclear explosive device". It is not believed that an actual weapon has ever been constructed using neptunium. As of 2009, the world production of neptunium-237 by commercial power reactors was over 1000 critical masses a year, but to extract the isotope from irradiated fuel elements would be a major industrial undertaking.

In September 2002, researchers at the Los Alamos National Laboratory briefly created the first known nuclear critical mass using neptunium in combination with shells of enriched uranium (uranium-235), discovering that the critical mass of a bare sphere of neptunium-237 "ranges from kilogram weights in the high fifties to low sixties," showing that it "is about as good a bomb material as [uranium-235]." The United States Federal government made plans in March 2004 to move America's supply of separated neptunium to a nuclear-waste disposal site in Nevada.

Np is used in devices for detecting high-energy (MeV) neutrons.
Neptunium accumulates in commercial household ionization-chamber smoke detectors from decay of the (typically) 0.2 microgram of americium-241 initially present as a source of ionizing radiation. With a half-life of 432 years, the americium-241 in an ionization smoke detector includes about 3% neptunium after 20 years, and about 15% after 100 years.

Neptunium-237 is the most mobile actinide in the deep geological repository environment. This makes it and its predecessors such as americium-241 candidates of interest for destruction by nuclear transmutation. Due to its long half-life, neptunium will become the major contributor of the total radiotoxicity in 10,000 years. As it is unclear what happens to the containment in that long time span, an extraction of the neptunium would minimize the contamination of the environment if the nuclear waste could be mobilized after several thousand years.
Neptunium does not have a biological role, as it has a short half-life and occurs only in small traces naturally. Animal tests showed that it is not absorbed via the digestive tract. When injected it concentrates in the bones, from which it is slowly released.

Finely divided neptunium metal presents a fire hazard because neptunium is pyrophoric; small grains will ignite spontaneously in air at room temperature.





</doc>
<doc id="21278" url="https://en.wikipedia.org/wiki?curid=21278" title="Nobelium">
Nobelium

Nobelium is a synthetic chemical element with symbol No and atomic number 102. It is named in honor of Alfred Nobel, the inventor of dynamite and benefactor of science. A radioactive metal, it is the tenth transuranic element and is the penultimate member of the actinide series. Like all elements with atomic number over 100, nobelium can only be produced in particle accelerators by bombarding lighter elements with charged particles. A total of twelve nobelium isotopes are known to exist; the most stable is No with a half-life of 58 minutes, but the shorter-lived No (half-life 3.1 minutes) is most commonly used in chemistry because it can be produced on a larger scale.

Chemistry experiments have confirmed that nobelium behaves as a heavier homolog to ytterbium in the periodic table. The chemical properties of nobelium are not completely known: they are mostly only known in aqueous solution. Before nobelium's discovery, it was predicted that it would show a stable +2 oxidation state as well as the +3 state characteristic of the other actinides: these predictions were later confirmed, as the +2 state is much more stable than the +3 state in aqueous solution and it is difficult to keep nobelium in the +3 state.

In the 1950s and 1960s, many claims of the discovery of nobelium were made from laboratories in Sweden, the Soviet Union, and the United States. Although the Swedish scientists soon retracted their claims, the priority of the discovery and therefore the naming of the element was disputed between Soviet and American scientists, and it was not until 1997 that International Union of Pure and Applied Chemistry (IUPAC) credited the Soviet team with the discovery, but retained nobelium, the Swedish proposal, as the name of the element due to its long-standing use in the literature.

The discovery of element 102 was a complicated process and was claimed by groups from Sweden, the United States, and the Soviet Union. The first complete and incontrovertible report of its detection only came in 1966 from the Joint Institute of Nuclear Research at Dubna (then in the Soviet Union).

The first announcement of the discovery of element 102 was announced by physicists at the Nobel Institute in Sweden in 1957. The team reported that they had bombarded a curium target with carbon-13 ions for twenty-five hours in half-hour intervals. Between bombardments, ion-exchange chemistry was performed on the target. Twelve out of the fifty bombardments contained samples emitting (8.5 ± 0.1) MeV alpha particles, which were in drops which eluted earlier than fermium (atomic number "Z" = 100) and californium ("Z" = 98). The half-life reported was 10 minutes and was assigned to either 102 or 102, although the possibility that the alpha particles observed were from a presumably short-lived mendelevium ("Z" = 101) isotope created from the electron capture of element 102 was not excluded. The team proposed the name "nobelium" (No) for the new element, which was immediately approved by IUPAC, a decision which the Dubna group characterized in 1968 as being hasty. The following year, scientists at the Lawrence Berkeley National Laboratory repeated the experiment but were unable to find any 8.5 MeV events which were not background effects.

In 1959, the Swedish team attempted to explain the Berkeley team's inability to detect element 102 in 1958, maintaining that they did discover it. However, later work has shown that no nobelium isotopes lighter than No (no heavier isotopes could have been produced in the Swedish experiments) with a half-life over 3 minutes exist, and that the Swedish team's results are most likely from thorium-225, which has a half-life of 8 minutes and quickly undergoes triple alpha decay to polonium-213, which has a decay energy of 8.53612 MeV. This hypothesis is lent weight by the fact that thorium-225 can easily be produced in the reaction used and would not be separated out by the chemical methods used. Later work on nobelium also showed that the divalent state is more stable than the trivalent one and hence that the samples emitting the alpha particles could not have contained nobelium, as the divalent nobelium would not have eluted with the other trivalent actinides. Thus, the Swedish team later retracted their claim and associated the activity to background effects.

The Berkeley team, consisting of Albert Ghiorso, Glenn T. Seaborg, John R. Walton and Torbjørn Sikkeland, then claimed the synthesis of element 102 in 1958. The team used the new heavy-ion linear accelerator (HILAC) to bombard a curium target (95% Cm and 5% Cm) with C and C ions. They were unable to confirm the 8.5 MeV activity claimed by the Swedes but were instead able to detect decays from fermium-250, supposedly the daughter of 102 (produced from the curium-246), which had an apparent half-life of ~3 s. Later 1963 Dubna work confirmed that 102 could be produced in this reaction, but that its half-life was actually . In 1967, the Berkeley team attempted to defend their work, stating that the isotope found was indeed Fm but the isotope that the half-life measurements actually related to was californium-244, granddaughter of 102, produced from the more abundant curium-244. Energy differences were then attributed to "resolution and drift problems", although these had not been previously reported and should also have influenced other results. 1977 experiments showed that 102 indeed had a 2.3-second half-life. However, 1973 work also showed that the Fm recoil could have also easily been produced from the isomeric transition of Fm (half-life 1.8 s) which could also have been formed in the reaction at the energy used. Given this, it is probable that no nobelium was actually produced in this experiment.

In 1959 the team continued their studies and claimed that they were able to produce an isotope that decayed predominantly by emission of an 8.3 MeV alpha particle, with a half-life of 3 s with an associated 30% spontaneous fission branch. The activity was initially assigned to 102 but later changed to 102. However, they also noted that it was not certain that nobelium had been produced due to difficult conditions. The Berkeley team decided to adopt the proposed name of the Swedish team, "nobelium", for the element.

Meanwhile, in Dubna, experiments were carried out in 1958 and 1960 aiming to synthesize element 102 as well. The first 1958 experiment bombarded plutonium-239 and -241 with oxygen-16 ions. Some alpha decays with energies just over 8.5 MeV were observed, and they were assigned to 102, although the team wrote that formation of isotopes from lead or bismuth impurities (which would not produce nobelium) could not be ruled out. While later 1958 experiments noted that new isotopes could be produced from mercury, thallium, lead, or bismuth impurities, the scientists still stood by their conclusion that element 102 could be produced from this reaction, mentioning a half-life of under 30 seconds and a decay energy of (8.8 ± 0.5) MeV. Later 1960 experiments proved that these were background effects. 1967 experiments also lowered the decay energy to (8.6 ± 0.4) MeV, but both values are too high to possibly match those of No or No. The Dubna team later stated in 1970 and again in 1987 that these results were not conclusive.

In 1961, Berkeley scientists claimed the discovery of element 103 in the reaction of californium with boron and carbon ions. They claimed the production of the isotope 103, and also claimed to have synthesized an alpha decaying isotope of element 102 that had a half-life of 15 s and alpha decay energy 8.2 MeV. They assigned this to 102 without giving a reason for the assignment. The values do not agree with those now known for No, although they do agree with those now known for No, and while this isotope probably played a part in this experiment its discovery was inconclusive.

Work on element 102 also continued in Dubna, and in 1964, experiments were carried out there to detect alpha-decay daughters of element 102 isotopes by synthesizing element 102 from the reaction of a uranium-238 target with neon ions. The products were carried along a silver catcher foil and purified chemically, and the isotopes Fm and Fm were detected. The yield of Fm was interpreted as evidence that its parent 102 was also synthesized: as it was noted that Fm could also be produced directly in this reaction by the simultaneous emission of an alpha particle with the excess neutrons, steps were taken to ensure that Fm could not go directly to the catcher foil. The half-life detected for 102 was 8 s, which is much higher than the more modern 1967 value of (3.2 ± 0.2) s. Further experiments were conducted in 1966 for 102, using the reactions Am(N,4n)102 and U(Ne,6n)102, finding a half-life of (50 ± 10) s: at that time the discrepancy between this value and the earlier Berkeley value was not understood, although later work proved that the formation of the isomer Fm was less likely in the Dubna experiments than at the Berkeley ones. In hindsight, the Dubna results on 102 were probably correct and can be now considered a conclusive detection of element 102.

One more very convincing experiment from Dubna was published in 1966, again using the same two reactions, which concluded that 102 indeed had a half-life much longer than the 3 seconds claimed by Berkeley. Later work in 1967 at Berkeley and 1971 at the Oak Ridge National Laboratory fully confirmed the discovery of element 102 and clarified earlier observations. In December 1966, the Berkeley group repeated the Dubna experiments and fully confirmed them, and used this data to finally assign correctly the isotopes they had previously synthesized but could not yet identify at the time, and thus claimed to have discovered nobelium in 1958 to 1961.

In 1969, the Dubna team carried out chemical experiments on element 102 and concluded that it behaved as the heavier homologue of ytterbium. The Russian scientists proposed the name "joliotium" (Jo) for the new element after Irène Joliot-Curie, who had recently died, creating an element naming controversy that would not be resolved for several decades, which each group using its own proposed names.

In 1992, the IUPAC-IUPAP Transfermium Working Group (TWG) reassessed the claims of discovery and concluded that only the Dubna work from 1966 correctly detected and assigned decays to nuclei with atomic number 102 at the time. The Dubna team are therefore officially recognised as the discoverers of nobelium although it is possible that it was detected at Berkeley in 1959. This decision was criticized by Berkeley the following year, calling the reopening of the cases of elements 101 to 103 a "futile waste of time", while Dubna agreed with the IUPAC's decision.

In 1994, as part of an attempted resolution to the element naming controversy, the IUPAC ratified names for elements 101–109. For element 102, it ratified the name "nobelium" (No) on the basis that it had become entrenched in the literature over the course of 30 years and that Alfred Nobel should be commemorated in this fashion. Because of outcry over the 1994 names, which mostly did not respect the choices of the discoverers, a comment period ensued, and in 1995 IUPAC named element 102 "flerovium" (Fl) as part of a new proposal, after either Georgy Flyorov or his eponymous Flerov Laboratory of Nuclear Reactions. This proposal was also not accepted, and in 1997 the name "nobelium" was restored. Today, the name "flerovium", with the same symbol, now refers to element 114.

In the periodic table, nobelium is located to the right of the actinide mendelevium, to the left of the actinide lawrencium, and below the lanthanide ytterbium. Nobelium metal has not yet been prepared in bulk quantities, and bulk preparation is currently impossible. Nevertheless, a number of predictions and some preliminary experimental results have been done regarding its properties.

The lanthanides and actinides, in the metallic state, can exist as either divalent (such as europium and ytterbium) or trivalent (most other lanthanides) metals. The former have fs configurations, whereas the latter have fds configurations. In 1975, Johansson and Rosengren examined the measured and predicted values for the cohesive energies (enthalpies of crystallization) of the metallic lanthanides and actinides, both as divalent and trivalent metals. The conclusion was that the increased binding energy of the [Rn]5f6d7s configuration over the [Rn]5f7s configuration for nobelium was not enough to compensate for the energy needed to promote one 5f electron to 6d, as is true also for the very late actinides: thus einsteinium, fermium, mendelevium, and nobelium were expected to be divalent metals, although for nobelium this prediction has not yet been confirmed. The increasing predominance of the divalent state well before the actinide series concludes is attributed to the relativistic stabilization of the 5f electrons, which increases with increasing atomic number: an effect of this is that nobelium is predominantly divalent instead of trivalent, unlike all the other lanthanides and actinides. In 1986, nobelium metal was estimated to have an enthalpy of sublimation between 126 kJ/mol, a value close to the values for einsteinium, fermium, and mendelevium and supporting the theory that nobelium would form a divalent metal. Like the other divalent late actinides (except the once again trivalent lawrencium), metallic nobelium should assume a face-centered cubic crystal structure. Divalent nobelium metal should have a metallic radius of around 197 pm. Nobelium's melting point has been predicted to be 827 °C, the same value as that estimated for the neighboring element mendelevium. Its density is predicted to be around 9.9 ± 0.4 g/cm.

The chemistry of nobelium is incompletely characterized and is known only in aqueous solution, in which it can take on the +3 or +2 oxidation states, the latter being more stable. It was largely expected before the discovery of nobelium that in solution, it would behave like the other actinides, with the trivalent state being predominant; however, Seaborg predicted in 1949 that the +2 state would also be relatively stable for nobelium, as the No ion would have the ground-state electron configuration [Rn]5f, including the stable filled 5f shell. It took nineteen years before this prediction was confirmed.

In 1967, experiments were conducted to compare nobelium's chemical behavior to that of terbium, californium, and fermium. All four elements were reacted with chlorine and the resulting chlorides were deposited along a tube, along which they were carried by a gas. It was found that the nobelium chloride produced was strongly adsorbed on solid surfaces, proving that it was not very volatile, like the chlorides of the other three investigated elements. However, both NoCl and NoCl were expected to exhibit nonvolatile behavior and hence this experiment was inconclusive as to what the preferred oxidation state of nobelium was. Determination of nobelium's favoring of the +2 state had to wait until the next year, when cation-exchange chromatography and coprecipitation experiments were carried out on around fifty thousand No atoms, finding that it behaved differently from the other actinides and more like the divalent alkaline earth metals. This proved that in aqueous solution, nobelium is most stable in the divalent state when strong oxidizers are absent. Later experimentation in 1974 showed that nobelium eluted with the alkaline earth metals, between Ca and Sr. Nobelium is the only f-block element for which the +2 state is the most common and stable one in aqueous solution. This occurs because of the large energy gap between the 5f and 6d orbitals at the end of the actinide series.

The relativistic stability of the 7s subshell greatly destabilizes nobelium dihydride, NoH, and relativistic stabilisation of the 7p spinor over the 6d spinor mean that excited states in nobelium atoms have 7s and 7p contribution instead of the expected 6d contribution. The long No–H distances in the NoH molecule and the significant charge transfer lead to extreme ionicity with a dipole moment of 5.94 D for this molecule. In this molecule, nobelium is expected to exhibit main-group-like behavior, specifically acting like an alkaline earth metal with its "n"s valence shell configuration and core-like 5f orbitals.

Nobelium's complexing ability with chloride ions is most similar to that of barium, which complexes rather weakly. Its complexing ability with citrate, oxalate, and acetate in an aqueous solution of 0.5 M ammonium nitrate is between that of calcium and strontium, although it is somewhat closer to that of strontium.

The standard reduction potential of the "E"°(No→No) couple was estimated in 1967 to be between +1.4 and +1.5 V; it was later found in 2009 to be only about +0.75 V. The positive value shows that No is more stable than No and that No is a good oxidizing agent. While the quoted values for the "E"°(No→No) and "E"°(No→No) vary among sources, the accepted standard estimates are −2.61 and −1.26 V. It has been predicted that the value for the "E"°(No→No) couple would be +6.5 V. The Gibbs energies of formation for No and No are estimated to be −342 and −480 kJ/mol, respectively.

A nobelium atom has 102 electrons, of which three can act as valence electrons. They are expected to be arranged in the configuration [Rn]5f7s (ground state term symbol S), although experimental verification of this electron configuration had not yet been made as of 2006. In forming compounds, all the three valence electrons may be lost, leaving behind a [Rn]5f core: this conforms to the trend set by the other actinides with their [Rn]5f electron configurations in the tripositive state. Nevertheless, it is more likely that only two valence electrons may be lost, leaving behind a stable [Rn]5f core with a filled 5f shell. The first ionization potential of nobelium was measured to be at most (6.65 ± 0.07) eV in 1974, based on the assumption that the 7s electrons would ionize before the 5f ones; this value has since not yet been refined further due to nobelium's scarcity and high radioactivity. The ionic radius of hexacoordinate and octacoordinate No had been preliminarily estimated in 1978 to be around 90 and 102 pm respectively; the ionic radius of No has been experimentally found to be 100 pm to two significant figures. The enthalpy of hydration of No has been calculated as 1486 kJ/mol.

Twelve isotopes of nobelium are known, with mass numbers 250–260 and 262; all are radioactive. Additionally, nuclear isomers are known for mass numbers 251, 253, and 254. Of these, the longest-lived isotope is No with a half-life of 58 minutes, and the longest-lived isomer is No with a half-life of 1.7 seconds. However, the still undiscovered isotope No is predicted to have a still longer half-life of 170 min. Additionally, the shorter-lived No (half-life 3.1 minutes) is more often used in chemical experimentation because it can be produced in larger quantities from irradiation of californium-249 with carbon-12 ions. After No and No, the next most stable nobelium isotopes are No (half-life 1.62 minutes), No (51 seconds), No (25 seconds), No (2.91 seconds), and No (2.57 seconds). All of the remaining nobelium isotopes have half-lives that are less than a second, and the shortest-lived known nobelium isotope (No) has a half-life of only 0.25 milliseconds. The isotope No is especially interesting theoretically as it is in the middle of a series of prolate nuclei from Pa to Rg, and the formation of its nuclear isomers (of which two are known) is controlled by proton orbitals such as 2f which come just above the spherical proton shell; it can be synthesized in the reaction of Pb with Ca.

The half-lives of nobelium isotopes increase smoothly from No to No. However, a dip appears at No, and beyond this the half-lives of even-even nobelium isotopes drop sharply as spontaneous fission becomes the dominant decay mode. For example, the half-life of No is almost three seconds, but that of No is only 1.2 milliseconds. This shows that at nobelium, the mutual repulsion of protons poses a limit to the region of long-lived nuclei in the actinide series. The even-odd nobelium isotopes mostly continue to have longer half-lives as their mass numbers increase, with a dip in the trend at No.

The isotopes of nobelium are mostly produced by bombarding actinide targets (uranium, plutonium, curium, californium, or einsteinium), with the exception of nobelium-262, which is produced as the daughter of lawrencium-262. The most commonly used isotope, No, can be produced from bombarding curium-248 or californium-249 with carbon-12: the latter method is more common. Irradiating a 350 μg cm target of californium-249 with three trillion (3 × 10) 73 MeV carbon-12 ions per second for ten minutes can produce around 1200 nobelium-255 atoms.

Once the nobelium-255 is produced, it can be separated out in a similar way as used to purify the neighboring actinide mendelevium. The recoil momentum of the produced nobelium-255 atoms is used to bring them physically far away from the target from which they are produced, bringing them onto a thin foil of metal (usually beryllium, aluminium, platinum, or gold) just behind the target in a vacuum: this is usually combined by trapping the nobelium atoms in a gas atmosphere (frequently helium), and carrying them along with a gas jet from a small opening in the reaction chamber. Using a long capillary tube, and including potassium chloride aerosols in the helium gas, the nobelium atoms can be transported over tens of meters. The thin layer of nobelium collected on the foil can then be removed with dilute acid without completely dissolving the foil. The nobelium can then be isolated by exploiting its tendency to form the divalent state, unlike the other trivalent actinides: under typically used elution conditions (bis-(2-ethylhexyl) phosphoric acid (HDEHP) as stationary organic phase and 0.05 M hydrochloric acid as mobile aqueous phase, or using 3 M hydrochloric acid as an eluant from cation-exchange resin columns), nobelium will pass through the column and elute while the other trivalent actinides remain on the column. However, if a direct "catcher" gold foil is used, the process is complicated by the need to separate out the gold using anion-exchange chromatography before isolating the nobelium by elution from chromatographic extraction columns using HDEHP.



</doc>
<doc id="21281" url="https://en.wikipedia.org/wiki?curid=21281" title="Norwegian Sea">
Norwegian Sea

The Norwegian Sea () is a marginal sea in the Arctic Ocean, northwest of Norway. It is located between the North Sea (i.e. north of the United Kingdom) and the Greenland Sea and adjoins the Barents Sea to the northeast. In the southwest, it is separated from the Atlantic Ocean by a submarine ridge running between Iceland and the Faroe Islands. To the North, the Jan Mayen Ridge separates it from the Greenland Sea.

Unlike many other seas, most of the bottom of the Norwegian Sea is not part of a continental shelf and therefore lies at a great depth of about two kilometres on average. Rich deposits of oil and natural gas are found under the sea bottom and are being explored commercially, in the areas with sea depths of up to about one kilometre. The coastal zones are rich in fish that visit the Norwegian Sea from the North Atlantic or from the Barents Sea (cod) for spawning. The warm North Atlantic Current ensures relatively stable and high water temperatures, so that unlike the Arctic seas, the Norwegian Sea is ice-free throughout the year. Recent research has concluded that the large volume of water in the Norwegian Sea with its large heat absorption capacity is more important as a source of Norway's mild winters than the Gulf Stream and its extensions.

The International Hydrographic Organization defines the limits of the Norwegian Sea as follows:

The Norwegian Sea was formed about 250 million years ago, when the Eurasian plate of Norway and the North American Plate, including Greenland, started to move apart. The existing narrow shelf sea between Norway and Greenland began to widen and deepen. The present continental slope in the Norwegian Sea marks the border between Norway and Greenland as it stood approximately 250 million years ago. In the north it extends east from Svalbard and on the southwest between Britain and the Faroes. This continental slope contains rich fishing grounds and numerous coral reefs. Settling of the shelf after the separation of the continents has resulted in landslides, such as the Storegga Slide about 8,000 years ago that induced a major tsunami.

The coasts of the Norwegian Sea were shaped during the last Ice Age. Large glaciers several kilometres high pushed into the land, forming fjords, removing the crust into the sea, and thereby extending the continental slopes. This is particularly clear off the Norwegian coast along Helgeland and north to the Lofoten Islands. The Norwegian continental shelf is between 40 and 200 kilometres wide, and has a different shape from the shelves in the North Sea and Barents Sea. It contains numerous trenches and irregular peaks, which usually have an amplitude of less than 100 metres, but can reach up to 400 metres. They are covered with a mixture of gravel, sand, and mud, and the trenches are used by fish as spawning grounds. Deeper into the sea, there are two deep basins separated by a low ridge (its deepest point at 3,000 m) between the Vøring Plateau and Jan Mayen island. The southern basin is larger and deeper, with large areas between 3,500 and 4,000 metres deep. The northern basin is shallower at 3,200–3,300 metres, but contains many individual sites going down to 3,500 metres. Submarine thresholds and continental slopes mark the borders of these basins with the adjacent seas. To the south lies the European continental shelf and the North Sea, to the east is the Eurasian continental shelf with the Barents Sea. To the west, the Scotland-Greenland Ridge separates the Norwegian Sea from the North Atlantic. This ridge is on average only 500 metres deep, only in a few places reaching the depth of 850 metres. To the north lie the Jan Mayen Ridge and Mohns Ridge, which lie at a depth of 2,000 metres, with some trenches reaching depths of about 2,600 meters.

Four major water masses originating in the Atlantic and Arctic oceans meet in the Norwegian Sea, and the associated currents are of fundamental importance for the global climate. The warm, salty North Atlantic Current flows in from the Atlantic Ocean, and the colder and less saline Norwegian Current originates in the North Sea. The so-called East Iceland Current transports cold water south from the Norwegian Sea toward Iceland and then east, along the Arctic Circle; this current occurs in the middle water layer. Deep water flows into the Norwegian Sea from the Greenland Sea. The tides in the sea are semi-diurnal; that is, they rise twice a day, to a height of about 3.3 metres.

The hydrology of the upper water layers is largely determined by the flow from the North Atlantic. It reaches a speed of 10 Sv (1 Sv = million m/s) and its maximum depth is 700 metres at the Lofoten Islands, but normally it is within 500 meters. Part of it comes through the Faroe-Shetland Channel and has a comparatively high salinity of 35.3‰ (parts per thousand). This current originates in the North Atlantic Current and passes along the European continental slope; increased evaporation due to the warm European climate results in the elevated salinity. Another part passes through the Greenland-Scotland trench between the Faroe Islands and Iceland; this water has a mean salinity between 35 and 35.2‰. The flow shows strong seasonal variations and can be twice as high in winter as in summer. While at the Faroe-Shetland Channel it has a temperature of about 9.5 °C; it cools to about 5 °C at Svalbard and releases this energy (about 250 terawatts) to the environment.

The current flowing from the North Sea originates in the Baltic Sea and thus collects most of the drainage from northern Europe; this contribution is however relatively small. The temperature and salinity of this current show strong seasonal and annual fluctuations. Long-term measurements within the top 50 metres near the coast show a maximum temperature of 11.2 °C at the 63° N parallel in September and a minimum of 3.9 °C at the North Cape in March. The salinity varies between 34.3 and 34.6‰ and is lowest in spring owing to the inflow of melted snow from rivers. The largest rivers discharging into the sea are Namsen, Ranelva and Vefsna. They are all relatively short, but have a high discharge rate owing to their steep mountainous nature.

A portion of the warm surface water flows directly, within the West Spitsbergen Current, from the Atlantic Ocean, off the Greenland Sea, to the Arctic Ocean. This current has a speed of 3–5 Sv and has a large impact on the climate. Other surface water (~1 Sv) flows along the Norwegian coast in the direction of the Barents Sea. This water may cool enough in the Norwegian Sea to submerge into the deeper layers; there it displaces water that flows back into the North Atlantic.

Arctic water from the East Iceland Current is mostly found in the southwestern part of the sea, near Greenland. Its properties also show significant annual fluctuations, with long-term average temperature being below 3 °C and salinity between 34.7 and 34.9‰. The fraction of this water on the sea surface depends on the strength of the current, which in turn depends on the pressure difference between the Icelandic Low and Azores High: the larger the difference, the stronger the current.

The Norwegian Sea is connected with the Greenland Sea and the Arctic Ocean by the 2,600-metre deep Fram Strait. The Norwegian Sea Deep Water (NSDW) occurs at depths exceeding 2,000 metres; this homogeneous layer with a salinity of 34.91‰ experiences little exchange with the adjacent seas. Its temperature is below 0 °C and drops to −1 °C at the ocean floor. Compared with the deep waters of the surrounding seas, NSDW has more nutrients but less oxygen and is relatively old.

The weak deep-water exchange with the Atlantic Ocean is due to the small depth of the relatively flat Greenland-Scotland Ridge between Scotland and Greenland, an offshoot of the Mid-Atlantic Ridge. Only four areas of the Greenland-Scotland Ridge are deeper than 500 metres: the Faroe Bank Channel (about 850 metres), some parts of the Iceland-Faroe Ridge (about 600 metres), the Wyville-Thomson Ridge (620 metres), and areas between Greenland and the Denmark Strait (850 meters) – this is much shallower than the Norwegian Sea. Cold deep water flows into the Atlantic through various channels: about 1.9 Sv through the Faroe Bank channel, 1.1 Sv through the Iceland-Faroe channel, and 0.1 Sv via the Wyville-Thomson Ridge. The turbulence that occurs when the deep water falls behind the Greenland-Scotland Ridge into the deep Atlantic basin mixes the adjacent water layers and forms the North Atlantic Deep Water, one of two major deep-sea currents providing the deep ocean with oxygen.

The thermohaline circulation affects the climate in the Norwegian Sea, and the regional climate can significantly deviate from average. There is also a difference of about 10 °C between the sea and the coastline. Temperatures rose between 1920 and 1960, and the frequency of storms decreased in this period. The storminess was relatively high between 1880 and 1910, decreased significantly in 1910–1960, and then recovered to the original level.

In contrast to the Greenland Sea and Arctic seas, the Norwegian Sea is ice-free year round, owing to its warm currents. The convection between the relatively warm water and cold air in the winter plays an important role in the Arctic climate. The 10-degree July isotherm (air temperature line) runs through the northern boundary of the Norwegian Sea and is often taken as the southern boundary of the Arctic. In winter, the Norwegian Sea generally has the lowest air pressure in the entire Arctic and where most Icelandic Low depressions form. The water temperature in most parts of the sea is 2–7 °C in February and 8–12 °C in August.

The Norwegian Sea is a transition zone between boreal and Arctic conditions, and thus contains flora and fauna characteristic of both climatic regions. The southern limit of many Arctic species runs through the North Cape, Iceland, and the center of the Norwegian Sea, while the northern limit of boreal species lies near the borders of the Greenland Sea with the Norwegian Sea and Barents Sea; that is, these areas overlap. Some species like the scallop "Chlamys islandica" and capelin tend to occupy this area between the Atlantic and Arctic oceans.

Most of the aquatic life in the Norwegian Sea is concentrated in the upper layers. Estimates for the entire North Atlantic are that only 2% of biomass is produced at depths below 1,000 metres and only 1.2% occurs near the sea floor.

The blooming of the phytoplankton is dominated by chlorophyll and peaks around 20 May. The major phytoplankton forms are diatoms, in particular the genus "Thalassiosira" and "Chaetoceros". After the spring bloom the haptophytes of the genus "Phaecocystis pouchetti" become dominant.

Zooplankton is mostly represented by the copepods "Calanus finmarchicus" and "Calanus hyperboreus", where the former occurs about four times more often than the latter and is mostly found in the Atlantic streams, whereas "C. hyperboreus" dominates the Arctic waters; they are the main diet of most marine predators. The most important krill species are "Meganyctiphanes norvegica", "Thyssanoessa inermis", and "Thyssanoessa longicaudata". In contrast to the Greenland Sea, there is a significant presence of calcareous plankton (Coccolithophore and Globigerinida) in the Norwegian Sea. Plankton production strongly fluctuates between years. For example, "C. finmarchicus" yield was 28 g/m² (dry weight) in 1995 and only 8 g/m² in 1997; this correspondingly affected the population of all its predators.

Shrimp of the species "Pandalus borealis" play an important role in the diet of fish, particularly cod and blue whiting, and mostly occur at depths between 200 and 300 metres. A special feature of the Norwegian Sea is extensive coral reefs of "Lophelia pertusa", which provide shelter to various fish species. Although these corals are widespread in many peripheral areas of the North Atlantic, they never reach such amounts and concentrations as at the Norwegian continental slopes. However, they are at risk due to increasing trawling, which mechanically destroys the coral reefs.

The Norwegian coastal waters are the most important spawning ground of the herring populations of the North Atlantic, and the hatching occurs in March. The eggs float to the surface and are washed off the coast by the northward current. Whereas a small herring population remains in the fjords and along the northern Norwegian coast, the majority spends the summer in the Barents Sea, where it feeds on the rich plankton. Upon reaching puberty, herring returns to the Norwegian Sea. The herring stock varies greatly between years. It increased in the 1920s owing to the milder climate and then collapsed in the following decades until 1970; the decrease was, however, at least partly caused by overfishing. The biomass of young hatched herring declined from 11 million tonnes in 1956 to almost zero in 1970; that affected the ecosystem not only of the Norwegian Sea but also of the Barents Sea.
Enforcement of environmental and fishing regulations has resulted in partial recovery of the herring populations since 1987. This recovery was accompanied by a decline of capelin and cod stocks. While the capelin benefited from the reduced fishing, the temperature rise in the 1980s and competition for food with the herring resulted in a near disappearance of young capelin from the Norwegian Sea. Meanwhile, the elderly capelin population was quickly fished out. This also reduced the population of cod – a major predator of capelin – as the herring was still too small in numbers to replace the capelin in the cod's diet.

Blue whiting ("Micromesistius poutassou") has benefited from the decline of the herring and capelin stocks as it assumed the role of major predator of plankton. The blue whiting spawns near the British Isles. The sea currents carry their eggs to the Norwegian Sea, and the adults also swim there to benefit from the food supply. The young spend the summer and the winter until February in Norwegian coastal waters and then return to the warmer waters west of Scotland. The Norwegian Arctic cod mostly occurs in the Barents Sea and at the Svalbard Archipelago. In the rest of the Norwegian Sea, it is found only during the reproduction season, at the Lofoten Islands, whereas "Pollachius virens" and haddock spawn in the coastal waters. Mackerel is an important commercial fish. The coral reefs are populated by different species of the genus "Sebastes".

Significant numbers of minke, humpback, sei, and orca whales are present in the Norwegian Sea, and white-beaked dolphins occur in the coastal waters. Orcas and some other whales visit the sea in the summer months for feeding; their population is closely related to the herring stocks, and they follow the herring schools within the sea. With a total population of about 110,000, minke whales are by far the most common whales in the sea. They are hunted by Norway and Iceland, with a quota of about 1,000 per year in Norway. In contrast to the past, nowadays primarily their meat is consumed, rather than fat and oil.

The bowhead whale used to be a major plankton predator, but it almost disappeared from the Norwegian Sea after intense whaling in the 19th century, and was temporarily extinct in the entire North Atlantic. Similarly, the blue whale used to form large groups between Jan Mayen and Spitsbergen, but is hardly present nowadays. Observations of northern bottlenose whales in the Norwegian Sea are rare. Other large animals of the sea are hooded and harp seals and squid.

Important waterfowl species of the Norwegian Sea are puffin, kittiwake and guillemot. Puffins and guillemots also suffered from the collapse of the herring population, especially the puffins on the Lofoten Islands. The latter hardly had an alternative to herring and their population was approximately halved between 1969 and 1987.

Norway, Iceland, and Denmark/Faroe Islands share the territorial waters of the Norwegian Sea, with the largest part belonging to the first. Norway has claimed twelve-mile limit as territorial waters since 2004 and an exclusive economic zone of 200 miles since 1976. Consequently, due to the Norwegian islands of Svalbard and Jan Mayen, the southeast, northeast and northwest edge of the sea fall within Norway. The southwest border is shared between Iceland and Denmark/Faroe Islands.

The largest damage to the Norwegian Sea was caused by extensive fishing, whaling, and pollution. The British nuclear complex of Sellafield is one of the greatest polluters, discharging radioactive waste into the sea. Other contamination is mostly by oil and toxic substances, but also from the great number of ships sunk during the two world wars. The environmental protection of the Norwegian Sea is mainly regulated by the OSPAR Convention.

Fishing has been practised near the Lofoten archipelago for hundreds of years. The coastal waters of the remote Lofoten islands are one of the richest fishing areas in Europe, as most of the Atlantic cod swims to the coastal waters of Lofoten in the winter to spawn. So in the 19th century, dried cod was one of Norway's main exports and by far the most important industry in northern Norway. Strong sea currents, maelstroms, and especially frequent storms made fishing a dangerous occupation: several hundred men died on the "Fatal Monday" in March 1821, 300 of them from a single parish, and about a hundred boats with their crews were lost within a short time in April 1875.

Whaling was also important for the Norwegian Sea. In the early 1600s, the Englishman Stephen Bennet started hunting walrus at Bear Island. In May 1607 the Muscovy Company, while looking for the Northwest Passage and exploring the sea, discovered the large populations of walrus and whales in the Norwegian Sea and started hunting them in 1610 near Spitsbergen. Later in the 17th century, Dutch ships started hunting bowhead whales near Jan Mayen; the bowhead population between Svalbard and Jan Mayen was then about 25,000 individuals. Britons and Dutch were then joined by Germans, Danes, and Norwegians. Between 1615 and 1820, the waters between Jan Mayen, Svalbard, Bear Island, and Greenland, between the Norwegian, Greenland, and Barents Seas, were the most productive whaling area in the world. However, extensive hunting had wiped out the whales in that region by the early 20th century.

For many centuries, the Norwegian Sea was regarded as the edge of the known world. The disappearance of ships there, due to the natural disasters, induced legends of monsters that stopped and sank ships (kraken). As late as in 1845, the "Encyclopædia metropolitana" contained a multi-page review by Erik Pontoppidan (1698–1764) on ship-sinking sea monsters half a mile in size. Many legends might be based on the work "Historia de gentibus septentrionalibus" of 1539 by Olaus Magnus, which described the kraken and maelstroms of the Norwegian Sea. The kraken also appears in Alfred Tennyson's poem of the same name, in Herman Melville's "Moby Dick", and in "Twenty Thousand Leagues Under the Sea" by Jules Verne.

Between the Lofoten islands of Moskenesøya and Værøy, at the tiny Mosken island, lies the Moskenstraumen – a system of tidal eddies and a whirlpool called a maelstrom. With a speed on the order of (the value strongly varies between sources), it is one of the strongest maelstroms in the world. It was described in the 13th century in the Old Norse Poetic Edda and remained an attractive subject for painters and writers, including Edgar Allan Poe, Walter Moers and Jules Verne. The word was introduced into the English language by Poe in his story "A Descent into the Maelström" (1841) describing the Moskenstraumen. The Moskenstraumen is created as a result of a combination of several factors, including the tides, the position of the Lofoten, and the underwater topography; unlike most other whirlpools, it is located in the open sea rather than in a channel or bay. With a diameter of 40–50 metres, it can be dangerous even in modern times to small fishing vessels that might be attracted by the abundant cod feeding on the microorganisms sucked in by the whirlpool.

The fish-rich coastal waters of northern Norway have long been known and attracted skilled sailors from Iceland and Greenland. Thus most settlements in Iceland and Greenland were on the west coasts of the islands, which were also warmer due to the Atlantic currents. The first reasonably reliable map of northern Europe, the Carta marina of 1539, represents the Norwegian Sea as coastal waters and shows nothing north of the North Cape. The Norwegian Sea off the coast regions appeared on the maps in the 17th century as an important part of the then sought Northern Sea Route and a rich whaling ground.

Jan Mayen island was discovered in 1607 and become an important base of Dutch whalers. The Dutchman Willem Barents discovered Bear Island and Svalbard, which was then used by Russian whalers called pomors. The islands on the edge of the Norwegian Sea have been rapidly divided between nations. During the peaks of whaling, some 300 ships with 12,000 crew members were yearly visiting Svalbard.

The first depth measurements of the Norwegian Sea were performed in 1773 by Constantine Phipps aboard HMS "Racehorse", as a part of his North Pole expedition. Systematic oceanographic research in the Norwegian Sea started in the late 19th century, when declines in the yields of cod and herring off the Lofoten prompted the Norwegian government to investigate the matter. The zoologist Georg Ossian Sars and meteorologist Henrik Mohn persuaded the government in 1874 to send out a scientific expedition, and between 1876 and 1878 they explored much of the sea aboard "Vøringen". The data obtained allowed Mohn to establish the first dynamic model of ocean currents, which incorporated winds, pressure differences, sea water temperature, and salinity and agreed well with later measurements.

Until the 20th century, the coasts of the Norwegian Sea were sparsely populated and therefore shipping in the sea was mostly focused on fishing, whaling, and occasional coastal transportation. Since the late 19th century, the Norwegian Coastal Express sea line has been established, connecting the more densely populated south with the north of Norway by at least one trip a day. The importance of shipping in the Norwegian Sea also increased with the expansion of the Russian and Soviet navies in the Barents Sea and development of international routes to the Atlantic through the Baltic Sea, Kattegat, Skagerrak, and North Sea.

The Norwegian Sea is ice-free and provides a direct route from the Atlantic to the Russian ports in the Arctic (Murmansk, Archangel, and Kandalaksha), which are directly linked to central Russia. This route was extensively used for supplies during World War II – of 811 US ships, 720 reached Russian ports, bringing some 4 million tonnes of cargo that included about 5,000 tanks and 7,000 aircraft. The Allies lost 18 convoys and 89 merchant ships on this route. The major operations of the German Navy against the convoys included PQ 17 in July 1942, the Battle of the Barents Sea in December 1942, and the Battle of the North Cape in December 1943 and were carried out around the border between the Norwegian Sea and Barents Sea, near the North Cape.

Navigation across the Norwegian Sea declined after World War II and intensified only in the 1960s–70s with the expansion of the Soviet Northern Fleet, which was reflected in major joint naval exercises of the Soviet Northern Baltic fleets in the Norwegian Sea. The sea was the gateway for the Soviet Navy to the Atlantic Ocean and thus to the United States, and the major Soviet port of Murmansk was just behind the border of the Norwegian and Barents Sea. The countermeasures by the NATO countries resulted in a significant naval presence in the Norwegian Sea and intense cat-and-mouse games between Soviet and NATO aircraft, ships, and especially submarines. A relic of the Cold War in the Norwegian Sea, the Soviet nuclear submarine K-278 Komsomolets, sank in 1989 southwest of Bear Island, at the border of the Norwegian and Barents seas, with radioactive material onboard that poses potential danger to flora and fauna.

The Norwegian Sea is part of the Northern Sea Route for ships from European ports to Asia. The travel distance from Rotterdam to Tokyo is via the Suez Canal and only through the Norwegian Sea. Sea ice is a common problem in the Arctic seas, but ice-free conditions along the entire northern route were observed at the end of August 2008. Russia is planning to expand its offshore oil production in the Arctic, which should increase the traffic of tankers through the Norwegian Sea to markets in Europe and America; it is expected that the number of oil shipments through the northern Norwegian Sea will increase from 166 in 2002 to 615 in 2015.

The most important products of the Norwegian Sea are no longer fish, but oil and especially gas found under the ocean floor. Norway started undersea oil production in 1993, followed by development of the Huldra gas field in 2001. The large depth and harsh waters of the Norwegian Sea pose significant technical challenges for offshore drilling. Whereas drilling at depths exceeding 500 meters has been conducted since 1995, only a few deep gas fields have been explored commercially. The most important current project is Ormen Lange (depth 800-1,100 m), where gas production started in 2007. With reserves of 1.4 cubic feet, it is the major Norwegian gas field. It is connected to the Langeled pipeline, currently the world's longest underwater pipeline, and thus to a major European gas pipeline network. Several other gas fields are being developed. A particular challenge is the Kristin field, where the temperature is as high as 170 °C and the gas pressure exceeds 900 bar (900 times the normal pressure).
Further north are Norne and Snøhvit.




</doc>
<doc id="21284" url="https://en.wikipedia.org/wiki?curid=21284" title="NMD">
NMD

NMD may refer to:


</doc>
<doc id="21285" url="https://en.wikipedia.org/wiki?curid=21285" title="Nuclear physics">
Nuclear physics

Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions. Other forms of nuclear matter are also studied.
Nuclear physics should not be confused with atomic physics, which studies the atom as a whole, including its electrons.

Discoveries in nuclear physics have led to applications in many fields. This includes nuclear power, nuclear weapons, nuclear medicine and magnetic resonance imaging, industrial and agricultural isotopes, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology. Such applications are studied in the field of nuclear engineering.

Particle physics evolved out of nuclear physics and the two fields are typically taught in close association. Nuclear astrophysics, the application of nuclear physics to astrophysics, is crucial in explaining the inner workings of stars and the origin of the chemical elements.

The history of nuclear physics as a discipline distinct from atomic physics starts with the discovery of radioactivity by Henri Becquerel in 1896, while investigating phosphorescence in uranium salts. The discovery of the electron by J. J. Thomson a year later was an indication that the atom had internal structure. At the beginning of the 20th century the accepted model of the atom was J. J. Thomson's "plum pudding" model in which the atom was a positively charged ball with smaller negatively charged electrons embedded inside it.

In the years that followed, radioactivity was extensively investigated, notably by Marie and Pierre Curie as well as by Ernest Rutherford and his collaborators. By the turn of the century physicists had also discovered three types of radiation emanating from atoms, which they named alpha, beta, and gamma radiation. Experiments by Otto Hahn in 1911 and by James Chadwick in 1914 discovered that the beta decay spectrum was continuous rather than discrete. That is, electrons were ejected from the atom with a continuous range of energies, rather than the discrete amounts of energy that were observed in gamma and alpha decays. This was a problem for nuclear physics at the time, because it seemed to indicate that energy was not conserved in these decays.

The 1903 Nobel Prize in Physics was awarded jointly to Becquerel for his discovery and to Marie and Pierre Curie for their subsequent research into radioactivity. Rutherford was awarded the Nobel Prize in Chemistry in 1908 for his "investigations into the disintegration of the elements and the chemistry of radioactive substances".

In 1905 Albert Einstein formulated the idea of mass–energy equivalence. While the work on radioactivity by Becquerel and Marie Curie predates this, an explanation of the source of the energy of radioactivity would have to wait for the discovery that the nucleus itself was composed of smaller constituents, the nucleons.

In 1906 Ernest Rutherford published "Retardation of the α Particle from Radium in passing through matter." Hans Geiger expanded on this work in a communication to the Royal Society with experiments he and Rutherford had done, passing alpha particles through air, aluminum foil and gold leaf. More work was published in 1909 by Geiger and Ernest Marsden, and further greatly expanded work was published in 1910 by Geiger. In 1911–1912 Rutherford went before the Royal Society to explain the experiments and propound the new theory of the atomic nucleus as we now understand it.

The key experiment behind this announcement was performed in 1910 at the University of Manchester: Ernest Rutherford's team performed a remarkable experiment in which Geiger and Marsden under Rutherford's supervision fired alpha particles (helium nuclei) at a thin film of gold foil. The plum pudding model had predicted that the alpha particles should come out of the foil with their trajectories being at most slightly bent. But Rutherford instructed his team to look for something that shocked him to observe: a few particles were scattered through large angles, even completely backwards in some cases. He likened it to firing a bullet at tissue paper and having it bounce off. The discovery, with Rutherford's analysis of the data in 1911, led to the Rutherford model of the atom, in which the atom had a very small, very dense nucleus containing most of its mass, and consisting of heavy positively charged particles with embedded electrons in order to balance out the charge (since the neutron was unknown). As an example, in this model (which is not the modern one) nitrogen-14 consisted of a nucleus with 14 protons and 7 electrons (21 total particles) and the nucleus was surrounded by 7 more orbiting electrons.

Around 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper "The Internal Constitution of the Stars". At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation "E = mc". This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered.

The Rutherford model worked quite well until studies of nuclear spin were carried out by Franco Rasetti at the California Institute of Technology in 1929. By 1925 it was known that protons and electrons each had a spin of . In the Rutherford model of nitrogen-14, 20 of the total 21 nuclear particles should have paired up to cancel each other's spin, and the final odd particle should have left the nucleus with a net spin of . Rasetti discovered, however, that nitrogen-14 had a spin of 1.

In 1932 Chadwick realized that radiation that had been observed by Walther Bothe, Herbert Becker, Irène and Frédéric Joliot-Curie was actually due to a neutral particle of about the same mass as the proton, that he called the neutron (following a suggestion from Rutherford about the need for such a particle). In the same year Dmitri Ivanenko suggested that there were no electrons in the nucleus — only protons and neutrons — and that neutrons were spin particles which explained the mass not due to protons. The neutron spin immediately solved the problem of the spin of nitrogen-14, as the one unpaired proton and one unpaired neutron in this model each contributed a spin of in the same direction, giving a final total spin of 1.

With the discovery of the neutron, scientists could at last calculate what fraction of binding energy each nucleus had, by comparing the nuclear mass with that of the protons and neutrons which composed it. Differences between nuclear masses were calculated in this way. When nuclear reactions were measured, these were found to agree with Einstein's calculation of the equivalence of mass and energy to within 1% as of 1934.

Alexandru Proca was the first to develop and report the massive vector boson field equations and a theory of the mesonic field of nuclear forces. Proca's equations were known to Wolfgang Pauli who mentioned the equations in his Nobel address, and they were also known to Yukawa, Wentzel, Taketani, Sakata, Kemmer, Heitler, and Fröhlich who appreciated the content of Proca's equations for developing a theory of the atomic nuclei in Nuclear Physics.

In 1935 Hideki Yukawa proposed the first significant theory of the strong force to explain how the nucleus holds together. In the Yukawa interaction a virtual particle, later called a meson, mediated a force between all nucleons, including protons and neutrons. This force explained why nuclei did not disintegrate under the influence of proton repulsion, and it also gave an explanation of why the attractive strong force had a more limited range than the electromagnetic repulsion between protons. Later, the discovery of the pi meson showed it to have the properties of Yukawa's particle.

With Yukawa's papers, the modern model of the atom was complete. The center of the atom contains a tight ball of neutrons and protons, which is held together by the strong nuclear force, unless it is too large. Unstable nuclei may undergo alpha decay, in which they emit an energetic helium nucleus, or beta decay, in which they eject an electron (or positron). After one of these decays the resultant nucleus may be left in an excited state, and in this case it decays to its ground state by emitting high energy photons (gamma decay).

The study of the strong and weak nuclear forces (the latter explained by Enrico Fermi via Fermi's interaction in 1934) led physicists to collide nuclei and electrons at ever higher energies. This research became the science of particle physics, the crown jewel of which is the standard model of particle physics which describes the strong, weak, and electromagnetic forces.

A heavy nucleus can contain hundreds of nucleons. This means that with some approximation it can be treated as a classical system, rather than a quantum-mechanical one. In the resulting liquid-drop model, the nucleus has an energy which arises partly from surface tension and partly from electrical repulsion of the protons. The liquid-drop model is able to reproduce many features of nuclei, including the general trend of binding energy with respect to mass number, as well as the phenomenon of nuclear fission.

Superimposed on this classical picture, however, are quantum-mechanical effects, which can be described using the nuclear shell model, developed in large part by Maria Goeppert Mayer and J. Hans D. Jensen. Nuclei with certain numbers of neutrons and protons (the magic numbers 2, 8, 20, 28, 50, 82, 126, ...) are particularly stable, because their shells are filled.

Other more complicated models for the nucleus have also been proposed, such as the interacting boson model, in which pairs of neutrons and protons interact as bosons, analogously to Cooper pairs of electrons.

Ab initio methods try to solve the nuclear many-body problem from the ground up, starting from the nucleons and their interactions.

Much of current research in nuclear physics relates to the study of nuclei under extreme conditions such as high spin and excitation energy. Nuclei may also have extreme shapes (similar to that of Rugby balls or even pears) or extreme neutron-to-proton ratios. Experimenters can create such nuclei using artificially induced fusion or nucleon transfer reactions, employing ion beams from an accelerator. Beams with even higher energies can be used to create nuclei at very high temperatures, and there are signs that these experiments have produced a phase transition from normal nuclear matter to a new state, the quark–gluon plasma, in which the quarks mingle with one another, rather than being segregated in triplets as they are in neutrons and protons.

Eighty elements have at least one stable isotope which is never observed to decay, amounting to a total of about 254 stable isotopes. However, thousands of isotopes have been characterized as unstable. These "radioisotopes" decay over time scales ranging from fractions of a second to trillions of years. Plotted on a chart as a function of atomic and neutron numbers, the binding energy of the nuclides forms what is known as the valley of stability. Stable nuclides lie along the bottom of this energy valley, while increasingly unstable nuclides lie up the valley walls, that is, have weaker binding energy.

The most stable nuclei fall within certain ranges or balances of composition of neutrons and protons: too few or too many neutrons (in relation to the number of protons) will cause it to decay. For example, in beta decay a nitrogen-16 atom (7 protons, 9 neutrons) is converted to an oxygen-16 atom (8 protons, 8 neutrons) within a few seconds of being created. In this decay a neutron in the nitrogen nucleus is converted by the weak interaction into a proton, an electron and an antineutrino. The element is transmuted to another element, with a different number of protons.

In alpha decay (which typically occurs in the heaviest nuclei) the radioactive element decays by emitting a helium nucleus (2 protons and 2 neutrons), giving another element, plus helium-4. In many cases this process continues through several steps of this kind, including other types of decays (usually beta decay) until a stable element is formed.

In gamma decay, a nucleus decays from an excited state into a lower energy state, by emitting a gamma ray. The element is not changed to another element in the process (no nuclear transmutation is involved).

Other more exotic decays are possible (see the first main article). For example, in internal conversion decay, the energy from an excited nucleus may eject one of the inner orbital electrons from the atom, in a process which produces high speed electrons, but is not beta decay, and (unlike beta decay) does not transmute one element to another.

In nuclear fusion, two low mass nuclei come into very close contact with each other, so that the strong force fuses them. It requires a large amount of energy for the strong or nuclear forces to overcome the electrical repulsion between the nuclei in order to fuse them; therefore nuclear fusion can only take place at very high temperatures or high pressures. When nuclei fuse, a very large amount of energy is released and the combined nucleus assumes a lower energy level. The binding energy per nucleon increases with mass number up to nickel-62. Stars like the Sun are powered by the fusion of four protons into a helium nucleus, two positrons, and two neutrinos. The uncontrolled fusion of hydrogen into helium is known as thermonuclear runaway. A frontier in current research at various institutions, for example the Joint European Torus (JET) and ITER, is the development of an economically viable method of using energy from a controlled fusion reaction.
Nuclear fusion is the origin of the energy (including in the form of light and other electromagnetic radiation) produced by the core of all stars including our own Sun.

Nuclear fission is the reverse process to fusion. For nuclei heavier than nickel-62 the binding energy per nucleon decreases with the mass number. It is therefore possible for energy to be released if a heavy nucleus breaks apart into two lighter ones.

The process of alpha decay is in essence a special type of spontaneous nuclear fission. It is a highly asymmetrical fission because the four particles which make up the alpha particle are especially tightly bound to each other, making production of this nucleus in fission particularly likely.

From certain of the heaviest nuclei whose fission produces free neutrons, and which also easily absorb neutrons to initiate fission, a self-igniting type of neutron-initiated fission can be obtained, in a chain reaction. Chain reactions were known in chemistry before physics, and in fact many familiar processes like fires and chemical explosions are chemical chain reactions. The fission or "nuclear" chain-reaction, using fission-produced neutrons, is the source of energy for nuclear power plants and fission type nuclear bombs, such as those detonated in Hiroshima and Nagasaki, Japan, at the end of World War II. Heavy nuclei such as uranium and thorium may also undergo spontaneous fission, but they are much more likely to undergo decay by alpha decay.

For a neutron-initiated chain reaction to occur, there must be a critical mass of the relevant isotope present in a certain space under certain conditions. The conditions for the smallest critical mass require the conservation of the emitted neutrons and also their slowing or moderation so that there is a greater cross-section or probability of them initiating another fission. In two regions of Oklo, Gabon, Africa, natural nuclear fission reactors were active over 1.5 billion years ago. Measurements of natural neutrino emission have demonstrated that around half of the heat emanating from the Earth's core results from radioactive decay. However, it is not known if any of this results from fission chain reactions.

According to the theory, as the Universe cooled after the Big Bang it eventually became possible for common subatomic particles as we know them (neutrons, protons and electrons) to exist. The most common particles created in the Big Bang which are still easily observable to us today were protons and electrons (in equal numbers). The protons would eventually form hydrogen atoms. Almost all the neutrons created in the Big Bang were absorbed into helium-4 in the first three minutes after the Big Bang, and this helium accounts for most of the helium in the universe today (see Big Bang nucleosynthesis).

Some relatively small quantities of elements beyond helium (lithium, beryllium, and perhaps some boron) were created in the Big Bang, as the protons and neutrons collided with each other, but all of the "heavier elements" (carbon, element number 6, and elements of greater atomic number) that we see today, were created inside stars during a series of fusion stages, such as the proton-proton chain, the CNO cycle and the triple-alpha process. Progressively heavier elements are created during the evolution of a star.

Since the binding energy per nucleon peaks around iron (56 nucleons), energy is only released in fusion processes involving smaller atoms than that. Since the creation of heavier nuclei by fusion requires energy, nature resorts to the process of neutron capture. Neutrons (due to their lack of charge) are readily absorbed by a nucleus. The heavy elements are created by either a "slow" neutron capture process (the so-called "s" process) or the "rapid", or "r" process. The "s" process occurs in thermally pulsing stars (called AGB, or asymptotic giant branch stars) and takes hundreds to thousands of years to reach the heaviest elements of lead and bismuth. The "r" process is thought to occur in supernova explosions which provide the necessary conditions of high temperature, high neutron flux and ejected matter. These stellar conditions make the successive neutron captures very fast, involving very neutron-rich species which then beta-decay to heavier elements, especially at the so-called waiting points that correspond to more stable nuclides with closed neutron shells (magic numbers).





</doc>
<doc id="21287" url="https://en.wikipedia.org/wiki?curid=21287" title="Nuremberg">
Nuremberg

Nuremberg (; ; )
is a city on the river Pegnitz and on the Rhine–Main–Danube Canal in the German state of Bavaria, in the administrative region of Middle Franconia, about north of Munich. It is the second-largest city in Bavaria (after Munich), and the largest in Franconia (). , it had a population of 517,498, making it Germany's fourteenth-largest city. The urban area also includes Fürth, Erlangen and Schwabach, with a total population of . the "European Metropolitan Area Nuremberg" had approximately 3.5 million inhabitants.

, according to the first documentary mention of the city in 1050, the location of an Imperial castle between the East Franks and the Bavarian March of the Nordgau. From 1050 to 1571, the city expanded and rose dramatically in importance due to its location on key trade routes. King Conrad III established a burgraviate, with the first burgraves coming from the Austrian House of Raab but, with the extinction of their male line around 1190, the burgraviate was inherited by the last count's son-in-law, of the House of Hohenzollern. From the late 12th century to the Interregnum (1254–73), however, the power of the burgraves diminished as the Hohenstaufen emperors transferred most non-military powers to a castellan, with the city administration and the municipal courts handed over to an Imperial mayor () from 1173/74. The strained relations between the burgraves and the castellan, with gradual transferral of powers to the latter in the late 14th and early 15th centuries, finally broke out into open enmity, which greatly influenced the history of the city.
Nuremberg is often referred to as having been the 'unofficial capital' of the Holy Roman Empire, particularly because Imperial Diet ("Reichstag") and courts met at Nuremberg Castle. The Diets of Nuremberg were an important part of the administrative structure of the empire. The increasing demand of the royal court and the increasing importance of the city attracted increased trade and commerce to Nuremberg. In 1219, Frederick II granted the "" ("Great Letter of Freedom"), including town rights, Imperial immediacy ("Reichsfreiheit"), the privilege to mint coins, and an independent customs policy, almost wholly removing the city from the purview of the burgraves. Nuremberg soon became, with Augsburg, one of the two great trade centers on the route from Italy to Northern Europe.

In 1298, the Jews of the town were falsely accused of having desecrated the host, and 698 of them were killed in one of the many Rintfleisch Massacres. Behind the massacre of 1298 was also the desire to combine the northern and southern parts of the city, which were divided by the Pegnitz. The Jews of the German lands suffered many massacres during the plague years. In 1349, Nuremberg's Jews were subjected to a pogrom. They were burned at the stake or expelled, and a marketplace was built over the former Jewish quarter.

The plague returned to the city in 1405, 1435, 1437, 1482, 1494, 1520 and 1534.
The largest gains for Nuremberg were in the 14th century; including Charles IV's Golden Bull of 1356, naming Nuremberg as the city where newly elected kings of Germany must hold their first Imperial Diet, making Nuremberg one of the three most important cities of the Empire. Charles was the patron of the Frauenkirche, built between 1352 and 1362 (the architect was likely Peter Parler), where the Imperial court worshipped during its stays in Nuremberg. The royal and Imperial connection was strengthened when Sigismund of Luxembourg granted the Imperial regalia to be kept permanently in Nuremberg in 1423, where they remained until 1796, when the advancing French troops required their removal to Regensburg and thence to Vienna.

In 1349 the members of the guilds unsuccessfully rebelled against the patricians in the "Handwerkeraufstand" ("Craftsmen's Uprising"), supported by merchants and some councillors, leading to a ban on any self-organisation of the artisans in the city, abolishing the guilds that were customary elsewhere in Europe; the unions were then dissolved, and the oligarchs remained in power while Nuremberg was a free city. Charles IV conferred upon the city the right to conclude alliances independently, thereby placing it upon a politically equal footing with the princes of the empire. Frequent fights took place with the burgraves without, however, inflicting lasting damage upon the city. After the castle had been destroyed by fire in 1420 during a feud between Frederick IV (since 1417 margrave of Brandenburg) and the duke of Bavaria-Ingolstadt, the ruins and the forest belonging to the castle were purchased by the city (1427), resulting in the city's total sovereignty within its borders. Through these and other acquisitions the city accumulated considerable territory. The Hussite Wars, recurrence of the Black Death in 1437, and the First Margrave War led to a severe fall in population in the mid-15th century. At the beginning of the 16th century, siding with Albert IV, Duke of Bavaria-Munich, in the Landshut War of Succession led the city to gain substantial territory, resulting in lands of , becoming one of the largest Imperial cities. During the Middle Ages, Nuremberg's literary culture was rich, varied, and influential.

The cultural flowering of Nuremberg, in the 15th and 16th centuries, made it the centre of the German Renaissance. In 1525, Nuremberg accepted the Protestant Reformation, and in 1532, the religious Peace of Nuremberg, by which the Lutherans gained important concessions, was signed there. During the 1552 revolution against Charles V, Nuremberg tried to purchase its neutrality, but the city was attacked without a declaration of war and was forced into a disadvantageous peace. At the Peace of Augsburg, the possessions of the Protestants were confirmed by the Emperor, their religious privileges extended and their independence from the Bishop of Bamberg affirmed, while the 1520s' secularisation of the monasteries was also approved.
The state of affairs in the early 16th century, increased trade routes elsewhere and the ossification of the social hierarchy and legal structures contributed to the decline in trade. Frequent quartering of Imperial, Swedish and League soldiers, the financial costs of the war and the cessation of trade caused irreparable damage to the city and a near-halving of the population. In 1632, the city, occupied by the forces of Gustavus Adolphus of Sweden, was besieged by the army of Imperial general Albrecht von Wallenstein. The city declined after the war and recovered its importance only in the 19th century, when it grew as an industrial centre. Even after the Thirty Years' War, however, there was a late flowering of architecture and culture – secular Baroque architecture is exemplified in the layout of the civic gardens built outside the city walls, and in the Protestant city's rebuilding of the Egidienkirche, destroyed by fire at the beginning of the 18th century, considered a significant contribution to the baroque church architecture of Middle Franconia.

After the Thirty Years' War, Nuremberg attempted to remain detached from external affairs, but contributions were demanded for the War of the Austrian Succession and the Seven Years' War and restrictions of imports and exports deprived the city of many markets for its manufactures. The Bavarian elector, Charles Theodore, appropriated part of the land obtained by the city during the Landshut War of Succession, to which Bavaria had maintained its claim; Prussia also claimed part of the territory. Realising its weakness, the city asked to be incorporated into Prussia but Frederick William II refused, fearing to offend Austria, Russia and France. At the Imperial diet in 1803, the independence of Nuremberg was affirmed, but on the signing of the Confederation of the Rhine on 12 July 1806, it was agreed to hand the city over to Bavaria from 8 September, with Bavaria guaranteeing the amortisation of the city's 12.5 million guilder public debt.

After the fall of Napoleon, the city's trade and commerce revived; the skill of its inhabitants together with its favourable situation soon made the city prosperous, particularly after its public debt had been acknowledged as a part of the Bavarian national debt. Having been incorporated into a Catholic country, the city was compelled to refrain from further discrimination against Catholics, who had been excluded from the rights of citizenship. Catholic services had been celebrated in the city by the priests of the Teutonic Order, often under great difficulties. After their possessions had been confiscated by the Bavarian government in 1806, they were given the Frauenkirche on the Market in 1809; in 1810 the first Catholic parish was established, which in 1818 numbered 1,010 souls.

In 1817, the city was incorporated into the district of Rezatkreis (named for the river Franconian Rezat), which was renamed to Middle Franconia () on 1 January 1838. The first German railway, the Bavarian Ludwigsbahn, from Nuremberg to nearby Fürth, was opened in 1835. The establishment of railways and the incorporation of Bavaria into Zollverein (the 19th-century German Customs Union), commerce and industry opened the way to greater prosperity. In 1852, there were 53,638 inhabitants: 46,441 Protestants and 6,616 Catholics. It subsequently grew to become the most important industrial city of Bavaria and one of the most prosperous towns of southern Germany but after the Austrian prussian war it was given to prussia in governmental power in extange for part of the taxes from there In 1905, its population, including several incorporated suburbs, was 291,351: 86,943 Catholics, 196,913 Protestants, 3,738 Jews and 3,766 members of other creeds.

Nuremberg held great significance during the Nazi Germany era. Because of the city's relevance to the Holy Roman Empire and its position in the centre of Germany, the Nazi Party chose the city to be the site of huge Nazi Party conventions — the Nuremberg rallies. The rallies were held in 1927, 1929 and annually from 1933 through 1938. After Adolf Hitler's rise to power in 1933 the Nuremberg rallies became huge Nazi propaganda events, a centre of Nazi ideals. The 1934 rally was filmed by Leni Riefenstahl, and made into a propaganda film called "Triumph des Willens" ("Triumph of the Will"). At the 1935 rally, Hitler specifically ordered the Reichstag to convene at Nuremberg to pass the Nuremberg Laws which revoked German citizenship for all Jews and other non-Aryans. A number of premises were constructed solely for these assemblies, some of which were not finished. Today many examples of Nazi architecture can still be seen in the city. The city was also the home of the Nazi propagandist Julius Streicher, the publisher of "Der Stürmer".
During the Second World War, Nuremberg was the headquarters of "Wehrkreis" (military district) XIII, and an important site for military production, including aircraft, submarines and tank engines. A subcamp of Flossenbürg concentration camp was located here, and extensively used slave labour. The city was severely damaged in Allied strategic bombing from 1943 to 1945. On 29 March 1944, the RAF endured its heaviest losses in the bombing campaign of Germany. Out of more than 700 planes participating, 106 were shot down or crash-landed on the way home to their bases, and more than 700 men were missing, as many as 545 of them dead. More than 160 became prisoners of war. On 2 January 1945, the medieval city centre was systematically bombed by the Royal Air Force and the U.S. Army Air Forces and about ninety percent of it was destroyed in only one hour, with 1,800 residents killed and roughly 100,000 displaced. In February 1945, additional attacks followed. In total, about 6,000 Nuremberg residents are estimated to have been killed in air raids.

Nuremberg was a heavily fortified city that was captured in a fierce battle lasting from 17 to 21 April 1945 by the U.S. 3rd Infantry Division, 42nd Infantry Division and 45th Infantry Division, which fought house-to-house and street-by-street against determined German resistance, causing further urban devastation to the already bombed and shelled buildings. Despite this intense degree of destruction, the city was rebuilt after the war and was to some extent, restored to its pre-war appearance including the reconstruction of some of its medieval buildings. However, over half of the historic look of the center, and especially the northeastern half of the old Imperial Free City was lost forever.

Between 1945 and 1946, German officials involved in war crimes and crimes against humanity were brought before an international tribunal in the Nuremberg trials. The Soviet Union had wanted these trials to take place in Berlin. However, Nuremberg was chosen as the site for the trials for specific reasons:

The same courtroom in Nuremberg was the venue of the Nuremberg Military Tribunals, organized by the United States as occupying power in the area.

Several old villages now belong to the city, for example Grossgründlach, Kraftshof, Thon, and Neunhof in the north-west; Ziegelstein in the north-east, Altenfurt and Fischbach in the south-east; and Katzwang, Kornburg in the south. Langwasser is a modern suburb.

Nuremberg has a humid continental climate (Köppen: Dfb) bordering on an oceanic climate (Köppen: Cfb). The city's climate is influenced by its inland position and higher altitude. Winters are changeable, with either mild or cold weather: the average temperature is around to , while summers are generally warm, mostly around at night to in the afternoon. Precipitation is evenly spread throughout the year, although February and April tend to be a bit drier whereas July tends to have more rainfall.

Nuremberg has been a popular destination for immigrants. 39.5 % of the residents had an immigrant background in 2010 (counted with MigraPro).

Nuremberg for many people is still associated with its traditional gingerbread ("Lebkuchen") products, sausages, and handmade toys. Pocket watches — "Nuremberg eggs" — were made here in the 16th century by Peter Henlein. In the 19th century Nuremberg became the "industrial heart" of Bavaria with companies such as Siemens and MAN establishing a strong base in the city. Nuremberg is still an important industrial centre with a strong standing in the markets of Central and Eastern Europe. Items manufactured in the area include electrical equipment, mechanical and optical products, motor vehicles, writing and drawing paraphernalia, stationery products, and printed materials. The city is also strong in the fields of automation, energy, and medical technology. Siemens is still the largest industrial employer in the Nuremberg region but a good third of German market research agencies are also located in the city. The Nuremberg International Toy Fair is the largest of its kind in the world.

Nuremberg was an early centre of humanism, science, printing, and mechanical invention. The city contributed much to the science of astronomy. In 1471 Johannes Mueller of Königsberg (Bavaria), later called Regiomontanus, built an astronomical observatory in Nuremberg and published many important astronomical charts. In 1515, Albrecht Dürer, a native of Nuremberg, created woodcuts of the first maps of the stars of the northern and southern hemispheres, producing the first printed star charts, which had been ordered by Johannes Stabius. Around 1515 Dürer also published the "Stabiussche Weltkarte", the first perspective drawing of the terrestrial globe. Perhaps most famously, the main part of Nicolaus Copernicus's work was published in Nuremberg in 1543.

Printers and publishers have a long history in Nuremberg. Many of these publishers worked with well-known artists of the day to produce books that could also be considered works of art. In 1470 Anton Koberger opened Europe's first print shop in Nuremberg. In 1493, he published the "Nuremberg Chronicles", also known as the "World Chronicles" ("Schedelsche Weltchronik"), an illustrated history of the world from the creation to the present day. It was written in the local Franconian dialect by Hartmann Schedel and had illustrations by Michael Wohlgemuth, Wilhelm Pleydenwurff, and Albrecht Dürer. Others furthered geographical knowledge and travel by map making. Notable among these was navigator and geographer Martin Behaim, who made the first world globe.

Sculptors such as Veit Stoss, Adam Kraft and Peter Vischer are also associated with Nuremberg.

Composed of prosperous artisans, the guilds of the Meistersingers flourished here. Richard Wagner made their most famous member, Hans Sachs, the hero of his opera "Die Meistersinger von Nürnberg". Baroque composer Johann Pachelbel was born here and was organist of St. Sebaldus Church.

The academy of fine arts situated in Nuremberg is the oldest art academy in central Europe and looks back to a tradition of 350 years of artistic education.

Nuremberg is also famous for its Christkindlesmarkt (Christmas market), which draws well over a million shoppers each year. The market is famous for its handmade ornaments and delicacies.


The Nuremberg State Theatre, founded in 1906, is dedicated to all types of opera, ballet and stage theatre. During the season 2009/2010, the theatre presented 651 performances for an audience of 240,000 persons. The State Philharmonic Nuremberg (Staatsphilharmonie Nürnberg) is the orchestra of the State Theatre. Its name was changed in 2011 from its previous name: The Nuremberg Philharmonic ("Nürnberger Philharmoniker"). It is the second-largest opera orchestra in Bavaria. Besides opera performances, it also presents its own subscription concert series in the Meistersingerhalle. Christof Perick was the principal conductor of the orchestra between 2006–2011. Marcus Bosch heads the orchestra since September 2011 .

The Nuremberg Symphony Orchestra ("Nürnberger Symphoniker") performs around 100 concerts a year to a combined annual audience of more than 180,000. The regular subscription concert series are mostly performed in the "Meistersingerhalle" but other venues are used as well, including the new concert hall of the "Kongresshalle" and the "Serenadenhof". Alexander Shelley has been the principal conductor of the orchestra since 2009.

The Nuremberg International Chamber Music Festival ("Internationales Kammermusikfestival Nürnberg") takes place in early September each year, and in 2011 celebrated its tenth anniversary. Concerts take place around the city; opening and closing events are held in the medieval "Burg". The Bardentreffen, an annual folk festival in Nuremberg, has been deemed the largest world music festival in Germany and takes place since 1976. 2014 the Bardentreffen starred 368 artists from 31 nations.

Nuremberg is known for Nürnberger Bratwurst (grilled sausage), which is shorter and thinner than other bratwurst sausages.

Another Nuremberg speciality is Nürnberger Lebkuchen, a kind of ginger bread eaten mainly around Christmas time.

Nuremberg offers 51 public and 6 private elementary schools in nearly all of its districts. Secondary education is offered at 23 Mittelschulen, 12 Realschulen, and 17 Gymnasien (state, city, church, and privately owned). There are also several other providers of secondary education such as Berufsschule, Berufsfachschule, Wirtschaftsschule etc. 

Nuremberg hosts the joint university Friedrich-Alexander-Universität Erlangen-Nürnberg, two Fachhochschulen (Technische Hochschule Nürnberg and "Evangelische Hochschule Nürnberg"), an art school ("Akademie der Bildenden Künste Nürnberg"), and a music conservatoire (Hochschule für Musik Nürnberg). There are also private schools such as the "Akademie Deutsche POP Nürnberg" offering higher education.


The city's location next to numerous highways, railways, and a waterway has contributed to its rising importance for trade with Eastern Europe.

Nürnberg Hauptbahnhof is a stop for IC and ICE trains on the German long-distance railway network. The Nuremberg–Ingolstadt–Munich high-speed line with operation opened 28 May 2006, and was fully integrated into the rail schedule on 10 December 2006. Travel times to Munich have been reduced to as little as one hour. The Nuremberg–Erfurt high-speed railway opened in December 2017.

The Nuremberg tramway network was opened in 1881. , it extended a total length of , had six lines, and carried 39.152 million passengers annually. The first segment of the Nuremberg U-Bahn metro system was opened in 1972. Nuremberg's trams, buses and metro system are operated by the VAG Nürnberg ("Verkehrsaktiengesellschaft Nürnberg" or Nuremberg Transport Corporation), itself a member of the VGN ("Verkehrsverbund Grossraum Nürnberg" or Greater Nuremberg Transport Network).

There is also a Nuremberg S-Bahn suburban metro railway and a regional train network, both centred on Nürnberg Hauptbahnhof.
Since 2008, Nuremberg has had the first U-Bahn in Germany (U2/U21 and U3) that works without a driver. It also was the first subway system worldwide in which both driver-operated trains and computer-controlled trains shared tracks.

Nuremberg is located at the junction of several important Autobahn routes. The A3 ("Netherlands"–Frankfurt–Würzburg–"Vienna") passes in a south-easterly direction along the north-east of the city. The A9 (Berlin–Munich) passes in a north–south direction on the east of the city. The A6 ("France"–Saarbrücken–"Prague") passes in an east–west direction to the south of the city. Finally, the A73 begins in the south-east of Nuremberg and travels north-west through the city before continuing towards Fürth and Bamberg.

Nuremberg Airport has flights to major German cities and many European destinations. The largest operators are currently Germania, TUI fly or SunExpress, while the low-cost Ryanair and WizzAir companies connect the city to various European centres. A significant amount of the airport's traffic flies to and from mainly touristic destinations during the peak winter season. The airport (Flughafen) is the terminus of subway line 2; it is the only airport in Germany served by a subway.

Nuremberg is an important port on the Rhine–Main–Danube Canal.

1. FC Nürnberg, known locally as "Der Club" (English: "The Club"), was founded in 1900 and currently plays in the Bundesliga. The official colours of the association are red and white, but the traditional colours are red and black. The current chairmen are Andreas Bornemann and Michael Meeske. They play in Max-Morlock-Stadion which was refurbished for the 2006 FIFA World Cup and accommodates 50,000 spectators.


The "SELLBYTEL Baskets Nürnberg" played in the Basketball Bundesliga from 2005 to 2007. Since then, teams from Nuremberg have attempted to return to Germany's elite league. The recently founded Nürnberg Falcons BC have already established themselves as one of the main teams in Germany's second division ProA and aim to take on the heritage of the SELLBYTEL Baskets Nürnberg. The Falcons play their home games at the "Halle im Berufsbildungszentrum (BBZ)".

Nuremberg is twinned with:
Apart from the official twin towns (sister cities), there are a number with which Nuremberg maintains "cordial relations":
Nuremberg also engages in cooperation with various other cities internationally:





</doc>
<doc id="21288" url="https://en.wikipedia.org/wiki?curid=21288" title="Nicholas Lemann">
Nicholas Lemann

Nicholas Berthelot Lemann is the Joseph Pulitzer II and Edith Pulitzer Moore Professor of Journalism and Dean Emeritus of the Faculty of Journalism at the Columbia University Graduate School of Journalism. He has been a staff writer at "The New Yorker" since 1999.

Nicholas Lemann was born, raised, and educated in a Jewish family in New Orleans. He describes his family's faith as a "kind of super-Reform Judaism" where there were "no kosher laws, no bar mitzvahs, no tallit, no kippot".

Lemann was educated at Metairie Park Country Day School, a private school in New Orleans, from which he graduated in 1972, followed by Harvard College, where he studied American history and literature, and was president of "The Harvard Crimson", where he wrote the "Brass Tacks" column, and from which he graduated "magna cum laude" in 1976.

Lemann began his journalism career as a 17-year-old writer for an alternative weekly newspaper, the "Vieux Carre Courier", in his home city of New Orleans. In 1975, amid reports of mass murder in Cambodia by the Khmer Rouge, Lemann wrote, "I continue to support the Khmer Rouge in its principles and goals but I have to admit that I deplore the way they are going about it." After graduation, he worked at the "Washington Monthly", as an associate editor and then managing editor; at "Texas Monthly", as an associate editor and then executive editor; at "The Washington Post", as a member of the national staff; at "The Atlantic Monthly", as national correspondent; and at "The New Yorker", as staff writer and then Washington correspondent.

On September 1, 2003, Lemann became dean of the Graduate School of Journalism at Columbia University. During Lemann's time as dean, the Journalism School launched and completed its first capital fundraising campaign, added 20 members to its full-time faculty, built a student center, started its first new professional degree program since the 1930s, and launched initiatives in investigative reporting, digital journalism, executive leadership for news organizations, and other areas. He stepped down as dean in 2013, following two five-year terms.

In 2015, Lemann launched "Columbia Global Reports", a university-funded publishing imprint that produces four to six ambitious works of journalism and analysis a year, each on a different underreported story in the world.

Lemann has published five books, most recently "Redemption: The Last Battle of the Civil War" (2006); "The Big Test: The Secret History of the American Meritocracy" (1999); and "The Promised Land: The Great Black Migration and How It Changed America" (1991), which won several book prizes. He has written widely for such publications as "The New York Times", "The New York Review of Books", "The New Republic", and "Slate"; worked in documentary television with Blackside, Inc., "Frontline", the Discovery Channel, and the BBC; and lectured at many universities.

Lemann serves on the boards of directors of the Authors Guild, the National Academy of Sciences’ Division of Behavioral and Social Sciences and Education, and the Academy of Political Science, and is a member of the New York Institute for the Humanities. He was named a fellow of the American Academy of Arts and Sciences in April 2010.

Lemann has been married twice. His first wife was Dominique Alice Browning, who later became an editor in chief of "House & Garden" until 2007; they married on May 20, 1983, have two sons, Alexander and Theodore, and later divorced. His second wife is Judith Anne Shulevitz, who was a columnist for "Slate" "The New York Times Book Review," and "The New Republic"; married on November 7, 1999, they have a son and a daughter.






</doc>
<doc id="21289" url="https://en.wikipedia.org/wiki?curid=21289" title="Nautical mile">
Nautical mile

A nautical mile is a unit of measurement defined as exactly . Historically, it was defined as one minute of latitude, which is equivalent to one sixtieth of a degree of latitude. Today, it is a non-SI unit which has a continued use in both air and marine navigation, and for the definition of territorial waters.

One tenth of a nautical mile is a cable length.

The derived unit of speed is the knot, defined as one nautical mile per hour. The geographical mile is the length of one minute of longitude along the Equator, about 1,855 m on the WGS 84 ellipsoid.

There is no internationally agreed symbol.
While using M itself, the International Bureau of Weights and Measures recognises that NM, Nm and nmi are also in use.

The word mile is from the Latin word for a thousand paces: mille passus. Navigation at sea was done by eye until around 1500 when navigational instruments were developed and cartographers began using a coordinate system with parallels of latitude and meridians of longitude. In 1617 the Dutch scientist Willebrord Snell assessed the circumference of the Earth at 24,630 Roman miles (24,024 statute miles). Around that time British mathematician Edmund Gunter improved navigational tools including a new quadrant to determine latitude at sea. He reasoned that the lines of latitude could be used as the basis for a unit of measurement for distance and proposed the nautical mile as one minute or one-sixtieth () of one degree of latitude. As one degree is of a circle, one minute of arc is of a circle (or, in radians, ). These sexagesimal (base 60) units originated in Babylonian astronomy. Gunter used Snell's circumference to define a nautical mile as 6,080 feet, the length of one minute of arc at 48 degrees latitude. Since the earth is not a perfect sphere but is an oblate spheroid with slightly flattened poles, a minute of latitude is not constant, but about at the poles and at the Equator, with a mean value of . Latitude and Longitude are equivalent or "square" at the Equator. Other countries measure the minute of arc at 45 degrees latitude, giving the nautical mile a length of .

In 1929, the international nautical mile was defined by the First International Extraordinary Hydrographic Conference in Monaco as precisely 1,852 metres.

Imperial units and United States customary units used a definition of the nautical mile based on the Clarke (1866) Spheroid. The United States nautical mile was defined as based in the Mendenhall Order foot of 1893. It was abandoned in favour of the international nautical mile in 1954.

The Imperial nautical mile, often called an Admiralty mile, or more correctly, an Admiralty measured mile, was defined by its relation to the Admiralty knot, 6,080 imperial feet per hour, so 1 imperial nautical mile is about 1,853.181 metres. It was abandoned in 1970 and, legally, references to the obsolete unit are now converted to 1,853 metres.

Despite the existence of precise modern definitions, in the early 21st century the old definitions are still in use. The Royal Yachting Association says in its manual for day skippers: "1 (minute) of Latitude = 1 sea mile", followed by "For most practical purposes distance is measured from the latitude scale, assuming that one minute of latitude equals one nautical mile".



</doc>
<doc id="21290" url="https://en.wikipedia.org/wiki?curid=21290" title="N">
N

N (named "en" ) is the fourteenth letter in the modern English alphabet and the ISO basic Latin alphabet.

One of the most common hieroglyphs, snake, was used in Egyptian writing to stand for a sound like the English , because the Egyptian word for "snake" was "djet". It is speculated by many that Semitic people working in Egypt adapted hieroglyphics to create the first alphabet, and that they used the same snake symbol to represent N, because their word for "snake" may have begun with that sound. However, the name for the letter in the Phoenician, Hebrew, Aramaic and Arabic alphabets is "nun", which means "fish" in some of these languages. The sound value of the letter was —as in Greek, Etruscan, Latin and modern languages.

 represents a dental or alveolar nasal in virtually all languages that use the Latin alphabet, and in the International Phonetic Alphabet. A common digraph with is , which represents a velar nasal in a variety of languages, usually positioned word-finally in English. Often, before a velar plosive (as in "ink" or "jungle"), alone represents a velar nasal. In Italian and French, represents a palatal nasal . The Portuguese and Vietnamese spelling for this sound is , while Spanish, Breton, and a few other languages use the letter . In English, is generally silent when it is preceded by an at the end of words, as in "hymn"; however, it is pronounced in this combination when occurring word medially, as in "hymnal".

In mathematics, the italic form "n" is a particularly common symbol for a variable quantity which represents an integer.





</doc>
<doc id="21291" url="https://en.wikipedia.org/wiki?curid=21291" title="Nail (fastener)">
Nail (fastener)

In woodworking and construction, a nail is a pin-shaped object of metal (or wood, called a tree nail or "trunnel") which is used as a fastener, as a peg to hang something, or sometimes as a decoration. Generally, nails have a sharp point on one end and a flattened head on the other, but headless nails are available. Nails are made in a great variety of forms for specialized purposes. The most common is a "wire nail". Other types of nails include "pins", "tacks", "brads", "spikes", and "cleats."

Nails are typically driven into the workpiece by a hammer, a pneumatic nail gun, or a small explosive charge or primer. A nail holds materials together by friction in the axial direction and shear strength laterally. The point of the nail is also sometimes bent over or "clinched" after driving to prevent pulling out.

The history of the nail is divided roughly into three distinct periods: 

The first nails were made of wrought-iron. Nails date back at least to Ancient Egypt — bronze nails found in Egypt have been dated 3400 BC. The Bible provides a number of references to nails, including the story in Judges of Jael the wife of Heber, who drives a nail (or tent-peg) into the temple of a sleeping Canaanite commander; the provision of iron for nails by King David for what would become Solomon's Temple; and in connection with the crucifixion of Christ.

The Romans made extensive use of nails. The Roman army, for example, left behind seven tons of nails when it evacuated the fortress of Inchtuthil in Perthshire in the United Kingdom in 86 to 87 CE.

The term "penny", as it refers to nails, probably originated in medieval England to describe the price of a hundred nails. Nails themselves were sufficiently valuable and standardized to be used as an informal medium of exchange.
Until around 1800 artisans known as "nailers" or "nailors" made nails by hand – note the surname Naylor.

At the time of the American Revolution, England was the largest manufacturer of nails in the world. Nails were expensive and difficult to obtain in the American colonies, so that abandoned houses were sometimes deliberately burned down to allow recovery of used nails from the ashes. This became such a problem in Virginia that a law was created to stop people from burning their houses when they moved. Families often had small nail-manufacturing setups in their homes; during bad weather and at night, the entire family might work at making nails for their own use and for barter. Thomas Jefferson wrote in a letter: "In our private pursuits it is a great advantage that every honest employment is deemed honorable. I am myself a nail maker." The growth of the trade in the American colonies was theoretically held back by the prohibition of new slitting mills in America by the Iron Act of 1750, though there is no evidence that the Act was actually enforced.

The production of wrought-iron nails continued well into the 19th century, but ultimately was reduced to nails for purposes for which the softer cut nails were unsuitable, including horseshoe nails.

The slitting mill, introduced to England in 1590, simplified the production of nail rods, but the real first efforts to mechanise the nail-making process itself occurred between 1790 and 1820, initially in the United States and England, when various machines were invented to automate and speed up the process of making nails from bars of wrought iron. These nails were known as "cut nails" or "square nails" because of their roughly rectangular cross section. Cut nails were one of the important factors in the increase in balloon framing beginning in the 1830s and thus the decline of timber framing with wooden joints. Though still used for historical renovations, and for heavy-duty applications, such as attaching boards to masonry walls, "cut nails" are much less common today than "wire nails".

The cut-nail process was patented in America by Jacob Perkins in 1795 and in England by Joseph Dyer, who set up machinery in Birmingham. The process was designed to cut nails from sheets of iron, while making sure that the fibres of the iron ran down the nails. The Birmingham industry expanded in the following decades, and reached its greatest extent in the 1860s, after which it declined due to competition from wire nails, but continued until the outbreak of World War I.

As the name implies, wire nails are formed from wire. Usually coils of wire are drawn through a series of dies to reach a specific diameter, then cut into short rods that are then formed into nails. The nail tip is usually cut by a blade; the head is formed by reshaping the other end of the rod under high pressure. Other dies are used to cut grooves and ridges. Wire nails were also known as "French nails" for their country of origin. Belgian wire nails began to compete in England in 1863. Joseph Henry Nettlefold was making wire nails at Smethwick by 1875. Over the following decades, the nail-making process was almost completely automated. Eventually the industry had machines capable of quickly producing huge numbers of inexpensive nails with little or no human intervention.

With the introduction of cheap wire nails, the use of wrought iron for nail making quickly declined, as more slowly did the production of cut nails. In the United States, in 1892 more steel-wire nails were produced than cut nails. In 1913, 90% of manufactured nails were wire nails. Nails went from being rare and precious to being a cheap mass-produced commodity. Today almost all nails are manufactured from wire, but the term "wire nail" has come to refer to smaller nails, often available in a wider, more precise range of gauges than is typical for larger common and finish nails.

Formerly made of bronze or wrought iron, today's nails are typically made of steel, often dipped or coated to prevent corrosion in harsh conditions or to improve adhesion. Ordinary nails for wood are usually of a soft, low-carbon or "mild" steel (about 0.1% carbon, the rest iron and perhaps a trace of silicon or manganese). Nails for concrete are harder, with 0.5–0.75% carbon.

Types of nail include:

Most countries, except the United States, use a metric system for describing nail sizes. A "50 × 3.0" indicates a nail 50 mm long (not including the head) and 3 mm in diameter. Lengths are rounded to the nearest millimetre.

For example, finishing nail* sizes typically available from German suppliers are:

In the United States, the length of a nail is designated by its penny size.


Nails have been used in art, such as the Nail Men—a form of fundraising common in Germany and Austria during World War I.

Before the 1850s bocce and pétanque boules were wooden balls, sometimes partially reinforced with hand-forged nails. When cheap, plentiful machine-made nails became available, manufacturers began to produce the "boule cloutée"—a wooden core studded with nails to create an all-metal surface. Nails of different metals and colors (steel, brass, and copper) were used to create a wide variety of designs and patterns. Some of the old "boules cloutées" are genuine works of art and valued collector's items.

Once nails became cheap and widely available, they were often used in folk art and outsider art as a method of decorating a surface with metallic studs. Another common artistic use is the construction of sculpture from welded or brazed nails.




</doc>
<doc id="21292" url="https://en.wikipedia.org/wiki?curid=21292" title="Namibia">
Namibia

Namibia (, ), officially the Republic of Namibia (German: ; ), is a country in southern Africa whose western border is the Atlantic Ocean. It shares land borders with Zambia and Angola to the north, Botswana to the east and South Africa to the south and east. Although it does not border Zimbabwe, less than 200 metres of the Zambezi River (essentially a small bulge in Botswana to achieve a Botswana/Zambia micro-border) separates the two countries. Namibia gained independence from South Africa on 21 March 1990, following the Namibian War of Independence. Its capital and largest city is Windhoek, and it is a member state of the United Nations (UN), the Southern African Development Community (SADC), the African Union (AU), and the Commonwealth of Nations.

Namibia, the driest country in Sub-Saharan Africa, was inhabited since early times by the San, Damara, and Nama peoples. Since about the 14th century, immigrating Bantu peoples arrived as part of the Bantu expansion. Since then the Bantu groups in total, one of which is known as the Ovambo people, have dominated the population of the country and since the late 19th century, have constituted a majority.

In 1878 the Cape of Good Hope, then a British colony, had annexed the port of Walvis Bay and the offshore Penguin Islands; these became an integral part of the new Union of South Africa at its creation in 1910. In 1884 the German Empire established rule over most of the territory as a protectorate ("Schutzgebiet"). It began to develop infrastructure and farming, and maintained this German colony until 1915, when South African forces defeated its military. In 1920, after the end of World War I, the League of Nations mandated the country to the United Kingdom, under administration by South Africa. It imposed its laws, including racial classifications and rules.

From 1948, with the National Party elected to power, South Africa applied apartheid also to what was then known as South West Africa.

In the later 20th century, uprisings and demands for political representation by native African political activists seeking independence resulted in the UN assuming direct responsibility over the territory in 1966, but South Africa maintained "de facto" rule. In 1973 the UN recognised the South West Africa People's Organisation (SWAPO) as the official representative of the Namibian people; the party is dominated by the Ovambo, who are a large plurality in the territory. Following continued guerrilla warfare, South Africa installed an interim administration in Namibia in 1985. Namibia obtained full independence from South Africa in 1990. However, Walvis Bay and the Penguin Islands remained under South African control until 1994.

Namibia has a population of 2.6 million people and a stable multi-party parliamentary democracy. Agriculture, herding, tourism and the mining industry – including mining for gem diamonds, uranium, gold, silver, and base metals – form the basis of its economy. The large, arid Namib Desert has resulted in Namibia being overall one of the least densely populated countries in the world.

The name of the country is derived from the Namib Desert, considered to be the oldest desert in the world. The name, "Namib" itself, is of Nama origin and means "vast place". Before its independence in 1990, the area was known first as German South-West Africa ("Deutsch-Südwestafrika"), then as South-West Africa, reflecting the colonial occupation by the Germans and the South Africans (technically on behalf of the British crown reflecting South Africa's dominion status within the British Empire).

The dry lands of Namibia were inhabited since early times by San, Damara, and Nama. From about the 14th century, immigrating Bantu people arrived during the Bantu expansion from central Africa. From the late 18th century onwards, Oorlam people from Cape Colony crossed the Orange River and moved into the area that today is southern Namibia. Their encounters with the nomadic Nama tribes were largely peaceful. The missionaries accompanying the Oorlam were well received by them, the right to use waterholes and grazing was granted against an annual payment. On their way further northwards, however, the Oorlam encountered clans of the Herero at Windhoek, Gobabis, and Okahandja, who resisted their encroachment. The Nama-Herero War broke out in 1880, with hostilities ebbing only after the German Empire deployed troops to the contested places and cemented the status quo among the Nama, Oorlam, and Herero.

The first Europeans to disembark and explore the region were the Portuguese navigators Diogo Cão in 1485 and Bartolomeu Dias in 1486, but the Portuguese crown did not try to claim the area. Like most of interior Sub-Saharan Africa, Namibia was not extensively explored by Europeans until the 19th century. At that time traders and settlers came principally from Germany and Sweden. In the late 19th century, Dorsland Trekkers crossed the area on their way from the South African Republic to Angola. Some of them settled in Namibia instead of continuing their journey.

Namibia became a German colony in 1884 under Otto von Bismarck to forestall perceived British encroachment and was known as German South West Africa ("Deutsch-Südwestafrika"). However, the Palgrave Commission by the British governor in Cape Town had determined that only the natural deep-water harbor of Walvis Bay was worth occupying – and this was annexed to the Cape province of British South Africa.

From 1904 to 1907, the Herero and the Namaqua took up arms against brutal German colonialism. In calculated punitive action by the German occupiers, what has been called the 'first genocide of the Twentieth Century' was committed, as government officials ordered extinction of the natives. In the Herero and Namaqua genocide, the Germans systematically killed 10,000 Nama (half the population) and approximately 65,000 Herero (about 80% of the population). The survivors, when finally released from detention, were subjected to a policy of dispossession, deportation, forced labor, racial segregation, and discrimination in a system that in many ways anticipated the apartheid established by South Africa in 1948.

Most Africans were confined to so-called native territories, which later under South African rule after 1949 were turned into "homelands" (Bantustans). Indeed, some historians have speculated that the German genocide in Namibia was a model used by Nazis in the Holocaust. The memory of genocide remains relevant to ethnic identity in independent Namibia and to relations with Germany. The German government formally apologized for the Namibian genocide in 2004.

During World War I, South African troops under General Louis Botha occupied the territory and deposed the German colonial administration. The end of the war and the Treaty of Versailles left South Africa in possession of South West Africa as a League of Nations mandate. The mandate system was formed as a compromise between those who advocated an Allied annexation of former German and Turkish territories, and another proposition put forward by those who wished to grant them to an international trusteeship until they could govern themselves. It permitted the South African government to administer South West Africa for an undefined period until that territory's inhabitants were prepared for political self-determination. However, South Africa interpreted the mandate as a veiled annexation and made no attempt to prepare South West Africa for future autonomy.

As a result of the Conference on International Organization in 1945, the League of Nations was formally superseded by the United Nations (UN) and former League mandates by a trusteeship system. Article 77 of the United Nations Charter stated that UN trusteeship "shall apply...to territories now held under mandate"; furthermore, it would "be a matter of subsequent agreement as to which territories in the foregoing territories will be brought under the trusteeship system and under what terms". The UN requested all former League of Nations mandates be surrendered to its Trusteeship Council in anticipation of their independence. South Africa declined to do so and instead requested permission from the UN to formally annex South West Africa, for which it received considerable criticism. When the UN General Assembly rejected this proposal, South Africa dismissed its opinion as irrelevant and began solidifying control of the territory. The UN responded by deferring to the International Court of Justice (ICJ), which held a number of discussions on the legality of South African rule between 1949 and 1966.

South Africa began imposing its codified system of racial segregation and discrimination—"apartheid"—on South West Africa during the late 1940s. Black South West Africans were subject to pass laws, curfews, and a host of draconian residential regulations; their movement became heavily restricted. Development was concentrated in the region of the country immediately adjacent to South Africa, formally denoted as the "Police Zone", where most of the German colonial era settlements and mines were also located. Outside the Police Zone, indigenous peoples were restricted to theoretically self-governing tribal homelands.

During the late 1950s and early 1960s, pressure for global decolonisation and national self-determination began mounting on the African continent; these factors had a radical impact on South West African nationalism. Early nationalist organisations such as the South West African National Union (SWANU) and South West African People's Organisation (SWAPO) made determined attempts to establish indigenous political structures for an independent South West Africa. In 1966, following the ICJ's controversial ruling that it had no legal standing to consider the question of South African rule, SWAPO launched an armed insurgency which escalated into part of a wider regional conflict known as the South African Border War. 

As SWAPO's insurgency intensified, South Africa's case for annexation in the international community continued to decline. The UN declared that South Africa had failed in its obligations to ensure the moral and material well-being of the indigenous inhabitants of South West Africa, and had thus disavowed its own mandate. On 12 June 1968, the UN General Assembly adopted a resolution which proclaimed that, in accordance with the desires of its people, South West Africa be renamed "Namibia". United Nations Security Council Resolution 269, adopted in August 1969, declared South Africa's continued occupation of Namibia illegal. In recognition of this landmark decision, SWAPO's armed wing was renamed the People's Liberation Army of Namibia (PLAN). 

Namibia became one of several flashpoints for Cold War proxy conflicts in southern Africa during the latter years of the PLAN insurgency. The insurgents sought out weapons and sent recruits to the Soviet Union for military training. SWAPO's political leadership, dependent on military aid from the Soviets, Cuba, and Angola, positioned the movement within the socialist bloc by 1975. This practical alliance reinforced the prevailing perspective of SWAPO as a Soviet proxy, which dominated Cold War ideology in South Africa and the United States. For its part, the Soviet Union supported SWAPO partly because it viewed South Africa as a regional Western ally. 

Growing war weariness and the reduction of tensions between the superpowers compelled South Africa, Angola, and Cuba to accede to the Tripartite Accord, under pressure from both the Soviet Union and the United States. South Africa accepted Namibian independence in exchange for a Cuban military withdrawal from the region and an Angolan commitment to cease all aid to PLAN. PLAN and South Africa adopted an informal ceasefire in August 1988, and a United Nations Transition Assistance Group (UNTAG) was formed to monitor the Namibian peace process and supervise the return of refugees. The ceasefire was broken after PLAN made a final incursion into the territory, possibly as a result of misunderstanding UNTAG's directives, in March 1989. A new ceasefire was later imposed with the condition that the insurgents were to be confined to their external bases in Angola until they could be disarmed and demobilised by UNTAG. 

By the end of the eleven month transition period, the last South African troops had been withdrawn from Namibia, all political prisoners granted amnesty, racially discriminatory legislation repealed, and 42,000 Namibian refugees returned to their homes. Just over 97% of eligible voters participated in the country's first parliamentary elections held under a universal franchise. SWAPO won a plurality of seats in the Constituent Assembly with 57% of the popular vote. This gave the party 41 seats, but not a two-thirds majority which would have enabled it to draft the constitution on its own. 

The Namibian Constitution adopted in February 1990 incorporated protection for human rights, compensation for state expropriations of private property, and established an independent judiciary, legislature, and an executive presidency (the constituent assembly became the national assembly). The country officially became independent on 21 March 1990. Sam Nujoma was sworn in as the first President of Namibia at a ceremony attended by Nelson Mandela of South Africa (who had been released from prison the previous month) and representatives from 147 countries, including 20 heads of state. Upon the end of Apartheid in South Africa in 1994, the nation ceded Walvis Bay to Namibia.

Since independence Namibia has successfully completed the transition from white minority apartheid rule to parliamentary democracy. Multiparty democracy was introduced and has been maintained, with local, regional and national elections held regularly. Several registered political parties are active and represented in the National Assembly, although the SWAPO has won every election since independence. The transition from the 15-year rule of President Sam Nujoma to his successor Hifikepunye Pohamba in 2005 went smoothly.

Since independence, the Namibian government has promoted a policy of national reconciliation. It issued an amnesty for those who had fought on either side during the liberation war. The civil war in Angola spilled over and adversely affected Namibians living in the north of the country. In 1998, Namibia Defence Force (NDF) troops were sent to the Democratic Republic of the Congo as part of a Southern African Development Community (SADC) contingent.

In 1999, the national government successfully quashed a secessionist attempt in the northeastern Caprivi Strip. The Caprivi conflict was initiated by the Caprivi Liberation Army (CLA), a rebel group led by Mishake Muyongo. It wanted the Caprivi Strip to secede in order to form its own society.

At , Namibia is the world's thirty-fourth largest country (after Venezuela). It lies mostly between latitudes 17° and 29°S (a small area is north of 17°), and longitudes 11° and 26°E.

Being situated between the Namib and the Kalahari deserts, Namibia has the least rainfall of any country in sub-Saharan Africa.

The Namibian landscape consists generally of five geographical areas, each with characteristic abiotic conditions and vegetation, with some variation within and overlap between them: the Central Plateau, the Namib, the Great Escarpment, the Bushveld, and the Kalahari Desert.

The Central Plateau runs from north to south, bordered by the Skeleton Coast to the northwest, the Namib Desert and its coastal plains to the southwest, the Orange River to the south, and the Kalahari Desert to the east. The Central Plateau is home to the highest point in Namibia at Königstein elevation .

The Namib is a broad expanse of hyper-arid gravel plains and dunes that stretches along Namibia's entire coastline. It varies between 100 and many hundreds of kilometres in width. Areas within the Namib include the Skeleton Coast and the Kaokoveld in the north and the extensive Namib Sand Sea along the central coast.

The Great Escarpment swiftly rises to over . Average temperatures and temperature ranges increase further inland from the cold Atlantic waters, while the lingering coastal fogs slowly diminish. Although the area is rocky with poorly developed soils, it is significantly more productive than the Namib Desert. As summer winds are forced over the Escarpment, moisture is extracted as precipitation.

The Bushveld is found in north-eastern Namibia along the Angolan border and in the Caprivi Strip. The area receives a significantly greater amount of precipitation than the rest of the country, averaging around per year. The area is generally flat and the soils sandy, limiting their ability to retain water and support agriculture.

The Kalahari Desert, an arid region that extends into South Africa and Botswana, is one of Namibia's well-known geographical features. The Kalahari, while popularly known as a desert, has a variety of localised environments, including some verdant and technically non-desert areas. The Succulent Karoo is home to over 5,000 species of plants, nearly half of them endemic; approximately 10 percent of the world's succulents are found in the Karoo. The reason behind this high productivity and endemism may be the relatively stable nature of precipitation.

Namibia's Coastal Desert is one of the oldest deserts in the world. Its sand dunes, created by the strong onshore winds, are the highest in the world. Because of the location of the shoreline, at the point where the Atlantic's cold water reaches Africa's hot climate, often extremely dense fog forms along the coast. Near the coast there are areas where the dunes are vegetated with hammocks. Namibia has rich coastal and marine resources that remain largely unexplored.

Namibia extends from 17°S to 25°S latitude: climatically the range of the sub-Tropical High Pressure Belt. Its overall climate description is arid, descending from the Sub-Humid [mean rain above ] through Semi-Arid [between ] (embracing most of the waterless Kalahari) and Arid [from ] (these three regions are inland from the western escarpment) to the Hyper-Arid coastal plain [less than ]. Temperature maxima are limited by the overall elevation of the entire region: only in the far south, Warmbad for instance, are mid- maxima recorded.

Typically the sub-Tropical High Pressure Belt, with frequent clear skies, provides more than 300 days of sunshine per year. It is situated at the southern edge of the tropics; the Tropic of Capricorn cuts the country about in half. The winter (June – August) is generally dry. Both rainy seasons occur in summer: the small rainy season between September and November, the big one between February and April. Humidity is low, and average rainfall varies from almost zero in the coastal desert to more than in the Caprivi Strip. Rainfall is highly variable, and droughts are common. The rainy season with rainfall far below the annual average occurred in summer 2006/07.

Weather and climate in the coastal area are dominated by the cold, north-flowing Benguela Current of the Atlantic Ocean, which accounts for very low precipitation ( per year or less), frequent dense fog, and overall lower temperatures than in the rest of the country. In Winter, occasionally a condition known as ' (German for "mountain breeze") or ' (Afrikaans for "east weather") occurs, a hot dry wind blowing from the inland to the coast. As the area behind the coast is a desert, these winds can develop into sand storms, leaving sand deposits in the Atlantic Ocean that are visible on satellite images.

The Central Plateau and Kalahari areas have wide diurnal temperature ranges of up to 30 °C.

"Efundja", the annual seasonal flooding of the northern parts of the country, often causes not only damage to infrastructure but loss of life. The rains that cause these floods originate in Angola, flow into Namibia's Cuvelai basin, and fill the "oshanas" (Oshiwambo: flood plains) there. The worst floods occurred in March 2011 and displaced 21,000 people.

Namibia is the driest country in sub-Saharan Africa and depends largely on groundwater. With an average rainfall of about per annum, the highest rainfall occurs in the Caprivi in the northeast (about per annum) and decreases in a westerly and southwesterly direction to as little as and less per annum at the coast. The only perennial rivers are found on the national borders with South Africa, Angola, Zambia, and the short border with Botswana in the Caprivi. In the interior of the country, surface water is available only in the summer months when rivers are in flood after exceptional rainfalls. Otherwise, surface water is restricted to a few large storage dams retaining and damming up these seasonal floods and their runoff. Where people do not live near perennial rivers or make use of the storage dams, they are dependent on groundwater. Even isolated communities and those economic activities located far from good surface water sources, such as mining, agriculture, and tourism, can be supplied from groundwater over nearly 80% of the country.

More than 100,000 boreholes have been drilled in Namibia over the past century. One third of these boreholes have been drilled dry.

Namibia is one of few countries in the world to specifically address conservation and protection of natural resources in its constitution. Article 95 states, "The State shall actively promote and maintain the welfare of the people by adopting international policies aimed at the following: maintenance of ecosystems, essential ecological processes, and biological diversity of Namibia, and utilisation of living natural resources on a sustainable basis for the benefit of all Namibians, both present and future."

In 1993, the newly formed government of Namibia received funding from the United States Agency for International Development (USAID) through its Living in a Finite Environment (LIFE) Project. The Ministry of Environment and Tourism, with the financial support from organisations such as USAID, Endangered Wildlife Trust, WWF, and Canadian Ambassador's Fund, together form a Community Based Natural Resource Management (CBNRM) support structure. The main goal of this project is promote sustainable natural resource management by giving local communities rights to wildlife management and tourism.

Namibia is a unitary semi-presidential representative democratic republic. The President of Namibia is elected to a five-year term and is both the head of state and the head of government. However, while the President is both head of state and government, all members of the government are individually and collectively responsible to the legislature.

The Constitution of Namibia guarantees the separation of powers:

While the constitution envisaged a multi-party system for Namibia's government, the SWAPO party has been dominant since independence in 1990.

Namibia follows a largely independent foreign policy, with persisting affiliations with states that aided the independence struggle, including Cuba. With a small army and a fragile economy, the Namibian Government's principal foreign policy concern is developing strengthened ties within the Southern African region. A dynamic member of the Southern African Development Community, Namibia is a vocal advocate for greater regional integration. Namibia became the 160th member of the UN on 23 April 1990. On its independence it became the fiftieth member of the Commonwealth of Nations.

Namibia does not have any enemies in the region although it has been involved in various disputes regarding borders and construction plans. It consistently spends more as a percentage of GDP on its military than all of its neighbours, except Angola. Military expenditure rose from 2.7% of GDP in 2000 to 3.7% in 2009, and the arrival of 12 Chengdu J-7 Airguard jets in 2006 and 2008 made Namibia for a short time one of the top arms importers in Sub-Saharan Africa. By 2015, military expenditure was estimated at between 4% and 5% of GDP.

The constitution of Namibia defined the role of the military as ""defending the territory and national interests."" Namibia formed the Namibian Defence Force (NDF), comprising former enemies in a 23-year bush war: the People's Liberation Army of Namibia (PLAN) and South West African "Territorial Force" (SWATF). The British formulated the plan for integrating these forces and began training the NDF, which consists of a small headquarters and five battalions.

The United Nations Transitional Assistance Group (UNTAG)'s Kenyan infantry battalion remained in Namibia for three months after independence to help train the NDF and to stabilise the north. According to the Namibian Defence Ministry, enlistments of both men and women will number no more than 7,500. The current minister of the Namibian Military is Hon Penda YaNdakolo

Namibia is divided into 14 regions and subdivided into 121 constituencies. The administrative division of Namibia is tabled by "Delimitation Commissions" and accepted or declined by the National Assembly. Since state foundation four Delimitation Commissions have delivered their work, the last one in 2013 under the chairmanship of Judge Alfred Siboleka.

Regional councillors are directly elected through secret ballots (regional elections) by the inhabitants of their constituencies.

Local authorities in Namibia can be in the form of municipalities (either Part 1 or Part 2 municipalities), town councils or villages.

Namibia's economy is tied closely to South Africa's due to their shared history. The largest economic sectors are mining (10.4% of the gross domestic product in 2009), agriculture (5.0%), manufacturing (13.5%), and tourism.

Namibia has a highly developed banking sector with modern infrastructure, such as online banking and cellphone banking. The Bank of Namibia (BoN) is the central bank of Namibia responsible for performing all other functions ordinarily performed by a central bank. There are 5 BoN authorised commercial banks in Namibia: Bank Windhoek, First National Bank, Nedbank, Standard Bank and Small and Medium Enterprises Bank.

According to the Namibia Labour Force Survey Report 2012, conducted by the , the country's unemployment rate is 27.4%. "Strict unemployment" (people actively seeking a full-time job) stood at 20.2% in 2000, 21.9% in 2004 and spiraled to 29.4% in 2008. Under a broader definition (including people that have given up searching for employment) unemployment rose to 36.7% in 2004. This estimate considers people in the informal economy as employed. Labour and Social Welfare Minister Immanuel Ngatjizeko praised the 2008 study as "by far superior in scope and quality to any that has been available previously", but its methodology has also received criticism.

In 2004 a labour act was passed to protect people from job discrimination stemming from pregnancy and HIV/AIDS status. In early 2010 the Government tender board announced that "henceforth 100 per cent of all unskilled and semi-skilled labour must be sourced, without exception, from within Namibia".

In 2013, global business and financial news provider, Bloomberg, named Namibia the top emerging market economy in Africa and the 13th best in the world. Only four African countries made the Top 20 Emerging Markets list in the March 2013 issue of Bloomberg Markets magazine, and Namibia was rated ahead of Morocco (19th), South Africa (15th) and Zambia (14th). Worldwide, Namibia also fared better than Hungary, Brazil and Mexico. Bloomberg Markets magazine ranked the top 20 based on more than a dozen criteria. The data came from Bloomberg's own financial-market statistics, IMF forecasts and the World Bank. The countries were also rated on areas of particular interest to foreign investors: the ease of doing business, the perceived level of corruption and economic freedom. In order to attract foreign investment, the government has made improvement in reducing red tape resulted from excessive government regulations making the country one of the least bureaucratic places to do business in the region. However, facilitation payments are occasionally demanded by customs due to cumbersome and costly customs procedures. Namibia is also classified as an Upper Middle Income country by the World Bank, and ranks 87th out of 185 economies in terms of ease of doing business.

The cost of living in Namibia is relatively high because most of the goods including cereals need to be imported. Its capital city, Windhoek is currently ranked as the 150th most expensive place in the world for expatriates to live.

Taxation in Namibia includes personal income tax, which is applicable to total taxable income of an individual and all individuals are taxed at progressive marginal rates over a series of income brackets. The value added tax (VAT) is applicable to most of the commodities and services.
Despite the remote nature of much of the country, Namibia has seaports, airports, highways, and railways (narrow-gauge). The country seeks to become a regional transportation hub; it has an important seaport and several landlocked neighbours. The Central Plateau already serves as a transportation corridor from the more densely populated north to South Africa, the source of four-fifths of Namibia's imports.

According to recent statistics on the country's income share held by the highest 10%, Namibia is a country with a substantive income disparity. The data indicates that the current income share held by the highest 10% is approximately 51.8%. This parity that exists in the country illustrates the large gap between the rich and the poor. An additional figure states that the poverty gap that lives on $2 USD or less in the country is approximately 17.72%.

About half of the population depends on agriculture (largely subsistence agriculture) for its livelihood, but Namibia must still import some of its food. Although per capita GDP is five times the per capita GDP of Africa's poorest countries, the majority of Namibia's people live in rural areas and exist on a subsistence way of life. Namibia has one of the highest rates of income inequality in the world, due in part to the fact that there is an urban economy and a more rural cash-less economy. The inequality figures thus take into account people who do not actually rely on the formal economy for their survival. Although arable land accounts for only 1% of Namibia, nearly half of the population is employed in agriculture.

About 4,000, mostly white, commercial farmers own almost half of Namibia's arable land. The governments of Germany and Britain will finance Namibia's land reform process, as Namibia plans to start expropriating land from white farmers to resettle landless black Namibians.

Agreement has been reached on the privatisation of several more enterprises in coming years, with hopes that this will stimulate much needed foreign investment. However, reinvestment of environmentally derived capital has hobbled Namibian per capita income. One of the fastest growing areas of economic development in Namibia is the growth of wildlife conservancies. These conservancies are particularly important to the rural, generally unemployed, population.

An aquifer called "Ohangwena II" has been discovered, capable of supplying the 800,000 people in the North for 400 years. Experts estimate that Namibia has 7,720 km of underground water.

Providing 25% of Namibia's revenue, mining is the single most important contributor to the economy. Namibia is the fourth largest exporter of non-fuel minerals in Africa and the world's fourth largest producer of uranium. There has been significant investment in uranium mining and Namibia is set to become the largest exporter of uranium by 2015. Rich alluvial diamond deposits make Namibia a primary source for gem-quality diamonds. While Namibia is known predominantly for its gem diamond and uranium deposits, a number of other minerals are extracted industrially such as lead, tungsten, gold, tin, fluorspar, manganese, marble, copper and zinc. There are offshore gas deposits in the Atlantic Ocean that are planned to be extracted in the future. According to "The Diamond Investigation", a book about the global diamond market, from 1978, De Beers, the largest diamond company, bought most of the Namibian diamonds, and would continue to do so, because "whatever government eventually comes to power they will need this revenue to survive".

Domestic supply voltage is 220 V AC. Electricity is generated mainly by thermal and hydroelectric power plants. Non-conventional methods of electricity generation also play some role. Encouraged by the rich uranium deposits the Namibian government plans to erect its first nuclear power station by 2018, also uranium enrichment is envisaged to happen locally.

Tourism is a major contributor (14.5%) to Namibia's GDP, creating tens of thousands of jobs (18.2% of all employment) directly or indirectly and servicing over a million tourists per year. The country is a prime destination in Africa and is known for ecotourism which features Namibia's extensive wildlife.

There are many lodges and reserves to accommodate eco-tourists. Sport hunting is also a large, and growing component of the Namibian economy, accounting for 14% of total tourism in the year 2000, or $19.6 million US dollars, with Namibia boasting numerous species sought after by international sport hunters. In addition, extreme sports such as sandboarding, skydiving and 4x4ing have become popular, and many cities have companies that provide tours. The most visited places include the capital city of Windhoek, Caprivi Strip, Fish River Canyon, Sossusvlei, the Skeleton Coast Park, Sesriem, Etosha Pan and the coastal towns of Swakopmund, Walvis Bay and Lüderitz.

The capital city of Windhoek plays a very important role in Namibia's tourism due to its central location and close proximity to Hosea Kutako International Airport. According to The Namibia Tourism Exit Survey, which was produced by the Millennium Challenge Corporation for the Namibian Directorate of Tourism, 56% of all tourists visiting Namibia during the time period, 2012 – 2013, visited Windhoek. Many of Namibia's tourism related parastatals and governing bodies such as Namibia Wildlife Resorts, Air Namibia and the Namibia Tourism Board as well as Namibia's tourism related trade associations such as the Hospitality Association of Namibia are also all headquartered in Windhoek. There are also a number of notable hotels in Windhoek such as Windhoek Country Club Resort and some international hotel chains also operate in Windhoek, such as Avani Hotels and Resorts and Hilton Hotels and Resorts.

Namibia's primary tourism related governing body, the Namibia Tourism Board (NTB), was established by an Act of Parliament: the Namibia Tourism Board Act, 2000 (Act 21 of 2000). Its primary objectives are to regulate the tourism industry and to market Namibia as a tourist destination. There are also a number of trade associations that represent the tourism sector in Namibia, such as the Federation of Namibia Tourism Associations (the umbrella body for all tourism associations in Namibia), the Hospitality Association of Namibia, the Association of Namibian Travel Agents, Car Rental Association of Namibia and the Tour and Safari Association of Namibia.

Namibia is the only country in Sub-Saharan Africa to provide water through municipal departments. The only bulk water supplier in Namibia is NamWater, which sells it to the respective municipalities which in turn deliver it through their reticulation networks. In rural areas, the Directorate of Rural Water Supply in the Ministry of Agriculture, Water and Forestry is in charge of drinking water supply.

The UN evaluated in 2011 that Namibia has improved its water access network significantly since independence
in 1990. A large part of the population can not, however, make use of
these resources due to the prohibitively high consumption cost and the
long distance between residences and water points in rural areas. As a result, many Namibians prefer the traditional wells over the available water points far away.

Compared to the efforts made to improve access to safe water, Namibia is lagging behind in the provision of adequate sanitation. This includes 298 schools that have no toilet facilities. Over 50% of child deaths are related to lack of water, sanitation, or hygiene; 23% are due to diarrhea alone. The UN has identified a "sanitation crisis" in the country.

Apart from residences for upper and middle class households,
sanitation is insufficient in most residential areas. Private flush
toilets are too expensive for virtually all residents in townships due to their water consumption and installation cost. As a result, access to improved sanitation has not increased much since independence:
In Namibia's rural areas 13% of the population had more than basic
sanitation, up from 8% in 1990. Many of Namibia's inhabitants have to
resort to "flying toilets", plastic bags to defecate which after use are
flung into the bush. The use of open areas close to residential land to urinate and defecate is very common and has been identified as a major health hazard.

Namibia has the second-lowest population density of any sovereign country, after Mongolia. The majority of the Namibian population is of Bantu-speaking origin  – mostly of the Ovambo ethnicity, which forms about half of the population – residing mainly in the north of the country, although many are now resident in towns throughout Namibia. Other ethnic groups are the Herero and Himba people, who speak a similar language, and the Damara, who speak the same "click" language as the Nama.

In addition to the Bantu majority, there are large groups of Khoisan (such as Nama and San), who are descendants of the original inhabitants of Southern Africa. The country also contains some descendants of refugees from Angola. There are also two smaller groups of people with mixed racial origins, called "Coloureds" and "Basters", who together make up 8.0% (with the Coloureds outnumbering the Basters two to one). There is a substantial Chinese minority in Namibia; it stood at 40,000 in 2006.
Whites (mainly of Afrikaner, German, British and Portuguese origin) make up between 4.0 and 7.0% of the population. Although their percentage of population decreased after independence due to emigration and lower birth rates, they still form the second-largest population of European ancestry, both in terms of percentage and actual numbers, in Sub-Saharan Africa (after South Africa). The majority of Namibian whites and nearly all those who are mixed race speak Afrikaans and share similar origins, culture, and religion as the white and coloured populations of South Africa. A large minority of whites (around 30,000) trace their family origins back to the German settlers who colonized Namibia prior to the British confiscation of German lands after World War One, and they maintain German cultural and educational institutions. Nearly all Portuguese settlers came to the country from the former Portuguese colony of Angola. The 1960 census reported 526,004 persons in what was then South-West Africa, including 73,464 whites (14%).
Namibia conducts a census every ten years. After independence the first Population and Housing Census was carried out in 1991; further rounds followed in 2001 and 2011. The data collection method is to count every person resident in Namibia on the census reference night, wherever they happen to be. This is called the "de facto" method. For enumeration purposes the country is demarcated into 4,042 "enumeration areas". These areas do not overlap with constituency boundaries to get reliable data for election purposes as well.

The 2011 Population and Housing Census counted 2,113,077 inhabitants of Namibia. Between 2001 and 2011 the annual population growth was 1.4%, down from 2.6% in the previous ten–year period.

The Christian community makes up 80%–90% of the population of Namibia, with at least 75% being Protestant, and at least 50% Lutheran. Lutherans are the largest religious group – a legacy of the German and Finnish missionary work during the country's colonial times. 10%–20% of the population hold indigenous beliefs.

Missionary activities during the second half of the 19th century resulted in many Namibians converting to Christianity. Today most Christians are Lutheran, but there also are Roman Catholic, Methodist, Anglican, African Methodist Episcopal, Dutch Reformed and Mormons (The Church of Jesus Christ of Latter-day Saints).

Islam in Namibia is subscribed to by about 9,000 Muslims, many of whom are Nama. Namibia is home to a small Jewish community of about 100 members.

Up to 1990, English, German and Afrikaans were official languages. Long before Namibia's independence from South Africa, SWAPO was of the opinion that the country should become officially monolingual, choosing this approach in contrast to that of its neighbour South Africa (which granted all 11 of its major languages official status), which was seen by them as "a deliberate policy of ethnolinguistic fragmentation." Consequently, SWAPO instituted English as the sole official language of Namibia though only about 3% of the population speaks it as a home language. Its implementation is focused on the civil service, education and the broadcasting system. Some other languages have received semi-official recognition by being allowed as medium of instruction in primary schools. It is expected of private schools to follow the same policy as state schools, and "English language" is a compulsory subject. As in other postcolonial African societies, the push for monolingual instruction and policy has resulted in a high rate of school drop-outs and of individuals whose academic competence in any language is low.

According to the 2011 census, the most common languages are Oshiwambo (the most spoken language for 49% of households), Nama/Damara (11.3%), Afrikaans (10.4%), Kavango (9%), Otjiherero (9%). However, the most widely understood national language is Afrikaans, which is the lingua franca of the country. Both Afrikaans and English are used primarily as a second language reserved for public communication.

Most of the white population speaks either German or Afrikaans. Even today, 103 years after the end of the German colonial era, the German language plays a role as a commercial language. Afrikaans is spoken by 60% of the white community, German is spoken by 32%, English is spoken by 7% and Portuguese by 1%. Geographical proximity to Portuguese-speaking Angola explains the relatively high number of Portuguese speakers; in 2011 these were estimated to be 100,000, or 4–5% of the total population.

The most popular sport in Namibia is association football. The Namibia national football team qualified for the 2008 Africa Cup of Nations but has yet to qualify for any World Cups.

The most successful national team is the Namibian rugby team, having competed in five separate World Cups. Namibia were participants in the 1999, 2003, 2007, 2011 and 2015 Rugby World Cups. Cricket is also popular, with the national side having played in the 2003 Cricket World Cup. In December 2017, Namibia Cricket reached the final of the Cricket South Africa (CSA) Provincial One Day Challenge for the first time. In February 2018 Namibia hosted the ICC World Cricket League Division 2 with Namibia, Kenya, UAE, Nepal, Canada and Oman to compete for the final two ICC Cricket World Cup Qualifier positions in Zimbabwe.

The most famous athlete from Namibia is Frankie Fredericks, sprinter in the 100 and 200 m events. He won four Olympic silver medals (1992, 1996) and also has medals from several World Athletics Championships. Golfer Trevor Dodds won the Greater Greensboro Open in 1998, one of 15 tournaments in his career. He achieved a career high world ranking of 78th in 1998. Professional cyclist and Namibian Road Race champion Dan Craven represented Namibia at the 2016 Summer Olympics in both the road race and individual time trial. Boxer Julius Indongo is the unified WBA, IBF, and IBO world champion in the Light welterweight division.

Although Namibia's population is fairly small, the country has a diverse choice of media; two TV stations, 19 radio stations (without counting community stations), 5 daily newspapers, several weeklies and special publications compete for the attention of the audience. Additionally, a mentionable amount of foreign media, especially South African, is available. Online media are mostly based on print publication contents. Namibia has a state-owned Press Agency, called NAMPA.

The first newspaper in Namibia was the German-language "Windhoeker Anzeiger", founded 1898. Radio was introduced in 1969, TV in 1981. During German rule, the newspapers mainly reflected the living reality and the view of the white German-speaking minority. The black majority was ignored or depicted as a threat. During South African rule, the white bias continued, with mentionable influence of the Pretoria government on the "South West African" media system. Independent newspapers were seen as a menace to the existing order, critical journalists threatened.

The daily newspapers include the private publications "The Namibian" (English and other languages), "Die Republikein" (Afrikaans), Allgemeine Zeitung (German) and "Namibian Sun" (English) as well as the state-owned "New Era" (predominantly English). Except for the largest newspaper, "The Namibian", which is owned by a trust, the other mentioned private newspapers are part of the Democratic Media Holdings.

Other mentionable newspapers are the tabloid "Informanté" owned by "TrustCo", the weekly "Windhoek Observer", the weekly Namibia Economist, as well as the regional Namib Times. Current affairs magazines include "Insight Namibia", "Vision2030 Focus magazine" and "Prime FOCUS". "Sister Namibia" Magazine stands out as the longest running NGO magazine in Namibia, while "Namibia Sport" is the only national sport magazine. Furthermore, the print market is complemented with party publications, student newspapers and PR publications.

The broadcasting sector is dominated by the state-run Namibian Broadcasting Corporation (NBC). The public broadcaster offers a TV station as well as a "National Radio" in English and nine language services in locally spoken languages. The nine private radio stations in the country are mainly English-language channels, except for Radio Omulunga (Oshiwambo) and Kosmos 94.1 (Afrikaans).
Privately held One Africa TV has competed with NBC since the 2000s.

Compared to neighbouring countries, Namibia has a large degree of media freedom. Over the past years, the country usually ranked in the upper quarter of the Press Freedom Index of Reporters without Borders, reaching position 21 in 2010, being on par with Canada and the best-positioned African country. The African Media Barometer shows similarly positive results. However, as in other countries, there is still mentionable influence of representatives of state and economy on media in Namibia. In 2009, Namibia dropped to position 36 on the Press Freedom Index. In 2013, it was 19th. In 2014 it ranked 22nd 

Media and journalists in Namibia are represented by the Namibian chapter of the Media Institute of Southern Africa and the Editors' Forum of Namibia. An independent media ombudsman was appointed in 2009 to prevent a state-controlled media council.

Namibia has free education for both primary and secondary education levels. Grades 1–7 are primary level, grades 8–12 are secondary. In 1998, there were 400,325 Namibian students in primary school and 115,237 students in secondary schools. The pupil-teacher ratio in 1999 was estimated at 32:1, with about 8% of the GDP being spent on education. Curriculum development, educational research, and professional development of teachers is centrally organised by the National Institute for Educational Development (NIED) in Okahandja.

Most schools in Namibia are state-run, but there are some private schools, which are also part of the country's education system. There are four teacher training universities, three colleges of agriculture, a police training college, and two universities: University of Namibia (UNAM) and Namibia University of Science and Technology (NUST).

Life expectancy at birth is estimated to be 64 years in 2017 – among the lowest in the world.

Namibia launched a National Health Extension Programme in 2012 deployment 1,800 (2015) of a total ceiling of 4,800 health extension workers trained for six months in community health activities including first aid, health promotion for disease prevention, nutritional assessment and counseling, water sanitation and hygiene practices, HIV testing and community-based antiretroviral treatment.

Namibia faces non-communicable disease burden. The Demographic and Health Survey (2013) summarizes findings on elevated blood pressure, hypertension, diabetes and obesity:
The HIV epidemic remains a public health issue in Namibia despite significant achievements made by the Ministry of Health and Social Services to expand HIV treatment services. In 2001, there were an estimated 210,000 people living with HIV/AIDS, and the estimated death toll in 2003 was 16,000. According to the 2011 UNAIDS Report, the epidemic in Namibia "appears to be leveling off." As the HIV/AIDS epidemic has reduced the working-aged population, the number of orphans has increased. It falls to the government to provide education, food, shelter and clothing for these orphans. A Demographic and Health Survey with an HIV biomarker was completed in 2013 and served as the fourth comprehensive, national-level population and health survey conducted in Namibia as part of the global Demographic and Health Surveys (DHS) programme. The DHS observed important characteristics associated to the HIV epidemic:
As of 2015, the Ministry of Health and Social Services and UNAIDS produced a Progress Report in which UNAIDS projected HIV prevalence among 15 – 49 year olds at 13.3% [12.2% – 14.5%] and an estimated 210 000 [200 000 – 230 000] living with HIV.

The malaria problem seems to be compounded by the AIDS epidemic. Research has shown that in Namibia the risk of contracting malaria is 14.5% greater if a person is also infected with HIV. The risk of death from malaria is also raised by approximately 50% with a concurrent HIV infection. The country had only 598 physicians in 2002.




</doc>
<doc id="21294" url="https://en.wikipedia.org/wiki?curid=21294" title="Geography of Namibia">
Geography of Namibia

At , Namibia is the world's thirty-fourth largest country (after Venezuela). After Mongolia, Namibia is the second least densely populated country in the world ().

The Namibian landscape consists generally of five geographical areas, each with characteristic abiotic conditions and vegetation with some variation within and overlap between them: the Central Plateau, the Namib Desert, the Great Escarpment, the Bushveld, and the Kalahari Desert.

The Central Plateau runs from north to south, bordered by the Skeleton Coast to the northwest, the Namib Desert and its coastal plains to the southwest, the Orange River to the south, and the Kalahari Desert to the east. The Central Plateau is home to the highest point in Namibia at Königstein elevation . Within the wide, flat Central Plateau is the majority of Namibia’s population and economic activity. Windhoek, the nation’s capital, is located here, as well as most of the arable land. Although arable land accounts for only 1% of Namibia, nearly half of the population is employed in agriculture.

The abiotic conditions here are similar to those found along the Escarpment; however, the topographic complexity is reduced. Summer temperatures in the area can reach , and frosts are common in the winter.

The Namib Desert is a broad expanse of hyper-arid gravel, gravel with no moisture, plains and dunes that stretches along the entire coastline, which varies in width between 100 and many hundreds of kilometres. Areas within the Namib include the Skeleton Coast and the Kaokoveld in the north and the extensive Namib Sand Sea along the central coast. The sands that make up the sand sea are a consequence of erosional processes that take place within the Orange River valley and areas further to the south. As sand-laden waters drop their suspended loads into the Atlantic, onshore currents deposit them along the shore. The prevailing south west winds then pick up and redeposit the sand in the form of massive dunes in the widespread sand sea, the largest sand dunes in the world. In areas where the supply of sand is reduced because of the inability of the sand to cross riverbeds, the winds also scour the land to form large gravel plains. In many areas within the Namib Desert, there is little vegetation with the exception of lichens found in the gravel plains, and in dry river beds where plants can access subterranean water.

The Great Escarpment swiftly rises to over . Average temperatures and temperature ranges increase as you move further inland from the cold Atlantic waters, while the lingering coastal fogs slowly diminish. Although the area is rocky with poorly developed soils, it is nonetheless significantly more productive than the Namib Desert. As summer winds are forced over the Escarpment, moisture is extracted as precipitation. The water, along with rapidly changing topography, is responsible for the creation of microhabitats which offer a wide range of organisms, many of them endemic. Vegetation along the escarpment varies in both form and density, with community structure ranging from dense woodlands to more shrubby areas with scattered trees. A number of Acacia species are found here, as well as grasses and other shrub vegetation.

The Bushveld is found in north eastern Namibia along the Angolan border and in the Caprivi Strip which is the vestige of a narrow corridor demarcated for the German Empire to access the Zambezi River. The area receives a significantly greater amount of precipitation than the rest of the country, averaging around per year. Temperatures are also cooler and more moderate, with approximate seasonal variations of between . The area is generally flat and the soils sandy, limiting their ability to retain water. Located adjacent to the Bushveld in north-central Namibia is one of nature’s most spectacular features: the Etosha pan. For most of the year it is a dry, saline wasteland, but during the wet season, it forms a shallow lake covering more than . The area is ecologically important and vital to the huge numbers of birds and animals from the surrounding savannah that gather in the region as summer drought forces them to the scattered waterholes that ring the pan. The Bushveld area has been demarcated by the World Wildlife Fund as part of the Angolan Mopane woodlands ecoregion, which extends north across the Cunene River into neighbouring Angola.

The Kalahari Desert is perhaps Namibia’s best known geographical feature. Shared with South Africa and Botswana, it has a variety of localised environments ranging from hyper-arid sandy desert, to areas that seem to defy the common definition of desert. One of these areas, known as the Succulent Karoo, is home to over 5,000 species of plants, nearly half of them endemic; fully one-third of the world’s succulents are found in the Karoo.

The reason behind this high productivity and endemism may be the relatively stable nature of precipitation. The Karoo apparently does not experience drought on a regular basis, so even though the area is technically desert, regular winter rains provide enough moisture to support the region’s interesting plant community. Another feature of the Kalahari, indeed many parts of Namibia, are inselbergs, isolated mountains that create microclimates and habitat for organisms not adapted to life in the surrounding desert matrix.

Namibia's Coastal Desert is one of the oldest deserts in the world. Its sand dunes, created by the strong onshore winds, are the highest in the world.

The Namib Desert and the Namib-Naukluft National Park is located here. The Namibian coastal deserts are the richest source of diamonds on earth, making Namibia the world's largest producer of diamonds. It is divided into the northern Skeleton Coast and the southern Diamond Coast. Because of the location of the shoreline—at the point where the Atlantic's cold water reaches Africa—there is often extremely dense fog.

Sandy beach comprises 54% and mixed sand and rock add another 28%. Only 16% of the total length is rocky shoreline. The coastal plains are dune fields, gravel plains covered with lichen and some scattered salt pans. Near the coast there are areas where the dunes are vegetated with hammocks. Namibia has rich coastal and marine resources that remain largely unexplored.

Namibia has more than 300 days of sunshine per year. It is situated at the southern edge of the tropics; the Tropic of Capricorn cuts the country about in half. The winter (June–August) is generally dry, both rainy seasons occur in summer, the small rainy season between September and November, the big one between February and April. Humidity is low, and average rainfall varies from almost zero in the coastal desert to more than 600 mm in the Caprivi Strip. Rainfall is however highly variable, and droughts are common. The bad rainy season occurred in summer 2006/07. Due to the dry winters snowfall has a very rare occurrence and prompts media coverage whenever it happens. The snow was reported at Spreetshoogte Pass in the Namib-Naukluft Park in June 2011.

Weather and climate in the coastal area are dominated by the cold, north-flowing Benguela Current of the Atlantic Ocean which accounts for very low precipitation (50 mm per year or less), frequent dense fog, and overall lower temperatures than in the rest of the country. In winter, occasionally a condition known as "Berg wind" or "Oosweer" (Afrikaans: East weather) occurs, a hot dry wind blowing from the inland to the coast. As the area behind the coast is a desert, these winds can develop into sand storms with sand deposits in the Atlantic Ocean visible on satellite images.

The Central Plateau and Kalahari areas have wide diurnal temperature ranges of up to 30C.

"Efundja", the annual flooding of the northern parts of the country, often causes not only damage to infrastructure but loss of life. The rains that cause these floods originate in Angola, flow into Namibia's Cuvelai basin, and fill the "Oshanas" (Oshiwambo: flood plains) there. The worst floods occurred in March 2011 and displaced 21,000 people.

The capital and largest city, Windhoek, is in the centre of the country. It is home to the country's Central Administrative Region, Windhoek Hosea Kutako International Airport and the country's railhead. Other important towns are:


Location:
Southern Africa, bordering the South Atlantic Ocean, between Angola and South Africa

Geographic coordinates:
Map references:
Africa

Area:
<br>"total:"
824,292 km²
<br>"land:"
823,290 km²
<br>"water:"
1,002 km²

Land boundaries:
<br>"total:"
3,936 km
<br>"border countries:"
Angola 1,376 km, Botswana 1,360 km, South Africa 967 km, Zambia 233 km

Coastline:
1 572 km

Maritime claims:
<br>"territorial sea:"

<br>"contiguous zone:"

<br>"exclusive economic zone:"
Terrain:
Mostly high plateau; Namib Desert along coast; Kalahari Desert in east. In the north near the border with Angola there is a flat area that has been designated by the World Wildlife Fund as part of the Angolan Mopane woodlands ecoregion.

Elevation extremes:
<br>"lowest point:"
Atlantic Ocean 0 m
<br>"highest point:"
Königstein 2,606 m

Natural resources:
diamonds, copper, uranium, gold, lead, tin, lithium, cadmium, zinc, salt, hydropower, fish
<br>"note:"
suspected deposits of oil, coal, and iron ore

Land use:
<br>"arable land:"
0.87% (2011), 0.99% (1998 est.) 1% (1993 est.)
<br>"permanent crops:"
0.01% (2011), 0% (1998, 1993 est.)
<br>"other:"
99.02% (2011), 99.01% (1998 est.)

Irrigated land:
75.73 hm (2003), 70 km² (1998 est.), 60 km² (1993 est.)

Total renewable water resources:
17.72 km (2011)

Natural hazards:
prolonged periods of drought

Environment - current issues:
limited natural freshwater resources; desertification; wildlife poaching; land degradation has led to few conservation areas

Environment - international agreements:
<br>"party to:"
Antarctic-Marine Living Resources, Biodiversity, Climate Change, Climate Change-Kyoto Protocol, Desertification, Endangered Species, Hazardous Wastes, Law of the Sea, Ozone Layer Protection, Wetlands
<br>"signed, but not ratified:"
none of the selected agreements

This is a list of the extreme points of Namibia, the points that are farther north, south, east or west than any other location.




</doc>
<doc id="21295" url="https://en.wikipedia.org/wiki?curid=21295" title="Demographics of Namibia">
Demographics of Namibia

This article is about the demographic features of the population of Namibia, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.

As required by the Namibian "Statistics Act" #66 of 1976, and in accordance with United Nations recommendations, a census is conducted every ten years. After Namibian independence the first Population and Housing Census was carried out in 1991, further rounds followed in 2001 and 2011. The data collection method is to count every person resident in Namibia on the census' reference night, wherever they happen to be. This is called the "de facto" method. For enumeration purposes the country is demarcated into 4,042 "enumeration areas". These areas do not overlap with constituency boundaries in order to get reliable data for election purposes as well.

The 2011 Population and Housing Census counted 2,113,077 inhabitants of Namibia. Between 2001 and 2011 the annual population growth was 1.4%, down from 2.6% in the previous ten–year period.

In 2011 the total fertility rate was 3.6 children per woman, down from 4.1 in 2001.

According to the total population was in , compared to only 485 000 in 1950. The proportion of children below the age of 15 in 2010 was 36.4%, 59.9% was between 15 and 65 years of age, while 3.7% was 65 years or older

Registration of vital events in Namibia is not complete. The Population Departement of the United Nations prepared the following estimates.

Total Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):

Fertility data as of 2013 (DHS Program):

Namibians are of diverse ethnic origins. The principal groups are the Ovambo, Kavango, Herero/Himba, Damara, mixed race (Coloured and Rehoboth Baster), White Namibians (Afrikaner, German, British and Portuguese), Nama, Caprivian (Lozi), San, and Tswana. The Coloureds and Basters share similar genealogical origins and cultural attributes (such as home language) but nonetheless maintain distinctly separate communal identities, as do most white Namibians and black Namibians, respectively.

The Ovambo make up about half of Namibia's people. The Ovambo, Kavango, and East Caprivian peoples, who occupy the relatively well-watered and wooded northern part of the country, are settled farmers and herders. Historically, they have shown little interest in the central and southern parts of Namibia, where conditions do not suit their traditional way of life.

Until the early 20th century, these tribes had little contact with the Nama, Damara, and Herero, who settled around the central part of the country vying for control of sparse pastureland. German colonial rule destroyed the warmaking ability of the tribes but did not erase their identities or traditional organization. People from the more populous north have settled throughout the country in recent decades as a result of urbanization, industrialization, and the demand for labor.

The minority white population is primarily of South African, British, and German descent, with a few Portuguese. About 60% of the whites speak Afrikaans (a language derived from the 17th century Dutch), 32% speak German, and 7% speak English.


Missionary work during the 19th century drew many Namibians to Christianity, especially Lutheranism. While most Namibian Christians are Lutheran, there also are Roman Catholic, Methodist, Anglican, African Methodist Episcopal, and Dutch Reformed Christians represented.


Modern education and medical care have been extended in varying degrees to most rural areas in recent years. The literacy rate of Africans is generally low except in sections where missionary and government education efforts have been concentrated, such as Ovamboland. The Africans speak various indigenous languages.

The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.

"at birth:"
1.03 male(s)/female (2003, 2000 est.)
<br>"under 15 years:"
1.03 male(s)/female (2003, 2000 est.)
<br>"15–64 years:"
0.99 male(s)/female (2003, 2000 est.)
<br>"65 years and over:"
0.78 male(s)/female (2003 est.), 0.76 male(s)/female (2000 est.)
<br>"total population:"
1 male(s)/female (2003 est.), 0.99 male(s)/female (2000 est.)


adult prevalence rate: 13.1% (2009 est.)

people living with HIV/AIDS: 180,000 (2009 est.)

deaths: 6,700 (2009 est.)

"noun:"
Namibian(s)
<br>"adjective:"
Namibian

"definition:"
age 15 and over can read and write
Also see Education in Namibia



</doc>
<doc id="21296" url="https://en.wikipedia.org/wiki?curid=21296" title="Politics of Namibia">
Politics of Namibia

Politics of Namibia takes place in a framework of a semi-presidential representative democratic republic, whereby the President of Namibia is both head of state and head of government, and of a pluriform multi-party system. Executive power is exercised by both the President and the Government. Legislative power is vested in both the Government and the two chambers of Parliament. The judiciary is independent of the executive and the legislature.

Additional to the government political structure Namibia has a network of traditional leadership with currently 51 recognised traditional authorities and their leaders. These authorities cover the entire Namibian territory. Traditional leaders are entrusted with the allocation of communal land and the formulation of the traditional group's customary laws. They also take over minor judicial work.

The Economist Intelligence Unit has rated Namibia as "flawed democracy" in 2016.

The Constituent Assembly of Namibia produced a constitution which established a multi-party system and a bill of rights. It also limited the executive president to two five-year terms and provided for the private ownership of property. The three branches of government are subject to checks and balances, and a provision is made for judicial review. The constitution also states that Namibia should have a mixed economy, and foreign investment should be encouraged.

The constitution is noted for being one of the first to incorporate protection of the environment into its text. Namibia is a democratic but one party dominant state with the South-West Africa People's Organisation in power. Opposition parties are allowed, but are widely considered to have no real chance of gaining power.

While the ethnic-based, three-tier, South African-imposed governing authorities have been dissolved, the current government pledged for the sake of national reconciliation to retain civil servants employed during the colonial period. The government is still organising itself on both national and regional levels.

The Constituent Assembly converted itself into the National Assembly on 16 February 1990, retaining all the members elected on a straight party ticket.

The Namibian head of state is the president, elected by popular vote every five years. Namibia's founding president is Sam Nujoma, who was in office for three terms from 21 March 1990 (Namibia's Independence Day) until 21 March 2005. Hifikepunye Pohamba was Namibia's second president serving from 2005 to 2015. Since 2015 Hage Geingob has been President of Namibia.

While the separation of powers is enshrined in the country's constitution, Namibia's civil society and the opposition repeatedly have criticised the overlap between executive and legislature. All cabinet members also sit in the National Assembly and dominate that body—not numerically but by being the superiors to ordinary members.

The government is headed by the prime minister, who, together with his cabinet, is appointed by the president. SWAPO, the primary force behind independence, is still the country's largest party. Hage Geingob was Namibia's first Prime Minister. He was appointed on 21 March 1990 and served until 28 August 2002. Theo-Ben Gurirab was Prime Minister from 28 August 2002 to 21 March 2005, and Nahas Angula occupied this position from 21 March 2005 to 4 December 2012. Since 21 March 2015, Hage Geingob is President of Namibia.

Parliament has two chambers. The National Assembly has 78 members, elected for a five-year term, 72 members elected by proportional representation and six members appointed by the president. The National Council has 26 representatives of the Regional Councils as members, elected for a six-year term. Every Regional Council in the 13 regions of Namibia elects two representatives to serve on this body. The Assembly is the primary legislative body, with the Council playing more of an advisory role.

The highest judicial body is the Supreme Court, whose judges are appointed by the president on the recommendation of the Judicial Service Commission. The judicial structure in Namibia parallels that of South Africa. In 1919, Roman-Dutch law was declared the common law of the territory and remains so to the present.

Elections were held in 1992, to elect members of 13 newly established Regional Councils, as well as new municipal officials. Two members from each Regional Council serve simultaneously as members of the National Council, the country's second house of Parliament. Nineteen of its members are from the ruling SWAPO party, and seven are from the Democratic Turnhalle Alliance (DTA). In December 1994, elections were held for the President and the National Assembly.

Namibia has about 40 political groups, ranging from modern political parties to traditional groups based on tribal authority. Some represent single tribes or ethnic groups while others encompass several. Most participate in political alliances, some of which are multiracial, with frequently shifting membership.

SWAPO is the ruling party, and all but one of the new government's first cabinet posts went to SWAPO members. A marxist-oriented movement, SWAPO has become more right wing and now espouses the need for a mixed economy. SWAPO has been a legal political party since its formation and was cautiously active in Namibia, although before implementation of the UN Plan, it was forbidden to hold meetings of more than 20 people, and its leadership was subject to frequent detention. In December 1976, the UN General Assembly recognised SWAPO as "the sole and authentic representative of the Namibian people," a characterisation other internal parties did not accept.

In the 1999 presidential and parliamentary elections, SWAPO continued its history of political dominance, taking 55 of the 72 Assembly seats, and returning President Sam Nujoma to the office for his third term. The principal opposition parties are the Congress of Democrats (CoD) and the Democratic Turnhalle Alliance (DTA), with each possessing seven seats in the National Assembly.

Namibian government has so far recognised 51 traditional authorities, and a further 40 applications are pending. These institutions are based on ethnicity and headed by the traditional leader of that ethnic group or clan. These positions are not paid by the state. Instead the traditional group's members are expected to sustain their leadership. Government did, however, give one car each to the recognised authorities, and awards allowances for fuel and administrative work. The parallel existence of traditional authorities and the Namibian government in Namibia is controversial.

Namibia is divided into 14 regions: Zambezi, Erongo, Hardap, !Karas, Kavango East, Kavango West, Khomas, Kunene, Ohangwena, Omaheke, Omusati, Oshana, Oshikoto, and Otjozondjupa.

Namibia is member of
ACP,
AfDB,
C,
ECA,
FAO,
G-77,
IAEA,
IBRD,
ICAO,
ICCt,
ICFTU,
ICRM,
IFAD,
IFC,
IFRCS,
ILO,
IMF,
IMO,
Interpol,
IOC,
IOM (observer),
ISO (correspondent),
ITU,
NAM,
OAU,
OPCW,
SACU,
SADC,
UN,
UNCTAD,
UNESCO,
UNHCR,
UNIDO,
UNMEE,
UPU,
WCL,
WHO,
WIPO,
WMO,
WToO,
WTrO


</doc>
<doc id="21297" url="https://en.wikipedia.org/wiki?curid=21297" title="Economy of Namibia">
Economy of Namibia

The Namibian economy has a modern market sector, which produces most of the country's wealth, and a traditional subsistence sector. Although the majority of the population engages in subsistence agriculture and herding, Namibia has more than 200,000 skilled workers and a considerable number of well-trained professionals and managerials.

Namibia is a higher middle income country with an estimated annual GDP per capita of US$5,828 but has extreme inequalities in income distribution and standard of living. It leads the list of countries by income inequality with a Gini coefficient of 59.7 (CIA) and 74.3 (UN), respectively.

Since independence, the Namibian Government has pursued free-market economic principles designed to promote commercial development and job creation to bring disadvantaged Namibians into the economic mainstream. To facilitate this goal, the government has actively courted donor assistance and foreign investment. The liberal Foreign Investment Act of 1990 provides guarantees against nationalisation, freedom to remit capital and profits, currency convertibility, and a process for settling disputes equitably. Namibia also is addressing the sensitive issue of agrarian land reform in a pragmatic manner. However, Government runs and owns a number of companies such as Air Namibia, Transnamib and NamPost, most of which need frequent financial assistance to stay afloat.

The country's sophisticated formal economy is based on capital-intensive industry and farming. However, Namibia's economy is heavily dependent on the earnings generated from primary commodity exports in a few vital sectors, including minerals, especially diamonds, livestock, and fish. Furthermore, the Namibian economy remains integrated with the economy of South Africa, as the bulk of Namibia's imports originate there.

In 1993, Namibia became a signatory of the General Agreement on Tariffs and Trade (GATT) signatory, and the Minister of Trade and Industry represented Namibia at the Marrakech signing of the Uruguay Round Agreement in April 1994. Namibia also is a member of the International Monetary Fund and the World Bank, and has acceded to the European Union's Lomé Convention.

Given its small domestic market but favourable location and a superb transport and communications base, Namibia is a leading advocate of regional economic integration. In addition to its membership in the Southern African Development Community (SADC), Namibia presently belongs to the Southern African Customs Union (SACU) with South Africa, Botswana, Lesotho, and Swaziland. Within SACU, there is no customs on goods produced in, and being transported amidst, its members. Namibia is a net receiver of SACU revenues; they are estimated to contribute 13.9 billion NAD in 2012.

The Namibian economy is closely linked to South Africa with the Namibian dollar pegged to the South African rand. Privatisation of several enterprises in coming years may stimulate long-run foreign investment, although with the trade union movement opposed, so far most politicians have been reluctant to advance the issue. In September 1993, Namibia introduced its own currency, the Namibia Dollar (N$), which is linked to the South African Rand at a fixed exchange rate of 1:1. There has been widespread acceptance of the Namibia Dollar throughout the country and, while Namibia remains a part of the Common Monetary Area, it now enjoys slightly more flexibility in monetary policy although interest rates have so far always moved very closely in line with the South African rates.

Namibia imports almost all of its goods from South Africa. Many exports likewise go to the South African market, or transit that country. Namibia's exports consist mainly of diamonds and other minerals, fish products, beef and meat products, karakul sheep pelts, and light manufactures. In recent years, Namibia has accounted for about 5% of total SACU exports, and a slightly higher percentage of imports.

Namibia is seeking to diversify its trading relationships away from its heavy dependence on South African goods and services. Europe has become a leading market for Namibian fish and meat, while mining concerns in Namibia have purchased heavy equipment and machinery from Germany, the United Kingdom, the United States, and Canada. The Government of Namibia is making efforts to take advantage of the American-led African Growth and Opportunity Act (AGOA), which will provide preferential access to American markets for a long list of products. In the short term, Namibia is likely to see growth in the apparel manufacturing industry as a result of AGOA.

Namibia is heavily dependent on the extraction and processing of minerals for export. Taxes and royalties from mining account for 25% of its revenue. The bulk of the revenue is created by diamond mining, which made up 7.2% of the 9.5% that mining contributes to Namibia's GDP in 2011. Rich alluvial diamond deposits make Namibia a primary source for gem-quality diamonds. Namibia is a large exporter of uranium and over the years the mining industry has seen a decline in the international commodity prices such as uranium, which has led to the reason behind several uranium projects being abandoned. Experts say that the prices are expected to rise in the next 3 years because of an increase in nuclear activities from both Japan and China. The mining industry in Namibia is supposedly going to reach US1.79bn by the year 2018.

Diamond production totalled 1.5 million carats (300 kg) in 2000, generating nearly $500 million in export earnings. Other important mineral resources are uranium, copper, lead, and zinc. The country also extracts gold, silver, tin, vanadium, semiprecious gemstones, tantalite, phosphate, sulphur, and mines salt.

Namibia is the fourth-largest exporter of nonfuel minerals in Africa, the world's fifth-largest producer of uranium, and the producer of large quantities of lead, zinc, tin, silver, and tungsten. Namibia has two uranium mines that are capable of providing 10% of the world mining output. The mining sector employs only about 3% of the population while about half of the population depends on subsistence agriculture for its livelihood. Namibia normally imports about 50% of its cereal requirements; in drought years food shortages are a major problem in rural areas.

During the pre-independence period, large areas of Namibia, including off-shore, were leased for oil prospecting. Some natural gas was discovered in 1974 in the Kudu Field off the mouth of the Orange River, but the extent of this find is only now being determined.

About half of the population depends on agriculture (largely subsistence agriculture) for its livelihood, but Namibia must still import some of its food. Although per capita GDP is five times the per capita GDP of Africa's poorest countries, the majority of Namibia's people live in rural areas and exist on a subsistence way of life. Namibia has one of the highest rates of income inequality in the world, due in part to the fact that there is an urban economy and a more rural cash-less economy. The inequality figures thus take into account people who do not actually rely on the formal economy for their survival. Although arable land accounts for only 1% of Namibia, nearly half of the population is employed in agriculture.

About 4,000, mostly white, commercial farmers own almost half of Namibia's arable land. The governments of Germany and Britain will finance Namibia's land reform process, as Namibia plans to start expropriating land from white farmers to resettle landless black Namibians.

Agreement has been reached on the privatisation of several more enterprises in coming years, with hopes that this will stimulate much needed foreign investment. However, reinvestment of environmentally derived capital has hobbled Namibian per capita income. One of the fastest growing areas of economic development in Namibia is the growth of wildlife conservancies. These conservancies are particularly important to the rural generally unemployed population.

An aquifer called "Ohangwena II" has been discovered, capable of supplying the 800,000 people in the North for 400 years. Experts estimate that Namibia has 7720 km of underground water.

The clean, cold South Atlantic waters off the coast of Namibia are home to some of the richest fishing grounds in the world, with the "potential" for sustainable yields of 1.5 million metric tonnes per year. Commercial fishing and fish processing is the fastest-growing sector of the Namibian economy in terms of employment, export earnings, and contribution to GDP.

The main species found in abundance off Namibia are pilchards (sardines), anchovy, hake, and horse mackerel. There also are smaller but significant quantities of sole, squid, deep-sea crab, rock lobster, and tuna.

At the time of independence, fish stocks had fallen to dangerously low levels, due to the lack of protection and conservation of the fisheries and the over-exploitation of these resources. This trend appears to have been halted and reversed since independence, as the Namibian Government is now pursuing a conservative resource management policy along with an aggressive fisheries enforcement campaign. The government seeks to develop fish-farming as an alternative.

In 2000, Namibia's manufacturing sector contributed about 20% of GDP. Namibian manufacturing is inhibited by a small domestic market, dependence on imported goods, limited supply of local capital, widely dispersed population, small skilled labour force and high relative wage rates, and subsidised competition from South Africa.

Walvis Bay is a well-developed, deepwater port, and Namibia's fishing infrastructure is most heavily concentrated there. The Namibian Government expects Walvis Bay to become an important commercial gateway to the Southern African region.

Namibia also boasts world-class civil aviation facilities and an extensive, well-maintained land transportation network. Construction is underway on two new arteries—the Trans-Caprivi Highway and Trans-Kalahari Highway—which will open up the region's access to Walvis Bay.

The Walvis Bay Export Processing Zone operates in the key port of Walvis Bay.

Tourism is a major contributor (14.5%) to Namibia's GDP, creating tens of thousands of jobs (18.2% of all employment) directly or indirectly and servicing over a million tourists per annum. The country is among the prime destinations in Africa and is known for ecotourism which features Namibia's extensive wildlife.

There are many lodges and reserves to accommodate eco-tourists. Sport Hunting is also a large, and growing component of the Namibian economy, accounting for 14% of total tourism in the year 2000, or $19.6 million US dollars, with Namibia boasting numerous species sought after by international sport hunters. In addition, extreme sports such as sandboarding, skydiving and 4x4ing have become popular, and many cities have companies that provide tours. The most visited places include the Caprivi Strip, Fish River Canyon, Sossusvlei, the Skeleton Coast Park, Sesriem, Etosha Pan and the coastal towns of Swakopmund, Walvis Bay and Lüderitz.

While many Namibians are economically active in one form or another, the bulk of this activity is in the informal sector, primarily subsistence agriculture. A large number of Namibians seeking jobs in the formal sector are held back due to a lack of necessary skills or training. The government is aggressively pursuing education reform to overcome this problem.

Namibia has a high unemployment rate. "Strict unemployment" (people actively seeking a full-time job) stood at 20.2% in 1999, 21.9% in 2002, and spiraled to 29,4 per cent in 2008. A 2012 study by the Namibia Statictics Agency (NSA) determined the rate of unemployment to be 27.4%. This study included subsistence farmers, work without pay, and any non-zero amount of weekly working hours, and did not count as unemployed people not actively seeking for a job.

Under a much broader definition (including people that have given up searching for employment) two different studies determined the unemployment rate to be 36.7% (2004) and 51.2% (2008), respectively. This estimate considers people in the informal economy as employed. 72% of jobless people have been unemployed for two years or more. Labour and Social Welfare Minister Immanuel Ngatjizeko praised the 2008 study as "by far superior in scope and quality to any that has been available previously", but its methodology has also received criticism. The total number of formally employed people stood at 400,000 in 1997 and fell 330,000 in 2008, according to a government survey. The NSA 2012 study counted 396,000 formal employees. Of annually 25,000 school leavers only 8,000 gain formal employment—largely a result of a failed education system.

Namibians in the informal sector as well as in low-paid jobs like homemakers, gardeners or factory workers are unlikely to be covered by medical aid or a pension fund. All in all only a quarter or the working population have medical aid, and about half have a pension fund.

Namibia's largest trade union federation, the National Union of Namibian Workers (NUNW) represents workers organised into seven affiliated trade unions. NUNW maintains a close affiliation with the ruling SWAPO party.

In the financial year March 2009 – February 2010, every Namibian earned 15,000 N$ (roughly 2,000 US$) on average. Household income was dominated by wages (49.1%) and subsistence farming (23%), with further significant sources of income being business activities (8.1%, farming excluded), old-age pensions from government (9.9%), and cash remittance (2.9%). Commercial farming only contributed 0.6%.

Every Namibian resident had on average 10,800 US$ of wealth accumulated in 2016, putting Namibia on third place in Africa. Individual wealth is, however, distributed very unequally; the country's Gini coefficient of 0.61 is one of the highest in the world. There are 3,300 US$-millionaires in Namibia, 1,400 of which live in the capital Windhoek.





</doc>
<doc id="21298" url="https://en.wikipedia.org/wiki?curid=21298" title="Telecommunications in Namibia">
Telecommunications in Namibia

Telecommunications in Namibia include radio, television, fixed and mobile telephones, and the Internet.


Since Independence in 1990, Namibia has seen a dramatic growth in radio stations, with both commercial (for instance Radio 99, Radio Wave, Radio Energy, Omulunga Radio, West Coast FM, etc.) and community (UNAM Radio, Katutura Community Radio, etc.) receiving licences. Most of these stations broadcast various types of music format, and political discussions, news and phone-in programs remain mostly the domain of the national broadcaster (NBC) which broadcasts nine radio services nationally (in various Namibian languages, including German - the only full-time German service outside of Europe), plus the new !Ha service, broadcasting to the San community in Tsumkwe.


The television network with the widest transmission range is the Namibian Broadcasting Corporation (NBC, not to be confused with the American NBC network). The NBC is the successor to the South Africa–run South West African Broadcasting Corporation (SWABC), which was modeled on the original SABC. Like the radio services of the NBC, the television service tries to cater to all the linguistic audiences in Namibia, although the dominant language is English (Namibia's official language).

The commercial "free to air" station is One Africa Television, the successor to the now defunct TV Africa. It has expanded its transmitter network and is now available in most major towns and cities in Namibia. In 2007 it commenced broadcasting a local television news bulletin each evening.

The Trinity Broadcasting Network (TBN) is a religion television station, with some material originating locally, although also carrying relays from the United States. It is based in Windhoek and holds a community television licence granted in 2001.



Telecom Namibia, which has offered ADSL access since late 2006, has a de facto monopoly on ADSL access. Their monopoly was unsuccessfully challenged in the courts by MWeb Namibia in May 2007 and again in August 2011.

In February 2007, ISP Namibia Mweb began offering broadband wireless services through WiMax, making Namibia the second African country (after Mozambique) to do so.

There are no government restrictions on access to the Internet; however, the Communications Act provides that the intelligence services can monitor e-mail and Internet usage with authorization from any magistrate. There have been some allegations and rumors that the government reviewed ways to block or curtail social media sites, but there is no concrete evidence of such action.

The constitution provides for freedom of speech and of the press, and the government generally respects these rights.





</doc>
<doc id="21299" url="https://en.wikipedia.org/wiki?curid=21299" title="Transport in Namibia">
Transport in Namibia

This article deals with the system of transport in Namibia, both public and private.

The beginnings of organised travel and transport routes in the territory of South West Africa, today Namibia, have not yet been established. This is due to the lack of any written records relating to roads prior to the twilight of the 19th century. Archaeological work has dated one stretch of road in the south-western Brandberg Massif to 1250 AD. Although no other such early examples have been found, it is certain that this road was not the only one of its kind.

The first permanent road, established for ox wagons, was built at the initiative of Heinrich Schmelen, Rhenish missionary in Bethanie in the early 19th century. It led from Bethanie to Angra Pequeña, today the town of Lüderitz, and was intended to serve the natural harbour there in order to become independent of the Cape Colony.

Namibia's road network is regarded as one of the best on the continent; road construction and maintenance adheres to international standards. The country's more than roads are administered by the "Roads Authority", a state-owned enterprise established by Act #17 of 1999. Due to low traffic volumes the majority of roads are not tarred. The distribution of road surfaces is:

The major highways in Namibia are:

Namibia has a relatively high prevalence of road accidents, compared to its sparse population. In 2011, 491 people died in 2,846 crashes. Causes are often speeding and reckless driving, as well as general non-observance of traffic rules. Stray animals are also a major cause of accidents, particularly in the Kavango Region.


Railways in Namibia are operated by TransNamib.

"Total track:"
2,382 km
"Total narrow gauge track:"
2,382 km gauge; single track (2002, 1995)









none (2002, 1999 est.)

135 (2002, 1999 est.)

Windhoek Hosea Kutako International Airport is the main international airport in the country, though there is only one other: the Walvis Bay Airport.

<br>"total:"
21 (2002), 22 (1999 est.)
<br>"over 3,047 m:"
2 (2002, 1999 est.)
<br>"2,438 to 3,047 m:"
2 (2002, 1999 est.)
<br>"1,524 to 2,437 m:"
13 (2002), 15 (1999 est.)
<br>"914 to 1,523 m:"
4 (2002, 3 (1999 est.)

<br>"total:"
114 (2002) 113 (1999 est.)
<br>"2,438 to 3,047 m:"
2 (2002, 1999 est.)
<br>"1,524 to 2,437 m:"
22 (2002 est.), 21 (1999 est.)
<br>"914 to 1,523 m:"
71 (2002), 69 (1999 est.)
<br>"under 914 m:"
19 (2002), 21 (1999 est.)



</doc>
<doc id="21300" url="https://en.wikipedia.org/wiki?curid=21300" title="Namibian Defence Force">
Namibian Defence Force

The Namibian Defence Force was created when South West Africa gained full independence from South Africa in 1990. The constitution of Namibia defines the role of the military as ""defending the territory and national interests.""

Namibia's military was born from the integration of the formerly belligerent People's Liberation Army of Namibia (PLAN), military wing of the South West African People's Organization, and the South West African Territorial Force (SWATF) - a security arm of the former South African administration. The British formulated the force integration plan and began training the NDF, which consists of five battalions and a small headquarters element. The United Nations Transitional Assistance Group (UNTAG)'s Kenyan infantry battalion remained in Namibia for three months after independence to assist in training the NDF and stabilize the north. Martin Shalli and Charles 'Ho Chi Minh' Namoloh were involved in the negotiations that allowed the Kenyan infantry battalion to remain for that period.

The main roles of the Namibian Defence Force are to ensure the sovereignty and territorial integrity of the country by guarding against external aggression, both conventional and unconventional; prevent violation of Namibia’s territorial integrity; and provide assistance to civil authorities in guarding and protecting government buildings and key installations as provided in the Defence Act.

Defence spending and percentage of GDP included $90 million in 1997/98, 2.6% of GDP. The 73.1 million figure in 2002 was 2.4% of GDP. These figures are almost certainly CIA World Factbook estimates.

Major General A W Dennis, CB, OBE (rtd), British Army, previously Director of Military Assistance Overseas, made the following comments on the initial phase in Namibia at a conference in Pretoria, South Africa on 6 August 1992:

You will no doubt recall that the Angola accords were signed in Luanda on 22 December 1988. In November 1989 SWAPO won 57% of the votes in the Namibian General Election and immediately requested the help of a British Military Advisory and Training Team following independence on 21 March 1990. The team, initially 55 strong, was duly deployed on 26 March 1990 and the first leaders cadre, for the 1st and 2nd Battalions, was run from 17 April to 2 June. By 1 July, the 1st Battalion, about 1 000 men strong, accompanied by 5 BMATT Advisors, had deployed to the northern border. By November 1990, only four months later, the 5th Battalion had deployed and in early 1991 the 21st Guards Battalion had also been formed, four staff courses had been run, support weapons and logistics training was well advanced (indeed a logistics battalion deployed as early as July 1990) and an operational test exercise had been conducted. In addition the Ministry of Defence, a mixture of civilian and military personnel, was operating as a department of state. No-one would pretend that everything was working perfectly, nevertheless, a great deal had been achieved in the first year following independence. Most people would probably agree that at some 7 500 strong the Army is unnecessarily large, but sensible plans will need to be made for the employment of any surplus soldiers before they are discharged. Integration has not been easy to achieve, at least in part, because of the need to use several interpreters to cope with the wide variety of languages involved. Battalions are made up of approximately 70% ex-PLAN and 30% ex-SWATF. This mixture could have proved explosive but hounded by their BMATT instructors they united in a common task (or perhaps in the face of a common enemy!) and soon realised that they could work well together. At the higher levels, integration has been more patchy, at least in part because of the departure of most white South African and SWATF officers. But the Government's intentions seem clear in that it decided to split the four MOD directorates evenly, appointing two white and two black (ex PLAN) directors. In all this, BMATT Namibia has played a role remarkably similar to that of BMATT Zimbabwe.

In August 1999, a separatist Lozi faction in the Caprivi Strip launched a coup attempt (see Caprivi conflict) which was summarily put down by the Namibian Defence Force. The army has conducted security operations along the northern border with Angola. In the process of these operations, there were allegations in 2001 that the army has tortured people suspected of being UNITA sympathisers. IRIN reported that the Ministry of Defence had admitted that two Namibian soldiers died fighting suspected UNITA rebels in southern Angola in July 2001. The Namibian Defence Force assists in putting out wildfires.

As of 13 October 2010, Sibbinda councillor Felix Mukupi has requested a meeting with the regional army commander in order to request 'the NDF to deploy its troops [on the Namibia/Zambia border area] stretching from Wenela to Kongola' in order to curtail stock thefts by gangs of cattle thieves from Zambia.

On 24 May 2010, Chen Bingde, Chief of the General Staff Department of the People's Liberation Army and member of the Central Military Commission, met Charles Namoloh and Peter Nambundunga, acting commander of the Namibian Defence Forces, in Windhoek. At their meeting, the two sides had in-depth discussions on further strengthening exchanges and cooperation between the two armed forces. Chen was accompanied by the chief of staff of the Second Artillery Corps and two PLA Military Region chiefs of staff. Chen also met President Pohamba that day.

In 2012, NDF officials announced the suspension of its recruitment campaign due to a lack of "accommodation facilities" for new personnel for a two-year period. The suspension how ever did not include the recruitment of specialist personnel as the Namibian Navy in 2013 had a recruitment exercise for sailors(officers and men) and marines. In 2014 recruitment resumed after accommodation issues were resolved.

The Chief Of Defence Force is the highest-ranking officer and exercises overall executive command of the force. Service Chiefs are two star General Officers, Air Officers and Flag Officers in command of their respective arms of service. NDF directorates are led by one star General Officers, Air Officers and Flag Officers.The exception however is the Joint Operations Directorate whose head is a Major General, who also doubles up as the GOC Special Forces.The Joint Operations Directorate is responsible for Force deployment in the Military.

The Chief of Defence Force is always a commissioned three star General/Air/Flag Officer from the officer corps.The first chief of the NDF was Lieutenant-General Dimo Hamaambo. He was previously the leader of PLAN, and a survivor of the Battle of Cassinga. Lieutenant-General Hamaambo was the first to be laid to rest at the Heroes' Acre memorial outside Windhoek, a few days after its official opening in 2002. Lieutenant-General Solomon Huwala replaced Hamaambo as Chief of the NDF on Hamaambo's retirement. After Lieutenant-General Huwala retired in October 2006, Lieutenant General Martin Shalli headed the NDF.

President Hifikepunye Pohamba suspended Lieutenant-General Shalli from his post as Chief of Defence Force in 2009 over corruption allegations, dating back to the time when Shalli served as Namibia’s High Commissioner to Zambia. During the time of the suspension, Army Commander Major General Peter Nambundunga acted as Chief. Shalli was eventually forced to retire in January 2011; the post of Chief of the NDF was given to Epaphras Denga Ndaitwah. Ndaitwah served until 31 December 2013 when the NDF Chief's position was given to Maj Gen John Mutwa.

As of February 2012, it was reported that a Chinese company paid US $499,950 into Shalli's account in Zambia while he was the NDF chief. Poly Technologies was supplying equipment to the NDF at the time.

NDF Sergeant Major is the highest appointment a Non Commission Officer may receive. Duties of the NDF Sergeant Major includes making sure that discipline, drills, dressing code, performance
standards and morale of the non commissioned officers are maintained. The current NDF Sergeant Major is Warrant Officer Class 1(WO1) Albert Siyaya. He took over from retired Namibian Navy WO1 Isak Nankela.

Previous Sergeant Major are:

The Force's Medical Service provides medical services to service personnel, it operates sick bays at all bases and units as well the Grootfontein Military Hospital and the Peter Mweshihange Military Health Center in Windhoek. The Medical Health Services also operate a Mobile Field Hospital received as a donation from Germany. The Mobile hospital is rated as a United Nations Level II hospital.Plans are underway to construct another military hospital in Windhoek.

There are some units that report directly to the Chief of Defence Force.
These are the:

The Namibian military's unconventional warfare specialists are part of this command. Specialist training, tough training courses and some of the best soldiers are found in the Namibian Special Forces.

NDF ranks are based on the Commonwealth rank structure. There is no approved four star General rank in the NDF. The Chief of Defence Force is a singular appointment comes with an elevation to the rank of Lieutenant General for an Army officer, Air Marshal for an Air Force officer and Vice Admiral for a Navy officer. Arms of Services Commanders i.e. Army, Air Force and Navy Commanders have a rank of Major General, Air Vice Marshal and Rear Admiral. The rank of Brigadier has also been transformed into Brigadier General. Directorate heads are always Brigadier Generals, i.e. the Chief of Staff for Defence Intelligence.
Warrant Officer Class 1 Appointments
Any warrant officer class 1 could be posted to substantive posts, including

The landward arm of service for the Defence force is the Namibian Army, it is also the largest of the NDF's service branches.

The aerial warfare branch is small, but was bolstered with deliveries of some fighter jets in 2006 and 2008.

Development of the maritime warfare branch has been slow, and the force was only formally established in 2004, 14 years after independence. Today, it numbers over 1100 personnel and deploys a small number of lightly armed patrol vessels. Extensive Brazilian aid assisted in its development.

The Joint Headquarters is an Arm of Service level institution in the Defence Force and is created by the Minister of Defence in terms of section 13 of the Defence Act.

The Namibian Military School is the main training and academic unit of the Namibian Defence Force.It offers Officer Cadets and NDF officers an opportunity to get a military oriented academic qualification.Training and teaching in the institution ranges from Basic Military Training, to technical mechanical training. 

The School of Military Science run in conjuction with the University of Namibia the School offers officers in the Defence force qualification's ranging from bachelor of Science Honors degrees in the field of nautical, Army and Aeronautical. A post graduate diploma in Security and strategics studies and a Master of Arts in Security and Strategic Studies (MA-SSS) are also offered.

The Namibian Command and Staff College offers the Junior Staff Course (JSC) and the Senior Command and Staff Course (SCSC). It provides staff training to prepare students for staff appointments. The NCSC's commandant is Brigadier General Simon Titus.

The force's parachute airborne school is based at the Grootfontein Air Force Base. Here students from all service branches are training to qualify as Parachute specialists. The school was set up with help by the South African private military parachute training company Chute Systems who are training Namibia's airborne forces and associated staff e.g. parachute riggers.




</doc>
<doc id="21301" url="https://en.wikipedia.org/wiki?curid=21301" title="Foreign relations of Namibia">
Foreign relations of Namibia

Namibia follows a largely independent foreign policy, with strong affiliations with states that aided the independence struggle, including Libya and Cuba.

In Africa, Namibia has been involved in conflicts in neighbouring Angola as well as the Democratic Republic of Congo.

Namibia is a member of 47 international organizations. These are:

Namibia became the 160th member of the United Nations on 23 April 1990 upon independence.

With a small army and a fragile economy, the Namibian Government's principal foreign policy concern is developing strengthened ties within the Southern African region. A dynamic member of the Southern African Development Community, Namibia is a vocal advocate for greater regional integration.

Namibia is involved in several minor international disputes.

Namibia has been a Commonwealth republic since 1990, when South West Africa became independent of South Africa.




</doc>
<doc id="21302" url="https://en.wikipedia.org/wiki?curid=21302" title="Nauru">
Nauru

Nauru (, or ), officially the Republic of Nauru () and formerly known as Pleasant Island, is an island country in Micronesia, a subregion of Oceania, in the Central Pacific. Its nearest neighbour is Banaba Island in Kiribati, to the east. It further lies northwest of Tuvalu, north of the Solomon Islands, east-northeast of Papua New Guinea, southeast of the Federated States of Micronesia and south of the Marshall Islands. With residents in a area, Nauru is the smallest state in the South Pacific, smallest republic and third smallest state by area in the world, behind only Vatican City and Monaco.

Settled by people from Micronesia and Polynesia , Nauru was annexed and claimed as a colony by the German Empire in the late 19th century. After World War I, Nauru became a League of Nations mandate administered by Australia, New Zealand and the United Kingdom. During World War II, Nauru was occupied by Japanese troops, who were bypassed by the Allied advance across the Pacific. After the war ended, the country entered into UN trusteeship. Nauru gained its independence in 1968.

Nauru is a phosphate rock island with rich deposits near the surface, which allowed easy strip mining operations. It has some remaining phosphate resources which, , are not economically viable for extraction. Nauru boasted the highest per-capita income enjoyed by any sovereign state in the world during the late 1960s and early 1970s. When the phosphate reserves were exhausted, and the island's environment had been seriously harmed by mining, the trust that had been established to manage the island's wealth diminished in value. To earn income, Nauru briefly became a tax haven and illegal money laundering centre. From 2001 to 2008, and again from 2012, it accepted aid from the Australian Government in exchange for hosting the Nauru Regional Processing Centre. As a result of heavy dependence on Australia, many sources have identified Nauru as a client state of Australia.

Nauru was first inhabited by Micronesians and Polynesians at least 3,000 years ago. There were traditionally 12 clans or tribes on Nauru, which are represented in the 12-pointed star on the country's flag. Traditionally, Nauruans traced their descent matrilineally. Inhabitants practised aquaculture: they caught juvenile "ibija" fish, acclimatised them to fresh water, and raised them in the Buada Lagoon, providing a reliable source of food. The other locally grown components of their diet included coconuts and pandanus fruit. The name "Nauru" may derive from the Nauruan word "", which means "I go to the beach."

The British sea captain John Fearn, a whale hunter, became the first Westerner to visit Nauru, in 1798, calling it "Pleasant Island". From around 1830, Nauruans had contact with Europeans from whaling ships and traders who replenished their supplies (particularly fresh water) at Nauru.

Around this time, deserters from European ships began to live on the island. The islanders traded food for alcoholic palm wine and firearms. The firearms were used during the 10-year Nauruan Tribal War that began in 1878.

After an agreement with Great Britain, Nauru was annexed by Germany in 1888 and incorporated into Germany's Marshall Islands Protectorate for administrative purposes. The arrival of the Germans ended the civil war, and kings were established as rulers of the island. The most widely known of these was King Auweyida. Christian missionaries from the Gilbert Islands arrived in 1888. The German settlers called the island Nawodo or Onawero. The Germans ruled Nauru for almost three decades. Robert Rasch, a German trader who married a Nauruan woman, was the first administrator, appointed in 1890.

Phosphate was discovered on Nauru in 1900 by the prospector Albert Fuller Ellis. The Pacific Phosphate Company began to exploit the reserves in 1906 by agreement with Germany, exporting its first shipment in 1907. In 1914, following the outbreak of World War I, Nauru was captured by Australian troops. In 1919 it was agreed by the Allied and Associated Powers that His Britannic Majesty should be the administering authority under a League of Nations mandate. The Nauru Island Agreement made in 1919 between the governments of the United Kingdom, Australia and New Zealand provided for the administration of the island and for working of the phosphate deposits by an intergovernmental British Phosphate Commission (BPC). The terms of the League of Nations Mandate were drawn up in 1920.

The island experienced an influenza epidemic in 1920, with a mortality rate of 18% among native Nauruans.

In 1923, the League of Nations gave Australia a trustee mandate over Nauru, with the United Kingdom and New Zealand as co-trustees. On 6 and 7 December 1940, the German auxiliary cruisers "Komet" and "Orion" sank five supply ships in the vicinity of Nauru. "Komet" then shelled Nauru's phosphate mining areas, oil storage depots, and the shiploading cantilever.
Japanese troops occupied Nauru on 25 August 1942. The Japanese built an airfield which was bombed for the first time on 25 March 1943, preventing food supplies from being flown to Nauru. The Japanese deported 1,200 Nauruans to work as labourers in the Chuuk islands, which was also occupied by Japan. Nauru, which had been bypassed and left to "wither on the vine" by American forces, was finally liberated on 13 September 1945, when commander Hisayaki Soeda surrendered the island to the Australian Army and the Royal Australian Navy.

This surrender was accepted by Brigadier J. R. Stevenson, who represented Lieutenant General Vernon Sturdee, the commander of the First Australian Army, on board the warship HMAS "Diamantina". Arrangements were made to repatriate from Chuuk the 737 Nauruans who survived Japanese captivity there. They were returned to Nauru by the BPC ship "Trienza" in January 1946.

In 1947, a trusteeship was established by the United Nations, with Australia, New Zealand, and the United Kingdom as trustees. Under those arrangements, the UK, Australia and New Zealand were a joint administering authority. The Nauru Island Agreement provided for the first Administrator to be appointed by Australia for 5 years, leaving subsequent appointments to be decided by the three governments. However, in practice, administrative power was exercised by Australia alone.

Nauru became self-governing in January 1966, and following a two-year constitutional convention it became independent in 1968 under founding president Hammer DeRoburt. In 1967, the people of Nauru purchased the assets of the British Phosphate Commissioners, and in June 1970 control passed to the locally owned Nauru Phosphate Corporation. Income from the mines gave Nauruans one of the highest standards of living in the world. In 1989, Nauru took legal action against Australia in the International Court of Justice over Australia's administration of the island, in particular Australia's failure to remedy the environmental damage caused by phosphate mining. "Certain Phosphate Lands: Nauru v. Australia" led to an out-of-court settlement to rehabilitate the mined-out areas of Nauru.

Nauru is a oval-shaped island in the southwestern Pacific Ocean, south of the Equator. The island is surrounded by a coral reef, which is exposed at low tide and dotted with pinnacles. The presence of the reef has prevented the establishment of a seaport, although channels in the reef allow small boats access to the island. A fertile coastal strip wide lies inland from the beach.

Coral cliffs surround Nauru's central plateau. The highest point of the plateau, called the Command Ridge, is above sea level.

The only fertile areas on Nauru are on the narrow coastal belt, where coconut palms flourish. The land around Buada Lagoon supports bananas, pineapples, vegetables, pandanus trees, and indigenous hardwoods, such as the tomano tree.

Nauru was one of three great phosphate-rock islands in the Pacific Ocean, along with Banaba (Ocean Island), in Kiribati, and Makatea, in French Polynesia. The phosphate reserves on Nauru are now almost entirely depleted. Phosphate mining in the central plateau has left a barren terrain of jagged limestone pinnacles up to high. Mining has stripped and devastated about 80% of Nauru's land area, and has also affected the surrounding Exclusive Economic Zone; 40% of marine life is estimated to have been killed by silt and phosphate runoff.

There are limited natural sources of fresh water on Nauru. Rooftop storage tanks collect rainwater. The islanders are mostly dependent on three desalination plants housed at Nauru's Utilities Agency.

Nauru's climate is hot and very humid year round because of its proximity to the equator and the ocean. Nauru is hit by monsoon rains between November and February, but usually no cyclones. Annual rainfall is highly variable and is influenced by the El Niño–Southern Oscillation, with several significant recorded droughts. The temperature on Nauru ranges between during the day and between at night.

Fauna is sparse on the island, because of a lack of vegetation and the consequences of phosphates mining. Many indigenous birds have disappeared or become rare owing to destruction of their habitat. There are about 60 recorded vascular plant species native to the island, none of which is endemic. Coconut farming, mining, and introduced species have seriously disturbed the native vegetation.

There are no native land mammals, but there are native insects, land crabs, and birds, including the endemic Nauru reed warbler. The Polynesian rat, cats, dogs, pigs, and chickens have been introduced to Nauru from ships. The diversity of the reef marine life makes fishing a popular activity for tourists on the island; also popular are SCUBA diving and snorkelling.

The president of Nauru is Baron Waqa, who heads a 19-member unicameral parliament. The country is a member of the United Nations, the Commonwealth of Nations, the Asian Development Bank and the Pacific Islands Forum. Nauru also participates in the Commonwealth and Olympic Games. Recently Nauru became a member country of the International Renewable Energy Agency (IRENA). The Republic of Nauru became the 189th member of the International Monetary Fund in April 2016.

Nauru is a republic with a parliamentary system of government. The president is both head of state and head of government. A 19-member unicameral parliament is elected every three years. The parliament elects the president from its members, and the president appoints a cabinet of five to six members.

Nauru does not have any formal structure for political parties, and candidates typically stand for office as independents; fifteen of the 19 members of the current Parliament are independents. Four parties that have been active in Nauruan politics are the Nauru Party, the Democratic Party, Nauru First, and the Centre Party. However, alliances within the government are often formed on the basis of extended family ties rather than party affiliation.

From 1992 to 1999, Nauru had a local government system known as the Nauru Island Council (NIC). This nine-member council was designed to provide municipal services. The NIC was dissolved in 1999 and all assets and liabilities became vested in the national government. Land tenure on Nauru is unusual: all Nauruans have certain rights to all land on the island, which is owned by individuals and family groups. Government and corporate entities do not own any land, and they must enter into a lease arrangement with landowners to use land. Non-Nauruans cannot own land on the island.

Nauru had 17 changes of administration between 1989 and 2003. Bernard Dowiyogo died in office in March 2003 and Ludwig Scotty was elected as the president, later being re-elected to serve a full term in October 2004. Following a vote of no confidence on 19 December 2007, Scotty was replaced by Marcus Stephen. Stephen resigned in November 2011, and Freddie Pitcher became President. Sprent Dabwido then filed a motion of no confidence in Pitcher, resulting in him becoming president. Following parliamentary elections in 2013, Baron Waqa was elected president.

Its Supreme Court, headed by the Chief Justice, is paramount on constitutional issues. Other cases can be appealed to the two-judge Appellate Court. Parliament cannot overturn court decisions, but Appellate Court rulings can be appealed to the High Court of Australia. In practice this rarely happens. Lower courts consist of the District Court and the Family Court, both of which are headed by a Resident Magistrate, who also is the Registrar of the Supreme Court. There are two other quasi-courts: the Public Service Appeal Board and the Police Appeal Board, both of which are presided over by the Chief Justice.

Following independence in 1968, Nauru joined the Commonwealth of Nations as a Special Member; it became a full member in 2000. The country was admitted to the Asian Development Bank in 1991 and to the United Nations in 1999. Nauru is a member of the Pacific Islands Forum, the South Pacific Regional Environment Programme, the South Pacific Commission, and the South Pacific Applied Geoscience Commission. The American Atmospheric Radiation Measurement Program operates a climate-monitoring facility on the island.
Nauru has no armed forces, though there is a small police force under civilian control. Australia is responsible for Nauru's defence under an informal agreement between the two countries. The September 2005 Memorandum of Understanding between Australia and Nauru provides the latter with financial aid and technical assistance, including a Secretary of Finance to prepare the budget, and advisers on health and education. This aid is in return for Nauru's housing of asylum seekers while their applications for entry into Australia are processed. Nauru uses the Australian dollar as its official currency.

Nauru has used its position as a member of the United Nations to gain financial support from both Taiwan (ROC) and China (PRC) by changing its recognition from one to the other under the One-China policy. On 21 July 2002, Nauru signed an agreement to establish diplomatic relations with the PRC, accepting $130 million from the PRC for this action. In response, the ROC severed diplomatic relations with Nauru two days later. Nauru later re-established links with the ROC on 14 May 2005, and diplomatic ties with the PRC were officially severed on 31 May 2005. However, the PRC continues to maintain a representative office on Nauru.

In 2008, Nauru recognised Kosovo as an independent country, and in 2009 Nauru became the fourth country, after Russia, Nicaragua, and Venezuela, to recognise Abkhazia, a breakaway region of Georgia. Russia was reported to be giving Nauru $50 million in humanitarian aid as a result of this recognition. On 15 July 2008, the Nauruan government announced a port refurbishment programme, financed with US$9 million of development aid received from Russia. The Nauru government claims this aid is not related to its recognising Abkhazia and South Ossetia.

A significant portion of Nauru's income has been in the form of aid from Australia. In 2001, the MV "Tampa", a Norwegian ship that had rescued 438 refugees from a stranded 20-metre-long boat, was seeking to dock in Australia. In what became known as the "Tampa" affair, the ship was refused entry and boarded by Australian troops. The refugees were eventually loaded onto Royal Australian Navy vessel HMAS "Manoora" and taken to Nauru to be held in detention facilities which later became part of the Howard government's Pacific Solution. Nauru operated two detention centres known as State House and Topside for these refugees in exchange for Australian aid. By November 2005, only two refugees, Mohammed Sagar and Muhammad Faisal, remained on Nauru from those first sent there in 2001, with Sagar finally resettling in early 2007. The Australian government sent further groups of asylum-seekers to Nauru in late 2006 and early 2007. The refugee centre was closed in 2008, but, following the Australian government's re-adoption of the Pacific Solution in August 2012, it has re-opened it. 
Amnesty International has described the conditions of the refugees of war living in Nauru, as "horror".

Nauru is divided into fourteen administrative districts which are grouped into eight electoral constituencies and are further divided into various villages. The most populous district is Denigomodu with a total of 1,804 residents, out of which 1,497 reside in NPC settlement called Location. The following table shows population size by district as per 2011 census.

The Nauruan economy peaked in the early 1980s, as it was dependent almost entirely on the phosphate deposits that originate from the droppings of sea birds. There are few other resources, and most necessities are imported. Small-scale mining is still conducted by RONPhos, formerly known as the Nauru Phosphate Corporation. The government places a percentage of RONPhos's earnings into the Nauru Phosphate Royalties Trust. The Trust manages long-term investments, which were intended to support the citizens once the phosphate reserves were exhausted.

Because of mismanagement, the Trust's fixed and current assets were reduced considerably and may never fully recover. The failed investments included financing "" in 1993. The Mercure Hotel in Sydney and Nauru House in Melbourne were sold in 2004 to finance debts and Air Nauru's only Boeing 737 was repossessed in December 2005. Normal air service resumed after the aircraft was replaced with a Boeing 737–300 airliner in June 2006. In 2005, the corporation sold its property asset in Melbourne, the vacant Savoy Tavern site, for $7.5 million.

The value of the Trust is estimated to have shrunk from A$1.3 billion in 1991 to A$138 million in 2002. Nauru currently lacks money to perform many of the basic functions of government; for example, the National Bank of Nauru is insolvent. The CIA World Factbook estimated a GDP per capita of $5,000 in 2005. The Asian Development Bank 2007 economic report on Nauru estimated GDP per capita at $2,400 to $2,715. The United Nations (2013) estimates the GDP per capita to 15,211 and ranks it 51 on its GDP per capita country list.

There are no personal taxes in Nauru. The unemployment rate is estimated to be 90%, and of those who have jobs, the government employs 95%. The Asian Development Bank notes that, although the administration has a strong public mandate to implement economic reforms, in the absence of an alternative to phosphate mining, the medium-term outlook is for continued dependence on external assistance. Tourism is not a major contributor to the economy.

In the 1990s, Nauru became a tax haven and offered passports to foreign nationals for a fee. The inter-governmental Financial Action Task Force on Money Laundering (FATF) identified Nauru as one of 15 "non-cooperative" countries in its fight against money laundering. During the 1990s, it was possible to establish a licensed bank in Nauru for only $25,000 with no other requirements. Under pressure from FATF, Nauru introduced anti-avoidance legislation in 2003, after which foreign hot money left the country. In October 2005, after satisfactory results from the legislation and its enforcement, FATF lifted the non-cooperative designation.

From 2001 to 2007, the Nauru detention centre provided a significant source of income for the country. The Nauruan authorities reacted with concern to its closure by Australia. In February 2008, the Foreign Affairs minister, Dr Kieren Keke, stated that the closure would result in 100 Nauruans losing their jobs, and would affect 10 percent of the island's population directly or indirectly: "We have got a huge number of families that are suddenly going to be without any income. We are looking at ways we can try and provide some welfare assistance but our capacity to do that is very limited. Literally we have got a major unemployment crisis in front of us." The detention centre was re-opened in August 2012.

In July 2017 the Organisation for Economic Co-operation and Development (OECD) upgraded its rating of Nauru's standards of tax transparency. 
Previously Nauru had been listed alongside fourteen other countries that had failed to show that they could comply with international tax transparency standards and regulations. The OECD subsequently put Nauru through a fast-tracked compliance process and the country was given a "largely compliant" rating.

The Nauru 2017/18 budget, delivered by Minister for Finance David Adeang, forecasted $128.7 million in revenues and $128.6 million in expenditures and projected modest economic growth for the nation over the next two years.

Nauru had  residents as of July , making it the second smallest sovereign state after Vatican City. The population was previously larger, but in 2006 1,500 people left the island during a repatriation of immigrant workers from Kiribati and Tuvalu. The repatriation was motivated by large force reductions in phosphate mining. Nauru is the least populous country in Oceania.

58% of people in Nauru are ethnically Nauruan, 26% are other Pacific Islander, 8% are European, and 8% are Han Chinese. Nauruans descended from Polynesian and Micronesian seafarers. Two of the 12 original tribal groups became extinct in the 20th century.

The official language of Nauru is Nauruan, a distinct Pacific island language, which is spoken by 96% of ethnic Nauruans at home.

English is widely spoken and is the language of government and commerce, as Nauruan is not common outside of the country.

The main religion practised on the island is Christianity (two-thirds Protestant, one-third Roman Catholic). The Constitution provides for freedom of religion. The government has restricted the religious practices of The Church of Jesus Christ of Latter-day Saints and the Jehovah's Witnesses, most of whom are foreign workers employed by the government-owned Nauru Phosphate Corporation. The Catholics are pastorally served by the Roman Catholic Diocese of Tarawa and Nauru, with see at Tarawa in Kiribati.

Angam Day, held on 26 October, celebrates the recovery of the Nauruan population after the two World Wars and the 1920 influenza epidemic. The displacement of the indigenous culture by colonial and contemporary Western influences is significant. Few of the old customs have been preserved, but some forms of traditional music, arts and crafts, and fishing are still practised.

There are no daily news publications on Nauru, although there is one fortnightly publication, "Mwinen Ko". There is a state-owned television station, Nauru Television (NTV), which broadcasts programs from New Zealand and Australia, and a state-owned non-commercial radio station, Radio Nauru, which carries programs from Radio Australia and the BBC.

Australian rules football is the most popular sport in Nauru – it and weightlifting are considered the country's national sports. There is an Australian Rules football league with eight teams. Other sports popular in Nauru include volleyball, netball, fishing and tennis. Nauru participates in the Commonwealth Games and has participated in the Summer Olympic Games in weightlifting and judo.

Nauru's national basketball team competed at the 1969 Pacific Games, where it defeated the Solomon Islands and Fiji.

Rugby sevens popularity has increased over the last two years, so much they have a national team.

Nauru competed in the 2015 Oceania Sevens Championship in New Zealand.

Independence Day is celebrated on 31 January.

Literacy on Nauru is 96 percent. Education is compulsory for children from six to sixteen years old, and two more non-compulsory years are offered (years 11 and 12). The island has three primary schools and two secondary schools, the latter being Nauru College and Nauru Secondary School. There is a campus of the University of the South Pacific on Nauru. Before this campus was built in 1987, students would study either by distance or abroad. Since 2011, the University of New England, Australia has established a presence on the island with around 30 Nauruan teachers studying for an associate degree in education. These students will continue onto the degree to complete their studies. This project is led by Associate Professor Pep Serow and funded by the Australian Department of Foreign Affairs and Trade.

The previous community public library had been destroyed in a fire. a new one had not yet been built, and no bookmobile services are available as of that year. Sites with libraries include the University of the South Pacific campus, Nauru Secondary, Kayser College, and Aiwo Primary. The Nauru Community Library is in the new University of the South Pacific Nauru Campus building, which was officially opened in May 2018

Life expectancy on Nauru in 2009 was 60.6 years for males and 68.0 years for females.

By measure of mean body mass index (BMI) Nauruans are the most overweight people in the world; 97% of men and 93% of women are overweight or obese. In 2012 the obesity rate was 71.7%. Obesity in the Pacific islands is common.

Nauru has the world's highest level of type 2 diabetes, with more than 40% of the population affected. Other significant dietary-related problems on Nauru include kidney disease and heart disease.





</doc>
<doc id="21303" url="https://en.wikipedia.org/wiki?curid=21303" title="History of Nauru">
History of Nauru

The history of human activity in Nauru, an island country in the Pacific Ocean, began roughly 3,000 years ago when 12 Micronesian and Polynesian clans settled the island.

Nauru was first settled by Micronesian and Polynesian people at least 3,000 years ago. Nauruans subsisted on coconut and pandanus fruit, and engaged in aquaculture by catching juvenile "ibija" fish, acclimated them to freshwater conditions, and raised them in Buada Lagoon, providing an additional reliable source of food. Traditionally only men were permitted to fish on the reef, and did so from canoes or by using trained man-of-war hawks.

There were traditionally 12 clans or tribes on Nauru, which are represented in the 12-pointed star in the nation's flag. Nauruans traced their descent on the female side. The first Europeans to encounter the island were on the British whaling ship "Hunter", in 1798. When the ship approached, "many canoes ventured out to meet the ship. The "Hunter"s crew did not leave the ship nor did Nauruans board, but Captain John Fearn's positive impression of the island and its people" led to its English name, Pleasant Island. This name was used until Germany annexed the island 90 years later.

From around 1830, Nauruans had contact with Europeans from whaling ships and traders who replenished their supplies (such as fresh water) at Nauru. The islanders traded food for alcoholic toddy and firearms. The first Europeans to live on the island, starting perhaps in 1830, were Patrick Burke and John Jones, Irish convicts who had escaped from Norfolk Island, according to "Paradise for Sale". Jones became "Nauru's first and last dictator," who killed or banished several other beachcombers who arrived later, until the Nauruans banished Jones from the island in 1841.

The introduction of firearms and alcohol destroyed the peaceful coexistence of the 12 tribes living on the island. A 10-year internal war began in 1878 and resulted in a reduction of the population from 1,400 (1843) to around 900 (1888). Ultimately, alcohol was banned and some arms were confiscated.

In 1886, Germany was granted the island under the Anglo-German Declaration. The island was annexed by Germany in 1888 and incorporated into Germany's New Guinea Protectorate. On 1 October 1888, the German gunboat SMS Eber landed 36 men on Nauru. Accompanied by William Harris the German marines marched around the island and returned with the twelve chiefs, the white settlers and a Gilbertese missionary. The chiefs were kept under house arrest until the next morning, when the annexation ceremony began with the raising of the German flag. The Germans told the chiefs that they had to surrender all weapons and ammunition within 24 hours or the chiefs would be taken prisoner. By the morning of 3 October 765 guns and 1,000 rounds of ammunition were turned over. The Germans called the island Nawodo or Onawero. The arrival of the Germans ended the war, and social changes brought about by the war established kings as rulers of the island, the most widely known being King Auweyida. Christian missionaries from the Gilbert Islands also arrived at the island in 1888. The Germans ruled Nauru for almost three decades. Robert Rasch, a German trader who married a native woman, was the first administrator, appointed in 1890.

At the time there were twelve tribes on Nauru: Deiboe, Eamwidamit, Eamwidara, Eamwit, Eamgum, Eano, Emeo, Eoraru, Irutsi, Iruwa, Iwi and Ranibok. Today the twelve tribes are represented by the twelve-pointed star in the flag of Nauru.

Phosphate was discovered on Nauru in 1900 by the prospector Albert Ellis. The Pacific Phosphate Company started to exploit the reserves in 1906 by agreement with Germany. The company exported its first shipment in 1907.

In 1914, following the outbreak of World War I, Nauru was captured by Australian troops, after which Britain held control until 1920. Australia, New Zealand, and the United Kingdom signed the Nauru Island Agreement in 1919, creating a board known as the British Phosphate Commission (BPC). This took over the rights to phosphate mining.
According to the Commonwealth Bureau of Census and Statistics (now the Australian Bureau of Statistics), "In common with other natives, the islanders are very susceptible to tuberculosis and influenza, and in 1921 an influenza epidemic caused the deaths of 230 islanders." In 1923, the League of Nations gave Australia a trustee mandate over Nauru, with the United Kingdom and New Zealand as co-trustees. In 1932, the first Angam Baby was born.

During World War II, Nauru was subject to significant damage from both Axis (German and Japanese) and Allied forces.

On 6 and 7 December 1940 the Nazi German auxiliary cruisers "Orion" and "Komet" sank four merchant ships. On the next day, "Komet" shelled Nauru's phosphate mining areas, oil storage depots, and the shiploading cantilever. The attacks seriously disrupted phosphate supplies to Australia and New Zealand (mostly used for munition and fertiliser purposes.)

Japanese troops occupied Nauru on 26 August 1942. The native Nauruans were badly treated by the occupying forces. On one occasion thirty nine leprosy sufferers were reputedly loaded onto boats which were towed out to sea and sunk. The Japanese troops built an airfield on Nauru which was bombed for the first time on 25 March 1943, preventing food supplies from being flown to Nauru. In 1943 the Japanese deported 1,200 Nauruans to work as labourers in the Chuuk islands.

Nauru was finally set free from the Japanese on 13 September 1945, when Captain Solda, the commander of all the Japanese troops on Nauru, surrendered the island to the Royal Australian Navy and Army. This surrender was accepted by Brigadier J. R. Stevenson, who represented Lieutenant General Vernon Sturdee, the commander of the First Australian Army, on board the warship HMAS "Diamantina" Arrangements were made to repatriate from Chuuk the 737 Nauruans who survived Japanese captivity there. They were returned to Nauru by the BPC ship "Trienza" in on 1 January 1946. In 1947, a trusteeship was established by the United Nations, and Australia, New Zealand, and the United Kingdom became the U.N. trustees of the island, with administration passing mostly to Australia.

Nauru became self-governing in January 1966. On 31 January 1968, following a two-year constitutional convention, Nauru became the world's smallest independent republic. It was led by founding president Hammer DeRoburt. In 1967, the people of Nauru purchased the assets of the British Phosphate Commissioners, and in June 1970, control passed to the locally owned Nauru Phosphate Corporation. Money gained from the exploitation of phosphate was put into the Nauru Phosphate Royalties Trust and gave Nauruans the second highest GDP Per Capita (second only to the United Arab Emirates) and one of the highest standards of living in the Third World.

In 1989, Nauru took legal actions against Australia in the International Court of Justice over Australia's actions during its administration of Nauru. In particular, Nauru made a legal complaint against Australia's failure to remedy the environmental damage caused by phosphate mining. "Certain Phosphate Lands: Nauru v. Australia" led to an out-of-court settlement to rehabilitate the mined-out areas of Nauru.

By the close of the twentieth century, the finite phosphate supplies were fast running out. Nauru finally joined the UN in 1999.

As its phosphate stores began to run out (by 2006, its reserves were exhausted), the island was reduced to an environmental wasteland. Nauru appealed to the International Court of Justice to compensate for the damage from almost a century of phosphate strip-mining by foreign companies. In 1993, Australia offered Nauru an out-of-court settlement of A$2.5 million annually for 20 years. New Zealand and the UK additionally agreed to pay a one-time settlement of $12 million each. Declining phosphate prices, the high cost of maintaining an international airline, and the government's financial mismanagement combined to make the economy collapse in the late 1990s. By the new millennium, Nauru was virtually bankrupt.

In December 1999, four major United States banks banned dollar transactions with four Pacific island states, including Nauru. The United States Department of State issued a report identifying Nauru as a major money laundering centre, used by narcotics traffickers and Russian organized crime figures.

President Bernard Dowiyogo took office in April 2000 for his fourth and, after a minimal hiatus, fifth stints as Nauru's top executive. Dowiyogo first served as president from 1976 to 1978. He returned to that office in 1989, and was re-elected in 1992. A vote in parliament, however, forced him to yield power to Kinza Clodumar in 1995. Dowiyogo regained the presidency when the Clodumar government fell in mid-1998.

In 2001, Nauru was brought to world attention by the Tampa affair, a Norwegian cargo ship at the centre of a diplomatic dispute between Australia, Norway and Indonesia. The ship carried asylum seekers, hailing primarily from Afghanistan, who were rescued while attempting to reach Australia. After much debate many of the immigrants were transported to Nauru, an arrangement known in Australia as the "Pacific Solution". Shortly thereafter, the Nauruan government closed its borders to most international visitors, preventing outside observers from monitoring the refugees' condition.

In December 2003, several dozen of these refugees, in protest of the conditions of their detention on Nauru, began a hunger strike. The hunger strike was concluded in early January 2004 when an Australian medical team agreed to visit the island. Since then, according to recent reports, all but two of the refugees have been allowed into Australia.

During 2002 Nauru severed diplomatic recognition with Taiwan (Republic of China) and signed an agreement to establish diplomatic relations with the People's Republic of China. This move followed China's promise to provide more than U.S. $130 million in aid. In 2004, Nauru broke off relations with the PRC and re-established them with the ROC.

Nauru was also approached by the U.S. with a deal to modernize Nauru's infrastructure in exchange for suppression of the island's lax banking laws that allow activities that are illegal in other countries to flourish. Under this deal, allegedly, Nauru would also establish an embassy in China and perform certain "safehouse" and courier services for the U.S. government, in a scheme codenamed "Operation Weasel". Nauru agreed to the deal and instituted banking reform, but the U.S. later denied knowledge of the deal. The matter is being pursued in an Australian court, and initial judgments have been in favor of Nauru.

The government is desperately in need of money to pay off salary arrears of civil servants and to continue funding the welfare state built up in the heyday of phosphate mining (Nauruans pay no taxes). Nauru has yet to develop a plan to remove the innumerable coral pinnacles created by mining and make those lands suitable for human habitation.




</doc>
<doc id="21304" url="https://en.wikipedia.org/wiki?curid=21304" title="Geography of Nauru">
Geography of Nauru

Nauru is a tiny phosphate rock island nation located in the South Pacific Ocean south of the Marshall Islands in Oceania. It is only south of the Equator at coordinates . Nauru is one of the three great phosphate rock islands in the Pacific Ocean — the others are Banaba (Ocean Island) in Kiribati and Makatea in French Polynesia.

Its land area is , and it has a . Maritime claims are a 200-nautical-mile (370 km) exclusive fishing zone, and a 12-nautical-mile (22 km) territorial sea.

The climate is tropical with a monsoonial rainy season from November to February.

A sandy beach rises to the fertile ring around raised coral reefs. The raised phosphate plateau ('Topside') takes up the central portion of the island. The highest point is 200 ft (61 m) above sea level, along the plateau rim.

Nauru's only economically significant natural resources are phosphates, formed from guano deposits by seabirds over many thousands of years, and fisheries, particularly for tuna. 

Due to being surrounded by corals and sandy beaches, the island houses no natural harbours, nor any rivers or substantial lakes.

Nauru has its own unique navigational system, which is only capable of being used on the island.

Nauru is a raised coral atoll positioned in the Nauru Basin of the Pacific Ocean, on a part of the Pacific Plate that formed at a mid oceanic ridge at 132 Ma.
From mid Eocene (35mya) to Oligocene times a submarine volcano built up over a hotspot, and formed a seamount composed of basalt. The seamount is over 4300 metres high. This hotspot was simultaneous with a major 
Pacific Plate reorganisation. The volcano was eroded to sealevel and a coral atoll grew on top to a thickness of about 500 metres. Coral near the surface has been dated from 5 Mya to 0.3 Mya. The original limestone has been dolomitised by magnesium from sea water. The coral was 
raised above sea level about 30 metres, and is now a dolomite limestone outcrop which was eroded in classic karst style into pinnacles up to 20 metres high. To at least a depth of 55 metres below sea level, the limestone has been dissolved forming cavities, sinkholes and caves. Holes on the topside of the island were filled up by a phosphate layer up to several metres thick.

Anibare Bay was formed by the underwater collapse of the east side of the volcano.
Buada Lagoon was formed by solution of the limestone when the sea level was lower, followed by collapse.

Nauru is moving at 104 mm per year to the north west along with the Pacific Plate.

Freshwater can be found in Buada lagoon, and also in some brackish ponds at the escarpment base in Ijuw and Anabar in the northeast. There is an underground lake in Moqua Cave in the southeast of the island.


Nauru is party to the international environmental agreements on biodiversity, climate change, desertification, law of the sea and marine dumping.

This is a list of the extreme points of Nauru, the points that are farther north, south, east or west than any other location.




</doc>
<doc id="21305" url="https://en.wikipedia.org/wiki?curid=21305" title="Demographics of Nauru">
Demographics of Nauru

The demographics of Nauru, an island country in the Pacific Ocean, are known through national censuses, which have been analysed by various statistical bureaus since the 1920s. The Nauru Bureau of Statistics have conducted this task since 1977—the first census since Nauru gained independence in 1968. The most recent census of Nauru was in 2011, when population had reached ten thousand. The population density is 478 inhabitants per square kilometre (185 per square mile), and the overall life expectancy is 59.7 years. The population rose steadily from the 1960s until 2006 when the Government of Nauru repatriated thousands of Tuvaluan and I-Kiribati workers from the country. Since 1992, Nauru's birth rate has exceeded its death rate; the natural growth rate is positive. In terms of age structure, the population is dominated by the 15–64-year-old segment (65.6%). The median age of the population is 21.5, and the estimated gender ratio of the population is 0.91 males per one female.

Nauru is inhabited mostly by Nauruans (93.6%), while minorities include I-Kiribati (1.8%), Chinese (1.5%) and other (3.1%). The demographic history of Nauru is marked by several migrations: the area was first inhabited by Micronesian people about 3,000 years ago. The first European to find the island was John Fearn in 1798. Then, the country was annexed by Germany in the 1888. The next was when Japanese occupied the island during World War II in the 1942. During this time, the Japanese deported several thousands of Nauruans to other islands. In the 1960s, the country gained independence, where the percentage of Nauruans started to increase. The most recent demographic switch was in the 2000s, when the government repatriated several non-Nauruan population from the country.

The Nauruan language is the official language of Nauru, but English is often used in the country. Nauruan is declared as the primary language of 95.3% of the population. The 2011 census revealed that 66.0% of the population spoke English and 11.9% another language. The main religions of Nauru are Nauru Congregational Church (35.71%) and Roman Catholic (32.96%). The literacy rate in Nauru is 96.5%. The proportion of the country's population aged 15 and over attaining academic degrees is one of the lowest in the world, reaching 7.9% in 2011. An estimated 10.7% of the gross domestic product (GDP) is spent on education. Nauru has a universal health care system, and in 2012, an estimated 7.5% of its GDP was spent on healthcare. Nauru has the highest obesity ranking in the world; 97 per cent of men and 93 per cent of women are obese. In 2006, the average net monthly income was A$2,597 (A$ in 2014). The most significant sources of employment are phosphate mining, banking industries, and various coconut products. In 2011, the unemployment rate was 23%. The 2011 census enumerated 1,647 total households, averaging 6.0 persons per house. Average urbanisation rate in Nauru is 100%.

With a population of ten thousand in 2011, Nauru ranks around 230th in the world by population. Its population density is 478 inhabitants per square kilometre (185 per square mile). The overall life expectancy in Nauru at birth is 59.7 years. The total fertility rate of 3.70 children per mother is one of the highest in the Oceania. The United Nations projects the population will stay around 10,000 in the 2020s, and the Nauru Bureau of Statistics estimates the population will increase to 20,000 in 2038.

In Nauru's history, there have been six major demographics changes. The island was first inhabited by Micronesian people roughly 3,000 years ago. The first European to find the island was John Fearn in 1798. In 1888, the country was annexed by Germany. The next demographic change came when Japanese occupied the island during World War II in the 1940s. During this time, the Japanese deported several thousands of Nauruans to other islands. The next major demographic change was in the 1960s; the country gained independence, where the percentage of Nauruans started to increase. The last major demographics change was in 2006 when the Government of Nauru repatriated almost all of the remaining Tuvaluan and I-Kiribati workers, following large scale reduction from the Republic of Nauru Phosphate Corporation (RONPhos) and government workers. The census of 2006 stated 9,233 people were in Nauru: down 2.13% per year from the previous census of 2002.

From 2002–11, there has been negative net migration, with an annual negative 109 net immigrants from 2006–11. In 2009 there were 1,820 arrivals and 1,736 departures, for a positive rate of 84 immigrants. This was the first time since collecting data in 2002, there was a positive rate. Data on arrivals and departures collected by the Nauruan Customs and Immigration Office is not available, so specific immigration data is unavailable. As of the 2011 census, 57% of the population over 15 years old were legally or "de facto" married, 35% were never married, while 7% were either widowed, separated, or divorced. There are 1,647 households in Nauru, making an average household size of 6.0 persons per household.

For births, deaths, and fertility rates, the Nauru Bureau of Statistics was used. For population, the United States Census Bureau's mid-year estimated were used. If a cell is shaded light green and a dagger stands beside a number, it indicates the estimate from "The World Factbook". In 2013, the number of births (366) and birth rate (38.8) was the second-highest during this period. In 2011, the total fertility rate of 4.2 was the highest since 1992 (4.5). Since 2009, there has been a natural change of at least 200 inhabitants—the first since the reparations of the population in 2006.

Nauru, as of 2011, is mainly inhabited by Nauruans (94%), while the main minority groups include Fijians (1%), Chinese (1%), and Solomon Islanders (1%). This shows a major change from the previous major census of 2002, when Nauruans represented 75% of the population. According to the Constitution Nauru does not exclude any ethnic group to become a citizen.

The Nauruan language is the official language of Nauru. English is widely understood and is used for most government and commercial purposes. According to the 2011 census, 95.3% of the population speaks Nauruan, 66.0% speak English, and 11.9% speak another language. Nauruan is an Austronesian language, however, no adequate written grammar of the language has been compiled, and its relationships to other Micronesian languages are not well understood.

The main religions in Nauru are Nauru Congregational (35.71%), Roman Catholic (32.96%), Assemblies of God (12.98%), and Nauru Independent (9.50%). The biggest changes from 2002–11 were an increase from 0 to 1,291 (Assemblies of God) and 1,417 to 282 (Other). Public holidays include New Year's Day (1 January), Independence Day (31 January), Good Friday, Easter Monday, Easter Tuesday, Constitution Day (17 May), National Youth Day (25 September), Christmas Day, and Boxer Day.

Nauruan Independent was the predominant religion in Nauru before the late nineteenth and early twentieth centuries, when foreign missionaries introduced Christianity to the island. It is still practised by 9.5% of the population, according to 2011 census. There are a few active Christian missionary organisations, including representatives of Anglicanism, Methodism, and Catholicism. The Constitution provides for freedom of religion; however, the Government restricted this right in some circumstances. The government has restricted the religious practices of The Church of Jesus Christ of Latter-day Saints and the Jehovah's Witnesses, most of whom are foreign workers employed by RONPhos.

Literacy rate in Nauru, defined as "people who are currently enrolled in school and/or have reached at least grade 5 of primary education", is 96.5%, as of 2011. There are 3,190 students and 104 teachers, as of 2013. The 2011 census stated 4 percent of the population aged 15 years or older have a primary education, 91 percent have a secondary education, and 5 percent have a tertiary education. Education is based on the British system, which requires attendance from 5 to 16 years old. Students spend three years at preschool, six years of primary education, and four years of secondary school. Tertiary school is not required. An estimated 10.7 percent of the GDP was spend on education in 1999. As of 2013, there are five preschools, four primary schools, three secondary schools, and one tertiary school. The lone college, University of South Pacific, opened in the 1970s via distance courses, and in 1987 a campus was built on the island. It offers accounting, management, primary education, and English studies as majors.

The education system had a near-collapse in 2000–2005. During this time, exams were not held, teachers were not paid, and schools did not have enough funding to continue. As a result, over half of the schools closed. In 2009, the Australian Government partnered with the Nauruan Department of Education to help. This agreement resulted in a 5.7% increase in students, teachers with a degree increased from 30% to 93%, and over A$11 million was used to construct a new secondary school.

Nauru has a universal health care system, which is provided to all citizens for free. There is a single hospital, the Republic of Nauru Hospital in Yaren. It offers basic medical and surgical care, along with radiological, laboratory, pharmacy, and dental services. The Nauru Public Health Centre offers treatments for diabetes and other obesity-related diseases. This is the only form of specialized medical care in Nauru. Patients who require higher treatment are flown to Australia. In 2012, an estimated 7.5% of its GDP was spent on healthcare. As of 2010, there are 50 hospital beds. In 2004, there were 149 physicians and 557 nurses per 100,000 people. In 2012, it was estimated 26.2% and 22.1% of the population under 15 years of age consumed tobacco and cigarettes, respectively. "The World Factbook" estimates the population has a life expectancy of 66.4 years at birth, which ranks Nauru 169th in the world.

In 2013, there were 366 live births: 193 male and 173 female. The average age of mothers at first birth, as of a 2007 estimate, was 22.1 years. General fertility rate, i.e. numbers of births per 1,000 women aged 15–49 is 105. In 2011, 75 people died in Nauru. Cardiovascular disease (44%) and cancer (10%) were the primary causes of deaths in 2002. In the 2007–11 period, the birth rate was 27.2, exceeding the death rate of 7.5. The estimated infant mortality rate was 8.21 deaths per live births in 2014. In terms of age structure, the population of Nauru is dominated by the 15–64 rate (65.6%), while the size of the population younger than 15 and older than 64 is relatively small (32.5% and 1.8% respectively). The median age of the population is 21.5. The sex ratio of the population is 0.91 males per 1 female. Nauru has the highest obesity ranking in the world, 97 per cent of men and 93 per cent of women are obese. In 2011, the average body mass index was around 34.5, which is greater than obese (30+). According to "The Independent" the main reason for the obesity is result of the importation of Western foods.

Net monthly income in 2006 averaged A$2,597 (A$ in 2014). In the same year, gross monthly income averaged A$9,554 (A$ in 2014). This was calculated during the mini-census of 2006, which featured 54.4% response rate of the population. The income was calculated using the following factors: first job salary, subsistence, other business income, second job salary, services to other households, benefits, house gifts consumed and received, and other income. Compared to other countries that use the Australian dollar—Kiribati, Australia, and Tuvalu—Nauru ranks number one in terms of income. Since 2013, Nauru does not have a minimum wage.

Nauru's number of employed people has steadily risen and fallen. According to the 2011 census, there are 908 employed persons and 2,883 unemployed persons, making an unemployment rate of 23%. The Nauru Bureau of Statistics predicted the unemployment rate will decrease to 22% in FY2014/15. The gross domestic product of Nauru was A$69.55 million in 2009, an increase of 40% increase from 2008. The GDP is broken down into three categories: primary (18.7%—agriculture, hunting, forestry, fishing, mining, and quarrying), secondary (36.5%—manufacturing, electric, gas, water, and construction), and tertiary (44.8%—trade, hotel, restaurants, and various services) industries.

A majority of the population are employed in phosphate mining, public administration, education, and transportation. A detention centre was closed in 2008, which caused the unemployment rate to rise to 30%, and approximately ten percent of the population relied on working at the centre. The centre reopened in 2012 and currently serves 1,162 prisoners, as of May 2014. During the 1990s, Nauru was famous for operating offshore banks, helping with money laundering. The United States State Department's International Narcotics Control Strategy Report estimated there were 400 offshore finance centres laundering an estimated $70 billion.

Phosphate mining in Nauru originally made Nauru the richest per capita nation in the world. In 1968, the Nauru Phosphate Royalties Trust (NPRT) was created to invest profits from mining, so Nauru would have money after the mining was exhausted. The owners of the trust purchased a fleet of ships and aircraft, a brewery in the Solomon Islands, hotels around the world, and real estate in Australia, the United States and Britain, which causes the trust to bankrupt. Phosphate exports peaked in 1973 with 2.3 million tonnes, but has decreased to 0.2 million tonnes in 2001. In 2006, mining of a secondary layer of phosphate began.



</doc>
<doc id="21306" url="https://en.wikipedia.org/wiki?curid=21306" title="Politics of Nauru">
Politics of Nauru

Politics of Nauru takes place in a framework of a parliamentary representative democratic republic, whereby the President of Nauru is the head of government of the executive branch. Legislative power is vested in both the government and the parliament. The Judiciary is independent of the executive and the legislature.

Nauru's economic viability has historically rested on its phosphate reserves. Phosphate — in actuality a resource derived from a 1,000-year cycle of bird droppings—has been mined on the island since 1906. In the 20th century, the small Pacific nation generated healthy revenues from this lucrative—but finite—resource.

The phosphate supply has virtually all been exhausted in recent years and as such, the future of the people of Nauru is uncertain. The challenge for the country's policy makers will be to determine a path of continued economic prosperity, without the benefits of this resource.

In this regard, the government has tried to develop the island into an offshore financial centre, imitating the success of the Bahamas and other island nations around the world that have emerged as major offshore banking centres. The government has also invested in property on other islands and the United States through its Nauru Phosphate Royalties Trust.

Over the course of recent years, however, offshore banking institutions and instruments have come under increasing scrunity by international bodies seeking to make international finance a more transparent system. Nauru, as a result, has been a casualty of this movement.

In December 1999, four major international banks banned dollar transactions with Nauru. The United States Department of State issued a report identifying Nauru as a major money laundering centre, used by narcotics traffickers and organised crime figures.

The last few years has seen repeated changes of government. Nauru's unsettled political situation never led to civil disturbances; the transitions were always sanctioned by parliament and occurred peacefully.

President Bernard Dowiyogo took office in April 2000 for his fourth and, after a minimal hiatus, fifth stints as Nauru's top executive. Dowiyogo first served as president from 1976 to 1978. He returned to that office in 1989, and was re-elected in 1992. A vote in parliament, however, forced him to yield power to Kinza Clodumar in 1995. Dowiyogo regained the presidency when the Clodumar government fell in mid-1998.

In April 2000, René Harris, former chairman of the Nauru Phosphate Corporation, became president as he briefly assembled support in parliament. Harris' attempt to put together an administration lasted for only a few days of parliamentary maneuvering. In the end, Harris proved unable to secure parliament's confidence, and Dowiyogo returned yet again to the presidency by the end of the month.

Rene Harris was finally able to claim power as the president of Nauru in March 2001 when he was elected to the presidency by the parliament; his term was to last three years, presumably ending in 2004.

Phosphate depletion will likely be one of the most important considerations for the government in the next few years as the supply is forecast to be exhausted by 2003. Since Nauru imports almost everything it consumes (including food, water and fuel) the need to diversify the economy and to generate other sources of revenue is of paramount importance.

As noted above, offshore banking has been one arena into which Nauru has traversed, however, the rewards are limited by growing concern about the ethical parameters of this business. Tourism is another industry that is also being gradually built.

Yet another concern is the ecological damage that resulted from a century of phosphate mining. Along with the United Kingdom, Australia and New Zealand were responsible for the large scale and indiscriminate mining of phosphate on the tiny island for most of the 20th century.

The mining left an ecological and economic disaster for Nauru to handle when the country achieved independence in 1968. Not only was the country's principal resource and employment generating activity almost entirely depleted by the rapid mining done by the three countries, the mining companies had also failed to follow the basic principles of restoring and regenerating the lands where mining had been completed. Thus, Nauru was left to handle the immense and expensive task of restoring large chunks of land which were destroyed by the mining.

Nauru demanded compensation from the three nations, but was refused. Finally, in 1993, Nauru was forced to turn to the International Court of Justice at The Hague in The Netherlands. It filed a claim of $73 million against the three countries. The case was soon afterwards settled out of court by Australia, with Britain and New Zealand also contributing to the reparations sought by Nauru.

Today, Nauru is almost totally dependent on trade with New Zealand, Australia and Fiji. Arable land is very limited as are all other natural resources, now that its long-time economic base of phosphate mines has been almost completely depleted.

On the international front, in late July 2002, Taiwan cut its diplomatic ties with Nauru. Taiwan and Nauru had shared diplomatic ties for 22 years; Taiwan has enjoyed diplomatic ties with several Pacific countries even in the face of the "One China policy" by Beijing. Nevertheless, this particular 22-year-long legacy was broken when Nauru's president decided to change its allegiance and establish formal relations with China. The move effectively shifted diplomatic recognition from Taipei to Beijing, thus angering the government of Taiwan, which described the shift in policy as "reckless."

Nauru's decision to recognise Beijing via the signing of diplomatic papers and a joint communque ultimately resulted in the cessation of Taiwanese aid. Nauru instead received a US$150 million aid package from Beijing.

In April 2005, during a state visit to the Marshall Islands, ROC President Chen Shui-bian met and spoke with the Nauruan President Ludwig Scotty. On 14 May 2005, the two countries signed the necessary documents to restore formal ties and reopen embassies. The People's Republic of China consequently severed ties two weeks later on 31 May.

In early 2003, a fight for power emerged between President Rene Harris and former President Bernard Dowiyogo. The power struggle occurred following a non-confidence vote in parliament, which effectively ejected Harris from the position of president. Reports suggested that Harris was ousted because of rising anxieties regarding economic mismanagement. At the time, Dowiyogo referred to Nauru's political scenario as being "critical."

It was reported that Dowiyogo became the president replacing Harris, however, information surrounding the shift in power was sparse. There was very little international coverage of the matter. Regardless, Dowiyogo's tenure did not last for long. In March 2003, Dowiyogo had heart surgery in the United States and died.

In May 2003, elections were held within the parliament to select a new president. In those elections, Ludwig Scotty gained the most support and became the new president. The actual results of the parliamentary vote were as follows: Ludwig Scotty—10 parliamentary votes, Kinza Clodumar—7 parliamentary votes. President Scotty became president on 29 May 2003. He served only until August 2003 when he was ousted in a non-confidence measure. Rene Harris was elected as president.

Meanwhile, in parliamentary elections held in May 2003, Nauru First Party won 3 seats and independents garnered 15 in total.

In late June 2004, Nauru's former parliament speaker Ludwig Scotty became the country's new president. His presidency followed the exit of outgoing President Rene Harris following yet another non-confidence measure.

For his part, Scotty had resigned as parliamentary speaker in April 2004 in protest of the Nauru's financial crisis which included the commencement of receivership proceedings by corporate giant, General Electric. During that period, Nauru faced the seizure of its assets if the country failed to honor its debt payments.

Since Scotty's resignation as parliamentary speaker, the parliament was unable to convene as members of parliament could not decide whom to appoint as his replacement. The scenario led to a political crisis, the financial crisis notwithstanding.

In mid-2004, the government of Australia sent envoys to help Nauru deal with its financial crisis. By August 2004, a report by the Australian Centre for Independent Studies suggested that Nauru might consider relinquishing its independent status in favour of becoming an Australian territory. The report called for radical economic reform as well as the restructuring of both governmental instruments and public service. The author of the report has offered Nauru economic advice in the past.

Scotty was re-elected to serve a full term in October 2004. Following a vote of "no confidence" by Parliament against President Scotty on 19 December 2007, Marcus Stephen became the President. Following Stephen’s resignation in November 2011, Freddie Pitcher became President. Sprent Dabwido then moved a motion of no confidence in Pitcher, and Dabwido was duly elected President by the parliament, with nine votes supporting his nomination and eight votes opposing.

Elections for Parliament were held in 2013, after which Baron Waqa was elected by Parliament as President.

In January 2014, Nauru's President Baron Waqa fired the country's only magistrate Peter Law and its Chief Justice Geoffrey Eames (both Australian-based justices). Eames, himself, was fired after issuing an injunction to temporarily halt Law's deportation.

In May and June 2014, Waqa suspended 5 of the 7 members of Nauru's Opposition from Parliament indefinitely.

Three of the MPs, Mathew Batsiua, Kieren Keke and Roland Kun, were suspended in May 2014 for making comments to international media critical of the government and the alleged breakdown of the rule of law. Another two, Sprent Dabwido (a former president) and Squire Jeremiah were suspended a month later for behaving in an unruly manner.

In June 2015, Jeremiah, Dabwido, and Batsiua were arrested and Kun had his passport cancelled amid claims that they had been trying to destabilise the Government by talking to foreign media.

The Parliament elects a president from amongst its members, who appoints a Cabinet of 5–6 people. The President is both the head of state and head of government.

A series of no-confidence votes, resignations and elections between 1999 and 2003 saw René Harris and Bernard Dowiyogo as President for numerous short periods during a period of political instability. Dowigoyo died in office on 10 March 2003, in Washington, D.C., after heart surgery. Ludwig Scotty was elected President on 29 May 2003, but this did not bring to an end the years of political uncertainty as he was replaced by Harris a few months later. Scotty regained the presidency in 2004, only to be ousted in a vote of no confidence in 2007.

Parliament has 19 members, elected for a three-year term in multi-seat constituencies. Each constituency returns 2 members to the Nauruan Parliament, except for Ubenide which returns 4.
Voting is compulsory for all citizens aged 20 or more.

Nauru does not have a formal structure for political parties; candidates typically stand as independents. 15 of the 18 members of the current parliament are independents, and alliances within the government are often formed on the basis of extended family ties. Four parties that have been active in Nauruan politics are the Nauru Party, the Democratic Party, Nauru First and the Centre Party.
For its size, Nauru has a complex legal system. The Supreme Court, headed by the Chief Justice, is paramount on constitutional issues. Other cases can be appealed to the two-judge Appellate Court. Parliament cannot overturn court decisions, but Appellate Court rulings can be appealed to the High Court of Australia; in practice, this rarely happens. Lower courts consist of the District Court and the Family Court, both of which are headed by a Resident Magistrate, who also is the Registrar of the Supreme Court. Finally, there also are two quasi-courts: the Public Service Appeal Board and the Police Appeal Board, both of which are presided over by the Chief Justice.

Since 1992, local government has been the responsibility of the Nauru Island Council (NIC). The NIC has limited powers and functions as an advisor to the national government on local matters. The role of the NIC is to concentrate its efforts on local activities relevant to Naurans. An elected member of the Nauru Island Council cannot simultaneously be a member of parliament. Land tenure in Nauru is unusual: all Naurans have certain rights to all land on the island, which is owned by individuals and family groups; government and corporate entities do not own land and must enter into a lease arrangement with the landowners to use land. Non-Nauruans cannot own lands.

Australia is responsible for Nauru's defence under an informal agreement between the two countries. However, there is a 100-person regular police force under civilian command, backed by volunteer reservists trained to provide support in the event of serious unrest. While officers are typically unarmed while on routine patrol, the Nauruan police force does possess 23 firearms.</small>




</doc>
<doc id="21307" url="https://en.wikipedia.org/wiki?curid=21307" title="Economy of Nauru">
Economy of Nauru

The economy of Nauru is tiny, based on a population in 2014 of only 11,000 people. The economy is primarily based on phosphate mining, offshore banking, and processing of coconut products. Mining of phosphate ceased after the exhaustion of the primary phosphate reserves, but in 2006–07 mining of a deeper layer of "secondary phosphate" began. It is hoped that this economic activity might lift Nauru from the bottom rung of global GDP per capita. The only other major source of government revenue is sale of fishing rights in Nauru's territorial waters.

Nauru is dependent on foreign aid, chiefly from Australia, Taiwan and New Zealand.

In the years after independence in 1968, Nauru possessed the highest GDP per capita in the world due to its rich phosphate deposits. In anticipation of the exhaustion of its phosphate deposits, substantial amounts of the income from phosphates were invested in trust funds aimed to help cushion the transition and provide for Nauru’s economic future. However, because of heavy spending from the trust funds, including some wasteful foreign investment activities, the government is now facing virtual bankruptcy. To cut costs the government has called for a freeze on wages, a reduction of over-staffed public service departments, privatization of numerous government agencies, and closure of some of Nauru's overseas consulates. Economic uncertainty caused by financial mismanagement and corruption, combined with shortages of basic goods, has resulted in some domestic unrest. In 2004 Nauru was faced with chaos amid political strife and the collapse of the island’s telecommunications system. Moreover, the deterioration of housing and hospitals has continued.

Few comprehensive statistics on the Nauru economy exist, with estimates of Nauru's GDP varying widely. According to the U.S. State Department, Nauru’s GDP volume was US$1 million in 2004. Nauru receives about US$20 million foreign aid a year from Australia.

The most recent 2017/2018 Nauru Budget indicates modest economic growth, with $128.7 million in revenues and $128.6 million in expenditures estimated.
Phosphate is Nauru’s only export product, although the government also receives relatively significant foreign exchange income from licensing its rich skipjack tuna fishing grounds to foreign fishing vessels, which land an annual average of 50,000 tonnes of Nauru zone-caught tuna overseas. In 2004 income from phosphate export was US$640,000, with Australia, New Zealand and Japan serving as the country's major export markets. In the same year the Nauru government budget shows that income from licensing foreign fishing vessels was over US$3,000,000.

Nauru needs to import almost all basic and capital goods, including food, water, fuel, and manufactured goods, with Australia and New Zealand as its major import sources. In 2004 Nauru’s imports totaled about US$19.8 million.

Nauru has been a cash economy since at least 2004, after the Bank of Nauru and the Republic of Nauru Finance Corporation went bankrupt and ceased operations in the early 2000s and the licenses of all offshore banks were revoked by the Nauru government in 2004. Nauru uses the Australian dollar for its currency. Deposit-taking institutions do not exist on the island, and community savings are required to be held in the form of cash, and all transactions are conducted using cash. The government is required to periodically fly in Australian currency to maintain liquidity.

The Australian government shut down Nauru’s banking system in 2006. Nauru’s government has been talking with Bendigo & Adelaide Bank Ltd., Australia’s fifth-largest lender, to set up a branch on the island and move to a system where the entire population uses a bank. Effective from the end of April 2016, Westpac, one of Australia's largest banks, ceased having any dealings with the Nauru government. On 21 April 2016, it was announced that the Bendigo Bank was facing pressure to also close its operation in Nauru. There are indications that the Australian banks have legal problems in Australia over efforts to combat money laundering.

On October 1, 2014, an income tax was imposed in Nauru for the first time, with high income earners paying a flat rate of 10%. The government spending in 2015 was forecast to be under US$92 million. Taxes include an airport departure tax and a bed tax at the Meneñ Hotel. The 2007–08 Budget saw the increase of existing excises on cigarettes and duty on imports. A tax on sugary foods was also introduced, chiefly to help combat Nauru's diabetes epidemic.

Historically Nauru was regarded as a tax haven due to the operation of its international financial centre, which offered amongst other things offshore banking services. In 2001, Nauru was blacklisted internationally over concerns it had become a haven for money laundering. Amendments made in 2004 abolished Nauru's Offshore Banking sector and, as recognised in Nauru's latest anti money laundering and countering the financing of terrorism (AML/CFT) review, Nauru's offshore sector is now limited to a small offshore company register.

In July 2017 the Organisation for Economic Co-operation and Development (OECD) upgraded its rating of Nauru's standards of tax transparency. Nauru had been listed alongside fourteen other countries that had failed to show that they could comply with international tax transparency standards and regulations. The OECD subsequently put Nauru through a fast-tracked compliance process and the country was given a "largely compliant" rating.

Currently, Nauru is heavily dependent on Australia as its major source of financial support. In 2001 Nauru signed an agreement with Australia to accommodate asylum seekers (mostly from Iraq and Afghanistan) on the island, in return for millions of dollars in aid. This agreement, referred to as the "Pacific Solution", came to an end in 2007, prompting Nauruan concerns about the future of the island's revenue. Australia has also sent financial experts to Nauru to help the tiny nation overcome its economic problems. However, serious questions remain about the long-term viability of Nauru’s economy, with uncertainties about the rehabilitation of mined land and the replacement of income from phosphates.

In 2008, talks began between Australia and Nauru regarding the future of the former's economic development aid to the latter. Nauruan Foreign and Finance Minister Dr. Kieren Keke stated that his country did not want aid handouts. One possible solution currently being explored would be for Australia to assist Nauru in setting up a "boat repair industry" for regional fishing vessels.

The Nauru detention centre was established by the Australian government, with Nauruan agreement, in 2001 to cater for up to 800 refugees and asylum seekers under Australia's Pacific solution. The centre is seen by Nauruans as an important source of employment opportunities, in addition to the pledge of A$20 million for development activities.


The fiscal year runs from July 1 to June 30.






</doc>
<doc id="21308" url="https://en.wikipedia.org/wiki?curid=21308" title="Telecommunications in Nauru">
Telecommunications in Nauru

Nauru has one government-owned radio station and two television stations. One station is government-owned and mainly rebroadcasts CNN and the other is a private sports network. The island's Internet service is provided by CenPacNet.

Telephones - main lines in use:
2,000 (1994)

Telephones - mobile cellular:
450 (1994)

Telephone system:
adequate local and international radiotelephone communications provided via Australian facilities
<br>"domestic:"
NA
<br>"international:"
satellite earth station - 1 Intelsat (Pacific Ocean)

Radio broadcast stations:
AM 0, FM 1, shortwave 0 (1998) The AM signal went off the air years ago, the government owned radio station is "FM105"

Radios:
7,000 (1997)

Television broadcast stations:
1 (1997)

Televisions:
500 (1997)

Internet Service Providers (ISPs):
1 (CenPacNet)

Country code (Top level domain): NR

In 2015, the Nauruan government blocked websites including Facebook as part of a crackdown on Internet pornography, especially child pornography. Opposition MP Matthew Batsiua has said that the Facebook ban is actually intended to stifle criticism of the government.


</doc>
<doc id="21309" url="https://en.wikipedia.org/wiki?curid=21309" title="Transport in Nauru">
Transport in Nauru

Transportation in Nauru includes pedestrian, bicycle, automobile, train, and airplane.

Nauru has one airport, Nauru International Airport. Nauru Airlines, which flies to Brisbane, Australia; Majuro, Marshall Islands; Nadi, Fiji; and Tarawa, Kiribati, is the only airline to fly to the airport. There are five airplanes in service.

Rail transport is used for moving phosphate from the island's interior to the cantilever jetties on the island's western coast, in Aiwo District. For this purpose, a 3.9 km long, narrow gauge railway was built by the Pacific Phosphate Company in 1907.


</doc>
<doc id="21312" url="https://en.wikipedia.org/wiki?curid=21312" title="Navassa Island">
Navassa Island

Navassa Island (; ; also "La Navasse", "La Navase") is a small, uninhabited island in the Caribbean Sea. The island is subject to an ongoing territorial dispute between Haiti and the United States, which administers it through the U.S. Fish and Wildlife Service. The U.S. has claimed the island since 1857, based on the Guano Islands Act of 1856. Haiti's claim over Navassa goes back to the Treaty of Ryswick in 1697 that established French possessions in Hispaniola, that were transferred from Spain by the treaty. As well as the western half of the main island and certain other specifically named nearby islands, Haiti's also claimed "other adjacent ("but unnamed") islands". Navassa was not one of the named islands. Since its 1874 Constitution, and after the establishment of the 1857 U.S. claim, Haiti has explicitly named "la Navase" as one of the territories it claims.

Navassa Island is about in area. It is located west of Haiti's southwest peninsula, south of the US naval base at Guantanamo Bay, Cuba, and about one-quarter of the way from mainland Haiti to Jamaica in the Jamaica Channel.

Navassa reaches an elevation of at Dunning Hill south of the lighthouse, Navassa Island Light. This location is from the southwestern coast or east of Lulu Bay.

The terrain of Navassa Island consists mostly of exposed coral and limestone, the island being ringed by vertical white cliffs high, but with enough grassland to support goat herds. The island is covered in a forest of four tree species: short-leaf fig ("Ficus populnea" var. "brevifolia"), pigeon plum ("Coccoloba diversifolia"), mastic ("Sideroxylon foetidissimum"), and poisonwood ("Metopium brownei").

Navassa Island's topography, ecology, and modern history are similar to that of Mona Island, a small limestone island located in the Mona Passage between Puerto Rico and the Dominican Republic, which were once centers of guano mining, and are nature reserves for the United States. Transient Haitian fishermen and others camp on the island but the island is otherwise uninhabited. It has no ports or harbors, only offshore anchorages, and its only natural resource is guano. Economic activity consists of subsistence fishing and commercial trawling activities.

There were eight species of native reptiles, all of which are believed to be, or to have been, endemic to Navassa Island: "Celestus badius" (an anguid lizard), "Aristelliger cochranae" (a gecko), "Sphaerodactylus becki" (a gecko), "Anolis longiceps" (an anole), "Cyclura (cornuta) onchiopsis" (a rock iguana), "Leiocephalus eremitus" (a curly-tailed lizard), "Tropidophis bucculentus" (a dwarf boa), and "Typhlops sulcatus" (a tiny snake). Of these the first four remain common with the last four likely extinct. Feral cats, dogs and pigs currently inhabit the island.

In 2012, a rare coral species ("Acropora palmata" - Elkhorn coral) was found underwater near Navassa Island; its population has been reduced by 98%. The coral was found to be in good condition.

In 1504, Christopher Columbus, stranded on Jamaica during his fourth voyage, sent some crew members by canoe to Hispaniola for help. They ran into the island on the way, but it had no water. They called it Navaza (from "nava-" meaning plain, or field), and it was avoided by mariners for the next 350 years.

From 1801 to 1867 the successive constitutions of Haiti claimed national sovereignty over adjacent islands, both named "and" unnamed, although Navassa was not specifically enumerated until 1874. Despite this implicit claim, Navassa Island was claimed for the United States on September 19, 1857, by Peter Duncan, an American sea captain, under the Guano Islands Act of 1856, for the rich guano deposits found on the island, and for not being within the lawful jurisdiction of any other government, nor occupied by another government's citizens.

Haiti protested the annexation, but on July 7, 1858, U.S. President James Buchanan issued an Executive Order upholding the American claim, which also called for military action to enforce it. Navassa Island has since been maintained by the United States as an unincorporated territory (according to the Insular Cases). The United States Supreme Court on November 24, 1890, in "Jones v. United States", 137 U.S. 202 (1890) Id. at 224 found that Navassa Island must be considered as appertaining to the United States, creating a legal history for the island under US law unlike many other islands originally claimed under the Guano Islands Act. As listed in its 1987 constitution, Haiti maintains its claim to the island.

Guano phosphate is a superior organic fertilizer that became a mainstay of American agriculture in the mid-19th century. Duncan transferred his discoverer's rights to his employer, an American guano trader in Jamaica, who sold them to the newly formed Navassa Phosphate Company of Baltimore. After an interruption for the American Civil War, the company built larger mining facilities on Navassa with barrack housing for 140 black contract laborers from Maryland, houses for white supervisors, a blacksmith shop, warehouses, and a church.

Mining began in 1865. The workers dug out the guano by dynamite and pick-axe and hauled it in rail cars to the landing point at Lulu Bay, where it was put into sacks and lowered onto boats for transfer to the Company barque, the S.S. "Romance". The living quarters at Lulu Bay were called "Lulu Town", as appears on old maps. Railway tracks eventually extended inland.

Hauling guano by muscle-power in the fierce tropical heat, combined with general disgruntlement with conditions on the island, eventually provoked a rebellion in 1889, in which five supervisors died. A U.S. warship returned eighteen of the workers to Baltimore for three separate trials on murder charges. A black fraternal society, the Order of Galilean Fisherman, raised money to defend the miners in federal court, and the defense built its case on the contention that the men acted in self-defense or in the heat of passion, and that the United States did not have jurisdiction over the island.

The cases, including "Jones v. United States", went to the U.S. Supreme Court in October 1890, which ruled the Guano Act constitutional, and three of the miners were scheduled for execution in the spring of 1891. A grass-roots petition driven by black churches around the country, also signed by white jurors from the three trials, reached President Benjamin Harrison, who commuted the sentences to imprisonment and mentioned the case in a State of the Union Address. Guano mining resumed on Navassa at a much reduced level. The Spanish–American War of 1898 forced the Phosphate Company to evacuate the island and file for bankruptcy, and the new owners abandoned the island after 1901.

Navassa became significant again with the opening of the Panama Canal in 1914. Shipping between the American eastern seaboard and the Canal goes through the Windward Passage between Cuba and Haiti. Navassa, a hazard to navigation, needed a lighthouse. The U.S. Lighthouse Service built Navassa Island Light, a tower on the island in 1917, above sea level. A keeper and two assistants were assigned to live there until the Lighthouse Service installed an automatic beacon in 1929.

After absorbing the Lighthouse Service in 1939, the U.S. Coast Guard serviced the light twice each year. The U.S. Navy set up an observation post for the duration of World War II. The island has been uninhabited since then. Fishermen, mainly from Haiti, fish the waters around Navassa.

A scientific expedition from Harvard University studied the land and marine life of the island in 1930. After World War II amateur radio operators occasionally visited to operate from the territory, which is accorded "entity" (country) status by the American Radio Relay League. The callsign prefix is KP1. From 1903 to 1917, Navassa was a dependency of the U.S. Guantanamo Bay Naval Base, and from 1917 to 1996 it was under United States Coast Guard administration.

In 1996 the Coast Guard dismantled the light on Navassa, which ended its interest in the island. Consequently, the Department of the Interior assumed responsibility for the civil administration of the area, and placed the island under its Office of Insular Affairs. For statistical purposes, Navassa was grouped with the now-obsolete term United States Miscellaneous Caribbean Islands and is now grouped with other islands claimed by the U.S. under the Guano Islands Act as the United States Minor Outlying Islands.

In 1997 an American salvager made a claim to Navassa to the Department of State based on the Guano Islands Act. On March 27, 1997, the Department of the Interior rejected the claim on the basis that the Guano Islands Act applies only to islands which, at the time of the claim, are not "appertaining to" the United States. The department's opinion said that Navassa is and remains a U.S. possession "appertaining to" the United States and is "unavailable to be claimed" under the Guano Islands Act.

A 1998 scientific expedition led by the Center for Marine Conservation in Washington, D.C. described Navassa as "a unique preserve of Caribbean biodiversity." The island's land and offshore ecosystems have survived the 20th century virtually untouched.

In September, 1999, the United States Fish and Wildlife Service established the Navassa Island National Wildlife Refuge, which encompasses of land and a 12 nautical mile (22.2 km) radius of marine habitat around the island. Later that year, full administrative responsibility for Navassa was transferred from the Office of Insular Affairs to the U.S. Fish and Wildlife Service.

Due to hazardous coastal conditions, and for preservation of species habitat, the refuge is closed to the general public. Visitors need permission from the Fish and Wildlife Office in Boquerón, Puerto Rico to enter its territorial waters or land.

Since it became a National Wildlife Refuge, amateur radio operators have repeatedly been denied entry. In October 2014 permission was granted for a two-week DX-pedition in February 2015. The operation made 138,409 contacts.





</doc>
<doc id="21323" url="https://en.wikipedia.org/wiki?curid=21323" title="History of Nepal">
History of Nepal

The history of Nepal has been influenced by its position in the Himalaya and its two neighbours, modern day India and China.

It is a multi-ethnic, multiracial, multicultural, multi-religious, and multilingual country. The most spoken language of Nepal is Nepali followed by several other ethnic languages.

Nepal experienced a struggle for democracy at times in the 20th century and early 21st century. During the 1990s and until 2008, the country was in a civil strife. A peace treaty was signed in 2006 and elections were held in the same year. In a historical vote for the election of the constituent assembly, Nepalese parliament voted to oust the monarchy in June 2006. Nepal became a federal republic and was formally renamed the 'Federal Democratic Republic of Nepal' ending the 200 year old Shah dynasty.


The derivation of the word Nepal is also the subject of a number of other theories:

Prehistoric site of palaeolithic, Mesolithic and Neolithic have been discovered in the Siwalik hills of Dang District. The earliest inhabitants of Nepal and adjoining areas were people from the Indus Valley Civilization. The Dravidian people whose history predates the onset of the Bronze Age in South Asia (around 3300 BC), before the coming of other ethnic groups like the Tibeto-Burmans and Indo-Aryans from across the border. Tharus, people of mixed Dravidian and Austro-Asiatic features are the forest-dwelling natives of the Cental Terai region of Nepal. The Kirat people arrived from Tibet some 2000 years ago and lived in northern Nepal. Other ethnic groups of Indo-Aryan origin later migrated to southern part of Nepal from India. From the ancient records Nepal was originally inhabited by the Mongoloid people. According to B.H. Hodgson in 1847 the earliest inhabitants of Nepal were probably the Kusunda people and were probably of Proto-Australoid origin. 

Very little is known about the early history of Nepal, legends and documented references reach far back to the 30th century BC:

The context of Kirats ruling in Nepal before Licchavi Dynasty and after Mahispal or Avir Dynasty can be found in different manuscripts. Mentioning the area between Sun Kosi and Tama Kosi as their native land, the list of Kirati kings is also given in the Gopal genealogy. By defeating the last king of Avir Dynasty Bhuwansingh in a battle, Kirati King Yalung or Yalamber had taken the regime of valley under his control. In Hindu mythological perspective, this event is believed to have taken place in the final phase of Dwaparyug or initial phase of Kaliyug or around the 6th century BC. We can find descriptions of 32, 28 and 29 Kirati kings according in Gopal genealogy, language-genealogy and Wright genealogy respectively. By means of the notices contained in the classics of the East and West, we are assured that Kiranti people was forth-coming in their present abode from 2000 to 2500 years back, and that their powers was great and their dominion extensive, reaching possibly at one time to the delta of the Ganges.

The kings of Lichhavi dynasty (originated from Vaishali of modern Bihar of India) have been found to rule Nepal after the Kirat monarchical dynasty. The context that 'Suryavansi Kshetriyas had established new regime by defeating the Kirats' can be found in some genealogies and Puranas. It has been written in Gopal genealogy that 'then, defeating the Kirat King with the impact of Suryavanshi, Lichhavi dynasty was established in Nepal'. Likewise, It has been written in Pashupati Purana that 'the masters of Vaishali established their own regime by confiding Kiratis with sweet words and defeating them in war. Similar contexts can be also found in 'Himbatkhanda'. That purana also mention that 'the masters of Vaishali had started ruling in Nepal by defeating Kirats'. In this way, Lichhavi's regime seems to have started in Nepal subsequently after the regime of Kirats. However, different genealogies have found to be stating different names of last Kirati King. The Lichhavi monarchical dynasty was established in Nepal by defeating last Kirati King 'Khigu', according to Gopal genealogy, 'Galiz' according to language-genealogy and 'Gasti', according to Wright genealogy.

The Simroun, Karnat or Dev Dynasty originated with an establishment of a kingdom in 1097 CE headquartered at present day Simroungarh in Bara District. The kingdom controlled the areas we today know as Tirhoot or Mithila in Nepal and Bihar of India. The rulers of Simroungarh are as follows:
In 1324 CE, Ghiyasuddin Tughlaq attacked Simroungarh and demolished the fort. The remains are still scattered all over the Simroungarh region. The king fled northwards into the then Nepal. The son of Harisingh Dev, Jagatsingh Dev married the widow princess of Bhaktapur Nayak Devi.

The Thakuri Dynasty was a Rajput Dynasty. After Aramudi, who is mentioned in the Kashmirian chronicle, the Rajatarangini of Kalhana (1150 CE), many Thakuri kings ruled over parts of the country up to the middle of the 12th century CE. Raghava Deva is said to have founded a ruling dynasty in 879 CE, when the Lichhavi rule came to an end. To commemorate this important event, Raghu Deva started the 'Nepal Era' which began on 20 October, 879 CE. After Amshuvarma, who ruled from 605 CE onward; the Thakuris had lost power and they could regain it only in 869 CE.

After the death of King Raghava Dev, many Thakuri kings ruled Southern Nepal up to the middle of the 12th century CE. During that period, Gunakama Deva was one of the famous kings. He ruled from 949 to 994 CE. During his rule, a big wooden house was built out of one single tree which was called 'Kasthamandapa', from which the name of the capital, 'Kathmandu', is derived. Gunakama Deva founded a town called Kantipur, the modern Kathmandu.
It was also Gunakama Deva who started the 'Indra Jatra' festival. He repaired the temple that lies to the northern part of the temple of Pashupatinath. He introduced Krishna Jatra and Lakhe Jatra as well. He also performed Kotihoma.

Bhola Deva succeeded Gunakama Deva. The next ruler was Laxmikama Deva who ruled from 1024 to 1040 CE. He built Laksmi Vihara and introduced the custom of worshipping a virgin girl as 'Kumari'. Then, Vijayakama Deva, the son of Laksmikama, became the Nepalese king. Vijaykama Deva was the last ruler of this dynasty. He introduced the worship of the "Naga" and "Vasuki". After his death, the Thakuri clan of Nuwakot occupied the throne of Nepal.

Bhaskara Deva, a Thakuri from Nuwakot, succeeded Vijayakama Deva and established Nuwakot-Thakuri rule. He is said to have built Navabahal and Hemavarna Vihara. After Bhaskara Deva, four kings of this line ruled over the country. They were Bala Deva, Padma Deva, Nagarjuna Deva and Shankara Deva.

Shankara Deva (1067–1080 CE) was the most illustrious ruler of this dynasty. He established the image of 'Shantesvara Mahadeva' and 'Manohara Bhagavati'. The custom of pasting the pictures of Nagas and Vasuki on the doors of houses on the day of Nagapanchami was introduced by him. During his time, the Buddhists wreaked vengeance on the Hindu Brahmins (especially the followers of Shaivism) for the harm they had received earlier from Shankaracharya. Shankara Deva tried to pacify the Brahmins harassed by the Buddhists.

Bama Deva, a descendant of Amshuvarma, defeated Shankar Deva in 1080 CE. He suppressed the Nuwakot-Thankuris with the help of nobles and restored the old Solar Dynasty rule in Nepal for the second time. Harsha Deva, the successor of Bama Deva was a weak ruler. There was no unity among the nobles and they asserted themselves in their respective spheres of influence. Taking that opportunity Nanya Deva, a Karnat dynasty king, attacked Nepal from Simraungarh. In reply Army of Nepal defended, won the battle and successfully protected Nepal from a foreign invasion.

After Harsha Deva, Shivadeva the third ruled from 1099 to 1126 CE. He was a brave and powerful king. He founded the town of Kirtipur and roofed the temple of Pashupatinath with gold. He introduced twenty-five paisa coins. He also constructed wells, canals, and tanks at different places.

After Sivadeva III, Mahendra Deva, Mana Deva, Narendra Deva II, Ananda Deva, Rudra Deva, Amrita Deva, Ratna Deva II, Somesvara Deva, Gunakama Deva II, Lakmikama Deva III and Vijayakama Deva II ruled Nepal in quick succession. Historians differ about the rule of several kings and their respective times. After the fall of the Thakuri dynasty, a new dynasty was founded by Arideva or Ari Malla, popularly known as the 'Malla Dynasty'.

Early Malla rule started with Ari Malla in the 12th century. Over the next two centuries, his kingdom expanded widely, into much of South Asia and western Tibet, before disintegrating into small principalities, which later became known as the Baise kingdoms.

Jayasthiti Malla, with whom commences the later Malla dynasty of the Kathmandu Valley, began to reign at the end of the 18th century. King Prithvi Narayan Shah unified Kathmandu at the day of Indra Jatra (festival). Malla Dynasty was the Longest ruling dynasty, ruling from the 12th century to the 18th century (about 600 years of ruling period). This era in the Valley is eminent for the various social and economic reforms such as the 'Sanskritization' of the Valley people, new methods of land measurement and allocation etc. In this Era, new Art and Architecture was introduced. The monuments in Kathmandu Valley which are listed by UNESCO these days were built during Malla rule. In the 14th century, before Kathmandu was divided into 3 princely states, Araniko went to China upon the request of Abhaya Malla for representing the skill of art and architecture, and he introduced Pagoda Style of architecture to China and subsequently, whole Asia. Yaksha Malla, the grandson of Jayasthiti Malla, ruled the Kathmandu Valley until almost the end of the 15th century. After his demise, the Valley was divided into three independent Valley kingdoms—Kathmandu, Bhaktapur and Patan—in about 1484 CE. This division led the Malla rulers into internecine clashes and wars for territorial and commercial gains. Mutually debilitating wars gradually weakened them, that facilitated conquest of the Kathmandu Valley by King Prithvi Narayan Shah of Gorkha. The last Malla rulers were Jaya Prakasha Malla, Teja Narasingha Malla and Ranjit Malla of Kathmandu, Patan, and Bhaktapur respectively.

Prithvi Narayan Shah (c. 1779–1875), with whom we move into the modern period of Nepal's history, was the ninth generation descendant of Dravya Shah (1559–1570), the founder of the ruling house of Gorkha. Prithvi Narayan Shah succeeded his father Nara Bhupal Shah to the throne of Gorkha in 1743 CE. King Prithvi Narayan Shah was quite aware of the political situation of the Valley kingdoms as well as of the Baise and Chaubise principalities. He foresaw the need for unifying the small principalities as an urgent condition for survival in the future and set himself to the task accordingly.

His assessment of the situation among the hill principalities was correct, and the principalities were subjugated fairly easily. King Prithvi Narayan Shah's victory march began with the conquest of Nuwakot, which lies between Kathmandu and Gorkha, in 1744. After Nuwakot, he occupied strategic points in the hills surrounding the Kathmandu Valley. The Valley's communications with the outside world were thus cut off. The occupation of the Kuti Pass in about 1756 stopped the Valley's trade with Tibet. Finally, King Prithvi Narayan Shah entered the Valley. After the victory of Kirtipur. King Jaya Prakash Malla of Kathmandu sought help from the British and so the than East India Company sent a contingent of soldiers under Captain Kinloch in 1767. The British force was defeated at Sindhuli by King Prithvi Narayan Shah's army. This defeat of the British completely shattered the hopes of King Jaya Prakash Malla. The capture of Kathmandu (September 25, 1768) was very dramatic. As the people of Kathmandu were celebrating the festival of Indrajatra, Prithvi Narayan Shah and his men marched into the city. A throne was put on the palace courtyard for the king of Kathmandu. Prithvi Narayan Shah sat on the throne and was hailed by the people as the king of Kathmandu. Jaya Prakash Malla somehow managed to escape with his life and took asylum in Patan. When Patan was too captured a few weeks later, both Jaya Prakash Malla and the king of Patan;Tej Narsingh Malla took refuge in Bhaktapur, which was also captured after some time.Thus, the Kathmandu Valley was conquered by King Prithvi Narayan Shah and Kathmandu became the capital of the modern Nepal by 1769.

In 1794, troops of Prithivi Narayan Shah has made a complete conquered on Nuwakot which they were commanded by Biraj Thapa, but they were badly defeated, largely because of the following reasons:

King Prithvi started unifying parts of Baise-Rajya in the Rapti region in around 1760CE. By 1763, Tulsipur-Dang Rajya fell and by 1775 CE, Chauhan Raja Nawal Singh of House of Tulsipur was completely defeated. After losing his northern hill territories to King Prithvi, Chauhan Raja Nawal Singh was forced to move to his southern territories (currently Tulsipur/Balarampur in modern-day India) and ruled as one of the largest Taluqdar of Oudh.

King Prithvi Narayan Shah was successful in bringing together diverse religio-ethnic groups under one nation. He was a true nationalist in his outlook and was in favor of adopting a closed-door policy with regard to the British. Not only his social and economic views guided the country's socio-economic course for a long time, his use of the imagery, 'a yam between two boulders' in Nepal's geopolitical context, formed the principal guideline of the country's foreign policy for future centuries. But in the modern days, this saying is to be modified as a 'link between two giant countries' and Nepal can be able to get benefit from both countries.

After decades of rivalry between the medieval kingdoms, modern Nepal was unified in the latter half of the 18th century, when Prithvi Narayan Shah, the ruler of the small principality of Gorkha, formed a unified country from a number of independent hill high states. Prithvi Narayan Shah dedicated himself at an early age to the conquest of the Kathmandu Valley and the creation of a single state, which he achieved in 1768.

The country was frequently called the Gorkha Kingdom. It is a misconception that the Gorkhali took their name from the Gorkha region of Nepal; actually, the region was given its name after the Gorkhali had established their control of these areas.

After Shah's death, the Shah dynasty began to expand their kingdom into much of South Asia. Between 1788 and 1791, during the Sino-Nepalese War, Nepal invaded Tibet and robbed Tashilhunpo Monastery in Shigatse. Alarmed, the Qianlong Emperor of the Chinese Qing Dynasty appointed Fuk'anggan commander-in-chief of the Tibetan campaign; Fuk'anggan signed treaty to protect his troops thus attaining a draw. The draw was later converted to victory by Nepali forces sent on commands of PM Jung Bahadur Rana.

After 1800, the heirs of Prithvi Narayan Shah proved unable to maintain firm political control over Nepal. A period of internal turmoil followed. Rivalry between Nepal and the British East India Company over the princely states bordering Nepal and British-India eventually led to the Anglo-Nepalese War (1814–16), in which Nepal suffered a heavy defeat. The Treaty of Sugauli was signed in 1816, ceding large parts of the Nepalese controlled territories to the British.
Jung Bahadur Rana was the first ruler from this dynasty. Rana rulers were titled "Shri Teen" and "Maharaja", whereas Shah kings were "Shri Panch" and "Maharajdiraj". Both the Rana dynasty and Shah dynasty are Rajput caste in the Hindu tradition. Jung Bahadur codified laws and modernized the state's bureaucracy. In the coup d'état of 1885, the nephews of Jung Bahadur and Ranodip Singh murdered Ranodip Singh and the sons of Jung Bahadur, stole the name of Jung Bahadur and took control of Nepal. Nine Rana rulers took the hereditary office of Prime Minister. All were styled (self proclaimed) Maharaja of Lamjung and Kaski. Slavery was abolished in Nepal in 1924 under premiership of Chandra Shamsher Jang Bahadur Rana.

The revolution of 1951 started when dissatisfaction against the family rule of the Ranas had started emerging from among the few educated people, who had studied in various South Asian schools and colleges, and also from within the Ranas, many of whom were marginalized within the ruling Rana hierarchy. Many of these Nepalese in exile had actively taken part in the Indian Independence struggle and wanted to liberate Nepal as well from the autocratic Rana occupation. The political parties such as "The Prajaparishad" and "Nepali Congress" were already formed in exile by leaders such as B. P. Koirala, Ganesh Man Singh, Subarna Sumsher Rana, Krishna Prasad Bhattarai, Girija Prasad Koirala, and many other patriotic-minded Nepalis who urged the military and popular political movement in Nepal to overthrow the autocratic Rana Regime. Thus Nepali congress formed a military wing Nepali Congress's Liberation Army Among the prominent martyrs to die for the cause, executed at the hands of the Ranas, were Dharma Bhakta Mathema, Shukraraj Shastri, Gangalal Shrestha, and Dasharath Chand who were the members of Praja Parisad. This turmoil culminated in King Tribhuvan, a direct descendant of Prithvi Narayan Shah, fleeing from his "palace prison" in 1950, to India, touching off an armed revolt against the Rana administration. This eventually ended in the return of the Shah family to power and the appointment of a non-Rana as prime minister according to a tri-partite agreement signed called 'Delhi Compromise'. A period of quasi-constitutional rule followed, during which the monarch, assisted by the leaders of fledgling political parties, governed the country. During the 1950s, efforts were made to frame a constitution for Nepal that would establish a representative form of government, based on a British model.there was a 10-member cabinent under Prime Minister Mohan Shumsher having 5 members of Rana and 5 of Nepali congress party. This government drafted a constitution called 'Interim Government Act' which was the first constitution of Nepal. But this Government doomed as Ranas and Congressmen were never on good terms. So, on 1 Mangsir 2008 BS, the king formed a new government of 14 ministers. which was also dissolved. later on Sharawan 2009 BS King formed 5 members councillors government which was also failed.

Declaring parliamentary democracy a failure, King Mahendra carried out a royal coup 18 months later, in 1960. He dismissed the elected Koirala government, declared that a "partyless" system would govern Nepal, and promulgated another new constitution on December 16, 1960.

Subsequently, the elected Prime Minister, Members of Parliament and hundreds of democratic activists were arrested. (In fact, this trend of arrest of political activists and democratic supporters continued for the entire 30-year period of partyless Panchayati System under King Mahendra and then his son, King Birendra).

The new constitution established a "partyless" system of panchayats (councils) which King Mahendra considered to be a democratic form of government, closer to Nepalese traditions. As a pyramidal structure, progressing from village assemblies to the Panchayat system constitutionalized the absolute power of the monarchy and kept the King as head of state with sole authority over all governmental institutions, including the Cabinet (Council of Ministers) and the Parliament. One-state-one-language became the national policy in an effort to carry out state unification, uniting various ethnic and regional groups into a singular Nepali nationalist bond. The 'Gaun Pharka Aviyan' launched in 1967, was one of the main rural development programs of the Panchayat system.

King Mahendra was succeeded by his 27-year-old son, King Birendra, in 1972. Amid student demonstrations and anti-regime activities in 1979, King Birendra called for a national referendum to decide on the nature of Nepal's government: either the continuation of the panchayat system along with democratic reforms or the establishment of a multiparty system. The referendum was held in May 1980, and the panchayat system won a narrow victory. The king carried out the promised reforms, including selection of the prime minister by the Rastriya Panchayat.

People in rural areas had expected that their interests would be better represented after the adoption of parliamentary democracy in 1990. The Nepali Congress with the support of "Alliance of leftist parties" decided to launch a decisive agitational movement, Jana Andolan, which forced the monarchy to accept constitutional reforms and to establish a multiparty parliament. In May 1991, Nepal held its first parliamentary elections in nearly 50 years. The Nepali Congress won 110 of the 205 seats and formed the first elected government in 32 years.

In 1992, in a situation of economic crisis and chaos, with spiraling prices as a result of the implementation of changes in policy of the new Congress government, the radical left stepped up their political agitation. A Joint People's Agitation Committee was set up by the various groups. A general strike was called for April 6.

Violent incidents began to occur on the evening before the strike. The Joint People's Agitation Committee had called for a 30-minute 'lights out' in the capital, and violence erupted outside Bir Hospital when activists tried to enforce the 'lights out'. At dawn on April 6, clashes between strike activists and police, outside a police station in Pulchok (Patan), left two activists dead.

Later in the day, a mass rally of the Agitation Committee at Tundikhel in the capital Kathmandu was attacked by police forces. As a result, riots broke out and the Nepal Telecommunications building was set on fire; police opened fire at the crowd, killing several persons. The Human Rights Organisation of Nepal estimated that 14 persons, including several onlookers, had been killed in police firing.

When promised land reforms failed to appear, people in some districts started to organize to enact their own land reform and to gain some power over their lives in the face of usurious landlords. However, this movement was repressed by the Nepali government, in "Operation Romeo" and "Operation Kilo Sera II", which took the lives of many of the leading activists of the struggle. As a result, many witnesses to this repression became radicalized.

In March 1997, the Communist Party of Nepal (Maoist) started a bid to replace the parliamentary monarchy with a people's new democratic republic, through a Maoist revolutionary strategy known as the people's war, which led to the Nepalese Civil War. Led by Dr. Baburam Bhattarai and Pushpa Kamal Dahal (also known as "Prachanda"), the insurgency began in five districts in Nepal: Rolpa, Rukum, Jajarkot, Gorkha, and Sindhuli. The Communist Party of Nepal (Maoist) established a provisional "people's government" at the district level in several locations.

On June 1, 2001, Prince Dipendra, went on a shooting-spree, assassinating 9 members of the royal family, including King Birendra and Queen Aishwarya, before shooting himself. Due to his survival, he temporarily became king before dying of his wounds, after which Prince Gyanendra (HMG King Birendra's brother) inherited the throne, as per tradition. Meanwhile, the rebellion escalated, and in October 2002 the king temporarily deposed the government and took complete control of it. A week later he reappointed another government, but the country was still very unstable.
In the face of unstable governments and a siege on the Kathmandu Valley in August 2004, popular support for the monarchy began to wane. On February 1, 2005, Gyanendra dismissed the entire government and assumed full executive powers, declaring a "state of emergency" to quash the revolution. Politicians were placed under house arrest, phone and internet lines were cut, and freedom of the press was severely curtailed.

The king's new regime made little progress in his stated aim to suppress the insurgents. Municipal elections in February 2006 were described by the European Union as "a backward step for democracy", as the major parties boycotted the election and some candidates were forced to run for office by the army. In April 2006 strikes and street protests in Kathmandu forced the king to reinstate the parliament. A seven-party coalition resumed control of the government and stripped the king of most of his powers. As of 15 January 2007, Nepal was governed by a unicameral legislature under an interim constitution. On December 24, 2007, seven parties, including the former Maoist rebels and the ruling party, agreed to abolish the monarchy and declare Nepal the Federal Republic. In the elections held on 10 April 2008, the Maoists secured a simple majority, with the prospect of forming a government to rule the proposed 'Republic of Nepal'.

On May 28, 2008, the newly-elected Constituent Assembly declared Nepal the Federal Democratic Republic, abolishing the 240-year-old monarchy. The motion for the abolition of the monarchy was carried by a huge majority: out of 564 members present in the assembly, 560 voted for the motion while 4 members voted against it. On June 11, 2008, ex-King Gyanendra left the palace. Ram Baran Yadav of the Nepali Congress became the first President of the Federal Democratic Republic of Nepal on July 23, 2008. Similarly, the Constituent Assembly elected Pushpa Kamal Dahal (popularly known as Prachanda) of the Unified Communist Party of Nepal (Maoist) as the first Republican Prime Minister on August 15, 2008, favoring him over Sher Bahadur Deuba of the Nepali Congress Party.

After failing to draft a constitution before the deadline, the existing Constituent Assembly was dissolved (28 May 2012) and a new interim government (2013-2014) formed under the Prime-Ministership of the Chief Justice of Nepal, Supreme Court judge Khil Raj Regmi. In the Constituent Assembly election of November 2013 the Nepali Congress won the largest share of the votes but failed to get a majority. The Communist Party of Nepal (Unified Marxist–Leninist) (CPN (UML)) and the Nepali Congress negotiated to form a consensus government, and Sushil Koirala of the Nepali Congress was elected as Prime Minister (February 2014) with support from the CPN (UML).

Minority ethnic groups like Madhesi and Tharu have protested vigorously against the constitution which came into effect on September 20, 2015. They point out that their concerns have not been addressed and there are few explicit protections for their ethnic groups in the document. At least 56 civilians and 11 police died in clashes over the draft constitution. In response to the Madhesi protests, India suspended vital supplies to landlocked Nepal, citing insecurity and violence in border areas.
It has been alleged that India's denial of petroleum and medicine to Nepal constituted a violation of human rights. The main reason for doing this was because India allegedly wants control over Nepal.




</doc>
<doc id="21324" url="https://en.wikipedia.org/wiki?curid=21324" title="Geography of Nepal">
Geography of Nepal

Nepal measures about along its Himalayan axis by across. Nepal has an area of .

Nepal is landlocked by India on three sides and China's Tibet Autonomous Region to the north. West Bengal's narrow "Siliguri Corridor" or Chicken's Neck separate Nepal and Bangladesh. To the east are India and Bhutan. Nepal depends on India for goods transport facilities and access to the sea, even for most goods imported from China.

For a small country, Nepal has tremendous geographic diversity. It rises from 59 meter elevation in the tropical Terai—the northern rim of the Gangetic Plain, beyond the perpetual snow line to some 90 peaks over including world highest Mount Everest or "Sagarmatha" which is located Sagarmatha zone. In addition to the continuum from tropical warmth to cold comparable to polar regions, average annual precipitation varies from as little as in the rainshadow north of the Himalaya to as much as on windward slopes.

Along a south-to-north transect, Nepal has been divided into three belts: Terai, hilly and mountain. In the other direction, it is divided into three major river systems, from east to west: "Koshi", "Gandaki/Narayani" and "Karnali" (including the "Mahakali/Sarda" along the western border), all tributaries of the Ganges. The Ganges-"Yarlung Zangbo/Brahmaputra" watershed largely coincides with the Nepal-Tibet border, however several Ganges tributaries rise inside Tibet.Nepal has divided into seven states and state number 2 is the largest state. In state number 5 there is the second largest valley of Asia.

 Inner Terai is a low land region containing some hill ranges and valley .The Terai ("also" Tarai) "or" Madhesh region begins at the Indian border and includes the southernmost part of the flat, intensively farmed Gangetic Plain called the "Outer Terai". By the 19th century, timber and other resources were being exported to India. Industrialization based on agricultural products such as jute began in the 1930s and infrastructure such roadways, and electricity were extended across the border before it reached Nepal's pahad.

The Outer Terai is culturally more similar to adjacent parts of India's Bihar and Uttar Pradesh than to the hilly region Nepal. Nepali is taught in schools and often spoken in government offices, however the local population mostly uses Maithali, Bhojpuri and Tharu languages.

The Outer Terai ends at the base of the first range of foothills called the "Siwaliks" or "Chura". This range has a densely forested skirt of coarse alluvium called the "bhabhar". Below the bhabhar, finer, less permeable sediments force groundwater to the surface in a zone of springs and marshes. In Persian, "terai" refers to wet or marshy ground. Before the use of DDT this was dangerously malarial. Nepal's rulers used this for a defensive frontier called the "char kose jhadi" (four "kos" forest, one kos equalling about three kilometers or two miles).

Above the bhabhar belt, the mahabhabharat rise to about with peaks as high as , steeper on their southern flanks because of faults known as the "Main Frontal Thrust". This range is composed of poorly consolidated, coarse sediments that do not retain water or support soil development so there is virtually no agricultural potential and sparse population but medicinal herbs are easily available in this region.

In several places beyond the Siwaliks there are "dūn" valleys called Inner Terai . These valleys have productive soil but were dangerously malarial except to indigenous "Tharu people" who had genetic resistance. In the mid-1950s DDT came into use to suppress mosquitos and the way was open to settlement from the land-poor hills, to the detriment of the Tharu but there are no more tharus in inner terai and no transmission of maleria also.

The terai ends and the hilly region begin at a higher range of foothills called the "Mahabharat Range".

Pahad is a mountain region which doesn't generally contain snow.It is situated south of the Himal, the Pahad is mostly betw altitude. This region begins at the "Mahabharat Range" (Lesser Himalaya) where a fault system called the "Main Boundary Thrust" creates an escarpment high, to a crest between .

These steep southern slopes are nearly uninhabited, thus an effective buffer between languages and culture in the Terai and Pahad. Hindu "Paharis" mainly populate river and stream bottoms that enable rice cultivation and are warm enough for winter/spring crops of wheat and potato. The increasingly urbanized "Kathmandu" and "Pokhara" valleys fall within the Hill region. "Newars" are an indigenous ethnic group with their own Tibeto-Burman language. The Newar were originally indigenous to the Kathmandu valley but have spread into Pokhara and other towns alongside urbanized Pahari.

Other indigenous "janajati" ethnic groups -— natively speaking highly localized Tibeto-Burman languages and dialects -— populate hillsides up to about . This group includes "Magar" and "Kham Magar" west of Pokhara, "Gurung" south of the "Annapurnas", "Tamang" around the periphery of Kathmandu Valley and "Rai", "Koinch Sunuwar" and "Limbu" further east. Temperate and subtropical fruits are grown as cash crops. Marijuana was grown and processed into "Charas" (hashish) until international pressure persuaded the government to outlaw it in 1976. There is increasing reliance on animal husbandry with elevation, using land above for summer grazing and moving herds to lower elevations in winter. Grain production has not kept pace with population growth at elevations above where colder temperatures inhibit double cropping. Food deficits drive emigration out of the pahad in search of employment.

The Pahad ends where ridges begin substantially rising out of the temperate climate zone into subalpine zone above .

Himal is a mountain region containing snow.
The Mountain Region or "Parbat" begins where high ridges (Nepali: लेक; lekh) begin substantially rising above into the subalpine and alpine zone which are mainly used for seasonal pasturage. A few tens kilometers further north the high Himalaya abruptly rise along the "Main Central Thrust" fault zone above the snow line at . Some 90 of Nepal's peaks exceed and eight exceed including Mount Everest at and Kanchenjunga at .

Unlike the "Mahabharats", the Himalaya are not continuous across Nepal. Instead there are some 20 subranges including the "Kanchenjunga" massif along the Sikkim border, "Mahalangur Himal" around Mt. Everest. "Langtang" north of Kathmandu, "Annapurna" and "Manaslu" north of Pokhara, then "Dhaulagiri" further west with "Kanjiroba" north of "Jumla" and finally "Gurans Himal" in the far west.

The main watershed between the Brahmaputra (called "Yarlung Tsangpo" in Tibet) and the Ganges system (including all of Nepal) actually lies north of the highest ranges. Alpine, often semi-arid valleys—including "Humla", "Jumla", "Dolpo", "Mustang", "Manang" and "Khumbu"—cut between Himalayan subranges or lie north of them.

Some of these valleys historically were more accessible from Tibet than Nepal and are populated by people with Tibetan affinities called "Bhotiya" or "Bhutia" including the famous Sherpas in Kumbu valley near Mount Everest. With Chinese cultural hegemony in Tibet itself, these valleys have become repositories of traditional ways. Valleys with better access from the hill regions to the south are culturally linked to Nepal as well as Tibet, notably the Kali Gandaki Gorge where Thakali culture shows influences in both directions.

Permanent villages in the mountain region stand as high as with summer encampments even higher. Bhotiyas graze yaks, grow cold-tolerant crops such as potatoes, barley, buckwheat and millet. They traditionally traded across the mountains, e.g., Tibetan salt for rice from lowlands in Nepal and India. Since trade was restricted in the 1950s they have found work as high altitude porters, guides, cooks and other accessories to tourism and alpinism.

Nepal's latitude is about the same as that of Florida, however with elevations ranging from less than to over and precipitation from to over the country has eight climate zones from tropical to perpetual snow.

The tropical zone below experiences frost less than once per decade. It can be subdivided into lower tropical (below 300 meters or 1,000 ft.) with 18% of the nation's land area) and upper (18% of land area) tropical zones. The best mangoes and well as papaya and banana are largely confined to the lower zone. Other fruit such as litchee, jackfruit, citrus and mangoes of lower quality grow in the upper tropical zone as well. Winter crops include grains and vegetables typically grown in temperate climates. The Outer Terai is virtually all in the lower tropical zone. Inner Terai valleys span both tropical zones. The Sivalik Hills are mostly upper tropical. Tropical climate zones extend far up river valleys across the Middle Hills and even into the Mountain regions.

The subtropical climate zone from occupies 22% of Nepal's land area and is the most prevalent climate of the Middle Hills above river valleys. It experiences frost up to 53 days per year, however this varies greatly with elevation, proximity to high mountains and terrain either draining or ponding cold air drainage. Crops include rice, maize, millet, wheat, potato, stone fruits and citrus.

The great majority of Nepal's population occupies the tropical and subtropical climate zones. In the Middle Hills, upper-caste Hindus are concentrated in tropical valleys which are well suited for rice cultivation while "Janajati" ethnic groups mostly live above in the subtropical zone and grow other grains more than rice.

The Temperate climate zone from occupies 12% of Nepal's land area and has up to 153 annual days of frost. It is encountered in higher parts of the Middle Hills and throughout much of the Mountain region. Crops include cold-tolerant rice, maize, wheat, barley, potato, apple, walnut, peach, various cole, amaranthus and buckwheat.

The Subalpine zone from occupies 9% of Nepal's land area, mainly in the Mountain and Himalayan regions. It has permanent settlements in the Himalaya, but further south it is only seasonally occupied as pasture for sheep, goats, yak and hybrids in warmer months. There are up to 229 annual days of frost here. Crops include barley, potato, cabbage, cauliflower, amaranthus, buckwheat and apple. Medicinal plants are gathered.

The Alpine zone from occupies 8% of the country's land area. There are a few permanent settlements above 4,000 meters. There is virtually no plant cultivation although medicinal herbs are gathered. Sheep, goats, yaks and hybrids are pastured in warmer months.

Above 5,000 meters the climate becomes Nival and there is no human habitation or even seasonal use.

Arid and semi-arid land in the rainshadow of high ranges have a Transhimalayan climate. Population density is very low. Cultivation and husbandry conform to subalpine and alpine patterns but depend on snowmelt and streams for irrigation.

Precipitation generally decreases from east to west with increasing distance from the Bay of Bengal, source of the summer monsoon. Eastern Nepal gets about annually; the Kathmandu area about and western Nepal about . This pattern is modified by adiabatic effects as rising air masses cool and drop their moisture content on windward slopes, then warm up as they descend so relative humidity drops. Annual precipitation reaches on windward slopes in the "Annapurna" Himalaya beyond a relatively low stretch of the "Mahabharat Range". In rainshadows beyond the high mountains, annual precipitation drops as low as .

The year is divided into a wet season from June to September—as summer warmth over Inner Asia creates a low pressure zone that draws in moist air from the Indian Ocean—and a dry season from October to June as cold temperatures in the vast interior creates a high pressure zone causing dry air to flow outward. April and May are months of intense water stress when cumulative effects of the long dry season are exacerbated by temperatures rising over in the tropical climate belt. Seasonal drought further intensifies in the "Siwaliks" hills consisting of poorly consolidated, coarse, permeable sediments that do not retain water, so hillsides are often covered with drought-tolerant scrub forest. In fact much of Nepal's native vegetation adapted to withstand drought, but less so at higher elevations where cooler temperatures mean less water stress.

The summer monsoon may be preceded by a buildup of thunderstorm activity that provides water for rice seedbeds. Sustained rain on average arrives in mid-June as rising temperatures over Inner Asia creates a low pressure zone that draws in moist air from the Indian Ocean, but this can vary up to a month. Significant failure of monsoon rains historically meant drought and famine while above-normal rains still cause flooding and landslides with losses in human lives, farmland and buildings.

The monsoon also complicates transportation with roads and trails washing out while unpaved roads and airstrips may become unusable and cloud cover reduces safety margins for aviation. Rains diminish in September and generally end by mid-October, ushering in generally cool, clear, and dry weather, as well as the most relaxed and jovial period in Nepal. By this time, the harvest is completed and people are in a festive mood. The two biggest and most important Hindu festivals—"Dashain" and "Tihar" ("Dipawali")—arrive during this period, about one month apart. The post monsoon season lasts until about December.

After the post monsoon comes the winter monsoon, a strong north easterly flow marked by occasional, short rainfalls in the lowlands and plains and snowfalls in the high-altitude areas. In this season the Himalayas function as a barrier to cold air masses from Inner Asia, so southern Nepal and northern India have warmer winters than would otherwise be the case. April and May are dry and hot, especially below where afternoon temperatures may exceed .

The dramatic changes in elevation along this transect result in a variety of biomes, from tropical savannas along the Indian border, to subtropical broadleaf and coniferous forests in the hills, to temperate broadleaf and coniferous forests on the slopes of the Himalaya, to montane grasslands and shrublands, and finally rock and ice at the highest elevations.

This corresponds to the Terai-Duar savanna and grasslands ecoregion.

Subtropical forests dominate the lower elevations of the Hill Region. They form a mosaic running east-west across Nepal, with Himalayan subtropical broadleaf forests between and Himalayan subtropical pine forests between . At higher elevations, to , are found temperate broadleaf forests: eastern Himalayan broadleaf forests to the east of the Gandaki River and western Himalayan broadleaf forests to the west.

The native forests of the Mountain Region change from east to west as precipitation decreases. They can be broadly classified by their relation to the Gandaki River. From are the eastern and western Himalayan subalpine conifer forests. To are the eastern and western Himalayan alpine shrub and meadows.


Nepal has three categories of rivers. The largest systems -— from east to west "Koshi", "Gandaki/Narayani", "Karnali/Goghra" and "Mahakali"—originate in multiple tributaries rising in or beyond the high Himalaya that maintain substantial flows from snowmelt through the hot, droughty spring before the summer monsoon. These tributaries cross the highest mountains in deep gorges, flow south through the Middle Hills, then join in candelabra-like configuration before crossing the Mahabharat Range and emerging onto the plains where they have deposited "megafans" exceeding area.

The Koshi is also called "Sapta Koshi" for its seven Himalayan tributaries in eastern Nepal: "Indrawati", "Sun Koshi", "Tama Koshi", "Dudh Koshi", "Liku", "Arun", and "Tamor". The Arun rises in Tibet some beyond Nepal's northern border. A tributary of the Sun Koshi, Bhote Koshi also rises in Tibet and is followed by the Arniko Highway connecting Kathmandu and Lhasa.

The "Gandaki/Narayani" has seven Himalayan tributaries in the center of the country: "Daraudi", "Seti Gandaki", "Madi", "Kali", "Marsyandi", "Budhi", and "Trisuli" also called "Sapta Gandaki". The "Kali Gandaki" rises on the edge of the Tibetan Plateau and flows through the semi-independent Kingdom of Mustang, then between the 8,000 meter "Dhaulagiri" and "Annapurna" ranges in the world's deepest valley. The Trisuli rises north of the international border inside Tibet. After the seven upper tributaries join, the river becomes the "Narayani" inside Nepal and is joined by the "(East) Rapti" from Chitwan Valley. Crossing into India, its name changes to "Gandak".

The "Karnali" drains western Nepal, with the "Bheri" and "Seti" as major tributaries. The upper Bheri drains Dolpo, a remote valley beyond the Dhaulagiri Himalaya with traditional Tibetan cultural affinities. The upper Karnali rises inside Tibet near sacred "Lake Manasarovar" and "Mount Kailash". The area around these features is the hydrographic nexus of South Asia since it holds the sources of the Indus and its major tributary the "Sutlej", the Karnali—a Ganges tributary—and the "Yarlung Tsangpo"/"Brahmaputra". It is the center of the universe according to traditional cosmography. The "Mahakali" or "Kali" along the Nepal-India border on the west joins the Karnali in India, where the river is known as "Goghra" or "Ghaghara".

Second category rivers rise in the Middle Hills and "Mahabharat Range", from east to west the "Mechi", "Kankai" and "Kamala" south of the Kosi; the "Bagmati" that drains Kathmandu Valley between the Kosi and Gandaki systems, then the "West Rapti" and the "Babai" between the Gandaki and Karnali systems. Without glacial sources, annual flow regimes in these rivers are more variable although limited flow persists through the dry season.

Third category rivers rise in the outermost "Siwalik" foothills and are mostly seasonal.

None of these river systems support significant commercial navigation. Instead, deep gorges create obstacles to establishing transport and communication networks and de-fragmenting the economy. Foot-trails are still primary transportation routes in many hill districts.

Rivers in all three categories are capable of causing serious floods. Koshi River in the first category caused a major flood in August 2008 in "Bihar" state, India after breaking through a poorly maintained embankment just inside Nepal. The West Rapti in the second category is called "Gorakhpur's Sorrow" for its history of urban flooding. Third category Terai rivers are associated with flash floods.

Since uplift and erosion are more or less in equilibrium in the Himalaya, at least where the climate is humid, rapid uplift must be balanced out by annual increments of millions tonnes of sediments washing down from the mountains; then on the plains settling out of suspension on vast alluvial fans over which rivers meander and change course at least every few decades, causing some experts to question whether manmade embankments can contain the problem of flooding. Traditional "Mithila" culture along the lower Koshi in Nepal and Bihar celebrated the river as the giver of life for its fertile alluvial soil, yet also the taker of life through its catastrophic floods.

Large reservoirs in the Middle Hills may be able to capture peak flows and mitigate downstream flooding, to store surplus monsoon flows for dry season irrigation and to generate electricity. Water for irrigation is especially compelling because the Indian Terai is suspected to have entered a "food bubble" where dry season crops are dependent on water from tubewells that in the aggregate are unsustainably "mining" groundwater.
Depletion of aquifers without building upstream dams as a sustainable alternative water source could precipitate a Malthusian catastrophe in India's food insecure states Uttar Pradesh and Bihar, with over 300 million combined population. With India already experiencing a Naxalite–Maoist insurgency in Bihar, Jharkhand and Andhra Pradesh, Nepalese reluctance to agree to water projects could even seem an existential threat to India.

As Nepal builds barrages to divert more water for irrigation during the dry season preceding the summer monsoon, there is less for downstream users in Bangladesh and India's Bihar and Uttar Pradesh states. The best solution could be building large upstream reservoirs, to capture and store surplus flows during summer monsoon as well as providing flood control benefits to Bangladesh and India. Then water sharing agreements could allocate a portion of the stored water to be left to flow into India during the following dry season.

Nevertheless, building dams in Nepal is controversial for several reasons. First, the region is seismically active. Dam failures caused by earthquakes could cause tremendous death and destruction downstream, particularly on the densely populated Gangetic Plain. Second, global warming has led to the formation of glacial lakes dammed by unstable moraines. Sudden failures of these moraines can cause floods with cascading failures of manmade structures downstream.

Third, sedimentation rates in the Himalaya are extremely high, leading to rapid loss of storage capacity as sediments accumulate behind dams. Fourth, there are complicated questions of cross-border equity in how India and Nepal would share costs and benefits that have proven difficult to resolve in the context of frequent acrimony between the two countries.



ICIMOD’s first and most complete national land cover database of Nepal prepared using public domain Landsat TM data of 2010 shows that show that forest is the dominant form of land cover in Nepal covering 57,538 km with a contribution of 39.09% to the total geographical area of the country. Most of this forest cover is broadleaved closed and open forest, which covers 21,200 km2 or 14.4% of the geographical area.

Needleleaved open forest is the least common of the forest areas covering 8267 km (5.62%). Agriculture area is significant extending over 43,910 km (29.83%). As would be expected, the high mountain area is largely covered by snow and glaciers and barren land.
The Hill region constitutes the largest portion of Nepal, covering 29.5% of the geographical area, and has a large area (19,783 km) of cultivated or managed lands, natural and semi natural vegetation (22,621 km) and artificial surfaces (200 km). The Tarai region has more cultivated or managed land (14,104 km) and comparatively less natural and semi natural vegetation (4280 km). The Tarai has only 267 km of natural water bodies. The High mountain region has 12,062 km of natural water bodies, snow/glaciers and 13,105 km barren areas.

25.4% of Nepal's land area, or about is covered with forest according to FAO figures from 2005. FAO estimates that around 9.6% of Nepal's forest cover consists of "primary forest" which is relatively intact. About 12.1% Nepal's forest is classified as "protected" while about 21.4% is "conserved" according to FAO. About 5.1% Nepal's forests are classified as "production forest". Between 2000 and 2005, Nepal lost about of forest. Nepal's 2000–2005 total deforestation rate was about 1.4% per year meaning it lost an average of of forest annually. Nepal's total deforestation rate from 1990 to 2000 was or 2.1% per year. The 2000–2005 true deforestation rate in Nepal, defined as the loss of primary forest, is -0.4% or per year. Forest is not changing in the plan land of Nepal, forest fragmenting on the Roof of the World.

According to ICIMOD figures from 2010, forest is the dominant form of land cover in Nepal covering 57,538 km with a contribution of 39.09% to the total geographical area of the country. Most of this forest cover is broadleaved closed and open forest, which covers 21,200 km or 14.4% of the geographical area. Needleleaved open forest is the least common of the forest areas covering 8,267 km2 (5.62%). At national level 64.8% area is covered by core forests of > 500 ha size and 23.8% forests belong to patch and edge category forests. The patch forest constituted 748 km at national level, out of which 494 km of patch forests are present in hill regions. Middle mountains, Siwaliks and Terai regions have more than 70% of the forest area under core forest category > 500 ha size. The edge forests constituted around 30% of forest area of High Mountain and Hill regions.
Forest Resource Assessment (FRA) which was conducted between 2010 and 2014 by the Ministry of Forest and Soil conservation with the financial and technical help of Government of Finland shows that 40.36% land of Nepal is forested. And 4.40% land have Shrubs and bushes.
Deforestation is driven by multiple processes.
Virtually throughout the nation, over-harvest of firewood remains problematic. Despite the availability of liquefied petroleum gas in towns and cities, firewood is sold more at energy-competitive prices because cutting and selling it is a fallback when better employment opportunities aren't forthcoming. Firewood still supplies 80% of Nepal's energy for heating and cooking. Harvesting construction timber and lopping branches for fodder for cattle and other farm animals are also deforestation/degradation drivers in all geographic zones.

Illegal logging is a problems in the Siwaliks, with sawlogs smuggled into India. Clearing for resettlement and agriculture expansion also causes deforestation as does urban expansion, building infrastructure such as schools, hospitals, electric transmission lines, water tanks, police and army barracks, temples and picnic areas.

In the Middle Hills road construction, reservoirs, transmission lines and extractive manufacturing such as cement factories cause deforestation. In the mountains building hotels, monasteries and trekking trails cause deforestation while timber-smuggling into the Tibet Autonomous Region and over-grazing cause degradation.

While India and Nepal have an open border with no restrictions on movement of their citizens on either side, there are 23 checkpoints for trade purposes. These are listed in counterclockwise order, east to west. The six in italics are also used for entry/exit by third country nationals.




</doc>
<doc id="21325" url="https://en.wikipedia.org/wiki?curid=21325" title="Demographics of Nepal">
Demographics of Nepal

In the 2011 census, Nepal's population was approximately 26 million people with a population growth rate of 1.35% and a median age of 21.6 years. In 2016, the female median age was approximately 25 years old and the male median age was approximately 22 years old. Only 4.4% of the population is estimated to be more than 65 years old, comprising 681,252 females and 597,628 males. 61% of the population is between 15 and 64 years old, and 34.6% is younger than 14 years. In 2011, the Birth rate is estimated to be 22.17 births per 1,000 people with an infant mortality rate of 46 deaths per 1,000 live births. Compared to the infant mortality rate in 2006 of 48 deaths per 1000 live births, the 2011 IMR is a slight decrease within that 5-year period. Infant mortality rate in Nepal is higher in rural regions at 44 deaths per 1000 live births, whereas in urban regions the IMR is lower at 40 deaths per 1000 live births. This difference is due to a lack of delivery assistance services in rural communities compared to their urban counterparts who have better access to hospitals and neonatal clinics. Life expectancy at birth is estimated to be 67.44 years for females and 64.94 years for males. The mortality rate is estimated to be 681 deaths per 100,000 people. Net migration rate is estimated to be 61 migrants per 100,000 people. According to the 2011 census, 65.9% of the total population is literate.

The population of Nepal has been steadily rising recent decades. In the June 2001 census, there was a population of about 23 million in Nepal. The population increased by 5 million from the last census (1991); the growth rate is 2.3%. The current population is roughly 30 million which contributes to an increase of about 3 million people every 5 years.

Sixty caste and linguistic subgroups have formed throughout time with the waves of migration from Tibet and India. There was a moderate amount of immigration early in Nepal's history, then the population essentially remained the same without any significant fluctuations for over one hundred years. Natural disasters and the following government resettlement programs in the 1950s led to a spike in internal migration from the hills to the Terai region. In the 1980s the Western Chitwan Valley became a major transportation hub for all of Nepal. Along with this major change came a dramatic increase in government services, business expansion, and growing employment, especially in the agricultural industry. The valley's population grew rapidly through both in-migration and natural increase.

Source:

Births and deaths 

Structure of the population (22.06.2011) (Census) :

Total Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):

The following demographic statistics are from the 2011 Nepal Demographic and Health Survey (NDHS).

Median birth intervals (Median number of months since preceding birth)

Median age at first birth

Fertility rate - past trend and present

Ideal family size - Mean ideal number of children

Ideal family size by gender and age group

The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.

Nationality

Religions
Literacy

Population

Age structure

Median age

Population growth rate

Birth rate

Death rate

Net migration rate

Total fertility rate

Urbanization

Sex ratio

Nepal's diverse linguistic heritage evolved from three major language groups: Indo-Aryan, Tibeto-Burman languages, and various indigenous language isolates. According to the 2001 national census, 92 different living languages are spoken in Nepal (a 93rd category was "unspecified"). Based upon the 2011 census, the major languages spoken in Nepal (percentage spoken out of the mother tongue language) includes

Nepali (derived from Khas bhasa) is considered to be a member of Indo-European language and is written in Devanagari script. Nepali was the language of the house of Gorkhas in the late 18th century and became the official, national language that serves as the "lingua franca" among Nepalese of different ethnolinguistic groups. Maithili language—along with regional dialects Awadhi and Bhojpuri—are mother tongue Nepalese languages and spoken in the southern Terai Region. Many Nepali in government and business uses English as an official language. English is the language of technical, medical, and scientific community as well as the elite bankers, traders, and entrepreneurs. There has been a surge in the number and percentage of people who understand English. Majority of the urban and a significant number of the rural schools are English-medium schools. Higher education in technical, medical, scientific and engineering fields are entirely in English. Nepal Bhasa, the mother-tongue of the Newars, is widely used and spoken in and around Kathmandu Valley and in major Newar trade towns across Nepal.

Other languages, particularly in the Inner Terai hill and mountain regions, are remnants of the country's pre-unification history of dozens of political entities isolated by mountains and gorges. These languages typically are limited to an area spanning about one day's walk. Beyond that distance, dialects and languages lose mutual intelligibility. Since Nepal's unification, various indigenous languages have come under threat of extinction as the government of Nepal has marginalized their use through strict policies designed to promote Nepali as the official language. Indigenous languages which have gone extinct or are critically threatened include Byangsi, Chonkha, and Longaba. Since democracy was restored in 1990, however, the government has worked to improve the marginalization of these languages. Tribhuvan University began surveying and recording threatened languages in 2010 and the government intends to use this information to include more languages on the next Nepalese census.

As of the 2011 census, 81.3% of the Nepalese population was Hindu, 9.0% Buddhist, 4.4% Muslim, 3.0% Kirant/Yumaist, 1.42% Christian, and 0.9% followed other or no religion.

Nepal defines itself as a Hindu nation based in the caste system of traditional Hindu ideology. It is common for many Hindus in the country to also worship Buddhist deities simultaneously with Hindu traditions. The notion of religion in Nepal is more fluid than other countries, particularly Western countries. The Nepali people build their social networks through their religious celebrations, which are a central part to the whole of communities within the country.

There is a general ideal held by the Nepalese people that there is an omnipotent, transcendental "moral order" that is sacred to Hinduism. This ideal exists along with the constant presence of chaos and disorder in the material world. In the Northwestern region of the country, this all-encompassing state of disorder in the world is synonymous with human affliction, for which the religious Shamans can alleviate. Shamans create a world of mythic time and space to restore order and balance to the world to cure the suffers.

Kathmandu Valley is home to the Newars, a major ethnic group in Nepal. The city Bhaktapur is located inside of Kathmandu Valley. Bhaktapur was once an independent Hindu Kingdom. Individual homes typically have at least one shrine devoted to personal deities, with an altar displaying flowers, fruit, and oil among other offerings to the Gods. The perimeter of Kathmandu Valley is lined with shrines devoted to Hindu goddesses, whose purpose is to protect the city from chaotic events.In fact, at least one shrine can be found on the vast majority of streets in Kathmandu. The people of Nepal do not feel the need to segregate or compete based upon religion, so Hindu and Buddhist shrines are often coexisting in the same areas. The areas outside of the city are perceived to always possess some form of wild or disordered nature, so the Nepalese people inside of the city lines regularly worship the Hindu gods through public ceremonies.

The Hindu god Vishnu symbolizes moral order in the Newar society. The natural human shortcomings in maintaining the godly moral order is represented by the Hindu god Shiva. Shiva is destructive and acts in greed, and he threatens the moral order. In ancient myths, Vishnu must step in to contain Shiva and restore the order. In recent times, there has been a rise in political violence, specifically Maoist violence. This increased violence, along with the widespread poverty creates times of hardship for the people of Nepal. During their struggles they find stability and peace in religion.

Nepal's constitution continues long-standing legal provisions prohibiting discrimination against other religions (but also proselytization). The king was defied as the earthly manifestation of the Hindu god Vishnu. On May 19, 2006, the government faced a constitutional crisis, the House of Representatives which had been just reformed, having been previously dissolved, declared Nepal a "secular state".

However, the 2001 census identified 80.6% of the population as Hindu and 10.7% as Buddhist (although many people labeled Hindu or Buddhist often practice a syncretic blend of Hinduism, Buddhism, or animist traditions), 4.2% of the population was Muslim, 3.6% of the population followed the indigenous Kirat Mundhum religion and Christianity was practiced by 0.45% of the population.

Buddhist and Hindu shrines and festivals are respected and celebrated by most Nepalese. Certain animist practices of old indigenous religions survive.

The Kirati people of eastern Nepal, Limbus together with Rais form one of the largest single ethnic groups in Nepal. Pahari Hill Hindus of the Khas Gorkha tribe (Bahun and Chhetri castes) and the Newar ethnicity dominated the civil service, the judiciary and upper ranks of the army throughout the Shah regime (1768–2008). Nepali was the national language and Sanskrit became a required school subject. Children who spoke Nepali natively and who were exposed to Sanskrit had much better chances of passing the national examinations at the end of high school, which meant they had better employment prospects and could continue into higher education. Children who natively spoke local languages of the Madhesh and Hills, or Tibetan dialects prevailing in the high mountains were at a considerable disadvantage. This history of exclusion coupled with poor prospects for improvement created grievances that encouraged many in ethnic communities such as Madhesi and Tharu in the Tharuhat and Madhesh and Kham Magar in the mid-western hills to support the Unified Communist Party of Nepal (Maoist) and various other armed Maoist opposition groups such as the JTMM during and after the Nepalese Civil War. The negotiated end to this war forced King Gyanendra to abdicate in 2008. Issues of ethnic and regional equity have tended to dominate the agenda of the new republican government and continue to be divisive. Today, even after the end of a 10-year-old Maoist conflict, the upper caste dominates every field in Nepal. Specifically, Brahmin and Chhetri (Indo-Aryan) have advantage everywhere. Although Newars are low in numbers, their urban living habitat gives them a competitive advantage. Thus, Newars are the toppers in Human Development Index. From a gender perspective, Newari women are the most literate and lead in every sector. Brahmins and Chhetris' women have experienced less social and economic mobility compared to Newari women. Specifically, Brahmin women experience less equality due to their predominately rural living conditions which deprives them of access to certain educational and healthcare advantages.

In the 2001 census, approximately 6,000 Nepalese were living in the UK. According to latest figure from Office for National Statistics estimates that 51,000 Nepal-born people are currently resident in the UK. There has been increasing interest in the opportunities offered in the UK by the Nepalese, especially education. Between the years of 2001 to 2006, there were 7,500 applications for student visas.

The Nepali people residing in Hong Kong are primarily made up of children of ex-Gurkhas; born in Hong Kong during their parents' service with the British Army's Brigade of Gurkhas, which was based in Hong Kong from the 1970s until the handover. Large groups of Nepali people can be found in Shek Kong and Yuen Long District off of the main bases of the British army. Many ex-Gurkhas remained in Hong Kong after the end of their service under the sponsorship of their Hong Kong-born children, who held right of abode.

Nepalese of middle age or older generations in Hong Kong are predominantly found in security, while those of younger generations are predominantly found in the business industry.

Mostly the people from Kirati ethnic groups such as Rai and Limbu are the ones residing in Hong Kong and other neighbouring nations such as Singapore and Japan

Nepalese migrants abroad have suffered tremendous hardships, including some 7,500 deaths in the Middle East and Malaysia alone since the year 2000, some 3,500 in Saudi Arabia.

According to the 2001 census, there were 116,571 foreign born citizens in Nepal; 90% of them were of Indian origin followed by Bhutan, Pakistan and China. This number does not include the refugees from Bhutan and Tibet.



</doc>
<doc id="21326" url="https://en.wikipedia.org/wiki?curid=21326" title="Politics of Nepal">
Politics of Nepal

The politics of Nepal function within a framework of a republic with a multi-party system. Currently, the position of President of Nepal (head of state) is occupied by Bidhya Devi Bhandari. The position of Prime Minister (head of government) is held by Khadga Prasad Oli. Executive power is exercised by the Prime Minister and his cabinet, while legislative power is vested in the Parliament.

Until May 28, 2008, Nepal was a constitutional monarchy. On that date, the constitution was altered by the Nepalese Constituent Assembly to make the country a republic.

The Economist Intelligence Unit has rated Nepal as "hybrid regime" in 2016.

On June 1, 2001, members of royal family, King Birendra, Queen Aishwarya, Crown Prince Dipendra, Prince Nirajan and many other were killed in the massacre. However, after the massacre, Crown Prince survived for a short while in coma.

Although he never regained consciousness before dying, Crown Prince Dipendra was nonetheless the monarch under the law of Nepalese royal succession. After his death two days later, the late King's surviving brother Gyanendra was proclaimed king.

On 1 February 2002 King Gyanendra suspended the Parliament, appointed a government led by himself, and enforced martial law. The King argued that civil politicians were unfit to handle the Maoist insurgency. Telephone lines were cut and several high-profile political leaders were detained. Other opposition leaders fled to India and regrouped there. A broad coalition called the Seven Party Alliance (SPA) was formed in opposition to the royal takeover, encompassing the seven parliamentary parties who held about 90% of the seats in the old, dissolved parliament.

The UN-OHCHR, in response to events in Nepal, set up a monitoring program in 2005 to assess and observe the human rights situation there

On 22 November 2005, the Seven Party Alliance (SPA) of parliamentary parties and the Communist Party of Nepal (Maoist) agreed on a historic and unprecedented 12-point memorandum of understanding (MOU) for peace and democracy. Nepalese from various walks of life and the international community regarded the MOU as an appropriate political response to the crisis that was developing in Nepal. Against the backdrop of the historical sufferings of the Nepalese people and the enormous human cost of the last ten years of violent conflict, the MOU, which proposes a peaceful transition through an elected constituent assembly, created an acceptable formula for a united movement for democracy. As per the 12-point MOU, the SPA called for a protest movement, and the Communist Party of Nepal (Maoist) supported it. This led to a countrywide uprising called the Loktantra Andolan that started in April 2006. All political forces including civil society and professional organizations actively galvanized the people. This resulted in massive and spontaneous demonstrations and rallies held across Nepal against King Gyanendra's autocratic rule.

The people's participation was so broad, momentous and pervasive that the king feared being overthrown. On 21 April 2006, King Gyanendra declared that "power would be returned to the people". This had little effect on the people, who continued to occupy the streets of Kathmandu and other towns, openly defying the daytime curfew. Finally King Gyanendra announced the reinstatement the House of Representatives, thereby conceding one of the major demands of the SPA, at midnight on 24 April 2006. Following this action the coalition of political forces decided to call off the protests.

Twenty-one people died and thousands were injured during the 19 days of protests.

On 19 May 2006, the parliament assumed total legislative power and gave executive power to the Government of Nepal (previously known as His Majesty's Government). Names of many institutions (including the army) were stripped of the "royal" adjective and the Raj Parishad (a council of the King's advisers) was abolished, with his duties assigned to the Parliament itself. The activities of the King became subject to parliamentary scrutiny and the King's properties were subjected to taxation. Moreover, Nepal was declared a secular state abrogating the previous status of a Hindu Kingdom. However, most of the changes have, as yet, not been implemented. On 19 July 2006, the prime minister, G. P. Koirala, sent a letter to the United Nations announcing the intention of the Nepalese government to hold elections to a constituent assembly by April 2007.

On 23 December 2007, an agreement was made for the monarchy to be abolished and the country to become a federal republic with the Prime Minister becoming head of state. Defying political experts, who had predicted it to be trounced in the April 2008 elections, the Communist Party of Nepal (Maoist) became the largest party amidst a general atmosphere of fear and intimidation from all sides. A federal republic was established in May 2008, with only four members of the 601-seat Constituent Assembly voting against the change, which ended 240 years of royal rule in Nepal. The government announced a public holiday for three days, (May 28 – May 30), to celebrate the country becoming a federal republic.

Major parties such as the Unified Communist Party of Nepal (Maoist), Communist Party of Nepal (Unified Marxist-Leninist) (CPN UML) and the Nepali Congress agreed to write a constitution to replace the interim one within 2 years. However, uncooperative and "selfish" behavior of the political parties has been cited as the major cause behind the de-railing of the peace process.

The Maoists, as the largest party of the country, took power right after the elections and named Pushpa Kamal Dahal (Prachanda) as the Prime Minister of the country. CPN UML also joined this government, but the Nepali Congress took the part of the main opposition party. People soon saw that the country's situation deteriorated and political turmoils were in store. Prachanda soon fell into a dispute with the then army chief Rookmangud Katwal and decided to sack him. But the President Ram Baran Yadav, as the supreme head of
military power in the country, revoked this decision and gave the army chief additional time in office. An angry Prachanda and his party quit the government, majorly citing this reason and decided to operate as the main opposition to the government headed by CPN UML and its co-partner Nepali Congress afterwards. Madhav Kumar Nepal was named the Prime Minister.

The Maoists have been to this date demanding civilian supremacy over the army.

The Maoists have been forcing closures – commonly known as "bandhs" – in the country, and have also declared autonomous states for almost all the ethnic groups in Nepal – seen as a part of revenge against the action that foiled their decision to sack the army chief.

Political leaders continue to discuss plans to end this turmoil, but none of the talks have been successful. Rising inflation, economic downturn, poverty, insecurity and uncertainty are the major problems. Many analysts opine that freedom has brought chaos to the country. Many doubt that the political parties will succeed in writing a constitution.

On May 2012 constitution assembly was dissolved and another election to select the constitution assembly members was declared by Dr. Baburam Bhattarai.

Madhes Movement (Nepali: मधेस अान्दोलन) is a political movement launched by various political parties, especially those based in Madhes, for equal rights, dignity and identity of Madhesis and Tharus, Muslisms and Janjati groups in Nepal. In nearly a decade, Nepal witnessed three Madhes Movements - the first Madhes Movement erupted in 2007, the second Madhes Movement in 2008 and the third Madhes Movement in 2015. About the origin of the first Madhes Movement, Journalist Amarendra Yadav writes in The RIsing Nepal"When the then seven-party alliance of the mainstream political parties and the CPN-Maoist jointly announced the Interim Constitution in 2007, it totally ignored the concept of federalism, the most desired political agenda of Madhesis and other marginalised communities. A day after the promulgation of the interim statute, a group of Madhesi activists under the Upendra Yadav-led Madhesi Janaadhikar Forum-Nepal (then a socio-intellectual NGO) burnt copies of the interim constitution at Maitighar Mandala, Kathmandu." This triggered the Madhes movement I.

The second Madhes Movement took place in 2008, jointly launched by Madhesi Janaadhikar Forum-Nepal, Terai Madhes Loktantrik Party and Sadbhawana Party led by Rajendra Mahato with three key agenda: federalism, proportional representation and population-based election constituency, which were later ensured in the Interim Constitution of Nepal 2008.

However, The Constitution of Nepal 2015 backtracked from those issues, that were already ensured by the Interim Constitution of Nepal 2008. Supreme Court of Nepal Advocate Dipendra Jha writes in The Kathmandu Post: "many other aspects of the new constitution are more regressive than the Interim Constitution of Nepal 2007. Out of all its deficiencies, the most notable one concerns the issue proportional representation or inclusion in all organs of the state." This triggered the third Madhes Movement by Madhesis in Nepal. Although the first amendment to the constitution was done, the resistance over the document by Madhesi and Tharus in Nepal still continues

From 1991 to 2002 the Parliament ("Sansad") had two chambers. The House of Representatives ("Pratinidhi Sabha") had 205 members elected for five-year term in single-seat constituencies. The National Council ("Rashtriya Sabha") had 60 members, 35 members elected by the Pratinidhi Sabha, 15 representatives of Regional Development Areas and 10 members appointed by the king.
Parliament was subsequently dissolved by the king in 2002 on the pretext that it was incapable of handling the Maoists rebels.

After the victory of Loktantra Andolan in the spring of 2006, a unicameral interim legislature replaced the previous parliament. The new body consists both of members of the old parliament as well as nominated members. As of December 2007, the legislature had the following composition.

In May 2008 the elections for the Constituent Assembly saw the Communist Party of Nepal as the largest party in the Constituent Assembly, which will have a term of two years.

The judiciary is composed of the Supreme Court (Sarbochha Adalat), appellate courts, and various Trial court|district courts. The Chief Justice of the Supreme Court was appointed by the monarch on recommendation of the Constitutional Council; the other judges were appointed by the monarch on the recommendation of the Judicial Council.

Nepal's judiciary is legally separate from the executive and legislative branches and has increasingly shown the will to be independent of political influence. The judiciary has the right of judicial review under the constitution.

AsDB, CCC, Colombo Plan, ESCAP, FAO, Group of 77, IBRD, ICAO, ICFTU, ICRM, International Development Association, IFAD, International Finance Corporation, IFRCS, International Labour Organization, International Monetary Fund, International Maritime Organization, Intelsat, Interpol, IOC, IOM, International Organization for Standardization (correspondent), ITU, MONUC, Non-Aligned Movement, OPCW, SAARC, United Nations, UNCTAD, UNDP, UNESCO, UNIDO, UNIFIL, UNMIBH, UNMIK, UNMOP, UNMOT, UNTAET, UPU, World Federation of Trade Unions, WHO, WIPO, WMO, WToO, WTrO CPC Nepal (applicant)


Jonathan Devendra. 2013. "Massacre at the Palace: The Doomed Royal Dynasty of Nepal". New York: Hyperion. .



</doc>
<doc id="21327" url="https://en.wikipedia.org/wiki?curid=21327" title="Economy of Nepal">
Economy of Nepal

Economic development in Nepal has been complicated and affected by the constant change in political scenarios which has ranged from monarchy to being ruled by the Communist party in present context. An isolated, agrarian society until the mid-20th century, Nepal entered the modern era in 1951 without schools, hospitals, roads, telecommunications, electric power, industry, or civil service. The country has, however, made progress toward sustainable economic growth since the 1950s and opened the country to economic liberalization leading to economic growth and improvement in living standards than compared to the past. The biggest challenges faced by the country in achieving higher economic development are the frequent changes in political leadership as well as corruption.

Nepal has used a series of five-year plans in an attempt to make progress in economic development. It completed its ninth economic development plan in 2002; its currency has been made convertible, and 17 state enterprises have been privatised. Foreign aid to Nepal accounts for more than half of the development budget. Government priorities over the years have been the development of transportation and communication facilities, agriculture, and industry. Since 1975, improved government administration and rural development efforts have been emphasised.

Agriculture remains Nepal's principal economic activity, employing about 65% of the population and providing 31.7% of GDP. Only about 20% of the total area is cultivable; another 40.7% is forested(i.e.covered by shurbs, pastureland & forest); most of the rest is mountainous. Fruits and vegetables (apples, pears, tomatoes, various salads, peach, nectarine, potatoes), as well as rice and wheat are the main food crops. The lowland Terai region produces an agricultural surplus, part of which supplies the food-deficient hill areas.

GDP is heavily dependent on remittances(29.1%) of foreign workers. Subsequently, economic development in social services and infrastructure in Nepal has not made dramatic progress. A countrywide primary education system is under development, and Tribhuvan University has several campuses. Although eradication efforts continue, malaria had been controlled in the fertile but previously uninhabitable Terai region in the south. Kathmandu is linked to India and nearby hill regions by road and an expanding highway network. The capital was almost out of fuel and transport of supplies caused by a crippling general strike in southern Nepal on 17 February 2008.

Major towns are connected to the capital by telephone and domestic air services. The export-oriented carpet and garment industries have grown rapidly in recent years and together now account for approximately 70% of merchandise exports.

The Cost of Living Index in Nepal is comparatively lower than many countries but not the least. The quality of life has declined to much less desirous value in recent years. Nepal was ranked 54th worst of 81 ranked countries (those with GHI > 5.0) on the Global Hunger Index in 2011, between Cambodia and Togo. Nepal's current score of 19.5 is better than in 2010 (20.0) and much improved than its score of 27.5 in 1990.

Huge numbers of Small Foreign Investments come to Nepal via the Non Resident Nepali, who are investing in shopping malls, plazas, real estate, tourism, etc. Nepal has a huge capacity for hydroelectricity. Accordingly, a large number of foreign companies are in line, but political instability has stopped the process at the same time as it's growing on its own.
Nepal has entered into agreements for avoidance of double taxation (all in credit method) with 10 countries (PSRD) since 0000. Similarly, it has Investment protection agreements with 5 countries (PSRD) since 1983. In 2014, Nepal restricted the Foreign aid by setting a minimum limit for foreign grants, soft and commercial loans from its development partners.

Nepal's merchandise trade balance has improved somewhat since 2000 with the growth of the carpet and garment industries. In the fiscal year 2000-2001, exports posted a greater increase (14%) than imports (4.5%), helping bring the trade deficit down by 4% from the previous year to $749 million. Recently, the European Union has become the largest buyer of ready-made garments (RMG); Fruits and vegetables (mostly: Apples, Pears, Tomatoes, various salads, peach, nectarine, potatoes, rice) from Nepal. Exports to the EU accounted for 46.13 percent of the country’s garment exports.

The annual monsoon rain, or lack of it, strongly influences economic growth. From 1996 to 1999, real GDP growth averaged less than 4%. The growth rate recovered in 1999, rising to 6% before slipping slightly in 2001 to 5.5%.

Strong export performance, including earnings from tourism, and external aid have helped improve the overall balance of payments and increase international reserves. Nepal receives substantial amounts of external assistance from the United Kingdom, the United States, Japan, Germany, and the Scandinavian countries.

Several multilateral organisations such as the World Bank, the Asian Development Bank, and the UN Development Programme also provide assistance. In June 1998, Nepal submitted its memorandum on a foreign trade regime to the World Trade Organization and in May 2000 began direct negotiations on its accession.

Progress has been made in exploiting Nepal's natural resources, tourism and hydroelectricity. With eight of the world's 10 highest mountain peaks, including Mount Everest at 8,848 m. In the early 1990s, one large public sector project and a number of private projects were planned; some have been completed. The most significant private sector financed hydroelectric projects currently in operation are the Khimti Khola (60 MW) and the Bhote Koshi Project (36 MW).The project is still undergoing and has dependency on China, India and Japan to take the further steps.

Nepal has 83,000 MW of theoretical and 42,133 MW of technically/financially viable hydroelectric potential, however the total installed capacity, at present, is mere 730.47 MW.

The environmental impact of Nepal's hydroelectric Own calendar (Bikram Sambat) New year in mid- April projects has been limited by the fact that most are "run-of-the-river" with only one storage project undertaken to date. The largest hydroelectric plant under consideration is the West Seti (750 MW) storage project dedicated to exports to be built by the private sector. Negotiations with India for a power purchase agreement have been underway for several years, but agreement on pricing and financing remains a problem. Currently demand for electricity is increasing at 8-10% a year whereas Nepal's option to have agreement with India will make this fulfilment against demand.

Population pressure on natural resources is increasing. Over-population is already straining the "carrying capacity" of the middle hill areas, particularly the Kathmandu Valley, resulting in the depletion of forest cover for crops, fuel, and fodder and contributing to erosion and flooding. Although steep mountain terrain makes exploitation difficult, mineral surveys have found small deposits of limestone, magnesite, zinc, copper, iron, mica, lead, and cobalt.

The development of hydroelectric power projects also cause some tension with local indigenous groups, recently empowered by Nepal's ratification of ILO Convention 169.

This is a chart of trend of gross domestic product of Nepal at market prices estimated by the International Monetary Fund and EconStats with figures in millions of Nepalese Rupees.

GDP:: purchasing power parity - $78.55 billion (2017 est.)

GDP - real growth rate: 7.5% (2017)

GDP - per capita: purchasing power parity (current international $) - $2700 (2017 est.)
GDP - composition by sector:
<br>"agriculture:" 27%
<br>"industry:" 13.5%
<br>"services:" 51.5% (2017 est.)

Population below poverty line: 23.8% (2011)

Household income or consumption by percentage share:
<br>"lowest 10%:" 3.2%
<br>"highest 10%:" 29.8% (1995–96)

Inflation rate (consumer prices): 4.5% (2017)

Labour force: 4 million (2016 est.) [Citation needed.]

Labor force - by occupation: agriculture 69%, services 19%, industry 12% (2014 est.)

Unemployment rate: 3.2% (2017 est.)

Budget:
<br>"revenues:" $5.954 billion
<br>"expenditures:" $5.974 billion, including capital expenditures of $NA (2017 est.)

Industries:
tourism, carpet, textile; small rice, jute, sugar, and oilseed mills; cigarette; cement and brick production

Industrial production growth rate: 10.9% (2017 est.):

Electricity - production: 4,083 GWh (2017)

Electricity - production by source:
<br>"fossil fuel:" 8.5%
<br>"hydro:" 91.5%
<br>"nuclear:" 0%
<br>"other:" 0% (2001)

Available energy:6257.73 GWh (2017)
NEA HYDRO:2290.78 GWh (2014)
NEA THERMAL:9.56 GWh (2014)
purchase(total):2331.17 GWh (2014)
India(purchase):2175.04 GWh (2017)
Nepal(IPP):1258.94 GWh (2014)

Electricity - consumption: 4,776.53 GWh (2017)

Electricity - exports: 95 GWh (2001)
Electricity - imports: 227 GWh (2001)

Oil - production: (2001 est.)

Oil - consumption: 2001

Agriculture - products:
, maize, wheat, sugarcane, root crops; milk, domestic buffalo meat

Exports: $818.7 million f.o.b., but does not include unrecorded border trade with India (2017 est.)

Exports - commodities: carpets, clothing, leather goods, jute goods, grain

Exports - partners: India 56.6%, US 11.5%, Turkey 4% (2016 est.)

Imports: $11.03 billion f.o.b. (2017 est.)

Imports - commodities: gold, machinery and equipment, petroleum products, electrical goods, medicine 

Imports - partners: India 70.1%, China 10.3%, UAE 2.6%, Singapore 2.1%, Saudi Arabia 1.2%. (2016 est.)

Debt - external: $5.948 billion (2017 est.)

Economic aid - recipient: $424 million (FY 00/01)

Currency: 1 Nepalese rupee (NPR) = 100 paisa

Fiscal year: 16 July - 15 July



</doc>
<doc id="21328" url="https://en.wikipedia.org/wiki?curid=21328" title="Telecommunications in Nepal">
Telecommunications in Nepal

In Nepal, operating any form of telecommunication service dates back to 1970. However, telecom service was formally provided mainly after the establishment of MOHAN AKASHWANI in B.S. 2005. Later as per the plan formulated in the First National Five year plan (2012-2017 BS); Telecommunication Department was established in B.S. 2016. To modernize the telecommunications services and to expand the services, during third five-year plan (2023-2028), Telecommunication Department was converted into Telecommunications Development Board in B.S. 2026.

After the enactment of Communications Corporation Act 2028, it was formally established as fully owned Government Corporation called Nepal Telecommunications Corporation in B.S. 2032 for the purpose of providing telecommunications services to Nepalese People. After serving the nation for 29 years with great pride and a sense of accomplishment, Nepal Telecommunication Corporation was transformed into Nepal Doorsanchar Company Limited (NDCL) from Baisakh 1, 2061. NDCL is a company registered under the companies Act 2053 with 85% government share. However, the company is known to the general public by the brand name Nepal Telecom (NT) as a registered trademark.

Nepal Telecommunications Authority (NTA) is the regulatory body of telecommunications in the country. According to the latest figures, 8 companies have been licensed to operate voice based telephony services out of which 5 are heavily invested by foreign companies. The investment market of telecom is a subject of interest for many foreign companies and NTA itself as it has to prepare the regulations on hand.

According to the latest Management Information system (MIS) report of Nepal Telecommunications Authority (NTA), 97.65 percent of 26.49 million people in the country have access to telephone service.The report includes data of up to mid-December, 2014. Telephone penetration increased by 12.88 percentage points in the one-year period. It stood at 84.77 percent in mid-December, 2013.

Some milestones:
The first telephone exchange was established in Kathmandu in 1960. Since 1960 to 2004, the state-owned Nepal Telecommunications Corporation (NTC), also now known as Nepal Telecom, or Nepal Doorsanchar Company Limited (NDCL) had been the monopoly telecom carrier. Now, other competing telecom service providers are United Telecom (UTL), and Ncell.

Telephones - PSTN: 644,347 (May 2013), CDMA

Telephones - 
Cellular service available by Nepal Telecom, formerly known as Nepal Telecommunications Corporation (NTC) and Spice Nepal (Pvt.) Ltd.

Telephone system:
Good telephone and telegraph service; fair radiotelephone communication service and mobile cellular telephone network
<br>"Domestic:" Microwave + Optical Fiber
<br>"International:"
Radiotelephone Communications; microwave landline to India; satellite earth station - 2 Intelsat (Indian Ocean)

Mobile Subscribers: 18,137,771 (May 2013)

Radio broadcast stations: AM 6, FM 20, shortwave 1 (January 2000)

Radios: 20,00,000 (2006)

Television broadcast stations: 19 (37 registered) (2012)

Televisions: 130,000 (1997)

Registered Internet Service Providers (ISPs): 42 (May 2013)

Internet users: 6,685,427 (May 2013)

Country code : 00977

Internet sites:



</doc>
<doc id="21329" url="https://en.wikipedia.org/wiki?curid=21329" title="Transport in Nepal">
Transport in Nepal

Nepal is a landlocked Himalayan country where transportation is difficult due to mountainous terrain.

Road is the country's primary transport mode.

Highways;

One 59 km line operates between Janakpur and Jainagar, close to the Indian border. The entire line is in broad gauge.

In 1998, two ZDM-5 diesel locomotives were donated by Indian Railway to Nepal Railways. In 2004, the Kolkata - Raxaul - Sirsiya broad gauge line started operations as an Inland Port. In 2008, three other proposals for rail connectivity to Nepal were considered and had preliminary engineering surveys done: New Jalpaiguri (India) to Kakrabitta (Nepal) via Panitanki (46.3 km); Nepalganj Road (India) to Nepalgunj (Nepal) (12.1 km); and Nautanwa (India) to Bhairahawa (Nepal) (15.3 km). In 2010, a line was proposed connecting Nepal and India. In 2011, metro trains were proposed in Kathmandu valley, while feasibility studies were conducted by the Delhi Metro Corporation and were declared in the Budget session.

In 2017, progress on an $8 billion rail link with China accelerated after Nepal formally signed up to Beijing's One Belt One Road Initiative. 



45 airports operated in Nepal as of 2002. Tribhuvan International Airport in Kathmandu is the only international airport and serves as the main aviation hub. 



Nepal's three dry "ports" are Birgunj, Biratnagar and Bhairahawa. Nepal is a landlocked country with no ocean borders. There is little use of water transportation within the country.


</doc>
<doc id="21330" url="https://en.wikipedia.org/wiki?curid=21330" title="Nepalese Armed Forces">
Nepalese Armed Forces

The Nepalese Armed Forces are the military forces of Nepal. The current Nepalese Army traces its direct historic roots from the Royal Nepalese Army, renamed in recognition of Nepal's transition from a monarchy to a popularly elected republic in 2006. Composed primarily of the ground-based Nepalese Army, organized into six active combat divisions, the Nepalese Armed Forces also operates the smaller Nepalese Army Air Service designed to support army operations and provide close light combat support. The Nepalese Army also operates smaller formations responsible for the organization of air defense, logistics, military communications, artillery, and airborne forces within Nepalese territory. In addition, the Nepalese Armed Police Force acts as a paramilitary force tasked with maintaining internal security within Nepal.

The Nepalese Armed Forces are a volunteer force with an estimated 95,000 active duty personnel in 2010, with an estimated annual military budget of around 60 million US dollars, not including military assistance funding from the Republic of India and People's Republic of China or more recently from the United States of America. Although most of Nepal's military equipment are imports from neighboring India or the People's Republic of China, Nepal has received 20,000 M-16 rifles, as well as night vision equipment from the United States to assist ongoing efforts in the post-September 11 global War on Terror campaign. The Nepalese Army bought 1,000 Galil rifles from Israel and received 2 V-5 helicopters from Russia.

Article 144 of Interim Constitution of Nepal states that The President of Nepal is the Supreme Commander Chief of Nepal Army.
Currently as the President of Nepal Bidhya Devi Bhandari who was elected president of Nepal on 28 oct 2015, is the supreme commander of Nepal Army.

Before 2006 democracy movement in Nepal forced King to restore democracy in 2006, Article 119 of the 1990 constitution stated that the His Majesty the King is the Supreme Commander of the Royal Nepal Army." However, following the People's Power revolution in April 2006, the 1990 constitution has been replaced by an interim constitution which has removed the King from anything to do with the army. On May 28, 2008 Monarchy was formally abolished and Nepal was declared Republic.

Nepal's Interim Constitution's Article 145 has envisioned National Defence Council which includes Prime Minister, Defence Minister, Home Minister and other three minister appointed by Prime Minister which recommends to the Council of Ministers on mobilization, operation and use of the Nepal Army. Upon Council of Ministers recommendation, President authorizes mobilization, operation and use of Nepal Army.

Before Interim Constitution replaces Constitution of Kingdom of Nepal 1990, 1990 constitution has prosion for defence council. This Council used to have three members, the Prime Minister, the Defence Minister, and the Chief of the Army Staff. In accordance with the Constitution, the King (as Supreme Commander) used to "operate and use" the "Royal Nepal Army on the recommendation" of this council.

Nepalese army fought various battles on the national unification campaigns of the 18th century. These battles of Nepal unification helped the Royal Nepalese army to gain more experience while helping to unify Nepal.


The fortress of Makawanpur has a historical and military significance for the Nepalese. It was here that the Nepalese defeated superior forces of Mir Kassim in 1763 and seized 500 guns and two cannons. Later on, these weapons were used by Nepalese troops and four companies were established regular, namely, Srinath, Kalibox, Barda Bahadur (Bardabahini) and Sabuj. (Purano) Gorakh Company was established a few months later. It was the first rank and file system beginning a proper organizational history for the Royal Nepalese Army. The battle against Mir Kassim troops was the first battle of the Royal Nepalese Army against a foreign power.

Sardar Nandu Shah was the fortress Commander of Makawanpur with 400 troops, some guns and home-made traditional weapons like Dhanu, Khukuri, Talwar, Ghuyatro etc. They devised different hit-and-run strategies to surprise the enemy. A spoiling attack base was set up on the Taplakhar mountain ridge
for night operations.

Mir Kassim's renowned warrior, Gurgin Khan was the commander on the other side with approximately 2,500 troops with cannons, guns, ammunition and a very good logistics back up. Their attack base was at the bottom of the Makawanpur Gadhi hill. They had planned a night attack. When the enemy’s heavy forces marched in December 1762 and arrived at Harnamadi in January 1763, they found all the local houses already evacuated and the area short of food provisions. Makawanpur Gadhi was on top of a mountain, about nine kilometers uphill from the Harnamadi area. Although the Nepalese had physically occupied all the fortresses en route, the enemy was able to initially push them back to the Makawanpur Gadhi area.

About 300 enemy launched a strong attack on 20 January 1763 putting the Nepalese still more on the defensive. But they were totally surprised when they were resting in Taplakhar, as Kaji Bamsa Raj Pandey led a downhill attack on them Kaju Naharsigh Basnyat led an uphill attack from below them and Nandu Shah led a frontal attack. The smooth coordination among the three, leading their, by now battle-hardened, troops in the dark of the night, led the bewildered enemy to scatter. About 1700 of them died and 30 Nepalese soldiers were lost in that battle. The Nepalese captured
500 rifles and two cannons with other military equipment. More importantly, the battle led to the beginning of a proper organization of the Royal Nepalese Army.


The relations started forming sour after the Malla rulers started to mint impure silver coins just before their downfall. The Tibetans demanded that the coins be replaced by pure silver ones. When Prithvi Narayan Shah took over, he found that it would be a great loss to him if he conceded to the Tibetan demands. That case remained unsolved due to his untimely demise. Queen Mother Rajendra Laxmi, the Regent of minor King Rana Bahadur Shah, inherited the coinage problem which reached the culminating point in 1888 AD. Another sore point in Nepal-Tibet relations was Nepal’s decision to provide refuge to Syamarpa Lama with his 14 Tibetan followers. He had fled from Tibet to Nepal on religious and political grounds. Yet
another cause for conflict was the low quality salt being provided by Tibetans to Nepal. All salt came from Tibet in those days. Tibet ignored the Nepalese ultimatums and that promoted the preparations for war. Nepal was soon preparing to launch multi-directional attacks.

Kerung Axis: Kaji Balbhadra Shah was the main Commander of the offensive attack from Kerung axis. Kaji Kirtimansingh Basnyat, Sardar Amarsingh Thapa and Bhotu Pandey were the subordinate commanders under him. Approximately 6,000 troops and 3,200 porters were despatched for this operation. Their main objective was to capture Dirgacha through Kerung. The march of the troops was delayed because Balbhadra Shah became seriously ill. They crossed
Kerung on 20 July 1788 and captured Jhunga on the 3rd of August 1788. Bhotu Pandey was captured by the Tibetans. The Nepalese troops were reinforced with 2,000 more troops and Bhotu Pandey was freed from the Tibetans on 14 October 1788.

Kuti Axis (I):Shree Krishna Shah was the Commander and Kaji Ranajit Pandey, Sardar Parath Bhandari, Captain Harsa Panta, Captain Naharsingh Basnyat and Captain Shiva Narayan Khatri were the subordinate commanders under him. About 5,800 soldiers and 3,000 porters were allotted for the offensive operation. Later on, Kaji Abhimansingh Basnyat and Ranajit Kunwar also joined this offensive. The Dalai Lama was taken by surprise and to protect his sovereignty, he initiated a parallel approach whereby he asked military help from Sovan Shahi, the King of Jumla in West Nepal, and requested him to launch guerrilla activities and revolt against the Nepalese Army in and around Jumla. Sovan Shahi did revolt at Humla and captured some fortresses. The Dalai Lama also asked for military help from the Chinese Emperor. Additionally, he himself and Panchen Lama of Dirgacha wrote a secret letter to the East India Company seeking military assistance. The Tibetans also initiated propaganda about having constructed a new road through the Tigri valley and establishing a post at the front. They also rumoured that they had assembled an Army of 1,25,000 men. But the Tibetans could get nothing from Jumla, China or the East India Company.

Kuti Axis (II):Kaji Damodar Pandey was leading his troops with subordinate commanders Bom Shah, Dev Dutta Thapa and others. He was given about 4,000 troops and his objective was to capture Dirgacha via the Kuti axis. The Battles Nepalese troops, having crossed the Himalayas captured Chhochyang and Kuti in June 1788 and Sikarjong on 3 August 1788, in spite of many difficult logistic limitations. Later, Bahadur Shah was able to provide some reinforcements and improve some logistics arrangements. Still that was not enough and progress was slow. When the Nepalese were about to capture Dirgacha via both Kuti and Kerung, the Tibetans started to make compromises with Nepalese commanders. Bahadur Shah started negotiations, ultimately arriving at a solution. Prisoners were handed back to the Tibetans. Tibet was ready to pay tributes to the tune of Rs. 50,000 in silver coins per annum to Nepal and a treaty was signed on 2 June 1789 in Kerung. The treaty is called the ‘Treaty of Kerung’ by historians Rasuwa Gadhi and Timure were the firm bases in the first Nepal-Tibet war. Syabru Besi and Rasuwa Gadhi were Strategic points in this war. Likewise, Listi and Duguna villages were the main bases for offensive operations against Tibet. They were the forward most dumping places of the Royal Nepalese Army. Although Rasuwa Gadhi and Duguna Gadhi Fortresses were not constructed at the time, the places themselves were important because of their military significance.




In 1974, The Royal Nepalese Army (RNA) was mobilized to disarm the Tibetan Khampas who had been using Nepalese soil to engage guerilla war against the invading the Chinese forces. The Khampas had secretly created their base in Mustang (north-west Nepal) and were operating from there against China. The RNA, under immense diplomatic pressure from China and the international community, moved nine infantry units towards the Khampa post in Mustang and gave them an ultimatum to either disarm themselves and surrender, or face consequences. The terms and conditions of their surrender was that they would be given Nepalese citizenship, land, and some money. The Khampa commander Wang Di agreed to surrender but eventually fled the camp. He was later killed in Doti, far-western Nepal by RNA forces while trying to loot a Nepal Police post. This was first time that the RNA was mobilized in such a large number domestically.


Nepal Army's long association with UN Peace Support Operations began with the deployment of five Military Observers in the Middle East, Lebanon (UNOGIL/ United Nations Observer Group in Lebanon) in 1958. And the first Nepalese contingent, Purano Gorakh battalion was deployed in Egypt in 1974. Nepal Army\'s participation in the UN peacekeeping operations spans a period of 50 years covering Nepal army involved UN Missions are 31, in which over sixty thousand six hundred and fifty two (60,652) Nepalese soldiers have served in support of UN peacekeeping endeavors. The Nepal Army has contributed Force Commanders, military contingent, military observers and staff officers. Nepalese troops have taken part in some of the most difficult operations, and have suffered casualties in the service of the UN. To date, the number of those lost on duty with the UN is 54, while 57 were seriously wounded.

Its most significant contribution has been of peace and stability in Africa. It has demonstrated its capacity of sustaining large troop commitments over prolonged periods. Presently, Nepal is ranked as the sixth largest troop contributing country (TCC) to the UN.


The U.S.-Nepali military relationship focuses on support for democratic institutions, civilian control of the military, and the professional military ethic to include respect for human rights. Both countries have had extensive contact over the years. Nepali Army units and Nepalese Army Air Service units have served with distinction alongside American forces in places such as Haiti, Iraq, and Somalia.

U.S.-Nepali military engagement continues today through IMET, Enhanced International Peacekeeping Capabilities (EIPC), and various conferences and seminars. The U.S. military sends many Nepalese Army officers to America to attend military schooling such as the Command and General Staff College and the U.S. Army War College. The IMET budget for FY2001 was $220,000.

The EPIC program is an interagency program between the Department of Defense and the Department of State to increase the pool of international peacekeepers and to promote interoperability. Nepal received about $1.9 million in EPIC funding.

Commander United States Pacific Command (CDRUSPACOM) coordinates military engagement with Nepal through the Office of Defense Cooperation (ODC). The ODC Nepal is located in the American Embassy, Kathmandu.

India has agreed to resume the military aid to Nepal. The aid was in the pipeline before India imposed an embargo in February 2005 following the seizure of power by the then King Gyanendra.
Even before this, India has been providing Nepal with military weapons free of cost.
In 2009, People's Republic of China pledged military aid worth Rs100 million to Nepal.

The command of Nepaleese army is divided into 6 parts namely.

Military branches: Nepalese Army (includes Nepalese Army Air Service), Armed Police Force Nepal, Nepalese Police Force

Military manpower - military age: 17 years of age

Military manpower - availability:
<br>"males age 15-49:" 6,674,014 (2003 est.)

Military manpower - fit for military service:
<br>"males age 15-49:" 3,467,511 (2003 est.)

Military manpower - reaching military age annually:
<br>"males:" 303,222 (2003 est.)

Military expenditures - dollar figure: $57.22 million (FY02)

Military expenditures - percent of GDP: 1.1% (FY02)

Nepal is also notable for the Gurkhas. Significant sections of the British Army and Indian Army are recruited from Nepal. This arrangement comes from the days of the British East India Company's rule of India when Company troops tried to invade Nepal and were beaten back. Both sides were impressed with the other, and Gurkhas were recruited into the Company's forces. The Gurkhas remained loyal during the Indian Mutiny of 1857 and were kept on in the Indian Army thereafter. Upon Indian independence in 1947, some units went to British service and some to Indian service, with a Britain-India-Nepal Tripartite Agreement signed between the three nations. The Gurkhas are feared troops, and their signature weapon is the khukuri.

The Nepal Army Air service has operated a flying and helicopter pilots training school since 2004 within the 11no. Brigade, it is the only helicopter pilots training school in Nepal,(there is a fixed-wing pilot training school in Bharatpur, Nepal by private pilots training school) This school produces army air service pilots and civilian too. The school provides the Mil Mi-17, and Eurocopter Ecureuil helicopter flying training.




</doc>
<doc id="21331" url="https://en.wikipedia.org/wiki?curid=21331" title="Foreign relations of Nepal">
Foreign relations of Nepal

Though the Ministry of Foreign Affairs (MOFA) is the government agency responsible for the conduct of foreign relations of Nepal, historically, it is the Office of Prime Minister(PMO) that has exercised the authority to formulate and conduct policies related to Nepal's foreign affairs. As a landlocked country wedged between two larger and far stronger powers, Nepal has tried to maintain good relations with both of its neighbor, People's Republic of China and Republic of India. However, relationship with India, the country with greater hegemonic power over Nepal, has seen major ups and downs in recent years. Given Nepal's geographical vulnerabilities, traditionally Nepal's southern neighbor India, has been able to shape Nepal's foreign policy to serve India's interest even to the detriment of Nepal's own interest. However, with the ongoing democratization of Nepal, and, shifting of the state-power from the hands of few elitists to democratically elected government institution, India has been facing increasing resistance within Nepal in implementing foreign policies that are detrimental to Nepal's own interest. In recent years, Indian government's attempts to deny landlocked Nepal 'Transit rights' via India as a fundamental right for a member of the UN, as guaranteed in the UN charter, and, the issues of occupation of some Nepalese territories by Indian forces have significantly hampered the relationship between the two countries. For most part though, Nepal has traditionally maintained a non-aligned policy and enjoys friendly relations with neighboring countries and almost all the major countries of the world.

Constitutionally, foreign policy is to be guided by “the principles of the United Nations Charter, nonalignment, Panchsheel (five principles of peaceful coexistence), international law and the value of world peace.” In practice, foreign policy has not been directed toward projecting influence internationally but toward preserving autonomy and addressing domestic economic and security issues.

Nepal’s most substantive international relations are perhaps with international economic institutions, such as the Asian Development Bank, the International Monetary Fund, the World Bank, and the South Asian Association for Regional Cooperation, a multilateral economic development association. Nepal also has strong bilateral relations with major providers of economic and military aid, such as France, Germany, Japan, Malaysia, South Korea, Switzerland, the United States, and particularly the United Kingdom, with whom military ties date to the nineteenth century. The country's external relations, barring relations with India and China, are primarily managed by its Ministry of Foreign Affairs while relationship with India and China, Nepal's most important partners, is still managed by the Prime Minister's Office. Nepal's relation with China has seen a major upswing in the recent years with China now becoming Nepal's 3rd largest aid donor (after the UK and Japan), and the largest source of FDI to Nepal.

Nepal has played an active role in the formation of the economic development-oriented South Asian Association for Regional Cooperation (SAARC) and is the site of its secretariat. On international issues, Nepal follows a nonaligned policy and often votes with the Non-Aligned Movement in the United Nations. Nepal participates in a number of UN specialized agencies and is a member of the World Bank, International Monetary Fund, Colombo Plan, and the Asian Development Bank.

In 2000, the government established the National Human Rights Commission, a government-appointed commission with a mandate to investigate human rights violations. To date, the Commission has investigated 51 complaints.. Although freedom of expression is widely used as constitutional right, some minor problems regarding it have been reported in the country. Trafficking in women and child labour remain serious problems. 

A joint border commission continues to work on small disputed sections of the border with India. Currently Nepal has border disputes with India at Lipulekh and Kalapani in Darchula district and Susta in Bihar.

Nepal has been a member of the World Trade Organization (WTO) since 11 September 2003 and on 24 January 2017 became the 108th WTO member to ratify the WTO's Trade Facilitation Agreement.

Illicit production of cannabis for domestic and international drug markets continues to be considered as an international problem, as do rumours that the country operates as a transit point for opiates from Southeast Asia and Pakistan to the West.

Both countries established diplomatic relations on 23 May 1972.

Nepal and Argentina established diplomatic relations on January 1, 1962. The relations between Nepal and Argentina are based on goodwill, friendship and mutual understanding. The Argentinean Government has shown interest to extend technical cooperation on leather processing industries in Nepal under the South-South Cooperation. However, the Argentinean proposal has not been materialized yet. Nepal's trade balance with Argentina is in favour of Argentina. There is no significant figure of export from Nepal. Major commodities imported by Nepal from Argentina are Crude soybean oil, soybean oil, vegetable waxes, sun flower oil and maize.

Both countries established diplomatic relations on 26 March 1993.

Nepal has good bilateral relations with Bangladesh. Though Nepal views Bangladesh as an access to the sea and seaports in Bangladesh, as perhaps an alternative to Indian seaport in Calcutta, successive Nepalese government have failed in increasing connectivity between Nepal and Bangladesh, and consequently, the volume of trade between Bangladesh and Nepal remains inconsequential. Till a decade ago, Bangladesh was the only country in the neighborhood with which Nepal enjoyed a positive trade balance. Recent initiatives like BBIN (Bhutan-Bangladesh-India-Nepal connectivity project), are being discussed as a potential tool for Nepal to address its connectivity issues, which still remains one of the poorest and the least connected country in the world. On May 28, 2009, a four-member delegation from Nepal visited Bangladesh and had talks on increasing trade and other relations. The meeting considered the movement of goods between the two countries in trucks transiting through India occupied Siliguri corrider territory. It also discussed the use of Mongla port in Bangladesh for transporting goods to and from Nepal at a concession rate. To promote tourism, travel agents and tour operators of both countries would jointly coordinate necessary steps.
Bangladesh transport experts note that following the visit of Bangladesh prime minister to India in January 2010, India agreed to provide transit facilities to Nepal by road and rail. Meanwhile, the Bangladesh Railway is working to find the most convenient route for rail transit to Nepal after India's positive response, according to Bangladesh Railway officials.

Nepal welcomed Bangladesh's independence on 16 January 1972. The turning point for the two nations occurred in April 1976, when the two nations signed, a four-point agreement on technical cooperation, trade, transit and civil aviation. They both seek cooperation in the fields of power generation and development of water resources. In 1986, relations further improved when Bangladesh insisted Nepal should be included on a deal regarding the distribution of water from the Ganges River. Also recently Nepal and Bangladesh had signed treaty that Nepal would sell 10,000 MW of electricity to Bangladesh once it's larger projects are completed.

Many foreign policy experts in Nepal, nowadays, advocate that Bangladesh should be provided a full-fledged ally status, and, that Nepal should seek political, economic, security and all possible assistance from Bangladesh while dealing with Nepal's hegemonic neighbor India to address Nepal's interest, as Nepal on its own lacks the economic and diplomatic weight to deal with India. However people familiar with the political culture of politics in Nepal remain highly skeptical of such a possibility and instead point to the fact that Nepal is on the verge of losing even more of its strategic autonomy because of the insertion of Indian fifth column - the madheshis, in Nepal's power structure.

Relations with Bhutan have been strained since 1992 over the nationality and possible repatriation of refugees from Bhutan.

Many Nepalese politicians and government officials criticized Canadian diplomats in the aftermath of the Kabul attack on Canadian Embassy guards in which the majority of victims were Nepalese citizens. Members of Parliament were among those who were critical of the way that Canada treated its security contractors at the embassy, leading to meetings in Ottawa between Nepalese and Canadian diplomats, including ambassador Nadir Patel.

Nepal formally established relations with the People's Republic of China on August 1, 1955. The two countries share a range of 1414 kilometers border in the Himalayan range of the northern side of Nepal. Nepal has established its embassy in Beijing, opened consulates general in Lhasa, Hong Kong and Guangzhou and appointed an honorary consul in Shanghai.

Economic:

The Nepal-China economic cooperation dates back to the formalization of bilateral relations in 1950’s. The first “Agreement between China and Nepal on “Economic Aid” was signed in October 1956. From the mid-80s the Chinese Government has been pledging grant assistance to the government of Nepal under the Economic and Technical Cooperation Program in order to implement mutually acceptable developmental projects.

The Chinese assistance to Nepal falls into three categories: Grants (aid gratis), interest free loans and concessional loans. These assistance of various kinds would be provided to Nepal via: different sources. The Chinese financial and technical assistance to Nepal has been greatly contributed to Nepal’s development efforts in the areas of infrastructure building, industrialization process, human resource development, health, education, water resources, sports and the like.

Some of the major on-going projects under Chinese assistance include:

1. Upper Trishuli Hydropower Project- Power station and Transmission Line Projects (Concessional loan)
2. Food/ Material Assistance (Grant) in 15 bordering districts of northern Nepal.
3. Kathmandu Ring Road Improvement Project with Flyover Bridges -(Grant)
4. Tatopani Frontier Inspection Station Project (Construction of ICDs at Zhangmu-Kodari)- (Grant)
5. Pokhara International Regional Airport (Loan)

With the signing of the Memorandum of Understanding on Cooperation under the Belt and Road Initiative on 12 May 2017 in Kathmandu between Nepal and China, new avenues for bilateral cooperation in the mutually agreed areas are expected to open. Nepal expects to upgrade its vital infrastructures, enhance cross-border connectivity with China and enhance people-to-people relations under this initiative. The major thrust of the MoU is to promote mutually beneficial cooperation between Nepal and China in various fields such as economy, environment, technology and culture. The MoU aims at promoting cooperation on policy exchanges, trade connectivity, financial integration and connectivity of people.

The Government of the People’s Republic of China provided substantial and spontaneous support in search, relief and rescue efforts of Nepal following the devastating earthquakes of 2015. China has provided 3 billion Yuan on Nepal’s Reconstruction to be used in the jointly selected 25 major projects for 2016–2018 period. On 23 December 2016, Nepal and the People’s Republic of China signed Agreement on Economic and Technical Cooperation in Beijing to provide grant assistance of RMB 1 billion to the Government of Nepal for implementing the Syaphrubesi-Rasuwagadhi Highway Repair and Improvement Project, Upgrading and Renovation Project of Civil Service Hospital, and Mutually agreed Post-Disaster Reconstruction Projects. The Letters of Exchange to initiate Syaphrubesi-Rasuwagadhi Highway Repair and Improvement Project was signed on May 9, 2017.

Trade and Investment:

China is the second largest trading partner of Nepal. In 2015/16, total exports to China stood at US$181 million with marginal increase from US$179 million in the previous fiscal year. In contrast, import from China has been growing at the rate of 39 per cent per year. It rose from US$421 million in fiscal year 2009/10 to US$1,247 million in fiscal year 2015/16. As a result, the trade deficit with China has risen from US$401 million in 2009/10 to US$1228 million in 2015/16. Although, China has given zero tariff entry facility to over 8000 Nepali products starting from 2009, Nepal hasn’t been able to bring the trade deficit down. Nepal exports 370 products including noodles and agro products to China. Nepal regularly participates various trade fairs and exhibitions organized in China. Nepal-China’s Tibet Economic and Trade Fair is the regular biannual event hosted by either side alternatively to enhance business interaction and promote economic cooperation between Nepal and TAR. The 15th Nepal China’s Tibet Economic and Trade Fair was held on 17–22 November 2015 in Bhrikutimandap, Kathmandu Nepal.

Nepal-China Non-Governmental Cooperation Forum established in 1996, which is led by the President of the Federation of the Nepali Chambers of Commerce and Industry (FNCCI) on the Nepali side and the Vice Head of the All-China Federation of Industry and Commerce (ACFIC) from the Chinese side. It is an initiative to mobilize the apex business organization of both sides to enhance cooperation between the private sectors of two sides. The 14th meeting of the Forum concluded in Kathmandu on 25–26 May 2017.

China is the largest source of Foreign Direct Investment in Nepal. Chinese investors have shown intent to spend over $8.3 billion in Nepal during the Nepal Investment Summit concluded in Kathmandu in March 2017.

Tourism:

China is the 2nd largest source of foreign tourist to Nepal. Over 100 thousands Chinese tourists visit Nepal annually. China has designated Nepal as the first tourist destination in South Asia for its people. The Government of Nepal has waived visa fees for the Chinese tourist effective from 1 January 2016. The Chinese Government has announced the year 2017 as Nepal Tourism Promotion Year in China. Both sides have been carrying out joint efforts to promote Nepal in China and encourage Chinese enterprises to invest in Nepal’s tourism sectors. Nepal has road connectivity via Rasuwagadhi and Zhangmu for trade and international travelers. There are 4 other border points designated for bilateral trade. Nepal has direct air link with Lhasa, Chengdu, Kunming, Guangzhou and Hong Kong SAR of China.

Education and Cultural Cooperation:

China provides scholarships every year not exceeding a total of 100 Nepali students studying in China. The Chinese side has been providing Chinese language training for 200 tourism entrepreneurs of Nepal for the next five years as per the understanding reached between two sides in March 2016. Both sides have been carrying out activities in culture and youth sectors as per the provisions of the MoU on Cultural Cooperation-1999 and MoU on Youth Exchange-2009. Both sides have been promoting people-to-people relations through regular hosting of cultural festival, friendly visits of the peoples of different walks of public life, exhibition, cultural and film show, food festivals etc. Sister city relations between the cities of two countries are growing and both sides have agreed to push cooperation though such relations. These relations are basically meant for carrying out exchanges and cooperation in the fields of economy, trade, transportation, science and technology, culture, tourism, education, sports and health, personnel, etc.

Regional and International Affairs:

Nepal is the founding member of the AIIB. Nepal holds the observer status in the Shanghai Cooperation Organization. Both countries are also the member of the Asia Cooperation Dialogue. China is the observer of the SAARC. Both countries have been cooperating each other in various regional and UN forums on the matters of common concerns.

Though Nepal initially let Tibetans Khampa rebels to make use of Nepalese territory in early 1960s, bilateral relations have generally been very good from 1975 onwards, after annexation of Kingdom of Sikkim by India in 1975. As much as twenty thousand Tibetan refugees live in Nepal and this has been a major issue of concern between China and Nepal. Kathmandu has in several instances been cracking down on the activities of the Tibetans receiving international condemnation. In 2005, Nepalese Foreign Minister Ramesh Nath Pandey called China "an all weather friend" and King Gyanendra's regime was also instrumental in inducting China into the SAARC. Nepalese in general, hold a positive view about the influence of China. In recent years, China has been one of the largest aid donors to Nepal just behind the UK. China is also Nepal's largest source of FDI.

See Denmark–Nepal relations.

Nepal formally tied diplomatic relations with the EU in 1975. EU established its Technical Office at Kathmandu in 1992. Nepal established residential embassy in Brussels in 1992. EU Delegation office in Kathmandu has been upgraded to the Ambassadorial level since 2009 December.

Development Cooperation:

EU is the largest development partner and the second largest trade partner (if taken as a single trade bloc) of Nepal. Until 2013, EU assistance to Nepal was provided in two main ways: on a bilateral basis through the formulation of successive Country Strategy Papers (CSPs) in close partnership with the Government of Nepal and on a multilateral basis including all actions outside the CSP mainly funded through thematic budget lines. Looking at the history of CSP’s for Nepal, the first CSP 2001–2006 allocated €70 million, and, the second CSP 2007–2013 allocated €114 million for Nepal. Cumulative contribution from EU to Nepal’s development has reached 360 million Euro spread over more than 70 projects till 2013.
Starting from 2014, the EU has begun channeling its development cooperation under its Multi-Annual Indicative Program (MIP). The EU has increased its development cooperation to Nepal by threefold for the current period of 2014–2020 compared to the proceeding period of the same duration. The MIP had identified three focal sectors for Nepal: €146 million for sustainable rural development, with focus on agricultural productivity and value addition, job creation, market access infrastructure, and nutrition (40.5%); €136.4 million for education, with the aim to improving basic education, quality, livelihood skills and equity for vulnerable and disadvantaged (38%); €74 million for strengthening democracy and decentralization, including its engagement in the area of public finance management reform efforts of the government at local and national level (20.5%); and remaining €3.6 million for other support measures (1%). The EU is also a major donor partner of the Nepal Peace Trust Fund.

Cooperation with European Investment Bank (EIB):

Nepal and EIB signed an umbrella agreement for financial cooperation on the 7th of May 2012 paving the way for major investments from EIB in Nepal’s infrastructure and energy sectors. Following the agreement EIB has already committed a loan assistance of Euro 55 million for Tanahu Hydropower Project (140 MW) which has a total cost of US$500 million. EIB has also expressed its commitments on immediate additional concessional loan assistance of Rs. 1.5 billion for the same project. Talks are underway for EIB investment of $120 million for Kaligandaki-Marsyangdi Corridor Transmission Project, and $30 million for upgrading the Trishuli Corridor Transmission Line. Ms. Magdalena Alvarez Arza, Vice President of the European Investment Bank visited Nepal in June 2014 to work out on those commitments.

Trade:

The EU is one of the principal trading partners of Nepal, second largest export market with 13% share. The EU imports mainly handmade carpets, textile, gems and jewellery, wood and paper products, leather products, etc. from Nepal. Nepal imports engineering goods, telecommunication equipment, chemical and minerals, metal and steel, agricultural products, etc. from the EU countries.

The EU started providing duty-free and quota-free facilities to the Nepalese exports under its Everything But Arms (EBA) policy for the LDCs from 2001. EU introduced the new Generalised System of Preferences (GSP) in 2006 which will remain valid till 2015. Under this scheme, for nearly 2,100 products out of 11,000, except arms and ammunitions, the EU duty rate will be zero.

Humanitarian Aid:

The European Commission is one of the biggest sources of humanitarian aid to Nepal. It has long been associated with the efforts of disaster management and mitigation projects in Nepal. EU Humanitarian Aid and Civil Protection Department (ECHO) has provided over Euro 74 million worth of humanitarian aid to Nepal since 2001 A.D. As a gesture of European solidarity to help those who are worst affected by the recent monsoon floods, the EU Delegation office in Kathmandu has provided Euro 250,000 assistance to flood affected people of mid-western region of Nepal in September 2014.

Nepal and the French Republic entered into diplomatic relations on 20 April 1949. Bilateral economic cooperation programme commenced in February 1981 when the two countries signed the First Protocol amounting to French Franc 50 million loan which was converted into debt in 1989. Food aid and the counterpart funds that it generated have been the main form of aid since 1991. Main areas of cooperation are national seismologic network, petroleum exploration, restructuring of Water Supply Corporation, the Kavre Integrated Project and Gulmi and Arghakhanchi Rural Development Project, rehabilitation of airports, ‘food for work’, and others.

Nepal and France have signed an agreement concerning Reciprocal Promotion and Protection of Investment in 1983. The major areas of French investment are hotels, restaurants, medicine, aluminium windows and doors, vehicle body building sectors. Alcatelhad became the leading supplier of the Nepal Telecommunication Corporation, with 200,000 lines installed, and fibre optic cables. Cegelec secured a 24 million dollars contract in respect of the construction of Kali Gandaki hydroelectric project.

The Government of Nepal awarded a contract to Oberthur Technologies of France in 2010, for printing, supply, and delivery of Machine Readable Passports. A significant number of French tourists (24,097 in 2014, 16, 405 in 2015, and, 20,863 in 2016) arrive in Nepal from France each year.

Diplomatic relations between Nepal and the Federal Republic of Germany were established in 1958. Nepal established its embassy in Berlin on 5 July 1965. The Federal Republic of Germany has been maintaining an embassy in Kathmandu since 1963.

Development cooperation:

The Federal Republic of Germany is one of the major donors for Nepal’s development efforts. Germany began its development cooperation to Nepal in 1961 with the technical assistance for the establishment of a Technical Training Institute at Thapathali. In 1964, it provided soft loans to Nepal Industrial Development Cooperation (NIDC). The most notable financial assistance from Germany for a single project has been that of DM 250 million for Middle Marsyangdi Hydroelectric Project.

Trade:

Germany is Nepal’s third largest trading partner after India and the US, and the biggest export market for Nepali products in Europe. Germany is an important market for Nepal, particularly for carpets and textile products. Nepal’s main imports from Germany are machinery and industrial products. In recent years, the bilateral balance of trade has regularly shown a surplus in Nepal’s favour. The annual volume of bilateral trade has remained fairly constant over the last few years, at around EUR 50 million. Besides carpet, export to Germany from Nepal include handicraft, silver jewellery, garments, leather, wooden and bamboo goods, lentils, tea, essential oils from herb and aromatic plants. Nepal imports mainly industrial raw materials, chemicals, machinery equipment and parts, electric and electronic goods, vehicles etc. from Germany.

Scientific and Academic Cooperation:

Nepal is also a priority country for the German Research Foundation (DFG) with more than 40 research projects operated in Nepal so far, including a major project by the University of Hamburg to catalogue some 160,000 Nepalese (Tibetan and Newari) manuscripts, which were able to be microfilmed with German support between 1970 and 2002.

Both countries established diplomatic relations in 1985. The Holy See has a nunciature in the country. Nepal Embassy, Berlin is accredited as non-residential embassy for Holy See.*Nepal (nunciature)

Both countries established diplomatic relations on May 25, 1981. 

As close neighbours, India and Nepal share a unique relationship of friendship and cooperation characterized by open borders and deep-rooted people-to-people contacts of kinship and culture. There has been a long tradition of free movement of people across the borders. The India-Nepal Treaty of Peace and Friendship of 1950 forms the bedrock of the special relations that exist between India and Nepal.

Political:

Beginning with the 12-Point understanding reached between the Seven Party Alliance and the Maoists at Delhi in November 2005, Government of India has welcomed the road-map laid down by the historic Comprehensive Peace Agreement of November 2006 towards political stabilization in Nepal, through peaceful reconciliation and inclusive democratic processes. India has consistently responded with a sense of urgency to the needs of the people and Government of Nepal in ensuring the success of the peace process and institutionalization of multi–party democracy through the framing of a new Constitution by a duly elected Constituent Assembly. India has always believed that only an inclusive Constitution with the widest possible consensus by taking on board all stakeholders would result in durable peace and stability in Nepal. India’s core interest in Nepal is a united Nepal’s peace and stability which has a bearing on India as well because of the long and open border shared between India and Nepal. Nepal’s second Constituent Assembly promulgated a Constitution on 20 September 2015 amid protests by Madhes-based parties and other groups. The Government of India has expressed grave concern regarding the ongoing protests and has urged the Government of Nepal to make efforts to resolve all issues including the new citizenship laws through a credible political dialogue.

Economic:

Indian firms are the biggest investors in Nepal, accounting for about 38.3% of Nepal’s total approved foreign direct investments. Till 15 July 2013, the Government of Nepal had approved a total of 3004 foreign investment projects with proposed FDI of Rs. 7269.4 crore. There are about 150 operating Indian ventures in Nepal engaged in manufacturing, services (banking, insurance, dry port, education and telecom), power sector and tourism industries. Some large Indian investors include ITC, Dabur India, Hindustan Unilever, VSNL, TCIL, MTNL, State Bank of India, Punjab National Bank, Life Insurance Corporation of India, Asian Paints, CONCOR, GMR India, IL&FS, Manipal Group, MIT Group Holdings, Nupur International, Transworld Group, Patel Engineering, Bhilwara Energy, Bhushan Group, Feedback Ventures, RJ Corp, KSK Energy, Berger Paints, Essel Infra Project Ltd. and Tata Power etc.

Development Assistance:

Government of India provides substantial financial and technical development assistance to Nepal, which is a broad-based programme focusing on creation of infrastructure at the grass-root level, under which various projects have been implemented in the areas of infrastructure, health, water resources, education and rural & community development. In recent years, India has been assisting Nepal in development of border infrastructure through upgradation of roads in the Terai areas; development of cross-border rail links at Jogbani–Biratnagar, Jaynagar-Bardibas, Nepalgunj Road-Nepalgunj, Nautanwa-Bhairhawa, and New Jalpaigudi-Kakarbhitta; and establishment of Integrated Check Posts at Raxaul-Birgunj, Sunauli-Bhairhawa, Jogbani-Biratnagar, and Nepalgunj Road-Nepalgunj. The total economic assistance extended under ‘Aid to Nepal’ budget in FY 2014–15 was Rs. 300 crore.

Currently, 36 intermediate and large projects such as construction of a National Police Academy at Panauti, Nepal Bharat Maitri Pashupati Dharmashala at Tilganga, a Polytechnic at Hetauda, and the National Trauma Centre at Kathmandu are at various stages of implementation. In addition, Government of India’s Small Development Projects (SDPs) programme in Nepal extends assistance for the implementation of projects costing less than NRs 5 crore (approx.. INR 3.125 crore) in critical sectors such as health, education & community infrastructure development. So far, 243 SDPs have been completed and 233 are under various stages of implementation in 75 districts of Nepal, with a total outlay of over Rs 550 crore. Till date, India has gifted 502 ambulances and 98 school buses to various institutions and health posts across Nepal’s 75 districts.

Education:

GOI provides around 3000 scholarships/seats annually to Nepali nationals for various courses at the Ph.D/Masters, Bachelors and plus–two levels in India and in Nepal.

Indian Community in Nepal:

Around 6,000 Indians are living/domiciled in Nepal.

Nepal was the first and until recently the only nation in South and Central Asia to establish diplomatic ties with Israel. The bilateral relation between the two countries has been good. Traditionally, Nepal votes in favor of Israel at the UN and abstains from resolution opposed by the Israeli government barring few exceptions. Israel-Nepal relations are based on mutual security concerns.

Bishweshwar Prasad Koirala, Prime Minister of Nepal from 1959 to 1960, had a strongly pro-Israel foreign policy. King Mahendra visited Israel in 1963 and maintained Koirala's special relationship. Nepal has continued to maintain strong diplomatic ties with Israel despite numerous change in government.

Nepal-Japan relations date back to the late eighteenth century. The relationship became formal with the establishment of diplomatic relations on 1 September 1956. The Embassy of Nepal was established in Tokyo in 1965 and Japan established its embassy in Kathmandu in 1967. Nepal has honorary consulates in Osaka and Fukuoka. Japan is one of the largest aid donors to Nepal.

Economic Cooperation:

Japan has been contributing to the socio-economic development of Nepal since 1954. Japan has been assisting Nepal in the form of bilateral grant, bilateral loan, multilateral aid and technical assistance. Japan has been assisting Nepal for the promotion of peace and democracy by contributing to the socio-economic development of the country. The major areas of Japan’s economic cooperation have been human resource development, social sectors including health, agriculture development, infrastructure development, environment protection, water supply, culture, etc. Japan also provides concessional loan for the infrastructure development in Nepal. Tanahun Hydro and Nagdhunga tunnel projects are ongoing projects under this scheme.
On human resource development, Japan has been providing annual scholarships to Government officials of Nepal in various fields under the JDS scheme starting from 2016. The Government of Japan started providing technical training to Nepali since Japan joined the Colombo Plan in 1954. Japan has also been providing Japan Overseas Cooperation Volunteers (JOCV) and Senior Volunteers to Nepal under JICA Volunteer Program. JOCV Nepal program was launched in 1970. The Government and people of Japan extended spontaneous support in the aftermath of 2015 earthquakes in Nepal. The Japanese Government made over NRS 26 billion grant for reconstruction works in Nepal.

Trade and Investment:

Japan is one of the important trading partners of Nepal. Nepal exports pashmina products, ready-made garments, woolen goods, carpets, handicrafts, Nepali paper and paper products, leather goods, and silverware and ornaments. Nepal’s imports from Japan include vehicles and spare parts, electronic goods, machinery and equipment, iron and steel products, photographic goods, medical equipment and fabric. There is an ample scope of collaborating in trade sector by introducing Japanese production process or integrating product development by exporting niche raw materials in Japan. Japan is one of major sources of Foreign Direct Investment in Nepal. The total FDI amount for the 2015/16 was NRS 223.4 billion.

Nepali Diaspora in Japan:
The number of Nepali nationals living in Japan is now more than 60,000, which was only 31,531 at the end of 2013. Nepali community is 5th largest foreign communities in Japan. Every year over 10,000 Nepali students go to Japan to pursue higher studies and Japanese languages. Japan is the 2nd most preferred destination for abroad study to the Nepali students.

Both countries established diplomatic relations on May 18, 2010.

Both countries established diplomatic relations on December 4, 2012.

Malaysia has an embassy in Kathmandu, and Nepal has an embassy in Kuala Lumpur. Both countries established diplomatic relations on 1 January 1960, with bilateral relations between Malaysia and Nepal have developed from historic grounds.

Diplomatic relations were established on 26 January 1973. Norway established an embassy in Kathmandu in 2000. Norway's aid to Nepal was around 2 million NOK in 2008. Norwegian aid prioritizes education, good governance and energy. 

In 2008, Norwegian Prime Minister Jens Stoltenberg and Minister of the Environment and International Development Erik Solheim visited Nepal. In 2009, Prime Minister Prachanda visited Norway. In May 2008, a small bomb exploded outside the Norwegian embassy in Kathmandu. No one was injured.

The bilateral relations between Nepal and the Islamic Republic of Pakistan were fully established between 1962 and 1963. Both nations have since sought to expand trade, strategic cooperation.

Nepal and the Soviet Union had established diplomatic relations in 1956. After the collapse of the Soviet Union, Nepal extended full diplomatic recognition to the Russian Federation as its legal successor. Since then numerous bilateral meetings have taken place between both sides. Since 1992 numerous Nepalese students have gone to Russia for higher studies on a financial basis. In October 2005 the Foreign ministers of both countries met to discuss cooperation on a variety of issues including political, economic, military, educational, and cultural. Both countries maintain embassies in each other's capitals. Russia has an embassy in Kathmandu while Nepal has an embassy in Moscow.

Both countries established diplomatic relations on September 27, 2007.


Both countries established diplomatic relations on December 15, 2011.

In addition to the in-kind and monetary donations and emergency relief workers sent by the government of the Republic of Korea immediately after the latest earthquake in Nepal the Korean government provided grant aid worth 10 million US dollars to assist with Nepal’s recovery and reconstruction efforts.

Diplomatic relations between Nepal and Switzerland were established in November 1959. Nepal has its embassy in Geneva. Switzerland opened its first residential embassy in Nepal on August 17, 2009.

Development Cooperation:

Switzerland’s cooperation to Nepal dates back to 1950s. Economic cooperation programme was initiated in 1956 with technical assistance scheme for cheese production in the eastern high hills. The Swiss Government, through the Swiss Agency for Development Cooperation (SDC), has been initiating various development projects ranging from Technical Instructor Training Project to Maternity Child Health Care Project. Nepal is among 16 priority countries to receive Swiss assistance worldwide.

Technical and Vocational Education and Training (TVET):

The history of Swiss cooperation in TVET in Nepal began in 1962 with the establishment of Balaju School of Engineering and Technology-BTTC (previously known as Mechanical Training Centre – MTC). The BTTC has trained estimated 10,000 youths. Similarly, SDC established Jiri Technical School (JTS) as another milestone of TVET in 1982. The JTS is estimated to have trained another 6,000 youths. Training Institute for Technical Instruction (TITI), previously known as Technical Instructors Training Institute was established in 1991 with the assistance from the Swiss Government, SDC, and Swisscontact, the Swiss Foundation for Technical Cooperation. TITI, already a centre of excellence in Nepal, has trained 19,000 instructional, managerial and curriculum developers from Nepal and other countries. Under Technical Support to Universities and CTEVT SDC supported to strengthening engineering education in Nepal through financial and technical support to Pulchowk Engineering Institute of TU, and its support to KU.

Trade:

Nepal-Switzerland trade relation has witnessed growth over the years. The balance of trade is mostly unfavorable to Nepal. Major Nepalese exports to Switzerland consist of tea, spices, plants and parts of plants, homeopathic medicaments, hand knotted woolen carpets, ready-made garments, handicrafts, woolen goods, Nepalese paper and paper products, hides and silverware &jewellery. Similarly Nepal imports time pieces, pharmaceutical products, chemicals, construction material, transport equipment (bus, truck and parts), machinery and parts, medical equipment, photographic paper, footwear, aluminium foil, air conditioner, etc. from Switzerland.

Both countries established diplomatic relations on September 13, 2005.

Nepal established diplomatic relations with the United Kingdom in 1816. The United Kingdom is also the first country in the world with which Nepal had established diplomatic relations. The United Kingdom is the first country in the world which established its embassy in Kathmandu, the capital of Nepal. This is the country where Nepal had established its first diplomatic mission (Legation). Nepal had established its legation in London in 1934, which was the first Nepalese diplomatic mission established at the foreign country. It was elevated to the ambassador level in 1947 A.D.

The UK remained one of the top development partners of Nepal with the annual British aid on an increasing trend. Tourism, trade, education, and the British Gurkha connection remained the key dimensions of the bilateral relations. Since then, relations between the two countries have continued to grow, with a new Treaty of Perpetual Peace and Friendship signed in 1950 which expanded areas of cooperation and an exchange of State Visits. Amicable relations continue today; Nepal continues to be the source of recruitment of Gurkha soldiers into the British army – a tradition dating back to the nineteenth century but still an essential part of Britain’s modern army – and the United Kingdom remains one of the most significant providers of development assistance to Nepal.

Nepal and the United States of America (USA) established the diplomatic relations between them on 25 April 1947. Nepal established its embassy in Washington D.C. on 3 February 1958. On 6 August 1959, American Embassy in Kathmandu was opened. 
According to the 2012 U.S. Global Leadership Report, 41% of Nepalese people approve of U.S. leadership. Since 1951, the United States has provided more than $7 billion in bilateral economic assistance to Nepal. In recent years, annual bilateral U.S. economic assistance through the U.S. Agency for International Development (USAID) has averaged $40 million per year.
Development Cooperation:

The USA extended development assistance to Nepal with its Point Four Program in 1951. USAID/Nepal is the development assistance arm of the US Mission to Nepal. Various sectors such as transport, communication, public health, family planning, malaria eradication, agriculture, forestry, energy etc. have come to benefit from the development assistance spanning more than 5 decades. Of late, US cooperation is also geared towards the institutionalization of peace and democracy and protection and promotion of human rights in accordance with its country strategy document. The US cooperation has equally been instrumental in the fields of human resource development and institution building. There have been regular exchanges of visits and sharing of expertise and experiences between the armies of the two countries in the area of training, disaster management, logistics management, counter terrorism, interoperability and so on.

Trade/Investment:

The US is one of the important trading partners of Nepal. It is also the biggest source of hard currency for Nepal primarily from the export of garments and carpets as well as from tourist incomes. After the end of the quota system under Multi-fibre Agreement (MFA) in 2004 the export of Nepali readymade garments to the United States of America has declined significantly. Nepal has been consistently advocating for duty-free facilities for its exports to the US, especially the ready-made garments.

Trade Preferences for Nepal: Trade Facilitation and Trade Enforcement Act 2015 (H.R.1907):

The duty-free program, specially designed for Nepal, came into effect on December 15, 2016 following introduction of the US Trade Facilitation and Trade Enforcement Act by former US President Barack Obama. The Act was introduced to support Nepal’s economic recovery in the aftermath of devastating 2015 earthquakes.
Under this program, 66 Nepali products are granted duty-free entry into the US till December 31, 2025. This means goods like carpets and rugs, shawls, scarves, luggage articles, handbags, pocket goods, such as wallets, travel bags and other containers, headbands, blankets, hats and gloves, which previously used to be subject to tariffs ranging from 5 percent to over 20 percent can now enter the US market at zero tariff. However, such goods must be grown, produced, or manufactured in Nepal, with the cost of the Nepali materials plus the cost of processing standing at at least 35 percent of the product’s sales price.

Millennium Challenge Corporation:

Nepal joined MCC in 2010 when it was considered eligible for its “Threshold Program”, after passing through a scorecard consisting of 20 indicators related to political, economic and social situation of the country. Threshold Program aimed to assist a country in becoming compact eligible by supporting targeted policy and institutional reforms. MCC works with Threshold Program – eligible countries on these reforms through country-specific threshold program. In December 2014, MCC decided to include Nepal for its Compact Development Program, The Readout of the MCC’s Board of Directors mentioned: ‘Nepal has consistently passed the scorecard criteria for the past four years and continues to demonstrate clear progress in institutionalizing democratic processes’.
At present an MCC Compact for Nepal (worth 630 million USD) is under consideration and is likely to be signed by Nepali and the US authorities soon.

US Peace Corps Volunteers:

The Peace Corps Nepal, which temporarily suspended its operations and activities from September 2004, has resumed its operations from January 17, 2012. The volunteers have been working in rural areas in various sectors including teaching in schools.





</doc>
<doc id="21332" url="https://en.wikipedia.org/wiki?curid=21332" title="Netherlands Antilles">
Netherlands Antilles

The Netherlands Antilles (, ; Papiamentu: "Antia Hulandes") was a constituent country of the Kingdom of the Netherlands. The country consisted of several island territories located in the Caribbean Sea. The islands were also informally known as the Dutch Antilles. The country came into being in 1954 as the autonomous successor of the Dutch colony of Curaçao and Dependencies, and was dissolved in 2010. The former Dutch colony of Surinam, although it was relatively close by on the continent of South America, did not become part of Netherlands Antilles but became a separate autonomous country at the same time. All the island territories that belonged to the Netherlands Antilles remain part of the kingdom today, although the legal status of each differs. As a group they are still commonly called the Dutch Caribbean, regardless of their legal status.

The islands of the Netherlands Antilles are all part of the Lesser Antilles island chain. Within this group, the country was spread over two smaller island groups: a northern group (part of Leeward Islands) and a western group (part of the Leeward Antilles). No part of the country was in the southern Windward Islands.

This island sub-group was located in the eastern Caribbean Sea, to the east of Puerto Rico. There were three islands, collectively known as the "SSS islands":
They lie approximately north-east of the ABC Islands.

This island sub-group was located in the southern Caribbean Sea off the north coast of Venezuela. There were three islands collectively known as the "ABC Islands":

The Netherlands Antilles had an equatorial climate, with hot weather all year round. The Leeward islands are subject to hurricanes in the summer months, while those islands located in the Leeward Antilles are warmer and drier.

Spanish-sponsored explorers discovered both the leeward (Alonso de Ojeda, 1499) and windward (Christopher Columbus, 1493) island groups. However, the Spanish Crown only founded settlements in the leeward islands. In the 17th century the islands were conquered by the Dutch West India Company and colonized by Dutch settlers. From the last quarter of the 17th century, the group consisted of six Dutch islands: Curaçao (settled in 1634), Aruba (settled in 1636), Bonaire (settled in 1636), Sint Eustatius (settled in 1636), Saba (settled in 1640) and Sint Maarten (settled in 1648). In the past, Anguilla (1631–1650), the present-day British Virgin Islands (1612–1672), St. Croix and Tobago had also been Dutch. During the American Revolution Sint Eustatius, along with Curaçao, was a major trade center in the Caribbean, with Sint Eustatius a major source of supplies for the Thirteen Colonies. It had been called "the "Golden Rock" because of the number of wealthy merchants and volume of trade there. The British sacked its only town, Oranjestad, in 1781 and the economy of the island never recovered. Unlike many other regions, few immigrants went to the Dutch islands, due to the weak economy. However, with the discovery of oil in Venezuela in the nineteenth century, British-Dutch Shell Oil Company established refineries in Curaçao, while the U.S. processed Venezuelan crude oil in Aruba. This resulted in booming economies on the two islands, which turned to bust in the 1980s when oil refineries were closed. The various islands were united as a single country — the Netherlands Antilles — in 1954, under the Dutch crown. The country was dissolved on 10 October 2010.

Curaçao and Sint Maarten became distinct constituent countries alongside Aruba which had become a distinct constituent country in 1986; whereas Bonaire, Sint Eustatius, and Saba (the "BES Islands") became special municipalities within the Netherlands proper.
From 1815 onwards Curaçao and Dependencies formed a colony of the Kingdom of the Netherlands. Slavery was abolished in 1863, and in 1865 a government regulation for Curaçao was enacted that allowed for some very limited autonomy for the colony. Although this regulation was replaced by a constitution () in 1936, the changes to the government structure remained superficial and Curaçao continued to be ruled as a colony.

The island of Curaçao was hit hard by the abolition of slavery in 1863. Its prosperity (and that of neighboring Aruba) was restored in the early 20th century with the construction of oil refineries to service the newly discovered Venezuelan oil fields.

Colonial rule ended after the conclusion of the Second World War. Queen Wilhelmina had promised in a 1942 speech to offer autonomy to the overseas territories of the Netherlands, and British and American occupation—with the consent of the Dutch government—of the islands during the war led to increasing demands for autonomy within the population as well.

In May 1948 a new constitution for the territory entered into force, allowing the largest amount of autonomy possible under the Dutch constitution of 1922. Among other things, universal suffrage was introduced. The territory was also renamed "Netherlands Antilles". After the Dutch constitution was revised in 1948, a new interim Constitution of the Netherlands Antilles was enacted in February 1951. Shortly afterwards, on 3 March 1951, the Island Regulation of the Netherlands Antilles () was issued by royal decree, giving fairly wide autonomy to the various island territories in the Netherlands Antilles. A consolidated version of this regulation remained in force until the dissolution of the Netherlands Antilles in 2010.

The new constitution was only deemed an interim arrangement, as negotiations for a Charter for the Kingdom were already under way. On 15 December 1954 the Netherlands Antilles, Suriname and the Netherlands acceded as equal partners to an overarching Kingdom of the Netherlands, established by the Charter for the Kingdom of the Netherlands. With this move, the United Nations deemed decolonization of the territory complete and removed the Netherlands Antilles from the United Nations list of Non-Self-Governing Territories.

Aruba seceded from the Netherlands Antilles on 1 January 1986, paving the way for a series of referenda among the remaining islands on the future of the Netherlands Antilles. Whereas the ruling parties campaigned for the dissolution of the Netherlands Antilles, the people voted for a restructuring of the Netherlands Antilles. The coalition campaigning for this option became the Party for the Restructured Antilles, which ruled the Netherlands Antilles for much of the time until its dissolution on 10 October 2010.

Even though the referendums held in the early 1990s resulted in a vote in favour of retaining the Netherlands Antilles, the arrangement continued to be an unhappy one. Between June 2000 and April 2005, each island of the Netherlands Antilles had a new referendum on its future status. The four options that could be voted on were the following:

Of the five islands, Sint Maarten and Curaçao voted for "status aparte", Saba and Bonaire voted for closer ties with the Netherlands, and Sint Eustatius voted to stay within the Netherlands Antilles.

On 26 November 2005, a Round Table Conference (RTC) was held between the governments of the Netherlands, Aruba, the Netherlands Antilles, and each island in the Netherlands Antilles. The final statement to emerge from the RTC stated that autonomy for Curaçao and Sint Maarten, plus a new status for Bonaire, Sint Eustatius, and Saba (BES) would come into effect by 1 July 2007.
On 12 October 2006, the Netherlands reached an agreement with Bonaire, Sint Eustatius, and Saba: this agreement would make these islands special municipalities.

On 3 November 2006, Curaçao and Sint Maarten were granted autonomy in an agreement, but this agreement was rejected by the then island council of Curaçao on 28 November. The Curaçao government was not sufficiently convinced that the agreement would provide enough autonomy for Curaçao. On 9 July 2007 the new island council of Curaçao approved the agreement previously rejected in November 2006. A subsequent referendum approved the agreement as well.

The acts of parliament integrating the "BES" islands (Bonaire, Sint Eustatius and Saba) into the Netherlands were given royal assent on 17 May 2010. After ratification by the Netherlands (6 July), the Netherlands Antilles (20 August), and Aruba (4 September), the "Kingdom act amending the Charter for the Kingdom of the Netherlands with regard to the dissolution of the Netherlands Antilles" was signed by the three countries in the closing Round Table Conference on 9 September 2010 in The Hague.

The Island Regulation had divided the Netherlands Antilles into four island territories: Aruba, Bonaire, Curaçao (ABC), and the islands in the Leeward Islands. In 1983, the island territory of the Leeward was split up to form the new island territories of Sint Maarten, Saba, and Sint Eustatius (SSS). In 1986, Aruba seceded from the Netherlands Antilles, reducing the number of island territories to five. After the dissolution of the Netherlands Antilles in 2010, Curaçao and Sint Maarten became autonomous countries within the Kingdom and Bonaire, Sint Eustatius and Saba (BES) became special municipalities of the Netherlands.

The islands of the former country of the Netherlands Antilles are currently divided are two main groups for political and constitutional purposes:

There are three Caribbean islands that are countries () within the Kingdom of the Netherlands: Aruba, Curaçao, and Sint Maarten. (The Netherlands is the fourth constituent country in the Kingdom of the Netherlands.)

Sint Maarten is approximately one half of the island of Saint Martin; the remaining northern half of the island — the "Collectivity of Saint-Martin" — is an overseas territory of France.

There are three Caribbean islands that are special municipalities of the Netherlands alone: Bonaire, Sint Eustatius, and Saba. Collectively, these special municipalities of the Netherlands are also known as the BES islands. There are also several smaller islands, like Klein Curaçao and Klein Bonaire, that belong to one of the island countries or special municipalities.

The Constitution of the Netherlands Antilles was proclaimed on 29 March 1955 by Order-in-Council for the Kingdom. Together with the Island Regulation of the Netherlands Antilles it formed the constitutional basis for the Netherlands Antilles. Because the Constitution depended on the Island Regulation, which gave fairly large autonomy to the different island territories, and the Island Regulation was older than the Constitution, many scholars describe the Netherlands Antilles as a federal arrangement.

The head of state was the monarch of the Kingdom of the Netherlands, who was represented in the Netherlands Antilles by a governor. The governor and the council of ministers, chaired by a prime minister, formed the government. The Netherlands Antilles had a unicameral legislature called the Estates of the Netherlands Antilles. Its 22 members were fixed in number for the islands making up the Netherlands Antilles: fourteen for Curaçao, three each for Sint Maarten and Bonaire, and one each for Saba and Sint Eustatius.

The Netherlands Antilles were not part of the European Union, but instead listed as overseas countries and territories (OCTs). This status was kept for all the islands after dissolution, and will be kept until at least 2015.

Tourism, petroleum transshipment and oil refinement (on Curaçao), as well as offshore finance were the mainstays of this small economy, which was closely tied to the outside world. The islands enjoyed a high per capita income and a well-developed infrastructure as compared with other countries in the region.

Almost all consumer and capital goods were imported, with Venezuela, the United States, and Mexico being the major suppliers, as well as the Dutch government which supports the islands with substantial development aid. Poor soils and inadequate water supplies hampered the development of agriculture. The Antillean guilder had a fixed exchange rate with the United States dollar of 1.79:1.

A large percentage of the Netherlands Antilleans descended from European colonists and African slaves who were brought and traded here from the 17th to 19th centuries. The rest of the population originated from other Caribbean islands as well as Latin America, East Asia and elsewhere in the world. In Curaçao there was a strong Jewish element going back to the 17th century.

The language Papiamentu was predominant on Curaçao and Bonaire (as well as the neighboring island of Aruba). This creole descended from Portuguese and West African languages with a strong admixture of Dutch, plus subsequent lexical contributions from Spanish and English. An English-based creole dialect, formally known as Netherlands Antilles Creole, was the native dialect of the inhabitants of Sint Eustatius, Saba and Sint Maarten.

After a decades-long debate, English and Papiamentu were made official languages alongside Dutch in early March 2007.
Legislation was produced in Dutch, but parliamentary debate was in Papiamentu or English, depending on the island. Due to a massive influx of immigrants from Spanish-speaking territories such as the Dominican Republic in the Windward Islands, and increased tourism from Venezuela in the Leeward Islands, Spanish had also become increasingly used.

The majority of the population were followers of the Christian faith, with a Protestant majority in Sint Eustatius and Sint Maarten, and a Roman Catholic majority in Bonaire, Curaçao and Saba. Curaçao also hosted a sizeable group of followers of the Jewish religion, descendants of a Portuguese group of Sephardic Jews that arrived from Amsterdam and Brazil from 1654. In 1982, there was a population of about 2,000 Muslims, with an Islamic association and a mosque in the capital.

Most Netherlands Antilleans were Dutch citizens and this status permitted and encouraged the young and university-educated to emigrate to the Netherlands. This exodus was considered to be to the islands' detriment, as it created a brain drain. On the other hand, immigrants from the Dominican Republic, Haiti, the Anglophone Caribbean and Colombia had increased their presence on these islands in later years.

The origins of the population and location of the islands gave the Netherlands Antilles a mixed culture.

Tourism and overwhelming media presence from the United States increased the regional United States influence. On all the islands, the holiday of Carnival had become an important event after its importation from other Caribbean and Latin American countries in the 1960s. Festivities included "jump-up" parades with beautifully colored costumes, floats, and live bands as well as beauty contests and other competitions. Carnival on the islands also included a middle-of-the-night j'ouvert (juvé) parade that ended at sunrise with the burning of a straw King Momo, cleansing the island of sins and bad luck.

Netherlands Lesser Antilles competed in the Winter Olympics of 1988, notably finishing 29th in the bobsled, ahead of Jamaica who famously competed but finished 30th.

Baseball is by far the most popular sport. Several players have made it to the Major Leagues, such as Xander Bogaerts, Andrelton Simmons, Hensley Meulens, Randall Simon, Andruw Jones, Kenley Jansen, Jair Jurrjens, Roger Bernadina, Sidney Ponson, Didi Gregorius, Shairon Martis, Wladimir Balentien, and Yurendell DeCaster. Xander Bogaerts competed in the 2013 World Series for the Boston Red Sox against the St. Louis Cardinals. Andruw Jones played for the Atlanta Braves in the 1996 World Series hitting two home runs in the first game against the New York Yankees.

Three athletes from the former Netherlands Antilles competed in the 2012 Summer Olympics. They, alongside one athlete from South Sudan, competed under the banner of Independent Olympic Athletes.

Unlike the metropolitan Netherlands, same-sex marriages were not performed in the Netherlands Antilles, but those performed in other jurisdictions were recognised.

The main prison of the Netherlands Antilles was Koraal Specht, later known as Bon Futuro. It was known for ill treatment of prisoners and bad conditions throughout the years.

The late Venezuelan President Hugo Chávez claimed that the Netherlands was helping the United States to invade Venezuela due to military games in 2006. Curaçao is under consideration as a Cooperative Security Location, not a full Main Operating Base.




</doc>
<doc id="21334" url="https://en.wikipedia.org/wiki?curid=21334" title="Geography of the Netherlands Antilles">
Geography of the Netherlands Antilles

The Netherlands Antilles was a constituent country in the Caribbean Sea. It consisted of two island groups, the ABC islands Curaçao, Bonaire and (until 1986) Aruba just north of Venezuela, and the SSS islands east of the Virgin Islands.

The Netherlands Antilles had 960 km² (1153 km² before 1986) of land, which included no major lakes or other bodies of water.

Territory included the islands of Aruba until 1986 and Curaçao, Bonaire, Saba, Sint Eustatius, Sint Maarten (which is the Dutch part of the island of Saint Martin) until 2010 when the Netherlands Antilles was dissolved. Its only land boundary was with France on the island of Saint Martin, which was 10.2 kilometers in length. The Netherlands Antilles had 364 kilometers (432 km before 1986) of coastline.

Tropical; ameliorated by northeast trade winds
Maritime claims:
<br>"exclusive fishing zone:"

<br>"territorial sea:"
Terrain:
generally hilly, volcanic interiors

Elevation extremes:
<br>"lowest point:"
Caribbean Sea 0 m
<br>"highest point:"
Mount Scenery 

Natural resources:
phosphates (Curaçao only), salt (Bonaire only)

Land use:
<br>"arable land:"
10%
<br>"permanent crops:"
0%
<br>"permanent pastures:"
0%
<br>"forests and woodland:"
0%
<br>"other:"
90% (1993 est.)

Irrigated land:
NA km²

Natural hazards:
Curaçao and Bonaire are south of Caribbean hurricane belt and are rarely threatened; Sint Maarten, Saba, and Sint Eustatius are subject to hurricanes from July to October.

Environment - current issues:
NA


</doc>
<doc id="21335" url="https://en.wikipedia.org/wiki?curid=21335" title="Demographics of the Netherlands Antilles">
Demographics of the Netherlands Antilles

This article is about the demographic features of the population of the former Netherlands Antilles, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.

According to the official estimates of the Central Bureau of Statistics of the Netherlands Antilles, the five islands had a combined population of 211,871 as at 1 January 2013. The population of the individual islands was as follows: 
Bonaire - 17,408
Curaçao - 154,843
Saba - 1,991
Sint Eustatius - 4,020
Sint Maarten - 33,609

For comparison : Aruba - 103,400

The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.
The capital and largest city was Willemstad.

Age structure:

Population growth rate: 0,79% (2006 est.)

Birth rate: 14,78 births/1,000 population (2006 est.)

Death rate: 6,45 deaths/1,000 population (2006 est.)

Net migration rate: -0.4 migrant(s)/1,000 population (2006 est.)

Human sex ratio:

Infant mortality rate: 9,76 deaths/1,000 live births (2006 est.)

Life expectancy at birth:

Total fertility rate: 1.98 children born/woman (2008 est.)

Nationality:

Ethnic groups: mixed black 85%, Carib Amerindian, white, East Asian 15%

Religions: Roman Catholic 72%, Pentecostal 4,9%, Protestant 3.5%, Seventh-day Adventist 3,1%, Methodist 2,9%, other Christian 4,2%, Jehovah's Witnesses 1,7%, Jewish 1,3%

Languages: Dutch, English and Papiamento are official languages. Papiamento (a Portuguese-West African creole with Dutch and Spanish influence) predominates on Curaçao and Bonaire, while English is widely spoken. English is the most commonly spoken language on Sint Maarten, Saba, and Sint Eustatius. 

Literacy:


</doc>
<doc id="21337" url="https://en.wikipedia.org/wiki?curid=21337" title="Economy of the Netherlands Antilles">
Economy of the Netherlands Antilles

The Netherlands Antilles was an autonomous Caribbean country within the Kingdom of the Netherlands, which was formally dissolved in 2010.

Tourism, petroleum transshipment, and offshore finance were the mainstays of the economy, which was closely tied to the outside world. The islands enjoyed a high per capita income and a well-developed infrastructure as compared with other countries in the region at the time of the dissolution. Almost all consumer and capital goods were imported, with Venezuela, the United States, and Mexico being the major suppliers. Poor soils and inadequate water supplies hampered the development of agriculture.

Gross Domestic product- $3.81 billion
GDP:
purchasing power parity - $3 600 million (3,6 G$) (2007 est.)

GDP - real growth rate:
4,0% (2007 est.)

GDP - per capita:
purchasing power parity - $19 000 (2007 est.)

GDP - composition by sector:
<br>"agriculture:"
1%
<br>"industry:"
15%
<br>"services:"
84% (2007 est.)

Population below poverty line:
NA%

Household income or consumption by percentage share:
<br>"lowest 10%:"
± 1,5%
<br>"highest 10%:"
± 31%

Inflation rate (consumer prices):
3,0% (2007)

Labour force:
83 600 (2005)

Labour force - by occupation:
agriculture 1%, industry 20%, services 79% (2007 est.)

Unemployment rate:
9% (2007 est.)

Budget:
<br>"revenues:"
$757,9 million
<br>"expenditures:"
$949,5 million, including capital expenditures of $NA (2004 est.)

Industries:
tourism (Curaçao, Sint Maarten, and Bonaire), petroleum refining (Curaçao), petroleum transhipment facilities (Curaçao and Bonaire), light manufacturing (Curaçao)

Industrial production growth rate:
NA%

Electricity - production:
1 005 GWh (2004)

Electricity - production by source:
<br>"fossil fuel:"
100%
<br>"hydro:"
0%
<br>"nuclear:"
0%
<br>"other:"
0% (1998)

Electricity - consumption:
934,7 GWh (2004)

Electricity - exports:
0 kWh (2004)

Electricity - imports:
0 kWh (2004)

Agriculture - products:
aloes, sorghum, peanuts, vegetables, tropical fruit

Exports:
$2 076 million (f.o.b., 2004)

Exports - commodities:
petroleum products

Exports - partners:
US 32%, Panama 10.1%, Guatemala 7,9%, Haiti 6,4%, The Bahamas 5,1% (2005)

Imports:
$4 383 billion (c.i.f., 2004)

Imports - commodities:
crude petroleum, food, manufactures

Imports - partners:
Venezuela 50%, US 22,2%, Italy 5.2%, Netherlands 5% (2005)

Debt - external:
$2 680 million (2004)

Economic aid - recipient:
IMF provided $61 million in 2000, and the Netherlands continued its support with $40 million (2004)

Currency:
1 Netherlands Antillean guilder, gulden, or florin (NAf.) = 100 cents

Exchange rates:
Netherlands Antillean guilders, gulden, or florins (NAf.) per US$1 – 1.790 (fixed rate since 1989)

Fiscal year:
calendar year



</doc>
<doc id="21338" url="https://en.wikipedia.org/wiki?curid=21338" title="Telecommunications in Curaçao">
Telecommunications in Curaçao

Telecommunications in Curaçao and the Netherlands Antilles

Telephones - main lines in use: 75,000 (1995)

Telephones - mobile cellular: 11,727 (1995)

Telephone system: generally adequate facilities

Radio broadcast stations: AM 9, FM 4 (1998), shortwave 1 (Radio Netherlands transmits programs to North and South America from there)

Radios: 217,000 (1997)

Television broadcast stations: 3 (there is also a cable service which supplies programs received from various US satellite networks and two Venezuelan channels) (1997)

Televisions: 69,000 (1997)

Internet Service Providers (ISPs): 6 (1999)

Country code: .cw (Curaçao West Indies)


</doc>
<doc id="21339" url="https://en.wikipedia.org/wiki?curid=21339" title="Transport in the Netherlands Antilles">
Transport in the Netherlands Antilles

This article lists forms of Transport in the Netherlands Antilles.

No railway tracks exist in the Netherlands Antilles

All driving is on the right. 
Kralendijk (Bonaire), Philipsburg (Saint Martin), Willemstad (Curaçao)


5 (2005 est.)





</doc>
<doc id="21342" url="https://en.wikipedia.org/wiki?curid=21342" title="New Caledonia">
New Caledonia

New Caledonia () is a special collectivity of France in the southwest Pacific Ocean, east of Australia and from Metropolitan France. The archipelago, part of the Melanesia subregion, includes the main island of Grande Terre, the Loyalty Islands, the Chesterfield Islands, the Belep archipelago, the Isle of Pines, and a few remote islets. The Chesterfield Islands are in the Coral Sea. Locals refer to Grande Terre as "" ("the pebble").

New Caledonia has a land area of . Its population of 268,767 (Aug. 2014 census) consists of a mix of Kanak people (the original inhabitants of New Caledonia), people of European descent (Caldoches and Metropolitan French), Polynesian people (mostly Wallisians), and Southeast Asian people, as well as a few people of Pied-Noir and North African descent. The capital of the territory is Nouméa.

The earliest traces of human presence in New Caledonia date back to the Lapita period c. 1600 BCE to c. 500 BCE. The Lapita were highly skilled navigators and agriculturists with influence over a large area of the Pacific.
British explorer Captain James Cook was the first European to sight New Caledonia, on 4 September 1774, during his second voyage. He named it "New Caledonia", as the northeast of the island reminded him of Scotland. The west coast of Grande Terre was approached by Jean-François de Galaup, comte de Lapérouse in 1788, shortly before his disappearance, and the Loyalty Islands were first visited between 1793 and 1796 when Mare, Lifou, Tiga, and Ouvea were mapped by William Raven. The American whaler encountered the island named then Britania, and today known as Mar (Loyalty Is.) in November 1793. From 1796 until 1840, only a few sporadic contacts with the archipelago were recorded. About fifty American whalers (identified by Robert Langsom from their log books) have been recorded in the region (Grande Terre, Loyalty Is., Walpole and Hunter) between 1793 and 1887. Contacts became more frequent after 1840, because of the interest in sandalwood.

As trade in sandalwood declined, it was replaced by a new business enterprise, "blackbirding", a euphemism for taking Melanesian or Western Pacific Islanders from New Caledonia, the Loyalty Islands, New Hebrides, New Guinea, and the Solomon Islands into indentured or forced labour in the sugar cane plantations in Fiji and Queensland by various methods of trickery and deception. Blackbirding was practiced by both French and British-Australian traders, but in New Caledonia's case, the trade in the early decades of the twentieth century involved relocating children from the Loyalty islands to the Grand Terre for labour in plantation agriculture. New Caledonia's primary experience with blackbirding revolved around a trade from the New Hebrides (now Vanuatu) to the Grand Terre for labour in plantation agriculture, mines, as well as guards over convicts and in some public works. The historian Dorothy Shineberg's milestone study, The People Trade, discusses this 'migration'. In the early years of the trade, coercion was used to lure Melanesian islanders onto ships. In later years indenture systems were developed; however, when it came to the French slave trade, which took place between its Melanesian colonies of the New Hebrides and New Caledonia, very few regulations were implemented. This represented a departure from the British experience, since increased regulations were developed to mitigate the abuses of blackbirding and 'recruitment' strategies on the coast lines.

The first missionaries from the London Missionary Society and the Marist Brothers arrived in the 1840s. In 1849, the crew of the American ship "Cutter" was killed and eaten by the Pouma clan. Cannibalism was widespread throughout New Caledonia.

On 24 September 1853, under orders from Napoleon III, Admiral Febvrier Despointes took formal possession of New Caledonia, and Port-de-France (Nouméa) was founded on 25 June 1854. A few dozen free settlers settled on the west coast in the following years. New Caledonia became a penal colony, and from the 1860s until the end of the transportations in 1897, about 22,000 criminals and political prisoners were sent to New Caledonia. The "Bulletin de la Société générale des prisons" for 1888 indicates that 10,428 convicts, including 2,329 freed ones, were on the island as of 1 May 1888, by far the largest number of convicts detained in overseas penitentiaries. Among the convicts were many Communards, arrested after the failed Paris Commune, including Henri de Rochefort and Louise Michel. Between 1873 and 1876, 4,200 political prisoners were "relegated" to New Caledonia. Only 40 of them settled in the colony; the rest returned to France after being granted amnesty in 1879 and 1880.
In 1864, nickel was discovered on the banks of the Diahot River; with the establishment of the Société Le Nickel in 1876, mining began in earnest. The French imported labourers to work in the mines from neighbouring islands and the New Hebrides, and later from Japan, the Dutch East Indies, and French Indochina. The French government also attempted to encourage European immigration, without much success.

The indigenous population or Kanak were excluded from the French economy and from mining work, ultimately confined to reservations. This sparked a violent reaction in 1878 as High Chief Atal of La Foa managed to unite many of the central tribes and launched a guerrilla war which cost 200 Frenchmen and 1,000 Kanaks their lives. A second guerrilla war took place in 1917, with Catholic missionaries like Maurice Leenhardt functioning as witnesses to the events of this war. Leenhardt would pen a number of ethnographic works on the Kanak of New Caledonia. Noel of Tiamou led the 1917 rebellion, which created a number of orphans, one of whom was taken into the care of Protestant Missionary Alphonse Rouel—Wenceslas Thi who would become the father of Jean-Marie Tjibaou.

Europeans brought new diseases such as smallpox and measles, of which many natives died. The Kanak population declined from around 60,000 in 1878 to 27,100 in 1921, and their numbers did not increase again until the 1930s.

In June 1940, after the fall of France, the "Conseil General" of New Caledonia voted unanimously to support the Free French government, and in September the pro-Vichy governor was forced to leave for Indochina. In March 1942, with the assistance of Australia, the territory became an important Allied base, and Nouméa the headquarters of the United States Navy and Army in the South Pacific. The fleet that turned back the Japanese navy in the Battle of the Coral Sea in May 1942 was based at Nouméa. American troops numbered as many as 50,000, the equivalent of the contemporary population.

In 1946, New Caledonia became an overseas territory. By 1953, French citizenship had been granted to all New Caledonians, regardless of ethnicity.

The European and Polynesian populations gradually increased in the years leading to the nickel boom of 1969–1972, and the Melanesians became a minority, though they were still the largest ethnic group.

Between 1976 and 1988, conflicts between French government actions and the Kanak independence movement saw periods of serious violence and disorder, culminating in 1988 with a bloody hostage-taking in Ouvéa. In 1983 a statute of "enlarged autonomy" for the territory, proposed a five-year transition period and a referendum in 1989. In March 1984, the Kanak resistance, Front Indépendantiste, seized farms and the Front de Libératíon Kanak Socialiste (FLNKS) formed a provisional government. In January 1985 the French Socialist government offered sovereignty to the Kanaks and legal protection for European settlers. The plan faltered as violence escalated. The government declared a state of emergency, however regional elections went ahead, and the FLNKS won control of three out of four provinces. The Conservative Party government elected in France in March 1986 began eroding the arrangements established under the Socialists, redistributing lands mostly without consideration of native land claims, resulting in over two thirds going to Europeans and less than a third to the Kanaks. By the end of 1987 roadblocks, gun battles, and the destruction of property culminated in a dramatic hostage crisis on the eve of the presidential elections in France. Pro-independence militants on Ouvea killed four gendarmes and took 27 hostage. The military response resulted in nineteen Kanak deaths and another three deaths in custody.

The Matignon Agreements, signed on 26 June 1988, ensured a decade of stability. The Nouméa Accord signed 5 May 1998, set the groundwork for a 20-year transition that will gradually transfer competences to the local government.

Following the timeline set by the Nouméa Accord that stated a vote must take place by the end of 2018, the groundwork was laid for a referendum on full independence from France at a meeting chaired by the French Prime Minister Édouard Philippe on 2 November 2017, to be held by November 2018. Voter list eligibility had been a subject of a long dispute, but the details have since been resolved. On 20 March 2018, it was announced that the independence referendum will be held on 4 November 2018.

New Caledonia is a territory "sui generis" to which France has gradually transferred certain powers. It is governed by a 54-member Territorial Congress, a legislative body composed of members of three provincial assemblies. The French State is represented in the territory by a High Commissioner. At a national level, New Caledonia is represented in the French Parliament by two deputies and two senators. At the 2012 French presidential election, the voter turnout in New Caledonia was 61.19%.

For 25 years, the party system in New Caledonia was dominated by the anti-independence The Rally–UMP. This dominance ended with the emergence of a new party, Avenir Ensemble, also opposed to independence, but considered more open to dialogue with the Kanak movement, which is part of the Kanak and Socialist National Liberation Front, a coalition of several pro-independence groups.

The Kanak society has several layers of customary authority, from the 4,000–5,000 family-based clans to the eight customary areas ("aires coutumières") that make up the territory. Clans are led by clan chiefs and constitute 341 tribes, each headed by a tribal chief. The tribes are further grouped into 57 customary chiefdoms ("chefferies"), each headed by a head chief, and forming the administrative subdivisions of the customary areas.

The Customary Senate is the assembly of the various traditional councils of the Kanaks, and has jurisdiction over the law proposals concerning the Kanak identity. The Customary Senate is composed of 16 members appointed by each traditional council, with two representatives per each customary area. In its advisory role, the Customary Senate must be consulted on law proposals "concerning the Kanak identity" as defined in the Nouméa Accord. It also has a deliberative role on law proposals that would affect identity, the civil customary statute, and the land system. A new president is appointed each year in August or September, and the presidency rotates between the eight customary areas.

Kanak people have recourse to customary authorities regarding civil matters such as marriage, adoption, inheritance, and some land issues. The French administration typically respects decisions made in the customary system. However, their jurisdiction is sharply limited in penal matters, as some matters relating to the customary justice system, including the use of corporal punishment, are seen as clashing with the human rights obligations of France.

The Armed Forces of New Caledonia () FANC, include about 2,000 soldiers, mainly deployed in Koumac, Nandaï, Tontouta, Plum, and Nouméa. The land forces consist of a regiment of the Troupes de marine, the Régiment d'infanterie de marine du Pacifique. The naval forces include two P400-class patrol vessels, a BATRAL, and a patrol boat of the Maritime Gendarmerie. The air force is made up of three Casa transport aircraft, four Puma helicopters and a Fennec helicopter, based in Tontouta. In addition, 760 gendarmes are deployed on the archipelago.

Since 1986, the United Nations Committee on Decolonization has included New Caledonia on the United Nations list of Non-Self-Governing Territories. An independence referendum was held the following year, but independence was rejected by a large majority.

Under the Nouméa Accord, signed in 1998 following a period of secessionist unrest in the 1980s and approved in a referendum, New Caledonia is to hold a second referendum on independence on 4 November 2018. The official date of the referendum has been set for November 4, 2018, the year the Nouméa Accord expires.

The official name of the territory, "Nouvelle-Calédonie", could be changed in the near future due to the accord, which stated that "a name, a flag, an anthem, a motto, and the design of banknotes will have to be sought by all parties together, to express the Kanak identity and the future shared by all parties." To date, however, there has been no consensus on a new name for the territory, although "Kanak Republic" is popular among 40% of the population. New Caledonia has increasingly adopted its own symbols, choosing an anthem, a motto, and a new design for its banknotes. In July 2010, New Caledonia adopted the Kanak flag, alongside the existing French tricolor, as dual official flags of the territory. The adoption made New Caledonia one of the few countries or territories in the world with two official national flags. The decision to use two flags has been a constant battleground between the two sides and led the coalition government to collapse in February 2011.

The institutional organization is the result of the organic law and ordinary law passed by the Parliament on 16 February 1999.

The archipelago is divided into three provinces:

New Caledonia is further divided into 33 municipalities: One commune, Poya, is divided between two provinces. The northern half of Poya, with the main settlement and most of the population, is part of the North Province, while the southern half of the commune, with only 127 inhabitants in 2009, is part of the South Province. The communes, with 2015 populations in brackets, and administrative centres, are as follows:



Notes: * provincial capital.
The population of the southern part of Poya commune is included in that for the northern part.

New Caledonia is part of Zealandia, a fragment of the ancient Gondwana super-continent. It is speculated that New Caledonia separated from Australia roughly 66 million years ago, subsequently drifting in a north-easterly direction, reaching its present position about 50 million years ago.

The mainland is divided in length by a central mountain range whose highest peaks are Mont Panié () in the north and Mont Humboldt () in the southeast. The east coast is covered by a lush vegetation. The west coast, with its large savannahs and plains suitable for farming, is a drier area. Many ore-rich massifs are found along this coast.

The Diahot River is the longest river of New Caledonia, flowing for some . It has a catchment area of and opens north-westward into the Baie d'Harcourt, flowing towards the northern point of the island along the western escarpment of the Mount Panié. Most of the island is covered by wet evergreen forests, while savannahs dominate the lower elevations. The New Caledonian lagoon, with a total area of is one of the largest lagoons in the world. It is surrounded by the New Caledonia Barrier Reef.

The climate is tropical, with a hot and humid season from November to March with temperatures between 27 °C and 30 °C, and a cooler, dry season from June to August with temperatures between 20 °C and 23 °C, linked by two short interstices. The tropical climate is strongly moderated by the oceanic influence and the trade winds that attenuate humidity, which can be close to 80%. The average annual temperature is 23 °C, with historical extremes of 2.3 °C and 39.1 °C.

The rainfall records show that precipitation differs greatly within the island. The of rainfall recorded in Galarino are three times the average of the west coast. There are also dry periods, because of the effects of El Niño. Between December and April, tropical depressions and cyclones can cause winds to exceed a speed of , with gusts of and very abundant rainfall. The last cyclone affecting New Caledonia was Cyclone Cook, in January 2017.

New Caledonia has many unique taxa, especially birds and plants. It has the richest diversity in the world per square kilometre. In its botany not only species but entire genera and even families are unique to the island, and survive nowhere else. The biodiversity is caused by Grande Terre's central mountain range, which has created a variety of niches, landforms and micro-climates where endemic species thrive.

Bruno Van Peteghem was in 2001 awarded the Goldman Environmental Prize for his efforts on behalf of the Caledonian ecological protection movement in the face of "serious challenges" from Jacques Lafleur's RPCR party. Progress has been made in a few areas in addressing the protection of New Caledonia's ecological diversity from fire, industrial and residential development, unrestricted agricultural activity and mining (such as the judicial revocation of INCO's mining license in June 2006 owing to claimed abuses).

New Caledonia's fauna and flora derive from ancestral species isolated in the region when it broke away from Gondwana many tens of millions of years ago. Not only endemic species have evolved here, but entire genera and even families are unique to the islands.

More of tropical gymnosperm species are endemic to New Caledonia than to any similar region on Earth. Of the 44 indigenous species of gymnosperms, 43 are endemic, including the only known parasitic gymnosperm ("Parasitaxus usta"). Also, of the 35 known species of "Araucaria", 13 are endemic to New Caledonia. New Caledonia also has the world's most divergent lineage of flowering plant, "Amborella trichopoda" which is at, or near, the base of the clade of all flowering plants.

The world's largest extant species of fern, "Cyathea intermedia", also is endemic to New Caledonia. It is very common on acid ground, and grows about one meter (yard) per year on the east coast, usually on fallow ground or in forest clearings. There also are other species of "Cyathea", notably "Cyathea novae-caledoniae".

New Caledonia also is one of five regions on the planet where species of southern beeches ("Nothofagus") are indigenous; five species are known to occur here.

New Caledonia has its own version of maquis ("maquis minier") occurring on metalliferous soils, mostly in the south. The soils of ultramafic rocks (mining terrains) have been a refuge for many native flora species because they are toxic and their mineral content is poorly suited to most foreign species of plants.

New Caledonia is home to the New Caledonian crow, a bird noted for its tool-making abilities, which rival those of primates. These crows are renowned for their extraordinary intelligence and ability to fashion tools to solve problems, and make the most complex tools of any animal yet studied apart from humans.

The endemic kagu, agile and able to run quickly, is a flightless bird, but it is able to use its wings to climb branches or glide. It is the surviving member of monotypic family Rhynochetidae, order Gruiformes.

There are 11 endemic fish species and 14 endemic species of decapod crustaceans in the rivers and lakes of New Caledonia. Some, such as "Neogalaxias", exist only in small areas. The nautilus, considered a living fossil and related to the ammonites which became extinct at the end of the Mesozoic era, occurs in Pacific waters around New Caledonia. There is a large diversity of marine fish in the surrounding waters, which are within the extents of the Coral Sea.

Several species of New Caledonia are remarkable for their size: "Ducula goliath" is the largest extant species of pigeon; "Rhacodactylus leachianus", the largest gecko in the world; "Phoboscincus bocourti" the largest skink in the world, thought to be extinct but rediscovered in 2003.

At the last census in 2014 New Caledonia had a population of 268,767. Of these, 17,436 live in the Loyalty Islands Province, 45,137 in the North Province, and 183,007 in the South Province. Population growth has slowed down since the 1990s, but remains strong with a yearly increase of 1.7% between 1996 and 2009.

Natural growth is responsible for 85% of the population growth, while the remaining 15% is attributable to net migration. The population growth is strong in South Province (2.3% per year between 1996 and 2009), moderate in North Province (0.7%), but negative in the Loyalty Islands, which are losing inhabitants (−1.3%).

Over 40% of the population is under 20, although the ratio of older people on the total population is increasing. Two residents of New Caledonia out of three live in Greater Nouméa. Three out of four were born in New Caledonia. The total fertility rate went from 3.2 children per woman in 1990 to 2.2 in 2007.

At the 2014 census, 39.1% of the population reported belonging to the Kanak community (down from 40.3% at the 2009 census), 27.2% to the European (Caldoche and Zoreille) community (down from 29.2% at the 2009 census), and 8.7% declared their community as "Caledonian" and other (up from 6.0% at the 2009 census). Most of the people who self-identified as "Caledonian" are thought to be ethnically European.

The other self-reported communities were Wallisians and Futunians (8.2% of the total population, down from 8.7% at the 2009 census), Indonesians (3.4% of the total population, up from 1.6% at the 2009 census), Tahitians (2.1% of the total population, up from 2.0% at the 2009 census), Ni-Vanuatu (1.0%, up from 0.9% at the 2009 census), Vietnamese (0.9%, down from 1.0% at the 2009 census), and other Asians (essentially ethnic Chinese) (0.4% of the total population, down from 0.8% at the 2009 census).

Finally 8.6% of the population reported belonging to multiple communities (mixed race) (up from 8.3% at the 2009 census), and 2.5% refused to report any community (up from 1.2% at the 2009 census). The question on community belonging, which had been left out of the 2004 census, was reintroduced in 2009 under a new formulation, different from the 1996 census, allowing multiple choices (mixed race) and the possibility to clarify the choice "other".
The Kanak people, part of the Melanesian group, are indigenous to New Caledonia. Their social organization is traditionally based around clans, which identify as either "land" or "sea" clans, depending on their original location and the occupation of their ancestors. According to the 2009 census, the Kanak constitute 94% of the population in the Loyalty Islands Province, 74% in the North Province and 27% in the South Province. The Kanak tend to be of lower socio-economic status than the Europeans and other settlers.

Europeans first settled in New Caledonia when France established a penal colony on the archipelago. Once the prisoners had completed their sentences, they were given land to settle. According to the 2009 census, of the 71,721 Europeans in New Caledonia 32,354 were native-born, 33,551 were born in other parts of France, and 5,816 were born abroad. The Europeans are divided into several groups: the Caldoches are usually defined as those born in New Caledonia who have ancestral ties that span back to the early French settlers. They often settled in the rural areas of the western coast of Grande Terre, where many continue to run large cattle properties.

Distinct from the Caldoches are those were born in New Caledonia from families that had settled more recently, and are called simply Caledonians. The Metropolitan French-born migrants who come to New Caledonia are called "Métros" or "Zoreilles", indicating their origins in metropolitan France. There is also a community of about 2,000 pieds noirs, descended from European settlers in France's former North African colonies; some of them are prominent in anti-independence politics, including Pierre Maresca, a leader of the RPCR.

A 2015 documentary by Al Jazeera English asserted that up to 10% of New Caledonia's population is descended from around 2,000 Arab-Berber people deported from French Algeria in the late 19th century to prisons on the island in reprisal for the Mokrani Revolt in 1871. After serving their sentences, they were released and given land to own and cultivate as part of colonisation efforts on the island. As the overwhelming majority of the Algerians imprisoned on New Caledonia were men, the community was continued through intermarriage with women of other ethnic groups, mainly French women from nearby women's prisons. Despite facing both assimilation into the Euro-French population and discrimination for their ethnic background, descendants of the deportees have succeeded in preserving a common identity as Algerians, including maintaining certain cultural practices (such as Arabic names) and in some cases Islamic religion. Some travel to Algeria as a rite of passage, though obtaining Algerian citizenship is often a difficult process. The largest population of Algerian-Caledonians lives in the commune of Bourail (particularly in the district, where there is an Islamic cultural centre and ), with smaller communities in Nouméa, Conné, Blambut, and Surianté.

The French language began to spread with the establishment of French settlements, and French is now spoken even in the most secluded villages. The level of fluency, however, varies significantly across the population as a whole, primarily due to the absence of universal access to public education before 1953, but also due to immigration and ethnic diversity. At the 2009 census, 97.3% of people aged 15 or older reported that they could speak, read and write French, whereas only 1.1% reported that they had no knowledge of French. Other significant language communities among immigrant populations are those of Wallisian and Javanese language speakers.

The 28 Kanak languages spoken in New Caledonia are part of the Oceanic group of the Austronesian family. Kanak languages are taught from kindergarten (four languages are taught up to the bachelor's degree) and an academy is responsible for their promotion. The four most widely spoken indigenous languages are Drehu (spoken in Lifou), Nengone (spoken on Maré) and Paicî (northern part of Grande Terre). Others include Iaai (spoken on Ouvéa). At the 2009 census, 35.8% of people aged 15 or older reported that they could speak (but not necessarily read or write) one of the indigenous Melanesian languages, whereas 58.7% reported that they had no knowledge of any of them.

The predominant religion is Christianity; half of the population is Roman Catholic, including most of the Europeans, Uveans, and Vietnamese and half of the Melanesian and Polynesian minorities. Roman Catholicism was introduced by French colonists. The island also has numerous Protestant churches, of which the Free Evangelical Church and the Evangelical Church in New Caledonia and the Loyalty Islands have the largest number of adherents; their memberships are almost entirely Melanesian. Protestantism gained ground in the late 20th century and continues to expand. There are also numerous other Christian groups and small numbers of Muslims. See Islam in New Caledonia and Bahá'í Faith in New Caledonia.

Education in New Caledonia is based on the French curriculum and delivered by both French teachers and French–trained teachers. Under the terms of the 1998 Nouméa Accord, primary education is the responsibility of the three provinces. As of 2010, secondary education was in the process of being transferred to the provinces. The majority of schools are located in Nouméa but some are found in the islands and the north of New Caledonia. When students reach high school age, most are sent to Nouméa to continue their secondary education. Education is compulsory from the age of six years.

New Caledonia's main tertiary education institution is the University of New Caledonia ("Université de la Nouvelle-Calédonie"), which was founded in 1993 and comes under the supervision of the Ministry of Higher Education, Research and Innovation. It is based in Nouméa and offers a range of vocational, Bachelor, MA, and PhD programmes and courses. The University of New Caledonia consists of three academic departments, one institute of technology, one PhD school, and one teacher's college. As of 2013, the University has approximately 3,000 students, 107 academics, and 95 administrative and library staff. Many New Caledonian students also pursue scholarships to study in metropolitan France. As part of the Nouméa Accord process, there is a "Cadre Avenir" which provides scholarships for Kanak professionals to study in France.

New Caledonia has one of the largest economies in the South Pacific, with a GDP of US$9.89 billion in 2011. The nominal GDP per capita was US$38,921 (at market exchange rates) in 2011. It is higher than New Zealand's, though there is significant inequality in income distribution, and long-standing structural imbalances between the economically dominant South Province and the less developed North Province and Loyalty Islands. The currency in use in New Caledonia is the CFP franc, pegged to the euro at a rate of 1,000 CFP to 8.38 euros. It is issued by the Institut d'Emission d'Outre-Mer.

Real GDP grew by 3.8% in 2010 and 3.2% in 2011, boosted by rising worldwide nickel prices and an increase in domestic demand due to rising employment, as well as strong business investments. In 2011, exports of goods and services from New Caledonia amounted to 2.11 billion US dollars, 75.6% of which were mineral products and alloys (mainly nickel ore and ferronickel). Imports of goods and services amounted to 5.22 billion US dollars. 22.1% of the imports of goods came from Metropolitan France and its overseas departments, 16.1% from other countries in the European Union, 14.6% from Singapore (essentially fuel), 9.6% from Australia, 4.5% from the United States, 4.2% from New Zealand, 2.0% from Japan, and 27.0% from other countries. The trade deficit in goods and services stood at 3.11 billion US dollars in 2011.

Financial support from France is substantial, representing more than 15% of the GDP, and contributes to the health of the economy. Tourism is underdeveloped, with 100,000 visitors a year, compared to 400,000 in the Cook Islands and 200,000 in Vanuatu. Much of the land is unsuitable for agriculture, and food accounts for about 20% of imports. According to FAOSTAT, New Caledonia is one of world's largest producers of: yams (33rd); taro (44th); plantains (50th); coconuts (52nd). The exclusive economic zone of New Caledonia covers . The construction sector accounts for roughly 12% of GDP, employing 9.9% of the salaried population in 2010. Manufacturing is largely confined to small-scale activities such as the transformation of foodstuffs, textiles and plastics.

New Caledonian soils contain about 25% of the world's nickel resources. The late-2000s recession has gravely affected the nickel industry, as the sector faced a significant drop in nickel prices (−31.0% year-on-year in 2009) for the second consecutive year. The fall in prices has led a number of producers to reduce or stop altogether their activity, resulting in a reduction of the global supply of nickel by 6% compared to 2008.

This context, combined with bad weather, has forced the operators in the sector to revise downwards their production target. Thus, the activity of mineral extraction has declined by 8% in volume year on year. The share of the nickel sector as a percentage of GDP fell from 8% in 2008 to 5% in 2009. A trend reversal and a recovery in demand have been recorded early in the second half of 2009, allowing a 2.0% increase in the local metal production.

Historically, nickel was transported by wire ropeway to ships waiting offshore.

Wood carving, especially of the houp ("Montrouziera cauliflora"), is a contemporary reflection of the beliefs of the traditional tribal society, and includes totems, masks, chambranles, or flèche faîtière, a kind of arrow which adorns the roofs of Kanak houses. Basketry is a craft widely practiced by tribal women, creating objects of daily use.

The Jean-Marie Tjibaou Cultural Centre, designed by Italian architect Renzo Piano and opened in 1998, is the icon of the Kanak culture.

The Kaneka is a form of local music, inspired by reggae and originating in the 1980s.

The Mwâ Ka is a 12m totem pole commemorating the French annexation of New Caledonia, and was inaugurated in 2005.

"Les Nouvelles Calédoniennes" is the only daily newspaper in the archipelago. A monthly publication, "Le Chien bleu", parodies the news from New Caledonia.

There are five radio stations: the public service broadcaster RFO radio Nouvelle-Calédonie, Océane FM (the collectivity's newest station), the youth-oriented station NRJ, Radio Djiido (established by Jean-Marie Tjibaou), and Radio Rythmes Bleus. The last two stations are primarily targeted to the various Kanak groups who are indigenous to New Caledonia ("Djiido" is a term from the Fwâi language, spoken in Hienghène in the North Province, denoting a metal spike used to secure straw thatching to the roof of a traditional Kanak house).

As for television, the public service broadcaster France Télévision operates a local channel, Réseau Outre-Mer 1re, along with France 2, France 3, France 4, France 5, France Ô, France 24 and Arte. Canal Plus Calédonie carries 17 digital channels in French, including Canal+ and TF1. Analogue television broadcasts ended in September 2011, completing the digital television transition in New Caledonia. Bids for two new local television stations, NCTV and NC9, were considered by the French broadcasting authorities. NCTV was launched in December 2013.

The media are considered to be able to operate freely, but Reporters Without Borders raised concerns in 2006 about "threats and intimidation" of RFO staff by members of a pro-independence group.

The largest sporting event to be held in New Caledonia is a round of the FIA Asia Pacific Rally Championship (APRC).

The New Caledonia football team began playing in 1950, and was admitted into FIFA, the international association of football leagues, in 2004. Prior to joining FIFA, New Caledonia held observer status with the Oceania Football Confederation, and became an official member of the OFC with its FIFA membership. They have won the South Pacific Games five times, most recently in 2007, and have placed third on two occasions in the OFC Nations Cup. Christian Karembeu is a prominent New Caledonian former footballer. The under-17 team qualified for the FIFA under 17 World Cup in 2017.

Horse racing is also very popular in New Caledonia, as are women's cricket matches.

The rugby league team participated in the Pacific Cup in 2004.

New Caledonia also has a national synchronised swimming team, which tours abroad.

The "Tour Cycliste de Nouvelle-Calédonie" is a multi-day cycling stage race that is held usually in October. The race is organised by the Comite Cycliste New Caledonia. The race attracts riders from Australia, New Zealand, France, Reunion, Europe and Tahiti. Australian Brendan Washington has finished last three times in the race between 2005–2009, and is known in New Caledonia as "The Lanterne Rouge".

The New Caledonia Handball team won the Oceania Handball Nations Cup in 2008 held in Wellington, New Zealand. They beat Australia in the final.

Due to low levels of domestic horticulture, fresh tropical fruits feature less highly in New Caledonian cuisine than in other Pacific nations, instead relying on rice, fish and root vegetables such as taro. One way this is frequently prepared is in a buried-oven-style feast, known as "Bougna". Wrapped in banana leaves, the fish, taro, banana and other seafood are buried with hot rocks to cook, then dug up and eaten.

La Tontouta International Airport is north of Nouméa, and connects New Caledonia with the airports of Paris, Tokyo, Sydney, Auckland, Brisbane, Melbourne, Osaka, Papeete, Fiji, Wallis, Port Vila, Seoul, and St. Denis. Most internal air services are operated by the International carrier Aircalin. Cruise ships dock at the Gare Maritime in Nouméa. The passenger-and-cargo boat "Havannah" sails to Port Vila, Malicolo and Santo in Vanuatu once a month.

New Caledonia's road network consists of:


The television series "McHale's Navy" was set in the islands in the area, with fleet headquarters being in New Caledonia, and so were the episodes "New Blood" and "Cruel Sea" of the 1999 BBC television show "Walking with Dinosaurs".

"Rebellion" ("") was released in 2011, and is based on the massacre by French military during the 1988 Ouvéa cave hostage taking in New Caledonia, as seen from the perspective of then GIGN leader Capt. Philippe Legorjus.

In 2009, South Korean television drama "Boys Over Flowers" filmed Episode 5 and Episode 6 at New Caledonia as a vacation spot for the richest of South Korea. With 10 million viewers, the sights filmed in the show have led to increased interest in New Caledonia among Koreans, who see it as a possible honeymoon location.





</doc>
<doc id="21344" url="https://en.wikipedia.org/wiki?curid=21344" title="Geography of New Caledonia">
Geography of New Caledonia

The geography of New Caledonia ("Nouvelle-Calédonie"), an overseas collectivity of France located in the subregion of Melanesia, makes the continental island group unique in the southwest Pacific. Among other things, the island chain has played a role in preserving unique biological lineages from the Mesozoic. It served as a waystation in the expansion of the predecessors of the Polynesians, the Lapita culture. Under the Free French it was a vital naval base for Allied Forces during the War in the Pacific.

The archipelago is located east of Australia, north of New Zealand, south of the Equator, and just west of Fiji and Vanuatu. New Caledonia comprises a main island, Grande Terre, the Loyalty Islands, and several smaller islands. Approximately half the size of Taiwan, the group has a land area of . The islands have a coastline of . New Caledonia claims an exclusive fishing zone to a distance of and a territorial sea of from shore.

New Caledonia is one of the northernmost parts of an almost entirely (93%) submerged continent called Zealandia which rifted away from Antarctica between 130 and 85 million years ago (mya), and from Australia 85–60 mya. (Most of the elongated triangular continental mass of Zealandia is a subsurface plateau. New Zealand is a mountainous above-water promontory in its center, and New Caledonia is a promontory ridge on the continent's northern edge.) New Caledonia itself drifted away from Australia 66 mya, and subsequently drifted in a north-easterly direction, reaching its present position about 50 mya. Given its long stability and isolation, New Caledonia serves as a unique island refugium—a sort of biological 'ark'—hosting a unique ecosystem and preserving Gondwanan plant and animal lineages no longer found elsewhere.

New Caledonia is made up of a main island, the Grande Terre, and several smaller islands, the Belep archipelago to the north of the Grande Terre, the Loyalty Islands to the east of the Grande Terre, the (Isle of Pines) to the south of the Grande Terre, the Chesterfield Islands and Bellona Reefs further to the west. Each of these four island groups has a different geological origin:


The Grande Terre is by far the largest of the islands, and the only mountainous island. It has an area of , and is elongated northwest–southeast, in length and wide. A mountain range runs the length of the island, with five peaks over . The highest point is Mont Panié at elevation. The total area of New Caledonia is , of those being land.

A territorial dispute exists with regard to the uninhabited Matthew and Hunter Islands, which are claimed by both France (as part of New Caledonia) and Vanuatu.

The New Caledonian archipelago is a microcontinental island chain which originated as a fragment of Zealandia, a nearly submerged continent or microcontinent which was part of the southern supercontinent of Gondwana during the time of the dinosaurs. The Grande Terre group of New Caledonia, with Mont Panié at as its highest point, is the most elevated part of the Norfolk Ridge, a long and mostly underwater arm of the continent. While they were still one landmass, Zealandia and Australia combined broke away from Antarctica between 85 and 130 million years ago. Australia and Zealandia split apart 60–85 million years ago. Although biologists consider it contrary to the evidence of surviving Gondwanan lineages, geologists consider the logical possibility that Zealandia may have been completely submerged about 23 million years ago. While a continent like Australia consists of a large body of land surrounded by a fringe of continental shelf, Zealandia consists almost entirely of continental shelf, with the vast majority, some 93%, submerged beneath the Pacific Ocean. This viewpoint is not universal. Bernard Pelletier argues that Grande Terre was completely submerged for millions of years, and hence the origin of the flora may not be local in nature, but due to long distance-dispersal.

Zealandia is in area, larger than Greenland or India, and almost half the size of Australia. It is unusually slender, stretching from New Caledonia in the north to beyond New Zealand's subantarctic islands in the south (from latitude 19° south to 56° south, analogous to ranging from Haiti to Hudson Bay or from Sudan to Sweden in the Northern Hemisphere). New Zealand is the greatest part of Zealandia above sea level, followed by New Caledonia.

Given its continental origin as a fragment of Zealandia, unlike many of the islands of the Pacific such as the Hawaiian chain, New Caledonia is not of geographically recent volcanic provenance. Its separation from Australia at the end of the Cretaceous (65 mya) and from New Zealand in the mid-Miocene has led to a long period of evolution in near complete isolation. New Caledonia’s natural heritage significantly comprises species whose ancestors were ancient and primitive flora and fauna present on New Caledonia when it broke away from Gondwana millions of years ago, not only species but entire genera and even families are unique to the island, and survive nowhere else.

Since the age of the dinosaurs, as the island moved north due to the effects of continental drift, some geologists assert that it may have been fully submerged at various intervals. Botanists, however, argue that there must have been some areas that remained above sea level, serving as refugia for the descendants of the original flora that inhabited the island when it broke away from Gondwana. The isolation of New Caledonia was not absolute, however. New species came to New Caledonia while species of Gondwanan origin were able to penetrate further eastward into the Pacific Island region.

The climate of New Caledonia is tropical, modified by southeasterly trade winds. It is hot and humid. Natural hazards are posed in New Caledonia by cyclones, which occur most frequently between November and March. While rainfall in the neighboring Vanuatu islands averages two meters annually, from the north of New Caledonia to the south the rain decreases to a little over . The mean annual temperature drops over the same interval from , and seasonality becomes more pronounced. The capital, Nouméa, located on a peninsula on the southwestern coast of the island normally has a dry season which increases in intensity from August until mid-December, ending suddenly with the coming of rain in January. The northeastern coast of the island receives the most rain, with having been recorded near sea level in Pouébo.

The terrain of Grande Terre consists of coastal plains, with mountains in the interior. The lowest point is the Pacific Ocean, with an elevation of 0 m, and the highest is Mont Panie, with an elevation of .

The Diahot River is the longest river of New Caledonia, flowing for some 60 miles (100 kilometres). It has a catchment area of 620 square kilometres and opens north-westward into the Baie d'Harcourt, flowing towards the northern point of the island along the western escarpment of the Mount Panie.

In 1993, 12% of New Caledonian land was used for permanent pasture, with 39% occupied by forests and woodland. In 1991, of the land was irrigated. A current environmental issue is erosion caused by mining exploitation and forest fires.

Given its geographical isolation since the end of the Cretaceous, New Caledonia is a refugium, in effect a biological "Noah's Ark", an island home to both unique living plants and animals and also to its own special fossil endowment. Birds such as the crested and almost flightless kagu (French, "cagou") "Rhynochetos jubatus", whose closest relative may be the distantly related sunbittern of South America, and plants such as "Amborella trichopoda", the only known member of the most basal living branch of flowering plants, make this island a treasure trove and a critical concern for biologists and conservationists. The island was home to horned fossil turtles ("Meiolania mackayi") and terrestrial fossil crocodiles ("Mekosuchus inexpectatus") which became extinct shortly after human arrival. There are no native amphibians, with geckos holding many of their niches. The crested gecko ("Rhacodactylus ciliatus"), thought to have gone extinct, was rediscovered in 1994. At 14 inches, Leach's giant gecko ("Rhacodactylus leachianus"), the world's largest and a predator of smaller lizards is another native. The only native mammals are four species of bat including the endemic New Caledonia flying fox.

New Caledonia is home to 13 of the 19 extant species of evergreens in the genus "Araucaria". The island has been called "a kind of 'Jurassic Park'" because of the archaic characteristics of its highly endemic vegetation. In addition to the basal angiosperm plant genus "Amborella", for example, the island is home to more gymnosperm species than any other tropic landmass, with 43 of its 44 conifer species being unique to the island, which is also home to the world's only known parasitic gymnosperm, the rootless conifer "Parasitaxus usta".

Given their prehistoric appearance, the dry forests of western New Caledonia were chosen as the location for filming the first episode of the BBC miniseries Walking with Dinosaurs, which was set in the Arizona of the late Triassic.

After a formation discovered in Oman in the 1970s, New Caledonia has the planet's largest known outcrop of ultrabasic rock, derived not from the crust, but from an upthrust fold of the more deeply underlying mantle of the earth. These mineral-rich rocks are a source of nickel, chromium, iron, cobalt, manganese, silver, gold, lead and copper. The toxicity of the mineral-rich soil has helped preserve the endemic vegetation, which has long been adapted to it, from competition from would-be colonizers which find it unsuitable.

Anthropologically, New Caledonia is considered the southernmost archipelago of Melanesia, grouping it with the more close-by islands to its north, rather than its geologically associated neighbour, New Zealand, to the south. The New Caledonian languages, whose speakers are called Kanaks, and the dialects of the Loyalty Islands find their closest relatives in the languages of Vanuatu to the east. Together, these comprise the Southern Oceanic language family, a member of the Oceanic language branch of the Austronesian language family. The Fijian languages, the Maori language of New Zealand, and other Polynesian languages such as Tahitian, Samoan and Hawaiian are cousins of the New Caledonian languages within the Oceanic language family.

Linguistic analysis using the comparative method provides a detailed family tree of the Austronesian languages to which the native languages of New Caledonia belong. The Lapita culture, hypothesized to have spoken proto-Oceanic, and defined by its typical style of pottery, originated to the northwest in the Bismarck Archipelago around 1500 BC. The earliest known human settlement of New Caledonia, dated to 1240 ±220 BC at the Tiwi rockshelter, is attributed to the Lapitans, who then moved on to Fiji by approximately 900 BC, whence the Polynesian expansion would begin.

Western colonization of the area began in the 18th century. The British explorer James Cook sighted Grande Terre in 1774 and named it New Caledonia, Caledonia being Latin for Scotland. In 1853, under Napoleon III, the area was made a French colony. The French brought colonial subjects such as Arabs from the Maghreb to settle in the territory. Given its strategic location and that it was unoccupied by Japanese troops it played a vital role under the Free French Forces as an Allied Forces naval base during the Second World War.

Today, while French is the official language, 28 indigenous tongues are still spoken. At the 2004 census, 97.0% reported they could speak French, whereas only 0.97% reported that they had no knowledge of French. In the same census, 37.1% reported that they could speak (but not necessarily read or write) one of the 28 indigenous Austronesian languages.

At the 1996 census, the indigenous Melanesian Kanak community represented 44.6% of the whole population. They are no longer a majority, their proportion of the population having declined due to immigration and other factors. The rest of the population is made up of ethnic groups that arrived in New Caledonia in the last 150 years: Europeans (34.5%) (predominantly French, with German, British and Italian minorities), Polynesians (Wallisians, Tahitians) (11.8%), Indonesians (2.6%), Vietnamese (1.4%), Ni-Vanuatu (1.2%), and various other groups (3.9%), Tamils, other South Asians, Berbers, Japanese, Chinese, Fijians, Arabs, West Indian (mostly from other French territories) and a small number of ethnic Africans.


"Other microcontinental islands:"



</doc>
<doc id="21345" url="https://en.wikipedia.org/wiki?curid=21345" title="Demographics of New Caledonia">
Demographics of New Caledonia

Demographics of New Caledonia.

Ethnic Melanesians known as Kanak constituted 40% of the population in 2009, followed by Europeans with 29%. The Europeans are the largest ethnic group in the province Sud (South), while Kanak are the majority in the other provinces. There is also a large minority from other Polynesian islands, mainly from Wallis and Futana (9%), but also from Tahiti (2%) and Vanuatu (1%). The remainder of the population (24%) is mainly from Asian countries.

The following demographic statistics are from the CIA World Factbook.


















</doc>
<doc id="21346" url="https://en.wikipedia.org/wiki?curid=21346" title="Politics of New Caledonia">
Politics of New Caledonia

New Caledonia is a French sui generis collectivity with a system of government based on parliamentarism and representative democracy. The President of the Government is the head of government, and there is a multi-party system, with Executive power being exercised by the government. Legislative power is vested in both the government and the Congress of New Caledonia. The judiciary is independent of the executive and the legislature.

Article 77 of the Constitution of France and the Organic Law 99-209 confers a unique status of New Caledonia between that of an independent country and a regular "collectivité d'outre-mer" or overseas "collectivité" of France. A territorial congress and government have been established, and the 1998 Nouméa Accord organized a devolution of powers. Key areas such as taxation, labor law, health and hygiene and foreign trade are already in the hands of the Congress. Further powers will supposedly be given to the Congress in the near future.

Under article 4 of the Organic Law 99-209 a New Caledonian "citizenship" has also been introduced: only New Caledonian "citizens" (defined by article 188) have the right to vote in the local elections. This measure has been criticized, because it creates a second-class status for French citizens living in New Caledonia who do not possess New Caledonian "citizenship" (because they settled in the territory recently). New Caledonia is also allowed to engage in international cooperation with independent countries of the Pacific Ocean. Finally, the territorial Congress is allowed to pass statutes that are contrary to French law in a certain number of areas.

On the other hand, New Caledonia remains an integral part of the French Republic. Inhabitants of New Caledonia are French citizens and carry French passports. They take part in the legislative and presidential French elections. New Caledonia sends two representatives to the French National Assembly and two senators to the French Senate. The representative of the French central state in New Caledonia is the High Commissioner of the Republic ("Haut-Commissaire de la République", locally known as ""haussaire""), who is the head of civil services, and who sits in the government of the territory.

The Nouméa Accord stipulates that the Congress will have the right to call for a referendum on independence after 2014, at a time of its choosing. Following the timeline set by the Nouméa Accord, the groundwork was laid for a Referendum on full independence from France at a meeting chaired by the French Prime Minister Édouard Philippe on 2 November 2017, with the referendum to be held by November 2018. Voter list eligibility had been a subject of a long dispute, but the details have were resolved at this meeting. As of 2017, there are no predictions on whether the vote for independence will prevail.

The current president of the government elected by the Congress is Philippe Germain, from the loyalist (i.e. anti-independence) Caledonia Together political party.

The high commissioner is appointed by the French president on the advice of the French Ministry of Interior. The president of the government is elected by the members of the Territorial Congress.

The Congress ("Congrès") has 54 members, being the members of the three regional councils, all elected for a five-year term by proportional representation. Furthermore, there is a 16-member Kanak Customary Senate (two members from each of the eight customary aires).



Court of Appeal or Cour d'Appel; County Courts; Joint Commerce Tribunal Court; Children's Court

New Caledonia is divided into three provinces: "Province des Îles", "Province Nord", and "Province Sud" - which are further subdivided into 33 communes.




</doc>
<doc id="21347" url="https://en.wikipedia.org/wiki?curid=21347" title="Economy of New Caledonia">
Economy of New Caledonia

New Caledonia is a major source for nickel and contains roughly 10% of the worlds known nickel supply. The islands contain about 7,100,000 tonnes of nickel. With the annual production of about 107,000 tonnes in 2009, New Caledonia was the world's fifth largest producer after Russia (266,000), Indonesia (189,000), Canada (181,000) and Australia (167,000). In recent years, the economy has suffered because of depressed international demand for nickel, due to the ongoing global financial crisis. Only a negligible amount of the land is suitable for cultivation, and food accounts for about 20% of imports. In addition to nickel, the substantial financial support from France and tourism are keys to the health of the economy. In the 2000s, large additions were made to nickel mining capacity. The Goro Nickel Plant is expected to be one of the largest nickel producing plants on Earth. When full-scale production begins in 2013 this plant will produce an estimated 20% of the global nickel supply. However, the need to respond to environmental concerns over the country's globally recognized ecological heritage, may increasingly need to be factored into capitalization of mining operations.

The GDP of New Caledonia in 2007 was 8.8 billion US dollars at market exchange rates, the fourth-largest economy in Oceania after Australia, New Zealand, and Hawaii. The GDP per capita was 36,376 US dollars in 2007 (at market exchange rates, not at PPP), lower than in Australia and Hawaii, but higher than in New Zealand.

In 2007, exports from New Caledonia amounted to 2.11 billion US dollars, 96.3% of which were mineral products and alloys (essentially nickel ore and ferronickel). Imports amounted to 2.88 billion US dollars. 26.6% of imports came from Metropolitan France, 16.1% from other European countries, 13.6% from Singapore (essentially fuel), 10.7% from Australia, 4.0% from New Zealand, 3.2% from the United States, 3.0% from China, 3.0% from Japan, and 22.7% from other countries.

As of 2007, about 200 Japanese couples travel to New Caledonia each year for their wedding and honeymoon. "Oceania Flash" reported in 2007 that one company planned to build a new wedding chapel to accommodate Japanese weddings to supplement the Le Meridien Resort in Nouméa.

New Caledonia is a popular destination for groups of Australian high school students who are studying French.



</doc>
<doc id="21348" url="https://en.wikipedia.org/wiki?curid=21348" title="Telecommunications in New Caledonia">
Telecommunications in New Caledonia

Telephones - main lines in use:

53,300 (2004) (up from 44,000 in 1995)

Telephones - mobile cellular:
116,400 (2004) (up from 825 in 1995)

Telephone system:
<br>"domestic:"
NA
<br>"international:"
satellite earth station - 1 Intelsat (Pacific Ocean)

Radio broadcast stations:
AM 1, FM 5, Digital Radio Oceane, shortwave 0 (2009)

Radios:
107,000 (1997)

Television broadcast stations:
6 (plus 25 low-power repeaters) (1997)

Televisions:
52,000 (1997)

Internet Service Providers (ISPs):
1 (1999)

Country calling code: 687

Country code (Top level domain): NC


</doc>
<doc id="21349" url="https://en.wikipedia.org/wiki?curid=21349" title="Transport in New Caledonia">
Transport in New Caledonia

This article is about the transport in New Caledonia.

0 km (The Nouméa-Païta railway was abandoned and demolished).

<br>"total:"
5,562 km
<br>"paved:"
975 km
<br>"unpaved:"
4,587 km (1993)

Mueo, Nouméa, Thio

<br>"total:"
2 ships (1,000 GRT or over) totaling 3 566 GRT/
<br>"ships by type:"
cargo 1; passenger/cargo 1 (2005)

25 (2005) See List of airports in New Caledonia.

<br>"total:"
11
<br>"over 3,047 m:"
1 (La Tontouta International Airport)
<br>"914 to 1,523 m:"
8
<br>"under 914 m:"
2 (2005)

<br>"total:"
14
<br>"914 to 1,523 m:"
8
<br>"under 914 m:"
6 (2005)

Heliports:
6 (2005)



</doc>
<doc id="21353" url="https://en.wikipedia.org/wiki?curid=21353" title="Geography of New Zealand">
Geography of New Zealand

New Zealand ("") is an island country located in the south-western Pacific Ocean, near the centre of the water hemisphere. It is a long and narrow country. The two largest islands are the North Island (or "Te Ika-a-Māui") and the South Island (or "Te Waipounamu"), separated by the Cook Strait; a third, less substantial island, Stewart Island (or "Rakiura"), is located off the tip of the South Island across Foveaux Strait. Other smaller islands include Waiheke Island, Chatham Island, Great Barrier Island and more, although many are uninhabited.

New Zealand's landscape ranges from the fiord-like sounds of the southwest to the sandy beaches of the far north. South Island is dominated by the Southern Alps while a volcanic plateau covers much of central North Island. Temperatures rarely fall below 0 °C or rise above 30 °C and conditions vary from wet and cold on South Island's West Coast to dry and continental a short distance away across the mountains and subtropical in the northern reaches of North Island.

New Zealand's varied landscape has appeared in television shows, such as "". An increasing number of feature films have also been filmed there, including the "Lord of the Rings" trilogy.

The country is situated about southeast of Australia across the Tasman Sea, its closest neighbours to the north being Tonga and Fiji. The relative proximity of New Zealand north of Antarctica has made South Island a gateway for scientific expeditions to the continent. It is the southernmost nation in Oceania

New Zealand is located in the South Pacific Ocean at . The country is long and narrow (over along its north-north-east axis with a maximum width of ).

New Zealand is an isolated country with no land borders. It consists of a large number of islands, estimated around 600. The islands give it of coastline and extensive marine resources. New Zealand claims the fifth-largest exclusive economic zone in the world, covering over (1.5 million sq mi), more than 15 times its land area.

The South Island is the largest land mass of New Zealand, and is the 12th-largest island in the world. The island is divided along its length by the Southern Alps. The east side of the island has the Canterbury Plains while the West Coast is famous for its rough coastlines, very high proportion of native bush, and glaciers.

The North Island is the second-largest island, and the 14th-largest in the world. It is separated from the South Island by the Cook Strait, with the shortest distance being . The North Island is less mountainous than the South Island, although a series of narrow mountain ranges form a roughly north-east belt that rises up to . Much of the surviving forest is located in this belt, and in other mountain areas and rolling hills.

Besides the North and South Islands, the five largest inhabited islands are Stewart Island, Chatham Island, Great Barrier Island (in the Hauraki Gulf), d'Urville Island (in the Marlborough Sounds) and Waiheke Island (about from central Auckland).

The country has a variety of natural resources, including:
New Zealand has a total land area of (including its outlying islands), making it slightly smaller than Italy and Japan and a little larger than the United Kingdom. A relatively small proportion of New Zealand's land is arable (1.76%); permanent crops cover 0.27% of the land. of the land is irrigated. New Zealand has total renewable water resources.

The South Island is much more mountainous than the North, but shows fewer manifestations of recent volcanic activity. There are 18 peaks of more than in the Southern Alps, which stretch for down the South Island. The closest mountains surpassing it in elevation are found not in Australia, but in New Guinea and Antarctica. As well as the towering peaks, the Southern Alps include huge glaciers such as Franz Josef and Fox. The country's highest mountain is Aoraki / Mount Cook; its height since 2014 is listed as (down from before December 1991, due to a rockslide and subsequent erosion). The second highest peak is Mount Tasman, with a height of .

The North Island Volcanic Plateau covers much of central North Island with volcanoes, lava plateaus, and crater lakes. The three highest volcanoes are Mount Ruapehu (), Mount Taranaki () and Mount Ngauruhoe (). Ruapehu's major eruptions have historically been about 50 years apart, in 1895, 1945 and 1995–1996. The 1886 eruption of Mount Tarawera, located near Rotorua, was New Zealand's largest and deadliest eruption in the last 200 years, killing over 100 people. Another long chain of mountains runs through the North Island, from Wellington to East Cape. The ranges include Tararua and Kaimanawa.

The lower mountain slopes are covered in native forest. Above this are shrubs, and then tussock grasses. Alpine tundra consists of cushion plants and herbfields; many of these plants have white and yellow flowers.

The proportion of New Zealand's area (excluding estuaries) covered by rivers, lakes and ponds, based on figures from the New Zealand Land Cover Database, is (357526 + 81936) / (26821559 – 92499–26033 – 19216) = 1.6%. If estuarine open water, mangroves, and herbaceous saline vegetation are included, the figure is 2.2%.
The mountainous areas of the North Island are cut by many rivers, many of which are swift and unnavigable. The east of the South Island is marked by wide, braided rivers such as the Wairau, Waimakariri and Rangitātā; formed from glaciers, they fan out into many strands on gravel plains. The Waikato, flowing through the North Island, is the New Zealand's longest river, with a length of . New Zealand's rivers feature hundreds of waterfalls; the most visited set of waterfalls are the Huka Falls that drain Lake Taupo.

Lake Taupo, located near the centre of the North Island, is the largest lake by surface area in the country. It lies in a caldera created by the Oruanui eruption, the largest eruption in the world in the past 70,000 years. There are 3,820 lakes with a surface area larger than one hectare. Many lakes have been used as reservoirs for hydroelectric projects.

Throughout the Karst rocks many caves were formed, especially in the Waitomo District of the North Island.

The phrase "From Cape Reinga to The Bluff" is frequently used within New Zealand to refer to the extent of the whole country. Cape Reinga is northwesternmost tip of the Aupouri Peninsula, at the northern end of the North Island. Bluff is Invercargill's port, located near the southern tip of the South Island, below the 46th parallel south. However, the extreme points of New Zealand are actually located in several outlying islands.

The points that are farther north, south, east or west than any other location in New Zealand are as follows:

New Zealand is part of Zealandia, a microcontinent nearly half the size of Australia that gradually submerged after breaking away from the Gondwanan supercontinent. Zealandia extends a significant distance east into the Pacific Ocean and south towards Antarctica. It also extends towards Australia in the north-west. This submerged continent is dotted with topographic highs that sometimes form islands. Some of these, such as the main islands (North and South), Stewart Island, and the Chatham Islands, are settled. Other smaller islands are eco-sanctuaries with carefully controlled access.

The New Zealand landmass has been uplifted due to transpressional tectonics between the Indo-Australian Plate and Pacific plates (these two plates are grinding together with one riding up and over the other). The associated geothermal energy is used in numerous hydrothermal power plants. Some volcanic places are also famous tourist destinations, such as the Rotorua geysers.
To the east of the North Island the Pacific Plate is forced under the Indo-Australian Plate. The North Island of New Zealand has widespread back-arc volcanism as a result of this subduction. There are many large volcanoes with relatively frequent eruptions. There are also several very large calderas, with the most obvious forming Lake Taupo. Taupo has a history of incredibly powerful eruptions, with the Oruanui eruption approx. 26,500 years ago ejecting of material and causing the downward collapse of several hundred square kilometers to form the lake. The last eruption occurred and ejected at least 100 cubic kilometers of material, and has been correlated with red skies seen at the time in Rome and China.

The subduction direction is reversed through the South Island, with the Indo-Australian Plate forced under the Pacific Plate. The transition between these two different styles of continental collision occurs through the top of the South Island. This area has significant uplift and many active faults; large earthquakes are frequent occurrences here. The most powerful in recent history, the M8.3 Wairarapa earthquake, occurred in 1855. This earthquake generated more than of vertical uplift in places, and caused a localised tsunami. Fortunately casualties were low due to the sparse settlement of the region. In 2013, the area was rattled by the M6.5 Seddon earthquake, but this caused little damage and no injuries. New Zealand's capital city, Wellington, is situated in the centre of this region.

The subduction of the Indo-Australian Plate drives rapid uplift in the centre of the South Island (approx. per year). This uplift forms the Southern Alps. These roughly divide the island, with a narrow wet strip to the west and wide and dry plains to the east. The resulting orographic rainfall enables the hydroelectric generation of most of the country's electricity. A significant amount of the movement between the two plates is accommodated by lateral sliding of the Indo-Australian Plate north relative to the Pacific Plate. The plate boundary forms the nearly long Alpine Fault. This fault has an estimated rupture reoccurrence interval of ~330 years, and last ruptured in 1717 along of its length. It passes directly under many settlements on the West Coast of the South Island and shaking from a rupture would likely affect many cities and towns throughout the country.

The rapid uplift and high erosion rates within the Southern Alps combine to expose high grade greenschist to amphibolite facies rocks, including the gemstone pounamu (jadeite). Geologists visiting the West Coast can easily access high-grade metamorphic rocks and mylonites associated with the Alpine Fault, and in certain places can stand astride the fault trace of an active plate boundary.

To the south of New Zealand the Indo-Australian Plate is subducting under the Pacific Plate, and this is beginning to result in back-arc volcanism. The youngest (geologically speaking) volcanism in the South Island occurred in this region, forming the Solander Islands (<2 million years old). This region is dominated by the rugged and relatively untouched Fiordland, an area of flooded glacially carved valleys with little human settlement.

New Zealand proper is subdivided into 16 regions: seven in the South Island and nine in the North. They have a geographical link with regional boundaries being based largely on drainage basins. Among the regions, eleven are administered by regional authorities (top tier of local government), while five are unitary authorities that combine the functions of regional authorities and those of territorial authorities (second tier). Regional authorities are primarily responsible for environmental resource management, land management, regional transport, and biosecurity and pest management. Territorial authorities administer local roading and reserves, waste management, building consents, the land use and subdivision aspects of resource management, and other local matters.

The Chatham Islands is not a region, although its council operates as a region under the Resource Management Act. There are a number of outlying islands that are not included within regional boundaries. The Kermadecs and the subantarctic islands are inhabited only by a small number of Department of Conservation staff.

The South Island contains a little under one-quarter of the population. Over three-quarters of New Zealands population live in the North Island, with one-third of the total population living in the Auckland Region. Auckland is also the fastest growing region, accounting for 46 percent of New Zealand's total population growth. Most Māori live in the North Island (87 percent), although a little under a quarter (24 percent) live in Auckland. New Zealand is a predominantly urban country, with  percent of the population living in an urban area. About  percent of the population live in the 17 main urban areas (population of 30,000 or more) and  percent live in the four largest cities of Auckland, Christchurch, Wellington, and Hamilton.

The main factors that influence New Zealand's climate are similar to those found in the British Isles—the temperate latitude, with prevailing westerly winds; the oceanic environment; and the mountains, especially the Southern Alps. The climate is mostly temperate with mean temperatures ranging from in the South Island to in the North Island. January and February are the warmest months, July the coldest. New Zealand does not have a large temperature range, apart from central Otago, but the weather can change rapidly and unexpectedly. Near subtropical conditions are experienced in Northland.

Most settled, lowland areas of the country have between 600 and 1600 mm of rainfall, with the most rain along the west coast of the South Island and the least on the east coast of the South Island and interior basins, predominantly on the Canterbury Plains and the Central Otago Basin (about 350 mm PA). Christchurch is the driest city, receiving about of rain per year, while Hamilton is the wettest, receiving more than twice that amount at 1325 mm PA, followed closely by Auckland. The wettest area by far is the rugged Fiordland region, in the south-west of the South Island, which has between 5000 and 8000 mm of rain per year, with up to 15,000 mm in isolated valleys, amongst the highest recorded rainfalls in the world.
The UV index can be very high and extreme in the hottest times of the year in the north of the North Island. This is partly due to the country's relatively little air pollution compared to many other countries and the high sunshine hours.
New Zealand has very high sunshine hours with most areas receiving over 2000 hours per year. The sunniest areas are Nelson/Marlborough and the Bay of Plenty with 2400 hours per year.

Flooding is the most regular natural hazard. Few regions have escaped winter floods. Settlements are usually close to hill-country areas which experience much higher rainfall than the lowlands due to the orographic effect. Mountain streams which feed the major rivers rise rapidly and frequently break their banks covering farms with water and silt. Close monitoring, excellent weather forecasting, stopbanks, multiple hydropower dams, river dredging and reafforestation programmes in hill country have ameliorated the worst effects.

New Zealand experiences around 14,000 earthquakes a year, some in excess of magnitude 7 (M7). Since the 2010, several large (M7, M6.3, M6.4, M6.2) and shallow (all <7 km) earthquakes have occurred immediately beneath Christchurch. These have resulted in 185 deaths, widespread destruction of buildings and significant liquefaction. These earthquakes are releasing distributed stress in the Pacific plate from the ongoing collision with the Indo-Australian plate to the west and north of the city. Volcanic activity is most common on the central North Island Volcanic Plateau. Tsunamis affecting New Zealand are associated with the Pacific Ring of Fire.

Droughts are not regular and occur mainly in Otago and the Canterbury Plains and less frequently over much of the North Island between January and April. Forest fires were rare in New Zealand before the arrival of humans. Fire bans exist in some areas in summer.

New Zealand's geographic isolation for 80 million years and island biogeography has influenced evolution of the country's species of animals, fungi and plants. Physical isolation has not caused biological isolation, and this has resulted in a dynamic evolutionary ecology with examples of very distinctive plants and animals as well as populations of widespread species. Evergreens such as the giant kauri and southern beech dominate the forests. It also has a diverse range of birds, several of which are flightless such as the kiwi (a national icon), the kakapo, the takahē and the weka, and several species of penguins. Around 30 bird species are currently listed as endangered or critically endangered. Conservationists recognised that threatened bird populations could be saved on offshore islands, where, once predators were exterminated, bird life flourished again.

Many bird species, including the giant moa, became extinct after the arrival of Polynesians, who brought dogs and rats, and Europeans, who introduced additional dog and rat species, as well as cats, pigs, ferrets, and weasels. Native flora and fauna continue to be hard-hit by invasive species. New Zealand conservationists have pioneered several methods to help threatened wildlife recover, including island sanctuaries, pest control, wildlife translocation, fostering, and ecological restoration of islands and other selected areas.

Massive deforestation occurred after humans arrived, with around half the forest cover lost to fire after Polynesian settlement. Much of the remaining forest fell after European settlement, being logged or cleared to make room for pastoral farming, leaving forest occupying only 23% of the land.

Pollution, particularly water pollution, is one of New Zealand's most significant environmental issues. Fresh water quality is under pressure from agriculture, hydropower, urban development, pest invasions and climate change, although much of the country's household and industrial waste is now increasingly filtered and sometimes recycled.

Some areas of land, the sea, rivers or lakes are protected by law, so their special plants, animals, landforms and other features are safe from harm. New Zealand has three World Heritage Sites, 13 national parks, 34 marine reserves, and thousands of scenic, historic, recreation and other reserves. The Department of Conservation is responsible for managing 8.5 million hectares of public land (approximately 30% of New Zealand's landmass).

New Zealand is party to several multilateral environmental agreements. The major agreements are listed below.


New Zealand, and especially the Bounty and Antipodes Islands, are near the center of the water hemisphere—the hemisphere of the Earth with the smallest amount of land. New Zealand proper is largely antipodal to the Iberian Peninsula of Europe. The northern half of the South Island corresponds to Galicia and northern Portugal. Christchurch, in the South Island, is one of very few cities in the world that have near-exact antipodal cities, with A Coruña, Galicia, as Christchurch's antipode. Most of the North Island corresponds to central and southern Spain, from Valladolid (opposite the southern point of the North Island, Cape Palliser), through Madrid and Toledo to Cordoba (directly antipodal to Hamilton), Lorca (opposite East Cape), Málaga (Cape Colville), and Gibraltar. Parts of the Northland Peninsula oppose Morocco, with Whangarei nearly coincident with Tangiers. The antipodes of the Chatham Islands lie in France, just north of the city of Montpellier.

In Europe the term "Antipodes" is often used to refer to New Zealand and Australia (and sometimes other South Pacific areas), and "Antipodeans" to their inhabitants.



</doc>
<doc id="21354" url="https://en.wikipedia.org/wiki?curid=21354" title="Demographics of New Zealand">
Demographics of New Zealand

The demographics of New Zealand encompass the gender, ethnic, religious, geographic, and economic backgrounds of the million people living in New Zealand. New Zealanders, informally known as "Kiwis", predominantly live in urban areas on the North Island. The five largest cities are Auckland (with one-third of the country's population), Christchurch (in the South Island, the largest island of the New Zealand archipelago), Wellington, Hamilton and Tauranga. Few New Zealanders live on New Zealand's smaller islands. Waiheke Island (near Auckland) is easily the most populated smaller island with residents, while Great Barrier Island, the Chatham and Pitt Islands and Stewart Island each have populations below 1,000. New Zealand is part of a realm and most people born in the realm's external territories of Tokelau, the Ross Dependency, the Cook Islands and Niue are entitled to New Zealand passports. In 2006, more people who identified themselves with these islands lived in New Zealand than on the Islands themselves.

The majority of New Zealand's population is of European descent (74 percent), with the indigenous Māori being the largest minority (14.9 percent), followed by Asians (11.8 percent) and non-Māori Pacific Islanders (7.4 percent). This is reflected in immigration, with most new migrants coming from Britain and Ireland, although the numbers from Asia are increasing. The largest Māori tribe ("iwi") is Ngāpuhi with 125,601 people or 18.8 percent of the Māori population. Auckland is the most ethnically diverse region in New Zealand with 59.3 percent identifying as Europeans, 23.1 percent as Asian, 10.7 percent as Māori and 14.6 percent as Pacific Islanders. The ethnicity of the population aged under 18 years is considerably more diverse than the population aged 65 years or older. Recent increases in interracial marriages have resulted in more people identifying with more than one ethnic group.

English, Māori and New Zealand Sign Language are the official languages, with English predominant. New Zealand English is mostly non-rhotic and sounds similar to Australian English, with a common exception being the centralisation of the short i. The Māori language has undergone a process of revitalisation and is spoken by 3.7 percent of the population. New Zealand has an adult literacy rate of 99 percent and over half of the population aged 15 to 29 hold a tertiary qualification. In the adult population 14.2 percent have a bachelor's degree or higher, 30.4 percent have some form of secondary qualification as their highest qualification and 22.4 percent have no formal qualification. As of the 2013 census, just under half the population identify as Christians, with Hinduism and Buddhism being the most significant minority religions. New Zealand has no state religion and just over 40 percent of the population does not have a religion.

Farming is a major occupation in New Zealand, although more people are employed as sales assistants. Most New Zealanders earn wage or salary income, with a median personal income in 2013 of NZ$28,500.

While the demonym for a New Zealand citizen is New Zealander, the informal "Kiwi" is commonly used both internationally and by locals. The name derives from the kiwi, a native flightless bird, which is the national symbol of New Zealand. The Māori loanword "Pākehā" usually refers to New Zealanders of European descent, although some reject this appellation, and some Māori use it to refer to all non-Polynesian New Zealanders. Most people born in New Zealand or one of the realm's external territories (Tokelau, the Ross Dependency, the Cook Islands and Niue) before 2006 are New Zealand citizens. Further conditions apply for those born from 2006 onwards.

In , New Zealand has an estimated population of ,
up from the 4,027,947 recorded in the 2006 census. The median child birthing age was 30 and the total fertility rate is 2.1 births per woman in 2010. In Māori populations the median age is 26 and fertility rate 2.8. In 2010 the age-standardised mortality rate was 3.8 deaths per 1000 (down from 4.8 in 2000) and the infant mortality rate for the total population was 5.1 deaths per 1000 live births. The life expectancy of a New Zealand child born in 2014-16 was 83.4 years for females, and 79.9 years for males, which is among the highest in the world. Life expectancy at birth is forecast to increase from 80 years to 85 years in 2050 and infant mortality is expected to decline. In 2050 the population is forecast to reach 5.3 million, the median age to rise from 36 years to 43 years and the percentage of people 60 years of age and older rising from 18 percent to 29 percent. (The number of people aged 65 and over increased by 22 percent between the 2006 and 2013 censuses.) During early migration in 1858, New Zealand had 131 males for every 100 females, but following changes in migration patterns and the modern longevity advantage of women, females came to outnumber males in 1971. As of 2012 there are 0.99 males per female, with males dominating under 15 years and females dominating in the 65 years and older range.

The total fertility rate is the number of children born per woman. It is based on fairly good data for the entire period. Sources: Our World In Data and Gapminder Foundation. 

The following figures show the total fertility rates since the first years of the English colonization. Before it it has not been possible to have data.

New Zealand has a growing population, as measured:



New Zealand's population density is relatively low, at The vast majority of the population live on the main North and South Islands, with New Zealand's major inhabited smaller islands being Waiheke Island (), the Chatham and Pitt Islands (), and Stewart Island (381). Over three-quarters of the population live in the North Island ( percent), with one-third of the total population living in the Auckland Region. This region is also the fastest growing, accounting for 46 percent of New Zealand's total population growth. Most Māori live in the North Island (86.0 percent), although less than a quarter (23.8 percent) live in Auckland. New Zealand is a predominantly urban country, with  percent of the population living in an urban area. About  percent of the population live in the 17 main urban areas (population of 30,000 or more) and  percent live in the four largest cities of Auckland, Christchurch, Wellington, and Hamilton.

Approximately 14 percent of the population live in four different categories of rural areas as defined by Statistics New Zealand. About 18 percent of the rural population live in areas that have a high urban influence (roughly 12.9 people per square kilometre), many working in the main urban area. Rural areas with moderate urban influence and a population density of about 6.5 people per square kilometre account for 26 percent of the rural population. Areas with low urban influence where the majority of the residents work in the rural area house approximately 42 percent of the rural population. Remote rural areas with a density of less than 1 person per square kilometre account for about 14 percent of the rural population.

Before local government reforms in the late 1980s, a borough council with more than 20,000 people could be proclaimed a city. The boundaries of councils tended to follow the edge of the built-up area, so there was little difference between the urban area and the local government area. In 1989, all councils were consolidated into regional councils (top tier) and territorial authorities (second tier) which cover a much wider area and population than the old city councils. Today a territorial authority must have a predominantly urban population of at least 50,000 before it can be officially recognised as a city. The 20 largest urban areas are listed below:

Demographic statistics according to the World Population Review.


Demographic statistics according to the CIA World Factbook, unless otherwise indicated.
















European 71.2%, Maori 14.1%, Asian 11.3%, Pacific peoples 7.6%, Middle Eastern, Latin American, African 1.1%, other 1.6%, not stated or unidentified 5.4%

Note: based on the 2013 census of the usually resident population; percentages add up to more than 100% because respondents were able to identify more than one ethnic group (2013 est.)

Christian 44.3% (Catholic 11.6%, Anglican 10.8%, Presbyterian and Congregational 7.8%, Methodist, 2.4%, Pentecostal 1.8%, other 9.9%), Hindu 2.1%, Buddhist 1.4%, Maori Christian 1.3%, Islam 1.1%, other religion 1.4% (includes Judaism, Spiritualism and New Age religions, Baha'i, Asian religions other than Buddhism), no religion 38.5%, not stated or unidentified 8.2%, objected to answering 4.1%

Note: based on the 2013 census of the usually resident population; percentages add up to more than 100% because respondents were able to identify more than one religion (2013 est.)

East Polynesians were the first people to reach New Zealand about 1280, followed by the early European explorers, notably James Cook in 1769 who explored New Zealand three times and mapped the coastline. Following the Treaty of Waitangi in 1840 when the country became a British colony, immigrants were predominantly from Britain, Ireland and Australia. Due to restrictive policies, limitations were placed on non-European immigrants. During the gold rush period (1858–1880s) large number of young men came from California and Victoria to New Zealand goldfields. Apart from British, there were Irish, Germans, Scandinavians, Italians and many Chinese. The Chinese were sent special invitations by the Otago Chamber of Commerce in 1866. By 1873 they made up 40 percent of the diggers in Otago and 25 percent of the diggers in Westland. From 1900 there was also significant Dutch, Dalmatian, and Italian immigration together with indirect European immigration through Australia, North America, South America and South Africa. Following the Great Depression policies were relaxed and migrant diversity increased. In 2008–09, a target of 45,000 migrants was set by the New Zealand Immigration Service (plus a 5,000 tolerance).

Just over 25 percent of New Zealand's population at the 2013 census was born overseas, up from 23 percent in 2006 and 20 percent in 2001. Over half (51.6 percent) of New Zealand's overseas-born population lives in the Auckland Region, including 72 percent of the country's Pacific Island-born population, 64 percent of its Asian-born population, and 56 percent of its Middle Eastern and African- born population. In the late 2000s, Asia overtook the British Isles as the largest source of overseas migrants; today around 32 percent of overseas-born New Zealand residents were born in Asia (mainly China, India, the Philippines and South Korea) compared to 26 percent born in the UK and Ireland. The number of fee-paying international students increased sharply in the late 1990s, with more than 20,000 studying in public tertiary institutions in 2002.

To be eligible for entry under the skilled migrant plan applicants are assessed by an approved doctor for good health, provide a police certificate to prove good character and speak sufficient English. Migrants working in some occupations (mainly health) must be registered with the appropriate profession body before they can work within that area. Skilled migrants are assessed by Immigration New Zealand and applicants that they believe will contribute are issued with a residential visa, while those with potential are issued with a work to resident visa. Under the work to residency process applicants are given a temporary work permit for two years and are then eligible to apply for residency. Applicants with a job offer from an accredited New Zealand employer, cultural or sporting talent, looking for work where there has been a long-term skill shortage or to establish a business can apply for work to residency.

While most New Zealanders live in New Zealand, there is also a significant diaspora abroad, estimated as of 2001 at over 460,000 or 14 percent of the international total of New Zealand-born. Of these, 360,000, over three-quarters of the New Zealand-born population residing outside of New Zealand, live in Australia. Other communities of New Zealanders abroad are concentrated in other English-speaking countries, specifically the United Kingdom, the United States and Canada, with smaller numbers located elsewhere. Nearly one quarter of New Zealand's highly skilled workers live overseas, mostly in Australia and Britain, more than any other developed nation. However many educated professionals from Europe and lesser developed countries have recently migrated to New Zealand. A common pathway for New Zealanders to move to the UK is through a job offer via the Tier 2 (General) visa, which grants a 3-year initial stay in the country and can later be extended with three more years. After 5 years the person can apply for permanent residency. Another popular option is the UK Working Holiday visa, also known as "Youth Mobility Scheme" (YMS), which grants New Zealanders 2-year rights to live and work in the UK.

New Zealand is a multiethnic society, and home to people of many different national origins. Originally composed solely of the Māori who arrived in the thirteenth century, the ethnic makeup of the population later became dominated by New Zealanders of European descent. In the nineteenth century, European settlers brought diseases for which the Māori had no immunity. By the 1890s, the Māori population was approximately 40 percent of its size pre-contact. The Māori population increased during the twentieth century, though it remains a minority.

At the latest census in 2013, 74.0 percent identified as European, 14.9 percent, as Māori, 11.8 percent as Asian, 7.4 percent as Pacific peoples, and 1.2 percent as Middle-Eastern, Latin American, and African (MELAA). Most New Zealanders are of English, Scottish, and Irish ancestry, with smaller percentages of other European ancestries, such as Dutch, Dalmatian, French, German and Scandinavian. Auckland was the most diverse region with 59.3 percent identifying as European, 23.1 percent as Asian, 10.7 percent as Māori, and 14.6 percent as Pacific Islanders. 

All major ethnic groups increased when compared with the 2006 census, in which 67.6 percent identified as European, 14.6 percent as Māori, 9.2 percent as Asian, and 6.9 percent of Pacific Islander origin. An additional 11.1 percent identified themselves simply as a "New Zealander" (or similar), and 1.0 percent identified with other ethnicities. There was significant public discussion about usage of the term "New Zealander" during the months leading up to the 2006 census. The number of people identifying with this term increased from approximately 80,000 (2.4 percent) in 2001 to just under 430,000 people (11.1 percent) in 2006. The European grouping significantly decreased from 80.0 percent of the population in 2001 to 67.6 percent in 2006, however, this is broadly proportional to the large increase in "New Zealanders". The number of people identifying as a "New Zealander" dropped back to under 66,000 in 2013.

As recorded in the 2013 census, the largest Māori "iwi" is Ngāpuhi with 125,601 people (or 18.8 percent of people of Māori descent). Since 2006, the number of people of Māori descent stating Ngāpuhi as their iwi increased by 3,390 people (2.8 percent). The second-largest was Ngāti Porou, with 71,049 people (down 1.2 percent from 2006). Ngāi Tahu was the largest in the South Island and the third-largest overall, with a count of 54,819 people (an increase of 11.4 percent from 2006). A total of 110,928 people (or 18.5 percent) of Māori descent did not know their iwi (an increase of 8.4 percent compared with 2006). A group of Māori migrated to "Rēkohu", now known as the Chatham Islands, where they developed their distinct Moriori culture. The Moriori population was decimated, first, by disease brought by European sealers and whalers and, second, by Taranaki Māori, with only 101 surviving in 1862 and the last known full-blooded Moriori dying in 1933. The number of people identifying as having Moriori descents increased from 105 in 1991 to 945 in 2006, but decreased to 738 in 2013.

Recent increases in interracial marriages has resulted in the New Zealand population of Māori, Asian and Pacific Islander descent growing at a higher rate than those of European descent. In 2013, 11.2 percent of people identified with more than one ethnic group, compared with 10.4 percent in 2006. The ethnic diversity of New Zealand is projected to increase. Europeans (including "New Zealanders") will remain the largest group, although it is predicted to fall to 70 percent in 2026. The Asian, Pacific and Māori groups are the fastest growing and will increase to 3.4 percent, 10 percent and 16 percent, respectively. In 2013, the ethnicity of the population aged under 18 years was 71 percent European, 25 percent Māori, 13 percent Pacific, 12 percent Asian, and 1 percent MELAA. The population aged 65 years or older consisted of 87.8 percent European, 5.6 percent Māori, 4.7 percent Asian and 2.4 percent Pacific.

The maps below (taken from 2013 census data) show the percentages of people in each census area unit identifying themselves as European, Māori, Asian, or Pacific Islander (as defined by Statistics New Zealand). As people could identify themselves with multiple groups, percentages are not cumulative.

English has long been entrenched as a "de facto" national language due to its widespread use. In the 2013 census, 96.1 percent of respondents spoke English. The New Zealand English dialect is mostly non-rhotic with an exception being the Southern Burr found principally in Southland and parts of Otago. It is similar to Australian English and many speakers from the Northern Hemisphere are unable to tell the accents apart. In New Zealand English the short i (as in kit) has become centralised, leading to the phrase "fish and chips" sounding like "fush and chups" to the Australian ear. The words "rarely" and "really", "reel" and "real", "doll" and "dole", "pull" and "pool", "witch" and "which", and "full" and "fill" can sometimes be pronounced as homophones. Some New Zealanders pronounce the past participles "grown", "thrown" and "mown" using two syllables, whereas "groan", "throne" and "moan" are pronounced as one syllable. New Zealanders often reply to a question or emphasise a point by adding a rising intonation at the end of the sentence.
Initially, the Māori language ("te reo Māori") was permitted in native schools to facilitate English instruction, but as time went on official attitudes hardened against any use of the language. Māori were discouraged from speaking their own language in schools and work places and it existed as a community language only in a few remote areas. The language underwent a revival beginning in the 1970s, and now more people speak Māori. The future of the language was the subject of a claim before the Waitangi Tribunal in 1985. As a result, Māori was declared an official language in 1987. In the 2013 census, 21.3 percent of Māori people—and 3.7 percent of all respondents, including some non-Māori people—reported conversational fluency in the language. There are now Māori language immersion schools and two Māori Television channels, the only nationwide television channels to have the majority of their prime-time content delivered in Māori. Many places have officially been given dual Māori and English names in recent years.

In the 2013 census, 20,235 people reported the ability to use New Zealand Sign Language. This is down 16 percent on the 2006 census. NZSL was declared one of New Zealand's official languages in 2006.

Samoan is the most widely spoken non-official language (2.2 percent), followed by Hindi (1.7 percent), "Northern Chinese" (including Mandarin, 1.3 percent) and French (1.2 percent). A considerable proportion of first- and second-generation migrants are multilingual.

Education follows the three-tier model, which includes primary schools, followed by secondary schools (high schools) and tertiary education at universities or polytechnics. The Programme for International Student Assessment ranked New Zealand's education as the seventh highest in 2009. The Education Index, published with the UN's 2014 Human Development Index and based on data from 2013, listed New Zealand at 0.917, ranked second after Australia.

Primary and secondary schooling is compulsory for children aged 6 to 16 with most children starting at 5. Early leaving exemptions may be granted to 15-year-old students that have been experiencing some ongoing difficulties at school or are unlikely to benefit from continued attendance. Parents and caregivers can home school their children if they obtain approval from the Ministry of Education and prove that their child will be taught "as regularly and as well as in a registered school". There are 13 school years and attending state (public) schools is nominally free from a person's fifth birthday until the end of the calendar year following their 19th birthday.

The academic year in New Zealand varies between institutions, but generally runs from late January until mid-December for primary and secondary schools and polytechnics, and from late February until mid-November for universities. New Zealand has an adult literacy rate of 99 percent, and over half of the population aged 15 to 29 hold a tertiary qualification. In the adult population 14.2 percent have a bachelor's degree or higher, 30.4 percent have some form of secondary qualification as their highest qualification and 22.4 percent have no formal qualification.

New Zealand does not have a state religion, but the principal religion is Christianity. As recorded in the 2013 census, about 49 percent of the population identified themselves as Christians, although regular church attendance is estimated at 15 percent. Another 41.9 percent indicated that they had no religion (up from 34.7 percent in 2006) and around 6 percent affiliated with other religions.

The indigenous religion of the Māori population was animistic, but with the arrival of missionaries from the early nineteenth century most of the Māori population converted to Christianity. In the 2013 census, 2,595 Māori still identify themselves as adhering to traditional Māori beliefs. The largest Christian denominations are Roman Catholicism, Anglicanism, Presbyterianism. There are also significant numbers of Christians who identify themselves with Methodist, Pentecostal, Baptist and Latter-day Saint churches, and the New Zealand-based Rātana church has adherents among Māori. Immigration and associated demographic change in recent decades has contributed to the growth of minority religions, especially Hinduism, Buddhism and Islam.

New Zealand's early economy was based on sealing, whaling, flax, gold, kauri gum, and native timber. During the 1880s agricultural products became the highest export earner and farming was a major occupation within New Zealand. Farming is still a major employer, with 75 000 people indicating farming as their occupation during the 2006 census, although dairy farming has recently taken over from sheep as the largest sector. The largest occupation recorded during the census was sales assistant with 93,840 people. Most people are on wages or salaries (59.9 percent), with the other sources of income being interest and investments (24.1 percent) and self-employment (16.6 percent).

In 1982 New Zealand had the lowest per-capita income of all the developed nations surveyed by the World Bank. In 2010 the estimated gross domestic product (GDP) at purchasing power parity (PPP) per capita was roughly US$28,250, between the thirty-first and fifty-first highest for all countries. The median personal income in 2006 was $24,400. This was up from $15,600 in 1996, with the largest increases in the $50,000 to $70,000 bracket. The median income for men was $31,500, $12,400 more than women. The highest median personal income were for people identifying with the European or "other" ethnic group, while the lowest was from the Asian ethnic group. The median income for people identifying as Māori was $20,900. In 2013, the median personal income had risen slightly to $28,500.

Unemployment peaked above 10 percent in 1991 and 1992, before falling to a record low of 3.7 percent in 2007 (ranking third from twenty-seven comparable OECD nations). Unemployment rose back to 7 percent in late 2009. In the June 2017 quarter, unemployment had fallen to 4.8 percent. This is the lowest unemployment rate since December 2008, after the start of the global financial crisis, when it was 4.4 percent. Most New Zealanders do some form of voluntary work, more women volunteer (92 percent) than males (86 percent). Home ownership has declined since 1991, from 73.8 percent to 66.9 percent in 2006.




</doc>
<doc id="21355" url="https://en.wikipedia.org/wiki?curid=21355" title="Politics of New Zealand">
Politics of New Zealand

The politics of New Zealand function within a framework of a unitary parliamentary representative democracy. New Zealand is a constitutional monarchy in which a hereditary monarch—since 6 February 1952, Queen Elizabeth II—is the sovereign and head of state.

The New Zealand Parliament holds legislative power and consists of the Queen and the House of Representatives. The Queen is usually represented by the Governor-General of New Zealand. Members are elected to the House of Representatives usually every three years.

Executive power in New Zealand is based on the principle that "The Queen reigns, but the government rules". Although an integral part of the process of government, the Queen and her Governor-General remain politically neutral and are not involved in the everyday aspects of governing. Ministers are selected from among the democratically elected members of the House of Representatives. Most ministers are members of the Cabinet, which is the main decision-making body of the New Zealand Government. The Prime Minister is the most senior minister, chair of the Cabinet, and thus head of government, holding office on commission from the Governor-General. The office of prime minister is, in practice, the most powerful political office in New Zealand. The Government is accountable to Parliament for its actions and policies.

The country has a multi-party system in which many of its legislative practices derive from the unwritten conventions of and precedents set by the United Kingdom's Westminster Parliament. However, New Zealand has evolved variations; minority governments are common and typically dependent on confidence and supply agreements with other parties. The two dominant political parties in New Zealand have historically been the Labour Party and the National Party (or its predecessors).

The Economist Intelligence Unit rated New Zealand as a "full democracy" in 2016.

New Zealand is a unitary parliamentary democracy under a constitutional monarchy. It has no formal codified constitution; the constitutional framework consists of a mixture of various documents (including certain acts of the United Kingdom and New Zealand Parliaments), the Treaty of Waitangi and constitutional conventions. The Constitution Act in 1852 established the system of government and these were later consolidated in 1986. Constitutional rights are protected under common law and are strengthened by the Bill of Rights Act 1990 and Human Rights Act 1993, although these are not entrenched and can be overturned by Parliament with a simple majority. The Constitution Act 1986 describes the three branches of government in New Zealand: the Executive (the Sovereign and Cabinet), the legislature (Parliament) and the judiciary (Courts).

Parliament is responsible for passing laws (statutes), adopting the state's budgets, and exercising control of the Government. It currently has a single chamber, the House of Representatives. Before 1951 there was a second chamber, the Legislative Council. Suffrage is extended to everyone over the age of 18 years, women having gained the vote in 1893. Members of Parliament (MPs) are elected for a maximum term of three years, although an election may be called earlier in exceptional circumstances. The House of Representatives meets in Parliament House, Wellington.

Almost all parliamentary general elections between 1853 and 1996 were held under the first past the post (FPP) electoral system. Under FPP the candidate in a given electorate that received the most votes was elected to parliament. The only deviation from the FPP system during this time occurred in the 1908 election when a second ballot system was tried. Under this system the elections since 1935 have been dominated by two political parties, National and Labour.

Criticism of the FPP system began in the 1950s and intensified after Labour lost the 1978 and 1981 elections despite having more overall votes than National. An indicative (non-binding) referendum to change the voting system was held in 1992, which led to a binding referendum during the 1993 election. As a result, New Zealand has used the mixed-member proportional (MMP) system since 1996. Under MMP, each member of Parliament is either elected by voters in a single-member constituency via FPP or appointed from party lists. Officially, the New Zealand parliament has 120 seats, however this sometimes differs due to overhangs and underhangs.

Several seats are reserved for MPs elected on a separate Māori roll. However, Māori may choose to vote in and to run for the non-reserved seats and for the party list (since 1996), and as a result many have now entered Parliament outside of the reserved seats.

Queen Elizabeth II is New Zealand's sovereign and head of state. The New Zealand monarchy has been distinct from the British monarchy since the Statute of Westminster Adoption Act 1947, and all Elizabeth II's official business in New Zealand is conducted in the name of the "Queen of New Zealand". The Queen's role is largely ceremonial, and her residual powers—the "royal prerogative"—are mostly exercised through the government of the day. These include the power to enact legislation, to sign treaties and to declare war.
Since the Queen is not usually resident in New Zealand, the functions of the monarchy are conducted by a representative, the Governor-General. , the Governor-General is Dame Patsy Reddy. The Governor-General formally has the power to appoint and dismiss ministers and to dissolve Parliament; and the power to reject or sign bills into law by Royal Assent after passage by Parliament. The Governor-General chairs the Executive Council, which is a formal committee consisting of all ministers. Members of the Executive Council are required to be members of Parliament (MPs), and most are also in the Cabinet.

Cabinet is the most senior policy-making body and is led by the Prime Minister, who is also, by convention, the parliamentary leader of the governing party or coalition. The Prime Minister, being the "de facto" leader of New Zealand, exercises executive functions that are formally vested in the monarch (by way of the royal prerogative powers). Cabinet is directly responsible to the New Zealand Parliament, from which its members are derived; ministers are collectively responsible for all decisions made.

The most recent general election, held in September 2017, saw Labour finish in second place but able to govern through a coalition with New Zealand First, and a confidence and supply agreement with the Green Party. The Sixth Labour Government, led by Jacinda Ardern, was sworn in on 26 October 2017.

The National Party forms the Official Opposition to the government. The Leader of the Opposition is viewed as the leader of the "government-in-waiting".

The New Zealand judiciary has four basic levels of courts:

The Supreme Court was established in 2004, under the Supreme Court Act 2003, and replaced the Privy Council in London as New Zealand's court of last resort. The High Court deals with serious criminal offences and civil matters, and hears appeals from subordinate courts. The Court of Appeal hears appeals from the High Court on points of law.

The Chief Justice of New Zealand, the head of the judiciary, presides over the Supreme Court, and is appointed by the Governor-General on the advice of the Prime Minister. the incumbent Chief Justice is Dame Sian Elias. All other superior court judges are appointed on the advice of the Chief Justice, the Attorney-General, and the Solicitor-General. Judges and judicial officers are appointed non-politically and under strict rules regarding tenure to help maintain independence from the executive government. Judges are appointed according to their qualifications, personal qualities, and relevant experience. A judge may not be removed from office except by the Attorney-General upon an address of the House of Representatives (Parliament) for proved misbehaviour.

New Zealand law has three principal sources: English common law, certain statutes of the United Kingdom Parliament enacted before 1947 (notably the Bill of Rights 1689), and statutes of the New Zealand Parliament. In interpreting common law, the courts have endeavoured to preserve uniformity with common law as interpreted in the United Kingdom and related jurisdictions.

New Zealand is a unitary state rather than a federation—regions are created by the authority of the central government, rather than the central government being created by the authority of the regions. Local government in New Zealand has only the powers conferred upon it by Parliament. These powers have traditionally been distinctly fewer than in some other countries; for example, police and education are run by central government.

Local elections are held every three years to elect the mayors, city and district councillors, community board members, and district health board members.

The first political party in New Zealand was founded in 1891, and its main rival was founded in 1909—New Zealand had a "de facto" two-party system from that point until a change of electoral system in 1996. Since then New Zealand has been a multi-party system, with at least five parties elected in every election since. No party has been able to govern without support from other groups since 1996, making coalition government standard.

Historically the two largest, and oldest, parties are the Labour Party (centre-left, formed in 1916) and the National Party (centre-right, formed in 1936). Other parties represented in Parliament are ACT (right-wing, free-market), the Greens (left-wing, environmentalist) and New Zealand First (populist).

Prior to New Zealand becoming a British colony in 1840, politics in New Zealand was dominated by Māori chiefs as leaders of "hapu" and "iwi", utilising Māori customs as a political system.

After the 1840 Treaty of Waitangi, a colonial Governor and his small staff acted on behalf of the British government based on the British political system. Whereas Māori systems had dominated prior to 1840 governors attempting to introduce British systems met with mixed success in Māori communities. More isolated Māori were little influenced by the Government. Most influences were felt in and around Russell, the first capital, and Auckland, the second capital.

The first voting rights in New Zealand were legislated in 1852 as the New Zealand Constitution Act for the 1853 elections and reflected British practice. Initially only property owners could vote, but by the late 1850s 75% of British males over 21 were eligible to vote compared to 20% in England and 12% in Scotland. Around 100 Māori chiefs voted in the 1853 election.

During the 1850s provincial-based government was the norm. It was abolished in 1876. Politics was initially dominated by conservative and wealthy "wool lords" who owned multiple sheep farms, mainly in Canterbury. During the gold rush era starting 1858 suffrage was extended to all British gold miners who owned a 1-pound mining license. The conservatives had been influenced by the militant action of gold miners in Victoria at Eureka. Many gold miners had moved to the New Zealand fields bringing their radical ideas. The extended franchise was modelled on the Victorian system. In 1863 the mining franchise was extended to goldfield business owners. By 1873 of the 41,500 registered voters 47% were gold field miners or owners.

After the brief Land War period ending in 1864, Parliament moved to extend the franchise to more Māori. Donald McLean introduced a bill for four temporary Māori electorates and extended the franchise to all Māori men over 21 in 1867. As such, Māori were universally franchised 12 years prior to European men.

In 1879 an economic depression hit, resulting in poverty and many people, especially miners, returning to Australia. Between 1879 and 1881 Government was concerned at the activities of Māori activists based on confiscated land at Parihaka. Activists destroyed settlers farm fences and ploughed up roads and land which incensed local farmers. Arrests followed but the activities persisted. Fears grew among settlers that the resistance campaign was a prelude to armed conflict. The government itself was puzzled as to why the land had been confiscated and offered a huge 25,000 acre reserve to the activists, provided they stopped the destruction. Commissioners set up to investigate the issue said that the activities "could fairly be called hostile". A power struggle ensued resulting in the arrest of all the prominent leaders by a large government force in 1881. Historian Hazel Riseborough describes the event as a conflict over who had authority or mana-the Government or the Parihaka protestors.
In 1882 the export of meat in the first refrigerated ship started a period of sustained economic export led growth. This period is notable for the influence of new social ideas and movements such as the Fabians and the creation in 1890 of the first political party, the Liberals. Their leader, former gold miner Richard Seddon from Lancashire, was Premier from 1893 to 1906. The Liberals introduced new taxes to break the influence of the wealthy conservative sheep farm owners. They also purchased more land from Māori. In 1896 Maori made up 2.9% of the population but owned 15% of the land. Far more small farms and a new land owning class were created during this period. 

Māori political affairs have been developing through legislation such as the Resource Management Act 1991 and the Te Ture Whenua Māori Act 1993 and many more. Since colonisation in the 1800s, Māori have had their customary laws oppressed, with the imposition of a Westminster democracy and political style. As reparations from the colonial war and general discrepancies during colonisation, the Crown and the New Zealand government have formally apologised to those iwi affected, through settlements and legislation. In the 1960s Māori Politics Relations began to exhibit more positivity. The legislature enacted a law to help Māori retrieve back their land, not hinder them, through the Māori Affairs Amendment Act 1967. Since then, this progressive change in attitude has materialised as legislation to protect the natural environment or Taonga, and the courts by establishing treaty principles that always have to be considered when deciding laws in the courts. Moreover, recently an Act enacted by parliament, such as the Māori Lands Act 2016, printed both in Te Reo Māori and English, the act itself affirms the status of Te Reo Māori, showing New Zealand's ever growing political realm, and its ability to become more inclusive and respectful of indigenous customary laws.

Women's suffrage was granted after about two decades of campaigning by women such as Kate Sheppard and Mary Ann Müller and organisations such as the New Zealand branch of the Women's Christian Temperance Union. On 19 September 1893 the governor, Lord Glasgow, signed a new Electoral Act into law. As a result, New Zealand became the first self-governing nation in the world in which all women had the right to vote in parliamentary elections. Women first voted in the 1893 election, with a high 85% turnout (compared to 70% of men).

Women were not eligible to be elected to the House of Representatives until 1919 though, when three women, including Ellen Melville stood. The first woman to win an election (to the seat held by her late husband) was Elizabeth McCombs in 1933. Mabel Howard became the first female cabinet minister in 1947, being appointed to the First Labour Government.

New Zealand was the first country in the world in which all the highest offices were occupied by women, between March 2005 and August 2006: the Sovereign Queen Elizabeth II of New Zealand, Governor-General Dame Silvia Cartwright, Prime Minister Helen Clark, Speaker of the New Zealand House of Representatives Margaret Wilson and Chief Justice Dame Sian Elias.

The right-leaning National Party and the left-leaning Labour Party have dominated New Zealand political life since a Labour government came to power in 1935. During fourteen years in office (1935–1949), the Labour Party implemented a broad array of social and economic legislation, including comprehensive social security, a large scale public works programme, a forty-hour working week, a minimum basic wage, and compulsory unionism. The National Party won control of the government in 1949 and adopted many welfare measures instituted by the Labour Party. Except for two brief periods of Labour governments in 1957-1960 and 1972–1975, National held power until 1984.

After regaining control in 1984, the Labour government instituted a series of radical market-oriented reforms in response to New Zealand's mounting external debt. It also enacted anti-nuclear legislation that effectively brought about New Zealand's suspension from the ANZUS security alliance with the United States and Australia, and instituted a number of other more left-wing reforms, such as allowing the Waitangi Tribunal to hear claims of breaches of the Treaty of Waitangi to be made back to 1840, reinstituting compulsory unionism and creating new government agencies to implement a social and environmental reform agenda (women's affairs, youth affairs, Pacific Island affairs, consumer affairs, Minister for the Environment).

In October 1990, the National Party again formed a government, for the first of three three-year terms. In 1996, New Zealand inaugurated the new electoral system (mixed-member proportional representation) to elect its Parliament. The system was expected (among numerous other goals) to increase representation of smaller parties in Parliament and appears to have done so in the MMP elections to date. Since 1996, neither National nor Labour has had an absolute majority in Parliament, and for all but two of those years a minority government has ruled. In 1995 Georgina Beyer became the world's first openly transsexual mayor, and in 1999 she became the world's first openly transsexual Member of Parliament.

After nine years in office, the National Party lost the November 1999 election. Labour under Helen Clark out-polled National by 39% to 30% and formed a coalition, minority government with the left-wing Alliance. The government often relied on support from the Green Party to pass legislation.

The Labour Party retained power in the 27 July 2002 election, forming a coalition with Jim Anderton's new party, the Progressive Coalition, and reaching an agreement for support with the United Future party. Helen Clark remained Prime Minister.

Following the 2005 general election on 17 September 2005, negotiations between parties culminated in Helen Clark announcing a third consecutive term of Labour-led government. The Labour Party again formed a coalition with Jim Anderton's Progressive Party, with confidence and supply from Winston Peters' New Zealand First and Peter Dunne's United Future. Jim Anderton retained his Cabinet position; Winston Peters became Minister of Foreign Affairs, Minister of Racing and Associate Minister for Senior Citizens; Peter Dunne became Minister of Revenue and Associate Minister of Health. Neither Peters nor Dunne were in Cabinet.

After the general election in November 2008, the National Party moved quickly to form a minority government with the ACT Party, the Maori Party and United Future. This arrangement allowed National to decrease its reliance on the right-leaning ACT party, whose policies are sometimes controversial with the greater New Zealand public. In 2008, John Key became Prime Minister, with Bill English his deputy. This arrangement conformed to a tradition of having a north-south split in the major parties' leadership, as Key's residence is in Auckland and English's electorate is in the South Island. On 12 December 2016, English was elected Prime Minister by the National Party caucus after Key's unexpected resignation a week earlier. Paula Bennett (member for Upper Harbour) was elected Deputy Prime Minister, thus continuing the tradition.

Following the 2017 general election National retained its plurality in the House of Representatives, while Labour greatly increased its proportion of the vote and number of seats. In a first under the MMP system, Labour formed a minority government after securing a coalition arrangement with New Zealand First. Jacinda Ardern, Labour leader, became Prime Minister, with Winston Peters becoming Deputy Prime Minister. The Labour government also has a confidence and supply agreement with the Green Party.





</doc>
<doc id="21356" url="https://en.wikipedia.org/wiki?curid=21356" title="Economy of New Zealand">
Economy of New Zealand

The economy of New Zealand is the 53rd-largest national economy in the world when measured by nominal gross domestic product (GDP) and the 68th-largest in the world when measured by purchasing power parity (PPP). New Zealand has one of the most globalised economies and depends greatly on international trade - mainly with Australia, the European Union, the United States, China, South Korea, Japan and Canada. New Zealand's Closer Economic Relations agreement with Australia means that the economy aligns closely with that of Australia.

New Zealand's diverse market economy has a sizable service sector, accounting for 63% of all GDP activity . Large-scale manufacturing industries include aluminium production, food processing, metal fabrication, wood and paper products. Mining, manufacturing, electricity, gas, water, and waste services accounted for 16.5% of GDP . The primary sector continues to dominate New Zealand's exports, despite accounting for only 6.5% of GDP .

The major capital market is the New Zealand Exchange, known as the NZX. , NZX had a total of 258 listed securities with a combined market capitalisation of NZD $94.1 billion. New Zealand's currency, the New Zealand dollar (informally known as the "Kiwi dollar") also circulates in five Pacific island territories. The New Zealand dollar is the 10th-most traded currency in the world.

The New Zealand economy has been ranked first in the world for Social Progression, which covers such areas as Basic Human Needs, Foundations of Wellbeing, and the level of Opportunity available to its citizens. However, the outlook includes some challenges. New Zealand income levels, which used to be above those of many other countries in Western Europe prior to the crisis of the 1970s, have dropped in relative terms and never recovered. As a result, the number of New Zealanders living in poverty has grown and income inequality has increased dramatically.

New Zealand has also had persistent current account deficits since the early 70s, peaking at -7.8% of GDP in 2006 but falling to -2.6% of GDP in FY 2014. Despite this, public debt (that owed by the Government) stands at 38.4% (2013 estimate) of GDP, which is small compared to many developed nations. However, between 1984 and 2006, net foreign debt increased 11-fold, to NZ$182 billion. By March 2014 net foreign debt had dropped back to NZ$141.6 billion, which represents 61.5% of GDP.
Despite New Zealand's persistent current account deficits, the balance on external goods and services has generally been positive. In FY 2014, export receipts exceeded imports by NZ$3.9 billion. There has been an investment income imbalance or net outflow for debt-servicing of external loans. In FY 2014, New Zealand's investment income from the rest of the world was NZ$7 billion, versus outgoings of NZ$16.3 billion, a deficit of NZ$9.3 billion. The proportion of the current account deficit that is attributable to the investment income imbalance (a net outflow to the Australian-owned banking sector) grew from one third in 1997 to roughly 70% in 2008.

Taxation in New Zealand is collected at a national level by the Inland Revenue Department (IRD) on behalf of the Government of New Zealand. National taxes are levied on personal and business income, and on the supply of goods and services (GST). There is no capital gains tax although certain "gains" such as profits on the sale of patent rights are deemed to be income, income tax does apply to property transactions in certain circumstances, particularly speculation. Local property taxes (rates) are managed and collected by local authorities. Some goods and services carry a specific tax, referred to as an excise or a duty such as alcohol excise or gaming duty. These are collected by a range of government agencies such as the New Zealand Customs Service. There is no social security (payroll) tax or land tax in New Zealand.

In the 2010 New Zealand budget, personal tax rates were cut with the top personal tax rate reduced from 38% to 33% The cuts gave New Zealand the second-lowest personal tax burden in the OECD. Only Mexico's citizens had a higher percentage-wise "take home" proportion of their salaries.

The cuts in income tax were estimated to have reduced revenue by $2.46 billion. To compensate, GST was raised from 12.5% to 15%. Treasury figures show that top income earners in New Zealand pay between 6% and 8% of their income on GST. Those at the bottom end, earning less than $356 a week, spend between 11% and 14% on GST. Based on these figures, the New Zealand Herald predicted that putting GST up to 15% would increase living costs for the poor more than twice as much as for the rich.

New Zealand was ranked 1st on the Transparency International Corruption Perceptions Index (CPI) of 2017.

In 2015 Statistics New Zealand published details of the break-down of Gross Domestic Product in the Regions of New Zealand for the year ended March 2015:

Prior to the economic shock created by Britain's decision to join the EEC in 1973, removing it as New Zealand's primary market for exports, unemployment in New Zealand was very low. The official number of people unemployed in 1959 was only 21. A year later it was 22. A recession and collapse in wool prices in 1966 led to unemployment rising by 131%, but was still only a 0.7% increase in unemployment.

After 1973, unemployment became a persistent economic and social issue in New Zealand. Recessions from 1976–78 and 1982-83 greatly increased unemployment again. Between 1985 and 2012, the unemployment rate averaged 6.29%. After the stock market crash of 1987, unemployment rose 170% reaching an all-time high of 11.20% in September 1991. The Asian financial crisis of 1997 set unemployment upwards again, by 28%. By 2007, it had dropped again and the rate stood at 3.5% (December 2007), its lowest level since the current method of surveying began in 1986. This gave the country the 5th-best ranking in the OECD (with an OECD average at the time of 5.5%). The low numbers correlated with a robust economy and a large backlog of job positions at all levels. Unemployment numbers are not always directly comparable between OECD nations, as they do not all keep labour market statistics in the same way.

The percentage of the population employed also increased in recent years, to 68.8% of all inhabitants, with full-time jobs increasing slightly, and part-time occupations decreasing in turn. The increase in the working population percentage is attributed to increasing wages and higher costs of living moving more people into employment. The low unemployment also had some disadvantages, with many companies unable to fill jobs.

From December 2007, mainly as a result of the global financial crisis, unemployment numbers began to rise, this time by 106% with job losses especially high amongst women. In the last quarter of 2012, the unemployment rate fell to 6.9% from a 13-year high. This now makes New Zealand the 14th lowest among developed nations, below Canada's 7.2% and above Israel's 6.7%. In the September 2014 quarter, unemployment was 5.4%.

Shamubeel Eaqub, formerly a Principal Economist at the New Zealand Institute of Economic Research (NZIER), said that thirty years ago, an average house in New Zealand cost two or three times the average household income. House prices rose dramatically in the first years of the 21st century and by 2007, an average house cost more than six times household income. International surveys in 2013 showed that housing was unaffordable in all eight of New Zealand's major markets – unaffordable being defined by house prices which are more than three times the median regional income.

Demand for property has been strongest in Auckland pushing up prices in the city by 52% in the last five years. In 2014 the average sales price there went from $619,136 to $696,047, a rise of 12% in that 12-month period alone. In 2015, prices rose another 14%. This makes Auckland New Zealand's least affordable market and one of the most expensive cities in the world with houses costing 8 times the average income. Between 2012 and April 2016, the average Auckland home increased in price by just over two-thirds reaching $931,000 - higher than the cost of an average home in Sydney.

As a result, more and more people are being pushed out of the property market. Those on low incomes are hardest hit, affecting many Maori and Pacific Islanders. New Zealand's relatively high mortgage rates are exacerbating the problem even making it difficult for young people with steady jobs to buy their first home. According to a submission made to the Housing Affordability Inquiry, escalating house prices are also impacting on many middle income groups, especially those with large families. Mortgage adviser Bruce Patten said the trend was 'disturbing' and added to the gap between the 'haves and have-nots'.

Property analysis company CoreLogic says 45% of house purchases in New Zealand are now made by investors who already own a home, while another 28% are made by people moving from one property to another. Approximately 8% are being made by overseas cash buyers primarily Australians, Chinese, and British – although most economists believe foreign investment is currently too small to have a significant affect on property prices.

Whether purchases are made by New Zealanders or foreigners, it is generally those who are already well off that are buying the bulk of properties on the market. This has had a dramatic effect on home ownership rates by Kiwis, now at its lowest level since 1951. Even as recently as 1991, 76% of New Zealand homes were occupied by their owners. By 2013, this was down to 63%, indicating that more and more people are having to rent. Raewyn Cox, chief executive of the Federation of Family Budgeting, says: "High prices and high interest rates (have) sentenced a rising number of New Zealanders to be lifetime tenants" where they are "stuck in expensive rental situations, heading towards retirement."

Between 1982 and 2011, New Zealand's gross domestic product grew by 35%. Almost half of that increase went to a small group who were already the richest in the country. During this period, the average income of the top 10% of earners in New Zealand (those earning more than $72,000) almost doubled going from $56,300 to $100,200. The average income of the poorest tenth increased by only 13% from $9700 to $11,000.

Growing inequality is confirmed by Statistics New Zealand which keeps track of income disparity using the P80/20 ratio. This ratio shows the difference between high household incomes (those in the 80th percentile) and low household incomes (those in the 20th percentile). The inequality ratio increased between 1988 and 2004, and decreased until the onset of the Global Financial Crisis in 2008, increasing again to 2011 and then declining again from then. By 2013, the disposable income of high-income households was more than two-and-a-half times larger than that of low-income households. Highlighting the disparity, the top 1% of the population now owns 16% of the country's wealth – the richest 5% owns 38% – while half the population, including beneficiaries and pensioners, earn less than $24,000.

Factors contributing to the growth in inequality include substantial cuts in the top income tax rate in 1986-88 combined with a surge in unemployment caused by Rogernomics and the stock market crash of 1987 which pushed more people onto welfare. Then in 1991, benefits were also cut back substantially as part of the 'reforms' and those on welfare have been struggling ever since.

Professor Jonathan Boston of Victoria University says nearly 20% of poorer households in New Zealand now depend on welfare benefits. He says the growing gap between rich and poor enables the rich to "exercise disproportionate political influence", and that "if disadvantaged citizens are not to be excluded from political life, they must have access to education, healthcare and social assistance". British epidemiologists, Richard Pickett and Kate Wilkinson, argue that inequality is damaging for everyone in society, not just the poor. They say that when the gap between the top and the bottom levels of society becomes too wide, this erodes trust and empathy between citizens leading to alienation and social fragmentation. This exacerbates a multitude of health and social problems such as high infant mortality, obesity, teenage pregnancy, crime and imprisonment.

In 2014, Pickett and Wilkinson were invited to Auckland and Dunedin to discuss the relevance of their research to New Zealand. They argued that as inequality in New Zealand has grown, there has been a dramatic increase in the youth suicides;, although in contrast, recent Department of Health data shows that the age-standardised suicide rate decreased by 19.5% from the peak rate of 15.1 deaths per 100,000 population in 1998 to 12.2 deaths per 100,000 population in 2012. The proliferation of food banks increased dramatically; and the number of families and children living in poverty has increased. However serious crimes causing injury and death decreased by 20% between 2012 and 2014, whilst assaults decreased by 3% over the same period. At the same time, health care spending has increased. In 2011 Health spending accounted for 10% of GDP, higher than the OECD average of 9.3%. As in many OECD countries, health spending in New Zealand slowed down post the GFC but still reached 3% in real terms in 2010 and 2011 – higher than the OECD average. in 2012 New Zealand has 2.7 doctors per 1,000 population, and increase from 2.2 in the year 2000.

In 2012, life expectancy at birth in New Zealand stood at 81.5 years, more than one year higher than
the OECD average of 80.2 years.

In December 2014, the OECD released the Global Income Inequality Report which said "[r]ising inequality is estimated to have knocked more than 10 percentage points off growth in ...New Zealand" between 1990 and 2010. The paper found no evidence that redistributive policies, such as taxes and social benefits, harm economic growth, provided these policies are well designed, targeted and implemented. It concluded that "focusing exclusively on growth and assuming that its benefits will automatically trickle down to the different segments of the population may undermine growth in the long run."

In the 21st century concern has been growing that an increasing number of New Zealanders, especially children, have been pushed into poverty where poverty is defined in income terms as households living at below 60% of the national median income. In 2005, an international report found that one in six children in New Zealand were being raised in poverty – making New Zealand children 23rd poorest out of 26 rich nations. In 2009 according to NCCSS, over half a million New Zealanders, including 163,000 children were living in poverty. The Expert Advisory Group established by the Children's Commissioner found that the number of children falling below the threshold has continued to grow. In 2013, around 265,000 children, a quarter of all children in New Zealand, were now "mired in poverty".

Statistics New Zealand also publishes a range of data on the economic well-being of New Zealanders and, in 2012, released a discussion paper highlighting the need for government agreement on the development of more useful criteria and statistics related to poverty. Currently the information that is collected is 'static data' – it shows the percentage of citizens below a certain level of income. But New Zealand is unique among western OECD countries in that it does not collect 'dynamic' data which captures the extent to which people move in and out of poverty.

In 2013 over a dozen different reports were released which focused on the issue and the need to develop agreed ways of describing and measuring poverty. However, the National Government resisted these attempts maintaining that "endless arguments about definition and measurement are a waste of time". Because of the Government's reluctance to define and measure the problem, in 2012 Children's Commissioner Dr Russell Wills, established an expert advisory group which produced a comprehensive report, called "Solutions to Child Poverty in New Zealand: Evidence for Action" which contains 78 recommendations to combat poverty. Dr Wills also set up the "Child Poverty Monitor" to highlight the living conditions of children in New Zealand on an ongoing basis.

New Zealand has a universal superannuation scheme. Everyone aged 65 years old or over, who is a New Zealand citizen or permanent resident and normally lives in New Zealand at the time they apply is eligible. They must also have lived in New Zealand for at least 10 years since they turned 20 with five of those years being since they turned 50. Time spent overseas in certain countries and for certain reasons may be counted for New Zealand Superannuation. New Zealand Superannuation is taxed, the rate of which depends on their other income. The amount of Superannuation paid depends on the person's household situation. For a married couple the net tax amount is set by legislation to be no less than 66% of net average wage.

Because of the growing number of elderly becoming eligible, superannuation costs rose from $7.3 billion a year in 2008 to $10.2 billion in 2014. In 2011 there were twice as many children in New Zealand as elderly (65 and over); by 2051 there are projected to be 60% more elderly than children. In the ten years from 2014, the number of New Zealanders over the age of 65 is projected to grow by about 200,000.

This poses a significant problem for superannuation. The age of eligibility was gradually increased from 61 to 65 between 1993 and 2001. In that year the Labour Government of Helen Clark introduced the New Zealand Superannuation Fund (known as the "Cullen Fund" after Minister of Finance Michael Cullen) to part-fund the superannuation scheme into the future. As at October 2014, the fund managed NZ$27.11 billion, 15.9% of which was invested in New Zealand.

In 2007 a new individual saving scheme was introduced by the same Government, known as KiwiSaver. The main purpose of KiwiSaver is for retirement savings, but younger participants can also use it to save a deposit for their first home. The scheme is voluntary, work-based and managed by private sector companies called KiwiSaver providers. As at 30 June 2014, KiwiSaver had 2.3 million active members or 60.9% of New Zealand's population under 65. NZ$4 billion was contributed annually, and a total of NZ$19.1 billion has been contributed since 2007.

According to the National Infrastructure Unit of the Treasury, New Zealand "...continues to face challenges to its infrastructure; all forms of infrastructure are long-term investments, and change does not come about easily or quickly."

New Zealand's transport infrastructure is "generally well developed."

The New Zealand state highway network consists of 11,000 km of road, with 5981.3 km in the North Island and 4924.4 km in the South Island, built and maintained by the NZ Transport Agency, and paid for from general taxation and fuel excise duty. Heavy road users must pay Road User Charges as well, there is limited use of tolling on state highways. There is also 83,000 km of local roads built and maintained by local authorities.

The railway network is owned by state-owned enterprise KiwiRail and consists of 3,898 km of railway line, built to the narrow gauge of . Of this, 506 km is electrified.

There are five international airports. Air New Zealand, 53% government-owned, is the national carrier and state owned enterprise. Airways New Zealand, another state owned enterprise, provides air traffic control and communications.

New Zealand has 14 international seaports.

Present-day telecommunications in New Zealand include telephony, radio, television, and internet usage. A competitive telecommunications market has seen mobile prices drop to some of the lowest in the OECD. The copper wire and fibre cable networks are mostly owned by Chorus Limited, a publicly listed company. Chorus wholesales services to retail providers (such as Spark). In the mobile sector, there are three operators: Spark, Vodafone and 2degrees.

New Zealand has a high rate of internet use. , there are 1,916,000 broadband connections and 65,000 dial-up connections in New Zealand, of which 1,595,000 are residential and 386,000 are business or government. The majority of connections are Digital Subscriber Line over phone lines.

The Government has two plans to bring Ultra-Fast Broadband to 97.8% of the population by 2019, and is spending NZ$1.35 billion on public-private partnerships to roll out fibre-to-the-home connection in all main towns and cities with population over 10,000. The program aims to deliver ultra-fast broadband capable of at least 100 Mbit/s download and 50Mbit/s upload to 75% of New Zealanders by 2019. In total, 1,340,000 households in 26 towns and cities will be connected.

Gigabit internet (1000Mbit/s download speeds) was made available to the entire Ultra-Fast Broadband (UFB) footprint on 1 October 2016, in an announcement from Chorus.

A $300 million Rural Broadband Initiative (RBI) has also been introduced by the Government, with the aim to bring broadband of at least 5Mbit/s to 86% of rural customers by 2016.

From 1995 to 2013, the energy intensity of the economy per unit of GDP declined by 25 percent. A contributing factor is the growth of relatively less energy-intensive service industries.

The electricity market is regulated by the Electricity Industry Participation Code administered by the Electricity Authority (EA). The electricity sector uses mainly renewable energy sources such as hydropower, geothermal power and increasingly wind energy.

The 70% share of renewable energy sources makes New Zealand one of the most sustainable economies in terms of energy generation. New Zealand suffers from a geographical imbalance between electricity production and consumption. The most substantial electricity generation (both existing and as remaining potential) is located on the South Island and to a lesser degree in the central North Island, while the main demand (which is continuing to grow) is in the northern North Island, particularly the Auckland Region. This requires electricity to be transmitted north through a power grid which is reaching its capacity more often.

For many years New Zealand's economy was built on a narrow range of agricultural products, such as wool, meat and dairy. These products became New Zealand's staple and most valuable exports, underpinning the success of the economy, from the 1850s until the 1970s. For example, from 1920 to the late 1930s, the dairy export quota was usually around 35% of New Zealand's total exports, and in some years made up almost 45%. Due to the high demand for these primary products, manifested by the New Zealand wool boom of 1951, New Zealand had one of the highest standards of living in the world for 70 years.

In the 1960s, prices for these traditional exports declined, and in 1973 New Zealand lost its preferential trading position with the United Kingdom when the latter joined the European Economic Community. Partly as a result, from 1970 to 1990, the relative New Zealand GDP per capita adjusted for purchasing power declined from about 115% of the OECD average to 80%.

Between 1984 and 1993, New Zealand changed from a somewhat closed and centrally controlled economy to one of the most open economies in the OECD. In a process often referred to in New Zealand as Rogernomics, successive governments introduced policies which dramatically liberalised the economy.

In 2005 the World Bank praised New Zealand as the most business-friendly country in the world. The economy diversified and by 2008, tourism had become the single biggest generator of foreign exchange.

Prior to European settlement and colonisation of New Zealand, Māori had a subsistence economy, the basic economic unit of which was the sub-tribe or hapū. From the 1790s, the waters around New Zealand were visited by British, French and American whaling, sealing and trading ships. Their crews traded European goods, including guns and metal tools, for Māori food, water, wood, flax and sex. Their increasing lawlessness and plans for formal settlement by the New Zealand Company were two of the drivers behind the signing of the Treaty of Waitangi in 1840, which established New Zealand as a colony. Settlers continued to be dependent on Māori for food until the 1860s. From then immigrants became self-sufficient in farming, and started quarrying a variety of minerals including gold, which was discovered at Gabriel's Gully in Central Otago, leading to the Otago Gold Rush in 1861. Settlements flourished in areas where these quarries were established. In the 1880s, Dunedin became the richest city in the country largely on the back of investments from the gold rush.

Sheep farming began in the Wairarapa but soon spread up and down the east coast from Southland to the East Cape once rudimentary roads and transport became available. Much of the land used for farming was taken or leased from Māori. Sheep numbers grew quickly and by the mid-1850s, there were already a million sheep in New Zealand; by the early 1870s, there were 10 million. Wool became the first staple export, initially exported from the Wellington settlement in the late 1850s, although unrefrigerated meat and dairy products were exported as far as Australia.

In the 1870s, Julius Vogel was periodically both colonial treasurer and premier. He viewed New Zealand as a "Britain of the South Seas" and began the development of infrastructure in New Zealand investing in heavily in roads, railways, telegraphs and bridges funded by public borrowing. Progress slowed after the collapse of the City of Glasgow Bank in 1878 which led to a contraction in credit from London, the centre of the world’s financial system at the time. Economic activity was depressed for some years afterwards, until refrigeration was introduced in 1882. This enabled New Zealand to start exporting meat and other frozen products to the United Kingdom. Refrigeration transformed and shaped the development of the economy but, in the process, established New Zealand's economic dependence on Britain.

The success of refrigeration was directly related to the growth and development of farming in the country. In the 19th century, the bulk of economic activity was in the South Island of New Zealand. From around 1900, dairy farming became increasingly viable in areas which were less suitable for sheep, particularly in Northland, the Waikato and Taranaki. As dairying developed, the North Island slowly became more important to the economy. As more land was cultivated and farmed, Britain became the sole market for New Zealand meat and animal products. The dairy farming can therefore be seen as a response to the powerful market demands in Europe, transforming not only New Zealand’s countryside, economy and production techniques, but also causing migration in order to create the needed supply of dairy farming.

The Reserve Bank of New Zealand was established as New Zealand's central bank on 1 August 1934. Up until that time New Zealand's monetary policy had been set in the United Kingdom, and the New Zealand Pound was issued by private banks. A separate central bank gave New Zealand's government control of monetary policy for the first time, although New Zealand remained part of the Sterling area by pegging its pound to the British Pound until the introduction of the New Zealand Dollar in 1967, when the dollar was instead pegged to the Australian dollar.

By the mid-20th century, pastoral-farming products made up more than 90% of New Zealand's exports, 65% of which was going to Britain in the 1950s. Having a secure market with guaranteed prices also enabled New Zealand to impose high tariffs on imported goods from other countries. Tough import controls gave local manufacturers the ability to produce similar products locally, broaden the base of jobs available in New Zealand and still compete against higher priced imports.

This prosperity continued up to 1955 at which point Britain stopped giving New Zealand guaranteed prices for its exports. From then on, what New Zealand received was dictated by the free market. As a result, during the 1950s and 1960s the country's standard of living began to slip as the export sector was no longer able to pay for the level of imported goods required to meet the country's growing consumerism.

Britain applied to join the European Economic Community (EEC) in 1961, but was vetoed by the French. The government of Keith Holyoake reacted by attempting to diversify New Zealand's export markets, signing the first free trade agreement (Australia New Zealand Free Trade Agreement) in 1965, and opening new diplomatic posts in Hong Kong, Jakarta, Saigon, Los Angeles and San Francisco. Britain applied again to join the EEC in 1967, and entered into negotiations for membership in 1970. Holyoake's deputy and successor, Jack Marshall, (briefly Prime Minister in 1972) negotiated continued access for New Zealand exports to the United Kingdom under the so-called "Luxembourg Agreement".

Britain gained full membership of the EEC on 1 January 1973, and all trade agreements with New Zealand came to an end, except the Luxembourg Agreement. By the end of that year, only 26.8% of New Zealand's exports were to Britain. This had a significant effect on the standard of living. In 1953, New Zealand had the third highest standard in the world. By 1978, it had dropped to 22nd place.

Having lost unrestricted access to its traditional market, New Zealand continued to search for alternative export markets and diversify its economy. The Government of Norman Kirk, who succeeded Marshall, put greater emphasis on expanding New Zealand's trade, especially with South East Asia. Following the Yom Kippur War in October 1973, an oil embargo was put in place by the Middle Eastern oil exporters, leading to the 1973 oil crisis. This compounded New Zealand's dire economic situation further. Inflation greatly increased as the cost of transport and imported goods soared, causing standards of living to decline.

Following the 1979 energy crisis resulting from the Iranian Revolution of that year, Robert Muldoon, the Prime Minister between 1975 and 1984, instituted an economic strategy known as Think Big. Large scale industrial plants were established based on New Zealand's abundant natural gas. A new range of products for export such as ammonia, urea fertilizer, methanol and petrol were produced and with greater use of electricity (with the electrification of the North Island Main Trunk railway) with the goal that this would reduce New Zealand's dependence on oil imports.

Other projects included the Clyde Dam on the Clutha River, which was built to meet a growing demand for electricity, and the expansion of the New Zealand Steel plant at Glenbrook.

The Tiwai Point Aluminium Smelter, which opened in 1971, was also upgraded as part of the Think Big strategy and now brings in approximately NZ$1 billion in exports every year.

Unfortunately for New Zealand, most of these projects only came on line at the same time as oil prices dropped during the 1980s oil glut. The price of crude went from more than US$90 a barrel in 1980, to about US$30 a few years later. Because these Think Big projects required massive borrowing to get started, public debt soared from $4.2 billion in 1975 when Muldoon became Prime Minister to $21.9 billion when he left office nine years later. Inflation remained rampant, averaging 11% in the 1980s. Once Labour came to power in 1984, many of these projects were sold to private industry as part of a wider sale of state-assets.

The Muldoon Government did make some moves towards deregulation however. For example, in 1982 it removed the transport licensing restrictions on road carriers carting goods more than 150 km, and turned the Railways Department into a statutory corporation.

The Fourth Labour government, elected in July 1984, moved away from government intervention in the economy and allowed free market mechanisms to dominate. These reforms became known as "Rogernomics", named after Minister of Finance from 1984-1988, Roger Douglas. The changes included making the Reserve Bank independent of political decisions; performance contracts for senior civil servants; public sector finance reform based on accrual accounting; tax neutrality; subsidy-free agriculture; and industry-neutral competition regulation. Government subsidies including agricultural subsidies were eliminated; import regulations were loosened up; the exchange rate was floated; and controls on interest rates, wages, and prices were removed; and personal rates of taxation were reduced. Tight monetary policy and major efforts to reduce the government budget deficit brought the inflation rate down from an annual rate of more than 18% in 1987. The deregulation of government-owned enterprises in the 1980s and 1990s reduced government's role in the economy and permitted the retirement of some public debt.

The new Government was faced with an exchange rate crisis the day after it was elected. Speculators expected the change of government to result in a 20% devaluation of the New Zealand dollar, which led to the 1984 New Zealand constitutional crisis due to Muldoon's refusal to devalue, worsening the currency crisis further. As a result, the dollar was floated on 4 March 1985, allowing for the value of the dollar to change with the market. Prior to the dollar being floated, the dollar was pegged against a basket of currencies.

Financial markets were deregulated and tariffs on imported goods lowered and phased out. At the same time subsidies to many industries, notably agriculture, were removed or significantly reduced. Income and company taxes were reduced and the top marginal tax rate was reduced from 66% to 33%. These were replaced by a comprehensive tax on goods and services (GST) initially set at 10%, then increased to 12.5% and recently increased to 15% in 2011. A surtax on universal superannuation was also introduced. Many government departments were corporatised, and from 1 April 1987 became State owned enterprises, required to make a profit. The new corporations shed thousands of jobs adding to unemployment; Electricity Corporation 3,000; Coal Corporation 4,000; Forestry Corporation 5,000; New Zealand Post 8,000.

The wage and price freeze of the early eighties coupled with the removal of financial restrictions and a lack of investment opportunities, led to a speculative bubble on New Zealand's sharemarket, sharemarket crash of 1987, in which New Zealand's sharemarket shed 60% from its 1987 peak, and taking several years to recover.

Inflation continued to be a major problem afflicting the New Zealand economy. Between 1985 and 1992, inflation averaged 9% per year and the economy was in recession. The unemployment rate rose from 3.6% to 11%, New Zealand's credit rating dropped twice, and foreign debt quadrupled. In 1989 the Reserve Bank Act 1989 was passed, creating strict monetary policy under the sole control of the Reserve Bank Governor. From then on the Reserve Bank focused on keeping inflation low and stable, using the Official Cash Rate (OCR) – the price of borrowing money in New Zealand – as its primary means to do so. As a result, inflation rates fell to an average of 2.5% in the 1990s, compared to 12% in the 1970s. However, the tightening of monetary policy contributed to rising unemployment in the early 1990s.

The Labour Party was greatly divided over Rogernomics, especially following the 1987 sharemarket crash and its effect on the economy, which slumped along with the rest of the world into recession in the early 1990s. The National Party was returned to power at the 1990 general election and Ruth Richardson became Minister of Finance under Prime Minister Jim Bolger. The new Government was again thrown a major economic challenge, with the then state-owned Bank of New Zealand needing a bail-out to stay operational.

Richardson's first budget in 1991, nicknamed the 'Mother of all Budgets', attempted to address constant fiscal deficits and borrowing by cutting state spending. Unemployment and social welfare benefits were cut and 'market rents' were introduced for state houses – in some cases tripling the rents of low-income people. Richardson also introduced user-pays requirements in hospitals and schools. These reforms became known derisively as Ruthanasia.

By this time, New Zealand's economy faced serious social problems; the number of New Zealanders estimated to be living in poverty grew by at least 35% between 1989 and 1992; many of the promised economic benefits of the experiment never materialised. Gross domestic product per capita stagnated between 1986–87 and 1993–94, and by March 1992 unemployment rose to 11.1% Between 1985 and 1992, New Zealand's economy grew by 4.7% during the same period in which the average OECD nation grew by 28.2%. From 1984 to 1993 inflation averaged 9% per year, New Zealand's credit rating dropped twice, and foreign debt quadrupled. Between 1986 and 1993, the unemployment rate rose from 3.6% to 11%.
Deregulation also created a business-friendly regulatory framework which has benefited those able to take advantage of it. A 2008 survey in The Heritage Foundation and "Wall Street Journal" ranked New Zealand 99.9% in "Business freedom", and 80% overall in "Economic freedom", noting that it takes, on average, only 12 days to establish a business in New Zealand, compared with a worldwide average of 43 days.

Deregulation has also been blamed for other significant negative effects. One of these was the leaky homes crisis, whereby the loosening up of building standards (in the expectation that market forces would assure quality) led to many thousands of severely deficient buildings, mostly residential homes and apartments, being constructed over a period of a decade. The costs of fixing the damage has been estimated at over NZ$11 billion ().

Unemployment continued to fall from 1993–94 fiscal year, until the onset of the 1997 Asian financial crisis again pushed the rate higher. By 2016 the unemployment rate decreased to 5.3 percent, the lowest level in 7 years.

Between 2000 and 2007, the New Zealand economy expanded by an average of 3.5% a year driven primarily by private consumption and the buoyant housing market. During this period, inflation averaged only 2.6% a year, within the Reserve Bank's target range of 1% to 3%. However, in early 2008 the economy entered recession, before the effects of the global financial crisis (GFC) set in later that year. A drought over the 2007/08 summer led to lower production of dairy products in the first half of 2008. Domestic activity slowed sharply over 2008 as high fuel and food prices dampened domestic consumption, while high interest rates and falling house prices drove a rapid decline in residential investment.

Around the world instability was developing in the finance sector. This reached a peak in September 2008 when Lehman Brothers, a major American bank, collapsed propelling the world into the global financial crisis.

Uncertainty began to dominate the global financial and economic environment. Business and consumer confidence in New Zealand plummeted as dozens of finance companies collapsed. To try and stop a flight of funds from New Zealand institutions to those in Australia, the Government established the Crown Retail Deposit Guarantee Scheme to cover depositors funds in the event that a bank or finance company went broke. This protected some investors but nevertheless, at least 67 finance companies collapsed within a short period of time. The largest of these was South Canterbury Finance which cost taxpayers NZ$1.58 billion when the company collapsed in August 2010. The directors of many of these finance companies were subsequently investigated for fraud and some high-profile directors went to prison.

In an attempt to stimulate the economy, the Reserve Bank lowered the Official Cash Rate (OCR) from a high of 8.25% (July 2008) to an all-time low of 2.5% at the end of April 2009.

Fortunately for New Zealand, the recession was relatively shallow compared to many other nations in the OECD, it was sixth least affected out of the 34 member
nations with negative real GDP growth totaling 3.5%. In 2009 the economy picked up, led by strong demand from major trading partners Australia and China, and historically high prices for New Zealand's dairy and log exports. In 2010 the GDP grew by a modest 1.6%, but over the next couple of years economic activity continued to improve, driven by the rebuild in Canterbury after the Christchurch earthquakes and recovery in domestic demand. Through 2011, global conditions deteriorated and the terms of trade eased off their 2011 peak, continuing to moderate until September 2012. Since then, commodity prices have rebounded strongly, with strong demand from China and the international situation improving. Commodity prices have been at record highs in recent quarters and remain elevated. High commodity prices are expected to provide a considerable boost to nominal GDP growth in the near term.

In 2013 the economy grew 3.3%. HSBC chief economist for Australia and New Zealand, Paul Bloxham, was so impressed that he predicted New Zealand's growth would outpace most of its peers, and he described New Zealand as the "rock star economy of 2014". Another financial commentator said the New Zealand dollar was the "hottest" currency of 2014. Only three months later, The New Zealand Productivity Commission expressed concern about low living standards and problems affecting the long-term drivers of growth. Paul Conway, Director of Economics & Research at the Productivity Commission, wrote: "New Zealand's broad policy settings should generate GDP per capita 20 per cent above the OECD average, but the actual result is more than 20 per cent below average. We may be punching above our weight, but that’s only because we are in the wrong weight division!" In August, Paul Bloxham, who coined the term "rock star economy", admitted that "the sharp decline in dairy prices over the last six months has clouded the outlook somewhat". In December however Bloxham stated that he thought the New Zealand economy would continue to grow strongly.

In 2014 increased attention was paid to the growing gap between rich and poor. In The Guardian, Max Rashbrook said policies implemented by both Labour and National governments have increased inequality. He claims that for 20 years outrage "has been muted", but "Alarm bells are finally beginning to sound. Recent polling shows three-quarters of New Zealanders think theirs is no longer an egalitarian country".

New Zealand's small size and long distances from major world markets creates significant challenges in its ability to compete in global markets. Australia, New Zealand's closest neighbour, is New Zealand's biggest trading partner. In 2013 New Zealand's main trading partners were Australia, China, the United States, Japan, Singapore and the United Kingdom. In March 2014, the total value of goods exported from New Zealand topped $50 billion for the first time, up from $30 billion in 2001. New Zealand Trade and Enterprise (NZTE) offers strategic advice and support to New Zealand businesses wanting to export goods and services to other countries.

Since 1960s New Zealand has pursued free trade agreements with many countries to diversify its export markets and increase the competitiveness of New Zealand's exports to the world. As well as reducing barriers to trade, Trade Agreements New Zealand has entered into are designed to ensure existing access is maintained. Trade agreements establish rules by which trade can take place and ensure regulators and officials in countries New Zealand is trading with work closely together.

Australia is New Zealand's largest bilateral trading partner. In 2013, trade between New Zealand and Australia was worth NZ$25.6 billion. Economic and trading links between Australia and New Zealand are underpinned by the "Closer Economic Relations" (CER) agreement, which allows for free trade in goods and most services. Since 1990, CER has created a single market of more than 25 million people. Australia is now the destination of 19% of New Zealand's exports, including light crude oil, gold, wine, cheese and timber, as well as a wide range of manufactured items.

The CER also creates a free labour market which allows New Zealand and Australian citizens to live and work freely in each other's country together with mutual recognition of professional qualifications. This means individuals who are registered to practise an occupation in one country can register to practise an equivalent occupation in the other country. Banking regulation and supervision are co-ordinated through the Trans-Tasman Council on Banking Supervision and there are also ongoing discussions about co-ordinating Australian and New Zealand business law.

China is New Zealand's second largest trading partner buying primarily meat, dairy products and pine logs. In 2013, trade between New Zealand and China was worth NZ$16.8 billion. This has occurred primarily because of soaring demand for imported dairy products, following the Chinese milk scandal in 2008. Since then demand for milk products has been so strong that in the 12 months to March 2014, there was a 51% increase in total exports to China. The increase was facilitated by the New Zealand–China Free Trade Agreement which came into force on 1 October 2008. Since that year exports to China have more than tripled.

The United States is New Zealand's third largest trading partner. In 2013, bilateral trade between the two countries was valued at NZ$11.8 billion. New Zealand's main exports to the United States are beef, dairy products and lamb. Imports from the US include specialised machinery, pharmaceutical products, oil and fuel. In addition to trade, there is a high level of corporate and individual investment between the two countries and the US is a major source of tourists coming to New Zealand. In March 2012, the United States had a total of $44 billion invested in New Zealand. A number of US companies have subsidiary branches in New Zealand. Many operate through local agents, with some joint venture associations. The United States Chamber of Commerce is active in New Zealand, with a main office in Auckland and a branch committee in Wellington.

According to the Ministry of Foreign Affairs, New Zealand and the United States "share a deep and longstanding friendship based on a common heritage, shared values and interests, and a commitment to promoting a free, democratic, secure and prosperous world". This common background has not translated into a free trade agreement between the two countries.

A growing number of New Zealand companies use the United Kingdom as a base to supply their products to the European market. However trade with the European Union is declining as demand from Asia continues to grow. The EU currently takes only 8% of New Zealand exports but provides around 12% of imports.

In July 2014, negotiations on the Partnership Agreement on Relations and Cooperation (PARC) between New Zealand and the European Union were concluded. The Agreement covers the trade and economic relationship between the EU and New Zealand with a view to further liberalisation of trade and investment and acknowledges the intention of the European Union to upgrade its diplomatic presence in New Zealand with a resident Ambassador.

In the 21st century, Asian economies have been developing rapidly providing significant demand for New Zealand's exports. New Zealand trades with Taiwan, Hong Kong, Malaysia, Indonesia, Singapore, Thailand, India and the Philippines and this now accounts for around 16% of total exports. New Zealand initiated a free trade agreement with Singapore in September 2000 which was extended in 2005 to include Chile and Brunei and is now known as the P4 agreement.

The Pacific region with numerous islands is New Zealand's sixth largest trading market and is growing every year. In 2011 exports to Pacific Islands were worth over $1.5 billion up 12% on the previous year. Fiji is the biggest individual market followed very closely by Papua New Guinea, French Polynesia and New Caledonia. Goods exported to the islands include refined oil, construction materials, medicines, sheep meat, milk, butter, fruit and vegetables. New Zealand also assists Pacific Islands with defence and regional security, and with management of the environment and fisheries.

Because of their small size, the Pacific Islands are some of the most vulnerable environments in the world and are on the receiving end of numerous cyclones every year. When disasters occur, they often have severe social and economic effects which last for years. Since 1992, New Zealand has co-operated with Australia and France to respond to disasters in the Pacific. New Zealand provides emergency supplies and transport, funding for roading and housing and the deployment of specialists to affected areas.

Through the Ministry of Foreign Affairs and Trade, New Zealand also provides international aid and development funding to help stimulate sustainable economic development in underdeveloped economies. The New Zealand Aid Programme, allocated about $550m a year, is focused primarily on promoting development in the Pacific. The allocation of $550 million represents about 0.26% of New Zealand's gross national income (GNI).

New Zealand welcomes and encourages foreign investment, which is overseen by the Overseas Investment Office. In 2014 foreign direct investment totaled NZ$107.69 billion. From 1989 foreign investment increased from $9.7 billion to $101.4 billion in 2013 – an increase of over 1,000%. Between 1989 and 2007, foreign ownership of the New Zealand sharemarket went from 19% to 41% but has since dropped back to 33%.

In 2007, around 7% of all New Zealand agriculturally productive land was foreign-owned. In 2011, economist Bill Rosenberg said that the figure is closer to 9% if foreign ownership of forestry is included. In March 2013 the financial sector, which includes the four big Australian owned banks, was worth $39.3 billion accounting for the largest portion of the $101.4 billion foreign ownership of New Zealand companies.

Between 1997 and 2007, foreign investors made $50.3 billion profit, 68% of which went overseas. The Campaign Against Foreign Control of Aotearoa (CAFCA) says this has a negative impact on the economy arguing that when foreign investors buy up New Zealand companies, they tend to cut staff and push down wages. Foreign ownership has also done nothing to improve New Zealand's foreign debt. In 1984, private and public foreign debt was $16 billion ($50 billion in March 2013 dollars) which was less than half New Zealand's Gross Domestic Product at the time. By March 2013, total foreign debt stood at $251 billion, well over 100% of New Zealand's Gross Domestic Product.

Industrial production growth rate: 5.9% (2004) / 1.5% (2007)

Household income or consumption by percentage share:

Agriculture – products: wheat, barley, potatoes, pulses, fruits, vegetables; wool, beef, dairy products; fish

Exports – commodities: dairy products, meat, wood and wood products, fish, machinery

Imports – commodities: machinery and equipment, vehicles and aircraft, petroleum, electronics, textiles, plastics

Electricity:

Electricity – production by source:

Oil:

Exchange rates:<br>
"New Zealand dollars (NZ$) per US$1" – 1.4771 (2016), 1.2652 (2012), 1.3869 (2005), 1.5248 (2004), 1.9071 (2003), 2.1622 (2002), 2.3788 (2001), 2.2012 (2000), 1.8886 (1999), 1.8632 (1998), 1.5083 (1997), 1.4543 (1996), 1.5235 (1995)





</doc>
<doc id="21357" url="https://en.wikipedia.org/wiki?curid=21357" title="Telecommunications in New Zealand">
Telecommunications in New Zealand

Telecommunications in New Zealand are fairly typical for an industrialised country.

Fixed-line broadband and telephone services are largely provided through copper-based networks, although fibre-based services are increasingly common. Spark New Zealand, Vodafone New Zealand, 2degrees and the Callplus group provide most services.

Mobile telephone services are provided by Spark, Vodafone and 2degrees, although a number of smaller mobile virtual network operators also exist.





The government charges a $50 million Telecommunications Development Levy annually to fund improvements to communications infrastructure such as the Rural Broadband Initiative. It is payable by telecommunications firms with an operating revenue of over $10 million, in proportion to their qualified revenue.



</doc>
<doc id="21358" url="https://en.wikipedia.org/wiki?curid=21358" title="Transport in New Zealand">
Transport in New Zealand

Transport in New Zealand, with its mountainous topography and a relatively small population mostly located near its long coastline, has always faced many challenges. Before Europeans arrived, Māori either walked or used watercraft on rivers or along the coasts. Later on, European shipping and railways revolutionised the way of transporting goods and people, before being themselves overtaken by road and air, which are nowadays the dominant forms of transport. However, bulk freight still continues to be transported by coastal shipping and by rail transport, and there are attempts to (re)introduce public transport as a major transport mode in the larger population centres.

Historically very car-dependent, as of 2010, transport funding in New Zealand is still heavily dominated by money for road projects–the National-led government proposes to spend $21 billion on roading infrastructure after 2012, yet only $0.7 billion on other transport projects (public transport, walking and cycling). This has been criticised by opponents of the current government strategy as irresponsible, in light of increasing fuel prices and congestion. Public transport is primarily a local government responsibility whereas state highways are the responsibility of central government.

The state highway network is the principal road infrastructure connecting New Zealand urban centres. It is administered by the NZ Transport Agency. The majority of smaller or urban roads are managed by city or district councils, although some fall under the control of other authorities, such as the New Zealand Department of Conservation or port and airport authorities.

New Zealand has left-hand traffic on its roads.

Before Europeans arrived, Māori either walked or used watercraft on rivers or along the coasts. The road network of New Zealand has its origins in these tracks and paths used by Māori and later by Europeans in their early travels through New Zealand. Several major Māori tracks were known, such as the western coastal track was used along the whole length of the North Island, and the track on the East Coast, which left the coast near Castlepoint and rejoined it near Napier. In the South Island, another major track existed down the east coast with tributary tracks following streams up to the mountain passes to the West Coast. Mountains, swamp, and dense bush made inland routes tricky to traverse, and early settlers also made use of beaches as roads, for walking, riding horses, and herding sheep. Many farms had access via beaches only, and beaches were used as runways for planes. Some beaches are still used by planes, for example at Okarito and on the west coast of Stewart Island.

Initial roads, such as the Great South Road southwards from Auckland, were often built by the British Army to move troops, and were constructed to a comparatively high standard. Early sheep farming required few high-standard roads, but the strong increase in dairy farming in the late 19th century created a strong demand for better links on which the more perishable goods could be transported to market or towards ports for export. In many cases, later roads for motor vehicles follow paths used by bullock carts which followed tracks made for humans. These in turn in some cases became highways – with attendant problems all over New Zealand (but especially in the more mountainous regions), as the geography and contours of a slow-speed road laid out in the first half of the 20th century usually do not conform to safety and comfort criteria of modern motor vehicles.

Early road construction was both hindered and helped by rail transport during the first half century of European settlement. Authorities were reluctant to expend large amounts of capital on more difficult sections of a route where there was a hope that a railway might instead be built. However, where railways were constructed, roads often either preceded them for construction or quickly followed it when the newly accessible land started to be settled more closely.

The New Zealand highway system was extended massively after World War II. The first motorway was built in the environs of Wellington and opened in 1950, between Takapu Road and Johnsonville. Following heavy investment in road construction from the 1950s onwards, public transport patronage fell nationwide. This has been described, in Auckland's case, as "one of the most spectacular declines in public transport patronage of any developed city in the world".

New Zealand has a state highway network of ( in the North Island and in the South Island, as of August 2006) of which are motorways. These link to of local authority roads, both paved and unpaved. The state highways carry 50% of all New Zealand road traffic, with the motorways alone carrying 9% of all traffic (even though they represent only 3% of the whole state highway network, and even less of the whole road network).

The default maximum speed limit on the open road is for cars and motorcycles, with the default limit in urban areas. Around of motorway and expressway in Waikato and the Bay of Plenty have a higher posted speed limit of . Speed limits of are also used in increments of , and the posted speed limit may be more than the allowed speed limit for a particular vehicle type. Speeds are often reduced to beside roadworks.

Private landowners may set their own speed limits, for example , although these are not enforced by police of road authorities.

The Land Transport Rule: Setting of Speed Limits (2003) allows road controlling authorities to set enforceable speed limits, including permanent speed limits, of less than 50 km/h on roads within their jurisdiction. However, there are still very few areas of the country's road network with permanent speed limits below 50 km/h.

Total road deaths in New Zealand are high by developed country standards. 2010 figures from the International Transport Forum placed New Zealand 25th out of 33 surveyed countries in terms of road deaths per capita, a rank that has changed little in 30 years. The fatality rate per capita is twice the level of Germany's, or that of the United Kingdom, Sweden or the Netherlands (2010 comparison). This is variously blamed on aggressive driving, insufficient driver training, old and unsafe cars, inferior road design and construction, and a lack of appreciation of the skill and responsibility required to safely operate a motor vehicle.

In 2010, 375 'road users' were killed in New Zealand, while 14,031 were injured, with 15- to 24-year-olds the group at highest risk. The three most common vehicle movements resulting in death or injury were "head-on collisions (while not overtaking)", "loss of control (on straight)" and "loss of control (while cornering)". In terms of deaths per 10,000 population, the most dangerous areas were the Waitomo District (121 deaths) and the Mackenzie District (110). Larger cities were comparatively safe, with Auckland City (28), Wellington (22) and Christchurch (28), while Dunedin had a higher rate of 43.

New Zealand has a large number of overseas drivers (tourists, business, students and new immigrants), as well as renting campervans/motorhomes/RV's during the New Zealand summer. Overseas licensed drivers are significantly more likely to be found at fault in a collision in which they are involved (66.9%), compared to fully licensed New Zealand drivers (51.9%), and only slightly less likely to be found at fault than restricted (novice) New Zealand drivers (68.9%).

Drunk driving is a major issue in New Zealand, especially among young drivers. New Zealand has relatively low penalties for drunk driving. In the late 2000s, reports indicated that the rate of drunk driving by under 20s in Auckland had risen 77% in three years, with similar increases in the rest of the country. Many drunk drivers already had convictions for previous drunk driving.

The road toll has decreased over the 5 years from 421 in 2007 to 284 in 2011

In the 'Safer Journeys' Strategy, intended to guide road safety developments between 2010 and 2020, the Ministry of Transport aims for a 'safe systems' approach, prioritised four areas, being "Increasing the safety of young drivers", "Reducing alcohol/drug impaired driving", "Safe roads and roadsides" and "Increasing the safety of motorcycling".

Historically, most roads in New Zealand were funded by local road authorities (often road boards) who derived their income from local rates. As the need for new roads was often most urgent in those parts of the country where little rate income could yet be collected, the funding was at least partly dependent on national-level subsidies, for which much lobbying was undertaken. Many acts and ordinances were passed in the first decades of the colony, but lack of funds and parochialism (the desire to spend locally raised money locally, rather than use it to link different provinces) hindered the growth of the road network. This lack of larger-scale planning eventually led to increased public works powers given to the Central Government.

Today, all funding for state highways and around 50% of funding for local roads comes directly from road users through the National Land Transport Fund. Road user revenue directed to the fund includes all fuel excise duty on LPG and CNG, around 55% of revenue from fuel excise duty on petrol, all revenue from road user charges (a prepaid distance/weight licence that all vehicles over 3.5 tonnes, and all non petrol/LPG/CNG vehicles are liable to pay) and most non-ACC revenue from motor vehicle registration and licensing fees. In addition, in the last three years the government has increasingly allocated additional funds to land transport, to the extent that today the total expenditure by the NZ Transport Agency on land transport projects exceeds road tax revenue collected. The remainder of funding for local city and district roads primarily comes from local authority property rates.

As of 2010, transport funding in New Zealand is still heavily biased towards road projects – the National government proposes to spend $21 billion on roading infrastructure after 2012, yet only $0.7 billion on other transport projects (public transport, walking and cycling). This has been criticised by opponents of the current government strategy as irresponsible, in light of increasing fuel prices and congestion. Government has claimed that their priority on roads is in line with New Zealanders' favoured travel modes, and as being the most promising in terms of economic benefits.

One of the earliest counts/estimates of motor vehicles in New Zealand had them at 82,000 in 1925. This soon increased to 170,000 on the eve of World War II in 1939, continuing to 425,000 in 1953 and increasing to 1,000,000 in 1971. Today, the New Zealand vehicle fleet (as of December 2015) counts 3.83 million vehicles. Of the fleet, 3.018 million were light passenger vehicles, 507,000 were light commercial vehicles, 137,000 were heavy trucks, 10,000 were buses and 160,000 were motorcycles and mopeds. The mean age of a New Zealand car (as of end of 2015) was 14.2 years, with trucks at 17.6 years.

Just over half of the light passenger vehicles first registered in New Zealand are used imports. In 2013 new car registrations were up 7% on 2012 to 82,235 sold, with used vehicle sales up to 98,971.

At the 2013 New Zealand census, 92.1 percent of households reported owning at least one car; 37.6 percent reported owning one car, 38.4 percent reported as owning two cars, and 16.1 percent reported owing three or more cars. Car ownership was highest in the Tasman Region (95.9 percent) and lowest in the Wellington Region (88.3 percent).

Transport by bus services form the main component of public transport services in New Zealand cities, and the country also has a network of long-distance bus or coach services, augmented by door-to-door inter-city "shuttle vans", a type of shared taxi.

The first widespread motor vehicle services were shared taxi services termed "service cars"; a significant early provider was Aard, operating elongated Hudson Super-Six Coaches. By 1920 AARD covered most of the North Island and even provided transport for the Prince of Wales. By 1924 the services covered even more areas. Aard was taken over by New Zealand Railways Road Services in 1928. The road fleet of New Zealand Railways Corporation was privatised in 1991 with the long-distance business still existing as InterCity, having more recently incorporated "Newmans Coachlines". Another former extensive coach business was Mount Cook Landlines, which closed in the 1990s. Internet-based nakedbus.com is building another nationwide network, partly as a reseller of several smaller bus operators' capacity.

Intercity and Tourism Holdings Ltd are significant sightseeing / tourism coach operators.

While relatively popular for sport and recreation, bicycle use is a very marginal commuting mode, with the percentage share hovering around 1% in many major cities, and around 2% nationwide (2000s figures). This is primarily due to safety fears. For instance Auckland Regional Transport Authority reports that “over half of Aucklanders believe it is usually unsafe, or always unsafe, to cycle”.

The high risk to bicycle users is due to a number of factors. Motorists tend to exhibit hostile attitudes towards bicycle riders. Bicycles are classed as 'vehicles', a transport class legally obliged to use the road, forcing bicycle users to mingle with heavy and fast-moving motor vehicles; only postal workers are legally permitted to ride on footpaths. Bicycle infrastructure and the standards underpinning bicycle infrastructure planning are poor and bicycles receive relatively very low levels of funding by both central and local government. It has also been argued that the introduction of New Zealand's compulsory bicycle helmet law contributed to the decline in the popularity of cycling.

There is a total of 3,898 km of railway line in New Zealand, built to the narrow gauge of . Of this, 506 km is electrified. The national network is owned by state-owned enterprise New Zealand Railways Corporation division KiwiRail. The national network consists of three main trunk lines, seven secondary main lines and during its peak in the 1950s, around ninety branch lines. The majority of the latter are now closed. Most lines were constructed by government but a few were of private origin, later nationalised. In 1931, the Transport Licensing Act was passed, protecting the railways from competition for fifty years. The transport industry became fully deregulated in 1983.

Between 1982 and 1993 the rail industry underwent a major overhaul involving corporatisation, restructuring, downsizing, line and station closures and privatisation. In 1993 the network was privatised, and until 2003 the national network was owned by Tranz Rail, previously New Zealand Rail Limited. The Government agreed to take over control of the national rail network back when Toll Holdings purchased Tranz Rail in 2003. In May 2008 the Government agreed to buy Toll NZ's rail and ferry operations for $665 million, and renamed the operating company KiwiRail.

Bulk freights dominate services, particularly coal, logs and wood products, milk and milk products, fertiliser, containers, steel and cars. Long distance passenger services are limited to three routes – the TranzAlpine (Christchurch – Greymouth), the TranzCoastal (Christchurch – Picton) and the Northern Explorer (Wellington – Auckland). Urban rail services operate in Wellington and Auckland, and interurban services run between Palmerston North and Wellington (the Capital Connection) and Masterton and Wellington (the Wairarapa Connection).

For most of its history, New Zealand's rail services were operated by the Railways Department. In 1982, the Department was corporatised as the New Zealand Railways Corporation. The Corporation was split in 1990 between a limited liability operating company, New Zealand Rail Limited, and the Corporation which retained a number of assets to be disposed. New Zealand Rail was privatised in 1993, and renamed Tranz Rail in 1995. In 2001, Tranz Rail's long-distance passenger operations, under the guise of Tranz Scenic, became a separate company; Tranz Rail chose not to bid for the contract to run Auckland's rail services, and the contract was won by Connex (now Transdev Auckland). Proposals to sell Tranz Rail's Wellington passenger rail services, Tranz Metro, did not come to fruition, although the division became a separate company in July 2003. In 2003 Tranz Rail was purchased by Australian freight firm Toll Holdings, which renamed the company Toll NZ.

The only other significant non-heritage operator is the tourist oriented Dunedin Railways in Otago, which runs regular passenger trains on part of the former Otago Central Railway and some on the Main South Line.

The Federation of Rail Organisations of New Zealand coordinates the work of approximately sixty heritage railways and rail museums. Most of these are operated by groups of volunteers and have a historical or tourist focus.

New Zealand has a long history of international and coastal shipping. Both Maori and the New Zealand European settlers arrived from overseas, and during the early European settler years, coastal shipping was one of the main methods of transportation, while it was hard to move goods to or from the hinterlands, thus limiting the locations of early settlement.

The two main islands are separated by Cook Strait, 24 km wide at its narrowest point, but requiring a 70-km ferry trip to cross. This is the only large-scale long-distance car / passenger shipping service left, with all others restricted to short ferry routes to islands like Stewart Island/Rakiura or Great Barrier Island.

New Zealand has 1,609 km of navigable inland waterways; however these are no longer significant transport routes.

Historically, international shipping to and from New Zealand started out with the first explorer-traders, with New Zealand waters soon becoming a favourite goal for whalers as well as merchants trading with the Maori and beginning European colonies.

In the 19th century, one of the most important changes for New Zealand shipping – and for New Zealand itself – came with the introduction of refrigerated ships, which allowed New Zealand to export meat to overseas, primarily to the United Kingdom. This led to a booming agricultural industry which was suddenly offered a way to ship their goods to markets around the world.

Larger, deeper-draught ships from the middle of the 19th century made dredges a common sight in shipping channels around New Zealand, and tugboats were also often bought to assist them to the quays, where electric or hydraulic cranes were increasingly used for on- and off-loading. However, manpower was still needed in large amounts, and waterfronts were the hotbeds of the industrial actions of the early 20th century.

In the 1970s, containerisation revolutionised shipping, eventually coming to New Zealand as well. The local harbour boards wrought massive changes on those ports selected (after much political wrangling) to handle the new giant vessels, such as Lyttelton and Auckland Port. Gantry cranes, straddle carriers and powerful tugboats were built or purchased, and shipping channels dredged deeper, while large areas of land were reclaimed to enable the new container terminals. The changes have been described as having been more radical than the changeover from sail to steam a century before.

However, containerisation made many of the smaller ports suffer, this being only later recovered somewhat with newer, smaller multi-purpose ships that could travel to smaller ports, and the loosening of the trade links with the United Kingdom, which diversified the trade routes. The time for river ports had gone however, and most of them disappeared, facing particular pressure from the new rail ferries, In the 1980s, deregulation also involved and heavily changed the port industry, with harbour boards abolished, and replaced by more commercially focused companies. Many port jobs were lost, though shipping costs fell.

As noted above, coastal shipping has long played a significant role in New Zealand. It was very efficient for moving large amounts of goods, and relatively quick. In 1910, it was noted in a discussion with the Minister of Railways that a fruit grower at Port Albert (near Wellsford, less than 150 km from Auckland) had found it cheaper to ship his canned fruit to Lyttleton in the South Island by boat, and thence back to Auckland again, rather than pay rail freight rates from nearby Wellsford to Auckland.

The industry however also faced a number of troubled times as well, such as during World War II when ship requisitioning caused shortages in the transport operation. While many ports reopened after the war, they (and coastal shipping in general) faced huge pressure from rail (presumably now offering improved freight rates compared to the 1910 era).

After cabotage was abolished in 1994, international shipping lines became able to undertake coastal shipping as opportune to them on their international routes to New Zealand. While reducing the cargo reshipment rates for New Zealand industry, this is seen by some as a heavy blow for local competitors, who, specialised in coastal shipping only, are less able to achieve the costs savings of large lines – these can generally operate profitably even without cargo on New Zealand-internal legs of their routes, and are thus able to underbid others. The law change has been accused of having turned the New Zealand business into a 'sunset industry' which will eventually die out.

In the financial year 2003 / 2004 coastal cargo in New Zealand totalled around 8.6 million tonnes, of which 85% was still carried by local, and 15% by overseas shipping.

In 2009, the National Party announced that funding for coastal shipping and supporting infrastructure, part of the "Sea Change" plan of the previous Labour government, would be cut to a substantial degree. The move was heavily criticised, amongst others, by the Green Party, and the Maritime Union of New Zealand.

Regular roll-on/roll-off ferry services have crossed Cook Strait, linking the North and South Islands between Wellington and Picton, since 1962. Services are provided five ferries operated by two companies: Interislander (a division of KiwiRail), and Bluebridge (Strait Shipping). One ferry used by the Interislander, , is a rail ferry capable of transporting both road and rail on separate decks. The four remaining ferries carry passengers and road vehicles only: Interislander's and , and Bluebridge's and .

Depending on the vessel, usual transit time between the North and South Islands is 3 to 3.5 hours. Faster catamaran ferries were used by Tranz Rail and its competitors between 1994 and 2004. To reduce voyage times, Tranz Rail proposed to relocate the South Island terminal of its services to Clifford Bay in Marlborough, which would also avoid a steep section of railway. This proposal has been shelved since the takeover by Toll Holdings in 2003.

Smaller ferries operate in the Bay of Islands, Rawene (Northland), Auckland, Tauranga, Wellington, the Marlborough Sounds and Lyttelton (Christchurch), and between Bluff and Halfmoon Bay (Stewart Island).

A passenger ferry service also operated for many years between Wellington and Lyttelton (the port closest to Christchurch). This service was operated by the Union Steam Ship Company, and the passenger ferries typically operated an overnight service, although in later years the last of these vessels, the "Rangatira", operated alternate nights in each direction plus a daylight sailing from Lyttelton to Wellington on Saturdays (so as to get a balance of four sailings in each direction, each week). One of these passenger ferries, the "Wahine", was lost in a storm as it entered Wellington Harbour on 10 April 1968, with the loss of 51 passengers and crew. The final sailing of the Rangatira, which was custom built and entered service in 1972, was on 15 September 1976, after two money-losing years (subsidised by the government).



New Zealand's air travel sector is served by 15 airlines. The largest airline is Air New Zealand, a state-owned flag carrier.

There are 123 airports (and aerodromes) in New Zealand. Six airports provide international air services (besides domestic services); Auckland and Christchurch (the largest airports in the North and South Islands, respectively) providing long-haul and short-haul international air services; and Wellington, Hamilton, Dunedin and Queenstown providing short-haul international air services to Australia and Fiji. There were around 30 other airports provide scheduled domestic air services. The busiest airport is Auckland, which handled 16,487,648 (9,005,612 international and 7,482,036 domestic) passengers in the year ended December 2015.

"total:" 39 ()
<br>over : 2
<br> to : 1
<br> to : 12
<br> to : 23
<br>under : 1

"total:" 84 ()
<br> to : 3
<br> to : 33
<br>under : 48

New Zealand has 55 functioning heliports (and helipads), including 36 hospital heliports.





</doc>
<doc id="21359" url="https://en.wikipedia.org/wiki?curid=21359" title="New Zealand Defence Force">
New Zealand Defence Force

The New Zealand Defence Force (Maori: "Te Ope Kaatua o Aotearoa", "Line of Defence of New Zealand") consists of three services: the New Zealand Army, Royal New Zealand Air Force and Royal New Zealand Navy; and is commanded and headed by the Chief of Defence Force (CDF)

Air Marshal Kevin Short took over as Chief of Defence Force on 1 July 2018. The NZDF also announced that Air Vice-Marshal Tony Davies will be the next Vice Chief of Defence Force. AVM Davies will step into the role in September 2018.

New Zealand's armed forces have three defence policy objectives: to defend New Zealand against low-level threats; to contribute to regional security; and to play a part in global security efforts. New Zealand considers its own national defence needs to be modest, due to its geographical isolation and benign relationships with neighbours. As of December 2015, 167 NZDF personnel are deployed overseas on operations and UN missions in the South Pacific, Asia, Africa, Antarctica and Middle East areas, and over 200 on other overseas engagements or exercises.

After the Treaty of Waitangi in 1840 New Zealand's security was dependent on British Imperial troops deployed from Australia and other parts of the empire. By 1841 the settlers, particularly those in the New Zealand Company settlement of Wellington, were calling for local militia to be formed. In 1843 a local militia had been formed in Wellington without official sanction. This prompted the Chief Police Magistrate Major Matthew Richmond to order its immediate disbandment. Richmond also dispatched 53 soldiers from the 96th Regiment from Auckland to Wellington.

These calls for a militia continued to grow with the Wairau Affray. The calls eventually lead to a bill being introduced to the Legislative Assembly in 1844. Those present noted their disapproval of the bill, unanimously deferring it for six months. On 22 March 1845 the Flagstaff War broke out, which proved to be the catalyst for passing the Bill.

In 1844 a Select Committee of the House of Commons had recommended that a militia, composed of both settlers and native Maori, and a permanent native force be set up.

On 25 March 1845, the Militia Ordinance was passed into law. Twenty-six officers were appointed in Auckland, thereby forming the start of New Zealand's own defence force. Major Richmond was appointed commander of the Wellington Battalion of the militia. The newspaper article of the time notes that Wellington had a mounted Volunteer Corp. The Nelson Battalion of Militia was formed 12 August 1845.

In June 1845, 75 members of the Auckland Militia under Lieutenant Figg became the first unit to support British Imperial troops in the Flagstaff War, serving as pioneers. Seven militia were wounded in action between 30 June and 1 July 1845. One, a man named Rily, later died of his wounds. The Auckland Militia was disbanded in August or early September 1845 because of budgetary constraints. Disbandment of the Nelson and Wellington Militias followed much to the dismay of their supporters. Those at Nelson under Captain Greenwood decided, regardless of pay or not, to continue training.

Trouble in the Hutt Valley, near Wellington, in early March 1846 prompted the new Lieutenant Governor George Grey to proclaim martial law and call out the Hutt Militia. Following on from this the local paper noted that the No 1 Company of the Wellington Militia had been called out, while the troops stationed in the town had been in the Hutt. The paper further noted that Grey intended to maintain two companies of Militia in Wellington. As problems continued in the area at least 160 Militia remained. These were supplemented by volunteers, and Maori warriors from the Te Aro pah.

On 28 October 1846, with the passing of the Armed Constabulary Ordinance in 1846, a fresh call was made by Mr Donnelly of the Legislature to do away with the Militia because of its expense. However the cost to Britain of maintaining a military force in New Zealand was considerable prompting a dispatch on 24 November 1846 from Right Hon Earl Grey to advise Lieutenant Governor George Grey that ... the formation of a well-organised Militia and of a force of Natives in the service of Her Majesty, would appear to be the measures most likely to be successfully adopted. Further pressure in the early 1850s from Britain for removing their forces prompted pleas for them to remain as the Militia were deemed insufficient for the purpose.

1854 brought a new threat to the attention of the colony, because up to that time the military focus had been upon internal conflicts between settlers and the native population. War had broken out between Russia and Turkey. This war began to involve the major European powers and exposed New Zealand and Australia to a possible external threat from Russian naval forces. Parliament discussed providing guns at ports around the country for use in the event of a war with a foreign power.

By 1858 attention had swung back to local issues with a land dispute in New Plymouth prompting Governor Thomas Gore Brown to call out its militia under Captain Charles Brown. A prelude to what was to become the First Taranaki War and a period of conflict in the North Island until 1872.

Parliament revised and expanded the Militia Ordinance, replacing it with the Militia Act 1858. Some of the main changes were clauses enabling volunteers to be included under such terms and conditions as the Governor may specify. The act also outlined the purposes under which Militia could be called upon, including invasion. Debates in Parliament had included expressions of concern about Russian naval expansion in the northern Pacific, pointed out that the sole naval defence consisted of one 24-gun frigate, and the time it would take for Britain to come to the colony's aid.

British Imperial troops remained in New Zealand until February 1870, during the later stage of the New Zealand Wars, by which time settler units had replaced them.

The Defence Act 1886 reclassified the militia as volunteers. These were the forerunners of the Territorials.

Although there were informal volunteer units as early as 1845, the appropriate approval and regulation of the units did not occur until The Militia Act 1858. Those who signed up for these units were exempt from militia duty, but had to be prepared to serve anywhere in New Zealand. One of the earliest gazetted units (13 January 1859) was the Taranaki Volunteer Rifle Company.

To the Volunteer Rifle Corps were added Volunteer Artillery Corps in mid-1859. The first of these Volunteer Artillery Corps were based in Auckland.

By late 1859 the number of volunteer units were so great that Captain H C Balneavis was appointed Deputy Adjunct-General, based at Auckland.

In 1863 the government passed the Colonial Defence Force Act 1862 creating the first Regular Force. This was to be a mounted body of not more than 500 troops, with both Maori and settlers, and costing no more than 30,000 pounds per annum. All were volunteers and expected to serve for three years.

Formation of the first unit did not begin until early April 1863, with 100 men being sought at New Plymouth under Captain Atkinson. Hawke's Bay was to have the next unit. By late April, papers were reporting few had enlisted in New Plymouth.

Formation of an Auckland unit under Colonel Nixon commenced in July and by the 14th had 30 men.

Commander: Major-General Galloway

By October 1863 there was no Wairarapa-based defence force, and 50 were based in Wanganui. The Otago force had earlier been moved to Wellington, with further Otago volunteers heading for the Auckland and Hawke's Bay Units. The total Defence Force numbered 375 by 3 November 1863.

In October 1864 the Government decided to reduce the numbers in the Colonial Defence Force to 75 with three units of 25 members each in Wellington, Hawkes Bay and Taranaki. By this time there were about 10,000 British Imperial troops in New Zealand, supplemented by about as many New Zealand volunteer and militia forces. There were calls, particularly from South Island papers, for the British Imperial troops to be replaced by local forces. Parliamentary debates in late 1864 also supported this view, especially as the cost of maintaining the Imperial troops was becoming a greater financial burden on the colony.

At the request of the governor in January 1865 a formal statement on the defence of the colony was presented on 20 March 1865. This proposed an armed constabulary force supported by friendly natives, volunteer units, and militia as the case may require be established to take the place of the Imperial troops. The proposed force was to consist of 1,350 Europeans and 150 Maori – 1,500 in total. They were to be divided into 30 companies of 50 men each based as follows:

The total Defence budget, which included purchasing a steamer for use on the Waikato, Patea, and Wanganui rivers, was 187,000 pounds per annum. The budget's focus was solely on internal conflict. The issue of external conflict did not begin to resurface until the following year, with thought being given again to coastal defences.

The Colonial Defence Force was disbanded in October 1867 by the Armed Constabulary Act 1867. Its members transferred to the Armed Constabulary.

From 1863 to 1867 Forest Ranger volunteer units were formed, tasked with searching out Maori war parties, acting as scouts, and protecting lines of communication. They arose out of the need to prevent ambushes and random attacks on civilians near forest areas. The Rangers were well armed and more highly paid. These units used guerrilla style tactics, moving through areas under cover of darkness and ambushing war parties. The Forest Rangers were disbanded on 1 October 1867.

Alongside the militia and the British Imperial forces were the Armed Constabulary. The Armed Constabulary were formed in 1846 with the passage of the Armed Constabulary Ordinance. The Constabulary's role was both regular law enforcement and during the New Zealand Wars militia support. From 1867 to 1886 the Armed Constabulary were the only permanent force in New Zealand. In 1886 the militia functions of the Armed Constabulary were transferred to the New Zealand Permanent Militia by the Defence Act 1886. Lieutenant Colonel John Roberts was the Permanent Militia's first commander from January 1887 to his retirement in 1888.

The Defence Act 1909 replaced the Volunteer forces with a Territorial force and compulsory military training, a regime that remained until the late 1960s, with breaks from 1918 to 1921, 1930 to 194?, and 194? to 1948.

Independent New Zealand armed forces developed in the early twentieth century; the Royal New Zealand Navy was the last to emerge as an independent service in 1941. Prior to that time it had been the New Zealand Division of the Royal Navy. New Zealand forces served alongside the British and other Empire and Commonwealth nations in World War I and World War II.

The fall of Singapore in 1942 showed that Britain could no longer protect its far-flung Dominions. Closer military ties were therefore necessary for New Zealand's defence. With United States entering the war, they were an obvious choice. Links with Australia had also been developed earlier; both nations sent troops to the Anglo-Boer War and New Zealand officer candidates had trained at Australia's Royal Military College, Duntroon since 1911, a practice that continues to this day. A combined Australian and New Zealand Army Corps (ANZAC) was formed for the Gallipoli Campaign during World War I, and its exploits are key events in the military history of both countries.

The NZDF came into existence under the Defence Act 1990. Under previous legislation, the three services were part of the Ministry of Defence. Post-1990, the Ministry of Defence is a separate, policy-making body under a Secretary of Defence, equal in status to the Chief of Defence Force.

A new HQNZDF facility was opened by Prime Minister Helen Clark in March 2007. The new facility on Aitken St in the Wellington CBD replaced the premises on Stout St that had been the headquarters of NZDF for nearly 75 years. The Aitken St facility initially was home to around 900 employees of the NZDF, the New Zealand Security Intelligence Service (NZSIS) and the New Zealand Ministry of Defence; the NZSIS moved across to Pipitea House in early 2013, and the NZDF were forced to vacate the Aitken St building after the 2016 Kaikoura earthquake, which seriously damaged the building. As of October 2017 it is undergoing demolition, with this scheduled to be completed in early 2018; HQNZDF functions having been moved into other buildings and facilities across the region. HQNZDF operates as the administrative and support headquarters for the New Zealand Defence Force, with operational forces under the separate administrative command and control of HQJFNZ.

The operational forces of the three services are directed from Headquarters Joint Forces New Zealand opposite Trentham Military Camp in Upper Hutt. HQ JFNZ was established at Trentham on 1 July 2001. From this building, a former NZ government computer centre that used to house the Army's Land Command, the Air Component Commander, Maritime Component Commander, and Land Component Commander exercise command over their forces. Commander Joint Forces New Zealand (COMJFNZ), controls all overseas operational deployments and most overseas exercises.

The Defence Force created a joint-service corporate services organisation known as the Joint Logistics and Support Organisation (JLSO) in the 2000s, which later became Defence Shared Services.

Following the establishment of Special Operations Command on 1 July 2015, the new position of Special Operations Component Commander was created. This officer reports to the Commander Joint Forces New Zealand, and is of equivalent status to the Maritime, Land and Air Component Commanders.

In recent years, the New Zealand Defence Force has implemented a policy of honoring veterans, and increased its support to still servicemen and women in a number of ways. This includes starting the Defence Force KiwiSaver Scheme, and appointing financial advisers to support the welfare of members.

The Royal New Zealand Navy (RNZN) has 2,132 full-time and 435 part-time sailors. The RNZN possess two "Anzac" class frigates, developed in conjunction with Australia, based on the German MEKO 200 design. Nine other vessels are in use, consisting of patrol vessels and logistics vessels. In 2010, the RNZN completed the acquisition of seven new vessels: one large Multi-Role Vessel named the HMNZS Canterbury, two Offshore Patrol Vessels, and four Inshore Patrol Vessels. All of these vessels were acquired under Project Protector, and were built to commercial, not naval, standards.

New Zealand's Army has 4,584 full-time and 1,671 part-time troops. They are organised as light infantry and motorised infantry equipped with 102 Canadian-manufactured LAV III Light Armoured Vehicles (NZLAV). There are also armoured reconnaissance, artillery, logistic, communications, medical and intelligence elements. The New Zealand Special Air Service is the NZDF's special forces capability, which operates in both conventional warfare and counter-terrorist roles. The Corps and Regiments of the New Zealand Army include:

The Royal New Zealand Air Force (RNZAF) has 2,403 full-time and 212 part-time airmen and airwomen. The RNZAF consists of 51 aircraft, consisting of P-3 Orion maritime patrol aircraft and Lockheed C-130 Hercules and other transport aircraft. The NHIndustries NH90 operates in a medium-utility role, and the AgustaWestland A109 operates the light utility helicopter role, in addition to the main training platform. RNZAF primary flight training occurs in Beechcraft T-6 Texan IIs, before moving onto the Beechcraft King Air.

The RNZAF does not have air combat capabilities following the retirement without replacement of its Air Combat Force of A-4 Skyhawks in December 2001.

New Zealand states it maintains a "credible minimum force", although critics (including the New Zealand National Party while in opposition) maintain that the country's defence forces have fallen below this standard. With a claimed area of direct strategic concern that extends from Australia to Southeast Asia to the South Pacific, and with defence expenditures that total around 1% of GDP, New Zealand necessarily places substantial reliance on co-operating with other countries, particularly Australia.

Acknowledging the need to improve its defence capabilities, the government in 2005 announced the Defence Sustainability Initiative allocating an additional NZ$4.6 billion over 10 years to modernise the country's defence equipment and infrastructure and increase its military personnel. The funding represented a 51% increase in defence spending since the Labour government took office in 1999.

New Zealand is an active participant in multilateral peacekeeping. It has taken a leading role in peace-keeping in the Solomon Islands and the neighbouring island of Bougainville. New Zealand has contributed to United Nations and other peacekeeping operations in Angola, Cambodia, Somalia, Lebanon and the former Yugoslavia. It also participated in the Multilateral Interception Force in the Persian Gulf. New Zealand has an ongoing peacekeeping commitment to East Timor, where it participated in the INTERFET, UNTAET and UMAMET missions from 1999–2002. At one point over 1,000 NZDF personnel were in East Timor. The deployment included the vessels HMNZS Canturbury, Te Kaha and Endeavour, six Iroquois helicopters, two C-130 Hercules and an infantry battalion. In response to renewed conflict in 2006 more troops were deployed as part of an international force. New Zealand has participated in 2 NATO-led coalitions; SFOR in the Former Yugoslavia (until December 2004) and an ongoing one in Afghanistan (which took over from a US-led coalition in 2006). New Zealand also participated in the European Union EUFOR operation in the former Yugoslavia from December 2004 until New Zealand ended its 15-year continuous contribution there on 30 June 2007.

As of December 2015, New Zealand has 167 personnel deployed across the globe. These deployments are to Afghanistan(8), Antarctica(8), South Korea(5), Iraq(106), Middle East(8), Sinai(26), South Sudan(3) and the United Arab Emirates(11). 209 NZDF personnel are on other deployments and exercises.

New Zealand shares training facilities, personnel exchanges, and joint exercises with the Philippines, Thailand, Indonesia, Papua New Guinea, Brunei, Tonga, and South Pacific states. It exercises with its Five Power Defence Arrangements partners, Australia, the United Kingdom, Malaysia, and Singapore. New Zealand military personnel participate in training exercises, conferences and visits as part of military diplomacy.

New Zealand is a signatory of the ANZUS treaty, a defence pact between New Zealand, Australia and the United States dating from 1951. After the 1986 anti-nuclear legislation that refused access of nuclear-powered or armed vessels to ports, the USA withdrew its obligations to New Zealand under ANZUS, and ANZUS exercises are now bilateral between Australia and the United States. Under anti-nuclear legislation, any ship must declare whether it is nuclear-propelled or carrying nuclear weapons before entering New Zealand waters. Due to the US policy at that time of "neither confirm nor deny", ship visits ceased although NZ and the USA remained "good friends". Despite the Presidential Directive of 27 September 1991 that removed tactical nuclear weapons from U.S. surface ships, attack submarines, and naval aircraft, ship visits have not resumed. Despite signs of rapprochement in recent years, military relationships with the US remain limited.

The NZDF served alongside NATO-led forces in Afghanistan in the first decade of the twenty-first century, and in 2004 the NZSAS was awarded a Presidential Unit Citation by US President George W Bush for "extraordinary heroism" in action. In 2008 US Secretary of State Condoleezza Rice during a visit to New Zealand said "New Zealand is now a friend and an ally".

New Zealand is a member of the ABCA Armies standardisation programme, the naval AUSCANNZUKUS forum, the Air and Space Interoperability Council (ASIC, the former ASCC, which, among other tasks, allocates NATO reporting names) and other Western 'Five Eyes' fora for sharing signals intelligence information and achieving interoperability with like-minded armed forces, such as The Technical Cooperation Program (TTCP).




</doc>
<doc id="21360" url="https://en.wikipedia.org/wiki?curid=21360" title="Foreign relations of New Zealand">
Foreign relations of New Zealand

The foreign relations of New Zealand are oriented chiefly toward developed democratic nations and emerging Pacific economies. The country’s major political parties have generally agreed on the broad outlines of foreign policy, and the current coalition government has been active in promoting free trade, nuclear disarmament, and arms control.

In summer 2013 Foreign Minister Murray McCully reported that:

New Zealand was first settled by Polynesians at some point between 800 and 1300 AD. From the 1760s New Zealand was visited by various European explorers and traders, and later missionaries and settlers. An informal system of trade was established, especially in Northland, and some iwi (tribes) became wealthy and powerful. As Māori was a tribal-level society of many shifting chiefdoms, relationships with Europeans were ad hoc and informal. In 1835 a group of Northland chiefs, under the guidance of British resident James Busby, signed a declaration of independence, which was recognised by Britain.

Many Māori were still worried that a European power might invade and dispossess them, and some iwi were having difficulties controlling the large numbers of Europeans who visited and settled in their areas. English missionaries were also concerned about the levels of lawlessness, which were undermining their efforts to convert Māori to Christianity. The British Colonial Office, influenced by the missionaries and by reports that the independent New Zealand Company was planning to privately colonise the islands, sent naval captain William Hobson to negotiate a treaty. The subsequent Treaty of Waitangi, signed in 1840, made New Zealand part of the British Empire, established a Governor of New Zealand, and gave Māori the rights of British subjects.

The annexation of New Zealand by Britain meant that Britain now controlled New Zealand's foreign policy. Subsidised large-scale immigration from Britain and Ireland began, and miners came for the gold rush around 1850-60. In the 1860s, the British sent 16,000 soldiers to contain the New Zealand wars in the North Island. The colony shipped gold and, especially, wool to Britain. From the 1880s the development of refrigerated shipping allowed the establishment of an export economy based on the mass export of frozen meat and dairy products to Britain. In 1899-1902 New Zealand made its first contribution to an external war, sending troops to fight on the British side in the Second Boer War. The country changed its status from colony to dominion with internal self governance in 1907.

New Zealand eagerly sent a large fraction of its young men to fight on Britain's side in the First World War. Their heroism in the failed Gallipoli campaign made their sacrifices iconic in New Zealand memory, and secured the psychological independence of the nation.

After the war New Zealand signed the Treaty of Versailles (1919) joined the League of Nations and pursued an independent foreign policy, while its defense was still controlled by Britain. Wellington trusted Conservative Party governments in London, but not Labour. When the British Labour Party took power in 1924 and 1929, the New Zealand government felt threatened by Labour's foreign policy because of its reliance upon the League of Nations. The League was distrusted and Wellington did not expect to see the coming of a peaceful world order under League auspices. What had been the Empire's most loyal dominion became a dissenter as it opposed efforts the first and second British Labour governments to trust the League's framework of arbitration and collective security agreements.

The governments of the Reform and United parties between 1912 and 1935 followed a "realistic" foreign policy. They made national security a high priority, were skeptical of international institutions such as the League, and showed no interest on the questions of self-determination, democracy, and human rights. However the opposition Labour Party was more idealistic and proposed a liberal internationalist outlook on international affairs. From 1935 the First Labour Government showed a limited degree of idealism in foreign policy, for example opposing the appeasement of Germany and Japan.

When World War II broke out in 1939, New Zealand whole-heartedly joined in the defence of Britain, with Prime Minister Michael Joseph Savage declaring that "where Britain goes, we go; where Britain stands, we stand". New Zealand soldiers served in North Africa, Italy and the Pacific, and airmen in England and the Pacific, throughout the war, even when New Zealand had concerns about invasion by the Japanese. In 1947 New Zealand ratified the 1931 Statute of Westminster, which made certain former colonies completely self-governing.

The Fall of Singapore during World War II made New Zealand realise that she could no longer rely on Britain to defend the British Empire. New Zealand troops supported the British in the successful battle against Communist insurrection in Malaysia and maintained an air-force fighter squadron in Singapore, and later on Cyprus, again supporting British forces. New Zealand diplomats sought an alliance with the United States of America, and in 1951 adhered to the ANZUS Treaty between New Zealand, Australia and the US. In return for America's guarantee of protection, New Zealand felt obliged to support America in its wars, and New Zealand committed forces to the Korean War (1950-1953) under United nation auspices and to the Vietnam War. By the 1970s, many New Zealanders began to feel uncomfortable with their country's support for the US, particularly in Vietnam and regarding the visits of nuclear-powered and armed American warships. The Third Labour government (1972–1975) pulled New Zealand troops out of the Vietnam War and protested against French nuclear testing in the Pacific, at one stage sending a warship to act as disapproving witness to the tests.

Britain's entry into the European Economic Community in 1973 forced New Zealand into a more independent role. The British move restricted New Zealand's trade access to its biggest market, and it sought new trading partners in Asia, America and the Middle East. Australia and New Zealand signed the free-trade Closer Economic Relations agreement in 1983. The election of the Fourth Labour Government in 1984 marked a new period of independent foreign policy. Nuclear-powered and nuclear-armed ships were banned from New Zealand waters, effectively removing New Zealand from the ANZUS pact. Immigration laws were liberalised, leading to a massive increase in immigration from Asia. The Fourth National Government (1990–1999) liberalised trade by removing most tariffs and import restrictions.

In 2008, Minister of Foreign Affairs Winston Peters announced what he called "a seismic change for New Zealand’s foreign service", designed to remedy the country's "struggling to maintain an adequate presence on the international stage". Peters said that the Ministry would receive additional funding and increase the number of New Zealand diplomats serving abroad by 50%. However this policy was reversed following the 2008 General Election which brought the John Key-led Fifth National Government of New Zealand to power.

New Zealand was a founding member of the United Nations in 1945. New Zealand Prime Minister Peter Fraser felt that in order for New Zealand to be secure in the South Pacific, it need to align itself with major world powers like the United States through some kind of organisation that could guarantee small powers a say in world affairs. Since the defeat of the Royal Navy during World War II it became clear that Britain was no longer able to protect New Zealand so the government decided that a policy of independent relations with a group of strong powers was the best way to defend New Zealand.

New Zealand participates in the United Nations (UN); the World Trade Organization (WTO); World Bank; the International Monetary Fund (IMF); the Organisation for Economic Co-operation and Development (OECD); the International Energy Agency; the Asian Development Bank; the Pacific Islands Forum; the Secretariat of the Pacific Community; the Colombo Plan; Asia-Pacific Economic Cooperation (APEC); and the International Whaling Commission. New Zealand also actively participates as a member of the Commonwealth. Despite the 1985 rupture in the ANZUS military alliance, New Zealand has maintained good working relations with the United States and Australia on a broad array of international issues.

In the past, New Zealand's geographic isolation and its agricultural economy's general prosperity minimised public interest in international affairs. However, growing global trade and other international economic events have made New Zealanders increasingly aware of their country’s dependence on unstable overseas markets. New Zealand governments strongly advocate free trade, especially in agricultural products, and the country belongs to the Cairns group of nations in the WTO.

New Zealand's economic involvement with Asia has become increasingly important. New Zealand is a “dialogue partner” with the Association of Southeast Asian Nations (ASEAN), a member of the East Asia Summit and an active participant in APEC.

As a charter member of the Colombo Plan, New Zealand has provided Asian countries with technical assistance and capital. It also contributes through the Asian Development Bank and through UN programs and is a member of the UN Economic and Social Council for Asia and the Pacific.

ABEDA, ANZUS (U.S. suspended security obligations to NZ on 11 August 1986), APEC, ARF (dialogue partner), AsDB, ASEAN (dialogue partner), Australia Group, Commonwealth, CP, EBRD, ESCAP, FAO, IAEA, IBRD, ICAO, ICC, ICCt, ICFTU, ICRM, IDA, IEA, IFAD, IFC, IFRCS, IHO, ILO, IMF, IMO, Interpol, IOC, IOM, ISO, ITU, NAM (guest), NSG, OECD, OPCW, PCA, PIF, Sparteca, SPC, UN, UNAMSIL, UNCTAD, UNESCO, UNHCR, UNIDO, UNMIK, UNMISET, UNMOP, UNTSO, UPU, WCO, WFTU, WHO, WIPO, WMO, WTO

New Zealand administers Tokelau (formerly known as the Tokelau Islands) as a non self-governing colonial territory. In February 2006 a UN-sponsored referendum was held in Tokelau on whether to become a self-governing state, but this failed to achieve the two-thirds majority required to pass.
Samoa was a New Zealand protectorate from 1918 to full independence in 1962. However New Zealand retains some responsibilities for former colonies Niue and the Cook Islands which are in free association with New Zealand. Citizens of all three countries hold New Zealand citizenship and the associated rights to healthcare and education in New Zealand.

New Zealand has also claimed part of Antarctica known as the Ross Dependency since 1923.

McGraw argues that, "Probably the greatest foreign policy achievement of [Helen] Clark's [1999–2008] term was the conclusion of a free trade agreement with China." Clark's government also set up a free-trade deal with Australia and the ten nations of ASEAN (the Association of South East Asian Nations).

New Zealand has existing free trade agreements with Australia, Brunei, Chile, the People's Republic of China, Hong Kong, Singapore, and Thailand; new free trade agreements are under negotiation with ASEAN, and Malaysia. New Zealand is involved in the WTO's Doha Development Agenda and was disappointed by the failure of the most recent talks in July 2006.

New Zealand is a signatory of the Trans Pacific Partnership. Its Labour-NZ First coalition government has committed to initiate a Closer Commonwealth Economic Relations (CCER) agreement with the UK, Australia, Canada and other countries and to work towards a Free Trade Agreement with the Russia-Belarus-Kazakhstan Customs Union.

New Zealand's main export is food, primarily dairy products, meat, fruit and fish; about 95% of the country's agricultural produce is exported. Other major exports are wood, and mechanical and electrical equipment. About 46% of exports are non-agricultural, but the largest industry is still the food industry. Tourism is also an extremely important component of international trade: transport and travel form around 20% of the country's export trade. New Zealand does not have large quantities of mineral resources, though it does produce some coal, oil, Aluminium and natural gas.

New Zealand's largest source of imports is China, followed by (in order) Australia, the United States, Japan, and Singapore. The largest destinations for exports are, in order, Australia, China, the U.S., Japan, and South Korea. Trade figures for 2011 with New Zealand's biggest trade partners are as follows:

The New Zealand Defence Force is small compared to many other countries although it is generally regarded as very professional.Its overseas duties consist mostly of peacekeeping, especially in the Pacific. In the 21st century, peacekeeping detachments have been deployed to East Timor, the Solomon Islands, and Tonga. Engineering and support forces have also been involved in the Iraq War, although New Zealand is not a member of the 'coalition of the willing'. New Zealand's heaviest military involvement in recent decades has been in Afghanistan following the United States-led invasion of that country after the 11 September 2001 attacks. The deployment has included SAS troops.

New Zealand's official aid programme is managed by the New Zealand Agency for International Development (NZAID), a semi-autonomous body within the Ministry of Foreign Affairs and Trade. In 2007 New Zealand was the sixth lowest foreign aid donor in the Organisation for Economic Co-operation and Development (OECD), based on proportion of gross national income (GNI) spent on overseas development assistance. New Zealand's contribution was 0.27% of GNI. Much this went to the Pacific region. However, the country is occasionally more generous in responding to major crises, for example donating around 100 million New Zealand dollars to the 2004 Indian Ocean earthquake relief effort. New Zealand troops and aircraft are also often sent to disaster areas in the Asia-Pacific.

In the 1970s and 1980s anti-nuclear sentiment increased across New Zealand fuelling concerns about French nuclear testing in the Pacific at Moruroa atoll. The third Labour Government under Norman Kirk, co-sponsored by Australia, took France before the International Court of Justice in 1972, requesting that the French cease atmospheric nuclear testing at Mururoa Atoll in French Polynesia in the southern Pacific Ocean. In 1972, as an act of defiance and protest the Kirk government sent two of its navy frigates, HMNZS Canterbury and Otago into the Moruroa test zone area. Peace yachts attempting to disrupt the French tests had been sailing in coordinated protests into the Mururoa exclusion zones between 1972–1991. Concerns about Nuclear proliferation and the presence of nuclear warheads or reactors on United States Navy ships visiting New Zealand ports continued to escalate. After it was elected in 1984, the Labour Party government of David Lange indicated its opposition to visits by such ships. In February 1985, New Zealand turned away the USS Buchanan (DDG-14) and in response the United States announced that it was suspending its treaty obligations to New Zealand unless port access was restored. In 1987 the labour Government strengthened its stance by declaring New Zealand a nuclear-free zone (New Zealand Nuclear Free Zone, Disarmament, and Arms Control Act 1987), effectively legally removing New Zealand from the nuclear deterrent scenario and banning the entry of nuclear powered warships into its ports. Warships that did not fall into this category were not blocked, but the US took the view that any subsequent visit by a warship to New Zealand could not be carried out without violating the US' security policy of "neither confirming nor denying" nuclear capability of its ships.

In 1987 New Zealand passed legislation making the country a nuclear free zone, namely the New Zealand Nuclear Free Zone, Disarmament, and Arms Control Act; in the same year the US retaliated with the Broomfield Act, designating New Zealand as a "friend" rather than an "ally". Relations between New Zealand and the US have had several ups and downs since then.

In recent years, some voices have suggested removing the anti-nuclear legislation, especially the ACT New Zealand political party; and up until February 2006 the National Party was in favour of holding a referendum on the issue. However, public opinion remains strongly in favour of the country's status as a nuclear free zone. In May 2006, US Assistant Secretary of State for East Asia and Pacific Affairs, Christopher Hill, described the disagreement between the US and New Zealand as "a relic" but also signalled that the US wanted a closer defence relationship with New Zealand and praised New Zealand’s involvement in Afghanistan and reconstruction in Iraq. "Rather than trying to change each other's minds on the nuclear issue, which is a bit of a relic, I think we should focus on things we can make work," he told the "Australian Financial Review". Pressure from the United States on New Zealand's foreign policy increased in 2006, with U.S. trade officials linking the repeal of the ban of American nuclear ships from New Zealand's ports to a potential free trade agreement between the two countries.

Relations between France and New Zealand were strained for two short periods in the 1980s and 1990s over the French nuclear tests at Moruroa and the bombing of the Rainbow Warrior in Auckland harbour. The latter was widely regarded as an act of state terrorism against New Zealand's sovereignty and was ordered by then French President François Mitterrand, although he denied any involvement at the time. These events worked to strengthen New Zealand's resolve to retain its anti-nuclear policy. Relations between the two countries are now cordial, with strong trade and many new bilateral links. 

New Zealand has well-established links to a number of Latin American countries, particularly in the economic sphere. New Zealand has Embassies in Mexico City, Santiago, Brasília and Buenos Aires – the first of which (Santiago) opened in 1972. The New Zealand Government's Latin America Strategy, published in May 2010, estimates New Zealand's annual exports to the region at NZ$1 billion, and New Zealand investments in the region (in areas such as agri-technology, energy, fisheries, and specialised manufacturing) at around NZ$1.3 billion. The Strategy argues that there is considerable scope to expand New Zealand's investment and services trade in the region. Focusing on six countries (Brazil, Mexico, Chile, Argentina, Uruguay and Peru), the Strategy posits that New Zealand should be seeking to: promote a better understanding of the region among New Zealand businesses to help identify prospects for increased investment, trade and joint ventures; lower barriers to business between New Zealand and Latin America; promote New Zealand tourism in the region; improve airlinks between New Zealand and the region; deepen education and research and science links. There are significant flows of tourists and students from Latin America to New Zealand. For example, in the year to June 2010, around 30,000 Latin Americans visited New Zealand. In addition, New Zealand has popular Working Holiday Schemes with Brazil, Argentina, Chile, Peru, Mexico and Uruguay.

Much of New Zealand's foreign policy is focused on the Pacific region, particularly Polynesia and Melanesia. Bilateral economic assistance resources have been focused on projects in the South Pacific island states, especially on Bougainville. The country’s long association with Samoa (formerly known as Western Samoa), reflected in a treaty of friendship signed in 1962, and its close association with Tonga have resulted in a flow of immigrants and visitors under work permit schemes from both countries. Recently New Zealand forces participated in peacekeeping efforts in the Pacific region in East Timor, the Solomon Islands and Tonga, see Military history of New Zealand.

In 1947, New Zealand joined Australia, France, the United Kingdom, and the United States to form the South Pacific Commission, a regional body to promote the welfare of the Pacific region. New Zealand has been a leader in the organisation. In 1971, New Zealand joined the other independent and self-governing states of the South Pacific to establish the South Pacific Forum (now known as the Pacific Islands Forum), which meets annually at the “heads of government” level.

On 26 May, New Zealand deployed forty-two troops, with a second contingent of 120 troops leaving Christchurch on 27 May, en route to Townsville, Queensland before being sent to East Timor. Clark said that the forces would be deployed where needed by the Australian command.




</doc>
<doc id="21362" url="https://en.wikipedia.org/wiki?curid=21362" title="Nicaragua">
Nicaragua

Nicaragua (; ), officially the Republic of Nicaragua (), is the largest country in the Central American isthmus, bordered by Honduras to the north, the Caribbean to the east, Costa Rica to the south, and the Pacific Ocean to the west. Managua is the country's capital and largest city and is also the third-largest city in Central America, behind Tegucigalpa and Guatemala City. The multi-ethnic population of six million includes people of indigenous, European, African, and Asian heritage. The main language is Spanish. Native tribes on the Mosquito Coast speak their own languages and English.

Originally inhabited by various indigenous cultures since ancient times, the Spanish Empire conquered the region in the 16th century. Nicaragua gained independence from Spain in 1821. The Mosquito Coast followed a different historical path, with the English colonizing it in the 17th century and later coming under the British rule, as well as some minor Spanish interludes in the 19th century. It became an autonomous territory of Nicaragua in 1860 and the northernmost part of it was later transferred to Honduras in 1960. Since its independence, Nicaragua has undergone periods of political unrest, dictatorship, and fiscal crisis, leading to the Nicaraguan Revolution of the 1960s and 1970s and the Contra War of the 1980s. Nicaragua is a representative democratic republic.

The mixture of cultural traditions has generated substantial diversity in folklore, cuisine, music, and literature, particularly the latter given the literary contributions of Nicaraguan poets and writers, such as Rubén Darío. Known as the "land of lakes and volcanoes", Nicaragua is also home to the second-largest rainforest of the Americas. The country has set a goal of 90% renewable energy by the year 2020. The biological diversity, warm tropical climate and active volcanoes make Nicaragua an increasingly popular tourist destination.

There are two prevailing theories on how the name "Nicaragua" came about. The first is that the name was coined by Spanish colonists based on the name Nicarao, who was the chieftain or cacique of a powerful indigenous tribe encountered by the Spanish conquistador Gil González Dávila during his entry into southwestern Nicaragua in 1522. This theory holds that the name Nicaragua was formed as a portmanteau of Nicarao and the word "agua" which means "water" in Spanish, to reference the fact that there are two large lakes and several other bodies of water within the country. However, as of 2002, it was determined that the cacique's real name was Macuilmiquiztli, which meant "Five Deaths" in the Nahuatl language, rather than Nicarao.

The second theory is that the country's name comes from any of the following Nahuatl words: "nic-anahuac", which meant "Anahuac reached this far", or "the Nahuas came this far", or "those who come from Anahuac came this far"; "nican-nahua", which meant "here are the Nahuas"; or "nic-atl-nahuac", which meant "here by the water" or "surrounded by water".

Paleo-Americans first inhabited what is now known as Nicaragua as far back as 12,000 BCE. In later pre-Columbian times, Nicaragua's indigenous people were part of the Intermediate Area, between the Mesoamerican and Andean cultural regions, and within the influence of the Isthmo-Colombian area. Nicaragua's central region and its Caribbean coast were inhabited by Macro-Chibchan language ethnic groups. They had coalesced in Central America and migrated also to present-day northern Colombia and nearby areas. They lived a life based primarily on hunting and gathering, as well as fishing, and performing slash-and-burn agriculture.

At the end of the 15th century, western Nicaragua was inhabited by several different indigenous peoples related by culture to the Mesoamerican civilizations of the Aztec and Maya, and by language to the Mesoamerican Linguistic Area. The Chorotegas were Mangue language ethnic groups who had arrived in Nicaragua from what is now the Mexican state of Chiapas sometime around 800 CE. The Pipil-Nicarao people were a branch of Nahuas who spoke the Nahuat dialect, and like the Chorotegas, they too had come from Chiapas to Nicaragua in approximately 1200 CE. Prior to that, the Pipil-Nicaraos had been associated with the Toltec civilization. Both the Chorotegas and the Pipil-Nicaraos were originally from Mexico's Cholula valley, and had gradually migrated southward. Additionally, there were trade-related colonies in Nicaragua that had been set up by the Aztecs starting in the 14th century.

In 1502, on his fourth voyage, Christopher Columbus became the first European known to have reached what is now Nicaragua as he sailed southeast toward the Isthmus of Panama. Columbus explored the Mosquito Coast on the Atlantic side of Nicaragua but did not encounter any indigenous people. 20 years later, the Spaniards returned to Nicaragua, this time to its southwestern part. The first attempt to conquer Nicaragua was by the conquistador Gil González Dávila, who had arrived in Panama in January 1520. In 1522, González Dávila ventured into the area that later became known as the Rivas Department of Nicaragua. It was there that he encountered an indigenous Nahua tribe led by a chieftain named Macuilmiquiztli, whose name has sometimes been erroneously referred to as "Nicarao" or "Nicaragua". At the time, the tribe's capital city was called Quauhcapolca. González Dávila had brought along two indigenous interpreters who had been taught the Spanish language, and thus he was able to have a discourse with Macuilmiquiztli. After exploring and gathering gold in the fertile western valleys, González Dávila and his men were attacked and driven off by Chorotega natives led by the chieftain Diriangen. The Spanish attempted to convert the tribes to Christianity; the people in Macuilmiquiztli's tribe were baptized, but Diriangen, however, was openly hostile to the Spaniards.

The first Spanish permanent settlements were founded in 1524. That year, the conquistador Francisco Hernández de Córdoba founded two of Nicaragua's principal cities: Granada on Lake Nicaragua was the first settlement, followed by León at a location west of Lake Managua. Córdoba soon built defenses for the cities and fought against incursions by other conquistadors. Córdoba was later publicly beheaded as a consequence for having defied the authority of his superior, Pedro Arias Dávila. Córdoba's tomb and remains were discovered in 2000 in the ruins of León Viejo.

The clashes among Spanish forces did not impede their destruction of the indigenous people and their culture. The series of battles came to be known as the "War of the Captains". Pedro Arias Dávila was a winner; although he had lost control of Panama, he moved to Nicaragua and successfully established his base in León. In 1527, León became the capital of the colony. Through adroit diplomatic machinations, Arias Dávila became the colony's first governor.

Without women in their parties, the Spanish conquerors took Nahua and Chorotega wives and partners, beginning the multiethnic mix of native and European stock now known as ""mestizo"", which constitutes the great majority of the population in western Nicaragua. Many indigenous people died as a result of new infectious diseases, compounded by neglect by the Spaniards, who controlled their subsistence. Furthermore, a large number of other natives were captured and transported to Panama and Peru between 1526 and 1540, where they were forced to perform slave labor.

In 1610, the Momotombo volcano erupted, destroying the city of León. The city was rebuilt northwest of the original, which is now known as the ruins of León Viejo. During the American Revolutionary War, Central America was subject to conflict between Britain and Spain. British navy admiral Horatio Nelson led expeditions in the Battle of San Fernando de Omoa in 1779 and on the San Juan River in 1780, the latter of which had temporary success before being abandoned due to disease.

The Captaincy General of Guatemala was dissolved in September 1821 with the Act of Independence of Central America, and Nicaragua soon became part of the First Mexican Empire. After the monarchy of the First Mexican Empire was overthrown in 1823, Nicaragua joined the newly formed United Provinces of Central America, which was later renamed as the Federal Republic of Central America. Nicaragua finally became an independent republic in 1838.

Rivalry between the Liberal elite of León and the Conservative elite of Granada characterized the early years of independence and often degenerated into civil war, particularly during the 1840s and 1850s. Managua was chosen as the nation's capital in 1852 to allay the rivalry between the two feuding cities. During the days of the California Gold Rush, Nicaragua provided a route for travelers from the eastern United States to journey to California by sea, via the use of the San Juan River and Lake Nicaragua. Invited by the Liberals in 1855 to join their struggle against the Conservatives, a United States adventurer and filibuster named William Walker set himself up as President of Nicaragua, after conducting a farcical election in 1856. Costa Rica, Honduras, and other Central American countries united to drive Walker out of Nicaragua in 1857, after which a period of three decades of Conservative rule ensued.

Great Britain, which had claimed the Mosquito Coast as a protectorate since 1655, delegated the area to Honduras in 1859 before transferring it to Nicaragua in 1860. The Mosquito Coast remained an autonomous area until 1894. José Santos Zelaya, President of Nicaragua from 1893 to 1909, negotiated the annexation of the Mosquito Coast to the rest of Nicaragua. In his honor, the region was named "Zelaya Department".

Throughout the late 19th century, the United States and several European powers considered a scheme to build a canal across Nicaragua, linking the Pacific Ocean to the Atlantic.

In 1909, the United States provided political support to Conservative-led forces rebelling against President Zelaya. On November 18, 1909, U.S. warships were sent to the area after 500 revolutionaries (including two Americans) were executed by order of Zelaya. Zelaya resigned later that year.

In August 1912, the President of Nicaragua, Adolfo Díaz, requested the secretary of war, General Luis Mena, to resign for fear he was leading an insurrection. Mena fled Managua with his brother, the chief of police of Managua, to start an insurrection. When the U.S. delegation asked President Díaz to ensure the safety of American citizens and property during the insurrection, he replied he could not, and asked the United States to intervene in the conflict.

United States Marines occupied Nicaragua from 1912 to 1933, except for a nine-month period beginning in 1925. In 1914, the Bryan–Chamorro Treaty was signed, giving the U.S. control over a proposed canal through Nicaragua, as well as leases for potential canal defenses. Following the evacuation of U.S. Marines, another violent conflict between Liberals and Conservatives took place in 1926, which resulted in the return of U.S. Marines.

From 1927 until 1933, rebel general Augusto César Sandino led a sustained guerrilla war first against the Conservative regime and subsequently against the U.S. Marines, whom he fought for over five years. When the Americans left in 1933, they set up the "Guardia Nacional" (national guard), a combined military and police force trained and equipped by the Americans and designed to be loyal to U.S. interests.

After the U.S. Marines withdrew from Nicaragua in January 1933, Sandino and the newly elected administration of President Juan Bautista Sacasa reached an agreement by which Sandino would cease his guerrilla activities in return for amnesty, a grant of land for an agricultural colony, and retention of an armed band of 100 men for a year. However, due to a growing hostility between Sandino and National Guard director Anastasio Somoza García and a fear of armed opposition from Sandino, Somoza García decided to order his assassination. Sandino was invited by Sacasa to have dinner and sign a peace treaty at the Presidential House in Managua on the night of February 21, 1934. After leaving the Presidential House, Sandino's car was stopped by soldiers of the National Guard and they kidnapped him. Later that night, Sandino was assassinated by soldiers of the National Guard. Hundreds of men, women, and children from Sandino's agricultural colony were executed later.

Nicaragua has experienced several military dictatorships, the longest being the hereditary dictatorship of the Somoza family, who ruled for 43 nonconsecutive years during the 20th century. The Somoza family came to power as part of a U.S.-engineered pact in 1927 that stipulated the formation of the "Guardia Nacional" to replace the marines who had long reigned in the country. Somoza García slowly eliminated officers in the national guard who might have stood in his way, and then deposed Sacasa and became president on January 1, 1937, in a rigged election.

Nicaragua declared war on Germany on December 8, 1941, during World War II. No soldiers were sent to the war, but Somoza García did seize the occasion to confiscate properties held by German Nicaraguan residents. In 1945, Nicaragua was among the first countries to ratify the United Nations Charter.
On September 21, 1956, Somoza García was shot to death by Rigoberto López Pérez, a 27-year-old Liberal Nicaraguan poet. Luis Somoza Debayle, the eldest son of the late president, was appointed president by the congress and officially took charge of the country. He is remembered by some for being moderate, but was in power only for a few years and then died of a heart attack. His successor as president was René Schick Gutiérrez, whom most Nicaraguans viewed "as nothing more than a puppet of the Somozas". Somoza García's youngest son, Anastasio Somoza Debayle, often referred to simply as "Somoza", became president in 1967.

An earthquake in 1972 destroyed nearly 90% of Managua, creating major losses. Instead of helping to rebuild Managua, Somoza siphoned off relief money. The mishandling of relief money also prompted Pittsburgh Pirates star Roberto Clemente to personally fly to Managua on December 31, 1972, but he died "en route" in an airplane accident. Even the economic elite were reluctant to support Somoza, as he had acquired monopolies in industries that were key to rebuilding the nation.

The Somoza family was among a few families or groups of influential firms which reaped most of the benefits of the country's growth from the 1950s to the 1970s. When Somoza was deposed by the Sandinistas in 1979, the family's worth was estimated to be between $500 million and $1.5 billion.

In 1961, Carlos Fonseca looked back to the historical figure of Sandino, and along with two other people (one of whom was believed to be Casimiro Sotelo, who was later assassinated), founded the Sandinista National Liberation Front (FSLN). After the 1972 earthquake and Somoza's apparent corruption, the ranks of the Sandinistas were flooded with young disaffected Nicaraguans who no longer had anything to lose.

In December 1974, a group of the FSLN, in an attempt to kidnap U.S. ambassador Turner Shelton, held some Managuan partygoers hostage (after killing the host, former agriculture minister, Jose Maria Castillo), until the Somozan government met their demands for a large ransom and free transport to Cuba. Somoza granted this, then subsequently sent his national guard out into the countryside to look for the perpetrators of the kidnapping, described by opponents of the kidnapping as "terrorists".

On January 10, 1978, Pedro Joaquín Chamorro Cardenal, the editor of the national newspaper "La Prensa" and ardent opponent of Somoza, was assassinated. It is alleged that the planners and perpetrators of the murder were at the highest echelons of the Somoza regime.

The Sandinistas forcefully took power in July 1979, ousting Somoza, and prompting the exodus of the majority of Nicaragua's middle class, wealthy landowners, and professionals, many of whom settled in the United States. The Carter administration decided to work with the new government, while attaching a provision for aid forfeiture if it was found to be assisting insurgencies in neighboring countries. Somoza fled the country and eventually ended up in Paraguay, where he was assassinated in September 1980, allegedly by members of the Argentinian Revolutionary Workers' Party.

In 1980, the Carter administration provided $60 million in aid to Nicaragua under the Sandinistas, but the aid was suspended when the administration obtained evidence of Nicaraguan shipment of arms to El Salvadoran rebels. In response to the coming to power of the Sandinistas, various rebel groups collectively known as the "contras" were formed to oppose the new government. The Reagan administration authorized the CIA to help the contra rebels with funding, armaments, and training. The contras operated out of camps in the neighboring countries of Honduras to the north and Costa Rica to the south.
They engaged in a systematic campaign of terror amongst the rural Nicaraguan population to disrupt the social reform projects of the Sandinistas. Several historians have criticized the contra campaign and the Reagan administration's support for it, citing the brutality and numerous human rights violations of the contras. LaRamee and Polakoff, for example, describe the destruction of health centers, schools, and cooperatives at the hands of the rebels, and others have contended that murder, rape, and torture occurred on a large scale in contra-dominated areas. The United States also carried out a campaign of economic sabotage, and disrupted shipping by planting underwater mines in Nicaragua's port of Corinto, an action condemned by the International Court of Justice as illegal. The U.S. also sought to place economic pressure on the Sandinistas, and the Reagan administration imposed a full trade embargo. The Sandinistas were also accused of human rights abuses.

In the Nicaraguan general elections of 1984, which were judged to have been free and fair, the Sandinistas won the parliamentary election and their leader Daniel Ortega won the presidential election. The Reagan administration criticized the elections as a "sham" based on the charge that Arturo Cruz, the candidate nominated by the Coordinadora Democrática Nicaragüense, comprising three right wing political parties, did not participate in the elections. However, the administration privately argued against Cruz's participation for fear his involvement would legitimize the elections, and thus weaken the case for American aid to the contras. According to Martin Kriele, the results of the election were rigged.

After the U.S. Congress prohibited federal funding of the contras in 1983, the Reagan administration nonetheless illegally continued to back them by covertly selling arms to Iran and channeling the proceeds to the contras (the Iran–Contra affair), for which several members of the Reagan administration were convicted of felonies. The International Court of Justice, in regard to the case of Nicaragua v. United States in 1984, found, "the United States of America was under an obligation to make reparation to the Republic of Nicaragua for all injury caused to Nicaragua by certain breaches of obligations under customary international law and treaty-law committed by the United States of America". During the war between the contras and the Sandinistas, 30,000 people were killed.

In the Nicaraguan general election, 1990, a coalition of anti-Sandinista parties (from the left and right of the political spectrum) led by Violeta Chamorro, the widow of Pedro Joaquín Chamorro Cardenal, defeated the Sandinistas. The defeat shocked the Sandinistas, who had expected to win.

Exit polls of Nicaraguans reported Chamorro's victory over Ortega was achieved with a 55% majority. Chamorro was the first woman president of Nicaragua. Ortega vowed he would govern "desde abajo" (from below). Chamorro came to office with an economy in ruins, primarily because of the financial and social costs of the contra war with the Sandinista-led government. In the next election, the Nicaraguan general election, 1996, Daniel Ortega and the Sandinistas of the FSLN were defeated again, this time by Arnoldo Alemán of the Constitutional Liberal Party (PLC).
In the 2001 elections, the PLC again defeated the FSLN, with Alemán's Vice President Enrique Bolaños succeeding him as President. Subsequently, however, Alemán was convicted and sentenced in 2003 to 20 years in prison for embezzlement, money laundering, and corruption; liberal and Sandinista parliament members subsequently combined to strip the presidential powers of President Bolaños and his ministers, calling for his resignation and threatening impeachment. The Sandinistas said they no longer supported Bolaños after U.S. Secretary of State Colin Powell told Bolaños to keep his distance from the FSLN. This "slow motion "coup d'état"" was averted partially by pressure from the Central American presidents, who vowed not to recognize any movement that removed Bolaños; the U.S., the OAS, and the European Union also opposed the action.

Before the general elections on November 5, 2006, the National Assembly passed a bill further restricting abortion in Nicaragua. As a result, Nicaragua is one of five countries in the world where abortion is illegal with no exceptions. Legislative and presidential elections took place on November 5, 2006. Ortega returned to the presidency with 37.99% of the vote. This percentage was enough to win the presidency outright, because of a change in electoral law which lowered the percentage requiring a runoff election from 45% to 35% (with a 5% margin of victory). Nicaragua's 2011 general election resulted in re-election of Ortega, with a landslide victory and 62.46% of the vote. In 2014 the National Assembly approved changes to the constitution allowing Ortega to run for a third successive term.

In November 2016, Ortega was elected for his third consecutive term (his fourth overall). International monitoring of the elections was initially prohibited, and as a result the validity of the elections has been disputed, but observation by the OAS was announced in October. Ortega was reported by Nicaraguan election officials as having received 72% of the vote. However the Broad Front for Democracy (FAD), having promoted boycotts of the elections, claimed that 70% of voters had abstained (while election officials claimed 65.8% participation).

In April 2018, demonstrations opposed a decree increasing taxes and reducing benefits in the country's pension system. Local independent press organizations had documented at least 19 dead and over 100 missing in the ensuing conflict. A reporter from NPR spoke to protestors who explained that while the initial issue was about the pension reform, the uprisings that spread across the country reflected many grievances about the government's time in office, and that the fight is for President Ortega and his Vice President wife to step down. April 24, 2018 marked the day of the greatest march in opposition of the Sandinista party. On May 2, 2018, university-student leaders publicly announced that they give the government seven days to set a date and time for a dialogue that was promised to the people due to the recent events of repression. The students also scheduled another march on that same day for a peaceful protest. As of May 2018, estimates of the death toll were as high as 63, many of them student protesters, and the wounded totalled more than 400. Following a working visit from May 17 to 21, the Inter-American Commission on Human Rights adopted precautionary measures aimed at protecting members of the student movement and their families after testimonies indicated the majority of them had suffered acts of violence and death threats for their participation. In the last week of May, thousands who accuse Mr. Ortega and his wife of acting like dictators joined in resuming anti-government rallies after attempted peace talks have remained unresolved.

Nicaragua occupies a landmass of . Nicaragua has three distinct geographical regions: the Pacific lowlands – fertile valleys which the Spanish colonists settled, the Amerrisque Mountains (North-central highlands), and the Mosquito Coast (Atlantic lowlands/Caribbean lowlands).

The low plains of the Atlantic Coast are wide in areas. They have long been exploited for their natural resources.

On the Pacific side of Nicaragua are the two largest fresh water lakes in Central America—Lake Managua and Lake Nicaragua. Surrounding these lakes and extending to their northwest along the rift valley of the Gulf of Fonseca are fertile lowland plains, with soil highly enriched by ash from nearby volcanoes of the central highlands. Nicaragua's abundance of biologically significant and unique ecosystems contribute to Mesoamerica's designation as a biodiversity hotspot. Nicaragua has made efforts to become less dependent on fossil fuels, and it expects to acquire 90% of its energy from renewable resources by the year 2020.

Nearly one fifth of Nicaragua is designated as protected areas like national parks, nature reserves, and biological reserves. Geophysically, Nicaragua is surrounded by the Caribbean Plate, an oceanic tectonic plate underlying Central America and the Cocos Plate. Since Central America is a major subduction zone, Nicaragua hosts most of the Central American Volcanic Arc.

In the west of the country, these lowlands consist of a broad, hot, fertile plain. Punctuating this plain are several large volcanoes of the Cordillera Los Maribios mountain range, including Mombacho just outside Granada, and Momotombo near León. The lowland area runs from the Gulf of Fonseca to Nicaragua's Pacific border with Costa Rica south of Lake Nicaragua. Lake Nicaragua is the largest freshwater lake in Central America (20th largest in the world), and is home to some of the world's rare freshwater sharks (Nicaraguan shark). The Pacific lowlands region is the most populous, with over half of the nation's population.

The eruptions of western Nicaragua's 40 volcanoes, many of which are still active, have sometimes devastated settlements but also have enriched the land with layers of fertile ash. The geologic activity that produces vulcanism also breeds powerful earthquakes. Tremors occur regularly throughout the Pacific zone, and earthquakes have nearly destroyed the capital city, Managua, more than once.

Most of the Pacific zone is "tierra caliente", the "hot land" of tropical Spanish America at elevations under . Temperatures remain virtually constant throughout the year, with highs ranging between . After a dry season lasting from November to April, rains begin in May and continue to October, giving the Pacific lowlands of precipitation. Good soils and a favourable climate combine to make western Nicaragua the country's economic and demographic centre. The southwestern shore of Lake Nicaragua lies within of the Pacific Ocean. Thus the lake and the San Juan River were often proposed in the 19th century as the longest part of a canal route across the Central American isthmus. Canal proposals were periodically revived in the 20th and 21st centuries. Roughly a century after the opening of the Panama Canal, the prospect of a Nicaraguan ecocanal remains a topic of interest.

In addition to its beach and resort communities, the Pacific lowlands contains most of Nicaragua's Spanish colonial architecture and artifacts. Cities such as León and Granada abound in colonial architecture; founded in 1524, Granada is the oldest colonial city in the Americas.

Northern Nicaragua is the most diversified region producing coffee, cattle, milk products, vegetables, wood, gold, and flowers. Its extensive forests, rivers and geography are suited for ecotourism.

The central highlands are a significantly less populated and economically developed area in the north, between Lake Nicaragua and the Caribbean. Forming the country's tierra templada, or "temperate land", at elevations between , the highlands enjoy mild temperatures with daily highs of . This region has a longer, wetter rainy season than the Pacific lowlands, making erosion a problem on its steep slopes. Rugged terrain, poor soils, and low population density characterize the area as a whole, but the northwestern valleys are fertile and well settled.

The area has a cooler climate than the Pacific lowlands. About a quarter of the country's agriculture takes place in this region, with coffee grown on the higher slopes. Oaks, pines, moss, ferns and orchids are abundant in the cloud forests of the region.

Bird life in the forests of the central region includes resplendent quetzals, goldfinches, hummingbirds, jays and toucanets.

This large rainforest region is irrigated by several large rivers and is sparsely populated. The area has 57% of the territory of the nation and most of its mineral resources. It has been heavily exploited, but much natural diversity remains. The Rio Coco is the largest river in Central America; it forms the border with Honduras. The Caribbean coastline is much more sinuous than its generally straight Pacific counterpart; lagoons and deltas make it very irregular.

Nicaragua's Bosawás Biosphere Reserve is in the Atlantic lowlands, part of which is located in the municipality of Siuna; it protects of La Mosquitia forest – almost 7% of the country's area – making it the largest rainforest north of the Amazon in Brazil.

The municipalities of Siuna, Rosita, and Bonanza, known as the "Mining Triangle", are located in the region known as the RAAN, in the Caribbean lowlands. Bonanza still contains an active gold mine owned by HEMCO. Siuna and Rosita do not have active mines but panning for gold is still very common in the region.

Nicaragua's tropical east coast is very different from the rest of the country. The climate is predominantly tropical, with high temperature and high humidity. Around the area's principal city of Bluefields, English is widely spoken along with the official Spanish. The population more closely resembles that found in many typical Caribbean ports than the rest of Nicaragua.

A great variety of birds can be observed including eagles, turkeys, toucans, parakeets and macaws. Animal life in the area includes different species of monkeys, anteaters, white-tailed deer and tapirs.

Nicaragua is home to a rich variety of plants and animals. Nicaragua is located in the middle of the Americas and this privileged location has enabled the country to serve as host to a great biodiversity. This factor, along with the weather and light altitudinal variations, allows the country to harbor 248 species of amphibians and reptiles, 183 species of mammals, 705 bird species, 640 fish species, and about 5,796 species of plants.

The region of great forests is located on the eastern side of the country. Rainforests are found in the Río San Juan Department and in the autonomous regions of RAAN and RAAS. This biome groups together the greatest biodiversity in the country and is largely protected by the Indio Maiz Biological Reserve in the south and the Bosawás Biosphere Reserve in the north. The Nicaraguan jungles, which represent about 2.4 million acres, are considered the lungs of Central America and comprise the second largest-sized rainforest of the Americas.

There are currently 78 protected areas in Nicaragua, covering more than , or about 17% of its landmass. These include wildlife refuges and nature reserves that shelter a wide range of ecosystems. There are more than 1,400 animal species classified thus far in Nicaragua. Some 12,000 species of plants have been classified thus far in Nicaragua, with an estimated 5,000 species not yet classified.

The bull shark is a species of shark that can survive for an extended period of time in fresh water. It can be found in Lake Nicaragua and the San Juan River, where it is often referred to as the "Nicaragua shark". Nicaragua has recently banned freshwater fishing of the Nicaragua shark and the sawfish in response to the declining populations of these animals.

Nicaragua was one of the few countries that did not enter an INDC at COP21. Nicaragua initially chose not to join the Paris Climate Accord because it felt that "much more action is required" by individual countries on restricting global temperature rise. However, in October 2017, Nicaragua made the decision to join the agreement. It ratified this agreement on November 22, 2017.

Politics of Nicaragua takes place in a framework of a presidential representative democratic republic, whereby the President of Nicaragua is both head of state and head of government, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in both the government and the national assembly. The judiciary is independent of the executive and the legislature.

Between 2007 and 2009, Nicaragua's major political parties discussed the possibility of going from a presidential system to a parliamentary system. Their reason: there would be a clear differentiation between the head of government (prime minister) and the head of state (president). Nevertheless, it was later argued that the true reason behind this proposal was to find a legal way for President Ortega to stay in power after January 2012, when his second and last government period was expected to end. Ortega was reelected to a third term in November 2016.

Nicaragua pursues an independent foreign policy. Nicaragua is in territorial disputes with Colombia over the Archipelago de San Andres y Providencia and Quita Sueno Bank and with Costa Rica over a boundary dispute involving the San Juan River.

The armed forces of Nicaragua consists of various military contingents. Nicaragua has an army, navy and an air force. There are roughly 14,000 active duty personnel, which is much less compared to the numbers seen during the Nicaraguan Revolution. Although the army has had a rough military history, a portion of its forces, which were known as the national guard, became integrated with what is now the National Police of Nicaragua. In essence, the police became a "gendarmerie". The National Police of Nicaragua are rarely, if ever, labeled as a "gendarmerie". The other elements and manpower that were not devoted to the national police were sent over to cultivate the new Army of Nicaragua.

The age to serve in the armed forces is 17 and conscription is not imminent. , the military budget was roughly 0.7% of Nicaragua's expenditures.

The National Police of Nicaragua Force (in Spanish: La Policía Nacional Nicaragüense) is the national police of Nicaragua. The force is in charge of regular police functions and, at times, works in conjunction with the Nicaraguan military, making it an indirect and rather subtle version of a gendarmerie. However, the Nicaraguan National Police work separately and have a different established set of norms than the nation's military.

Nicaragua is the safest country in Central America and one of the safest in Latin America, according to the United Nations Development Program, with a homicide rate of 8.7 per 100,000 inhabitants.

Nicaragua is a unitary republic. For administrative purposes it is divided into 15 departments ("departamentos") and two self-governing regions (autonomous communities) based on the Spanish model. The departments are then subdivided into 153 municipios (municipalities). The two autonomous regions are the North Caribbean Coast Autonomous Region
and South Caribbean Coast Autonomous Region, often referred to as RACCN and RACCS, respectively.

Nicaragua is among the poorest countries in the Americas. Its gross domestic product (GDP) in purchasing power parity (PPP) in 2008 was estimated at $17.37 billion USD. Agriculture represents 17% of GDP, the highest percentage in Central America. Remittances account for over 15% of the Nicaraguan GDP. Close to one billion dollars are sent to the country by Nicaraguans living abroad. The economy grew at a rate of about 4% in 2011.

According to the United Nations Development Programme, 48% of the population of Nicaragua live below the poverty line, 79.9% of the population live with less than $2 per day, According to UN figures, 80% of the indigenous people (who make up 5% of the population) live on less than $1 per day.

According to the World Bank, Nicaragua ranked as the 123rd out of 190 best economy for starting a business. In 2007, Nicaragua's economy was labelled "62.7% free" by the Heritage Foundation, with high levels of fiscal, government, labor, investment, financial, and trade freedom. It ranked as the 61st freest economy, and 14th (of 29) in the Americas.

In March 2007, Poland and Nicaragua signed an agreement to write off 30.6 million dollars which was borrowed by the Nicaraguan government in the 1980s. Inflation reduced from 33,500% in 1988 to 9.45% in 2006, and the foreign debt was cut in half.
Nicaragua is primarily an agricultural country; agriculture constitutes 60% of its total exports which annually yield approximately US $300 million. Nearly two-thirds of the coffee crop comes from the northern part of the central highlands, in the area north and east of the town of Estelí. Tobacco, grown in the same northern highlands region as coffee, has become an increasingly important cash crop since the 1990s, with annual exports of leaf and cigars in the neighborhood of $200 million per year. Soil erosion and pollution from the heavy use of pesticides have become serious concerns in the cotton district. Yields and exports have both been declining since 1985. Today most of Nicaragua's bananas are grown in the northwestern part of the country near the port of Corinto; sugarcane is also grown in the same district. Cassava, a root crop somewhat similar to the potato, is an important food in tropical regions. Cassava is also the main ingredient in tapioca pudding. Nicaragua's agricultural sector has benefited because of the country's strong ties to Venezuela. It is estimated that Venezuela will import approximately $200 million in agricultural goods. In the 1990s, the government initiated efforts to diversify agriculture. Some of the new export-oriented crops were peanuts, sesame, melons, and onions.

Fishing boats on the Caribbean side bring shrimp as well as lobsters into processing plants at Puerto Cabezas, Bluefields, and Laguna de Perlas. A turtle fishery thrived on the Caribbean coast before it collapsed from overexploitation.

Mining is becoming a major industry in Nicaragua, contributing less than 1% of gross domestic product (GDP). Restrictions are being placed on lumbering due to increased environmental concerns about destruction of the rain forests. But lumbering continues despite these obstacles; indeed, a single hardwood tree may be worth thousands of dollars.

During the war between the US-backed Contras and the government of the Sandinistas in the 1980s, much of the country's infrastructure was damaged or destroyed. Transportation throughout the nation is often inadequate. For example, one cannot travel all the way by highway from Managua to the Caribbean coast. The road ends at the town of El Rama. Travelers have to transfer and make the rest of the trip by riverboat down the Río Escondido—a five-hour journey. The Centroamérica power plant on the Tuma River in the Central highlands has been expanded, and other hydroelectric projects have been undertaken to help provide electricity to the nation's newer industries. Nicaragua has long been considered as a possible site for a new sea-level canal that could supplement the Panama Canal.

Nicaragua's minimum wage is among the lowest in the Americas and in the world. Remittances are equivalent to roughly 15% of the country's gross domestic product. Growth in the "maquila" sector slowed in the first decade of the 21st century with rising competition from Asian markets, particularly China. Land is the traditional basis of wealth in Nicaragua, with great fortunes coming from the export of staples such as coffee, cotton, beef, and sugar. Almost all of the upper class and nearly a quarter of the middle class are substantial landowners.

A 1985 government study classified 69.4 percent of the population as poor on the basis that they were unable to satisfy one or more of their basic needs in housing, sanitary services (water, sewage, and garbage collection), education, and employment. The defining standards for this study were very low; housing was considered substandard if it was constructed of discarded materials with dirt floors or if it was occupied by more than four persons per room.

Rural workers are dependent on agricultural wage labor, especially in coffee and cotton. Only a small fraction hold permanent jobs. Most are migrants who follow crops during the harvest period and find other work during the off-season. The "lower" peasants are typically smallholders without sufficient land to sustain a family; they also join the harvest labor force. The "upper" peasants have sufficient resources to be economically independent. They produce enough surplus, beyond their personal needs, to allow them to participate in the national and world markets.

The urban lower class is characterized by the informal sector of the economy. The informal sector consists of small-scale enterprises that utilize traditional technologies and operate outside the legal regime of labor protections and taxation. Workers in the informal sector are self-employed, unsalaried family workers or employees of small-enterprises, and they are generally poor.

Nicaragua's informal sector workers include tinsmiths, mattress makers, seamstresses, bakers, shoemakers, and carpenters; people who take in laundry and ironing or prepare food for sale in the streets; and thousands of peddlers, owners of small businesses (often operating out of their own homes), and market stall operators. Some work alone, but others labor in the small talleres (workshops/factories) that are responsible for a large share of the country's industrial production. Because informal sector earnings are generally very low, few families can subsist on one income. Like most Latin American nations Nicaragua is also characterized by a very small upper-class, roughly 2% of the population, that is very wealthy and wields the political and economic power in the country that is not in the hands of foreign corporations and private industries. These families are oligarchical in nature and have ruled Nicaragua for generations and their wealth is politically and economically horizontally and vertically integrated.

Nicaragua is currently a member of the Bolivarian Alliance for the Americas, which is also known as ALBA. ALBA has proposed creating a new currency, the Sucre, for use among its members. In essence, this means that the Nicaraguan córdoba will be replaced with the Sucre. Other nations that will follow a similar pattern include: Venezuela, Ecuador, Bolivia, Honduras, Cuba, Saint Vincent and the Grenadines, Dominica and Antigua and Barbuda.

Nicaragua is considering construction of a canal linking the Atlantic to the Pacific Ocean, which President Daniel Ortega has said will give Nicaragua its "economic independence." Scientists have raised concerns about environmental impacts, but the government has maintained that the canal will benefit the country by creating new jobs and potentially increasing its annual growth to an average of 8% per year. The project was scheduled to begin construction in December 2014, however the Nicaragua Canal has yet to be started.

By 2006, tourism had become the second largest industry in Nicaragua. Previously, tourism had grown about 70% nationwide during a period of 7 years, with rates of 10%–16% annually. The increase and growth led to the income from tourism to rise more than 300% over a period of 10 years. The growth in tourism has also positively affected the agricultural, commercial, and finance industries, as well as the construction industry. President Daniel Ortega has stated his intention to use tourism to combat poverty throughout the country. The results for Nicaragua's tourism-driven economy have been significant, with the nation welcoming one million tourists in a calendar year for the first time in its history in 2010.

Every year about 60,000 U.S. citizens visit Nicaragua, primarily business people, tourists, and those visiting relatives. Some 5,300 people from the U.S. reside in Nicaragua. The majority of tourists who visit Nicaragua are from the U.S., Central or South America, and Europe. According to the Ministry of Tourism of Nicaragua (INTUR), the colonial cities of León and Granada are the preferred spots for tourists. Also, the cities of Masaya, Rivas and the likes of San Juan del Sur, El Ostional, the Fortress of the Immaculate Conception, Ometepe Island, the Mombacho volcano, and the Corn Islands among other locations are the main tourist attractions. In addition, ecotourism, sport fishing and surfing attract many tourists to Nicaragua.

According to the "TV Noticias" news program, the main attractions in Nicaragua for tourists are the beaches, the scenic routes, the architecture of cities such as León and Granada, ecotourism, and agritourism particularly in northern Nicaragua. As a result of increased tourism, Nicaragua has seen its foreign direct investment increase by 79.1% from 2007 to 2009.

Nicaragua is referred to as ""the land of lakes and volcanoes"" due to the number of lagoons and lakes, and the chain of volcanoes that runs from the north to the south along the country's Pacific side. Today, only 7 of the 50 volcanoes in Nicaragua are considered active. Many of these volcanoes offer some great possibilities for tourists with activities such as hiking, climbing, camping, and swimming in crater lakes.

The Apoyo Lagoon Natural Reserve was created by the eruption of the Apoyo Volcano about 23,000 years ago, which left a huge 7 km-wide crater that gradually filled with water. It is surrounded by the old crater wall. The rim of the lagoon is lined with restaurants, many of which have kayaks available. Besides exploring the forest around it, many water sports are practiced in the lagoon, most notably kayaking.

Sand skiing has become a popular attraction at the Cerro Negro volcano in León. Both dormant and active volcanoes can be climbed. Some of the most visited volcanoes include the Masaya Volcano, Momotombo, Mombacho, Cosigüina and Ometepe's Maderas and Concepción.
Ecotourism aims to be ecologically and socially conscious; it focuses on local culture, wilderness, and adventure. Nicaragua's ecotourism is growing with every passing year. It boasts a number of ecotourist tours and perfect places for adventurers. Nicaragua has three eco-regions (the Pacific, Central, and Atlantic) which contain volcanoes, tropical rainforests, and agricultural land. The majority of the eco-lodges and other environmentally-focused touristic destinations are found on Ometepe Island, located in the middle of Lake Nicaragua just an hour's boat ride from Granada. While some are foreign-owned, such as the tropical permaculture lodge at Finca El Zopilote, others are owned by local families, like the small but well-acclaimed Finca Samaria.

According to a 2014 research published in the journal "Genetics and Molecular Biology", European ancestry predominates in 69% of Nicaraguans, followed by African ancestry in 20%, and lastly Native American ancestry in 11%. A Japanese research of "Genomic Components in America's demography" demonstrated that, on average, the ancestry of Nicaraguans is 58-62% European, 28% Native American, and 14% African, with a very small Near Eastern contribution. Non-genetic data from the "CIA World Factbook" establish that from Nicaragua's 2016 population of 5,966,798, around 69% are mestizo, 17% white, 5% Native American, and 9% black and other races. This fluctuates with changes in migration patterns. The population is 58% urban .

The capital Managua is the biggest city, with an estimated population of 1,042,641 in 2016. In 2005, over 5 million people lived in the Pacific, Central and North regions, and 700,000 in the Caribbean region.

There is a growing expatriate community, the majority of whom move for business, investment or retirement from across the world, such as from the US, Canada, Taiwan, and European countries; the majority have settled in Managua, Granada and San Juan del Sur.

Many Nicaraguans live abroad, particularly in Costa Rica, the United States, Spain, Canada, and other Central American countries.

Nicaragua has a population growth rate of 1.5% . This is the result of one of the highest birth rates in the Western Hemisphere: 24.9 per 1,000 according to the United Nations for the period 2005–2010. The death rate was 4.7 per 1,000 during the same period according to the United Nations.

The majority of the Nicaraguan population is composed of mestizos, roughly 69%. 17% of Nicaragua's population is of unmixed European stock, with the majority of them being of Spanish descent, while others are of German, Italian, English, Turkish, Danish or French ancestry.

About 9% of Nicaragua's population is black and mainly resides on the country's Caribbean (or Atlantic) coast. The black population is mostly composed of black English-speaking Creoles who are the descendants of escaped or shipwrecked slaves; many carry the name of Scottish settlers who brought slaves with them, such as Campbell, Gordon, Downs and Hodgeson. Although many Creoles supported Somoza because of his close association with the US, they rallied to the Sandinista cause in July 1979 only to reject the revolution soon afterwards in response to a new phase of 'westernization' and imposition of central rule from Managua. There is a smaller number of Garifuna, a people of mixed West African, Carib and Arawak descent. In the mid-1980s, the government divided the Zelaya Department – consisting of the eastern half of the country – into two autonomous regions and granted the black and indigenous people of this region limited self-rule within the republic.
The remaining 5% of Nicaraguans are Native Americans, the descendants of the country's indigenous inhabitants. Nicaragua's pre-Columbian population consisted of many indigenous groups. In the western region, the Nahua (Pipil-Nicarao) people were present along with other groups such as the Chorotega people and the Subtiabas (also known as Maribios or Hokan Xiu). The central region and the Caribbean coast of Nicaragua were inhabited by indigenous peoples who were Macro-Chibchan language groups that had migrated to and from South America in ancient times, primarily what is now Colombia and Venezuela. These groups include the present-day Matagalpas, Miskitos, Ramas, as well as Mayangnas and Ulwas who are also known as Sumos. In the 19th century, there was a substantial indigenous minority, but this group was largely assimilated culturally into the mestizo majority.

Nicaraguan Spanish has many indigenous influences and several distinguishing characteristics. For example, some Nicaraguans have a tendency to replace /s/ with /h/ when speaking. Although Spanish is spoken throughout, the country has great variety: vocabulary, accents and colloquial language can vary between towns and departments.

On the Caribbean coast, indigenous languages, English-based creoles, and Spanish are spoken. The Miskito language, spoken by the Miskito people as a first language and some other indigenous and Afro-descendants people as a second, third, or fourth language, is the most commonly spoken indigenous language. The indigenous Misumalpan languages of Mayangna and Ulwa are spoken by the respective peoples of the same names. Many Miskito, Mayangna, and Ulwa people also speak Miskito Coast Creole, and a large majority also speak Spanish. Fewer than three dozen of nearly 2,000 Rama people speak their Chibchan language fluently, with nearly all Ramas speaking Rama Cay Creole and the vast majority speaking Spanish. Linguists have attempted to document and revitalize the language over the past three decades.

The Garifuna people, descendants of indigenous and Afro-descendant people who came to Nicaragua from Honduras in the early twentieth century, have recently attempted to revitalize their Arawakan language. The majority speak Miskito Coast Creole as their first language and Spanish as their second. The Creole or Kriol people, descendants of enslaved Africans brought to the Mosquito Coast during the British colonial period and European, Chinese, Arab, and British West Indian immigrants, also speak Miskito Coast Creole as their first language and Spanish as their second.

Religion plays a significant part of the culture of Nicaragua and is afforded special protections in the constitution. Religious freedom, which has been guaranteed since 1939, and religious tolerance are promoted by the government and the constitution.

Nicaragua has no official religion. Catholic bishops are expected to lend their authority to important state occasions, and their pronouncements on national issues are closely followed. They can be called upon to mediate between contending parties at moments of political crisis. In 1979, Miguel D'Escoto Brockman, a priest who had embraced Liberation Theology, served in the government as foreign minister when the Sandinistas came to power. The largest denomination, and traditionally the religion of the majority, is the Roman Catholic Church. It came to Nicaragua in the 16th century with the Spanish conquest and remained, until 1939, the established faith.

The number of practicing Roman Catholics have been declining, while members of evangelical Protestant groups and Mormons have been rapidly growing since the 1990s. There is a significant LDS missionary effort in Nicaragua, with two missions, and 95,768 Mormons (1.54% of the population). There are also strong Anglican and Moravian communities on the Caribbean coast in what once constituted the sparsely populated Mosquito Coast colony. It was under British influence for nearly three centuries. Protestantism was brought to the Mosquito Coast mainly by British and German colonists in forms of Anglicanism and the Moravian Church. Other kinds of Protestant and other Christian denominations were introduced to the rest of Nicaragua during the 19th century.

Popular religion revolves around the saints, who are perceived as intercessors (but not mediators) between human beings and God. Most localities, from the capital of Managua to small rural communities, honour patron saints, selected from the Roman Catholic calendar, with annual "fiestas". In many communities, a rich lore has grown up around the celebrations of patron saints, such as Managua's Saint Dominic (Santo Domingo), honoured in August with two colourful, often riotous, day-long processions through the city. The high point of Nicaragua's religious calendar for the masses is neither Christmas nor Easter, but La Purísima, a week of festivities in early December dedicated to the Immaculate Conception, during which elaborate altars to the Virgin Mary are constructed in homes and workplaces.

The country's close political ties have encouraged religious ties. Buddhism has increased with a steady influx of immigration.

Relative to its overall population, Nicaragua has never experienced any large-scale immigrant waves. The number of immigrants to Nicaragua, both originating from other Latin American countries and all other countries, never surpassed 1% of its total population before 1995. The 2005 census showed the foreign-born population at 1.2%, having risen a mere .06% in 10 years.

In the 19th century, Nicaragua experienced modest waves of immigration from Europe. In particular, families from Germany, Italy, Spain, France and Belgium immigrated to Nicaragua, particularly the departments in the Central and Pacific region.

Also present is a small Middle Eastern-Nicaraguan community of Syrians, Armenians, Jewish Nicaraguans, and Lebanese people in Nicaragua with a population of about 30,000. There is an East Asian community mostly consisting of Chinese, Taiwanese, and Japanese. The Chinese Nicaraguan population is estimated at around 12,000. The Chinese arrived in the late 19th century but were unsubstantiated until the 1920s.

The Civil War forced many Nicaraguans to start lives outside of their country. Many people emigrated during the 1990s and the first decade of the 21st century due to the lack of employment opportunities and poverty. The majority of the Nicaraguan Diaspora migrated to the United States and Costa Rica. Today one in six Nicaraguans live in these two countries.

The diaspora has seen Nicaraguans settling around in smaller communities in other parts of the world, particularly Western Europe. Small communities of Nicaraguans are found in France, Germany, Italy, Spain, Norway, Sweden and the United Kingdom. Communities also exist in Australia and New Zealand. Canada, Brazil and Argentina host small groups of these communities. In Asia, Japan hosts a small Nicaraguan community.

Due to extreme poverty at home, many Nicaraguans are now living and working in neighboring El Salvador, a country that has the US dollar as currency.

Although Nicaragua's health outcomes have improved over the past few decades with the efficient utilization of resources relative to other Central American nations, healthcare in Nicaragua still confronts challenges responding to its populations' diverse healthcare needs.

The Nicaraguan government guarantees universal free health care for its citizens. However, limitations of current delivery models and unequal distribution of resources and medical personnel contribute to the persistent lack of quality care in more remote areas of Nicaragua, especially amongst rural communities in the Central and Atlantic region. To respond to the dynamic needs of localities, the government has adopted a decentralized model that emphasizes community-based preventative and primary medical care.

The adult literacy rate in 2005 was 78.0%.

Primary education is free in Nicaragua. A system of private schools exists, many of which are religiously affiliated and often have more robust English programs. As of 1979, the educational system was one of the poorest in Latin America. One of the first acts of the newly elected Sandinista government in 1980 was an extensive and successful literacy campaign, using secondary school students, university students and teachers as volunteer teachers: it reduced the overall illiteracy rate from 50.3% to 12.9% within only five months. This was one of a number of large-scale programs which received international recognition for their gains in literacy, health care, education, childcare, unions, and land reform. The Sandinistas also added a leftist ideological content to the curriculum, which was removed after 1990. In September 1980, UNESCO awarded Nicaragua the Soviet Union sponsored Nadezhda Krupskaya award for the literacy campaign.

The majority of higher education institutions are in Managua. Nicaragua's higher education system consists of 48 universities, and 113 colleges and technical institutes in the areas of electronics, computer systems and sciences, agroforestry, construction and trade-related services. In 2005, almost 400,000 (7%) of Nicaraguans held a university degree. Nicaragua also has several more specialized institutions, with a focus on education that will promote economic development.

Nicaraguan culture has strong folklore, music and religious traditions, deeply influenced by European culture but also including Native American sounds and flavors. Nicaraguan culture can further be defined in several distinct strands. The Pacific coast has strong folklore, music and religious traditions, deeply influenced by Europeans. It was colonized by Spain and has a similar culture to other Spanish-speaking Latin American countries. The indigenous groups that historically inhabited the Pacific coast have largely been assimilated into the mestizo culture.

The Caribbean coast of Nicaragua was once a British protectorate. English is still predominant in this region and spoken domestically along with Spanish and indigenous languages. Its culture is similar to that of Caribbean nations that were or are British possessions, such as Jamaica, Belize, the Cayman Islands, etc. Unlike on the west coast, the indigenous peoples of the Caribbean coast have maintained distinct identities, and some still speak their native languages as first languages.

Nicaraguan music is a mixture of indigenous and Spanish influences. Musical instruments include the marimba and others common across Central America. The marimba of Nicaragua is played by a sitting performer holding the instrument on his knees. He is usually accompanied by a bass fiddle, guitar and guitarrilla (a small guitar like a mandolin). This music is played at social functions as a sort of background music.

The marimba is made with hardwood plates placed over bamboo or metal tubes of varying lengths. It is played with two or four hammers. The Caribbean coast of Nicaragua is known for a lively, sensual form of dance music called "Palo de Mayo" which is popular throughout the country. It is especially loud and celebrated during the Palo de Mayo festival in May. The Garifuna community (Afro-Native American) is known for its popular music called "Punta".
Nicaragua enjoys a variety of international influence in the music arena. Bachata, Merengue, Salsa and Cumbia have gained prominence in cultural centres such as Managua, Leon and Granada. Cumbia dancing has grown popular with the introduction of Nicaraguan artists, including Gustavo Leyton, on Ometepe Island and in Managua. Salsa dancing has become extremely popular in Managua's nightclubs. With various influences, the form of salsa dancing varies in Nicaragua. New York style and Cuban Salsa (Salsa Casino) elements have gained popularity across the country.

Dance in Nicaragua varies depending upon the region. Rural areas tend to have a stronger focus on movement of the hips and turns. The dance style in cities focuses primarily on more sophisticated footwork in addition to movement and turns. Combinations of styles from the Dominican Republic and the United States can be found throughout Nicaragua. Bachata dancing is popular in Nicaragua. A considerable amount of Bachata dancing influence comes from Nicaraguans living abroad, in cities that include Miami, Los Angeles and, to a much lesser extent, New York City. Tango has also surfaced recently in cultural cities and ballroom dance occasions.

The origin of Nicaraguan literature can arguably be traced to pre-Columbian times. The myths and oral literature formed the cosmogenic view of the world of the indigenous people. Some of these stories are still known in Nicaragua. Like many Latin American countries, the Spanish conquerors have had the most effect on both the culture and the literature. Nicaraguan literature has historically been an important source of poetry in the Spanish-speaking world, with internationally renowned contributors such as Rubén Darío who is regarded as the most important literary figure in Nicaragua. He is called the "Father of Modernism" for leading the "modernismo" literary movement at the end of the 19th century. Other literary figures include Carlos Martinez Rivas, Pablo Antonio Cuadra, Alberto Cuadra Mejia, Manolo Cuadra, Pablo Alberto Cuadra Arguello, Orlando Cuadra Downing, Alfredo Alegría Rosales, Sergio Ramirez Mercado, Ernesto Cardenal, Gioconda Belli, Claribel Alegría and José Coronel Urtecho, among others.

The satirical drama "El Güegüense" was the first literary work of post-Columbian Nicaragua. Written in both Aztec Nahuatl and Spanish it is regarded as one of Latin America's most distinctive colonial-era expressions and as Nicaragua's signature folkloric masterpiece, a work of resistance to Spanish colonialism that combined music, dance and theatre. The theatrical play was written by an anonymous author in the 16th century, making it one of the oldest indigenous theatrical/dance works of the Western Hemisphere. In 2005 it was recognized by UNESCO as "a patrimony of humanity". After centuries of popular performance, the play was first published in a book in 1942.

Nicaraguan cuisine is a mixture of Spanish food and dishes of a pre-Columbian origin. Traditional cuisine changes from the Pacific to the Caribbean coast. The Pacific coast's main staple revolves around local fruits and corn, the Caribbean coast cuisine makes use of seafood and the coconut.
As in many other Latin American countries, maize is a staple food and is used in many of the widely consumed dishes, such as the nacatamal, and "indio viejo". Maize is also an ingredient for drinks such as pinolillo and chicha as well as sweets and desserts. In addition to corn, rice and beans are eaten very often.

Gallo pinto, Nicaragua's national dish, is made with white rice and red beans that are cooked individually and then fried together. The dish has several variations including the addition of coconut milk and/or grated coconut on the Caribbean coast. Most Nicaraguans begin their day with Gallopinto. Gallopinto is most usually served with "carne asada", a salad, fried cheese, plantains or maduros.

Many of Nicaragua's dishes include indigenous fruits and vegetables such as jocote, mango, papaya, tamarindo, pipian, banana, avocado, yuca, and herbs such as cilantro, oregano and achiote.

Traditional street food snacks found in Nicaragua include "quesillo", a thick tortilla with soft cheese and cream, "tajadas", deep-fried plantain chips, "maduros", sautéed ripe plantain, and "fresco", fresh juices such as hibiscus and tamarind commonly served in a plastic bag with a straw.

Nicaraguans have been known to eat guinea pigs, known as "cuy". Tapirs, iguanas, turtle eggs, armadillos and boas are also sometimes eaten, but because of extinction threats to these wild creatures, there are efforts to curb this custom.

For most Nicaraguans radio and TV are the main sources of news. There are more than 100 radio stations and several TV networks. Cable TV is available in most urban areas.

The Nicaraguan print media are varied and partisan, representing pro and anti-government positions. Publications include La Prensa, El Nuevo Diario, Confidencial, Hoy, and Mercurio. Online news publications include Confidencial and The Nicaragua Dispatch.

Baseball is the most popular sport in Nicaragua. Although some professional Nicaraguan baseball teams have recently folded, the country still enjoys a strong tradition of American-style baseball.

Baseball was introduced to Nicaragua during the 19th century. In the Caribbean coast, locals from Bluefields were taught how to play baseball in 1888 by Albert Addlesberg, a retailer from the United States. Baseball did not catch on in the Pacific coast until 1891 when a group of mostly college students from the United States formed "La Sociedad de Recreo" (Society of Recreation) where they played various sports, baseball being the most popular.

Nicaragua has had its share of MLB players, including shortstop Everth Cabrera and pitcher Vicente Padilla, but the most notable is Dennis Martínez, who was the first baseball player from Nicaragua to play in Major League Baseball. He became the first Latin-born pitcher to throw a perfect game, and the 13th in the major league history, when he played with the Montreal Expos against the Dodgers at Dodger Stadium in 1991.

Boxing is the second most popular sport in Nicaragua. The country has had world champions such as Alexis Argüello and Ricardo Mayorga as well as Román González. Recently, football has gained popularity. The Dennis Martínez National Stadium has served as a venue for both baseball and football. The first ever national football-only stadium in Managua, the Nicaragua National Football Stadium, was completed in 2011.




</doc>
<doc id="21363" url="https://en.wikipedia.org/wiki?curid=21363" title="History of Nicaragua">
History of Nicaragua

Nicaragua is the third least densely populated nation in Central America, with a demographic similar in size to its smaller neighbors. It is located about midway between Mexico and Colombia, bordered by Honduras to the north and Costa Rica to the south. Nicaragua ranges from the Caribbean Sea on the nation's east coast, and the Pacific Ocean bordering the west. Nicaragua also possesses a series of islands and cays located in the Caribbean Sea.

Nicaragua's name is derived from Nicarao, the name of the Nahuatl-speaking tribe which inhabited the shores of Lake Nicaragua before the Spanish conquest of the Americas, and the Spanish word 'Agua', meaning water, due to the presence of the large Lake Cocibolca (or Lake Nicaragua) and Lake Managua (or Lake Xolotlán), as well as lagoons and rivers in the region.

The people migrated from Central Mexico after 500 CE.

Most of Nicaragua's Caribbean lowlands area was inhabited by tribes that migrated north from what is now Colombia. The various dialects and languages in this area are related to Chibcha, spoken by groups in northern Colombia. Eastern Nicaragua's population consisted of extended families or tribes. Food was obtained by hunting, fishing, and slash-and-burn agriculture. Crops like cassava and pineapples were the staple foods. The people of eastern Nicaragua appear to have traded with and been influenced by the native peoples of the Caribbean, as round thatched huts and canoes, both typical of the Caribbean, were common in eastern Nicaragua.

When the Spanish arrived in western Nicaragua in the early 16th century, they found three principal tribes, each with a different culture and language: the Niquirano, the Chorotegano, and the Chontal. Each one of these diverse groups occupied much of Nicaragua territory, with independent chieftains who ruled according to each group's laws and customs. Their weapons consisted of swords, lances, and arrows made out of wood. Monarchy was the form of government of most tribes; the supreme ruler was the chief, or cacique, who, surrounded by his princes, formed the nobility. Laws and regulations were disseminated by royal messengers who visited each township and assembled the inhabitants to give their chief's orders.

Occupying the territory between Lake Nicaragua and the Pacific Coast, the Niquirano were governed by chief Nicarao, or Nicaragua, a rich ruler who lived in Nicaraocali, now the city of Rivas. The Chorotegano lived in the central region. These two groups had intimate contact with the Spanish conquerors, paving the way for the racial mix of native and European stock now known as mestizos. The Chontal (which means foreigner in Nahua) occupied the central mountain region. This group was smaller than the other two, and it is not known when they first settled in Nicaragua.

In the west and highland areas where the Spanish settled, the indigenous population was almost completely wiped out by the rapid spread of new diseases brought by the Spaniards, for which the native population had no immunity, and the virtual enslavement of the remainder of the indigenous people. In the east where the Europeans did not settle most indigenous groups survived. The English introduced guns and ammunition to one of the local peoples, the Bawihka, who lived in northeast Nicaragua. The Bawihka later intermarried with runaway slaves from Britain's Caribbean possessions, and the resulting population, with its access to superior weapons, began to expand its territory and push other indigenous groups into the interior. This Afro-indigenous group became known to the Europeans as Miskito, and the displaced survivors of their expansionist activities were called the Sumu.

Nicaragua was first "discovered" by Europeans when Christopher Columbus invaded from Honduras and explored the eastern coast on his fourth voyage in 1502.

In 1522, the first Spaniards entered the region of what would become known as Nicaragua. Gil González Dávila with a small force reached its western portion after a trek through Costa Rica. He proceeded to explore the fertile western valleys and was impressed with the Indian civilization he found there. He and his small army gathered gold and baptized Indians along the way. Eventually, they became so imposed upon the Indians that they were attacked and nearly annihilated. González Dávila returned to his expedition's starting point in Panama and reported on his find, naming the area "Nicaragua". However, governor Pedrarias Dávila attempted to arrest him and confiscate his treasure. He was forced to flee to Santo Domingo to outfit another expedition.

Within a few months, Nicaragua was invaded by several Spanish forces, each led by a conquistador. González Dávila was authorized by royal decree and came in from the Caribbean coast of Honduras. Francisco Hernández de Córdoba at the command of the governor of Panama approached from Costa Rica. Pedro de Alvarado and Cristóbal de Olid at the command of Hernán Cortés, came from Guatemala through San Salvador and Honduras.

Córdoba apparently came with the intention of colonization. In 1524, he established permanent settlements in the region, including two of Nicaragua's principal towns: Granada on Lake Nicaragua and León west of Lake Managua. But he soon found it necessary to prepare defenses for the cities and go on the offensive against incursions by the other conquistadores.

The inevitable clash between the Spanish forces devastated the indigenous population. The Indian civilization was destroyed. The series of battles came to be known as "The War of the Captains". By 1529, the conquest of Nicaragua was complete. Several conquistadores came out winners, and some were executed or murdered. Pedrarias Dávila was one such winner. Although he lost control of Panama, he moved to Nicaragua and established his base in León.

The land was parceled out to the conquistadores. The area of most interest was the western portion. It included a wide, fertile valley with huge, freshwater lakes, a series of volcanoes, and volcanic lagoons. Many Indians were soon enslaved to develop and maintain "estates" there. Others were put to work in mines in northern Nicaragua, but the great majority were sent as slaves to Panama and Peru, for significant profit to the new landed aristocracy. Many Indians died through disease and neglect by the Spaniards, who controlled everything necessary for their subsistence.

In 1538, the Viceroyalty of New Spain was established, encompassing all of Mexico and Central America, except Panama. By 1570, the southern part of New Spain was designated the Captaincy General of Guatemala. The area of Nicaragua was divided into administrative "parties" with León as the capital. In 1610, the volcano known as Momotombo erupted, destroying the capital. It was rebuilt northwest of its original site.

The history of Nicaragua remained relatively static for three hundred years following the conquest. There were minor civil wars and rebellions, but they were quickly suppressed. The region was subject to frequent raids by Dutch, French and British pirates, with the city of Granada being invaded twice, in 1658 and 1660.

Nicaragua became a part of the First Mexican Empire in 1821, was a part of the United Provinces of Central America in 1823, and then became an independent republic in its own right in 1838. The Mosquito Coast based on Bluefields on the Atlantic was claimed by the United Kingdom as a protectorate from 1655 to 1850. This area was designated to Honduras in 1859 and transferred to Nicaragua in 1860, though it remained until 1894.

Much of Nicaragua's politics since independence has been characterized by the rivalry between the liberal elite of León and the conservative elite of Granada. The rivalry often degenerated into civil war, particularly during the 1840s and 1850s. Initially invited by the Liberals in 1855 to join their struggle against the Conservatives, a United States adventurer named William Walker declared himself President in 1856, and made English the official language. (See Walker affair.) Honduras and other Central American countries united to drive him out of Nicaragua in 1857, after which a period of three decades of Conservative rule ensued.

Taking advantage of divisions within the conservative ranks, José Santos Zelaya led a liberal revolt that brought him to power in 1893. Zelaya ended the longstanding dispute with the United Kingdom over the Atlantic coast in 1894, and "reincorporated" the Mosquito Coast into Nicaragua.

In 1909, the United States provided political support to conservative-led forces rebelling against President Zelaya. U.S. motives included differences over the proposed Nicaragua Canal, Nicaragua's potential as a destabilizing influence in the region, and Zelaya's attempts to regulate foreign access to Nicaraguan natural resources. On November 17, 1909, two Americans were executed by order of Zelaya after the two men confessed to having laid a mine in the San Juan River with the intention of blowing up the "Diamante". The U.S. justified the intervention by claiming to protect U.S. lives and property. Zelaya resigned later that year.

In August 1912, the President of Nicaragua, Adolfo Díaz, requested the resignation of the Secretary of War, General Luis Mena, concerned that Díaz was leading an insurrection, fled Managua with his brother, the Chief of Police of Managua, and the insurrection escalated. When the U.S. Legation asked President Adolfo Díaz to ensure the safety of American citizens and property during the insurrection, Díaz replied that he could not and that... In consequence my Government desires that the Government of the United States guarantee with its forces security for the property of American Citizens in Nicaragua and that it extend its protection to all the inhabitants of the Republic.United States Marines were stationed in Nicaragua from 1912 to 1933, except for a nine-month period beginning in 1925. From 1910 to 1926, the conservative party ruled Nicaragua. The Chamorro family, which had long dominated the party, effectively controlled the government during that period. In 1914, the Bryan-Chamorro Treaty was signed, giving the U.S. control over the proposed canal, as well as leases for potential canal defenses.

Following the evacuation of U.S. Immigrants in 1925, another violent conflict between liberals and conservatives known as the Constitutionalist War took place in 1926, when Liberal soldiers in the Caribbean port of Puerto Cabezas revolted against Conservative President Adolfo Díaz, recently installed as a result of United States pressure following a coup. The leader of this revolt, Gen. José María Moncada, declared that he supported the claim of exiled Liberal vice-president Juan Bautista Sacasa, who arrived in Puerto Cabezas in December, declaring himself president of a "constitutional" government. The U.S., using the threat of military intervention, forced the Liberal generals to agree to a cease-fire.

On May 4, 1927, representatives from the two warring factions signed the Espino Negro accord, negotiated by Henry Stimson, appointed by U.S. President Calvin Coolidge as a special envoy to Nicaragua. Under the terms of the accord, both sides agreed to disarm, Díaz would be allowed to finish his term and a new national army would be established, the Guardia Nacional (National Guard), with U.S. soldiers remaining in the country to supervise the upcoming November presidential election. Later, a battalion of the U.S army under the command of Gen. Logan Feland arrived to enforce the agreement.

From 1927 until 1933, General Augusto César Sandino who rejected the negotiated agreement led a sustained guerrilla war, first against the Conservative regime and subsequently against the U.S. Marines, who withdrew upon the establishment of a new Liberal government. When the Americans left in 1933 as a result of Sandino's guerrilla war and the Great Depression, they set up the Guardia Nacional (National Guard), a combined military and police force trained and equipped by the Americans, designed to be loyal to U.S. interests. Anastasio Somoza García, a close friend of the American government, was put in charge. He was one of the three rulers of the country, the others being Sandino and the mostly figurehead President Juan Bautista Sacasa.

The Nicaraguan Campaign Medal, a decoration of the United States Navy, was later issued for those American service members who had performed military duty in Nicaragua during the early years of the 20th century.

With U.S. support, Anastasio Somoza García outmaneuvered his political opponents, including Sandino (who was executed by National Guard officers in February 1934), and took over the presidency in 1936. The Somoza family would rule until 1979.

The earliest opposition to Somoza came from the educated middle class and the normally conservative wealthy, such as Pedro Joaquín Chamorro. On September 21, 1956, Rigoberto López Pérez sneaked into a party attended by the President and shot him in the chest. In his memoirs "Nicaragua Betrayed", Anastasio Debayle (Somoza's son) claims that Chamorro had knowledge of the assassination plot. While the assassin quickly died in a hail of gunfire, Somoza himself died a few days later, in an American hospital in the Panama Canal Zone.

Divisions within the Conservative Party in the 1932 elections paved the way for the Liberal Juan Bautista Sacasa to assume power. This initiated an inherently weak presidency—hardly a formidable obstacle to Somoza as he set about building his personal influence over Congress and the ruling Liberal Party. President Sacasa's popularity decreased as a result of his poor leadership and accusations of fraud in the 1934 congressional elections. Somoza García benefited from Sacasa's diminishing power, and at the same time brought together the National Guard and the Liberal Party (Partido Liberal-PL) in order to win the presidential elections in 1936. Somoza Garcia also cultivated support from former presidents Moncada and Chamorro while consolidating control within the Liberal Party.

Early in 1936, Somoza openly confronted President Sacasa by using military force to displace local government officials loyal to the president and replacing them with close associates. Somoza García's increasing military confrontation led to Sacasa's resignation on June 6, 1936. The Congress appointed Carlos Brenes Jarquín, a Somoza García associate, as interim president and postponed presidential elections until December. In November, Somoza resigned as chief director of the National Guard, thus complying with constitutional requirements for eligibility to run for the presidency. The Liberal Nationalist Party (Partido Liberal Nacionalista—PLN) was established with support from a faction of the Conservative Party to support Somoza Garcia's candidacy. Somoza was elected president in the December election by the remarkable margin of 107,201 votes to 108. On January 1, 1937, he resumed control of the National Guard, combining the roles of president and chief director of the military.

After Somoza's win in the December 1936 presidential elections, he proceeded to consolidate his power within the National Guard, while at the same time dividing his political opponents. Family members and close associates were given key positions within the government and the military. The Somoza family also controlled the PLN, which in turn controlled the legislature and judicial system, thus giving Somoza absolute power over every sphere of Nicaraguan politics. Nominal political opposition was allowed as long as it did not threaten the ruling elite. Somoza Garcia's National Guard repressed serious political opposition and antigovernment demonstrations. The institutional power of the National Guard grew in most government owned enterprises, until eventually it controlled the national radio and telegraph networks, the postal and immigration services, health services, the internal revenue service, and the national railroads.

In less than two years after his election, Somoza Garcia, defying the Conservative Party, declared his intention to stay in power beyond his presidential term. Thus, in 1938, Somoza Garcia named a Constituent Assembly that gave the president extensive power and elected him for another eight-year term. A Constituent Assembly, extension of the presidential term from four years to six years, and clauses empowering the president to decree laws relating to the National Guard without consulting Congress, ensured Somoza's absolute control over the state and military. Control over electoral and legislative machinery provided the basis for a permanent dictatorship.

Somoza García was succeeded by his two sons. Luis Somoza Debayle became President (29 September 1956 to 1 May 1963), and was effectively dictator of the country until his death, but his brother Anastasio Somoza Debayle held great power as head of the National Guard. A graduate of West Point, Anastasio was even closer to the Americans than his father and was said to speak better English than Spanish.

The revolutionaries opposing the Somozas were greatly strengthened by the Cuban Revolution. The revolution provided both hope and inspiration to the insurgents, as well as weapons and funding. Operating from Costa Rica they formed the "Frente Sandinista de Liberacion Nacional" (FSLN) and came to be known as Sandinistas. They took their name from the still legendary Augusto César Sandino. With aid from the United States, the Somoza brothers succeeded in defeating the guerrillas.

President Luis Somoza Debayle, under pressure from the rebels, announced that national elections would be held in February 1963. Election reforms had been made that established secret ballots and a supervising electoral commission, although the Conservative Party never elected any members of the commission. Somoza had also introduced a constitutional amendment that would prevent family members from succeeding him. The opposition was extremely skeptical of Somoza's promises, and ultimately control of the country passed to Anastasio Somoza Debayle after Luis died of a heart attack in 1967.

Landless peasants worked on large plantations during short harvest seasons and received wages as low as US$1 per day. In desperation, many of these poor laborers migrated east, seeking their own land near the rain forest. In 1968, the World Health Organization found that polluted water led to 17% of all Nicaraguan deaths.

From 1945 to 1960, the U.S.-owned Nicaraguan Long Leaf Pine Company (NIPCO) directly paid the Somoza family millions of dollars in exchange for favorable benefits to the company, such as not having to re-forest clear cut areas. By 1961, NIPCO had cut all of the commercially viable coastal pines in northeast Nicaragua. Expansion of cotton plantations in the 1950s and cattle ranches in the 1960s forced peasant families from the areas they had farmed for decades. Some were forced by the National Guard to relocate into colonization projects in the rainforest.

Some moved eastward into the hills, where they cleared forests in order to plant crops. Soil erosion forced them, however, to abandon their land and move deeper into the rainforest. Cattle ranchers then claimed the abandoned land. Peasants and ranchers continued this movement deep into the rain forest. By the early 1970s, Nicaragua had become the United States' top beef supplier. The beef supported fast-food chains and pet food production. President Anastasio Somoza Debayle owned the largest slaughterhouse in Nicaragua, as well as six meat-packing plants in Miami, Florida.

Also in the 1950s and 1960s, 40% of all U.S. pesticide exports went to Central America. Nicaragua and its neighbors widely used compounds banned in the U.S., such as DDT, endrin, dieldrin and lindane. In 1977 a study revealed that mothers living in León had 45 times more DDT in their breast milk than the World Health Organization safe level.

A major turning point was the December 1972 Managua earthquake that killed over 10,000 people and left 500,000 homeless. A great deal of international relief was sent to the nation. Violent opposition to the government, especially to its widespread corruption, was then renewed with the Sandinistas being revived. The Sandinistas received some support from Cuba and the Soviet Union.

On 27 December 1974, a group of nine FSLN guerrillas invaded a party at the home of a former Minister of Agriculture, killing him and three guards in the process of taking several leading government officials and prominent businessmen hostage. In return for the hostages they succeeded in getting the government to pay US$2 million ransom, broadcast an FSLN declaration on the radio and in the opposition newspaper "La Prensa", release fourteen FSLN members from jail, and fly the raiders and the released FSLN members to Cuba. Archbishop Miguel Obando y Bravo acted as an intermediary during the negotiations.

The incident humiliated the government and greatly enhanced the prestige of the FSLN. Somoza, in his memoirs, refers to this action as the beginning of a sharp escalation in terms of Sandinista attacks and government reprisals. Martial law was declared in 1975, and the National Guard began to raze villages in the jungle suspected of supporting the rebels. Human rights groups condemned the actions, but U.S. President Gerald Ford refused to break the U.S. alliance with Somoza.

The country tipped into full-scale civil war with the 1978 murder of Pedro Chamorro, who had opposed violence against the regime. 50,000 turned out for his funeral. It was assumed by many that Somoza had ordered his assassination (evidence implicated Somoza's son and other members of the National Guard). A nationwide strike, including labour and private businesses, commenced in protest, demanding an end to the dictatorship. At the same time, the Sandinistas stepped up their rate of guerrilla activity.
Several towns, assisted by Sandinista guerrillas, expelled their National Guard units. Somoza responded with increasing violence and repression. When León became the first city in Nicaragua to fall to the Sandinistas, he responded with aerial bombardment, famously ordering the air force to "bomb everything that moves until it stops moving."

The U.S. media grew increasingly unfavorable in its reporting on the situation in Nicaragua. Realizing that the Somoza dictatorship was unsustainable, the Carter administration attempted to force him to leave Nicaragua. Somoza refused and sought to maintain his power through the National Guard. At that point, the U.S. ambassador sent a cable to the White House saying it would be "ill-advised" to call off the bombing, because such an action would help the Sandinistas gain power. When ABC reporter Bill Stewart was executed by the National Guard, and graphic film of the killing was broadcast on American TV, the American public became more hostile to Somoza. In the end, President Carter refused Somoza further U.S. military aid, believing that the repressive nature of the government had led to popular support for the Sandinista uprising.

In May 1979, another general strike was called, and the FSLN launched a major push to take control of the country. By mid July they had Somoza and the National Guard isolated in Managua.

As Nicaragua's government collapsed and the National Guard commanders escaped with Somoza, the U.S. first promised and then denied them exile in Miami. The rebels advanced on the capital victoriously. On July 19, 1979, a new government was proclaimed under a provisional junta headed by 33-year-old Daniel Ortega and including Violeta Chamorro, Pedro's widow.

The United Nations estimated material damage from the revolutionary war to be US$480 million. The FSLN took over a nation plagued by malnutrition, disease, and pesticide contaminations. Lake Managua was considered dead because of decades of pesticide runoff, toxic chemical pollution from lakeside factories, and untreated sewage. Soil erosion and dust storms were also a problem in Nicaragua at the time due to deforestation. To tackle these crises, the FSLN created the Nicaraguan Institute of Natural Resources and the Environment. It was created 10 years after the National Environmental Policy Act of 1969 in the United States.

The Sandinistas were victorious in the national election of November 4, 1984, gathering 67% of the vote. The election was certified as "free and fair" by the majority of international observers. The Nicaraguan political opposition and the Reagan administration claimed political restrictions were placed on the opposition by the government. The primary opposition candidate was the U.S.-backed Arturo Cruz, who succumbed to pressure from the United States government not to take part in the 1984 elections; later US officials were quoted as saying, "the (Reagan) Administration never contemplated letting Cruz stay in the race, because then the Sandinistas could justifiably claim that the elections were legitimate." Other opposition parties such as the Conservative Democratic Party and the Independent Liberal party, were both free to denounce the Sandinista government and participate in the elections. Ortega was overwhelmingly elected President in 1984, but the long years of war had decimated Nicaragua's economy.
American support for the long rule of the Somoza family had soured relations, and the FSLN government was committed to a Marxist ideology, with many of the leading Sandinista continuing long-standing relationships with the Soviet Union and Cuba. U.S. President Carter initially hoped that continued American aid to the new government would keep the Sandinistas from forming a doctrinaire Marxist-Leninist government aligned with the Soviet bloc, but the Carter administration allotted the Sandinistas minimal funding to start them off, and the Sandinistas resolutely turned away from the U.S., investing Cuban and East European assistance into a new army of 75,000. The buildup included T-55 heavy tanks, heavy artillery and HIND attack helicopters, an unprecedented military buildup that made the Sandinista Army more powerful than all of its neighbors combined. The Soviets also pledged to provide MiG 21 fighters, but, to the annoyance of the Sandinistas, the aircraft were never delivered.

Managua became the second capital in the hemisphere after Cuba to host an embassy from North Korea. Ironically, in light of the tensions between their Soviet sponsors and China, the Sandinistas allowed Taiwan to retain its mission and refused to allow a Chinese mission to enter the country.

The first challenge to the powerful new army came from the Contras, groups of Somoza's National Guard who had fled to Honduras, organized, trained and funded by CIA elements involved in cocaine trafficking in Central America. The Contra chain of command included some ex-National Guardsmen, including Contra founder and commander Enrique Bermúdez and others. One prominent Contra commander, however, was ex-Sandinista hero Edén Pastora, aka "Commadante Zero," who rejected the Leninist orientation of his fellow comandantes. The Contras operated out of camps in the neighboring countries of Honduras to the north and Costa Rica to the south. They engaged in a systematic campaign of terror amongst the rural Nicaraguan population in order to disrupt the social reform projects of the Sandinistas. Several Historians have criticized the contra campaign and the Reagan Administration's support for it, citing the brutality and numerous human rights violations of the Contras. LaRamee and Polakoff, for example, describe the destruction of health centers, schools and cooperatives at the hands of the rebels. Others have contended that large scale murder, rape and torture also occurred in Contra dominated areas. The US also sought to place economic pressure on the Sandinistas, and the Reagan administration imposed a full trade embargo.

With the election of Ronald Reagan in 1980, relations between the United States and the Sandinista regime became an active front in the Cold War. The Reagan administration insisted on the "Communist threat" posed by the Sandinistas—reacting particularly to the support provided to the Sandinistas by Cuban president Fidel Castro, by the Sandinistas' close military relations with the Soviets and Cubans, but also furthering the Reagan administration's desire to protect U.S. interests in the region, which were threatened by the policies of the Sandinista government. The United States quickly suspended aid to Nicaragua and expanded the supply of arms and training to the Contra in neighbouring Honduras, as well as allied groups based to the south in Costa Rica. President Reagan called the Contras "the moral equivalent of our founding fathers."

American pressure against the government escalated throughout 1983 and 1984; the Contras began a campaign of economic sabotage and disrupted shipping by planting underwater mines in Nicaragua's Port of Corinto, an action condemned by the International Court of Justice as illegal. The U.S. refused to pay restitution and claimed that the ICJ was not competent to judge the case. The United Nations General Assembly passed a resolution in order to pressure the U.S. to pay the fine. Although only Israel and El Salvador, which was receiving massive amounts of military aid to fight its own guerrilla insurgency, voted with the U.S., the money still has not been paid. Jeane Kirkpatrick, the American ambassador to the UN under Reagan, criticized the Court as a "semi-judicial" body. despite the fact that the U.S. was legally bound by the court's decision, had signed the relevant treaty, and had made use of the court in other cases. On May 1, 1985, Reagan issued an executive order that imposed a full economic embargo on Nicaragua, which remained in force until March 1990.

In 1982, legislation was enacted by US Congress to prohibit further direct aid to the Contras. Reagan's officials attempted to illegally supply them out of the proceeds of arms sales to Iran and third party donations, triggering the Iran-Contra Affair of 1986–87. Mutual exhaustion, Sandinista fears of Contra unity and military success, and mediation by other regional governments led to the Sapoa ceasefire between the Sandinistas and the Contras on March 23, 1988. Subsequent agreements were designed to reintegrate the Contras and their supporters into Nicaraguan society in preparation for general elections.

The FSLN lost to the National Opposition Union by 14 points in elections on February 25, 1990. ABC news had been predicting a 16-point Sandinista victory. At the beginning of Violeta Chamorro's nearly 7 years in office the Sandinistas still largely controlled the army, labor unions, and courts. Her government made moves towards consolidating democratic institutions, advancing national reconciliation, stabilizing the economy, privatizing state-owned enterprises.

In February 1995, Sandinista Popular Army Cmdr. Gen. Humberto Ortega was replaced, in accordance with a new military code enacted in 1994 by Gen. Joaquín Cuadra, who espoused a policy of greater professionalism in the renamed Army of Nicaragua. A new police organization law, passed by the National Assembly and signed into law in August 1996, further codified both civilian control of the police and the professionalization of that law enforcement agency.

The October 20, 1996 presidential, legislative, and mayoral elections also were judged free and fair by international observers and by the groundbreaking national electoral observer group Ética y Transparencia (Ethics and Transparency) despite a number of irregularities, due largely to logistical difficulties and a baroquely complicated electoral law. This time Nicaraguans elected former-Managua Mayor Arnoldo Alemán, leader of the center-right Liberal Alliance, which later consolidated into the Constitutional Liberal Party (PLC). Alemán continued to privatize the economy and promote infrastructure projects such as highways, bridges, and wells, assisted in large part by foreign assistance received after Hurricane Mitch hit Nicaragua in October 1998. His administration was besieged by charges of corruption, resulting in the resignation of several key officials in mid-2000. Alemán himself was subsequently convicted of official corruption and sentenced to twenty years in jail.

In November 2000, Nicaragua held municipal elections. Alemán's PLC won a majority of the overall mayoral races. The FSLN fared considerably better in larger urban areas, winning a significant number of departmental capitals including - Managua.

Presidential and legislative elections were held on November 4, 2001, the country's fourth free and fair election since 1990. Enrique Bolaños of the PLC was elected to the Nicaraguan presidency, defeating the FSLN candidate Daniel Ortega, by 14 percentage points. The elections were characterized by international observers as free, fair and peaceful. Bolaños was inaugurated on January 10, 2002.

In November 2006 the presidential election was won by Daniel Ortega, returned to power after 16 years in opposition. International observers, including the Carter Center, judged the election to be free and fair.

The country partly rebuilt its economy during the 1990s, but was hit hard by Hurricane Mitch at the end of October 1998, almost exactly a decade after the similarly destructive Hurricane Joan and again in 2007 it was hit by Hurricane Felix, a category 5 hurricane. Ten years later, Hurrican Nate also hit Nicaragua and destroyed much of the infrastructure in the countryside, such as communication masts.

In the Nicaraguan general election, 2006 Daniel Ortega gained some 38% of the vote in the single round, thus returning to power for his second term overall. The constitution at the time included a ban on immediate reelection of an incumbent president and on any one individual serving more than two terms as president. That notwithstanding, Ortega ran again and won the Nicaraguan general election, 2011 amid accusations of fraud by losing candidate Fabio Gadea Mantilla. Economic growth during most of those two terms was strong and Tourism in Nicaragua grew especially strongly, in part thanks to the perception of Nicaragua as a safe country to visit. The Nicaraguan general election, 2016 saw a partial electoral boycott by the opposition and again accusations of electoral fraud as well, as accusations that the abstention rate was higher than the one officially published by the government. The Nicaraguan Canal was an issue of public debate and some controversy. Starting 19 April 2018, criticism of the Ortega government over the canal, forest fires in the Indio Maíz nature reserve, and a planned reform of the social security system led to the 2018 Nicaraguan protests to which the government responded with violence and harsh repression.


General:





</doc>
<doc id="21364" url="https://en.wikipedia.org/wiki?curid=21364" title="Geography of Nicaragua">
Geography of Nicaragua

Nicaragua (officially the Republic of Nicaragua ) is a country in Central America, bordering both the Caribbean Sea and the North Pacific Ocean, between Costa Rica and Honduras. Nicaragua is the largest country in Central America. 

Nicaragua covers a total area of 130,370 square kilometers (119,990 square kilometers of which is land area) and contains a variety of climates and terrains. The country's physical geography divides it into three major zones: the Pacific lowlands, the wetter, cooler central highlands, and the Caribbean lowlands.

The natural regions are the following:

The Pacific lowlands extend about 75 kilometers inland from the Pacific coast. Most of the area is flat, except for a line of young volcanoes, many of which are still active, running between the Golfo de Fonseca and Lago de Nicaragua. These peaks lie just west of a large crustal fracture or structural rift that forms a long, narrow depression passing southeast across the isthmus from the Golfo de Fonseca to the Río San Juan. 

The rift is occupied in part by the largest freshwater lakes in Central America: Lago de Managua (56 kilometers long and 24 kilometers wide) and Lago de Nicaragua (about 160 kilometers long and 75 kilometers wide). These two lakes are joined by the Río Tipitapa, which flows south into Lago de Nicaragua. Lago de Nicaragua in turn drains into the Río San Juan (the boundary between Nicaragua and Costa Rica), which flows through the southern part of the rift lowlands to the Caribbean Sea. 

The valley of the Río San Juan forms a natural passageway close to sea level across the Nicaraguan isthmus from the Caribbean Sea to Lago de Nicaragua and the rift. From the southwest edge of Lago de Nicaragua, it is only nineteen kilometers to the Pacific Ocean. This route was considered as a possible alternative to the Panama Canal at various times in the past.

Surrounding the lakes and extending northwest of them along the rift valley to the Golfo de Fonseca are fertile lowland plains highly enriched with volcanic ash from nearby volcanoes. These lowlands are densely populated and well cultivated. More directly west of the lake region is a narrow line of ash-covered hills and volcanoes that separate the lakes from the Pacific Ocean. This line is highest in the central portion near the cities of León and Managua.

Because Western Nicaragua is located where two major tectonic plates collide, it is subject to earthquakes and volcanic eruptions. Although periodic volcanic eruptions have caused agricultural damage from fumes and ash, earthquakes have been by far more destructive to life and property. Hundreds of shocks occur each year, some of which cause severe damage. The capital city of Managua was virtually destroyed in 1931 and again in 1972.

The triangular area known as the central highlands lies northeast and east of the Pacific lowlands. These rugged mountains are composed of ridges 900 to 1,809 meters high and a mixed forest of oak and pine alternating with deep valleys that drain primarily toward the Caribbean. Very few significant streams flow west to the Pacific Ocean. Those that do are steep, short, and flow intermittently. 

The relatively dry western slopes of the central highlands, protected by the ridges of the highlands from the moist winds of the Caribbean, have drawn farmers from the Pacific region since colonial times. The eastern slopes are among the wettest places in the world, being too wet for agriculture, and have an economy dominated by timber extraction.

The eastern Caribbean lowlands of Nicaragua form the extensive and exaggerated (occupying more than 50 percent of national territory) and still sparsely settled lowland area known as the Costa de Mosquitos (Miskito Coast). The Caribbean lowlands are sometimes considered synonymous with the former department of Zelaya, which is now divided into the North Caribbean Coast Autonomous Region (Región Autónoma de la Costa Caribe Norte, RACCN) and the South Caribbean Coast Autonomous Region (Región Autónoma de la Costa Caribe Sur, RACCS) and constitutes about 45 percent of Nicaragua's territory. 

These lowlands are a hot, humid area that includes coastal plains, the eastern spurs of the central highlands, and the lower portion of the Río San Juan basin. The soil is generally leached and infertile. Pine and palm savannas predominate as far south as the Laguna de Perlas. Tropical rain forests are characteristic from the Laguna de Perlas to the Río San Juan, in the interior west of the savannas, and along rivers through the savannas. 

Fertile soils are found only along the natural levees and narrow floodplains of the numerous rivers, including the Escondido, the Río Grande de Matagalpa, the Prinzapolka, and the Coco, and along the many lesser streams that rise in the central highlands and cross the region en route to the complex of shallow bays, lagoons, and salt marshes of the Caribbean coast.

Temperature varies little with the seasons in Nicaragua and is largely a function of elevation. The "hot land" is characteristic of the foothills and lowlands from sea level to about of elevation. At night temperatures drop to most of the year. 

The tierra templada, or the "temperate land," is characteristic of most of the central highlands, where elevations range between .The "cold land" at elevations above , is found only on and near the highest peaks of the central highlands. Daytime averages in this region are , with nighttime lows below .

Rainfall varies greatly in Nicaragua. The Caribbean lowlands are the wettest section of Central America, receiving between of rain annually. The western slopes of the central highlands and the Pacific lowlands receive considerably less annual rainfall, being protected from moisture-laden Caribbean trade winds by the peaks of the central highlands. 

Mean annual precipitation for the rift valley and western slopes of the highlands ranges from . Rainfall is seasonal—May through October is the rainy season, and December through April is the driest period.

During the rainy season, Eastern Nicaragua is subject to heavy flooding along the upper and middle reaches of all major rivers. Near the coast, where river courses widen and river banks and natural levees are low, floodwaters spill over onto the floodplains until large sections of the lowlands become continuous sheets of water. River bank agricultural plots are often heavily damaged, and considerable numbers of savanna animals die during these floods. 

The coast is also subject to destructive tropical storms and hurricanes, particularly from July through October. The high winds and floods, accompanying these storms often cause considerable destruction of property. In addition, heavy rains (called papagayo storms) accompanying the passage of a cold front or a low-pressure area may sweep from the north through both eastern and western Nicaragua (particularly the rift valley) from November through March. 

Hurricanes or heavy rains in the central highlands where agriculture has destroyed much of the natural vegetation also cause considerable crop damage and soil erosion. In 1988 Hurricane Joan forced hundreds of thousands of Nicaraguans to flee their homes and caused more than US$1 billion in damage, most of it along the Caribbean coast.

Although nowhere in Central America is antipodal to another land mass, Nicaragua comes closest, with its Big Corn Island within half a kilometer of being antipodal to the South Island of the Australian Cocos (Keeling) Islands in the Indian Ocean.

Area:
<br>"total:"
130,370 km
<br>"land:"
119,254 km
<br>"water:"
10,380 km

Elevation extremes:
<br>"lowest point:"
Pacific Ocean 0 m
<br>"highest point:"
Mogotón 2,438 m

Land use:
<br>"arable land:"
14.57%
<br>"permanent crops:"
1.76%
<br>"other:"
83.66% (2011.)

Irrigated land:
942.4 km (2011)

"Total renewable water resources:"'
196.6 km (2011)

Nicaragua is subject to destructive earthquakes, volcanoes, landslides, and occasionally severe hurricanes. It currently faces deforestation, soil erosion, and water pollution. It is a party to the United Nations Framework Convention on Climate Change, the Climate Change-Kyoto Protocol, the Nuclear Test Ban, and the Ozone Layer Protection, and has signed but not ratified the Law of the Sea.





</doc>
<doc id="21365" url="https://en.wikipedia.org/wiki?curid=21365" title="Demographics of Nicaragua">
Demographics of Nicaragua

"This article is about the demographic features of the population of Nicaragua, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population."
According to , Nicaragua has a population of . Mestizos (who are a mix of Native American and white) make up about 69% of the population, while whites make up 17%. The remainder of the Nicaraguan population is 9% black, and 5% Native American.

Nicaraguan demographics reflected a different composition prior to the Sandinista revolution of 1979 since most of the migration during the years that followed were primarily of upper or middle class Nicaraguans, a group primarily made up of whites. A growing number of these expats have returned, while many continue to live abroad.

The 42.5% of the population live below to the poverty line, The general poverty rate is estimated of 47.3%, although much of the population lives in the lower middle class because of low salaries and a minimal amount of PIB (US$1000–3000).

The most populous city in the country is the capital city, Managua, with a population of 1.2 million (2005). As of 2005, over 4.4 million inhabitants live in the Pacific, Central and North regions. 2.7 million inhabitants reside in the Pacific region alone, while inhabitants in the Caribbean region only reach an estimated 700,000.

The Census Bureau in Nicaragua is the National Institute of Statistics and Census (INEC). The institution is in charge of completing censuses and surveys. INEC ran its first census in 1906, the last census was taken in 2005, it was the eighth to date.

According to the total population was in , compared to only 1,295,000 in 1950. The proportion of children below the age of 15 in 2010 was 34.5%, 60.9% was between 15 and 65 years of age, while 4.6% was 65 years or older

Structure of the population (01.07.2009) (estimates):

Ninety percent of Nicaraguans live in the Pacific lowlands and the adjacent interior highlands. The population is 54% urban. The most populous city in Nicaragua is the capital city, Managua, with a population of 1.2 million (2005). As of 2005, over 4.4 million inhabitants live in the Pacific, Central and North regions of the country. There are 2.7 million residents in the Pacific region. The Caribbean region has an estimated 700,000 residents. In addition, many Nicaraguans live abroad. 

Registration of vital events is in Nicaragua not complete. The Population Department of the United Nations prepared the following estimates.

Total Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):

Births and deaths

In the 19th century, there had been a substantial indigenous minority, but this group was also largely assimilated culturally into the mestizo majority. Primarily in the 19th century, Nicaragua saw several waves of immigration from other European nations. In particular the northern cities of Estelí, Jinotega and Matagalpa have significant fourth generation Germans. Most of Nicaragua's population lives in the western region of the country in the departments of Managua, Granada and Léon.

According to the 2005 census 443,847 (8.6%) residents consider themselves to belong to an indigenous people or to an ethnic community. The remaining majority of the Nicaraguan population (91.6%) are deemed mestizo and white, with the majority of these being of Spanish, with some German, Italian, Portuguese and French ancestry. Mestizos and whites mainly reside in the western region of the country.

Possibly also a part of the black or Afro-Nicaraguan population, which mainly resides on the country's sparsely populated Caribbean (or Atlantic) coast, is included in the majority population which does not consider itself to belong to an ethnic community. In the 2005 census, there were only 19,890 Creoles (0.4% of the total population). The Creole population is mostly of West Indian (Antillean) origin, the descendants of indentured laborers brought mostly from Jamaica when the region was a British protectorate.

The Garifuna, a people of mixed Carib, Angolan, Congolese and Arawak descent, numbered 3,271 in 2005 (0.1%). 112,253 people considered themselves "Mestizo de la Costa Caribe" (mestizo of the Caribbean coast). In addition to the inhabitants who declared themselves Indigenous or Ethnic community, 13,740 answered "Other". Another 47,473 responded "Not Sure" and an additional 19,460 responded "Ignore".

The Native American population, the unmixed descendants of the country's indigenous inhabitants, numbered 227,760 (4.4% of the total population) in 2005. Nicaragua's pre-Columbian consisted of many indigenous groups. In the western region, the Nahua people (also known as the Pipil-Nicaraos) were present along with other groups such as the Chorotega people.

The central region and the Caribbean coast of Nicaragua were inhabited by indigenous peoples who were mostly Chibcha-related groups that had migrated from South America, primarily present day Colombia and Venezuela. These groups include the Miskitos (120,817 people), Matagalpa (15,240 people), Ramas (4,185 people), Sumos (9,756 people) and Ulwa (698 people). Other indigenous peoples include the Subtiaba (19,949 people) and modern-day Chorotegas who are also known as the Mangue (46,002 people).

In the 19th century, there was still a substantial indigenous minority, but this group was largely assimilated culturally into the mestizo majority. In the mid-1980s, the government divided the department of Zelaya – consisting of the eastern half of the country — into two autonomous regions and granted the black and indigenous people of this region limited self-rule within the Republic.

Relative to its overall population, Nicaragua has never experienced any large scale wave of immigrants. The total number of immigrants to Nicaragua, both originating from other Latin American countries and all other countries, never surpassed 1% of its total population prior to 1995. The 2005 census showed the foreign-born population at 1.2%, having risen a mere .06% in 10 years. However, in the 19th century, Nicaragua received immigrants from Europe, who established many agricultural businesses such as coffee and sugar cane plantations, and also newspapers, hotels and banks.

During the Nicaraguan Revolution and the Civil War, thousands of Nicaraguans left the country. After the 1990 Nicaraguan Elections some people returned, but many more emigrated during the rest of the decade. In 1998, the Hurricane Mitch killed almost 4,000 people in the country and destroyed much of the Nicaraguan economy, as a result thousands of Nicaraguans received the TPS for emigrate to the United States as "refugees". In recent years, many Nicaraguans had left the country to escape poverty and unemployment.

Nicaraguan emigration is a recent process. During the 1990–2004 period, more than 800,000 Nicaraguans left the country, compared to 100,000 during the 1970–1989 period. According to the World Bank, in 2005 there were 683,520 Nicaraguans living outside Nicaragua legally. If those who are undocumented are counted, some sources estimate as many as 1,500,000 Nicaraguans living abroad by the end of 2005. Nicaraguans are the third largest community of Central Americans living abroad, after Guatemalans and Salvadorans. Nicaragua is also the second country in Central America by percentage of population living abroad.

Remittances to Nicaragua represent about 15% of the country's GDP. In 2008 Nicaragua received close to one billion dollars in remittances; an increase from the $750,000,000 received in 2007, according to the World Bank

The official language of Nicaragua is Spanish, or Nicañol as Nicaraguan Spanish is sometimes referred to, and is spoken by the country's population. In Nicaragua the Voseo form is common, just as in other countries in Central and South America like Honduras, Argentina, and Uruguay. Spanish has many different dialects spoken throughout Latin America, Central American Spanish is the dialect spoken in Nicaragua.


Some other characteristics of Nicaraguan phonology include:

Nicaraguans on the Caribbean coast speak their indigenous languages and also English. The indigenous peoples of the east who use their original language tend to also speak Spanish and/or English, the main languages being Miskito language, Sumo language, and Rama language. Creole languages are also present in the Caribbean coast, Nicaragua Creole English has 30,000 speakers.

Nicaragua has many minority groups. Many ethnic groups in Nicaragua, such as the Chinese Nicaraguans and Palestinian Nicaraguans, have maintained their ancestral languages while also speaking Spanish and/or English. Minority languages include Chinese, Arabic, German, Italian among others. Nicaragua also has a total of 3 extinct languages.

Nicaraguan Sign Language is also of particular interest to linguists.

Religion is a significant part of the culture of Nicaragua and forms part of the constitution. Religious freedom, which has been guaranteed since 1939, and religious tolerance is promoted by both the Nicaraguan government and the constitution. Bishops are expected to lend their authority to important state occasions, and their pronouncements on national issues are closely followed. They can also be called upon to mediate between contending parties at moments of political crisis.

Although Nicaragua has no official religion it is nominally Roman Catholic. Practicing Roman Catholics are no longer the majority and are declining while evangelical Protestant groups and Mormons are growing rapidly have been growing since the 1990s. There are also strong Anglican and Moravian communities on the Caribbean coast.

Roman Catholicism came to Nicaragua in the 16th century with the Spanish conquest and remained, until 1939, the established faith. Protestantism and various Christian sects came to Nicaragua during the 19th century, but only during the 20th century have Protestant denominations gained large followings in the Caribbean Coast of the country. Popular religion revolves around the saints, who are perceived as intermediaries between human beings and God.

Most localities, from the capital of Managua to small rural communities, honor patron saints selected from the Roman Catholic calendar with annual "fiestas". In many communities, a rich lore has grown up around the celebrations of patron saints, such as Managua's Saint Dominic (Santo Domingo), honored in August with two colorful, often riotous, day-long processions through the city. The high point of Nicaragua's religious calendar for the masses is neither Christmas nor Easter, but La Purísima, a week of festivities in early December dedicated to the Immaculate Conception, during which elaborate altars to the Virgin Mary are constructed in homes and workplaces.


General:



</doc>
<doc id="21366" url="https://en.wikipedia.org/wiki?curid=21366" title="Politics of Nicaragua">
Politics of Nicaragua

Nicaragua is a presidential representative democratic republic, in which the President of Nicaragua is both head of state and head of government, and there is a multi-party system. Executive power is exercised by the government.

Legislative power is vested in both the government and the National Assembly. The judiciary is independent of the executive and the legislature.

In 1995, the executive and legislative branches negotiated a reform of the 1987 Sandinista constitution which gave extensive new powers and independence to the National Assembly, including permitting the Assembly to override a presidential veto with a simple majority vote and eliminating the president's ability to pocket veto a bill. Members of the unicameral National Assembly are elected to concurrent five-year terms.

In January 2014, the National Assembly approved changes to the constitution, removing presidential term limits. This allowed current President Daniel Ortega to run for a third successive term.

The president and the vice president are elected for a single five-year term. With the reform of the constitution in 2014 the ban on re-election of the president has been removed. 
The president appoints the Council of Ministers.

The National Assembly "(Asamblea Nacional)" consists of 90 deputies elected from party lists drawn at the department and national level, plus the outgoing president and the runner-up in the presidential race, for a total of 92. In the 2011 elections, the Sandinista National Liberation Front won 63 seats (securing a majority), the Independent Liberal Party won 27 seats, and the Constitutionalist Liberal Party won 2 seats. This includes seats given to outgoing Vice President Jaime Morales Carazot and presidential runner-up Fabio Gadea Mantilla.

Outgoing Vice President Jaime Morales Carazot's seat would usually be given to the outgoing president. However, Danial Ortega was re-elected after the Constitution was modified to remove term limits.

The Supreme Court supervises the functioning of the still largely ineffective and overburdened judicial system. As part of the 1995 constitutional reforms, the independence of the Supreme Court was strengthened by increasing the number of magistrates from 9 to 12. In 2000, the number of Supreme Court Justices was increased to 16. Supreme Court justices are nominated by the political parties and elected to 5-year terms by the National Assembly.

Led by a council of seven magistrates, the Supreme Electoral Council (CSE) is the co-equal branch of government responsible for organizing and conducting elections, plebiscites, and referendums. The magistrates and their alternates are elected to 5-year terms by the National Assembly. Constitutional changes in 2000 expanded the number of CSE magistrates from five to seven and gave the PLC and the FSLN a freer hand to name party activists to the council, prompting allegations that both parties were politicizing electoral institutions and processes and excluding smaller political parties.

Freedom of speech is a right guaranteed by the Nicaraguan constitution and vigorously exercised by its people. Diverse viewpoints are freely and openly discussed in the media and in academia. There is no state censorship in Nicaragua. Other constitutional freedoms include peaceful assembly and association, freedom of religion, and freedom of movement within the country, as well as foreign travel, emigration, and repatriation. The government also permits domestic and international human rights monitors to operate freely in Nicaragua.
The constitution prohibits discrimination based on birth, nationality, political belief, race, gender, language, religion, opinion, national origin, economic or social condition. Homosexuality has been legal since 2008.

All public and private sector workers, except the military and the police, are entitled to form and join unions of their own choosing, and they exercise this right extensively. Nearly half of Nicaragua's work force, including agricultural workers, is unionized. Workers have the right to strike. Collective bargaining is becoming more common in the private sector.

Nicaragua is divided in 15 departments : Boaco, Carazo, Chinandega, Chontales, Estelí, Granada, Jinotega, León, Madriz, Managua, Masaya, Matagalpa, Nueva Segovia, Rivas, Río San Juan, as well as in two autonomous regions: North Caribbean Coast Autonomous Region and South Caribbean Coast Autonomous Region.

Nicaragua President Daniel Ortega said March 6, 2008 that the nation is breaking relations with Colombia "in solidarity with the Ecuadoran people", following the 2008 Andean diplomatic crisis. The relations were restored soon after.

Some political pressure groups are:




</doc>
