<doc id="13617" url="https://en.wikipedia.org/wiki?curid=13617" title="History of Scotland">
History of Scotland

The is known to have begun by the end of the last glacial period (in the paleolithic), roughly 10,000 years ago. Prehistoric Scotland entered the Neolithic Era about 4000 BC, the Bronze Age about 2000 BC, and the Iron Age around 700 BC. Scotland's recorded history began with the arrival of the Roman Empire in the 1st century, when the province of Britannia reached as far north as the Antonine Wall. North of this was Caledonia, inhabited by the "Picti", whose uprisings forced Rome's legions back to Hadrian's Wall. As Rome finally withdrew from Britain, Gaelic raiders called the "Scoti" began colonising Western Scotland and Wales.

The Gaelic kingdom of Dál Riata was founded on the west coast of Scotland in the 6th century. In the following century, Irish missionaries introduced the previously pagan Picts to Celtic Christianity. Following England's Gregorian mission, the Pictish king Nechtan chose to abolish most Celtic practices in favour of the Roman rite, restricting Gaelic influence on his kingdom and avoiding war with Anglian Northumbria. Towards the end of the 8th century, the Viking invasions began, forcing the Picts and Gaels to cease their historic hostility to each other and to unite in the 9th century, forming the Kingdom of Scotland.

The Kingdom of Scotland was united under the House of Alpin, whose members fought among each other during frequent disputed successions. The last Alpin king, Malcolm II, died without issue in the early 11th century and the kingdom passed through his daughter's son to the House of Dunkeld or Canmore. The last Dunkeld king, Alexander III, died in 1286. He left only his infant granddaughter Margaret, Maid of Norway as heir, who died herself four years later. England, under Edward I, would take advantage of this questioned succession to launch a series of conquests, resulting in the Wars of Scottish Independence, as Scotland passed back and forth between the House of Balliol and the House of Bruce. Scotland's ultimate victory confirmed Scotland as a fully independent and sovereign kingdom.

When King David II died without issue, his nephew Robert II established the House of Stuart, which would rule Scotland uncontested for the next three centuries. James VI, Stuart king of Scotland, also inherited the throne of England in 1603, and the Stuart kings and queens ruled both independent kingdoms until the Act of Union in 1707 merged the two kingdoms into a new state, the Kingdom of Great Britain. Ruling until 1714, Queen Anne was the last Stuart monarch. Since 1714, the succession of the British monarchs of the houses of Hanover and Saxe-Coburg and Gotha (Windsor) has been due to their descent from James VI and I of the House of Stuart.

During the Scottish Enlightenment and Industrial Revolution, Scotland became one of the commercial, intellectual and industrial powerhouses of Europe. Later, its industrial decline following the Second World War was particularly acute. In recent decades Scotland has enjoyed something of a cultural and economic renaissance, fuelled in part by a resurgent financial services sector and the proceeds of North Sea oil and gas. Since the 1950s, nationalism has become a strong political topic, with serious debates on Scottish independence, and a referendum in 2014 about leaving the British Union.

People lived in Scotland for at least 8,500 years before Britain's recorded history. At times during the last interglacial period (130,000–70,000 BC) Europe had a climate warmer than today's, and early humans may have made their way to Scotland, with the possible discovery of pre-Ice Age axes on Orkney and mainland Scotland. Glaciers then scoured their way across most of Britain, and only after the ice retreated did Scotland again become habitable, around 9600 BC. Upper Paleolithic hunter-gatherer encampments formed the first known settlements, and archaeologists have dated an encampment near Biggar to around 12000 BC. Numerous other sites found around Scotland build up a picture of highly mobile boat-using people making tools from bone, stone and antlers. The oldest house for which there is evidence in Britain is the oval structure of wooden posts found at South Queensferry near the Firth of Forth, dating from the Mesolithic period, about 8240 BC. The earliest stone structures are probably the three hearths found at Jura, dated to about 6000 BC.

Neolithic farming brought permanent settlements. Evidence of these includes the well-preserved stone house at Knap of Howar on Papa Westray, dating from around 3500 BC and the village of similar houses at Skara Brae on West Mainland, Orkney from about 500 years later. The settlers introduced chambered cairn tombs from around 3500 BC, as at Maeshowe, and from about 3000 BC the many standing stones and circles such as those at Stenness on the mainland of Orkney, which date from about 3100 BC, of four stones, the tallest of which is in height. These were part of a pattern that developed in many regions across Europe at about the same time.

The creation of cairns and Megalithic monuments continued into the Bronze Age, which began in Scotland about 2000 BC. As elsewhere in Europe, hill forts were first introduced in this period, including the occupation of Eildon Hill near Melrose in the Scottish Borders, from around 1000 BC, which accommodated several hundred houses on a fortified hilltop. From the Early and Middle Bronze Age there is evidence of cellular round houses of stone, as at Jarlshof and Sumburgh on Shetland. There is also evidence of the occupation of crannogs, roundhouses partially or entirely built on artificial islands, usually in lakes, rivers and estuarine waters.

In the early Iron Age, from the seventh century BC, cellular houses began to be replaced on the northern isles by simple Atlantic roundhouses, substantial circular buildings with a dry stone construction. From about 400 BC, more complex Atlantic roundhouses began to be built, as at Howe, Orkney and Crosskirk, Caithness. The most massive constructions that date from this era are the circular broch towers, probably dating from about 200 BC. This period also saw the first wheelhouses, a roundhouse with a characteristic outer wall, within which was a circle of stone piers (bearing a resemblance to the spokes of a wheel), but these would flourish most in the era of Roman occupation. There is evidence for about 1,000 Iron Age hill forts in Scotland, most located below the Clyde-Forth line, which have suggested to some archaeologists the emergence of a society of petty rulers and warrior elites recognisable from Roman accounts.

The surviving pre-Roman accounts of Scotland originated with the Greek Pytheas of Massalia, who may have circumnavigated the British Isles of Albion (Britain) and Ierne (Ireland) sometime around 325 BC. The most northerly point of Britain was called "Orcas" (Orkney). By the time of Pliny the Elder, who died in AD 79, Roman knowledge of the geography of Scotland had extended to the "Hebudes" (The Hebrides), "Dumna" (probably the Outer Hebrides), the Caledonian Forest and the people of the Caledonii, from whom the Romans named the region north of their control Caledonia. Ptolemy, possibly drawing on earlier sources of information as well as more contemporary accounts from the Agricolan invasion, identified 18 tribes in Scotland in his "Geography", but many of the names are obscure and the geography becomes less reliable in the north and west, suggesting early Roman knowledge of these areas was confined to observations from the sea.

The Roman invasion of Britain began in earnest in AD 43, leading to the establishment of the Roman province of Britannia in the south. By the year 71, the Roman governor Quintus Petillius Cerialis had launched an invasion of what is now Scotland. In the year 78, Gnaeus Julius Agricola arrived in Britain to take up his appointment as the new governor and began a series of major incursions. He is said to have pushed his armies to the estuary of the "River Taus" (usually assumed to be the River Tay) and established forts there, including a legionary fortress at Inchtuthil. After his victory over the northern tribes at Mons Graupius in 84, a series of forts and towers were established along the Gask Ridge, which marked the boundary between the Lowland and Highland zones, probably forming the first Roman "limes" or frontier in Scotland. Agricola's successors were unable or unwilling to further subdue the far north. By the year 87, the occupation was limited to the Southern Uplands and by the end of the first century the northern limit of Roman expansion was a line drawn between the Tyne and Solway Firth. The Romans eventually withdrew to a line in what is now northern England, building the fortification known as Hadrian's Wall from coast to coast.

Around 141, the Romans undertook a reoccupation of southern Scotland, moving up to construct a new "limes" between the Firth of Forth and the Firth of Clyde, which became the Antonine Wall. The largest Roman construction inside Scotland, it is a sward-covered wall made of turf around high, with nineteen forts. It extended for . Having taken twelve years to build, the wall was overrun and abandoned soon after 160. The Romans retreated to the line of Hadrian's Wall. Roman troops penetrated far into the north of modern Scotland several more times, with at least four major campaigns. The most notable invasion was in 209 when the emperor Septimius Severus led a major force north. After the death of Severus in 210 they withdrew south to Hadrian's Wall, which would be Roman frontier until it collapsed in the 5th century. By the close of the Roman occupation of southern and central Britain in the 5th century, the Picts had emerged as the dominant force in northern Scotland, with the various Brythonic tribes the Romans had first encountered there occupying the southern half of the country. Roman influence on Scottish culture and history was not enduring.

In the centuries after the departure of the Romans from Britain, there were four groups within the borders of what is now Scotland. In the east were the Picts, with kingdoms between the river Forth and Shetland. In the late 6th century the dominant force was the Kingdom of Fortriu, whose lands were centred on Strathearn and Menteith and who raided along the eastern coast into modern England. In the west were the Gaelic (Goidelic)-speaking people of Dál Riata with their royal fortress at Dunadd in Argyll, with close links with the island of Ireland, from whom comes the name Scots. In the south was the British (Brythonic) Kingdom of Strathclyde, descendants of the peoples of the Roman influenced kingdoms of "Hen Ogledd" (Old north), often named Alt Clut, the Brythonic name for their capital at Dumbarton Rock. Finally, there were the English or "Angles", Germanic invaders who had overrun much of southern Britain and held the Kingdom of Bernicia, in the south-east. The first English king in the historical record is Ida, who is said to have obtained the throne and the kingdom about 547. Ida’s grandson, Æthelfrith, united his kingdom with Deira to the south to form Northumbria around the year 604. There were changes of dynasty, and the kingdom was divided, but it was re-united under Æthelfrith's son Oswald (r. 634-42).

Scotland was largely converted to Christianity by Irish-Scots missions associated with figures such as St Columba, from the fifth to the seventh centuries. These missions tended to found monastic institutions and collegiate churches that served large areas. Partly as a result of these factors, some scholars have identified a distinctive form of Celtic Christianity, in which abbots were more significant than bishops, attitudes to clerical celibacy were more relaxed and there was some significant differences in practice with Roman Christianity, particularly the form of tonsure and the method of calculating Easter, although most of these issues had been resolved by the mid-7th century.

Conversion to Christianity may have speeded a long term process of gaelicisation of the Pictish kingdoms, which adopted Gaelic language and customs. There was also a merger of the Gaelic and Pictish crowns, although historians debate whether it was a Pictish takeover of Dál Riata, or the other way around. This culminated in the rise of Cínaed mac Ailpín (Kenneth MacAlpin) in the 840s, which brought to power the House of Alpin. In 867 AD the Vikings seized the southern half of Northumbria, forming the Kingdom of York; three years later they stormed the Britons’ fortress of Dumbarton and subsequently conquered much of England except for a reduced Kingdom of Wessex, leaving the new combined Pictish and Gaelic kingdom almost encircled. When he died as king of the combined kingdom in 900, Domnall II (Donald II) was the first man to be called "rí Alban" (i.e. "King of Alba"). The term Scotia was increasingly used to describe the kingdom between North of the Forth and Clyde and eventually the entire area controlled by its kings was referred to as Scotland.
The long reign (900–942/3) of Causantín (Constantine II) is often regarded as the key to formation of the Kingdom of Alba. He was later credited with bringing Scottish Christianity into conformity with the Catholic Church. After fighting many battles, his defeat at Brunanburh was followed by his retirement as a Culdee monk at St. Andrews. The period between the accession of his successor Máel Coluim I (Malcolm I) and Máel Coluim mac Cináeda (Malcolm II) was marked by good relations with the Wessex rulers of England, intense internal dynastic disunity and relatively successful expansionary policies. In 945, Máel Coluim I annexed Strathclyde as part of a deal with King Edmund of England, where the kings of Alba had probably exercised some authority since the later 9th century, an event offset somewhat by loss of control in Moray. The reign of King Donnchad I (Duncan I) from 1034 was marred by failed military adventures, and he was defeated and killed by MacBeth, the Mormaer of Moray, who became king in 1040. MacBeth ruled for seventeen years before he was overthrown by Máel Coluim, the son of Donnchad, who some months later defeated MacBeth's step-son and successor Lulach to become king Máel Coluim III (Malcolm III).

It was Máel Coluim III, who acquired the nickname "Canmore" ("Cenn Mór", "Great Chief"), which he passed to his successors and who did most to create the Dunkeld dynasty that ruled Scotland for the following two centuries. Particularly important was his second marriage to the Anglo-Hungarian princess Margaret. This marriage, and raids on northern England, prompted William the Conqueror to invade and Máel Coluim submitted to his authority, opening up Scotland to later claims of sovereignty by English kings. When Malcolm died in 1093, his brother Domnall III (Donald III) succeeded him. However, William II of England backed Máel Coluim's son by his first marriage, Donnchad, as a pretender to the throne and he seized power. His murder within a few months saw Domnall restored with one of Máel Coluim sons by his second marriage, Edmund, as his heir. The two ruled Scotland until two of Edmund's younger brothers returned from exile in England, again with English military backing. Victorious, Edgar, the oldest of the three, became king in 1097. Shortly afterwards Edgar and the King of Norway, Magnus Bare Legs concluded a treaty recognising Norwegian authority over the Western Isles. In practice Norse control of the Isles was loose, with local chiefs enjoying a high degree of independence. He was succeeded by his brother Alexander, who reigned 1107–24.
When Alexander died in 1124, the crown passed to Margaret's fourth son David I, who had spent most of his life as a Norman French baron in England. His reign saw what has been characterised as a "Davidian Revolution", by which native institutions and personnel were replaced by English and French ones, underpinning the development of later Medieval Scotland. Members of the Anglo-Norman nobility took up places in the Scottish aristocracy and he introduced a system of feudal land tenure, which produced knight service, castles and an available body of heavily armed cavalry. He created an Anglo-Norman style of court, introduced the office of justicar to oversee justice, and local offices of sheriffs to administer localities. He established the first royal burghs in Scotland, granting rights to particular settlements, which led to the development of the first true Scottish towns and helped facilitate economic development as did the introduction of the first recorded Scottish coinage. He continued a process begun by his mother and brothers helping to establish foundations that brought reform to Scottish monasticism based on those at Cluny and he played a part in organising diocese on lines closer to those in the rest of Western Europe.

These reforms were pursued under his successors and grandchildren Malcolm IV of Scotland and William I, with the crown now passing down the main line of descent through primogeniture, leading to the first of a series of minorities. The benefits of greater authority were reaped by William's son Alexander II and his son Alexander III, who pursued a policy of peace with England to expand their authority in the Highlands and Islands. By the reign of Alexander III, the Scots were in a position to annex the remainder of the western seaboard, which they did following Haakon Haakonarson's ill-fated invasion and the stalemate of the Battle of Largs with the Treaty of Perth in 1266.

The death of king Alexander III in 1286, and the death of his granddaughter and heir Margaret, Maid of Norway in 1290, left 14 rivals for succession. To prevent civil war the Scottish magnates asked Edward I of England to arbitrate, for which he extracted legal recognition that the realm of Scotland was held as a feudal dependency to the throne of England before choosing John Balliol, the man with the strongest claim, who became king in 1292. Robert Bruce, 5th Lord of Annandale, the next strongest claimant, accepted this outcome with reluctance. Over the next few years Edward I used the concessions he had gained to systematically undermine both the authority of King John and the independence of Scotland. In 1295 John, on the urgings of his chief councillors, entered into an alliance with France, known as the Auld Alliance.
In 1296 Edward invaded Scotland, deposing King John. The following year William Wallace and Andrew de Moray raised forces to resist the occupation and under their joint leadership an English army was defeated at the Battle of Stirling Bridge. For a short time Wallace ruled Scotland in the name of John Balliol as Guardian of the realm. Edward came north in person and defeated Wallace at the Battle of Falkirk in 1298. Wallace escaped but probably resigned as Guardian of Scotland. In 1305 he fell into the hands of the English, who executed him for treason despite the fact that he owed no allegiance to England.

Rivals John Comyn and Robert the Bruce, grandson of the claimant, were appointed as joint guardians in his place. On 10 February 1306, Bruce participated in the murder of Comyn, at Greyfriars Kirk in Dumfries. Less than seven weeks later, on 25 March, Bruce was crowned as King. However, Edward's forces overran the country after defeating Bruce's small army at the Battle of Methven. Despite the excommunication of Bruce and his followers by Pope Clement V, his support slowly strengthened; and by 1314 with the help of leading nobles such as Sir James Douglas and Thomas Randolph only the castles at Bothwell and Stirling remained under English control. Edward I had died in 1307. His heir Edward II moved an army north to break the siege of Stirling Castle and reassert control. Robert defeated that army at the Battle of Bannockburn in 1314, securing "de facto" independence. In 1320 the Declaration of Arbroath, a remonstrance to the Pope from the nobles of Scotland, helped convince Pope John XXII to overturn the earlier excommunication and nullify the various acts of submission by Scottish kings to English ones so that Scotland's sovereignty could be recognised by the major European dynasties. The Declaration has also been seen as one of the most important documents in the development of a Scottish national identity.

In 1326, what may have been the first full Parliament of Scotland met. The parliament had evolved from an earlier council of nobility and clergy, the "colloquium", constituted around 1235, but perhaps in 1326 representatives of the burghs — the burgh commissioners — joined them to form the Three Estates. In 1328, Edward III signed the Treaty of Edinburgh–Northampton acknowledging Scottish independence under the rule of Robert the Bruce. However, four years after Robert's death in 1329, England once more invaded on the pretext of restoring Edward Balliol, son of John Balliol, to the Scottish throne, thus starting the Second War of Independence. Despite victories at Dupplin Moor and Halidon Hill, in the face of tough Scottish resistance led by Sir Andrew Murray, the son of Wallace's comrade in arms, successive attempts to secure Balliol on the throne failed. Edward III lost interest in the fate of his protégé after the outbreak of the Hundred Years' War with France. In 1341 David II, King Robert's son and heir, was able to return from temporary exile in France. Balliol finally resigned his claim to the throne to Edward in 1356, before retiring to Yorkshire, where he died in 1364.

After David II's death, Robert II, the first of the Stewart kings, came to the throne in 1371. He was followed in 1390 by his ailing son John, who took the regnal name Robert III. During Robert III's reign (1390–1406), actual power rested largely in the hands of his brother, Robert Stewart, Duke of Albany. After the suspicious death (possibly on the orders of the Duke of Albany) of his elder son, David, Duke of Rothesay in 1402, Robert, fearful for the safety of his younger son, the future James I, sent him to France in 1406. However, the English captured him en route and he spent the next 18 years as a prisoner held for ransom. As a result, after the death of Robert III, regents ruled Scotland: first, the Duke of Albany; and later his son Murdoch. When Scotland finally paid the ransom in 1424, James, aged 32, returned with his English bride determined to assert this authority. Several of the Albany family were executed; but he succeeded in centralising control in the hands of the crown, at the cost of increasing unpopularity, and was assassinated in 1437. His son James II (reigned 1437–1460), when he came of age in 1449, continued his father's policy of weakening the great noble families, most notably taking on the powerful Black Douglas family that had come to prominence at the time of the Bruce.

In 1468 the last significant acquisition of Scottish territory occurred when James III was engaged to Margaret of Denmark, receiving the Orkney Islands and the Shetland Islands in payment of her dowry. Berwick upon Tweed was captured by England in 1482. With the death of James III in 1488 at the Battle of Sauchieburn, his successor James IV successfully ended the quasi-independent rule of the Lord of the Isles, bringing the Western Isles under effective Royal control for the first time. In 1503, he married Margaret Tudor, daughter of Henry VII of England, thus laying the foundation for the 17th century Union of the Crowns.

Scotland advanced markedly in educational terms during the 15th century with the founding of the University of St Andrews in 1413, the University of Glasgow in 1450 and the University of Aberdeen in 1495, and with the passing of the Education Act 1496, which decreed that all sons of barons and freeholders of substance should attend grammar schools. James IV's reign is often considered to have seen a flowering of Scottish culture under the influence of the European Renaissance.
In 1512 the Auld Alliance was renewed and under its terms, when the French were attacked by the English under Henry VIII, James IV invaded England in support. The invasion was stopped decisively at the Battle of Flodden Field during which the King, many of his nobles, and a large number of ordinary troops were killed, commemorated by the song "Flowers of the Forest". Once again Scotland's government lay in the hands of regents in the name of the infant James V.

James V finally managed to escape from the custody of the regents in 1528. He continued his father's policy of subduing the rebellious Highlands, Western and Northern isles and the troublesome borders. He also continued the French alliance, marrying first the French noblewoman Madeleine of Valois and then after her death Marie of Guise. James V's domestic and foreign policy successes were overshadowed by another disastrous campaign against England that led to defeat at the Battle of Solway Moss (1542). James died a short time later, a demise blamed by contemporaries on "a broken heart". The day before his death, he was brought news of the birth of an heir: a daughter, who would become Mary, Queen of Scots.

Once again, Scotland was in the hands of a regent. Within two years, the Rough Wooing began, Henry VIII's military attempt to force a marriage between Mary and his son, Edward. This took the form of border skirmishing and several English campaigns into Scotland. In 1547, after the death of Henry VIII, forces under the English regent Edward Seymour, 1st Duke of Somerset were victorious at the Battle of Pinkie Cleugh, the climax of the Rough Wooing, and followed up by the occupation of Haddington. Mary was then sent to France at the age of five, as the intended bride of the heir to the French throne. Her mother, Marie de Guise, stayed in Scotland to look after the interests of Mary — and of France — although the Earl of Arran acted officially as regent. Guise responded by calling on French troops, who helped stiffen resistance to the English occupation. By 1550, after a change of regent in England, the English withdrew from Scotland completely.

From 1554, Marie de Guise, took over the regency, and continued to advance French interests in Scotland. French cultural influence resulted in a large influx of French vocabulary into Scots. But anti-French sentiment also grew, particularly among Protestants, who saw the English as their natural allies. In 1560 Marie de Guise died, and soon after the Auld Alliance also ended, with the signing of the Treaty of Edinburgh, which provided for the removal of French and English troops from Scotland. The Scottish Reformation took place only days later when the Scottish Parliament abolished the Roman Catholic religion and outlawed the Mass.
Meanwhile, Queen Mary had been raised as a Catholic in France, and married to the Dauphin, who became king as Francis II in 1559, making her queen consort of France. When Francis died in 1560, Mary, now 19, returned to Scotland to take up the government. Despite her private religion, she did not attempt to re-impose Catholicism on her largely Protestant subjects, thus angering the chief Catholic nobles. Her six-year personal reign was marred by a series of crises, largely caused by the intrigues and rivalries of the leading nobles. The murder of her secretary, David Riccio, was followed by that of her unpopular second husband Lord Darnley, and her abduction by and marriage to the Earl of Bothwell, who was implicated in Darnley's murder. Mary and Bothwell confronted the lords at Carberry Hill and after their forces melted away, he fled and she was captured by Bothwell's rivals. Mary was imprisoned in Loch Leven Castle, and in July 1567, was forced to abdicate in favour of her infant son James VI. Mary eventually escaped and attempted to regain the throne by force. After her defeat at the Battle of Langside in 1568, she took refuge in England, leaving her young son in the hands of regents. In Scotland the regents fought a civil war on behalf of James VI against his mother's supporters. In England, Mary became a focal point for Catholic conspirators and was eventually tried for treason and executed on the orders of her kinswoman Elizabeth I.

During the 16th century, Scotland underwent a Protestant Reformation that created a predominantly Calvinist national Kirk, which became Presbyterian in outlook and severely reduced the powers of bishops. In the earlier part of the century, the teachings of first Martin Luther and then John Calvin began to influence Scotland, particularly through Scottish scholars, often training for the priesthood, who had visited Continental universities. The Lutheran preacher Patrick Hamilton was executed for heresy in St. Andrews in 1528. The execution of others, especially the Zwingli-influenced George Wishart, who was burnt at the stake on the orders of Cardinal Beaton in 1546, angered Protestants. Wishart's supporters assassinated Beaton soon after and seized St. Andrews Castle, which they held for a year before they were defeated with the help of French forces. The survivors, including chaplain John Knox, were condemned to be galley slaves in France, stoking resentment of the French and creating martyrs for the Protestant cause.

Limited toleration and the influence of exiled Scots and Protestants in other countries, led to the expansion of Protestantism, with a group of lairds declaring themselves Lords of the Congregation in 1557 and representing their interests politically. The collapse of the French alliance and English intervention in 1560 meant that a relatively small, but highly influential, group of Protestants were in a position to impose reform on the Scottish church. A confession of faith, rejecting papal jurisdiction and the mass, was adopted by Parliament in 1560, while the young Mary, Queen of Scots, was still in France.

Knox, having escaped the galleys and spent time in Geneva as a follower of Calvin, emerged as the most significant figure of the period. The Calvinism of the reformers led by Knox resulted in a settlement that adopted a Presbyterian system and rejected most of the elaborate trappings of the medieval church. The reformed Kirk gave considerable power to local lairds, who often had control over the appointment of the clergy. There were widespread, but generally orderly outbreaks of iconoclasm. At this point the majority of the population was probably still Catholic in persuasion and the Kirk found it difficult to penetrate the Highlands and Islands, but began a gradual process of conversion and consolidation that, compared with reformations elsewhere, was conducted with relatively little persecution.

Women shared in the religiosity of the day. The egalitarian and emotional aspects of Calvinism appealed to men and women alike. Historian Alasdair Raffe finds that, "Men and women were thought equally likely to be among the elect...Godly men valued the prayers and conversation of their female co-religionists, and this reciprocity made for loving marriages and close friendships between men and women." Furthermore, there was an increasingly intense relationship In the pious bonds between minister and his women parishioners. For the first time, laywomen gained numerous new religious roles,And took a prominent place in prayer societies.

In 1603, James VI King of Scots inherited the throne of the Kingdom of England, and became King James I of England, leaving Edinburgh for London, uniting England and Scotland under one monarch. The Union was a personal or dynastic union, with the Crowns remaining both distinct and separate—despite James's best efforts to create a new "imperial" throne of "Great Britain". The acquisition of the Irish crown along with the English, facilitated a process of settlement by Scots in what was historically the most troublesome area of the kingdom in Ulster, with perhaps 50,000 Scots settling in the province by the mid-17th century. Attempts to found a Scottish colony in North America in Nova Scotia were largely unsuccessful, with insufficient funds and willing colonists.

Although James had tried to get the Scottish Church to accept some of the High Church Anglicanism of his southern kingdom, he met with limited success. His son and successor, Charles I, took matters further, introducing an English-style Prayer Book into the Scottish church in 1637. This resulted in anger and widespread rioting. (The story goes that it was initiated by a certain Jenny Geddes who threw a stool in St Giles Cathedral.) Representatives of various sections of Scottish society drew up the National Covenant in 1638, objecting to the King's liturgical innovations. In November of the same year matters were taken even further, when at a meeting of the General Assembly in Glasgow the Scottish bishops were formally expelled from the Church, which was then established on a full Presbyterian basis. Charles gathered a military force; but as neither side wished to push the matter to a full military conflict, a temporary settlement was concluded at Pacification of Berwick. Matters remained unresolved until 1640 when, in a renewal of hostilities, Charles's northern forces were defeated by the Scots at the Battle of Newburn to the west of Newcastle. During the course of these Bishops' Wars Charles tried to raise an army of Irish Catholics, but was forced to back down after a storm of protest in Scotland and England. The backlash from this venture provoked a rebellion in Ireland and Charles was forced to appeal to the English Parliament for funds. Parliament's demands for reform in England eventually resulted in the English Civil War. This series of civil wars that engulfed England, Ireland and Scotland in the 1640s and 1650s is known to modern historians as the Wars of the Three Kingdoms. The Covenanters meanwhile, were left governing Scotland, where they raised a large army of their own and tried to impose their religious settlement on Episcopalians and Roman Catholics in the north of the country. In England his religious policies caused similar resentment and he ruled without recourse to parliament from 1629.

As the civil wars developed, the English Parliamentarians appealed to the Scots Covenanters for military aid against the King. A Solemn League and Covenant was entered into, guaranteeing the Scottish Church settlement and promising further reform in England. Scottish troops played a major part in the defeat of Charles I, notably at the battle of Marston Moor. An army under the Earl of Leven occupied the North of England for some time.

However, not all Scots supported the Covenanter's taking arms against their King. In 1644, James Graham, 1st Marquess of Montrose attempted to raise the Highlands for the King. Few Scots would follow him, but, aided by 1,000 Irish, Highland and Islesmen troops sent by the Irish Confederates under Alasdair MacDonald (MacColla), and an instinctive genius for mobile warfare, he was stunningly successful. A Scottish Civil War began in September 1644 with his victory at battle of Tippermuir. After a series of victories over poorly trained Covenanter militias, the lowlands were at his mercy. However, at this high point, his army was reduced in size, as MacColla and the Highlanders preferred to continue the war in the north against the Campbells. Shortly after, what was left of his force was defeated at the Battle of Philiphaugh. Escaping to the north, Montrose attempted to continue the struggle with fresh troops; but in July 1646 his army was disbanded after the King surrendered to the Scots army at Newark, and the civil war came to an end.

The following year Charles, while he was being held captive in Carisbrooke Castle, entered into an agreement with moderate Scots Presbyterians. In this secret 'Engagement', the Scots promised military aid in return for the King's agreement to implement Presbyterianism in England on a three-year trial basis. The Duke of Hamilton led an invasion of England to free the King, but he was defeated by Oliver Cromwell in August 1648 at the Battle of Preston.

The execution of Charles I in 1649 was carried out in the face of objections by the Covenanter government and his son was immediately proclaimed as King Charles II in Edinburgh. Oliver Cromwell led an invasion of Scotland in 1650, and defeated the Scottish army at Dunbar and then defeated a Scottish invasion of England at Worcester on 3 September 1651 (the anniversary of his victory at Dunbar). Cromwell emerged as the leading figure in the English government and Scotland was occupied by an English force under George Monck. The country was incorporated into the Puritan-governed Commonwealth and lost its independent church government, parliament and legal system, but gained access to English markets. Various attempts were made to legitimise the union, calling representatives from the Scottish burghs and shires to negotiations and to various English parliaments, where they were always under-represented and had little opportunity for dissent. However, final ratification was delayed by Cromwell's problems with his various parliaments and the union did not become the subject of an act until 1657 (see Tender of Union).

After the death of Cromwell and the regime's collapse, Charles II was restored in 1660 and Scotland again became an independent kingdom. Scotland regained its system of law, parliament and kirk, but also the Lords of the Articles (by which the crown managed parliament), bishops and a king who did not visit the country. He ruled largely without reference to Parliament, through a series of commissioners. These began with John, Earl of Middleton and ended with the king's brother and heir, James, Duke of York (known in Scotland as the Duke of Albany). The English Navigation Acts prevented the Scots engaging in what would have been lucrative trading with England's colonies. The restoration of episcopacy was a source of trouble, particularly in the south-west of the country, an area with strong Presbyterian sympathies. Abandoning the official church, many of the inhabitants began to attend illegal field assemblies, known as conventicles. Official attempts to suppress these led to a rising in 1679, defeated by James, Duke of Monmouth, the King's illegitimate son, at the Battle of Bothwell Bridge. In the early 1680s a more intense phase of persecution began, later to be called "the Killing Time". When Charles died in 1685 and his brother, a Roman Catholic, succeeded him as James VII of Scotland (and II of England), matters came to a head.

James put Catholics in key positions in the government and attendance at conventicles was made punishable by death. He disregarded parliament, purged the Council and forced through religious toleration to Roman Catholics, alienating his Protestant subjects. It was believed that the king would be succeeded by his daughter Mary, a Protestant and the wife of William of Orange, Stadtholder of the Netherlands, but when in 1688, James produced a male heir, James Francis Edward Stuart, it was clear that his policies would outlive him. An invitation by seven leading Englishmen led William to land in England with 40,000 men, and James fled, leading to the almost bloodless "Glorious Revolution". The Estates issued a "Claim of Right" that suggested that James had forfeited the crown by his actions (in contrast to England, which relied on the legal fiction of an abdication) and offered it to William and Mary, which William accepted, along with limitations on royal power. The final settlement restored Presbyterianism and abolished the bishops who had generally supported James. However, William, who was more tolerant than the Kirk tended to be, passed acts restoring the Episcopalian clergy excluded after the Revolution.

Although William's supporters dominated the government, there remained a significant following for James, particularly in the Highlands. His cause, which became known as Jacobitism, from the Latin "(Jacobus)" for James, led to a series of risings. An initial Jacobite military attempt was led by John Graham, Viscount Dundee. His forces, almost all Highlanders, defeated William's forces at the Battle of Killiecrankie in 1689, but they took heavy losses and Dundee was slain in the fighting. Without his leadership the Jacobite army was soon defeated at the Battle of Dunkeld. In the aftermath of the Jacobite defeat on 13 February 1692, in an incident since known as the Massacre of Glencoe, 38 members of the Clan MacDonald of Glencoe were killed by members of the Earl of Argyll's Regiment of Foot, on the grounds that they had not been prompt in pledging allegiance to the new monarchs.

The closing decade of the 17th century saw the generally favourable economic conditions that had dominated since the Restoration come to an end. There was a slump in trade with the Baltic and France from 1689 to 1691, caused by French protectionism and changes in the Scottish cattle trade, followed by four years of failed harvests (1695, 1696 and 1698-9), an era known as the "seven ill years". The result was severe famine and depopulation, particularly in the north. The Parliament of Scotland of 1695 enacted proposals to help the desperate economic situation, including setting up the Bank of Scotland. The "Company of Scotland Trading to Africa and the Indies" received a charter to raise capital through public subscription.

With the dream of building a lucrative overseas colony for Scotland, the Company of Scotland invested in the Darien scheme, an ambitious plan devised by William Paterson to establish a colony on the Isthmus of Panama in the hope of establishing trade with the Far East. The Darién scheme won widespread support in Scotland as the landed gentry and the merchant class were in agreement in seeing overseas trade and colonialism as routes to upgrade Scotland's economy. Since the capital resources of the Edinburgh merchants and landholder elite were insufficient, the company appealed to middling social ranks, who responded with patriotic fervour to the call for money; the lower classes volunteered as colonists. But the English government opposed the idea: involved in the War of the Grand Alliance from 1689 to 1697 against France, it did not want to offend Spain, which claimed the territory as part of New Granada. The English investors withdrew. Returning to Edinburgh, the Company raised 400,000 pounds in a few weeks. Three small fleets with a total of 3,000 men eventually set out for Panama in 1698. The exercise proved a disaster. Poorly equipped; beset by incessant rain; under attack by the Spanish from nearby Cartagena; and refused aid by the English in the West Indies, the colonists abandoned their project in 1700. Only 1,000 survived and only one ship managed to return to Scotland.

Scotland was a poor rural, agricultural society with a population of 1.3 million in 1755. Although Scotland lost home rule, the Union allowed it to break free of a stultifying system and opened the way for the Scottish enlightenment as well as a great expansion of trade and increase in opportunity and wealth. Edinburgh economist Adam Smith concluded in 1776 that "By the union with England, the middling and inferior ranks of people in Scotland gained a complete deliverance from the power of an aristocracy which had always before oppressed them." Historian Jonathan Israel holds that the Union "proved a decisive catalyst politically and economically," by allowing ambitious Scots entry on an equal basis to a rich expanding empire and its increasing trade.

Scotland's transformation into a rich leader of modern industry came suddenly and unexpectedly in the next 150 years, following its union with England in 1707 and its integration with the advanced English and imperial economies. The transformation was led by two cities that grew rapidly after 1770. Glasgow, on the river Clyde, was the base for the tobacco and sugar trade with an emerging textile industry. Edinburgh was the administrative and intellectual centre where the Scottish Enlightenment was chiefly based.

By the start of the 18th century, a political union between Scotland and England became politically and economically attractive, promising to open up the much larger markets of England, as well as those of the growing English Empire. With economic stagnation since the late 17th century, which was particularly acute in 1704; the country depended more and more heavily on sales of cattle and linen to England, who used this to create pressure for a union. The Scottish parliament voted on 6 January 1707, by 110 to 69, to adopt the Treaty of Union. It was also a full economic union; indeed, most of its 25 articles dealt with economic arrangements for the new state known as "Great Britain". It added 45 Scots to the 513 members of the House of Commons and 16 Scots to the 190 members of the House of Lords, and ended the Scottish parliament. It also replaced the Scottish systems of currency, taxation and laws regulating trade with laws made in London. Scottish law remained separate from English law, and the religious system was not changed. England had about five times the population of Scotland at the time, and about 36 times as much wealth.

Jacobitism was revived by the unpopularity of the union. In 1708 James Francis Edward Stuart, the son of James VII, who became known as "The Old Pretender", attempted an invasion with a French fleet carrying 6,000 men, but the Royal Navy prevented it from landing troops. A more serious attempt occurred in 1715, soon after the death of Anne and the accession of the first Hanoverian king, the eldest son of Sophie, as George I of Great Britain. This rising (known as "The 'Fifteen") envisaged simultaneous uprisings in Wales, Devon, and Scotland. However, government arrests forestalled the southern ventures. In Scotland, John Erskine, Earl of Mar, nicknamed "Bobbin' John", raised the Jacobite clans but proved to be an indecisive leader and an incompetent soldier. Mar captured Perth, but let a smaller government force under the Duke of Argyll hold the Stirling plain. Part of Mar's army joined up with risings in northern England and southern Scotland, and the Jacobites fought their way into England before being defeated at the Battle of Preston, surrendering on 14 November 1715. The day before, Mar had failed to defeat Argyll at the Battle of Sheriffmuir. At this point, James belatedly landed in Scotland, but was advised that the cause was hopeless. He fled back to France. An attempted Jacobite invasion with Spanish assistance in 1719 met with little support from the clans and ended in defeat at the Battle of Glen Shiel.

In 1745 the Jacobite rising known as "The 'Forty-Five" began. Charles Edward Stuart, son of the "Old Pretender", often referred to as "Bonnie Prince Charlie" or the "Young Pretender", landed on the island of Eriskay in the Outer Hebrides. Several clans unenthusiastically joined him. At the outset he was successful, taking Edinburgh and then defeating the only government army in Scotland at the Battle of Prestonpans. The Jacobite army marched into England, took Carlisle and advanced as far as south as Derby. However, it became increasingly evident that England would not support a Roman Catholic Stuart monarch. The Jacobite leadership had a crisis of confidence and they retreated to Scotland as two English armies closed in and Hanoverian troops began to return from the continent. Charles' position in Scotland began to deteriorate as the Whig supporters rallied and regained control of Edinburgh. After an unsuccessful attempt on Stirling, he retreated north towards Inverness. He was pursued by the Duke of Cumberland and gave battle with an exhausted army at Culloden on 16 April 1746, where the Jacobite cause was crushed. Charles hid in Scotland with the aid of Highlanders until September 1746, when he escaped back to France. There were bloody reprisals against his supporters and foreign powers abandoned the Jacobite cause, with the court in exile forced to leave France. The Old Pretender died in 1760 and the Young Pretender, without legitimate issue, in 1788. When his brother, Henry, Cardinal of York, died in 1807, the Jacobite cause was at an end.

With the advent of the Union and the demise of Jacobitism, access to London and the Empire opened up very attractive career opportunities for ambitious middle-class and upper-class Scots, who seized the chance to become entrepreneurs, intellectuals, and soldiers. Thousands of Scots, mainly Lowlanders, took up positions of power in politics, civil service, the army and navy, trade, economics, colonial enterprises and other areas across the nascent British Empire. Historian Neil Davidson notes that “after 1746 there was an entirely new level of participation by Scots in political life, particularly outside Scotland”. Davidson also states that “far from being ‘peripheral’ to the British economy, Scotland – or more precisely, the Lowlands – lay at its core”. British officials especially appreciated Scottish soldiers. As the Secretary of War told Parliament in 1751, "I am for having always in our army as many Scottish soldiers as possible...because they are generally more hardy and less mutinous". The national policy of aggressively recruiting Scots for senior civilian positions stirred up resentment among Englishmen, ranging from violent diatribes by John Wilkes, to vulgar jokes and obscene cartoons in the popular press, and the haughty ridicule by intellectuals such as Samuel Johnson that was much resented by Scots. In his great "Dictionary" Johnson defined oats as, "a grain, which in England is generally given to horses, but in Scotland supports the people." To which Lord Elibank retorted, "Very true, and where will you find such men and such horses?"

Scottish politics in the late 18th century was dominated by the Whigs, with the benign management of Archibald Campbell, 3rd Duke of Argyll (1682–1761), who was in effect the "viceroy of Scotland" from the 1720s until his death in 1761. Scotland generally supported the king with enthusiasm during the American Revolution. Henry Dundas (1742–1811) dominated political affairs in the latter part of the century. Dundas put a brake on intellectual and social change through his ruthless manipulation of patronage in alliance with Prime Minister William Pitt the Younger, until he lost power in 1806.

The main unit of local government was the parish, and since it was also part of the church, the elders imposed public humiliation for what the locals considered immoral behaviour, including fornication, drunkenness, wife beating, cursing and Sabbath breaking. The main focus was on the poor and the landlords ("lairds") and gentry, and their servants, were not subject to the parish's control. The policing system weakened after 1800 and disappeared in most places by the 1850s.

After the battle of Culloden the leaders were declared to be traitors, with Jacobite officers executed and many of the rebel soldiers shipped to the colonies as indentured servants. Key laws included the Dress Act 1746, the Act of Proscription 1746, and especially the Heritable Jurisdictions Act of 1746. Parliament also banned the bearing of arms and the wearing of tartans, and limited the activities of the Episcopalian Church. After a generation the Highlands had been transformed and the laws were no longer needed; they were mostly repealed.

Historians debate whether the dramatic changes merely reflect long-term trends that were more-or-less inevitable, or whether government intervention played the decisive role in changing the goals and roles of the chiefs. As Conway (2006) concludes, the new policies "went far beyond earlier efforts to promote economic development in the Highlands and ... represented the first real endeavour to transform the region's social system ... the post-rebellion legislation certainly seems to have accelerated the change." However Devine (1999) and Ray (2001) argue that long-term economic and social changes were already undermining the clan system.

The major result of these changes were the Highland Clearances, in which many of the population of the Highlands were evicted as lands were enclosed, principally so that they could be used for sheep farming. The clearances followed patterns of agricultural change throughout Britain, but were notorious as a result of the late timing, the lack of legal protection for year-by-year tenants under Scots law, the abruptness of the change from the traditional clan system, and the brutality of some evictions. Many of those who remained were now crofters: poor families living on "crofts"—very small rented farms with indefinite tenure used to raise various crops and animals, with kelping industry (where men burned kelp for the ashes), fishing, spinning of linen and military service as important sources of revenue.

The era of the Napoleonic wars, 1790–1815, brought prosperity, optimism, and economic growth to the Highlands. The economy grew thanks to higher wages, as well as large-scale infrastructure spending such as the Caledonian Canal project. On the East Coast, farmlands were improved, and high prices for cattle brought money to the community. Service in the Army was also attractive to young men from Highlands, who sent pay home and retired there with their army pensions, but the prosperity ended after 1815, and long-run negative factors began to undermine the economic position of the crofters.

Historian Jonathan Israel argues that by 1750 Scotland's major cities had created an intellectual infrastructure of mutually supporting institutions, such as universities, reading societies, libraries, periodicals, museums and masonic lodges. The Scottish network was "predominantly liberal Calvinist, Newtonian, and 'design' oriented in character which played a major role in the further development of the transatlantic Enlightenment ." In France Voltaire said "we look to Scotland for all our ideas of civilization," and the Scots in turn paid close attention to French ideas. Historian Bruce Lenman says their "central achievement was a new capacity to recognize and interpret social patterns." The first major philosopher of the Scottish Enlightenment was Francis Hutcheson, who held the Chair of Philosophy at the University of Glasgow from 1729 to 1746. A moral philosopher who produced alternatives to the ideas of Thomas Hobbes, one of his major contributions to world thought was the utilitarian and consequentialist principle that virtue is that which provides, in his words, "the greatest happiness for the greatest numbers". Much of what is incorporated in the scientific method (the nature of knowledge, evidence, experience, and causation) and some modern attitudes towards the relationship between science and religion were developed by his protégés David Hume and Adam Smith. Hume became a major figure in the skeptical philosophical and empiricist traditions of philosophy. He and other Scottish Enlightenment thinkers developed what he called a 'science of man', which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behave in ancient and primitive cultures with a strong awareness of the determining forces of modernity. Modern sociology largely originated from this movement and Hume's philosophical concepts that directly influenced James Madison (and thus the US Constitution) and when popularised by Dugald Stewart, would be the basis of classical liberalism. Adam Smith published "The Wealth of Nations", often considered the first work on modern economics. It had an immediate impact on British economic policy and in the 21st century still framed discussions on globalisation and tariffs. The focus of the Scottish Enlightenment ranged from intellectual and economic matters to the specifically scientific as in the work of the physician and chemist William Cullen, the agriculturalist and economist James Anderson, chemist and physician Joseph Black, natural historian John Walker and James Hutton, the first modern geologist.

With tariffs with England now abolished, the potential for trade for Scottish merchants was considerable. However, Scotland in 1750 was still a poor rural, agricultural society with a population of 1.3 million. Some progress was visible: agriculture in the Lowlands was steadily upgraded after 1700 and standards remained high. there were the sales of linen and cattle to England, the cash flows from military service, and the tobacco trade that was dominated by Glasgow Tobacco Lords after 1740. Merchants who profited from the American trade began investing in leather, textiles, iron, coal, sugar, rope, sailcloth, glassworks, breweries, and soapworks, setting the foundations for the city's emergence as a leading industrial centre after 1815. The tobacco trade collapsed during the American Revolution (1776–83), when its sources were cut off by the British blockade of American ports. However, trade with the West Indies began to make up for the loss of the tobacco business, reflecting the British demand for sugar and the demand in the West Indies for herring and linen goods.

Linen was Scotland's premier industry in the 18th century and formed the basis for the later cotton, jute, and woollen industries. Scottish industrial policy was made by the Board of Trustees for Fisheries and Manufactures in Scotland, which sought to build an economy complementary, not competitive, with England. Since England had woollens, this meant linen. Encouraged and subsidised by the Board of Trustees so it could compete with German products, merchant entrepreneurs became dominant in all stages of linen manufacturing and built up the market share of Scottish linens, especially in the American colonial market. The British Linen Company, established in 1746, was the largest firm in the Scottish linen industry in the 18th century, exporting linen to England and America. As a joint-stock company, it had the right to raise funds through the issue of promissory notes or bonds. With its bonds functioning as bank notes, the company gradually moved into the business of lending and discounting to other linen manufacturers, and in the early 1770s banking became its main activity. It joined the established Scottish banks such as the Bank of Scotland (Edinburgh, 1695) and the Royal Bank of Scotland (Edinburgh, 1727). Glasgow would soon follow and Scotland had a flourishing financial system by the end of the century. There were over 400 branches, amounting to one office per 7,000 people, double the level in England, where banks were also more heavily regulated. Historians have emphasised that the flexibility and dynamism of the Scottish banking system contributed significantly to the rapid development of the economy in the 19th century.

German sociologist Max Weber mentioned Scottish Presbyterianism in The Protestant Ethic and the Spirit of Capitalism (1905), and many scholars in recent decades argued that "this worldly asceticism" of Calvinism was integral to Scotland's rapid economic modernisation.

In the 1690s the Presbyterian establishment purged the land of Episcopalians and heretics, and made blasphemy a capital crime. Thomas Aitkenhead, the son of an Edinburgh surgeon, aged 18, was indicted for blasphemy by order of the Privy Council for calling the New Testament "The History of the Imposter Christ"; he was hanged in 1696. Their extremism led to a reaction known as the "Moderate" cause that ultimately prevailed and opened the way for liberal thinking in the cities.

The early 18th century saw the beginnings of a fragmentation of the Church of Scotland. These fractures were prompted by issues of government and patronage, but reflected a wider division between the hard-line Evangelicals and the theologically more tolerant Moderate Party. The battle was over fears of fanaticism by the former and the promotion of Enlightenment ideas by the latter. The Patronage Act of 1712 was a major blow to the evangelicals, for it meant that local landlords could choose the minister, not the members of the congregation. Schisms erupted as the evangelicals left the main body, starting in 1733 with the First Secession headed by figures including Ebenezer Erskine. The second schism in 1761 lead to the foundation of the independent Relief Church. These churches gained strength in the Evangelical Revival of the later 18th century. A key result was the main Presbyterian church was in the hands of the Moderate faction, which provided critical support for the Enlightenment in the cities.

Long after the triumph of the Church of Scotland in the Lowlands, Highlanders and Islanders clung to an old-fashioned Christianity infused with animistic folk beliefs and practices. The remoteness of the region and the lack of a Gaelic-speaking clergy undermined the missionary efforts of the established church. The later 18th century saw some success, owing to the efforts of the SSPCK missionaries and to the disruption of traditional society. Catholicism had been reduced to the fringes of the country, particularly the Gaelic-speaking areas of the Highlands and Islands. Conditions also grew worse for Catholics after the Jacobite rebellions and Catholicism was reduced to little more than a poorly run mission. Also important was Episcopalianism, which had retained supporters through the civil wars and changes of regime in the 17th century. Since most Episcopalians had given their support to the Jacobite rebellions in the early 18th century, they also suffered a decline in fortunes.

Although Scotland increasingly adopted the English language and wider cultural norms, its literature developed a distinct national identity and began to enjoy an international reputation. Allan Ramsay (1686–1758) laid the foundations of a reawakening of interest in older Scottish literature, as well as leading the trend for pastoral poetry, helping to develop the Habbie stanza as a poetic form. James Macpherson was the first Scottish poet to gain an international reputation, claiming to have found poetry written by Ossian, he published translations that acquired international popularity, being proclaimed as a Celtic equivalent of the Classical epics. "Fingal" written in 1762 was speedily translated into many European languages, and its deep appreciation of natural beauty and the melancholy tenderness of its treatment of the ancient legend did more than any single work to bring about the Romantic movement in European, and especially in German, literature, influencing Herder and Goethe. Eventually it became clear that the poems were not direct translations from the Gaelic, but flowery adaptations made to suit the aesthetic expectations of his audience. Both the major literary figures of the following century, Robert Burns and Walter Scott, would be highly influenced by the Ossian cycle. Burns, an Ayrshire poet and lyricist, is widely regarded as the national poet of Scotland and a major figure in the Romantic movement. As well as making original compositions, Burns also collected folk songs from across Scotland, often revising or adapting them. His poem (and song) "Auld Lang Syne" is often sung at Hogmanay (the last day of the year), and "Scots Wha Hae" served for a long time as an unofficial national anthem of the country.

A legacy of the Reformation in Scotland was the aim of having a school in every parish, which was underlined by an act of the Scottish parliament in 1696 (reinforced in 1801). In rural communities this obliged local landowners (heritors) to provide a schoolhouse and pay a schoolmaster, while ministers and local presbyteries oversaw the quality of the education. The headmaster or "dominie" was often university educated and enjoyed high local prestige. The kirk schools were active in the rural lowlands but played a minor role in the Highlands, the islands, and in the fast-growing industrial towns and cities. The schools taught in English, not in Gaelic, because that language was seen as a leftover of Catholicism and was not an expression of Scottish nationalism. In cities such as Glasgow the Catholics operated their own schools, which directed their youth into clerical and middle class occupations, as well as religious vocations.

A "democratic myth" emerged in the 19th century to the effect that many a "lad of pairts" had been able to rise up through the system to take high office and that literacy was much more widespread in Scotland than in neighbouring states, particularly England. Historical research has largely undermined the myth. Kirk schools were not free, attendance was not compulsory and they generally imparted only basic literacy such as the ability to read the Bible. Poor children, starting at age 7, were done by age 8 or 9; the majority were finished by age 11 or 12. The result was widespread basic reading ability; since there was an extra fee for writing, half the people never learned to write. Scots were not significantly better educated than the English and other contemporary nations. A few talented poor boys did go to university, but usually they were helped by aristocratic or gentry sponsors. Most of them became poorly paid teachers or ministers, and none became important figures in the Scottish Enlightenment or the Industrial Revolution.

By the 18th century there were five universities in Scotland, at Edinburgh, Glasgow, St. Andrews and King's and Marischial Colleges in Aberdeen, compared with only two in England. Originally oriented to clerical and legal training, after the religious and political upheavals of the 17th century they recovered with a lecture-based curriculum that was able to embrace economics and science, offering a high quality liberal education to the sons of the nobility and gentry. It helped the universities to become major centres of medical education and to put Scotland at the forefront of Enlightenment thinking.

Scotland's transformation into a rich leader of modern industry came suddenly and unexpectedly. The population grew steadily in the 19th century, from 1,608,000 in the census of 1801 to 2,889,000 in 1851 and 4,472,000 in 1901. The economy, long based on agriculture, began to industrialise after 1790. At first the leading industry, based in the west, was the spinning and weaving of cotton. In 1861 the American Civil War suddenly cut off the supplies of raw cotton and the industry never recovered. Thanks to its many entrepreneurs and engineers, and its large stock of easily mined coal, Scotland became a world centre for engineering, shipbuilding, and locomotive construction, with steel replacing iron after 1870.

The Scottish Reform Act 1832 increased the number of Scottish MPs and significantly widened the franchise to include more of the middle classes. From this point until the end of the century, the Whigs and (after 1859) their successors the Liberal Party, managed to gain a majority of the Westminster Parliamentary seats for Scotland, although these were often outnumbered by the much larger number of English and Welsh Conservatives. The English-educated Scottish peer Lord Aberdeen (1784–1860) led a coalition government from 1852–5, but in general very few Scots held office in the government. From the mid-century there were increasing calls for Home Rule for Scotland and when the Conservative Lord Salisbury became prime minister in 1885 he responded to pressure by reviving the post of Secretary of State for Scotland, which had been in abeyance since 1746. He appointed the Duke of Richmond, a wealthy landowner who was both Chancellor of Aberdeen University and Lord Lieutenant of Banff. Towards the end of the century Prime Ministers of Scottish descent included the Tory, Peelite and Liberal William Gladstone, who held the office four times between 1868 and 1894. The first Scottish Liberal to become prime minister was the Earl of Rosebery, from 1894 to 1895, like Aberdeen before him a product of the English education system. In the later 19th century the issue of Irish Home Rule led to a split among the Liberals, with a minority breaking away to form the Liberal Unionists in 1886. The growing importance of the working classes was marked by Keir Hardie's success in the Mid Lanarkshire by-election, 1888, leading to the foundation of the Scottish Labour Party, which was absorbed into the Independent Labour Party in 1895, with Hardie as its first leader.

From about 1790 textiles became the most important industry in the west of Scotland, especially the spinning and weaving of cotton, which flourished until in 1861 the American Civil War cut off the supplies of raw cotton. The industry never recovered, but by that time Scotland had developed heavy industries based on its coal and iron resources. The invention of the hot blast for smelting iron (1828) revolutionised the Scottish iron industry. As a result, Scotland became a centre for engineering, shipbuilding and the production of locomotives. Toward the end of the 19th century, steel production largely replaced iron production. Coal mining continued to grow into the 20th century, producing the fuel to heat homes, factories and drive steam engines locomotives and steamships. By 1914 there were 1,000,000 coal miners in Scotland. The stereotype emerged early on of Scottish colliers as brutish, non-religious and socially isolated serfs; that was an exaggeration, for their life style resembled the miners everywhere, with a strong emphasis on masculinity, equalitarianism, group solidarity, and support for radical labour movements.

Britain was the world leader in the construction of railways, and their use to expand trade and coal supplies. The first successful locomotive-powered line in Scotland, between Monkland and Kirkintilloch, opened in 1831. Not only was good passenger service established by the late 1840s, but an excellent network of freight lines reduce the cost of shipping coal, and made products manufactured in Scotland competitive throughout Britain. For example, railways opened the London market to Scottish beef and milk. They enabled the Aberdeen Angus to become a cattle breed of worldwide reputation. By 1900 Scotland had 3500 miles of railway; their main economic contribution was moving supplies in and product out for heavy industry, especially coal-mining.
Scotland was already one of the most urbanised societies in Europe by 1800. The industrial belt ran across the country from southwest to northeast; by 1900 the four industrialised counties of Lanarkshire, Renfrewshire, Dunbartonshire, and Ayrshire contained 44 per cent of the population. Glasgow became one of the largest cities in the world, and known as "the Second City of the Empire" after London. Shipbuilding on Clydeside (the river Clyde through Glasgow and other points) began when the first small yards were opened in 1712 at the Scott family's shipyard at Greenock. After 1860 the Clydeside shipyards specialised in steamships made of iron (after 1870, made of steel), which rapidly replaced the wooden sailing vessels of both the merchant fleets and the battle fleets of the world. It became the world's pre-eminent shipbuilding centre. "Clydebuilt" became an industry benchmark of quality, and the river's shipyards were given contracts for warships.

The industrial developments, while they brought work and wealth, were so rapid that housing, town-planning, and provision for public health did not keep pace with them, and for a time living conditions in some of the towns and cities were notoriously bad, with overcrowding, high infant mortality, and growing rates of tuberculosis. The companies attracted rural workers, as well as immigrants from Catholic Ireland, by inexpensive company housing that was a dramatic move upward from the inner-city slums. This paternalistic policy led many owners to endorse government sponsored housing programs as well as self-help projects among the respectable working class.

While the Scottish Enlightenment is traditionally considered to have concluded toward the end of the 18th century, disproportionately large Scottish contributions to British science and letters continued for another 50 years or more, thanks to such figures as the mathematicians and physicists James Clerk Maxwell, Lord Kelvin, and the engineers and inventors James Watt and William Murdoch, whose work was critical to the technological developments of the Industrial Revolution throughout Britain.

In literature the most successful figure of the mid-nineteenth century was Walter Scott, who began as a poet and also collected and published Scottish ballads. His first prose work, Waverley in 1814, is often called the first historical novel. It launched a highly successful career that probably more than any other helped define and popularise Scottish cultural identity. In the late 19th century, a number of Scottish-born authors achieved international reputations. Robert Louis Stevenson's work included the urban Gothic novella "Strange Case of Dr Jekyll and Mr Hyde" (1886), and played a major part in developing the historical adventure in books like "Kidnapped" and "Treasure Island". Arthur Conan Doyle's "Sherlock Holmes" stories helped found the tradition of detective fiction. The "kailyard tradition" at the end of the century, brought elements of fantasy and folklore back into fashion as can be seen in the work of figures like J. M. Barrie, most famous for his creation of Peter Pan, and George MacDonald, whose works, including "Phantasies", played a major part in the creation of the fantasy genre.

Scotland also played a major part in the development of art and architecture. The Glasgow School, which developed in the late 19th century, and flourished in the early 20th century, produced a distinctive blend of influences including the Celtic Revival the Arts and Crafts Movement, and Japonisme, which found favour throughout the modern art world of continental Europe and helped define the Art Nouveau style. Among the most prominent members were the loose collective of The Four: acclaimed architect Charles Rennie Mackintosh, his wife the painter and glass artist Margaret MacDonald, her sister the artist Frances, and her husband, the artist and teacher Herbert MacNair.

This period saw a process of rehabilitation for highland culture. Tartan had already been adopted for highland regiments in the British army, which poor highlanders joined in large numbers until the end of the Napoleonic Wars in 1815, but by the 19th century it had largely been abandoned by the ordinary people. In the 1820s, as part of the Romantic revival, tartan and the kilt were adopted by members of the social elite, not just in Scotland, but across Europe, prompted by the popularity of Macpherson's Ossian cycle and then Walter Scott's Waverley novels. The world paid attention to their literary redefinition of Scottishness, as they forged an image largely based on characteristics in polar opposition to those associated with England and modernity. This new identity made it possible for Scottish culture to become integrated into a wider European and North American context, not to mention tourist sites, but it also locked in a sense of "otherness" which Scotland began to shed only in the late 20th century. Scott's "staging" of the royal Visit of King George IV to Scotland in 1822 and the king's wearing of tartan, resulted in a massive upsurge in demand for kilts and tartans that could not be met by the Scottish linen industry. The designation of individual clan tartans was largely defined in this period and became a major symbol of Scottish identity. The fashion for all things Scottish was maintained by Queen Victoria, who helped secure the identity of Scotland as a tourist resort, with Balmoral Castle in Aberdeenshire becoming a major royal residence from 1852.

Despite these changes the highlands remained very poor and traditional, with few connections to the uplift of the Scottish Enlightenment and little role in the Industrial Revolution. A handful of powerful families, typified by the dukes of Argyll, Atholl, Buccleuch, and Sutherland, owned the best lands and controlled local political, legal and economic affairs. Particularly after the end of the boom created by the Revolutionary and Napoleonic Wars (1790–1815), these landlords needed cash to maintain their position in London society, and had less need of soldiers. They turned to money rents, displaced farmers to raise sheep, and downplayed the traditional patriarchal relationship that had historically sustained the clans. This was exacerbated after the repeal of the Corn Laws in mid-century, when Britain adopted a free trade policy, and grain imports from America undermined the profitability of crop production. The Irish potato famine of the 1840s was caused by a plant disease that reached the Highlands in 1846, where 150,000 people faced disaster because their food supply was largely potatoes (with a little herring, oatmeal and milk). They were rescued by an effective emergency relief system that stands in dramatic contrast to the failures of relief in Ireland.

The unequal concentration of land ownership remained an emotional subject and eventually became a cornerstone of liberal radicalism. The politically powerless poor crofters embraced the popularly oriented, fervently evangelical Presbyterian revival after 1800, and the breakaway "Free Church" after 1843. This evangelical movement was led by lay preachers who themselves came from the lower strata, and whose preaching was implicitly critical of the established order. This energised the crofters and separated them from the landlords, preparing them for their successful and violent challenge to the landlords in the 1880s through the Highland Land League. Violence began on the Isle of Skye when Highland landlords cleared their lands for sheep and deer parks. It was quieted when the government stepped in passing the Crofters' Holdings (Scotland) Act, 1886 to reduce rents, guarantee fixity of tenure, and break up large estates to provide crofts for the homeless. In 1885 three Independent Crofter candidates were elected to Parliament, leading to explicit security for the Scottish smallholders; the legal right to bequeath tenancies to descendants; and creating a Crofting Commission. The Crofters as a political movement faded away by 1892, and the Liberal Party gained most of their votes.

The population of Scotland grew steadily in the 19th century, from 1,608,000 in the census of 1801 to 2,889,000 in 1851 and 4,472,000 in 1901. Even with the development of industry there were insufficient good jobs; as a result, during the period 1841–1931, about 2 million Scots migrated to North America and Australia, and another 750,000 Scots relocated to England. Scotland lost a much higher proportion of its population than England and Wales, reaching perhaps as much as 30.2 per cent of its natural increase from the 1850s onwards. This not only limited Scotland's population increase, but meant that almost every family lost members due to emigration and, because more of the migrants were young males, it skewed the sex and age ratios of the country.

Scots-born migrants that played a leading role in the foundation and development of the United States included cleric and revolutionary John Witherspoon, sailor John Paul Jones, industrialist and philanthropist Andrew Carnegie, and scientist and inventor Alexander Graham Bell. In Canada they included soldier and governor of Quebec James Murray, Prime Minister John A. Macdonald and politician and social reformer Tommy Douglas. For Australia they included soldier and governor Lachlan Macquarie, governor and scientist Thomas Brisbane and Prime Minister Andrew Fisher. For New Zealand they included politician Peter Fraser and outlaw James Mckenzie. By the 21st century, there would be about as many people who were Scottish Canadians and Scottish Americans as the 5 million remaining in Scotland.

After prolonged years of struggle, in 1834 the Evangelicals gained control of the General Assembly and passed the Veto Act, which allowed congregations to reject unwanted "intrusive" presentations to livings by patrons. The following "Ten Years' Conflict" of legal and political wrangling ended in defeat for the non-intrusionists in the civil courts. The result was a schism from the church by some of the non-intrusionists led by Dr Thomas Chalmers known as the Great Disruption of 1843. Roughly a third of the clergy, mainly from the North and Highlands, formed the separate Free Church of Scotland. The evangelical Free Churches, which were more accepting of Gaelic language and culture, grew rapidly in the Highlands and Islands, appealing much more strongly than did the established church. Chalmers's ideas shaped the breakaway group. He stressed a social vision that revived and preserved Scotland's communal traditions at a time of strain on the social fabric of the country. Chalmers's idealised small equalitarian, kirk-based, self-contained communities that recognised the individuality of their members and the need for co-operation. That vision also affected the mainstream Presbyterian churches, and by the 1870s it had been assimilated by the established Church of Scotland. Chalmers's ideals demonstrated that the church was concerned with the problems of urban society, and they represented a real attempt to overcome the social fragmentation that took place in industrial towns and cities.

In the late 19th century the major debates were between fundamentalist Calvinists and theological liberals, who rejected a literal interpretation of the Bible. This resulted in a further split in the Free Church as the rigid Calvinists broke away to form the Free Presbyterian Church in 1893. There were, however, also moves towards reunion, beginning with the unification of some secessionist churches into the United Secession Church in 1820, which united with the Relief Church in 1847 to form the United Presbyterian Church, which in turn joined with the Free Church in 1900 to form the United Free Church of Scotland. The removal of legislation on lay patronage would allow the majority of the Free Church to rejoin Church of Scotland in 1929. The schisms left small denominations including the Free Presbyterians and a remnant that had not merged in 1900 as the Free Church.

Catholic Emancipation in 1829 and the influx of large numbers of Irish immigrants, particularly after the famine years of the late 1840s, principally to the growing lowland centres like Glasgow, led to a transformation in the fortunes of Catholicism. In 1878, despite opposition, a Roman Catholic ecclesiastical hierarchy was restored to the country, and Catholicism became a significant denomination within Scotland. Episcopalianism also revived in the 19th century as the issue of succession receded, becoming established as the Episcopal Church in Scotland in 1804, as an autonomous organisation in communion with the Church of England. Baptist, Congregationalist and Methodist churches had appeared in Scotland in the 18th century, but did not begin significant growth until the 19th century, partly because more radical and evangelical traditions already existed within the Church of Scotland and the free churches. From 1879 they were joined by the evangelical revivalism of the Salvation Army, which attempted to make major inroads in the growing urban centres.

Industrialisation, urbanisation and the Disruption of 1843 all undermined the tradition of parish schools. From 1830 the state began to fund buildings with grants, then from 1846 it was funding schools by direct sponsorship, and in 1872 Scotland moved to a system like that in England of state-sponsored largely free schools, run by local school boards. Overall administration was in the hands of the Scotch (later Scottish) Education Department in London. Education was now compulsory from five to thirteen and many new board schools were built. Larger urban school boards established "higher grade" (secondary) schools as a cheaper alternative to the burgh schools. The Scottish Education Department introduced a Leaving Certificate Examination in 1888 to set national standards for secondary education and in 1890 school fees were abolished, creating a state-funded national system of free basic education and common examinations.

At the beginning of the 19th century, Scottish universities had no entrance exam, students typically entered at ages of 15 or 16, attended for as little as two years, chose which lectures to attend and could leave without qualifications. After two commissions of enquiry in 1826 and 1876 and reforming acts of parliament in 1858 and 1889, the curriculum and system of graduation were reformed to meet the needs of the emerging middle classes and the professions. Entrance examinations equivalent to the School Leaving Certificate were introduced and average ages of entry rose to 17 or 18. Standard patterns of graduation in the arts curriculum offered 3-year ordinary and 4-year honours degrees and separate science faculties were able to move away from the compulsory Latin, Greek and philosophy of the old MA curriculum. The historic University of Glasgow became a leader in British higher education by providing the educational needs of youth from the urban and commercial classes, as well as the upper class. It prepared students for non-commercial careers in government, the law, medicine, education, and the ministry and a smaller group for careers in science and engineering. St Andrews pioneered the admission of women to Scottish universities, creating the Lady Licentiate in Arts (LLA), which proved highly popular. From 1892 Scottish universities could admit and graduate women and the numbers of women at Scottish universities steadily increased until the early 20th century.

The years before the First World War were the golden age of the inshore fisheries. Landings reached new heights, and Scottish catches dominated Europe's herring trade, accounting for a third of the British catch. High productivity came about thanks to the transition to more productive steam-powered boats, while the rest of Europe's fishing fleets were slower because they were still powered by sails.

In the Khaki Election of 1900, nationalist concern with the Boer War meant that the Conservatives and their Liberal Unionist allies gained a majority of Scottish seats for the first time, although the Liberals regained their ascendancy in the next election. The Unionists and Conservatives merged in 1912, usually known as the Conservatives in England and Wales, they adopted the name Unionist Party in Scotland. Scots played a major part in the leadership of UK political parties producing a Conservative Prime Minister in Arthur Balfour (1902–05) and a Liberal one in Henry Campbell-Bannerman (1905–08). Various organisations, including the Independent Labour Party, joined to make the British Labour Party in 1906, with Keir Hardie as its first chairman.

Scotland played a major role in the British effort in the First World War. It especially provided manpower, ships, machinery, food (particularly fish) and money, engaging with the conflict with some enthusiasm. Scotland's industries were directed at the war effort. For example, the Singer Clydebank sewing machine factory received over 5000 government contracts, and made 303 million artillery shells, shell components, fuses, and aeroplane parts, as well as grenades, rifle parts, and 361,000 horseshoes. Its labour force of 14,000 was about 70 percent female at war's end.

With a population of 4.8 million in 1911, Scotland sent 690,000 men to the war, of whom 74,000 died in combat or from disease, and 150,000 were seriously wounded. Scottish urban centres, with their poverty and unemployment, were favourite recruiting grounds of the regular British army, and Dundee, where the female-dominated jute industry limited male employment, had one of the highest proportion of reservists and serving soldiers than almost any other British city. Concern for their families' standard of living made men hesitate to enlist; voluntary enlistment rates went up after the government guaranteed a weekly stipend for life to the survivors of men who were killed or disabled. After the introduction of conscription from January 1916 every part of the country was affected. Occasionally Scottish troops made up large proportions of the active combatants, and suffered corresponding loses, as at the Battle of Loos, where there were three full Scots divisions and other Scottish units. Thus, although Scots were only 10 per cent of the British population, they made up 15 per cent of the national armed forces and eventually accounted for 20 per cent of the dead. Some areas, like the thinly populated Island of Lewis and Harris, suffered some of the highest proportional losses of any part of Britain. Clydeside shipyards and the nearby engineering shops were the major centres of war industry in Scotland. In Glasgow, radical agitation led to industrial and political unrest that continued after the war ended. After the end of the war in June 1919 the German fleet interned in Scapa Flow was scuttled by its German crews, to avoid its ships being taken over by the victorious allies.

A boom was created by the First World War, with the shipbuilding industry expanding by a third, but a serious depression hit the economy by 1922. The most skilled craftsmen were especially hard hit, because there were few alternative uses for their specialised skills. The main social indicators such as poor health, bad housing, and long-term mass unemployment, pointed to terminal social and economic stagnation at best, or even a downward spiral. The heavy dependence on obsolescent heavy industry and mining was a central problem, and no one offered workable solutions. The despair reflected what Finlay (1994) describes as a widespread sense of hopelessness that prepared local business and political leaders to accept a new orthodoxy of centralised government economic planning when it arrived during the Second World War.

A few industries did grow, such as chemicals and whisky, which developed a global market for premium "Scotch". However, in general the Scottish economy stagnated leading to growing unemployment and political agitation among industrial workers.

After World War I the Liberal Party began to disintegrate and Labour emerged as the party of progressive politics in Scotland, gaining a solid following among working classes of the urban lowlands. As a result, the Unionists were able to gain most of the votes of the middle classes, who now feared Bolshevik revolution, setting the social and geographical electoral pattern in Scotland that would last until the late 20th century. The fear of the left had been fuelled by the emergence of a radical movement led by militant trades unionists. John MacLean emerged as a key political figure in what became known as Red Clydeside, and in January 1919, the British Government, fearful of a revolutionary uprising, deployed tanks and soldiers in central Glasgow. Formerly a Liberal stronghold, the industrial districts switched to Labour by 1922, with a base in the Irish Catholic working class districts. Women were especially active in building neighbourhood solidarity on housing and rent issues. However, the "Reds" operated within the Labour Party and had little influence in Parliament; in the face of heavy unemployment the workers' mood changed to passive despair by the late 1920s. Scottish educated Bonar Law led a Conservative government from 1922 to 1923 and another Scot, Ramsay MacDonald, would be the Labour Party's first Prime Minister in 1924 and again from 1929 to 1935.

With all the main parties committed to the Union, new nationalist and independent political groupings began to emerge, including the National Party of Scotland in 1928 and Scottish Party in 1930. They joined to form the Scottish National Party (SNP) in 1934, with the goal of creating an independent Scotland, but it enjoyed little electoral success in the Westminster system.

As in World War I, Scapa Flow in Orkney served as an important Royal Navy base. Attacks on Scapa Flow and Rosyth gave RAF fighters their first successes downing bombers in the Firth of Forth and East Lothian. The shipyards and heavy engineering factories in Glasgow and Clydeside played a key part in the war effort, and suffered attacks from the Luftwaffe, enduring great destruction and loss of life. As transatlantic voyages involved negotiating north-west Britain, Scotland played a key part in the battle of the North Atlantic. Shetland's relative proximity to occupied Norway resulted in the Shetland Bus by which fishing boats helped Norwegians flee the Nazis, and expeditions across the North Sea to assist resistance. Significant individual contributions to the war effort by Scots included the invention of radar by Robert Watson-Watt, which was invaluable in the Battle of Britain, as was the leadership at RAF Fighter Command of Air Chief Marshal Hugh Dowding.

In World War II, Prime Minister Winston Churchill appointed Labour politician Tom Johnston as Secretary of State for Scotland in February 1941; he controlled Scottish affairs until the war ended. He launched numerous initiatives to promote Scotland, attracting businesses and new jobs through his new Scottish Council of Industry. He set up 32 committees to deal with social and economic problems, ranging from juvenile delinquency to sheep farming. He regulated rents, and set up a prototype national health service, using new hospitals set up in the expectation of large numbers of casualties from German bombing. His most successful venture was setting up a system of hydro electricity using water power in the Highlands. A long-standing supporter of the Home Rule movement, Johnston persuaded Churchill of the need to counter the nationalist threat north of the border and created a Scottish Council of State and a Council of Industry as institutions to devolve some power away from Whitehall.

In World War II, despite extensive bombing by the Luftwaffe, Scottish industry came out of the depression slump by a dramatic expansion of its industrial activity, absorbing unemployed men and many women as well. The shipyards were the centre of more activity, but many smaller industries produced the machinery needed by the British bombers, tanks and warships. Agriculture prospered, as did all sectors except for coal mining, which was operating mines near exhaustion. Real wages, adjusted for inflation, rose 25 per cent, and unemployment temporarily vanished. Increased income, and the more equal distribution of food, obtained through a tight rationing system, dramatically improved the health and nutrition; the average height of 13-year-olds in Glasgow increased by .

While emigration began to tail off in England and Wales after the First World War, it continued apace in Scotland, with 400,000 Scots, ten per cent of the population, estimated to have left the country between 1921 and 1931. The economic stagnation was only one factor; other push factors included a zest for travel and adventure, and the pull factors of better job opportunities abroad, personal networks to link into, and the basic cultural similarity of the United States, Canada, and Australia. Government subsidies for travel and relocation facilitated the decision to emigrate. Personal networks of family and friends who had gone ahead and wrote back, or sent money, prompted emigrants to retrace their paths. When the Great Depression hit in the 1930s there were no easily available jobs in the US and Canada and emigration fell to less than 50,000 a year, bringing to an end the period of mass migrations that had opened in the mid-18th century.

In the early 20th century there was a new surge of activity in Scottish literature, influenced by modernism and resurgent nationalism, known as the Scottish Renaissance. The leading figure in the movement was Hugh MacDiarmid (the pseudonym of Christopher Murray Grieve). MacDiarmid attempted to revive the Scots language as a medium for serious literature in poetic works including "A Drunk Man Looks at the Thistle" (1936), developing a form of Synthetic Scots that combined different regional dialects and archaic terms. Other writers that emerged in this period, and are often treated as part of the movement, include the poets Edwin Muir and William Soutar, the novelists Neil Gunn, George Blake, Nan Shepherd, A. J. Cronin, Naomi Mitchison, Eric Linklater and Lewis Grassic Gibbon, and the playwright James Bridie. All were born within a fifteen-year period (1887 and 1901) and, although they cannot be described as members of a single school, they all pursued an exploration of identity, rejecting nostalgia and parochialism and engaging with social and political issues.

In the 20th century the centre of the education system became more focused on Scotland, with the ministry of education partly moving north in 1918 and then finally having its headquarters relocated to Edinburgh in 1939. The school leaving age was raised to 14 in 1901, but despite attempts to raise it to 15 this was only made law in 1939 and then postponed because of the outbreak of war. In 1918 Roman Catholic schools were brought into the state system, but retained their distinct religious character, access to schools by priests and the requirement that school staff be acceptable to the Church.

The first half of the 20th century saw Scottish universities fall behind those in England and Europe in terms of participation and investment. The decline of traditional industries between the wars undermined recruitment. English universities increased the numbers of students registered between 1924 and 1927 by 19 per cent, but in Scotland the numbers fell, particularly among women. In the same period, while expenditure in English universities rose by 90 per cent, in Scotland the increase was less than a third of that figure.

Scotland's Scapa Flow was the main base for the Royal Navy in the 20th century. As the Cold War intensified in 1961, the United States deployed Polaris ballistic missiles, and submarines, in the Firth of Clyde's Holy Loch. Public protests from CND campaigners proved futile. The Royal Navy successfully convinced the government to allow the base because it wanted its own "Polaris"-class submarines, and it obtained them in 1963. The RN's nuclear submarine base opened four "Resolution" class Polaris submarines at the expanded Faslane Naval Base on the Gare Loch. The first patrol of a Trident-armed submarine occurred in 1994, although the US base was closed at the end of the Cold War.

After World War II, Scotland's economic situation became progressively worse due to overseas competition, inefficient industry, and industrial disputes. This only began to change in the 1970s, partly due to the discovery and development of North Sea oil and gas and partly as Scotland moved towards a more service-based economy. This period saw the emergence of the Scottish National Party and movements for both Scottish independence and more popularly devolution. However, a referendum on devolution in 1979 was unsuccessful as it did not achieve the support of 40 per cent of the electorate (despite a small majority of those who voted supporting the proposal.)

A national referendum to decide on Scottish independence was held on 18 September 2014. Voters were asked to answer either "Yes" or "No" to the question: "Should Scotland be an independent country?" 55.3% of voters answered "No" and 44.7% answered "Yes", with a voter turnout of 84.5%.

In the second half of the 20th century the Labour Party usually won most Scottish seats in the Westminster parliament, losing this dominance briefly to the Unionists in the 1950s. Support in Scotland was critical to Labour's overall electoral fortunes as without Scottish MPs it would have gained only two UK electoral victories in the 20th century (1945 and 1966). The number of Scottish seats represented by Unionists (known as Conservatives from 1965 onwards) went into steady decline from 1959 onwards, until it fell to zero in 1997. Politicians with Scottish connections continued to play a prominent part in UK political life, with Prime Ministers including the Conservatives Harold Macmillan (whose father was Scottish) from 1957 to 1963 and Alec Douglas-Home from 1963 to 1964.

The Scottish National Party gained its first seat at Westminster in 1945 and became a party of national prominence during the 1970s, achieving 11 MPs in 1974. However, a referendum on devolution in 1979 was unsuccessful as it did not achieve the necessary support of 40 per cent of the electorate (despite a small majority of those who voted supporting the proposal) and the SNP went into electoral decline during the 1980s. The introduction in 1989 by the Thatcher-led Conservative government of the Community Charge (widely known as the Poll Tax), one year before the rest of the United Kingdom, contributed to a growing movement for a return to direct Scottish control over domestic affairs. The electoral success of New Labour in 1997 was led by two Prime Ministers with Scottish connections: Tony Blair (who was brought up in Scotland) from 1997 to 2007 and Gordon Brown from 2007 to 2010, opened the way for constitutional change. On 11 September 1997, the 700th anniversary of Battle of Stirling Bridge, the Blair led Labour government again held a referendum on the issue of devolution. A positive outcome led to the establishment of a devolved Scottish Parliament in 1999. A coalition government, which would last until 2007, was formed between Labour and the Liberal Democrats, with Donald Dewar as First Minister. The new Scottish Parliament Building, adjacent to Holyrood House in Edinburgh, opened in 2004. Although not initially reaching its 1970s peak in Westminster elections, the SNP had more success in the Scottish Parliamentary elections with their system of mixed member proportional representation. It became the official opposition in 1999, a minority government in 2007 and a majority government from 2011. In 2014 the independence referendum saw voters reject independence, choosing instead to remain in the United Kingdom. In the 2015 Westminster election, the SNP won 56 out of 59 Scottish seats, making them the third largest party in Westminster.

After World War II, Scotland's economic situation became progressively worse due to overseas competition, inefficient industry, and industrial disputes. This only began to change in the 1970s, partly due to the discovery and development of North Sea oil and gas and partly as Scotland moved towards a more service-based economy. The discovery of the giant Forties oilfield in October 1970 signalled that Scotland was about to become a major oil producing nation, a view confirmed when Shell Expro discovered the giant Brent oilfield in the northern North Sea east of Shetland in 1971. Oil production started from the Argyll field (now Ardmore) in June 1975, followed by Forties in November of that year. Deindustrialisation took place rapidly in the 1970s and 1980s, as most of the traditional industries drastically shrank or were completely closed down. A new service-oriented economy emerged to replace traditional heavy industries. This included a resurgent financial services industry and the electronics manufacturing of Silicon Glen.

In the 20th century existing Christian denominations were joined by other organisations, including the Brethren and Pentecostal churches. Although some denominations thrived, after World War II there was a steady overall decline in church attendance and resulting church closures for most denominations. Talks began in the 1950s aiming at a grand merger of the main Presbyterian, Episcopal and Methodist bodies in Scotland. The talks were ended in 2003, when the General Assembly of the Church of Scotland rejected the proposals. In the 2011 census, 53.8% of the Scottish population identified as Christian (declining from 65.1% in 2001). The Church of Scotland is the largest religious grouping in Scotland, with 32.4% of the population. The Roman Catholic Church accounted for 15.9% of the population and is especially important in West Central Scotland and the Highlands. In recent years other religions have established a presence in Scotland, mainly through immigration and higher birth rates among ethnic minorities, with a small number of converts. Those with the most adherents in the 2011 census are Islam (1.4%, mainly among immigrants from South Asia), Hinduism (0.3%), Buddhism (0.2%) and Sikhism (0.2%). Other minority faiths include the Bahá'í Faith and small Neopagan groups. There are also various organisations which actively promote humanism and secularism, included within the 43.6% who either indicated no religion or did not state a religion in the 2011 census.

Although plans to raise the school leaving age to 15 in the 1940s were never ratified, increasing numbers stayed on beyond elementary education and it was eventually raised to 16 in 1973. As a result, secondary education was the major area of growth in the second half of the 20th century. New qualifications were developed to cope with changing aspirations and economics, with the Leaving Certificate being replaced by the Scottish Certificate of Education Ordinary Grade ('O-Grade') and Higher Grade ('Higher') qualifications in 1962, which became the basic entry qualification for university study. The higher education sector expanded in the second half of the 20th century, with four institutions being given university status in the 1960s (Dundee, Heriot-Watt, Stirling and Strathclyde) and five in the 1990s (Abertay, Glasgow Caledonian, Napier, Paisley and Robert Gordon). After devolution, in 1999 the new Scottish Executive set up an Education Department and an Enterprise, Transport and Lifelong Learning Department. One of the major diversions from practice in England, possible because of devolution, was the abolition of student tuition fees in 1999, instead retaining a system of means-tested student grants.

Some writers that emerged after the Second World War followed Hugh MacDiarmid by writing in Scots, including Robert Garioch and Sydney Goodsir Smith. Others demonstrated a greater interest in English language poetry, among them Norman MacCaig, George Bruce and Maurice Lindsay. George Mackay Brown from Orkney, and Iain Crichton Smith from Lewis, wrote both poetry and prose fiction shaped by their distinctive island backgrounds. The Glaswegian poet Edwin Morgan became known for translations of works from a wide range of European languages. He was also the first Scots Makar (the official national poet), appointed by the inaugural Scottish government in 2004. Many major Scottish post-war novelists, such as Muriel Spark, with "The Prime of Miss Jean Brodie" (1961) spent much or most of their lives outside Scotland, but often dealt with Scottish themes. Successful mass-market works included the action novels of Alistair MacLean, and the historical fiction of Dorothy Dunnett. A younger generation of novelists that emerged in the 1960s and 1970s included Shena Mackay, Alan Spence, Allan Massie and the work of William McIlvanney. From the 1980s Scottish literature enjoyed another major revival, particularly associated with a group of Glasgow writers focused around critic, poet and teacher Philip Hobsbaum and editor Peter Kravitz. In the 1990s major, prize winning, Scottish novels, often overtly political, that emerged from this movement included Irvine Welsh's "Trainspotting" (1993), Warner's "Morvern Callar" (1995), Gray’s "Poor Things" (1992) and Kelman’s "How Late It Was, How Late" (1994). Scottish crime fiction has been a major area of growth, particularly the success of Edinburgh's Ian Rankin and his Inspector Rebus novels. This period also saw the emergence of a new generation of Scottish poets that became leading figures on the UK stage, including Carol Ann Duffy, who was named as Poet Laureate in May 2009, the first woman, the first Scot and the first openly gay poet to take the post.















</doc>
<doc id="13621" url="https://en.wikipedia.org/wiki?curid=13621" title="Hadrian">
Hadrian

Hadrian (; ; 24 January 76 – 10 July 138 AD) was Roman emperor from 117 to 138. He was born Publius Aelius Hadrianus, probably at Italica, near Santiponce (in modern-day Spain), into a Hispano-Roman family. His father was of senatorial rank, and was a first cousin of the emperor Trajan. Early in Hadrian's career, before Trajan became emperor, he married Trajan's grand-niece Vibia Sabina, possibly at the behest of Trajan's wife, Pompeia Plotina. Plotina and Trajan's close friend and adviser Licinius Sura, were well disposed towards Hadrian. When Trajan died, his widow claimed that immediately before his death, he had nominated Hadrian as emperor. 

Rome's military and Senate approved Hadrian's succession, but soon after, four leading senators who had opposed Hadrian, or seemed to threaten his succession, were unlawfully put to death; the senate held Hadrian responsible for this, and never forgave him. He earned further disapproval among the elite by abandoning Trajan's expansionist policies and recent territorial gains in Mesopotamia, Assyria and Armenia, and parts of Dacia. Hadrian preferred to invest in the development of stable, defensible borders, and the unification, under his overall leadership, of the empire's disparate peoples. He is known for building Hadrian's Wall, which marked the northern limit of Britannia. 

Hadrian energetically pursued his own Imperial ideals and personal interests. He visited almost every province of the Empire, accompanied by a probably vast Imperial retinue of specialists and administrators. He encouraged military preparedness and discipline, and fostered, designed or personally subsidised various civil and religious institutions and building projects. In Rome itself, he rebuilt or completed the Pantheon, and constructed the vast Temple of Venus and Roma. In Egypt, he may have rebuilt the Serapeum of Alexandria. An ardent admirer of Greece, he sought to make Athens the cultural capital of the Empire and ordered the construction of many opulent temples there. His intense relationship with the Greek youth Antinous, and the latter's untimely death, led to Hadrian's establishment of an enduring and widespread popular cult. Late in his reign he suppressed the Bar Kokhba revolt in Judaea; with this major exception, Hadrian's reign was generally peaceful.

Hadrian's last years were marred by chronic illness. He saw the Bar Kokhba revolt as the failure of his panhellenic ideal. His execution of two more senators for their alleged plots against him provoked further resentment. His marriage to Vibia Sabina had been unhappy, and childless; in 138 he adopted Antoninus Pius and nominated him as a successor, on the condition that Antoninus adopt Marcus Aurelius and Lucius Verus as his own heirs. Hadrian died the same year at Baiae, and Antoninus had him deified, despite opposition from the Senate. Edward Gibbon includes him among the Empire's "Five good emperors", a "benevolent dictator"; Hadrian's own senate found him remote and authoritarian. He has been described as enigmatic and contradictory, with a capacity for both great personal generosity and extreme cruelty, and driven by insatiable curiosity, self-conceit, and above all, ambition. Modern interest was revived largely thanks to Marguerite Yourcenar's novel "Mémoires d'Hadrien" (1951).

Hadrian was born on 24January 76, probably in Italica (near modern Seville) in the Roman province of Hispania Baetica; one Roman biographer claims he was born at Rome. He was named Publius Aelius Hadrianus. His father was Publius Aelius Hadrianus Afer, a senator of praetorian rank, born and raised in Italica but paternally linked, through many generations over several centuries, to a family from Hadria (modern Atri), an ancient town in Picenum. The family had settled in Italica soon after its founding by Scipio Africanus. Hadrian's mother was Domitia Paulina, daughter of a distinguished Hispano-Roman senatorial family from Gades (Cádiz). His only sibling was an elder sister, Aelia Domitia Paulina. Hadrian's great-nephew, Gnaeus Pedanius Fuscus Salinator, from Barcino (Barcelona) would become Hadrian's colleague as co-consul in 118. As a senator, Hadrian's father would have spent much of his time in Rome. In terms of his later career, Hadrian's most significant family connection was to Trajan, his father's first cousin, who was also of senatorial stock, and had been born and raised in Italica. Hadrian and Trajan were both considered to bein the words of Aurelius Victor"aliens", people "from the outside" ("advenae").

Hadrian's parents died in 86, when he was ten years old. He and his sister became wards of Trajan and Publius Acilius Attianus (who later became Trajan's Praetorian prefect). Hadrian was physically active, and enjoyed hunting; when he was 14, Trajan called him to Rome and arranged his further education in subjects appropriate to a young Roman aristocrat. Hadrian's enthusiasm for Greek literature and culture earned him the nickname "Graeculus" ("Greekling"). Trajan married Paulina off to the three-times consul Lucius Julius Ursus Servianus; the couple had a daughter, Julia Serviana Paulina.

Hadrian's first official post in Rome was as a judge at the Inheritance court, one among many vigintivirate offices at the lowest level of the cursus honorum ("course of honours") that could lead to higher office and a senatorial career. He then served as a military tribune, first with the LegioII "Adiutrix" in 95, then with the Legio V Macedonica. During Hadrian's second stint as tribune, the frail and aged reigning emperor Nerva adopted Trajan as his heir; Hadrian was dispatched to give Trajan the news— or most probably was one of many emissaries charged with this same commission. Then he was transferred to Legio XXII Primigenia and a third tribunate. Hadrian's three tribunates gave him some career advantage. Most scions of the older senatorial families might serve one, or at most two military tribunates as a prerequisite to higher office. When Nerva died in 98, Hadrian is said to have hastened to Trajan, to inform him ahead of the official envoy sent by the governor, Hadrian's brother-in-law and rival Lucius Julius Ursus Servianus.

In 101, Hadrian was back in Rome; he was elected quaestor, then "quaestor imperatoris Traiani", liaison officer between Emperor and the assembled Senate, to whom he read the Emperor's communiqués and speeches – which he possibly composed on the emperor's behalf. In his role as imperial ghostwriter, Hadrian took the place of the recently deceased Licinius Sura, Trajan's all-powerful friend and kingmaker. His next post was as "ab actis senatus", keeping the Senate's records. During the First Dacian War, Hadrian took the field as a member of Trajan's personal entourage, but was excused from his military post to take office in Rome as Tribune of the Plebs, in 105. After the war, he was probably elected praetor. During the Second Dacian War, Hadrian was in Trajan's personal service again, but was released to serve as legate of Legio I Minervia, then as governor of Lower Pannonia in 107, tasked with "holding back the Sarmatians". 

Now in his mid-thirties, Hadrian travelled to Greece; he was granted Athenian citizenship and was appointed eponymous archon of Athens for a brief time (in 112). The Athenians awarded him a statue with an inscription in the Theater of Dionysus ( IG II2 3286) offering a detailed account of his "cursus honorum" thus far. Thereafter no more is heard of him until Trajan's Parthian War. It is possible that he remained in Greece until his recall to the imperial retinue, when he joined Trajan's expedition against Parthia as a legate. When the governor of Syria was sent to deal with renewed troubles in Dacia, Hadrian was appointed his replacement, with independent command. Trajan became seriously ill, and took ship for Rome, while Hadrian remained in Syria, "de facto" general commander of the Eastern Roman army. Trajan got as far as the coastal city of Selinus, in Cilicia, and died there, on 8 August; he would be regarded as one of Rome's most admired, popular and best emperors. 

Around the time of his quaestorship, in 100 or 101, Hadrian had married Trajan's twelve-year-old grandniece, Vibia Sabina. Trajan himself seems to have been less than enthusiastic about the marriage, and with good reason, as the couple's relationship would prove to be scandalously poor. The marriage might have been arranged by Trajan's empress, Plotina. This highly cultured, influential woman shared many of Hadrian's values and interests, including the idea of the Roman Empire as a commonwealth with an underlying Hellenic culture. If Hadrian were to be appointed Trajan's successor, Plotina and her extended family could retain their social profile and political influence after Trajan's death. Hadrian could also count on the support of his mother-in-law, Salonina Matidia, who was daughter of Trajan's beloved sister Ulpia Marciana. When Ulpia Marciana died, in 112, Trajan had her deified, and made Salonina Matidia an Augusta.

Hadrian's personal relationship with Trajan was complex, and may have been difficult. Hadrian seems to have sought influence over Trajan, or Trajan's decisions, through cultivation of the latter's boy favourites; this gave rise to some unexplained quarrel, around the time of Hadrian's marriage to Sabina. Late in Trajan's reign, Hadrian failed to achieve a senior consulship, being only suffect consul for 108; this gave him parity of status with other members of the senatorial nobility, but no particular distinction befitting an heir designate. Had Trajan wished it, he could have promoted his protege to patrician rank and its privileges, which included opportunities for a fast track to consulship without prior experience as tribune; he chose not to. While Hadrian seems to have been granted the office of Tribune of the Plebs a year or so younger than was customary, he had to leave Dacia, and Trajan, to take up the appointment; Trajan might simply have wanted him out of the way. The "Historia Augusta" describes Trajan's gift to Hadrian of a diamond ring that Trajan himself had received from Nerva, which "encouraged [Hadrian's] hopes of succeeding to the throne". While Trajan actively promoted Hadrian's advancement, he did so with caution.
Failure to nominate an heir could invite chaotic, destructive wresting of power by a succession of competing claimants - a civil war. Too early a nomination could be seen as an abdication, and reduce the chance for an orderly transmission of power. As Trajan lay dying, nursed by his wife, Plotina, and closely watched by Prefect Attianus, he could have lawfully adopted Hadrian as heir, by means of a simple deathbed wish, expressed before witnesses; but when an adoption document was eventually presented, it was signed not by Trajan but by Plotina, and was dated the day after Trajan's death. That Hadrian was still in Syria was a further irregularity, as Roman adoption law required the presence of both parties at the adoption ceremony. Rumours, doubts, and speculation attended Hadrian's adoption and succession. It has been suggested that Trajan's young manservant Phaedimus, who died very soon after Trajan, was killed (or killed himself) rather than face awkward questions. Ancient sources are divided on the legitimacy of Hadrian's adoption: Dio Cassius saw it as bogus and the "Historia Augusta" writer as genuine. An aureus minted early in Hadrian's reign represents the official position; it presents Hadrian as Trajan's "Caesar" (Trajan's heir designate).

According to the "Historia Augusta", Hadrian informed the Senate of his accession in a letter as a "fait accompli", explaining that "the unseemly haste of the troops in acclaiming him emperor was due to the belief that the state could not be without an emperor". The new emperor rewarded the legions' loyalty with the customary bonus, and the Senate endorsed the acclamation. Various public ceremonies were organized on Hadrian's behalf, celebrating his "divine election" by all the gods, whose community now included Trajan, deified at Hadrian's request. 
Hadrian remained in the east for a while, suppressing the Jewish revolt that had broken out under Trajan. He relieved Judea's governor, the outstanding Moorish general Lusius Quietus, of his personal guard of Moorish auxiliaries; 
then he moved on to quell disturbances along the Danube frontier. In Rome, Hadrian's former guardian and current Praetorian Prefect, Attianus, claimed to have uncovered a conspiracy involving four leading senators, who included Lusius Quietus. There was no public trial for the four – they were tried "in absentia", hunted down and killed. Hadrian claimed that Attianus had acted on his own initiative, and rewarded him with senatorial status and consular rank; then pensioned him off, no later than 120. Hadrian assured the senate that henceforth their ancient right to prosecute and judge their own would be respected. 

The reasons for these four executions remain obscure. Official recognition of Hadrian as legitimate heir may have come too late to dissuade other potential claimants. Hadrian's greatest rivals were Trajan's closest friends, the most experienced and senior members of the imperial council; any of them might have been a legitimate competitor for the imperial office ("capaces imperii"); and any of them might have supported Trajan's expansionist policies, which Hadrian intended to change. One of their number was Aulus Cornelius Palma who as a former conqueror of Arabia Nabatea would have retained a stake in the East. The "Historia Augusta" describes Palma and a third executed senator, Lucius Publilius Celsus (consul for the second time in 113), as Hadrian's personal enemies, who had spoken in public against him. The fourth was Gaius Avidius Nigrinus, an ex-consul, intellectual, friend of Pliny the Younger and (briefly) Governor of Dacia at the start of Hadrian's reign. He was probably Hadrian's chief rival for the throne; a senator of highest rank, breeding, and connections; according to the "Historia Augusta", Hadrian had considered making Nigrinus his heir apparent, before deciding to get rid of him.

Soon after, in 125, Hadrian appointed Marcius Turbo as his Praetorian Prefect. Turbo was his close friend, a leading figure of the equestrian order, a senior court judge and a procurator. As Hadrian also forbade equestrians to try cases against senators, the Senate retained full legal authority over its members; it also remained the highest court of appeal, and formal appeals to the emperor regarding its decisions were forbidden. If this was an attempt to repair the damage done by Attianus, with or without Hadrian's full knowledge, it was not enough; Hadrian's reputation and relationship with his Senate were iredeemably soured, for the rest of his reign. Some sources describe Hadrian's occasional recourse to a network of informers, the "frumentarii" to discreetly investigate persons of high social standing, including senators and his close friends.

Hadrian was to spend more than half his reign outside Italy. Whereas previous emperors had, for the most part, relied on the reports of their imperial representatives around the Empire, Hadrian wished to see things for himself. Previous emperors had often left Rome for long periods, but mostly to go to war, returning once the conflict was settled. Hadrian's near-incessant travels may represent a calculated break with traditions and attitudes in which the empire was a purely Roman hegemony. Hadrian sought to include provincials in a commonwealth of civilized peoples and a common Hellenic culture under Roman supervision. He supported the creation of provincial towns (municipia), semi-autonomous urban communities with their own customs and laws, rather than the imposition of new Roman colonies with Roman constitutions.

A cosmopolitan, ecumenical intent is evident in coin issues of Hadrian's later reign, showing the emperor "raising up" the personifications of various provinces. Aelius Aristides would later write that Hadrian "extended over his subjects a protecting hand, raising them as one helps fallen men on their feet". All this did not go well with Roman traditionalists. The self-indulgent emperor Nero had enjoyed a prolonged and peaceful tour of Greece, and had been criticised by the Roman elite for abandoning his fundamental responsibilities as emperor. In the eastern provinces, and to some extent in the west, Nero had enjoyed popular support; claims of his imminent return or rebirth emerged almost immediately after his death. Hadrian may have consciously exploited these positive, popular connections during his own travels. In the "Historia Augusta", Hadrian is described as "a little too much Greek", too cosmopolitan for a Roman emperor.

Prior to Hadrian's arrival in Britannia, the province had suffered a major rebellion, from 119 to 121. Inscriptions tell of an "expeditio Britannica" that involved major troop movements, including the dispatch of a detachment (vexillatio), comprising some 3,000 soldiers. Fronto writes about military losses in Britannia at the time. Coin legends of 119-120 attest that Pompeius Falco was sent to restore order. In 122 Hadrian initiated the construction of a wall, "to separate Romans from barbarians". This deterred attacks on Roman territory at a lower cost than a massed border army, and controlled cross-border trade and immigration. A shrine was erected in York to Brittania as the divine personification of Britain; coins were struck, bearing her image, identified as . By the end of 122, Hadrian had concluded his visit to Britannia. He never saw the finished wall that bears his name.

Hadrian appears to have continued through southern Gaul. At Nemausus, he may have overseen the building of a basilica dedicated to his patroness Plotina, who had recently died in Rome and had been deified at Hadrian's request. At around this time, Hadrian dismissed his secretary "ab epistulis", the historian Suetonius, for "excessive familiarity" towards the empress. Marcius Turbo's colleague as Praetorian Prefect, Gaius Septicius Clarus, was dismissed for the same alleged reason, perhaps a pretext to remove him from office. Hadrian spent the winter of 122/123 at Tarraco, in Spain, where he restored the Temple of Augustus.

In 123, Hadrian crossed the Mediterranean to Mauretania, where he personally led a minor campaign against local rebels. The visit was cut short by reports of war preparations by Parthia; Hadrian quickly headed eastwards. At some point, he visited Cyrene, where he personally funded the training of young men from well-bred families for the Roman military. Cyrene had benefited earlier (in 119) from his restoration of public buildings destroyed during the earlier Jewish revolt.

When Hadrian arrived on the Euphrates, he personally negotiated a settlement with the Parthian King Osroes I, inspected the Roman defences, then set off westwards, along the Black Sea coast. He probably wintered in Nicomedia, the main city of Bithynia. Nicomedia had been hit by an earthquake only shortly before his stay; Hadrian provided funds for its rebuilding, and was acclaimed as restorer of the province.

It is possible that Hadrian visited Claudiopolis and saw the beautiful Antinous, a young man of humble birth who became Hadrian's beloved. Literary and epigraphic sources say nothing on when or where they met; depictions of Antinous show him aged 20 or so, shortly before his death in 130. In 123 he would most likely have been a youth of 13 or 14. It is also possible that Antinous was sent to Rome to be trained as a page to serve the emperor and only gradually rose to the status of imperial favourite. The actual history of their relationship is mostly unknown.

With or without Antinous, Hadrian travelled through Anatolia. Various traditions suggest his presence at particular locations, and allege his foundation of a city within Mysia, Hadrianutherae, after a successful boar hunt. At about this time, plans to complete the Temple of Zeus in Cyzicus, begun by the kings of Pergamon, were put into practice. The temple received a colossal statue of Hadrian. Cyzicus, Pergamon, Smyrna, Ephesus and Sardes were promoted as regional centres for the Imperial cult ("neocoros")

Hadrian arrived in Greece during the autumn of 124, and participated in the Eleusinian Mysteries. He had a particular commitment to Athens, which had previously granted him citizenship and an archonate; at the Athenians' request, he revised their constitution – among other things, he added a new phyle (tribe), which was named after him. Hadrian combined active, hands-on interventions with cautious restraint. He refused to intervene in a local dispute between producers of olive oil and the Athenian Assembly and Council, who had imposed production quotas on oil producers; yet he granted an imperial subsidy for the Athenian grain supply. Hadrian created two foundations, to fund Athens' public games, festivals and competitions if no citizen proved wealthy or willing enough to sponsor them as a Gymnasiarch or Agonothetes. Generally Hadrian preferred that Greek notables, including priests of the Imperial cult, focus on more durable provisions, such as aqueducts and public fountains ("nymphaea"). Athens was given two such fountains; another was given to Argos.

During the winter he toured the Peloponnese. His exact route is uncertain, but it took in Epidaurus; Pausanias describes temples built there by Hadrian, and his statue – in heroic nudity – erected by its citizens in thanks to their "restorer". Antinous and Hadrian may have already been lovers at this time; Hadrian showed particular generosity to Mantinea, which shared ancient, mythic, politically useful links with Antinous' home at Bithynia. He restored Mantinea's Temple of Poseidon Hippios, and according to Pausanias, restored the city's original, classical name. It had been renamed Antigoneia since Hellenistic times, after the Macedonian King Antigonus III Doson. Hadrian also rebuilt the ancient shrines of Abae and Megara, and the Heraion of Argos. 
During his tour of the Peloponnese, Hadrian persuaded the Spartan grandee Eurycles Herculanus – leader of the Euryclid family that had ruled Sparta since Augustus' day – to enter the Senate, alongside the Athenian grandee Herodes Atticus the Elder. The two aristocrats would be the first from "Old Greece" to enter the Roman Senate, as representatives of the two "great powers" of the Classical Age. This was an important step in overcoming Greek notables' reluctance to take part in Roman political life. In March 125, Hadrian presided at the Athenian festival of Dionysia, wearing Athenian dress. The Temple of Olympian Zeus had been under construction for more than five centuries; Hadrian committed the vast resources at his command to ensure that the job would be finished. He also organised the planning and construction of a particularly challenging and ambitious aqueduct to bring water to the Athenian agora.

On his return to Italy, Hadrian made a detour to Sicily. Coins celebrate him as the restorer of the island. Back in Rome, he saw the rebuilt Pantheon, and his completed villa at nearby Tibur, among the Sabine Hills. In early March 127 Hadrian set off on a tour of Italy; his route has been reconstructed through the evidence of his gifts and donations. He restored the shrine of Cupra in Cupra Maritima, and improved the drainage of the Fucine lake. Less welcome than such largesse was his decision in 127 to divide Italy into four regions under imperial legates with consular rank, acting as governors. They were given jurisdiction over all of Italy, excluding Rome itself, therefore shifting Italian cases from the courts of Rome. Having Italy effectively reduced to the status of a group of mere provinces did not go down well with the Roman Senate, and the innovation did not long outlive Hadrian's reign.

Hadrian fell ill around this time; whatever the nature of his illness, it did not stop him from setting off in the spring of 128 to visit Africa. His arrival coincided with the good omen of rain, which ended a drought. Along with his usual role as benefactor and restorer, he found time to inspect the troops; his speech to them survives. Hadrian returned to Italy in the summer of 128 but his stay was brief, as he set off on another tour that would last three years.

In September 128, Hadrian attended the Eleusinian mysteries again. This time his visit to Greece seems to have concentrated on Athens and Sparta – the two ancient rivals for dominance of Greece. Hadrian had played with the idea of focusing his Greek revival around the Amphictyonic League based in Delphi, but by now he had decided on something far grander. His new Panhellenion was going to be a council that would bring Greek cities together. Having set in motion the preparations – deciding whose claim to be a Greek city was genuine would take time – Hadrian set off for Ephesus. From Greece, Hadrian proceeded by way of Asia to Egypt, probably conveyed across the Aegean with his entourage by an Ephesian merchant, Lucius Erastus. Hadrian later sent a letter to the Council of Ephesus, supporting Erastus as a worthy candidate for town councillor and offering to pay the requisite fee.

Hadrian opened his stay in Egypt by restoring Pompey the Great's tomb at Pelusium, offering sacrifice to him as a hero and composing an epigraph for the tomb. As Pompey was universally acknowledged as responsible for establishing Rome's power in the east, this restoration was probably linked to a need to reaffirm Roman Eastern hegemony, following social unrest there during Trajan's late reign. Hadrian and Antinous held a lion hunt in the Libyan desert; a poem on the subject by the Greek Pankrates is the earliest evidence that they travelled together.

In October 130, while Hadrian and his entourage were sailing on the Nile, Antinous drowned. The exact circumstances surrounding his death are unknown, and accident, suicide, murder and religious sacrifice have all been postulated. "Historia Augusta" offers the following account:

Hadrian's movements after the founding of Antinopolis on 30 October 130 are uncertain. Whether or not he returned to Rome, he travelled in the East during 130/131, to organise and inaugurate his new Panhellenion, which was to be focused on the Athenian Temple to Olympian Zeus. Successful applications for membership involved mythologised or fabricated claims to Greek origins, and affirmations of loyalty to Imperial Rome, to satisfy Hadrian's personal, idealised notions of Hellenism. Hadrian saw himself as protector of Greek culture and the "liberties" of Greece – in this case, urban self-government. It allowed Hadrian to appear as the fictive heir to Pericles, who supposedly had convened a previous Panhellenic Congress – such a Congress is mentioned only in Pericles' biography by Plutarch, who respected Rome's Imperial order.

Epigraphical evidence suggests that the prospect of applying to the Panhellenion held little attraction to the wealthier, Hellenised cities of Asia Minor, which were jealous of Athenian and European Greek preeminence within Hadrian's scheme. Hadrian's notion of Hellenism was narrow and deliberately archaising; he defined "Greekness" in terms of classical roots, rather than a broader, Hellenistic culture. The German sociologist Georg Simmel remarked that the Panhellenion was based on "games, commemorations, preservation of an ideal, an entirely non-political Hellenism".
Hadrian bestowed honorific titles on many regional centres. Palmyra received a state visit and was given the civic name Hadriana Palmyra. Hadrian also bestowed honours on various Palmyrene magnates, among them one Soados, who had done much to protect Palmyrene trade between the Roman Empire and Parthia.

Hadrian had spent the winter of 131–32 in Athens, where he dedicated the now-completed Temple of Olympian Zeus, At some time in 132, he headed East, to Judaea.

In Roman Judaea Hadrian visited Jerusalem, which was still in ruins after the First Roman–Jewish War of 66–73. He may have planned to rebuild Jerusalem as a Roman colony – as Vespasian had done with Caesarea Maritima – with various honorific and fiscal privileges. The non-Roman population would have no obligation to participate in Roman religious rituals, but were expected to support the Roman imperial order; this is attested in Caesarea, where some Jews served in the Roman army during both the 66 and 132 rebellions. It has been speculated that Hadrian intended to assimilate the Jewish Temple to the traditional Roman civic-religious Imperial cult; such assimilations had long been commonplace practise in Greece and in other provinces, and on the whole, had been successful. The neighbouring Samaritans had already integrated their religious rites with Hellenistic ones. Strict Jewish monotheism proved more resistant to Imperial cajoling, and then to Imperial demands. A massive anti-Hellenistic and anti-Roman Jewish uprising broke out, led by Simon bar Kokhba. The Roman governor Tineius (Tynius) Rufus asked for an army to crush the resistance; bar Kokhba punished any Jew who refused to join his ranks. According to Justin Martyr and Eusebius, that had to do mostly with Christian converts, who opposed bar Kokhba's messianic claims. 

A tradition based on the "Historia Augusta" suggests that the revolt was spurred by Hadrian's abolition of circumcision ("brit milah"); which as a Hellenist he viewed as mutilation. The scholar Peter Schäfer maintains that there is no evidence for this claim, given the notoriously problematical nature of the "Historia Augusta" as a source, the "tomfoolery" shown by the writer in the relevant passage, and the fact that contemporary Roman legislation on "genital mutilation" seems to address the general issue of castration of slaves by their masters. Other issues could have contributed to the outbreak; a heavy-handed, culturally insensitive Roman administration; tensions between the landless poor and incoming Roman colonists privileged with land-grants; and a strong undercurrent of messianism, predicated on Jeremiah's prophecy that the Temple would be rebuilt seventy years after its destruction, as the First Temple had been after the Babylonian exile. 

The Romans were overwhelmed by the organised ferocity of the uprising. Hadrian called his general Sextus Julius Severus from Britain, and brought troops in from as far as the Danube. Roman losses were heavy; an entire legion or its numeric equivalent of around 4,000. Hadrian's report on the war to the Roman Senate omitted the customary salutation, "If you and your children are in health, it is well; I and the legions are in health." The rebellion was quashed by 135. According to Cassius Dio, Roman war operations in Judea left some 580,000 Jews dead, and 50 fortified towns and 985 villages razed. An unknown proportion of the population was enslaved. Beitar, a fortified city southwest of Jerusalem, fell after a three and a half year siege. The extent of punitive measures against the Jewish population remains a matter of debate.

Hadrian erased the province's name from the Roman map, renaming it Syria Palaestina. He renamed Jerusalem Aelia Capitolina after himself and Jupiter Capitolinus, and had it rebuilt in Greek style. According to Epiphanius, Hadrian appointed Aquila from Sinope in Pontus as "overseer of the work of building the city", since he was related to him by marriage. Hadrian is said to have placed the city's main Forum at the junction of the main Cardo and Decumanus Maximus, now the location for the (smaller) Muristan. After the suppression of the Jewish revolt, Hadrian provided the Samaritans with a temple, dedicated to Zeus Hypsistos ("Highest Zeus") on Mount Gerizim. The bloody repression of the revolt ended Jewish political independence from the Roman Imperial order.

Inscriptions make it clear that in 133 Hadrian took to the field with his armies against the rebels. He then returned to Rome, probably in that year and almost certainly – judging from inscriptions – via Illyricum.

Hadrian spent the final years of his life at Rome. In 134, he took an Imperial salutation for the end of the Second Jewish War (which was not actually concluded until the following year). Commemorations and achievement awards were kept to a minimum, as Hadrian came to see the war "as a cruel and sudden disappointment to his aspirations" towards a cosmopolitan empire. 

The Empress Sabina died, probably in 136, after an unhappy marriage with which Hadrian had coped as a political necessity. The "Historia Augusta" biography states that Hadrian himself declared that his wife's "ill-temper and irritability" would be reason enough for a divorce, were he a private citizen. That gave credence, after Sabina's death, to the common belief that Hadrian had her poisoned. In keeping with well-established Imperial propriety, Sabina – who had been made an "Augusta" sometime around 128 – was deified not long after her death. 
Hadrian's marriage to Sabina had been childless. Suffering from poor health, Hadrian turned to the problem of the succession. In 136 he adopted one of the ordinary consuls of that year, Lucius Ceionius Commodus, who as an emperor-in waiting took the name Lucius Aelius Caesar. He was the son-in-law of Gaius Avidius Nigrinus, one of the "four consulars" executed in 118, but was himself in delicate health, apparently with a reputation more "of a voluptuous, well educated great lord than that of a leader". Various modern attempts have been made to explain Hadrian's choice: Jerome Carcopino proposes that Aelius was Hadrian's natural son. It has also been speculated that his adoption was Hadrian's belated attempt to reconcile with one of the most important of the four senatorial families whose leading members had been executed soon after Hadrian's succession. Aelius acquitted himself honourably as joint governor of Pannonia Superior and Pannonia Inferior; he held a further consulship in 137, but died on 1 January 138.

Hadrian next adopted Titus Aurelius Fulvus Boionius Arrius Antoninus (the future emperor Antoninus Pius), who had served Hadrian as one of the five imperial legates of Italy, and as proconsul of Asia. In the interests of dynastic stability, Hadrian required that Antoninus adopt both Lucius Ceionius Commodus (son of the deceased Aelius Caesar) and Marcus Annius Verus (grandson of an influential senator of the same name who had been Hadrian's close friend); Annius was already betrothed to Aelius Caesar's daughter Ceionia Fabia. It may not have been Hadrian, but rather Antoninus Pius – Annius Verus's uncle – who supported Annius Verus' advancement; the latter's divorce of Ceionia Fabia and subsequent marriage to Antoninus' daughter Annia Faustina points in the same direction. When he eventually became Emperor, Marcus Aurelius would co-opt Ceionius Commodus as his co-Emperor, under the name of Lucius Verus, on his own initiative. 

Hadrian's last few years were marked by conflict and unhappiness. His adoption of Aelius Caesar proved unpopular, not least with Hadrian's brother-in-law Lucius Julius Ursus Servianus and Servianus's grandson Gnaeus Pedanius Fuscus Salinator. Servianus, though now far too old, had stood in the line of succession at the beginning of Hadrian's reign; Fuscus is said to have had designs on the imperial power for himself. In 137 he may have attempted a coup in which his grandfather was implicated; Hadrian ordered that both be put to death. Servianus is reported to have prayed before his execution that Hadrian would "long for death but be unable to die". During his final, protracted illness, Hadrian was prevented from suicide on several occasions.

Hadrian died in the year 138 on the 10th of July, in his villa at Baiae at the age of 62. The cause of death is believed to have been heart failure. Dio Cassius and the "Historia Augusta" record details of his failing health. He had reigned for 21 years, the longest since Tiberius, and the fourth longest in the Principate, after Augustus, Hadrian's successor Antoninus Pius, and Tiberius.

He was buried first at Puteoli, near Baiae, on an estate that had once belonged to Cicero. Soon after, his remains were transferred to Rome and buried in the Gardens of Domitia, close by the almost-complete mausoleum. Upon completion of the Tomb of Hadrian in Rome in 139 by his successor Antoninus Pius, his body was cremated, and his ashes were placed there together with those of his wife Vibia Sabina and his first adopted son, Lucius Aelius, who also died in 138. The Senate had been reluctant to grant Hadrian divine honours; but Antoninus persuaded them by threatening to refuse the position of Emperor. Hadrian was given a temple on the Campus Martius, ornamented with reliefs representing the provinces. The Senate awarded Antoninus the title of "Pius", in recognition of his filial piety in pressing for the deification of his adoptive father. At the same time, perhaps in reflection of the senate's ill will towards Hadrian, commemorative coinage honouring his consecration was kept to a minimum.

Most of Hadrian's military activities were consistent with his ideology of Empire as a community of mutual interest and support. He focused on protection from external and internal threats; on "raising up" existing provinces, rather than the aggressive acquisition of wealth and territory through subjugation of "foreign" peoples that had characterised the early Empire. While the empire as a whole benefited from this, military careerists resented the loss of opportunities. 

The 4th-century historian Aurelius Victor saw Hadrian's withdrawal from Trajan's territorial gains in Mesopotamia as a jealous belittlement of Trajan's achievements ("Traiani gloriae invidens"). More likely, an expansionist policy was no longer sustainable; the Empire had lost two legions, the Legio XXII Deiotariana and the "lost legion" IX Hispania, possibly destroyed in a late Trajanic uprising by the Brigantes in Britain. Trajan himself may have thought his gains in Mesopotamia indefensible, and abandoned them shortly before his death. Hadrian granted parts of Dacia to the Roxolani Sarmatians; their king Rasparaganus received Roman citizenship, client king status, and possibly an increased subsidy. Hadrian's presence on the Dacian front at this time is mere conjecture, but Dacia was included in his coin series with allegories of the provinces. A controlled, partial withdrawal of troops from the Dacian plains would have been less costly than maintaining several Roman cavalry units and a supporting network of fortifications.

Hadrian retained control over Osroene through the client king Parthamaspates, who had once served as Trajan's client king of Parthia; and around 121, Hadrian negotiated a peace treaty with the now-independent Parthia. Late in his reign (135), the Alani attacked Roman Cappadocia with the covert support of Pharasmanes, king of Caucasian Iberia. The attack was repulsed by Hadrian's governor, the historian Arrian, who subsequently installed a Roman "adviser" in Iberia. Arrian kept Hadrian well-informed on matters related to the Black Sea and the Caucasus. Between 131 and 132 he sent Hadrian a lengthy letter ("Periplus of the Euxine") on a maritime trip around the Black Sea, intended to offer relevant information in case a Roman intervention was needed.

Hadrian also developed permanent fortifications and military posts along the empire's borders ("limites", sl. "limes") to support his policy of stability, peace and preparedness. This helped keep the military usefully occupied in times of peace; his Wall across Britania was built by ordinary troops. A series of mostly wooden fortifications, forts, outposts and watchtowers strengthened the Danube and Rhine borders. Troops practised intensive, regular drill routines. Although his coins showed military images almost as often as peaceful ones, Hadrian's policy was peace through strength, even threat, with an emphasis on "disciplina" (discipline), which was the subject of two monetary series. Cassius Dio praised Hadrian's emphasis on "spit and polish" as cause for the generally peaceful character of his reign. Fronto expressed other opinions on the subject. In his view, Hadrian preferred war games to actual war, and enjoyed "giving eloquent speeches to the armies" – like the inscribed series of addresses he made while on an inspection tour, during 128, at the new headquarters of Legio III Augusta in Lambaesis

Faced with a shortage of legionary recruits from Italy and other Romanised provinces, Hadrian systematised the use of less costly "numeri" – ethnic non-citizen troops with special weapons, such as Eastern mounted archers – in low-intensity, mobile defensive tasks such as dealing with border infiltrators and skirmishers. Hadrian is also credited with introducing units of heavy cavalry (cataphracts) into the Roman army. Fronto later blamed Hadrian for declining standards in the Roman army of his own time.

Hadrian enacted, through the jurist Salvius Julianus, the first attempt to codify Roman law. This was the Perpetual Edict, according to which the legal actions of praetors became fixed statutes, and as such could no longer be subjected to personal interpretation or change by any magistrate other than the Emperor. At the same time, following a procedure initiated by Domitian, Hadrian made the Emperor's legal advisory board, the "consilia principis" ("council of the princeps") into a permanent body, staffed by salaried legal aides. Its members were mostly drawn from the equestrian class, replacing the earlier freedmen of the Imperial household. This innovation marked the superseding of surviving Republican institutions by an openly autocratic political system. The reformed bureaucracy was supposed to exercise administrative functions independently of traditional magistracies; objectively it did not detract from the Senate's position. The new civil servants were free men and as such supposed to act on behalf of the interests of the "Crown", not of the Emperor as an individual. However, the Senate never accepted the loss of its prestige caused by the emergence of a new aristocracy alongside it, placing more strain on the already troubled relationship between the Senate and the Emperor.

Hadrian codified the customary legal privileges of the wealthiest, most influential or highest status citizens (described as "splendidiores personae" or "honestiores"), who held a traditional right to pay fines when found guilty of relatively minor, non-treasonous offences. Low ranking persons - "alii" ("the others"), including low-ranking citizens - were "humiliores" who for the same offences could be subject to extreme physical punishments, including forced labour in the mines or in public works, as a form of fixed-term servitude. While Republican citizenship had carried at least notional equality under law, and the right to justice, offences in Imperial courts were judged and punished according to the relative prestige, rank, reputation and moral worth of both parties; senatorial courts were apt to be lenient when trying one of their peers, and to deal very harshly with offences committed against one of their number by low ranking citizens or non-citizens. For treason (maiestas) beheading was the worst punishment that the law could inflict on "honestiores"; the "humiliores" might suffer crucifixion, burning, or condemnation to the beasts in the arena. 

A great number of Roman citizens maintained a precarious social and economic advantage at the lower end of the hierarchy. Hadrian found it necessary to clarify that decurions, the usually middle-class, elected local officials responsible for running the ordinary, everyday official business of the provinces, counted as "honestiores"; so did soldiers, veterans and their families, as far as civil law was concerned; by implication, all others, including freedmen and slaves, counted as "humiliores". Like most Romans, Hadrian seems to have accepted slavery as morally correct, an expression of the same natural order that rewarded "the best men" with wealth, power and respect. When confronted by a crowd demanding the freeing of a popular slave charioteer, Hadrian replied that he could not free a slave belonging to another person. However, he limited the punishments that slaves could suffer; they could be lawfully tortured to provide evidence, but they could not be lawfully killed unless guilty of a capital offence. Masters were also forbidden to sell slaves to a gladiator trainer (lanista) or to a procurer, except as legally justified punishment. Hadrian also forbade torture of free defendants and witnesses. He abolished ergastula, private prisons for slaves in which kidnapped free men had sometimes been illegally detained.

Hadrian issued a general rescript, imposing a ban on castration, performed on freedman or slave, voluntarily or not, on pain of death for both the performer and the patient. Under the "Lex Cornelia de Sicaris et Veneficis", castration was placed on a par with conspiracy to murder, and punished accordingly. Notwithstanding his philhellenism, Hadrian was also a traditionalist. He enforced dress-standards among the "honestiores"; senators and knights were expected to wear the toga when in public. He imposed strict separation between the sexes in theaters and public baths; to discourage idleness, the latter were not allowed to open until 2.00 in the afternoon, "except for medical reasons".

One of Hadrian's immediate duties on accession was to seek senatorial consent for the apotheosis of his predecessor, Trajan, and any members of Trajan's family to whom he owed a debt of gratitude. Matidia Augusta, Hadrian's mother-in-law, died in December 119, and was duly deified. Hadrian may have stopped at Nemausus during his return from Britannia, to oversee the completion or foundation of a basilica dedicated to his patroness Plotina. She had recently died in Rome and had been deified at Hadrian's request. 

As Emperor, Hadrian was also Rome's pontifex maximus, responsible for all religious affairs and the proper functioning of official religious institutions throughout the empire. His Hispano-Roman origins and marked pro-Hellenism shifted the focus of the official imperial cult, from Rome to the Provinces. While his standard coin issues still identified him with the traditional "genius populi Romani", other issues stressed his personal identification with "Hercules Gaditanus" (Hercules of Gades), and Rome's imperial protection of Greek civilisation. He promoted Sagalassos in Greek Pisidia as the Empire's leading Imperial cult centre; his exclusively Greek "Panhellenion" extolled Athens as the spiritual centre of Greek culture. 

Hadrian added several Imperial cult centres to the existing roster, particularly in Greece, where traditional intercity rivalries were commonplace. Cities promoted as Imperial cult centres drew Imperial sponsorship of festivals and sacred games, attracted tourism, trade and private investment. Local worthies and sponsors were encouraged to seek self-publicity as cult officials under the aegis of Roman rule, and to foster reverence for Imperial authority. Hadrian's rebuilding of long-established religious centres would have further underlined his respect for the glories of classical Greece – something well in line with contemporary antiquarian tastes. During Hadrian's third and last trip to the Greek East, there seems to have been an upwelling of religious fervour, focused on Hadrian himself. He was given personal cult as a deity, monuments and civic homage, according to the religious syncretism at the time. He may have had the great Serapeum of Alexandria rebuilt, following damage sustained in 116, during the Kitos War.

In 136, just two years before his death, Hadrian dedicated his Temple of Venus and Roma. It was built on land he had set aside for the purpose in 121, formerly the site of Nero's Golden House. The temple was the largest in Rome, and was built in an Hellenising style, more Greek than Roman. The temple's dedication and statuary associated the worship of the traditional Roman goddess Venus, divine ancestress and protector of the Roman people, with the worship of the goddess Roma – herself a Greek invention, hitherto worshiped only in the provinces – to emphasise the universal nature of the empire.
Hadrian had Antinous deified as Osiris-Antinous by an Egyptian priest at the ancient Temple of Ramesses II, very near the place of his death. Hadrian dedicated a new temple-city complex there, built in a Graeco-Roman style, and named it Antinopolis. It was a proper Greek polis; it was granted an Imperially subsidised alimentary scheme similar to Trajan's alimenta, and its citizens were allowed intermarriage with members of the native population, without loss of citizen-status. Hadrian thus identified an existing native cult (to Osiris) with Roman rule. The cult of Antinous was to become very popular in the Greek-speaking world, and also found support in the West. In Hadrian's villa, statues of the Tyrannicides, with a bearded Aristogeiton and a clean-shaven Harmodios, linked his favourite to the classical tradition of Greek love. In the west, Antinous was identified with the Celtic sun-god Belenos.

Hadrian was criticized for the open intensity of his grief at Antinous's death, particularly as he had delayed the apotheosis of his own sister Paulina after her death. Nevertheless, his recreation of the deceased youth as a cult-figure found little opposition. Though not a subject of the state-sponsored, official Roman imperial cult, Antinous offered a common focus for the emperor and his subjects, emphasizing their sense of community. Medals were struck with his effigy, and statues erected to him in all parts of the empire, in all kinds of garb, including Egyptian dress. Temples were built for his worship in Bithynia and Mantineia in Arcadia. In Athens, festivals were celebrated in his honour and oracles delivered in his name. As an "international" cult figure, Antinous had an enduring fame, far outlasting Hadrian's reign. Local coins with his effigy were still being struck during Caracalla's reign, and he was invoked in a poem to celebrate the accession of Diocletian.

Hadrian continued Trajan's policy on Christians; they should not be sought out, and should only be prosecuted for specific offences, such as refusal to swear oaths. In a rescript addressed to the proconsul of Asia, Minutius Fundanus, and preserved by Justin Martyr, Hadrian laid down that accusers of Christians had to bear the burden of proof for their denunciations or be punished for "calumnia" (defamation).

Hadrian had an abiding and enthusiastic interest in art, architecture and public works. Rome's Pantheon (temple "to all the gods"), originally built by Agrippa and destroyed by fire in 80, was partly restored under Trajan and completed under Hadrian in the domed form it retains to this day. Hadrian's Villa at Tibur (Tivoli) provides the greatest Roman equivalent of an Alexandrian garden, complete with domed Serapeum, recreating a sacred landscape. An anecdote from Cassius Dio's history suggests Hadrian had a high opinion of his own architectural tastes and talents, and took their rejection as a personal offense: at some time before his reign, his predecessor Trajan was discussing an architectural problem with Apollodorus of Damascus - architect and designer of Trajan's Forum, the Column commemorating his Dacian conquest, and his bridge across the Danube - when Hadrian interrupted to offer his advice. Apollodorus gave him a scathing response: "Be off, and draw your gourds [a sarcastic reference to the domes which Hadrian apparently liked to draw]. You don't understand any of these matters". Dio claims that once Hadrian became emperor, he showed Apollodorus drawings of the gigantic Temple of Venus and Roma, implying that great buildings could be created without his help. When Apollodorus pointed out the building's various insoluble problems and faults, Hadrian was enraged, sent him into exile and later put him to death on trumped up charges.

Hadrian wrote poetry in both Latin and Greek; one of the few surviving examples is a Latin poem he reportedly composed on his deathbed (see below). Some of his Greek productions found their way into the Palatine Anthology. He also wrote an autobiography, which "Historia Augusta" says was published under the name of Hadrian's freedman Phlegon of Tralles. It was not, apparently, a work of great length or revelation, but designed to scotch various rumours or explain Hadrian's most controversial actions. It is possible that this autobiography had the form of a series of open letters to Antoninus Pius.

Hadrian was a passionate hunter from a young age. In northwest Asia, he founded and dedicated a city to commemorate a she-bear he killed. It is documented that in Egypt he and his beloved Antinous killed a lion. In Rome, eight reliefs featuring Hadrian in different stages of hunting decorate a building that began as a monument celebrating a kill.

Hadrian's philhellenism may have been one reason for his adoption, like Nero before him, of the beard as suited to Roman imperial dignity; Dio of Prusa had equated the growth of the beard with the Hellenic ethos. Hadrian's beard may also have served to conceal his natural facial blemishes. Most emperors before him had been clean-shaven; most who came after him were bearded, at least until Constantine the Great.

Hadrian was familiar with the Stoic philosophers Epictetus, and Favorinus, and with their works. During his first stay in Greece, before he became emperor, he attended lectures by Epictetus at Nicopolis. Shortly before the death of Plotina, Hadrian had granted her wish that the leadership of the Epicurean School in Athens be open to a non-Roman candidate.

During Hadrian's time as Tribune of the Plebs, omens and portents supposedly announced his future imperial condition. According to the "Historia Augusta", Hadrian had a great interest in astrology and divination and had been told of his future accession to the Empire by a grand-uncle who was himself a skilled astrologer.

According to the "Historia Augusta", Hadrian composed the following poem shortly before his death:

The poem has enjoyed remarkable popularity, but uneven critical acclaim. According to Aelius Spartianus, the alleged author of Hadrian's biography in the "Historia Augusta", Hadrian "wrote also similar poems in Greek, not much better than this one". T. S. Eliot's poem "Animula" may have been inspired by Hadrian's, though the relationship is not unambiguous.

Hadrian has been described as the most versatile of all Roman emperors, who "adroitly concealed a mind envious, melancholy, hedonistic, and excessive with respect to his own ostentation; he simulated restraint, affability, clemency, and conversely disguised the ardor for fame with which he burned." His successor Marcus Aurelius, in his "Meditations", lists those to whom he owes a debt of gratitude; Hadrian is conspicuously absent. Hadrian's tense, authoritarian relationship with his senate was acknowledged a generation after his death by Fronto, himself a senator, who wrote in one of his letters to Marcus Aurelius that "I praised the deified Hadrian, your grandfather, in the senate on a number of occasions with great enthusiasm, and I did this willingly, too [...] But, if it can be said – respectfully acknowledging your devotion towards your grandfather – I wanted to appease and assuage Hadrian as I would Mars Gradivus or Dis Pater, rather than to love him." Fronto adds, in another letter, that he kept some friendships, during Hadrian's reign, "under the risk of my life" ("cum periculo capitis"). Hadrian underscored the autocratic character of his reign by counting his "dies imperii" from the day of his acclamation by the armies, rather than the senate, and legislating by frequent use of imperial decrees to bypass the Senate's approval. The veiled antagonism between Hadrian and the Senate never grew to overt confrontation as had happened during the reigns of overtly "bad" emperors, because Hadrian knew how to remain aloof and avoid an open clash. That Hadrian spent half of his reign away from Rome in constant travel probably helped to mitigate the worst of this permanently strained relationship.
In 1503, Niccolò Machiavelli, though an avowed republican, esteemed Hadrian as an ideal "princeps", one of Rome's Five Good Emperors. Friedrich Schiller called Hadrian "the Empire's first servant". Edward Gibbon admired his "vast and active genius" and his "equity and moderation", and considered Hadrian's era as part of the "happiest era of human history". In Ronald Syme's view, Hadrian "was a Führer, a Duce, a Caudillo". According to Syme, Tacitus' description of the rise and accession of Tiberius is a disguised account of Hadrian's authoritarian Principate. According, again, to Syme, Tacitus' Annals would be a work of contemporary history, written "during Hadrian's reign and hating it".

While the balance of ancient literary opinion almost invariably compares Hadrian unfavourably to his predecessor, modern historians have sought to examine his motives, purposes and the consequences of his actions and policies. For M.A. Levi, a summing-up of Hadrian's policies should stress the ecumenical character of the Empire, his development of an alternate bureaucracy disconnected from the Senate and adapted to the needs of an "enlightened" autocracy, and his overall defensive strategy; this would qualify him as a grand Roman political reformer, creator of an openly absolute monarchy to replace a sham senatorial republic. Robin Lane Fox credits Hadrian as creator of a unified Greco-Roman cultural tradition, and as the end of this same tradition; Hadrian's attempted "restoration" of Classical culture within a non-democratic Empire drained it of substantive meaning, or, in Fox's words, "kill[ed] it with kindness".

In Hadrian's time, there was already a well established convention that one could not write a contemporary Roman imperial history for fear of contradicting what the emperors wanted to say, read or hear about themselves. Fronto's correspondence and works attest to Hadrian's character and the internal politics of his rule. Greek authors such as Philostratus and Pausanias wrote shortly after Hadrian's reign, but confined their scope to the general historical framework that shaped Hadrian's decisions, especially those relating to Greece. Political histories of Hadrian's reign come mostly from later sources, some of them written centuries after the reign itself. The early 3rd-century "Roman History" by Cassius Dio gave a general account of Hadrian's reign, but the original is lost, and what survives is a brief, Byzantine-era abridgment by the 11th-century monk Xiphilinius, who focused on Hadrian's religious interests, the Bar Kokhba war, and little else. The principal source for Hadrian's life and reign is one of several late 4th-century imperial biographies, collectively known as the "Historia Augusta". The collection as a whole is notorious for its unreliability ("a mish mash of actual fact, cloak and dagger, sword and sandal, with a sprinkling of "Ubu Roi""), but most modern historians consider its account of Hadrian to be relatively free of outright fictions, and probably based on sound historical sources, principally one of a lost series of imperial biographies by the prominent 3rd-century senator Marius Maximus, who covered the reigns of Nerva through to Elagabalus.

The first modern historian to produce a chronological account of Hadrian's life, supplementing the written sources with other epigraphical, numismatic, and archaeological evidence, was the German 19th-century medievalist Ferdinand Gregorovius. A 1907 biography by Weber, a German nationalist and later Nazi Party supporter, incorporates the same archaeological evidence to produce an account of Hadrian, and especially his Bar Kokhba war, that has been described as ideologically loaded. Epigraphical studies in the post-war period help support alternate views of Hadrian. Anthony Birley's 1997 biography of Hadrian sums up and reflects these developments in Hadrian historiography.


Inscriptions:





</doc>
<doc id="13623" url="https://en.wikipedia.org/wiki?curid=13623" title="Herman Melville">
Herman Melville

Herman Melville (August 1, 1819 – September 28, 1891) was an American novelist, short story writer, and poet of the American Renaissance period. His best known works include "Typee" (1846), a romantic account of his experiences in Polynesian life, and his whaling novel "Moby-Dick" (1851). His work was almost forgotten during his last 30 years.

His writing draws on his experience at sea as a common sailor, exploration of literature and philosophy, and engagement in the contradictions of American society in a period of rapid change. He developed a complex, baroque style; the vocabulary is rich and original, a strong sense of rhythm infuses the elaborate sentences, the imagery is often mystical or ironic, and the abundance of allusion extends to biblical scripture, myth, philosophy, literature, and the visual arts.

Melville was born in New York City, the third child of a merchant in French dry goods and his wife. His formal education ended abruptly after his father died in 1832, as this left the family in financial straits. He briefly became a schoolteacher before he took to sea in 1839 as a sailor on a merchant ship. In 1840, he signed aboard the whaler "Acushnet" for his first whaling voyage but jumped ship in the Marquesas Islands. He returned to Boston in 1844 after further adventures.

His first book was "Typee" (1846), a highly romanticized account of his life among Polynesians. It became such a best-seller that he wrote the sequel "Omoo" (1847). These successes gave him the financial basis to marry Elizabeth Shaw, daughter of a prominent Boston family, but the success proved hard to sustain. His first novel that was not based on his own experiences was "Mardi" (1849), a sea narrative that develops into a philosophical allegory—but it was not well received. He received warmer reviews for "Redburn" (1849), a story of life on a merchant ship, and his 1850 description of the harsh life aboard a man-of-war in "White-Jacket", but they did not provide financial security.

In August 1850, Melville moved his growing family to Arrowhead, a farm in Pittsfield, Massachusetts, where he established a profound but short-lived friendship with Nathaniel Hawthorne, to whom he dedicated "Moby-Dick". This novel was another commercial failure, published to mixed reviews. Melville's career as a popular author effectively ended with the cool reception of "" (1852), in part a satirical portrait of the literary culture at the time. His Revolutionary War novel "Israel Potter" appeared in 1855.

From 1853 to 1856, Melville published short fiction in magazines, most notably "Bartleby, the Scrivener" (1853), "The Encantadas" (1854), and "Benito Cereno" (1855). These and three other stories were collected in 1856 as "The Piazza Tales". In 1857, he traveled to England where he reunited with Hawthorne for the first time since 1852, and then toured the Near East. "The Confidence-Man" (1857) was the last prose work that he published. He moved to New York to take a position as Customs Inspector and turned to poetry. "Battle-Pieces and Aspects of the War" (1866) was his poetic reflection on the moral questions of the American Civil War.

In 1867, his oldest child Malcolm died at home from a self-inflicted gunshot. "Clarel: A Poem and Pilgrimage in the Holy Land" was published in 1876, a metaphysical epic. In 1886, his son Stanwix died, and Melville retired. During his last years, he privately published two volumes of poetry, left one volume unpublished, and returned to prose of the sea. The novella "Billy Budd" was left unfinished at his death but was published in 1924.

Melville's death from cardiovascular disease in 1891 subdued a reviving interest in his work. The 1919 centennial of his birth became the starting point of the "Melville Revival". Critics discovered his work, scholars explored his life; his major novels and stories have come to be considered world classics, and his poetry has gradually gained respect.

Born Herman Melvill in New York City on August 1, 1819, to Allan Melvill (1782–1832) and Maria (Gansevoort) Melvill (1791–1872), Herman was the third of eight children. His siblings, who played important roles in his career as well as in his emotional life, were Gansevoort (1815–1846); Helen Maria (1817–1888); Augusta (1821–1876); Allan (1823–1872); Catherine (1825–1905); Frances Priscilla (1827–1885); and Thomas (1830–1884), who eventually became a governor of Sailors Snug Harbor. Part of a well-established and colorful Boston family, Melville's father spent much time out of New York and in Europe as a commission merchant and an importer of French dry goods.

Both of Melville's grandfathers were heroes of the Revolutionary War. Major Thomas Melvill (1751–1832) had taken part in the Boston Tea Party, and his maternal grandfather, General Peter Gansevoort (1749–1812), was famous for having commanded the defense of Fort Stanwix in New York in 1777. Melville found satisfaction in his "double revolutionary descent." Major Melvill sent his son Allan (Herman's father) not to college but to France at the turn of the nineteenth century, where he spent two years in Paris and learned to speak and write French fluently. He subscribed to his father's Unitarianism. In 1814, Allan married Maria Gansevoort, who was committed to the Dutch Reformed version of the Calvinist creed of her family. The severe Protestantism of the Gansevoort's tradition ensured that she knew her Bible well, in English as well as in Dutch, the language she had grown up speaking with her parents.

Almost three weeks after his birth, on August 19, Herman Melville was baptized at home by a minister of the South Reformed Dutch Church. During the 1820s, Melville lived a privileged, opulent life, in a household with three or more servants at a time. At four-year intervals, the family would move to more spacious and elegant quarters, finally settling on Broadway in 1828. Allan Melvill lived beyond his means and on large sums he borrowed from both his father and his wife's widowed mother. His wife's opinion of his financial conduct is unknown. Biographer Hershel Parker suggests Maria "thought her mother's money was infinite and that she was entitled to much of her portion now, while she had small children." How well, biographer Delbanco adds, the parents managed to hide the truth from their children is "impossible to know."

In 1830, Maria's family finally lost patience and their support came to a halt, at which point Allan's total debt to both families exceeded $20,000 (roughly equivalent to $518,000 in 2018 dollars). The felicity of Melville's early childhood, biographer Newton Arvin writes, depended not so much on wealth as on the "exceptionally tender and affectionate spirit in all the family relationships, especially in the immediate circle." Arvin describes Allan as "a man of real sensibility and a particularly warm and loving father," while Maria was "warmly maternal, simple, robust, and affectionately devoted to her husband and her brood."

Melville's education began when he was five, around the time the Melvills moved to a newly built house at 33 Bleecker Street. In 1826, the same year that Melville contracted scarlet fever, Allan Melvill, who sent both Gansevoort and Herman to the New York Male High School, described Melville in a letter to Peter Gansevoort Jr. as "very backwards in speech & somewhat slow in comprehension." His older brother Gansevoort appeared to be the brightest of the children, but soon Melville's development increased its pace. "You will be as much surprised as myself to know," Allan wrote Peter Gansevoort Jr., "that Herman proved the best Speaker in the introductory Department, at the examination of the High School, he has made rapid progress during the 2 last quarters." In 1829, both Gansevoort and Herman were transferred to Columbia Grammar & Preparatory School, and Herman enrolled in the English Department on September 28. "Herman I think is making more progress than formerly," Allan wrote in May 1830 to Major Melvill, "& without being a bright Scholar, he maintains a respectable standing, & would proceed further, if he could only be induced to study more—being a most amiable & innocent child, I cannot find it in my heart to coerce him."

Emotionally unstable and behind on paying the rent for the house on Broadway, the father Allan tried to recover from his setbacks by moving his family to Albany in 1830 and going into the fur business. Herman attended the Albany Academy from October 1830 to October 1831, where he took the standard preparatory course, studying reading and spelling; penmanship; arithmetic; English grammar; geography; natural history; universal, Greek, Roman and English history; classical biography; and Jewish antiquities. It is unknown why he left the Academy in October 1831; Parker suggests that by then "even the tiny tuition fee seemed too much to pay." His brothers Gansevoort and Allan continued their attendance a few months longer, Gansevoort until March the next year. "The ubiquitous classical references in Melville's published writings," as Melville scholar Merton Sealts observed, "suggest that his study of ancient history, biography, and literature during his school days left a lasting impression on both his thought and his art, as did his almost encyclopedic knowledge of both the Old and the New Testaments."

In December, Melville's father returned from New York City by steamboat, but ice forced him to travel the last seventy miles for two days and two nights in an open horse carriage at two degrees below zero (Fahrenheit), with the result that he developed a cold. In early January, he began to show "signs of delirium," and his situation grew worse until he—in the words of his wife—"by reason of severe suffering was deprive'd of his Intellect." Two months before reaching fifty, Allan Melvill died on January 28, 1832. As Melville was no longer attending school, he must have witnessed these scenes: twenty years later he described such a death in the death of Pierre's father in "Pierre".

The death of Allan caused many major shifts in the family's material (and spiritual) circumstances. One result was the greater influence of his mother's religious beliefs. Maria sought consolation in her faith and in April was admitted as a member of the First Reformed Dutch Church. Herman's saturation in orthodox Calvinism is for Arvin "surely the most decisive intellectual and spiritual influence of his early life."

Two months after his father's death, Gansevoort entered the cap and fur business. Uncle Peter Gansevoort, a director of the New York State Bank, got Herman a job as clerk for $150 a year. Biographers cite a passage from "Redburn" when trying to answer what Herman must have felt then: "I had learned to think much and bitterly before my time," the narrator remarks, adding, "I must not think of those delightful days, before my father became a bankrupt ... and we removed from the city; for when I think of those days, something rises up in my throat and almost strangles me." Arvin argues Melville was forced to reckon with the "tormented psychology, of the decayed patrician."

When Melville's paternal grandfather died on September 16, 1832, Maria and her children discovered he had borrowed more than his share of his inheritance, meaning Maria received only $20. His paternal grandmother died almost exactly seven months later.

Melville did his job well at the bank; though he was only fourteen in 1834, the bank considered him competent enough to be sent to Schenectady on an errand. Not much else is known from this period, except that he was very fond of drawing. The visual arts became a lifelong interest.

Around May 1834, the Melvilles moved to another house in Albany, a three-story brick house. That same month a fire destroyed Gansevoort's skin-preparing factory, which left him with personnel he could neither employ nor afford. Instead he pulled Melville out of the bank to man the cap and fur store. (Biographer Andrew Delbanco says that Gansevoort was doing so well he could hire his younger brother until a fire broke out in 1835, destroying both factory and the store.)

In 1835, while still working in the store, Melville enrolled in Albany Classical School, perhaps using Maria's part of the proceeds from the sale of the estate of his maternal grandmother in March 1835. In September of the following year Herman was back in Albany Academy in the Latin course. He also participated in debating societies, in an apparent effort to make up as much as he could for his missed years of schooling. In this period he read Shakespeare—at least "Macbeth", whose witch scenes gave him the chance to teasingly scare his sisters. In March 1837, he was again withdrawn from Albany Academy.

Gansevoort served as a role model and support for Melville in many ways throughout his life, at this time particularly in forming a self-directed educational plan. In early 1834 Gansevoort had become a member of Albany's Young Men's Association for Mutual Improvement, and in January 1835 Melville joined. Gansevoort also had copies of John Todd's "Index Rerum", a blank register for indexing remarkable passages from books one had read for easy retrieval. Among the sample entries was "Pequot, beautiful description of the war with," with a short title reference to the place in Benjamin Trumbull's "A Complete History of Connecticut" (1797 or 1818) where the description could be found. The two surviving volumes of Gansevoort's are the best evidence for Melville's reading in this period. Gansevoort's entries include books Melville used for "Moby-Dick" and "Clarel", such as "Parsees—of India—an excellent description of their character, & religion & an account of their descent—East India Sketch Book p. 21." Other entries are on Panther, the pirate's cabin, and storm at sea from James Fenimore Cooper's "The Red Rover", Saint-Saba.

The Panic of 1837 forced Gansevoort to file for bankruptcy in April. Uncle Thomas Jr. had not paid taxes on the farm, so he left Pittsfield secretly; he settled in Galena, Illinois. In June Maria told the younger children they must leave Albany for somewhere cheaper. Gansevoort began studying law in New York City while Herman managed the farm. However, that summer Herman decided to become a schoolteacher. He succeeded in getting a position at Sikes District School near Lenox, Massachusetts, where he taught some 30 students of many ages, including his own.

His term over, he returned to his mother in 1838. In February he was elected president of the Philo Logos Society, which Peter Gansevoort invited to move into Stanwix Hall for no rent. In the "Albany Microscope" in March, Melville published two polemical letters about issues in vogue in the debating societies whose subjects are now obscure. Leon Howard and Hershel Parker suggest that the real issue was the youthful desire to have his rhetorical skills publicly recognized.

In May the Melvilles moved to a rented house in Lansingburgh, almost 12 miles north of Albany. Nothing is known about what Melville did or where he went for several months after he finished teaching at Sikes.

On November 12, five days after arriving in Lansingburgh, Melville paid for a term at Lansingburgh Academy to study surveying and engineering. In an April 1839 letter recommending Herman for a job in the Engineer Department of the Erie Canal, Peter Gansevoort says his nephew "possesses the ambition to make himself useful in a business which he desires to make his profession," but no job resulted.

Just weeks after this failure, Melville's first known published essay appeared. Using the initials "L.A.V.", Herman contributed "Fragments from a Writing Desk" to the weekly newspaper "Democratic Press and Lansingburgh Advertiser", which printed it in two installments, the first on May 4. According to Merton Sealts, his use of heavy-handed allusions reveals familiarity with the work of William Shakespeare, John Milton, Walter Scott, Richard Brinsley Sheridan, Edmund Burke, Samuel Taylor Coleridge, Lord Byron, and Thomas Moore. Parker calls the piece "characteristic Melvillean mood-stuff" and considers its style "excessive enough [...] to indulge his extravagances and just enough overdone to allow him to deny that he was taking his style seriously." For Delbanco, the style is "overheated in the manner of Poe, with sexually charged echoes of Byron and "The Arabian Nights.""

On May 31, 1839, Gansevoort, then living in New York City, wrote that he was sure Herman could get a job on a whaler or merchant vessel. The next day, he signed aboard the merchant ship "St. Lawrence" as a "boy" (a green hand), which cruised from New York to Liverpool; he arrived back in New York October 1. "Redburn: His First Voyage" (1849) draws on his experiences in this journey. Melville resumed teaching, now at Greenbush, New York, but left after one term because he had not been paid. In the summer of 1840 he and his friend James Murdock Fly went to Galena, Illinois to see if his Uncle Thomas could help them find work. On this trip it is possible that Herman went up the Mississippi, where he may well have witnessed scenes of frontier life he later used in his books. Unsuccessful, he and his friend returned home in autumn, very likely by way of St. Louis and up the Ohio River.

Probably inspired by his reading of Richard Henry Dana, Jr.'s new book "Two Years Before the Mast," and by Jeremiah N. Reynolds's account in the May 1839 issue of "The Knickerbocker" magazine of the hunt for a great white sperm whale named Mocha Dick, Melville and Gansevoort traveled to New Bedford, where Melville signed up for a whaling voyage aboard a new ship, the "Acushnet". Built in 1840, the ship measured some 104 feet in length, almost 28 feet in breadth, and almost 14 feet in depth. She measured slightly less than 360 tons, had two decks and three masts, but no galleries. Melville signed a contract on Christmas Day with the ship's agent as a "green hand" for 1/175th of whatever profits the voyage would yield. On Sunday the 27th the brothers heard the Reverend Enoch Mudge preach at the Seamen's Bethel on Johnny-Cake Hill, where white marble cenotaphs on the walls memorialized local sailors who had died at sea, often in battle with whales. When he signed the crew list the next day he was advanced $84.

On January 3, 1841, the "Acushnet" set sail. Melville slept with some twenty others in the forecastle; Captain Valentine Pease, the mates, and the skilled men slept aft. Whales were found near The Bahamas, and in March 150 barrels of oil were sent home from Rio de Janeiro. Cutting in and trying-out (boiling) a single whale took some three days, and a whale yielded approximately one barrel of oil per foot of length and per ton of weight (the average whale weighed 40 to 60 tons). The oil was kept on deck for a day to cool off, and was then stowed down; scrubbing the deck completed the labor. An average voyage meant that some forty whales were killed to yield some 1600 barrels of oil.

On April 15, the "Acushnet" sailed around Cape Horn, and traveled to the South Pacific, where the crew sighted whales without catching any. Then up the coast of Chile to the region of Selkirk Island and on 7 May, near Juan Fernández Islands, she had 160 barrels. On June 23 the ship anchored for the first time since Rio, in Santa Harbor. The cruising grounds the "Acushnet" was sailing attracted much traffic, and Captain Pease not only paused to visit other whalers, but at times hunted in company with them. From July 23 into August the "Acushnet" regularly gammed with the "Lima" from Nantucket, and Melville met William Henry Chase, the son of Owen Chase, who gave him a copy of his father's account of his adventures aboard the "Essex". Ten years later Melville wrote in his other copy of the book: "The reading of this wondrous story upon the landless sea, & close to the very latitude of the shipwreck had a surprising effect upon me."

On September 25 the ship reported 600 barrels of oil to another whaler, and in October 700 barrels. On October 24 the "Acushnet" crossed the equator to the north, and six or seven days later arrived at the Galápagos Islands. This short visit would be the basis for "The Encantadas". On November 2, the Acushnet and three other American whalers were hunting together near the Galápagos Islands; Melville later exaggerated that number in Sketch Fourth of "The Encantadas". From November 19 to 25 the ship anchored at Chatham's Isle, and on December 2 reached the coast of Peru and anchored at Tombez near Paita, with 570 barrels of oil on board. On December 27 the "Acushnet" sighted Cape Blanco, off Ecuador, Point St. Elena was sighted the next day, and on January 6, 1842 the ship approached the Galápagos Islands from the southeast. From February 13 to 7 May, seven sightings of sperm whales were recorded but none killed. From early May to early June the "Acushnet" gammed several times with the "Columbus" of New Bedford, which also took letters from Melville's ship; the two ships were in the same area just south of the Equator. On June 16 she carried 750 barrels, and sent home 200 on the "Herald the Second".

On June 23, the "Acushnet" reached the Marquesas Islands, and anchored at Nuku Hiva. On July 9, 1842, Melville and his shipmate Richard Tobias Greene jumped ship at Nukahiva Bay and ventured into the mountains to avoid capture. Melville's first book, Typee (1845), is loosely based on his stay in or near the Taipi Valley. Scholarly research starting in the 1930s and extending into the twenty-first century has increasingly shown that much if not all of this account was either taken from Melville's readings or exaggerated to dramatize a contrast between idyllic native culture and Western civilization. 

On August 9, Melville boarded the "Lucy Ann", bound for Tahiti, where he took part in a mutiny and was briefly jailed in the native "Calabooza Beretanee".

In October, he and crew mate John B. Troy escaped Tahiti for Eimeo. He then spent a month as beachcomber and island rover ("omoo" in Tahitian), eventually crossing over to Moorea. He drew on these experiences for "Omoo", the sequel to "Typee" In November, he signed articles on the Nantucket whaler "Charles & Henry" for a six-month cruise (November 1842 − April 1843), and was discharged at Lahaina, Maui in the Hawaiian Islands in May 1843. After four months of working several jobs, including as a clerk, he joined the US Navy initially as one of the crew of the frigate as an ordinary seaman on August 20. During the next year, the homeward bound ship visited the Marquesas Islands, Tahiti, and Valparaiso, and then, from summer to fall 1844, Mazatlan, Lima, and Rio de Janeiro, before reaching Boston on October 3. Melville was discharged on October 14. This navy experience is used in "White-Jacket" (1850) Melville's fifth book.

Melville's wander-years created what biographer Arvin calls "a settled hatred of external authority, a lust for personal freedom" and a "growing and intensifying sense of his own exceptionalness as a person," along with "the resentful sense that circumstance and mankind together had already imposed their will upon him in a series of injurious ways." Scholar Milder believes the encounter with the wide ocean, where he was seemingly abandoned by God, led Melville to experience a "metaphysical estrangement" and influenced his social views in two ways: first that he belonged to the genteel classes but sympathized with the "disinherited commons" he had been placed among; second that experiencing the cultures of Polynesia let him view the West from an outsider's perspective.

Upon his return, Melville regaled his family and friends with his adventurous tales and romantic experiences, and they urged him to put them into writing. Melville completed "Typee", his first book, in the summer of 1845 while living in Troy. His brother Gansevoort found a publisher for it in London, where it was published in February 1846 by John Murray and became an overnight bestseller, then in New York on March 17 by Wiley & Putnam. Inspired by his adventures in the Marquesas, the book was far from a reliable autobiographical account.

Melville extended the period his narrator spent on the island to three months more than he himself did, made it appear that he understood the native language, and incorporated material from source books he had assembled. Scholar Robert Milder calls "Typee" "an appealing mixture of adventure, anecdote, ethnography, and social criticism presented with a genial latitudinarianism that gave novelty to a South Sea idyll at once erotically suggestive and romantically chaste."

An unsigned review in the "Salem Advertiser", actually written by Nathaniel Hawthorne, called the book a "skilfully managed" narrative, "lightly but vigorously written" by an author with "that freedom of view ... which renders him tolerant of codes of morals that may be little in accordance with our own." The depictions of the "native girls are voluptuously colored, yet not more so than the exigencies of the subject appear to require." Pleased but slightly bemused by the adulation of his new public, Melville later complained in a letter to Nathaniel Hawthorne that he would "go down to posterity ... as a ‘man who lived among the cannibals'!"

Some readers found it unbelievable, however. The book brought Melville into contact with his friend Greene again, Toby in the book, who wrote to the newspapers confirming Melville's account. The two corresponded until 1863, and sustained a bond for life: in his final years Melville "traced and successfully located his old friend."

In March 1847, "Omoo", a sequel to "Typee", was published by Murray in London, and in May by Harper in New York. "Omoo" is "a slighter but more professional book," according to Milder. "Typee" and "Omoo" gave Melville overnight renown as a writer and adventurer, and he often entertained by telling stories to his admirers. As the writer and editor Nathaniel Parker Willis wrote, "With his cigar and his Spanish eyes, he "talks" Typee and Omoo, just as you find the flow of his delightful mind on paper." In 1847 Melville tried unsuccessfully to find a "government job" in Washington.
In June 1847, Melville and Elizabeth Knapp Shaw were engaged, after knowing each other for approximately three months. A brief courtship, yet Melville had already asked her father for her hand in March but was turned down. Lizzie's father was Lemuel Shaw, the Chief Justice of the Massachusetts Supreme Judicial Court. Arvin describes Lemuel Shaw as a man "of an almost childlike tenderness of heart and gentleness of feeling." He had been an intimate friend of Melville's father, and had fallen in love with Allen's sister Nancy. He would have married her had she not died early. His grief was such that for many years he did not marry, and his friendship with the Melvilles continued after Allen's death. His wife Elizabeth died when she gave birth to their first daughter, who was named Elizabeth in remembrance of her mother. Elizabeth was raised by her grandmother and the Irish nurse. Arvin suggests that Melville's interest in Elizabeth may have been stimulated by "his need of Judge Shaw's paternal presence." They were married on August 4, 1847. As Lizzie wrote a relative, "My marriage was very unexpected, and scarcely thought of until about two months before it actually took place." She wanted to be married in church, "but we all thought if it were to get about previously that 'Typee' was to be seen on such a day, a great crowd might rush out of mere curiosity to see 'the author' who would have no personal interest in us whatsoever, and make it very unpleasant for us both." Because of Melville's celebrity status a private wedding ceremony took place at home. The couple honeymooned in Canada, and traveled to Montreal. They settled in a house on Fourth Avenue in New York City.

According to scholars Joyce Deveau Kennedy and Frederick James Kennedy, Lizzie brought the following qualities to their marriage: a sense of religious obligation in marriage, an intent to make a home with Melville regardless of place, a willingness to please her husband by performing such "tasks of drudgery" as mending stockings, an ability to hide her agitation, and a desire "to shield Melville from unpleasantness." The Kennedys conclude their assessment with:

Biographer Robertson-Lorant's cites "Lizzie's adventurous spirit and abundant energy," and she suggests that "her pluck and good humor might have been what attracted Melville to her, and vice versa." An example of such good humor appears in a letter about her not yet used to being married: "It seems sometimes exactly as if I were here for a "visit". The illusion is quite dispelled however when Herman stalks into my room without even the ceremony of knocking, bringing me perhaps a button to sew on, or some equally romantic occupation."

In March 1848, "Mardi" was published by Richard Bentley in London, and in April by Harper in New York. Nathaniel Hawthorne thought it a rich book, he told Evert Augustus Duyckinck, a friend of Melville's, "with depths here and there that compel a man to swim for his life." According to Robert Milder, the book began as another South Sea story but, as he wrote, Melville left that genre behind, first in favor of "a romance of the narrator Taji and the lost maiden Yillah," and then "to an allegorical voyage of the philosopher Babbalanja and his companions through the imaginary archipelago of Mardi," On February 16, 1849, the Melvilles' first child, Malcolm, was born.

In October 1849, "Redburn" was published by Bentley in London, and in November by Harper in New York. The bankruptcy and death of Allan Melville, and Melville's own youthful humiliations surface in this "story of outward adaptation and inner impairment." Biographer Robertson-Lorant regards the work as a deliberate attempt for popular appeal: "Melville modeled each episode almost systematically on every genre that was popular with some group of antebellum readers," combining elements of "the picaresque novel, the travelogue, the nautical adventure, the sentimental novel, the sensational French romance, the gothic thriller, temperance tracts, urban reform literature, and the English pastoral."

In January 1850, "White-Jacket" was published by Bentley in London, and in March by Harper in New York.

In early May 1850 Melville wrote to fellow sea author Richard Henry Dana Jr. a letter that contains what is the earliest surviving mention of the writing of "Moby-Dick", saying he was already "half way" done. In June he described the book to his English publisher as "a romance of adventure, founded upon certain wild legends in the Southern Sperm Whale Fisheries," and promised it would be done by the fall. The manuscript has not survived, so it is impossible to know its state at this juncture. Over the next several months, Melville radically transformed his initial plan, conceiving what Delbanco has described as "the most ambitious book ever conceived by an American writer."

From August 4 to 12, 1850, the Melvilles and their neighbor Sarah Morewood, Evert Duyckinck, Oliver Wendell Holmes, and other literary figures from New York and Boston, came to Pittsfield to enjoy a period of social parties, picnics, dinners, and the like. On August 5, Nathaniel Hawthorne and his publisher James T. Fields joined the group, while Hawthorne's wife stayed at home to look after the children. As the men took a stroll through Ice Glen, someone noticed that Hawthorne and Melville were absent, and the group went looking for the two, who were at last found "deep in conversation." The following day, the group visited the Hawthornes, who since May had been living in nearby Lenox. "I liked Melville so much," Hawthorne wrote to his friend Horatio Bridge, "that I have asked him to spend a few days with me," an uncharacteristic move for a man so attached to his work that he would not suffer overnight guests to keep him from it. The following days, Melville wrote the essay "Hawthorne and His Mosses," a review of Hawthorne's "Mosses from an Old Manse" that appeared in two installments, on August 17 and 24, in "The Literary World".

Later that summer, Melville received a package from Duyckinck with a letter asking him to forward it to Hawthorne. When he delivered the package to Hawthorne, he did not know it contained his last three books. Hawthorne read them, as he wrote to Duyckinck on August 29, "with a progressive appreciation of their author." He thought Melville in "Redburn" and "White-Jacket" put the reality "more unflinchingly" before his reader than any writer, and he thought "Mardi" was "a rich book, with depths here and there that compel a man to swim for his life. It is so good that one scarcely pardons the writer for not having brooded long over it, so as to make it a great deal better."

In September 1850, Melville borrowed three thousand dollars from his father-in-law Lemuel Shaw to buy a 160-acre farm in Pittsfield. Melville called his new home Arrowhead because of the arrowheads that were dug up around the property during planting season.

That winter, Melville on an impulse paid Hawthorne a visit, only to discover he was "not in the mood for company" as he was finishing "The House of the Seven Gables". Hawthorne's wife Sophia entertained him while he waited for Hawthorne to come down for supper, and gave him copies of "Twice-Told Tales" and, for Malcolm, "The Grandfather's Chair". Melville invited them to visit Arrowhead some time in the next two weeks, and when Sophia agreed, he looked forward to "discussing the Universe with a bottle of brandy & cigars" with Hawthorne. A few days later Sophia notified the Melvilles that Hawthorne could not stop working on his new book for more than one day, and Melville felt moved to repeat his invitation: "Your bed is already made, & the wood marked for your fire." Eventually Melville visited the Hawthornes again, and the next day Hawthorne surprised him by arriving at Arrowhead with his daughter Una. According to Robertson-Lorant, "The handsome Hawthorne made quite an impression on the Melville women, especially Augusta, who was a great fan of his books." They spent the day mostly "smoking and talking metaphysics."

In Robertson-Lorant's assessment of the friendship, Melville was "infatuated with Hawthorne's intellect, captivated by his artistry, and charmed by his elusive personality," and though the two writers were "drawn together in an undeniable sympathy of soul and intellect, the friendship meant something different to each of them," with Hawthorne offering Melville "the kind of intellectual stimulation he needed." They may have been "natural allies and friends," yet they were also "fifteen years apart in age and temperamentally quite different," and Hawthorne "found Melville's manic intensity exhausting at times."

Melville wrote ten letters to Hawthorne, "all of them effusive, profound, deeply affectionate." Melville was inspired and encouraged by his new relationship with Hawthorne during the period that he was writing "Moby-Dick." He dedicated this new novel to Hawthorne, though their friendship was to wane only a short time later.

On October 18, 1851, "The Whale" was published in Britain in three volumes, and on November 14 "Moby-Dick" appeared in the United States as a single volume. In between these dates, on October 22, 1851, the Melvilles' second child, Stanwix, was born. On December 1 Hawthorne wrote a letter of appraisal to Duyckinck, prompted in part by his disagreement with the review in "Literary World": "What a book Melville has written! It gives me an idea of much greater power than his preceding ones. It hardly seemed to me that the review of it, in the "Literary World", did justice to its best points."

In early December 1852, Melville visited the Hawthornes in Concord, and discussed the idea of the "Agatha" story which he had pitched to Hawthorne. This was the last known contact between the two writers before Melville visited Hawthorne in Liverpool four years later.

Melville had high hopes that his next book would please the public and restore his finances. In April 1851 he wrote to his British publisher, Richard Bentley, that his new book, "possessing unquestionable novelty" is "as I believe, very much more calculated for popularity than anything you have yet published of mine—being a regular romance, with a mysterious plot to it & stirring passions at work, and withall, representing a new & elevated aspect of American life..." In fact,
"" was heavily psychological, though drawing on the conventions of the romance, and difficult in style. It was not well received. The New York "Day Book" on September 8, 1852, published a venomous attack headlined "HERMAN MELVILLE CRAZY." The item, offered as a news story, reported,
On May 22, 1853, Elizabeth (Bessie) was born, the Melvilles' third child and first daughter, and on or about that day Herman finished work on "Isle of the Cross"—one relative wrote that 'The Isle of the Cross' is almost a twin sister of the little one ..." Herman traveled to New York to discuss it with his publisher, but later wrote that Harper & Brothers was "prevented" from publishing his manuscript, presumed to be "Isle of the Cross", which has been lost.

Finding it difficult to find a publisher for his follow-up novel to the commercial and critical failure of "Pierre", "Israel Potter", the narrative of a Revolutionary War veteran, was first serialized in "Putnam's Monthly Magazine" in 1853. From November 1853 to 1856, Melville published fourteen tales and sketches in "Putnam's" and "Harper"'s magazines. In December 1855 he proposed to Dix & Edwards, the new owners of "Putnam's", that they publish a selection of the short fiction titled "Benito Cereno and Other Sketches". The collection would eventually be named after a new introductory story Melville had written for it, "The Piazza," and was published as "The Piazza Tales", with five previously published stories, including "Bartleby, the Scrivener" and "Benito Cereno."

On March 2, 1855, Frances (Fanny) was born, the Melvilles' fourth child. In this period his book "Israel Potter" was published.

The writing of "The Confidence-Man" put great strain on Melville, leading Sam Shaw, a nephew of Lizzie, to write to his uncle Lemuel Shaw, "Herman I hope has had no more of those ugly attacks"—a reference to what Robertson-Lorant calls "the bouts of rheumatism and sciatica that plagued Melville." Melville's father-in-law apparently shared his daughter's "great anxiety about him" when he wrote a letter to a cousin, in which he described Melville's working habits: "When he is deeply engaged in one of his literary works, he confines him[self] to hard study many hours in the day, with little or no exercise, & this specially in winter for a great many days together. He probably thus overworks himself & brings on severe nervous affections." Shaw advanced Melville $1,500 from Lizzie's inheritance to travel four or five months in Europe and the Holy Land.

From October 11, 1856, to May 20, 1857, Melville made a six-month Grand Tour of the British Isles and the Mediterranean. While in England, in November he spent three days with Hawthorne, who had taken an embassy position there. At the seaside village of Southport, amid the sand dunes where they had stopped to smoke cigars, they had a conversation which Hawthorne later described in his journal:
Melville's subsequent visit to the Holy Land inspired his epic poem "Clarel."

On April 1, 1857, Melville published his last full-length novel, "The Confidence-Man". This novel, subtitled "His Masquerade", has won general acclaim in modern times as a complex and mysterious exploration of issues of fraud and honesty, identity and masquerade. But, when it was published, it received reviews ranging from the bewildered to the denunciatory.

To repair his faltering finances, Melville was advised by friends to enter what had proven to be, at least for others, a remunerative field: public lecturing. From late 1857 to 1860, he embarked upon three lecture tours, and spoke at lyceums, chiefly on Roman statuary and sightseeing in Rome. Melville's lectures, which mocked the pseudo-intellectualism of lyceum culture, were panned by contemporary audiences.

On May 30, 1860, Melville boarded the clipper "Meteor" for California, with his brother Thomas at the helm. After a shaky trip around Cape Horn Melville returned alone to New York via Panama in November. Turning to poetry, he submitted a collection of verse to a publisher in 1860, but it was not accepted. In 1861 he shook hands with Abraham Lincoln. In 1862, Melville met with a road accident which left him seriously injured. He also suffered from rheumatism. In 1863, he bought his brother's house at 104 East 26th Street in New York City and moved there. Allan, his brother, for his part bought Arrowhead. On March 30, his father-in-law died.

In 1864, Melville and Allan paid a visit to the Virginia battlefields of the American Civil War. After the end of the war, he published "Battle Pieces and Aspects of the War" (1866), a collection of 72 poems that has been described as "a polyphonic verse journal of the conflict". It was generally ignored by reviewers, who gave him at best patronizingly favorable reviews. The volume did not sell well; of the Harper & Bros. printing of 1200 copies, only 525 had been sold ten years later. Uneven as a collection of individual poems, "its achievement lies in the interplay of voices and moods throughout which Melville patterns a shared historical experience into formative myth".

In 1866, Melville's wife and her relatives used their influence to obtain a position for him as customs inspector for the City of New York, a humble but adequately paying appointment. He held the post for 19 years and won the reputation of being the only honest employee in a notoriously corrupt institution. Unbeknownst to him, his modest position and income "were protected throughout the periodic turmoil of political reappointments by a customs official who never spoke to Melville but admired his writings: future US president Chester A. Arthur". In 1867 his oldest son Malcolm shot himself, perhaps accidentally, and died at home at the age of 18. Some psychologists believe it was a suicide.

The job came as a relief for his family, because he would be out of the house for much of the day. Melville suffered from unpredictable mood swings, habitually "bullying his servants, wife, and children." As Robertson-Lorant's writes, "Like the tyrannical captains he had portrayed in his novels, Melville probably provoked rebellious feelings in his 'crew' by the capricious way he ruled the home, especially when he was drinking." Nervous exhaustion and physical pain rendered him short-tempered, made worse by his drinking. Robertson-Lorant takes the different ways one can look at Melville in this period to their extremes:

In May 1867, Sam Shaw contacted Lizzie's minister Henry Bellows asking assistance with his "sister's case", which "has been a cause of anxiety to all of us for years past." A wife then could not leave her husband without losing all claims to the children, so Bellows suggested Lizzie be kidnapped and brought to Boston. Shaw suspected that Lizzie would not agree to such melodramatic scheme. He thought up a different scheme, in which Lizzie would visit Boston and friends would inform Herman she would not come back. To get a divorce, she would then have to bring charges against Melville, believing her husband to be insane.

Though Melville's professional writing career had ended, he remained dedicated to his writing. He bought books on poetry, landscape, art, and engraving. He had a Rembrandt mezzotint framed in his New York residence. In 1872 his brother Allan died, as did his mother, aged eighty-two. Melville devoted years to "his autumnal masterpiece," an 18,000-line epic poem titled " Clarel: A Poem and a Pilgrimage", inspired by his 1856 trip to the Holy Land. It is among the longest single poems in American literature. The title character is a young American student of divinity who travels to Jerusalem to renew his faith. One of the central characters, Rolfe, is similar to Melville in his younger days, a seeker and adventurer, while the reclusive Vine is loosely based on Hawthorne, who had died twelve years before. A bequest from his uncle, Peter Gansevoort, paid for publication in 1876, but epic-length verse-narrative was considered quite obscure even in his own time. The book had an initial printing of 350 copies, but sales failed miserably, and the unsold copies were burned when Melville was unable to afford to buy them at cost. The critic Lewis Mumford found a copy in the New York Public Library in 1925 "with its pages uncut"—in other words, it had sat there unread for 50 years.

In 1884, Mrs. Melville received a legacy, which enabled her to allow Melville a monthly sum of $25 to spend on books and prints. While Melville had his steady customs job, he no longer showed signs of depression, which recurred after the death of his second son. On February 23, 1886, Stanwix Melville died in San Francisco at age 36. Melville retired on December 31, 1885, after several of his wife's relatives died and left the couple more legacies which Mrs. Melville administered with skill and good fortune. In 1889 Melville became a member of the New York Society Library.

As English readers, pursuing the vogue for sea stories represented by such writers as G. A. Henty, rediscovered Melville's novels in the late nineteenth century, the author had a modest revival of popularity in England, though not in the United States. He wrote a series of poems, with prose head notes, inspired by his early experiences at sea. He published them in two collections, each issued in a tiny edition of 25 copies for his relatives and friends. Of these, scholar Robert Milder calls "John Marr and Other Poems" (1888), "the finest of his late verse collections". The second privately printed volume is "Timoleon" (1891).

Intrigued by one of these poems, Melville began to rework the headnote, expanding it first as a short story and eventually as a novella. He worked on it on and off for several years, but when he died in September 1891, the piece was unfinished. Also left unpublished were another volume of poetry, "Weeds and Wildings", and a sketch, "Daniel Orme." To "Billy Budd", his widow added notes and edited it, but the manuscript was not discovered until 1919, by Raymond Weaver, his first biographer. He worked at transcribing and editing a full text, which he published in 1924 as "Billy Budd, Sailor." It was an immediate critical success in England and soon one in the United States. The authoritative version was published in 1962, after two scholars studied the papers for several years. In 1951 it was adapted as a stage play on Broadway, and as an opera by English composer Benjamin Britten with assistance on the libretto by E. M. Forster. In 1961 Peter Ustinov released a movie based on the stage play and starring Terence Stamp.

Melville died at his home in New York City early on the morning of September 28, 1891, at age 72. The doctor listed "cardiac dilation" on the death certificate. He was interred in the Woodlawn Cemetery in The Bronx, New York City. A common story recounts that his "The New York Times" obituary called him ""Henry" Melville," implying that he was unknown and unappreciated at his time of death, but the story is not true. A later article was published on October 6 in the same paper, referring to him as "the late Hiram Melville," but this appears to have been a typesetting error.

Melville's writing style shows enormous changes throughout the years as well as consistencies. In the words of Arvin, Melville's development "had been abnormally postponed, and when it came, it came with a rush and a force that had the menace of quick exhaustion in it." As early a juvenile piece as "Fragments from a Writing Desk" from 1839, scholar Sealts points out, already shows "a number of elements that anticipate Melville's later writing, especially his characteristic habit of abundant literary allusion". "Typee" and "Omoo" were documentary adventures that called for a division of the narrative in short chapters. Such compact organization bears the risk of fragmentation when applied to a lengthy work such as "Mardi", but with "Redburn" and "White Jacket," Melville turned the short chapter into an instrument of form and concentration.

A number of chapters of "Moby-Dick" are no longer than two pages in standard editions, and an extreme example is Chapter 122, consisting of a single paragraph of 36 words (including the thrice-repeated "Um, um, um.") The skillful handling of chapters in "Moby-Dick" is one of the most fully developed Melvillean signatures, and is a measure of "his manner of mastery as a writer". Individual chapters have become "a touchstone for appreciation of Melville's art and for explanation" of his themes. In contrast, the chapters in "Pierre", called Books, are divided into short numbered sections, seemingly an "odd formal compromise" between Melville's natural length and his purpose to write a regular romance that called for longer chapters. As satirical elements were introduced, the chapter arrangement restores "some degree of organization and pace from the chaos". The usual chapter unit then reappears for "Israel Potter", "The Confidence-Man" and even "Clarel", but only becomes "a vital part in the whole creative achievement" again in the juxtaposition of accents and of topics in "Billy Budd".

Newton Arvin points out that only superficially the books after "Mardi" seem as if Melville's writing went back to the vein of his first two books. In reality, his movement "was not a retrograde but a spiral one", and while "Redburn" and "White-Jacket" may lack the spontaneous, youthful charm of his first two books, they are "denser in substance, richer in feeling, tauter, more complex, more connotative in texture and imagery." The rhythm of the prose in "Omoo" "achieves little more than easiness; the language is almost neutral and without idiosyncrasy", yet "Redburn" shows a "gain in rhythmical variety and intricacy, in sharpness of diction, in syntactical resource, in painterly bravura and the fusing of image and emotion into a unity of strangeness, beauty and dread".

Melville's early works were "increasingly baroque" in style, and with "Moby-Dick" Melville's vocabulary had grown superabundant. Bezanson calls it an "immensely varied style". According to critic Warner Berthoff, three characteristic uses of language can be recognized. First, the exaggerated repetition of words, as in the series "pitiable," "pity," "pitied," and "piteous" (Ch. 81, "The Pequod Meets the Virgin"). A second typical device is the use of unusual adjective-noun combinations, as in "concentrating brow" and "immaculate manliness" (Ch. 26, "Knights and Squires"). A third characteristic is the presence of a participial modifier to emphasize and to reinforce the already established expectations of the reader, as the words "preluding" and "foreshadowing" ("so still and subdued and yet somehow preluding was all the scene ...," "In this foreshadowing interval ...").

After his use of hyphenated compounds in "Pierre", Melville's writing gives Berthoff the impression of becoming less exploratory and less provocative in his choices of words and phrases. Instead of providing a lead "into possible meanings and openings-out of the material in hand," the vocabulary now served "to crystallize governing impressions," the diction no longer attracted attention to itself, except as an effort at exact definition. The language, Berthoff continues, reflects a "controlling intelligence, of right judgment and completed understanding". The sense of free inquiry and exploration which infused his earlier writing and accounted for its "rare force and expansiveness," tended to give way to "static enumeration." For Berthoff, added "seriousness of consideration" came at the cost of losing "pace and momentum". The verbal music and kinetic energy of "Moby-Dick" seem "relatively muted, even withheld" in the later works.

Melville's paragraphing in his best work Berthoff considers to be the virtuous result of "compactness of form and free assembling of unanticipated further data", such as when the mysterious sperm whale is compared with Exodus's invisibility of God's face in the final paragraph of Chapter 86 ("The Tail"). Over time Melville's paragraphs became shorter as his sentences grew longer, until he arrived at the "one-sentence paragraphing characteristic of his later prose." Berthoff points to the opening chapter of "The Confidence-Man" for an example, as it counts fifteen paragraphs, seven of which consist of only one elaborate sentence, and four that have only two sentences. The use of similar technique in "Billy Budd" contributes in large part, Berthoff says, to its "remarkable narrative economy".

In Nathalia Wright's view, Melville's sentences generally have a looseness of structure, easy to use for devices as catalogue and allusion, parallel and refrain, proverb and allegory. The length of his clauses may vary greatly, but the "torterous" writing in "Pierre" and "The Confidence-Man" is there to convey feeling, not thought. Unlike Henry James, who was an innovator of sentence ordering to render the subtlest nuances in thought, Melville made few such innovations. His domain is the mainstream of English prose, with its rhythm and simplicity influenced by the King James Bible.

Another important characteristic of Melville's writing style is in its echoes and overtones. Melville's imitation of certain distinct styles is responsible for this. His three most important sources, in order, are the Bible, Shakespeare, and Milton. Scholar Nathalia Wright has identified three stylistic categories of Biblical influence. Direct quotation from any of the sources is slight; only one sixth of his Biblical allusions can be qualified as such.

First, far more unmarked than acknowledged quotations occur, some favorites even numerous times throughout his whole body of work, taking on the nature of refrains. Examples of this idiom are the injunctions to be 'as wise as serpents and as harmless as doves,' 'death on a pale horse,' 'the man of sorrows', the 'many mansions of heaven;' proverbs 'as the hairs on our heads are numbered,' 'pride goes before a fall,' 'the wages of sin is death;' adverbs and pronouns as 'verily, whoso, forasmuch as; phrases as come to pass, children's children, the fat of the land, vanity of vanities, outer darkness, the apple of his eye, Ancient of Days, the rose of Sharon.'

Second, there are paraphrases of individual and combined verses. Redburn's "Thou shalt not lay stripes upon these Roman citizens" makes use of language of the Ten Commandments in Ex.20, and Pierre's inquiry of Lucy: "Loveth she me with the love past all understanding?" combines John 21:15–17 and Philippians 4:7

Third, certain Hebraisms are used, such as a succession of genitives ("all the waves of the billows of the seas of the boisterous mob"), the cognate accusative ("I dreamed a dream," "Liverpool was created with the Creation"), and the parallel ("Closer home does it go than a rammer; and fighting with steel is a play without ever an interlude").

A passage from "Redburn" (see quotebox) shows how all these different ways of alluding interlock and result in a fabric texture of Biblical language, though there is very little direct quotation.

In addition to this, Melville successfully imitates three Biblical strains: he sustains the apocalyptic for a whole chapter of "Mardi;" the prophetic strain is expressed in "Moby-Dick", most notably in Father Mapple's sermon; and the tradition of the Psalms is imitated at length in "The Confidence-Man".

Melville owned an edition of Shakespeare's works by 1849, and his reading of it greatly influenced the style of his next book, "Moby-Dick" (1851). The critic F. O. Matthiessen found that the language of Shakespeare far surpasses other influences upon the book, in that it inspired Melville to discover his own full strength. On almost every page, debts to Shakespeare can be discovered. The "mere sounds, full of Leviathanism, but signifying nothing" at the end of "Cetology" (Ch.32) echo the famous phrase in "Macbeth:" "Told by an idiot, full of sound and fury/ Signifying nothing." Ahab's first extended speech to the crew, in the "Quarter-Deck" (Ch.36) is practically blank verse and so is Ahab's soliloquy at the beginning of "Sunset" (Ch.37):'I leave a white and turbid wake;/ Pale waters, paler cheeks, where'er I sail./ The envious billows sidelong swell to whelm/ My track; let them; but first I pass.' Through Shakespeare, Melville infused "Moby-Dick" with a power of expression he had not previously possessed. Reading Shakespeare had been "a catalytic agent" for Melville, one that transformed his writing from merely reporting to "the expression of profound natural forces." The extent to which Melville assimilated Shakespeare is evident in the description of Ahab, Matthiessen continues, which ends in language that seems Shakespearean yet is no imitation: 'Oh, Ahab! what shall be grand in thee, it must needs be plucked from the skies and dived for in the deep, and featured in the unbodied air!' The imaginative richness of the final phrase seems particularly Shakespearean, "but its two key words appear only once each in the plays...and to neither of these usages is Melville indebted for his fresh combination." Melville's diction depended upon no source, and his prose is not based on anybody else's verse but on an awareness of "speech rhythm".

Melville's mastering of Shakespeare, Matthiessen finds, supplied him with verbal resources that enabled him to create dramatic language through three essential techniques. First, the use of verbs of action creates a sense of movement and meaning. The effective tension caused by the contrast of "thou launchest navies of full-freighted worlds" and "there's that in here that still remains indifferent" in "The Candles" (Ch. 119) makes the last clause lead to a "compulsion to strike the breast," which suggests "how thoroughly the drama has come to inhere in the words;" Second, Melville took advantage of the Shakespearean energy of verbal compounds, as in "full-freighted". Third, Melville employed the device of making one part of speech act as another – for example, 'earthquake' as an adjective, or turning an adjective into a noun, as in "placeless".

Melville's style, in Nathalia Wright's analysis, seamlessly flows over into theme, because all these borrowings have an artistic purpose, which is to suggest an appearance "larger and more significant than life" for characters and themes that are in fact unremarkable. The allusions suggest that beyond the world of appearances another world exists, one that influences this world, and where ultimate truth can be found. Moreover, the ancient background thus suggested for Melville's narratives – ancient allusions being next in number to the Biblical ones – invests them with a sense of timelessness.

Melville was not financially successful as a writer, having earned just over $10,000 for his writing during his lifetime. His popularity declined dramatically after his success with travelogues based on voyages to the South Seas and his stories based on misadventures in the merchant marine and navy. By 1876, all of his books were out of print. He was viewed as a minor figure in American literature in the later years of his life and during the years after his death.

The "Melville Revival" of the late 1910s and 1920s brought about a reassessment of his work. The centennial of his birth was in 1919. Carl Van Doren's 1917 article on Melville in a standard history of American literature was the start of renewed appreciation. Van Doren also encouraged Raymond Weaver, who wrote the author's first full-length biography, "Herman Melville: Mariner and Mystic" (1921). Discovering the unfinished manuscript of "Billy Budd", among papers shown to him by Melville's granddaughter, Weaver edited it and published it in a new collected edition of Melville's works. Other works that helped fan the flames for Melville were Carl Van Doren's "The American Novel" (1921), D. H. Lawrence's "Studies in Classic American Literature" (1923), Carl Van Vechten's essay in "The Double Dealer" (1922), and Lewis Mumford's biography, "Herman Melville: A Study of His Life and Vision" (1929).
Starting in the mid-1930s, the Yale University scholar Stanley Williams supervised more than a dozen dissertations on Melville that were eventually published as books. Where the first wave of Melville scholars focused on psychology, Williams' students were prominent in establishing Melville Studies as an academic field concerned with texts and manuscripts, tracing Melville's influences and borrowings (even plagiarism), and exploring archives and local publications. Jay Leyda, known for his work in film, spent more than a decade in archives and small-town libraries gathering documents and records for a day by day record, published as "Melville Log" (1951). Sparked by Leyda, the second phase of the Melville Revival emphasized research rather than accepting Melville's early books as reliable accounts.

The postwar scholars tended to think that Weaver, Harvard psychologist Henry Murray, and Mumford favored Freudian interpretations which read Melville's fiction too literally as autobiography; exaggerated his suffering in the family; and inferred a homosexual attachment to Hawthorne. They saw a different arc to Melville's writing career. The first biographers saw a tragic withdrawal after the cold critical reception for his prose works and largely dismissed his poetry. A new view emerged of Melville's turn to poetry as a conscious choice that placed him among the most important American poets.

Other post-war studies, however, continued the broad imaginative and interpretive style. Charles Olson's "Call Me Ishmael" (1947) presented Ahab as a Shakespearean tragic hero, and Newton Arvin's critical biography, "Herman Melville" (1950) won the National Book Award for non-fiction in 1951. Hershel Parker published his two volume "Herman Melville: A Biography", in 1996 and 2002, based on extensive original research and his involvement as editor of the Northwestern-Newberry Melville edition. 

In the 1960s, Northwestern University Press, in alliance with the Newberry Library and the Modern Language Association, organized a project to edit and published reliable critical texts of Melville's complete works, including unpublished poems, journals, and correspondence. The aim of the editors was to present a text "as close as possible to the author's intention as surviving evidence permits". The volumes have extensive appendices, including textual variants from each of the editions published in Melville's lifetime, an historical note on the publishing history and critical reception, and related documents. In many cases, it was not possible to establish a "definitive text", but the edition supplies all evidence available at the time. Because the texts were prepared with financial support from the United States Department of Education, no royalties are charged, and they have been widely reprinted.

In 1945, The Melville Society was founded, a non-profit organisation dedicated to the study of Melville's life and works. Between 1969 and 2003 it published 125 issues of "Melville Society Extracts," which are now freely available on the society's website. Since 1999 it publishes "Leviathan: A Journal of Melville Studies", currently three issues a year, published by Johns Hopkins University Press.

Melville did not publish poetry until late in life, and his reputation as a poet was not high until late in the 20th century.

Melville, says recent literary critic Lawrence Buell, "is justly said to be nineteenth-century America's leading poet after Whitman and Dickinson, yet his poetry remains largely unread even by many Melvillians." True, Buell concedes, even more than most Victorian poets, Melville turned to poetry as an "instrument of meditation rather than for the sake of melody or linguistic play." It is also true that he turned from fiction to poetry late in life. Yet he wrote twice as much poetry as Dickinson and probably as many lines as Whitman, and he wrote distinguished poetry for a quarter of a century, twice as long as his career publishing prose narratives. The three novels of the 1850s which Melville worked on most seriously to present his philosophical explorations, "Moby-Dick", "Pierre", and "The Confidence Man", seem to make the step to philosophical poetry a natural one rather than simply a consequence of commercial failure.

In 2000, the Melville scholar Elizabeth Renker wrote "a sea change in the reception of the poems is incipient." Some critics now place him as the first modernist poet in the United States; others assert that his work more strongly suggests what today would be a postmodern view. Henry Chapin wrote in an introduction to "John Marr and Other Sailors (1888)", a collection of Melville's late poetry, "Melville's loveable freshness of personality is everywhere in evidence, in the voice of a true poet." The poet and novelist Robert Penn Warren was a leading champion of Melville as a great American poet. Warren issued a selection of Melville's poetry prefaced by an admiring critical essay. The poetry critic Helen Vendler remarked of "Clarel" : "What it cost Melville to write this poem makes us pause, reading it. Alone, it is enough to win him, as a poet, what he called 'the belated funeral flower of fame.'"

Melville's writings did not attract the attention of women's studies scholars of the 1970s and 1980s, though his preference for sea-going tales that involved almost only males has been of interest to scholars in men's studies and especially gay and queer studies. Melville was remarkably open in his exploration of sexuality of all sorts. For example, Alvin Sandberg claimed that the short story "The Paradise of Bachelors and the Tartarus of Maids" offers "an exploration of impotency, a portrayal of a man retreating to an all-male childhood to avoid confrontation with sexual manhood," from which the narrator engages in "congenial" digressions in heterogeneity. In line with this view, Warren Rosenberg argues the homosocial "Paradise of Bachelors" is shown to be "superficial and sterile."

David Harley Serlin observes in the second half of Melville's diptych, "The Tartarus of Maids," the narrator gives voice to the oppressed women he observes:

Issues of sexuality have been observed in other works as well. Rosenberg notes Taji, in "Mardi", and the protagonist in "Pierre" "think they are saving young 'maidens in distress' (Yillah and Isabel) out of the purest of reasons but both are also conscious of a lurking sexual motive". When Taji kills the old priest holding Yillah captive, he says,
In "Pierre," the motive of the protagonist's sacrifice for Isabel is admitted: "womanly beauty and not womanly ugliness invited him to champion the right." Rosenberg argues,
Rosenberg says that Melville fully explores the theme of sexuality in his major epic poem, "Clarel." When the narrator is separated from Ruth, with whom he has fallen in love, he is free to explore other sexual (and religious) possibilities before deciding at the end of the poem to participate in the ritualistic order represented by marriage. In the course of the poem, "he considers every form of sexual orientation – celibacy, homosexuality, hedonism, and heterosexuality – raising the same kinds of questions as when he considers Islam or Democracy."

Some passages and sections of Melville's works demonstrate his willingness to address all forms of sexuality, including the homoerotic, in his works. Commonly noted examples from "Moby-Dick" are the "marriage bed" episode involving Ishmael and Queequeg, which is interpreted as male bonding; and the "Squeeze of the Hand" chapter, describing the camaraderie of sailors' extracting spermaceti from a dead whale. Rosenberg notes that critics say that "Ahab's pursuit of the whale, which they suggest can be associated with the feminine in its shape, mystery, and in its naturalness, represents the ultimate fusion of the epistemological and sexual quest." In addition, he notes that Billy Budd's physical attractiveness is described in quasi-feminine terms: "As the Handsome Sailor, Billy Budd's position aboard the seventy-four was something analogous to that of a rustic beauty transplanted from the provinces and brought into competition with the highborn dames of the court."

Since the late 20th century, "Billy Budd" has become a central text in the field of legal scholarship known as law and literature. In the novel, Billy, a handsome and popular young sailor, is impressed from the merchant vessel "Rights of Man" to serve aboard H.M.S. "Bellipotent" in the late 1790s, during the war between Revolutionary France and Great Britain. He excites the enmity and hatred of the ship's master-at-arms, John Claggart. Claggart brings phony charges against Billy, accusing him of mutiny and other crimes, and the Captain, the Honorable Edward Fairfax Vere, brings them together for an informal inquiry. At this encounter, Billy is frustrated by his stammer, which prevents him from speaking, and strikes Claggart. The blow catches Claggart squarely on the forehead and, after a gasp or two, the master-at-arms dies.

Vere immediately convenes a court-martial, at which, after serving as sole witness and as Billy's "de facto" counsel, Vere urges the court to convict and sentence Billy to death. The trial is recounted in chapter 21, the longest chapter in the book. It has become the focus of scholarly controversy; was Captain Vere a good man trapped by bad law, or did he deliberately distort and misrepresent the applicable law to condemn Billy to death?

As early as 1839, in the juvenile sketch "Fragments from a Writing Desk," Melville explores a problem which would reappear in the short stories "Bartleby" (1853) and "Benito Cereno" (1855): the impossibility to find common ground for mutual communication. The sketch centers on the protagonist and a mute lady, leading scholar Sealts to observe: "Melville's deep concern with expression and communication evidently began early in his career."

According to scholar Nathalia Wright, Melville's characters are all preoccupied by the same intense, superhuman and eternal quest for "the absolute amidst its relative manifestations," an enterprise central to the Melville canon: "All Melville's plots describe this pursuit, and all his themes represent the delicate and shifting relationship between its truth and its illusion." It is not clear, however, what the moral and metaphysical implications of this quest are, because Melville did not distinguish between these two aspects. Throughout his life Melville struggled with and gave shape to the same set of epistemological doubts and the metaphysical issues these doubts engendered. An obsession for the limits of knowledge led to the question of God's existence and nature, the indifference of the universe, and the problem of evil.

In 1985, the New York City Herman Melville Society gathered at 104 East 26th Street to dedicate the intersection of Park Avenue south and 26th Street as Herman Melville Square. This is the street where Melville lived from 1863 to 1891 and where, among other works, he wrote "Billy Budd".

In 2010, a species of extinct giant sperm whale, "Livyatan melvillei", was named in honor of Melville. The paleontologists who discovered the fossil were fans of "Moby-Dick" and dedicated their discovery to the author.

Melville Hall, the Commissioned Officer's Club at the United States Merchant Marine Academy, in Kings Point, New York is named in his honor.





</doc>
<doc id="13624" url="https://en.wikipedia.org/wiki?curid=13624" title="High fidelity">
High fidelity

High fidelity (often shortened to hi-fi or hifi) is a term used by listeners, audiophiles and home audio enthusiasts to refer to high-quality reproduction of sound. This is in contrast to the lower quality sound produced by inexpensive audio equipment, or the inferior quality of sound reproduction that can be heard in recordings made until the late 1940s.

Ideally, high-fidelity equipment has inaudible noise and distortion, and a flat (neutral, uncolored) frequency response within the human hearing range.

Bell Laboratories began experimenting with a range of recording techniques in the early 1930s. Performances by Leopold Stokowski and the Philadelphia Orchestra were recorded in 1931 and 1932 using telephone lines between the Academy of Music in Philadelphia and the Bell labs in New Jersey. Some multitrack recordings were made on optical sound film, which led to new advances used primarily by MGM (as early as 1937) and 20th Century-Fox Film Corporation (as early as 1941). RCA Victor began recording performances by several orchestras using optical sound around 1941, resulting in higher-fidelity masters for 78-rpm discs. During the 1930s, Avery Fisher, an amateur violinist, began experimenting with audio design and acoustics. He wanted to make a radio that would sound like he was listening to a live orchestra—that would achieve high fidelity to the original sound. After World War II, Harry F. Olson conducted an experiment whereby test subjects listened to a live orchestra through a hidden variable acoustic filter. The results proved that listeners preferred high fidelity reproduction, once the noise and distortion introduced by early sound equipment was removed.

Beginning in 1948, several innovations created the conditions that made for major improvements of home-audio quality possible:


In the 1950s, audio manufacturers employed the phrase "high fidelity" as a marketing term to describe records and equipment intended to provide faithful sound reproduction. While some consumers simply interpreted "high fidelity" as fancy and expensive equipment, many found the difference in quality between "hi-fi" and the then standard AM radios and 78 rpm records readily apparent and bought 33⅓ LPs such as RCA's New Orthophonics and London's ffrr (Full Frequency Range Recording, a UK Decca system); and high-fidelity phonographs. Audiophiles paid attention to technical characteristics and bought individual components, such as separate turntables, radio tuners, preamplifiers, power amplifiers and loudspeakers. Some enthusiasts even assembled their own loudspeaker systems. In the 1950s, "hi-fi" became a generic term for home sound equipment, to some extent displacing "phonograph" and "record player".

In the late 1950s and early 1960s, the development of the Westrex single-groove stereophonic record cutterhead led to the next wave of home-audio improvement, and in common parlance "stereo" displaced "hi-fi". Records were now played on "a stereo". In the world of the audiophile, however, the concept of "high fidelity" continued to refer to the goal of highly accurate sound reproduction and to the technological resources available for approaching that goal. This period is regarded as the "Golden Age of Hi-Fi", when vacuum tube equipment manufacturers of the time produced many models considered endearing by modern audiophiles, and just before solid state (transistorized) equipment was introduced to the market, subsequently replacing tube equipment as the mainstream technology.

A popular type of system for reproducing music beginning in the 1970s was the integrated music centre—which combined a phonograph turntable, AM-FM radio tuner, tape player, preamplifier, and power amplifier in one package, often sold with its own separate, detachable or integrated speakers. These systems advertised their simplicity. The consumer did not have to select and assemble individual components, or be familiar with impedance and power ratings. Purists generally avoid referring to these systems as high fidelity, though some are capable of very good quality sound reproduction. Audiophiles in the 1970s and 1980s preferred to buy each component separately. That way, they could choose models of each component with the specifications that they desired. In the 1980s, a number of audiophile magazines became available, offering reviews of components and articles on how to choose and test speakers, amplifiers and other components.

Listening tests are used by hi-fi manufacturers, audiophile magazines and audio engineering researchers and scientists. If a listening test is done in such a way that the listener who is assessing the sound quality of a component or recording can see the components that are being used for the test (e.g., the same musical piece listened to through a tube power amplifier and a solid state amplifier), then it is possible that the listener's pre-existing biases towards or against certain components or brands could affect their judgment. To respond to this issue, researchers began to use blind tests, in which the researchers can see the components being tested, but not the listeners undergoing the experiments. In a double-blind experiment, neither the listeners nor the researchers know who belongs to the control group and the experimental group, or which type of audio component is being used for which listening sample. Only after all the data has been recorded (and in some cases, analyzed) do the researchers learn which components or recordings were preferred by the listeners. A commonly used variant of this test is the ABX test. A subject is presented with two known samples (sample "A", the reference, and sample "B", an alternative), and one unknown sample "X," for three samples total. "X" is randomly selected from "A" and "B", and the subject identifies "X" as being either "A" or "B". Although there is no way to prove that a certain methodology is transparent, a properly conducted double-blind test can prove that a method is "not" transparent.

Scientific double-blind tests are sometimes used as part of attempts to ascertain whether certain audio components (such as expensive, exotic cables) have any subjectively perceivable effect on sound quality. Data gleaned from these double-blind tests is not accepted by some "audiophile" magazines such as "Stereophile" and "The Absolute Sound" in their evaluations of audio equipment. John Atkinson, current editor of "Stereophile", stated (in a 2005 July editorial named "Blind Tests & Bus Stops") that he once purchased a solid-state amplifier, the Quad 405, in 1978 after seeing the results from blind tests, but came to realize months later that "the magic was gone" until he replaced it with a tube amp. Robert Harley of "The Absolute Sound" wrote, in a 2008 editorial (on Issue 183), that: "...blind listening tests fundamentally distort the listening process and are worthless in determining the audibility of a certain phenomenon."

Doug Schneider, editor of the online Soundstage network, refuted this position with two editorials in 2009. He stated: "Blind tests are at the core of the decades' worth of research into loudspeaker design done at Canada's National Research Council (NRC). The NRC researchers knew that for their result to be credible within the scientific community and to have the most meaningful results, they had to eliminate bias, and blind testing was the only way to do so." Many Canadian companies such as Axiom, Energy, Mirage, Paradigm, PSB and Revel use blind testing extensively in designing their loudspeakers. Audio professional Dr. Sean Olive of Harman International shares this view.

Stereophonic sound provided a partial solution to the problem of creating the illusion of live orchestral performers by creating a phantom middle channel when the listener sits exactly in the middle of the two front loudspeakers. When the listener moves slightly to the side, however, this phantom channel disappears or is greatly reduced. An attempt to provide for the reproduction of the reverberation was tried in the 1970s through quadraphonic sound but, again, the technology at that time was insufficient for the task. Consumers did not want to pay the additional costs and space required for the marginal improvements in realism. With the rise in popularity of home theater, however, multi-channel playback systems became affordable, and many consumers were willing to tolerate the six to eight channels required in a home theater. The advances made in signal processors to synthesize an approximation of a good concert hall can now provide a somewhat more realistic illusion of listening in a concert hall.

In addition to spatial realism, the playback of music must be subjectively free from noise, such as hiss or hum, to achieve realism. The compact disc (CD) provides about 90 decibels of dynamic range, which exceeds the 80 dB dynamic range of music as normally perceived in a concert hall. Audio equipment must be able to reproduce frequencies high enough and low enough to be realistic. The human hearing range, for healthy young persons, is 20 Hz to 20,000 Hz.

"Integrated", "mini", or "lifestyle" systems (also known by the older terms "music centre" or "midi system") contain one or more sources such as a CD player, a tuner, or a cassette deck together with a preamplifier and a power amplifier in one box. Although some high-end manufacturers do produce integrated systems, such products are generally disparaged by audiophiles, who prefer to build a system from "separates" (or "components"), often with each item from a different manufacturer specialising in a particular component. This provides the most flexibility for piece-by-piece upgrades and repairs.

For slightly less flexibility in upgrades, a preamplifier and a power amplifier in one box is called an "integrated amplifier"; with a tuner, it is a "receiver". A monophonic power amplifier, which is called a "monoblock", is often used for powering a subwoofer. Other modules in the system may include components like cartridges, tonearms, hi-fi turntables, Digital Media Players, digital audio players, DVD players that play a wide variety of discs including CDs, CD recorders, MiniDisc recorders, hi-fi videocassette recorders (VCRs) and reel-to-reel tape recorders. Signal modification equipment can include equalizers and signal processors.

This modularity allows the enthusiast to spend as little or as much as they want on a component that suits their specific needs. In a system built from separates, sometimes a failure on one component still allows partial use of the rest of the system. A repair of an integrated system, though, means complete lack of use of the system. Another advantage of modularity is the ability to spend money on only a few core components at first and then later add additional components to the system. Some of the disadvantages of this approach are increased cost, complexity, and space required for the components.

In the 2000s, modern hi-fi equipment can include signal sources such as digital audio tape (DAT), digital audio broadcasting (DAB) or HD Radio tuners. Some modern hi-fi equipment can be digitally connected using fibre optic TOSLINK cables, universal serial bus (USB) ports (including one to play digital audio files), or Wi-Fi support. Another modern component is the "music server" consisting of one or more computer hard drives that hold music in the form of computer files. When the music is stored in an audio file format that is lossless such as FLAC, Monkey's Audio or WMA Lossless, the computer playback of recorded audio can serve as an audiophile-quality source for a hi-fi system.




</doc>
<doc id="13625" url="https://en.wikipedia.org/wiki?curid=13625" title="Holden">
Holden

Holden, formally known as General Motors-Holden, is an Australian automobile importer and a former automobile manufacturer with its headquarters in Port Melbourne, Victoria. The company was founded in 1856 as a saddlery manufacturer in South Australia. In 1908 it moved into the automotive field, becoming a subsidiary of the United States-based General Motors (GM) in 1931, when the company was renamed General Motors-Holden's Ltd. It was renamed Holden Ltd in 1998, and General Motors Holden in 2005.

Holden sells the remaining stock of the locally produced range of Commodore vehicles, and imported GM models. Holden has offered badge engineered models in sharing arrangements with Chevrolet, Isuzu, Nissan, Opel, Suzuki, Toyota and Vauxhall Motors. In 2013, the vehicle lineup consisted of models from GM Korea, GM Thailand, GM in the US, and the self-developed Commodore, Caprice, and Ute. Holden also distributed the European Opel brand in Australia in 2012 until its Australian demise in mid-2013.

From 1994 to 2017, all Australian-built Holden vehicles were manufactured in Elizabeth, South Australia, and engines were produced at the Fishermans Bend plant in Melbourne. Historically, production or assembly plants were operated in all mainland states of Australia. The consolidation of car production at Elizabeth was completed in 1988, but some assembly operations continued at Dandenong until 1994.

General Motors assembly plants were operated in New Zealand from 1926 until 1990 by General Motors New Zealand Limited in an earlier and quite separate operation from Holden in Australia. Although Holden's involvement in exports has fluctuated since the 1950s, the declining sales of large cars in Australia led the company to look to international markets to increase profitability. From 2010 Holden incurred losses due to the strong Australian dollar, and reductions of government grants and subsidies. This led to the announcement on 11 December 2013 that Holden would cease vehicle and engine production by the end of 2017. However, the company will continue in Australia importing and selling cars as national sales company. Holden will retain their design centre, but with reduced staffing.

On 20 October 2017, the last existing vehicle plant located in Elizabeth, South Australia was closed. Holden continues solely as an importer of vehicles.

In 1852, James Alexander Holden emigrated to South Australia from Walsall, England and in 1856 established "J.A. Holden & Co", a saddlery business in Adelaide. In 1879 J A Holden’s eldest son Henry James (HJ) Holden, became a partner and effectively managed the company. In 1885, German-born H. A. Frost joined the business as a junior partner and J.A. Holden & Co became "Holden & Frost Ltd". Edward Holden, James' grandson, joined the firm in 1905 with an interest in automobiles. From there, the firm evolved through various partnerships and, in 1908, Holden & Frost moved into the business of minor repairs to car upholstery. The company began re-body older chassis using motor bodies produced by F T Hack and Co from 1914. Holden & Frost mounted the body, painted and trimmed it. The company began to produce complete motorcycle sidecar bodies after 1913. After 1917, wartime trade restrictions led the company to start full-scale production of vehicle body shells. H.J. Holden founded a new company in late 1917, and registered "Holden's Motor Body Builders Ltd" (HMBB) on 25 February 1919 specialising in car bodies and using the former F T Hack & Co facility at 400 King William Street in Adelaide before erecting a large 4 story factory on the site.

By 1923, HMBB were producing 12,000 units per year. During this time, HMBB assembled bodies for Ford Motor Company of Australia until its Geelong plant was completed. From 1924, HMBB became the exclusive supplier of car bodies for GM in Australia, with manufacturing taking place at the new Woodville plant. These bodies were made to suit a number of chassis imported from manufacturers such as Chevrolet and Dodge. In 1926 General Motors (Australia) was established with assembly plants at Newstead, Queensland; Marrickville, New South Wales; City Road, Melbourne; Birkenhead, South Australia; and Cottesloe, Western Australia using bodies produced by Holden Motor Body Builders and imported complete knock down (CKD) chassis. In 1930 alone, the still independent Woodville plant built bodies for Austin, Chrysler, DeSoto, Morris, Hillman, Humber, Hupmobile and Willys-Overland as well GM cars. The last of this line of business was the assembly of Hillman Minx sedans in 1948. The Great Depression led to a substantial downturn in production by Holden, from 34,000 units annually in 1930 to just 1,651 units one year later. In 1931 General Motors purchased Holden Motor Body Builders and merged it with General Motors (Australia) Pty Ltd to form General Motors-Holden's Ltd (GM-H). Throughout the 1920s Holden also supplied tramcars to the Melbourne & Metropolitan Tramways Board, of which several examples have been preserved in both Australia and New Zealand.

Holden's second full-scale car factory, located in Fishermans Bend (Port Melbourne), was completed in 1936, with construction beginning in 1939 on a new plant in Pagewood, New South Wales. However, World War II delayed car production with efforts shifted to the construction of vehicle bodies, field guns, aircraft and engines. Before the war ended, the Australian Government took steps to encourage an Australian automotive industry. Both GM and Ford provided studies to the Australian Government outlining the production of the first Australian-designed car. Ford's proposal was the government's first choice, but required substantial financial assistance. GM's study was ultimately chosen because of its low level of government intervention. After the war, Holden returned to producing vehicle bodies, this time for Buick, Chevrolet, Pontiac and Vauxhall. The Oldsmobile Ace was also produced from 1946 to 1948.

From here, Holden continued to pursue the goal of producing an Australian car. This involved compromise with GM, as Holden's managing director, Laurence Hartnett, favoured development of a local design, while GM preferred to see an American design as the basis for "Australia's Own Car". In the end, the design was based on a previously rejected post-war Chevrolet proposal. The Holden was launched in 1948, creating long waiting lists extending through 1949 and beyond. The name "Holden" was chosen in honour of Sir Edward Holden, the company's first chairman and grandson of J.A. Holden. Other names considered were "GeM", "Austral", "Melba", "Woomerah", "Boomerang", "Emu" and "Canbra", a phonetic spelling of Canberra. Although officially designated "48-215", the car was marketed simply as the "Holden". The unofficial usage of the name "FX" originated within Holden, referring to the updated suspension on the 48-215 of 1953.

During the 1950s, Holden dominated the Australian car market. GM invested heavily in production capacity, which allowed the company to meet increased post-war demand for motor cars. Less expensive four-cylinder cars did not offer Holden's ability to deal with rugged rural areas. 48-215 sedans were produced in parallel with the 50-2106 coupé utility from 1951; the latter was known colloquially as the "ute" and became ubiquitous in Australian rural areas as the workhorse of choice. Production of both the utility and sedan continued with minor changes until 1953, when they were replaced by the facelifted FJ model, introducing a third panel van body style. The FJ was the first major change to the Holden since its 1948 introduction. Over time it gained iconic status and remains one of Australia's most recognisable automotive symbols. A new horizontally slatted grille dominated the front-end of the FJ, which received various other trim and minor mechanical revisions. In 1954 Holden began exporting the FJ to New Zealand. Although little changed from the 48-215, marketing campaigns and price cuts kept FJ sales steady until a completely redesigned model was launched. At the 2005 Australian International Motor Show in Sydney, Holden paid homage to the FJ with the Efijy concept car.

Holden's next model, the FE, launched in 1956; offered in a new station wagon body style dubbed "Station Sedan" in the company's sales literature. In the same year Holden commenced exports to Malaya, Thailand and North Borneo. Strong sales continued in Australia, and Holden achieved a market share of more than 50 percent in 1958 with the revised FC model. This was the first Holden to be tested on the new "Holden Proving Ground" based in Lang Lang, Victoria. 1957 saw Holden's export markets grow to 17 countries, with new additions including Indonesia, Hong Kong, Singapore, Fiji, Sudan, the East Africa region and South Africa. Indonesian market cars were assembled locally by P.T. Udatin. The opening of the Dandenong, Victoria, production facility in 1956 brought further jobs; by 1959 Holden employed 19,000 workers country-wide. In 1959 complete knock down assembly began in South Africa and Indonesia.

In 1960, Holden introduced its third major new model, the FB. The car's style was inspired by 1950s Chevrolets, with tailfins and a wrap-around windshield with "dog leg" A-pillars. By the time it was introduced, many considered the appearance dated. Much of the motoring industry at the time noted that the adopted style did not translate well to the more compact Holden. The FB became the first Holden that was adapted for left-hand-drive markets, enhancing its export potential, and as such was exported to New Caledonia, New Hebrides, the Philippines, and Hawaii.
In 1960, Ford unveiled the new Falcon in Australia, only months after its introduction in the United States. To Holden's advantage, the Falcon was not durable, particularly in the front suspension, making it ill-suited for Australian conditions. In response to the Falcon, Holden introduced the facelifted EK series in 1961; the new model featured two-tone paintwork and optional "Hydramatic" automatic transmission. A restyled EJ series came in 1962, debuting the new luxury oriented Premier model. The EH update came a year later bringing the new "Red" motor, providing better performance than the previous "Grey" motor. The HD series of 1965 saw the introduction of the "Powerglide" automatic transmission. At the same time, an "X2" performance option with a more powerful version of the six-cylinder engine was made available. In 1966, the HR was introduced, including changes in the form of new front and rear styling and higher-capacity engines. More significantly, the HR fitted standard front seat belts; Holden thus became the first Australian automaker to provide the safety device as standard equipment across all models. This coincided with the completion of the production plant in Acacia Ridge, Queensland. By 1963, Holden was exporting cars to Africa, the Middle East, South-East Asia, the Pacific Islands, and the Caribbean.

Holden began assembling the compact HA series Vauxhall Viva in 1964. This was superseded by the Holden Torana in 1967, a development of the Viva ending Vauxhall production in Australia. Holden offered the LC, a Torana with new styling, in 1969 with the availability of Holden's six-cylinder engine. In the development days, the six-cylinder Torana was reserved for motor racing, but research had shown that there was a business case for such a model. The LC Torana was the first application of Holden's new three-speed "Tri-Matic" automatic transmission. This was the result of Holden's A$16.5 million transformation of the Woodville, South Australia factory for its production.
Holden's association with the manufacture of Chevrolets and Pontiacs ended in 1968, coinciding with the year of Holden's next major new model, the HK . This included Holden's first V8 engine, a Chevrolet engine imported from Canada. Models based on the HK series included an extended-length prestige model, the Brougham, and a two-door coupé, the Monaro. The mainstream Holden Special was rebranded the Kingswood, and the basic fleet model, the Standard, became the Belmont. On 3 March 1969 Alexander Rhea, managing director of General Motors-Holden's at the time, was joined by press photographers and the Federal Minister of Shipping and Transport, Ian Sinclair as the two men drove the two millionth Holden, an HK Brougham off the production line. This came just over half a decade since the one millionth car, an EJ Premier sedan rolled off the Dandenong line on 25 October 1962. Following the Chevrolet V8 fitted to the HK, the first Australian-designed and mass-produced V8, the Holden V8 engine debuted in the Hurricane concept of 1969 before fitment to facelifted HT model. This was available in two capacities: and . Late in HT production, use of the new "Tri-Matic" automatic transmission, first seen in the LC Torana was phased in as "Powerglide" stock was exhausted, but Holden's official line was that the HG of 1971 was the first full-size Holden to receive it.
Despite the arrival of serious competitors—namely, the Ford Falcon, Chrysler Valiant, and Japanese cars—in the 1960s, Holden's locally produced large six- and eight-cylinder cars remained Australia's top-selling vehicles. Sales were boosted by exporting the Kingswood sedan, station wagon, and utility body styles to Indonesia, Trinidad and Tobago, Pakistan, the Philippines, and South Africa in complete knock down form.

Holden launched the new HQ series in 1971. At this time, the company was producing all of its passenger cars in Australia, and every model was of Australian design; however, by the end of the decade, Holden was producing cars based on overseas designs. The HQ was thoroughly re-engineered, featuring a perimeter frame and semi-monocoque (unibody) construction. Other firsts included an all-coil suspension and an extended wheelbase for station wagons, while the utilities and panel vans retained the traditional coil/leaf suspension configuration. The series included the new prestige Statesman brand, which also had a longer wheelbase, replacing the Brougham. The Statesman remains noteworthy because it was not marketed as a "Holden", but rather a "Statesman".

The HQ framework led to a new generation of two-door Monaros, and, despite the introduction of the similar sized competitors, the HQ range became the top-selling Holden of all time, with 485,650 units sold in three years. 14,558 units were exported and 72,290 CKD kits were constructed. The HQ series was facelifted in 1974 with the introduction of the HJ, heralding new front panel styling and a revised rear fascia. This new bodywork was to remain, albeit with minor upgrades through the HX and HZ series. Detuned engines adhering to government emission standards were brought in with the HX series, whilst the HZ brought considerably improved road handling and comfort with the introduction of "Radial Tuned Suspension" (RTS). As a result of GM's toying with the Wankel rotary engine, as used by Mazda of Japan, an export agreement was initiated in 1975. This involved Holden exporting with powertrains, HJ, and later, HX series Premiers as the Mazda Roadpacer AP. Mazda then fitted these cars with the "13B" rotary engine and three-speed automatic transmission. Production ended in 1977, after just 840 units sold.

During the 1970s, Holden ran an advertising jingle "Football, Meat Pies, Kangaroos and Holden cars", based on the "Baseball, Hot Dogs, Apple Pies and Chevrolet" jingle used by Chevrolet in the United States. Also, development of the Torana continued in with the larger mid-sized LH series released in 1974, offered only as a four-door sedan. The LH Torana was one of the few cars worldwide engineered to accommodate four-, six-and eight-cylinder engines. This trend continued until Holden introduced the Sunbird in 1976; essentially the four-cylinder Torana with a new name. Designated LX, both the Sunbird and Torana introduced a three-door hatchback variant. A final UC update appeared in 1978. During its production run, the Torana achieved legendary racing success in Australia, achieving victories at the Mount Panorama Circuit in Bathurst, New South Wales.

In 1975, Holden introduced the compact Gemini, the Australian version of the "T-car", based on the Opel Kadett C. The Gemini was an overseas design developed jointly with Isuzu, GM's Japanese affiliate; and was powered by a 1.6-litre four-cylinder engine. Fast becoming a popular car, the Gemini rapidly attained sales leadership in its class, and the nameplate lived on until 1987.

Holden's most popular car to date, the Commodore, was introduced in 1978 as the VB. The new family car was loosely based on the Opel Rekord E body shell, but with the front from the Opel Senator grafted to accommodate the larger Holden six-cylinder and V8 engines. Initially, the Commodore maintained Holden's sales leadership in Australia. However, some of the compromises resulting from the adoption of a design intended for another market hampered the car's acceptance. In particular, it was narrower than its predecessor and its Falcon rival, making it less comfortable for three rear-seat passengers. With the abandonment of left-hand drive markets, Holden exported almost 100,000 Commodores to markets such as New Zealand, Thailand, Hong Kong, Malaysia, Indonesia, Malta and Singapore.

Holden discontinued the Torana in 1979 and the Sunbird in 1980. After the 1978 introduction of the Commodore, the Torana became the "in-between" car, surrounded by the smaller and more economical Gemini and the larger, more sophisticated Commodore. The closest successor to the Torana was the Camira, released in 1982 as Australia's version of GM's medium-sized "J-car".
The 1980s were challenging for Holden and the Australian automotive industry. The Australian Government tried to revive the industry with the Button car plan, which encouraged car makers to focus on producing fewer models at higher, more economical volumes, and to export cars. The decade opened with the shut-down of the Pagewood, New South Wales production plant and introduction of the light commercial Rodeo, sourced from Isuzu in Japan. The Rodeo was available in both two- and four-wheel drive chassis cab models with a choice of petrol and diesel powerplants. The range was updated in 1988 with the TF series, based on the Isuzu TF. Other cars sourced from Isuzu during the 1980s were the four-wheel drive Jackaroo (1981), the Shuttle (1982) van and the Piazza (1986) three-door sports hatchback. The second generation Holden Gemini from 1985 was also based on an Isuzu design, although, its manufacture was undertaken in Australia.

While GM Australia's commercial vehicle range had originally been mostly based on Bedford products, these had gradually been replaced by Isuzu products. This process began in the 1970s and by 1982 Holden's commercial vehicle arm no longer offered any Bedford products.

The new Holden WB commercial vehicles and the Statesman WB limousines were introduced in 1980. However, the designs, based on the HQ and updated HJ, HX and HZ models from the 1970s were less competitive than similar models in Ford's lineup. Thus, Holden abandoned those vehicle classes altogether in 1984. Sales of the Commodore also fell, with the effects of the 1979 energy crisis lessening, and for the first time the Commodore lost ground to the Ford Falcon. Sales in other segments also suffered when competition from Ford intensified, and other Australian manufacturers: Mitsubishi, Nissan and Toyota gained market share. When released in 1982, the Camira initially generated good sales, which later declined because buyers considered the 1.6-litre engine underpowered, and the car's build and ride quality below-average. The Camira lasted just seven years, and contributed to Holden's accumulated losses of over A$500 million by the mid-1980s.

In 1984, Holden introduced the VK Commodore, with significant styling changes from the previous VH. The Commodore was next updated in 1986 as the VL, which had new front and rear styling. Controversially, the VL was powered by the 3.0-litre Nissan "RB30" six-cylinder engine and had a Nissan-built, electronically controlled four-speed automatic transmission. Holden even went to court in 1984 to stop local motoring magazine Wheels from reporting on the matter. The engine change was necessitated by the legal requirement that all new cars sold in Australia after 1986 had to consume unleaded petrol. Because it was unfeasible to convert the existing six-cylinder engine to run on unleaded fuel, the Nissan engine was chosen as the best engine available. However, changing exchange rates doubled the cost of the engine and transmission over the life of the VL. The decision to opt for a Japanese-made transmission led to the closure of the Woodville, South Australia assembly plant. Confident by the apparent sign of turnaround, GM paid off Holden's mounted losses of A$780 million on 19 December 1986. At GM headquarters' request, Holden was then reorganised and recapitalised, separating the engine and car manufacturing divisions in the process. This involved the splitting of Holden into "Holden's Motor Company" (HMC) and "Holden's Engine Company" (HEC). For the most part, car bodies were now manufactured at Elizabeth, South Australia, with engines as before, confined to the Fishermans Bend plant in Port Melbourne, Victoria. The engine manufacturing business was successful, building four-cylinder "Family II" engines for use in cars built overseas. The final phase of the Commodore's recovery strategy involved the 1988 VN, a significantly wider model powered by the American-designed, Australian-assembled 3.8-litre Buick V6 engine.

Holden began to sell the subcompact Suzuki Swift-based Barina in 1985. The Barina was launched concurrently with the Suzuki-sourced Holden Drover, followed by the Scurry later on in 1985. In the previous year, Nissan Pulsar hatchbacks were rebadged as the Holden Astra, as a result of a deal with Nissan. This arrangement ceased in 1989 when Holden entered a new alliance with Toyota, forming a new company: United Australian Automobile Industries (UAAI). UAAI resulted in Holden selling rebadged versions of Toyota's Corolla and Camry, as the Holden Nova and Apollo respectively, with Toyota re-branding the Commodore as the Lexcen.

The company changed throughout the 1990s, increasing its Australian market share from 21 percent in 1991 to 28.2 percent in 1999. Besides manufacturing Australia's best selling car, which was exported in significant numbers, Holden continued to export many locally produced engines to power cars made elsewhere. In this decade, Holden adopted a strategy of importing cars it needed to offer a full range of competitive vehicles. During 1998, General Motors-Holden's Ltd name was shortened to "Holden Ltd".

On 26 April 1990, GM's New Zealand subsidiary Holden New Zealand announced that production at the assembly plant based in Trentham would be phased out and vehicles would be imported duty-free—this came after the 1984 closure of the Petone assembly line due to low output volumes. During the 1990s, Holden, other Australian automakers and trade unions pressured the Australian Government to halt the lowering of car import tariffs. By 1997, the federal government had already cut tariffs to 22.5 percent, from 57.5 percent ten years earlier; by 2000, a plan was formulated to reduce the tariffs to 15 percent. Holden was critical, saying that Australia's population was not large enough, and that the changes could tarnish the local industry.

Holden re-introduced its defunct Statesman title in 1990—this time under the Holden marque, as the Statesman and Caprice. For 1991, Holden updated the Statesman and Caprice with a range of improvements, including the introduction of four-wheel anti-lock brakes (ABS); although, a rear-wheel system had been standard on the Statesman Caprice from March 1976. ABS was added to the short-wheelbase Commodore range in 1992. Another returning variant was the full-size utility, and on this occasion it was based on the Commodore. The VN Commodore received a major facelift in 1993 with the VR—compared to the VN, approximately 80 percent of the car model was new. Exterior changes resulted in a smoother overall body and a "twin-kidney" grille—a Commodore styling trait that remained until the 2002 VY model and, as of 2013, remains a permanent staple on HSV variants.

Holden introduced the all-new VT Commodore in 1997, the outcome of a A$600 million development programme that spanned more than five years. The new model featured a rounded exterior body shell, improved dynamics and many firsts for an Australian-built car. Also, a stronger body structure increased crash safety. The locally produced Buick-sourced V6 engine powered the Commodore range, as did the 5.0-litre Holden V8 engine, and was replaced in 1999 by the 5.7-litre "LS" unit.

The UAAI badge-engineered cars first introduced in 1989 sold in far fewer numbers than anticipated, but the Holden Commodore, Toyota Camry, and Corolla were all successful when sold under their original nameplates. The first generation Nova and the donor Corolla were produced at Holden's Dandenong, Victoria facility until 1994. UAAI was dissolved in 1996, and Holden returned to selling only GM products. The Holden Astra and Vectra, both designed by Opel in Germany, replaced the Toyota-sourced Holden Nova and Apollo. This came after the 1994 introduction of the Opel Corsa replacing the already available Suzuki Swift as the source for the Holden Barina. Sales of the full-size Holden Suburban SUV sourced from Chevrolet commenced in 1998—lasting until 2001. Also in 1998, local assembly of the Vectra began at Elizabeth, South Australia. These cars were exported to Japan and Southeast Asia with Opel badges. However, the Vectra did not achieve sufficient sales in Australia to justify local assembly, and reverted to being fully imported in 2000.

Holden's market surge from the 1990s reversed in the 2000s decade. In Australia, Holden's market share dropped from 27.5 percent in 2000 to 15.2 percent in 2006. From March 2003, Holden no longer held the number one sales position in Australia, losing ground to Toyota.

This overall downturn affected Holden's profits; the company recorded a combined gain of A$842.9 million from 2002 to 2004, and a combined loss of A$290 million from 2005 to 2006. Factors contributing to the loss included the development of an all-new model, the strong Australian dollar and the cost of reducing the workforce at the Elizabeth plant, including the loss of 1,400 jobs after the closure of the third-shift assembly line in 2005, after two years in operation. Holden fared better in 2007, posting an A$6 million loss. This was followed by an A$70.2 million loss in the 2008, an A$210.6 million loss in 2009, and a profit of A$112 million in 2010. On 18 May 2005, "Holden Ltd" became "GM Holden Ltd", coinciding with the resettling to the new Holden headquarters on 191 Salmon Street, Port Melbourne, Victoria.

Holden caused controversy in 2005 with their Holden Employee Pricing television advertisement, which ran from October to December 2005. The campaign publicised, "for the first time ever, all Australians can enjoy the financial benefit of Holden Employee Pricing". However, this did not include a discounted dealer delivery fee and savings on factory fitted options and accessories that employees received. At the same time, employees were given a further discount of 25 to 29 percent on selected models.

Holden revived the Monaro coupe in 2001. Based on the VT Commodore architecture, the coupe attracted worldwide attention after being shown as a concept car at Australian auto shows. The VT Commodore received its first major update in 2002 with the VY series. A mildly facelifted VZ model launched in 2004, introducing the "High Feature" engine. This was built at the Fishermans Bend facility completed in 2003, with a maximum output of 900 engines per day. This has reportedly added A$5.2 billion to the Australian economy; exports account for about A$450 million alone. After the VZ, the "High Feature" engine powered the all-new Holden Commodore (VE). In contrast to previous models, the VE no longer used an Opel-sourced platform adapted both mechanically and in size, but was based on the GM Zeta platform that was earmarked to become a "Global RWD Architecture", until plans were cancelled due to the 2007/08 global financial crisis.

Throughout the 1990s, Opel had also been the source of many Holden models. To increase profitability, Holden looked to the South Korean Daewoo brand for replacements after acquiring a 44.6 percent stake—worth US$251 million—in the company in 2002 as a representative of GM. This was increased to 50.9 percent in 2005, but when GM further increased its stake to 70.1 percent around the time of its 2009 Chapter 11 reorganisation, Holden's interest was relinquished and transferred to another (undisclosed) part of GM.

The commencement of the Holden-branded Daewoo models began with the 2005 Holden Barina, which based on the Daewoo Kalos, replaced the Opel Corsa as the source of the Barina. In the same year, the Viva, based on the Daewoo Lacetti, replaced the entry-level Holden Astra Classic, although the new-generation Astra introduced in 2004 continued on. The Captiva crossover SUV came next in 2006. After discontinuing the Frontera and Jackaroo models in 2003, Holden was only left with one all-wheel drive model: the Adventra, a Commodore-based station wagon. The fourth model to be replaced with a South Korean alternative was the Vectra by the mid-size Epica in 2007. As a result of the split between GM and Isuzu, Holden lost the rights to use the "Rodeo" nameplate. Consequently, the Holden Rodeo was facelifted and relaunched as the Colorado in 2008. Following Holden's successful application for a A$149 million government grant to build a localised version of the Chevrolet Cruze in Australia from 2011, Holden in 2009 announced that it would initially import the small car unchanged from South Korea as the Holden Cruze.

Following the government grant announcement, Kevin Rudd, Australia's Prime Minister at the time, stated that production would support 600 new jobs at the Elizabeth facility; however, this failed to take into account Holden's previous announcement, whereby 600 jobs would be shed when production of the "Family II" engine ceased in late 2009. In mid-2013, Holden sought a further A$265 million, in addition to the A$275 million that was already committed by the governments of Canberra, South Australia and Victoria, to remain viable as a car manufacturer in Australia. A source close to Holden informed the "Australian" news publication that the car company is losing money on every vehicle that it produces and consequently initiated negotiations to reduce employee wages by up to A$200 per week to cut costs, following the announcement of 400 job cuts and an assembly line reduction of 65 (400 to 335) cars per day. From 2001 to 2012, Holden received over A$150 million a year in subsidy from Australian government. The subsidy from 2007 was more than Holden's capital investment of the same period. From 2004, Holden was only able to make a profit in 2010 and 2011.

In March 2012, Holden was given a $270 million lifeline by the Gillard Federal Government, Weatherill and Baillieu ministries. In return, Holden planned to inject over $1 billion into car manufacturing in Australia. They estimated the new investment package would return around $4 billion to the Australian economy and see GM Holden continue making cars in Australia until at least 2022.

Industry Minister Kim Carr confirmed on 10 July 2013 that talks had been scheduled between the Australian government and Holden. On 13 August 2013, 1,700 employees at the Elizabeth plant in northern Adelaide voted to accept a three-year wage freeze in order to decrease the chances of the production line's closure in 2016. Holden's ultimate survival, though, depended on continued negotiations with the Federal Government—to secure funding for the period from 2016 to 2022—and the final decision of the global headquarters in Detroit, US.

Following an unsuccessful attempt to secure the extra funding required from the new Liberal/National coalition government, on 10 December 2013, General Motors announced that Holden would cease engine and vehicle manufacturing operations in Australia by the end of 2017. As a result, 2,900 jobs would be lost over four years. Beyond 2017 Holden's Australian presence will consist of: a national sales company, a parts distribution centre and a global design studio.

In May 2014 GM reversed their decision to abandon the Lang Lang Proving Ground and decided to keep it as part of their engineering capability in Australia.

In 2015, Holden again began selling a range of Opel-derived cars comprising the Astra VXR and Insignia VXR (both based on the OPC models sold by Vauxhall) and Cascada. Later that year, Holden also announced plans to sell the European Astra and the Korean Cruze alongside each other from 2017.

In December 2015, Belgian entrepreneur Guido Dumarey commenced negotiations to buy the Commodore manufacturing plant in South Australia, with a view to continue producing a rebadged Zeta-based premium range of rear and all-wheel drive vehicles for local and export sales. The proposal was met with doubt in South Australia, and it later came to nothing. On 20 October 2017 it ceased manufacturing vehicles in Australia.

On 8 May 2015 Jeff Rolfs, Holden's CFO, became interim chairman and managing director. Holden announced on 6 February 2015 that Mark Bernhard would return to Holden as chairman and managing director, the first Australian to hold the post in 25 years. In 2010 vehicles were sold countrywide through the Holden Dealer Network (310 authorised stores and 12 service centres), which employed more than 13,500 people.

In 1987, Holden Special Vehicles (HSV) was formed in partnership with Tom Walkinshaw, who primarily manufactured modified, high-performance Commodore variants. To further reinforce the brand, HSV introduced the HSV Dealer Team into the V8 Supercar fold in 2005 under the naming rights of Toll HSV Dealer Team.

The logo, or "Holden lion and stone" as it is known, has played a vital role in establishing Holden's identity. In 1928, Holden's Motor Body Builders appointed Rayner Hoff to design the emblem. The logo refers to a prehistoric fable, in which observations of lions rolling stones led to the invention of the wheel. With the 1948 launch of the 48-215, Holden revised its logo and commissioned another redesign in 1972 to better represent the company. The emblem was reworked once more in 1994.

Holden began to export vehicles in 1954, sending the FJ to New Zealand. Exports to New Zealand continued, but to broaden their export potential, Holden began to cater their Commodore, Monaro and Statesman/Caprice models for both right- and left-hand drive markets. The Middle East was Holden's largest export market, with the Commodore sold as the Chevrolet Lumina from 1998, and the Statesman from 1999 as the Chevrolet Caprice. Commodores were also sold as the Chevrolet Lumina in Brunei, Fiji and South Africa, and as the Chevrolet Omega in Brazil. Pontiac in North America also imported Commodore sedans from 2008 through to 2009 as the G8. The G8's cessation was a consequence of GM's Chapter 11 bankruptcy resulting in the demise of the Pontiac brand.

Sales of the Monaro began in 2003 to the Middle East as the Chevrolet Lumina Coupe. Later on that year, a modified version of the Monaro began selling in the United States (but not Canada) as the Pontiac GTO, and under the Monaro name through Vauxhall dealerships in the United Kingdom. This arrangement continued through to 2005 when the car was discontinued. The long-wheelbase Statesman sales in the Chinese market as the Buick Royaum began in 2005, before being replaced in 2007 by the Statesman-based Buick Park Avenue. Statesman/Caprice exports to South Korea also began in 2005. These Korean models were sold as the Daewoo Statesman, and later as the Daewoo Veritas from 2008. Holden's move into international markets was profitable; export revenue increased from A$973 million in 1999 to just under $1.3 billion in 2006.

From 2011 the WM Caprice was exported to North America as the Chevrolet Caprice PPV, a version of the Caprice built exclusively for law enforcement in North America sold only to police. From 2007, the HSV-based Commodore was exported to the United Kingdom as the Vauxhall VXR8.

In 2013, it was announced that exports of the Commodore would resume to North America in the form of the VF Commodore as the Chevrolet SS sedan for the 2014 model year. The Chevrolet SS Sedan was imported to the United States (but again, not to Canada) again for 2015 with only minor changes, notably the addition of Magnetic Ride Control suspension and a Tremec TR-6060 manual transmission. For the 2016 model year, the SS sedan received a facelift based on the VF Series II Commodore unveiled in September 2015. In 2017, production of Holden's last two American exports, the SS and the Caprice PPV was discontinued.


Holden has been involved with factory backed teams in Australian touring car racing since 1968. The main factory-backed teams have been the Holden Dealer Team (1969–1987) and the Holden Racing Team (1990–2016). Since 2017, Triple Eight Race Engineering has been Holden's factory team. Holden has won the Bathurst 1000 32 times, more than any other manufacturer, and has won the Australian Touring Car and Supercars Championship title 20 times. Brad Jones Racing, Charlie Schwerkolt Racing, Erebus Motorsport, Tekno Autosports and Walkinshaw Andretti United also run Holden Commodores in the series.





</doc>
<doc id="13627" url="https://en.wikipedia.org/wiki?curid=13627" title="Hank Greenberg">
Hank Greenberg

Henry Benjamin Greenberg (born Hyman Greenberg; January 1, 1911 – September 4, 1986), nicknamed "Hammerin' Hank", "Hankus Pankus", or "The Hebrew Hammer", was an American professional baseball player and team executive. He played in Major League Baseball (MLB), primarily for the Detroit Tigers as a first baseman in the 1930s and 1940s. A member of the Baseball Hall of Fame and a two-time MVP winner, he was one of the premier power hitters of his generation and is widely considered as one of the greatest sluggers in baseball history. He had 47 months of military service including service in World War II, all of which took place during his major league career.

Greenberg played the first twelve of his thirteen seasons in the major leagues on the Detroit team. He was an American League (AL) All-Star for four seasons and an AL Most Valuable Player in 1935 (first baseman) and 1940 (left fielder). He had a batting average over .300 in eight seasons, and he was a member of four Tigers World Series teams which won two championships (1935 and 1945). He was the AL home run leader four times and his 58 home runs for the Tigers in 1938 equaled Jimmie Foxx's 1932 mark for the most in one season by anyone but Babe Ruth, and tied Foxx for the most home runs between Ruth's record 60 in 1927 and Roger Maris' record 61 in 1961. Greenberg was the first major league player to hit 25 or more home runs in a season in each league, and remains the AL record-holder for most RBIs in a single season by a right-handed batter (183 in 1937, a 154-game schedule).

Greenberg was the first Jewish superstar in American team sports. He attracted national attention in 1934 when he refused to play on Yom Kippur, the holiest holiday in Judaism, even though he was not particularly observant religiously and the Tigers were in the middle of a pennant race. In 1947, Greenberg signed a contract with a $30,000 raise to a record $85,000 before being sold to the Pittsburgh Pirates where he played his final MLB season that year. He was one of the few opposing players to publicly welcome Jackie Robinson that year to the major leagues.

Hank Greenberg was born Hyman Greenberg on January 1, 1911, in Greenwich Village, New York City, to Romanian-born Jewish immigrant parents David and Sarah Greenberg, who owned a successful cloth-shrinking plant in New York. He had two brothers, Ben, four years older, and Joe, five years younger, who also played baseball, and a sister, Lillian, two years older. His family moved to the Bronx when he was about seven. 

He attended James Monroe High School in the Bronx, where he was an outstanding all-around athlete and was bestowed with the long-standing nickname of "Bruggy" by his basketball coach. His preferred sport was baseball, and his preferred position was first base. In high school basketball, he was on the Monroe team that won the city championship.

In 1929, the 18-year-old 6-foot-4-inch Greenberg was recruited by the New York Yankees, who already had Lou Gehrig at first base. Greenberg turned them down and instead attended New York University for a year, where he was a member of Sigma Alpha Mu, after which he signed with the Detroit Tigers for $9,000 ($ today).

Greenberg played minor league baseball for three years. Greenberg played 17 games in 1930 for the Hartford Senators, then played at Raleigh, North Carolina, for the Raleigh Capitals, where he hit .314 with 19 home runs. In 1931, he played at Evansville for the Evansville Hubs in the Illinois–Indiana–Iowa League (.318, 15 homers, 85 RBIs). In 1932, at Beaumont for the Beaumont Exporters in the Texas League, he hit 39 homers with 131 RBIs, won the MVP award, and led Beaumont to the Texas League title.

When he broke into the major leagues in 1930, Greenberg was the youngest MLB player (19).

In 1933, he rejoined the Tigers and hit .301 while driving in 87 runs. At the same time, he was third in the league in strikeouts (78).
In 1934, his second major-league season, he hit .339 and helped the Tigers reach their first World Series in 25 years. He led the league in doubles, with 63 (the fourth-highest all-time in a single season), and extra base hits (96). He was third in the AL in slugging percentage (.600) – behind Jimmie Foxx and Lou Gehrig, but ahead of Babe Ruth, and in RBIs (139), sixth in batting average (.339), seventh in home runs (26), and ninth in on-base percentage (.404).

Late in the 1934 season, he announced that he would not play on September 10, which was Rosh Hashanah, the Jewish New Year, or on September 19, the Day of Atonement, Yom Kippur. Fans grumbled, "Rosh Hashanah comes every year but the Tigers haven't won the pennant since 1909." Greenberg did considerable soul-searching, and discussed the matter with his rabbi; finally he relented and agreed to play on Rosh Hashanah, but stuck with his decision not to play on Yom Kippur. Dramatically, Greenberg hit two home runs in a 2–1 Tigers victory over Boston on Rosh Hashanah. The next day's "Detroit Free Press" ran the Hebrew lettering for "Happy New Year" across its front page. Columnist and poet Edgar A. Guest expressed the general opinion in a poem titled "Speaking of Greenberg", in which he used the Irish (and thus Catholic) names Murphy and Mulroney. The poem ends with the lines ""We shall miss him on the infield and shall miss him at the bat / But he's true to his religion—and I honor him for that."" The complete text of the poem is at the end of Greenberg's biography page at the website of the International Jewish Sports Hall of Fame. The Detroit press was not so kind regarding the Yom Kippur decision, nor were many fans, but Greenberg in his autobiography recalled that he received a standing ovation from congregants at the Shaarey Zedek synagogue when he arrived. Absent Greenberg, the Tigers lost to the New York Yankees, 5–2. The Tigers went on to face the St. Louis Cardinals in the 1934 World Series.

In 1935 Greenberg led the league in RBIs (170), total bases (389), and extra base hits (98), tied Foxx for the AL title in home runs (36), was 2nd in the league in doubles (46), slugging percentage (.628), was 3rd in the league in triples (16), and in runs scored (121), 6th in on-base percentage (.411) and walks (87), and was 7th in batting average (.328). He was unanimously voted the American League's Most Valuable Player. At the All-Star break that season, Greenberg hit 25 home runs and set an MLB record (still standing) of 103 RBIs – but was not selected to the AL All-Star roster (both managers put themselves on the rosters but did not play). He helped lead the Tigers to their first World Series title, but sprained his wrist in the second game and did not play in the other 4 games.

In 1936, Greenberg reinjured his wrist in a collision with Jake Powell of the Washington Senators in April and did not play the remainder of the season. He finished the season with 16 hits, 1 home run, and 15 RBIs in 12 games.

In 1937, Greenberg recovered from his injury and was voted to the AL All-Star roster, but did not play. On September 19, 1937, he hit the first home run into the center field bleachers at Yankee Stadium. He led the AL by driving in 183 runs (third all-time, behind Hack Wilson in 1930 and Lou Gehrig in 1931), and in extra base hits (103), while batting .337 with 200 hits. He was second in the league in home runs (40), doubles (49), total bases (397), slugging percentage (.668), and walks (102), third in on-base percentage (.436), and seventh in batting average (.337). Greenberg came in third in the vote for MVP.

A prodigious home run hitter, Greenberg narrowly missed breaking Babe Ruth's single-season home run record in 1938, when he hit 58 home runs, leading the league for the second time. That year, he had 11 games with multiple home runs, a new major league record. Sammy Sosa tied the record in 1998. Greenberg matched what was then the single-season home run record by a right-handed batter, (Jimmie Foxx, 1932); the mark stood for 66 years until it was broken by Sammy Sosa and Mark McGwire. Greenberg also had a 59th home run washed away in a rainout. It has been long speculated that Greenberg was intentionally walked late in the season to prevent him from breaking Ruth's record, but Greenberg dismissed this speculation, calling it "crazy stories." Nonetheless, Howard Megdal has calculated that in September 1938, Greenberg was walked in over 20% of his plate appearances, the highest percentage in his career by far. Megdal's article cited this walk percentage statistic as evidence of AL teams not wanting Greenberg to break Ruth's record due to anti-Semitism. However, an examination of the box scores indicate this spike in walks was due to a few games against St. Louis Browns' pitchers with horrific control, not a general league tendency.

Greenberg was again voted to the AL All-Star roster in 1938, but because he was not named to the 1935 AL All-Star roster and was benched in the 1937 game, he declined to accept a starting position on the 1938 AL team and did not play (the NL won 4-1). He led the league in runs scored (144) and at-bats per home run (9.6), tied for the AL lead in walks (119), was second in RBIs (146), slugging percentage (.683), and total bases (380), and third in OBP (.438) and set a still-standing major league record of 39 homers in his home park, the newly reconfigured Briggs Stadium. He also set a major-league record with 11 multiple-home run games. He came in third in the vote for MVP.

In 1939 Greenberg was voted to the AL All-Star roster for the third year in a row and was a starter at first base, and singled and walked in 4 at-bats (AL won 3-1). He finished second in the AL in home runs (33) and strikeouts (95), third in doubles (42) and slugging percentage (.622), fourth in RBIs (112), sixth in walks (91), and ninth in on-base percentage (.420).

After the 1939 season ended, Greenberg was asked by general manager Jack Zeller to take a salary cut of $5,000 ($ today) as a result of his off year in power and run production. He was asked to move from first base to the outfield to accommodate Rudy York, who was one of the best young hitters of his generation; York was tried at catcher, third baseman, and outfielder and proved to be a defensive liability at each position. Greenberg in turn, demanded a $10,000 dollar bonus if he mastered the outfield, stating "he" was the one taking the risk in learning a new position. Greenberg received his bonus at the end of spring training.

In 1940, Greenberg switched from playing the first base position to the left field position. For the 4th consecutive time, he was voted by the season's AL All-Star team manager to the AL All-Star team. In the bottom of the 6th inning, Greenberg and Lou Finney were sent into the game to replace right fielder Charlie Keller and left fielder Ted Williams with Greenberg playing in left field and Finney in right field. Greenberg batted twice in the game and fouled out to the catcher two-times. The NL won the game 4-0. That season, he led the AL in home runs for the third time in 6 years with 41; in RBIs (150), doubles (50), total bases (384), extra base hits (99), at-bats per home run (14.0), and slugging percentage (.670; 44 points ahead of Joe DiMaggio). He was second in the league behind Williams in runs scored (129) and OBP (.433), all while batting .340 (fifth best in the AL). He also led the Tigers to the AL pennant, and won his second American League MVP award, becoming the first player in major-league history to win an MVP award at two different playing positions.

On October 16, 1940, Greenberg became the first American League player to register for the nation's first peacetime draft. In the spring of 1941, the Detroit draft board initially classified Greenberg as 4F for "flat feet" after his first physical for military service and was recommended for light duty. The rumors that he had bribed the board, and concern that he would be likened to Jack Dempsey who had received negative publicity for failure to serve in World War I, led Greenberg to request to be reexamined. On April 18, he was found fit for regular military service and was reclassified.
On May 7, 1941, he was inducted into the U.S. Army after playing left field in 19 games and reported to Fort Custer at Battle Creek, Michigan. His salary was cut from $55,000 ($ today) a year to $21 ($ today) a month. He was not bitter, and stated, "I made up my mind to go when I was called. My country comes first." In November, while serving as an anti-tank gunner, he was promoted to sergeant, but was honorably discharged on December 5 (the United States Congress released men aged 28 years and older from service), two days before Japan bombed Pearl Harbor.

Greenberg re-enlisted as a sergeant on February 1, 1942, and volunteered for service in the Army Air Forces, becoming the first major league player to do so. He graduated from Officer Candidate School and was commissioned as a first lieutenant in the Air Corps and was assigned to the Physical Education Program. In February 1944, he was sent to the U.S. Army Special Services school. Promoted to captain, he requested overseas duty later that year and served in the China-Burma-India Theater for over six months, scouting locations for B-29 bomber bases and was a physical training officer with the 58th Bomber Wing. He was a Special Services officer of the 20th Bomber Command, 20th Air Force in China when it began bombing Japan on June 15. He was ordered to New York, and in late 1944, to Richmond, Virginia. Greenberg served 47 months, the longest of any major league player.

Greenberg remained in military uniform until he was placed on the military inactive list and discharged from the U.S. Army on June 14, 1945. He was the first major league player to return to MLB after the war. He returned to the Tigers team, and in his first game back on July 1, he homered. The All-Star Game scheduled for July 10 had been officially cancelled on April 24 and MLB did not name All-Stars that season due to strict travel restrictions during the last days of the war with Germany and Japan and the ending of World War II. In place of the All-Star Game, seven interleague games were played (eight had been scheduled) on July 9 and 10 to benefit the American Red Cross and the War Relief fund. An Associated Press All-Star roster was named (no game was played) for the AL and NL by a group of their sportswriters that included Greenberg as one of the All-Stars.

Greenberg, who played left field in 72 games and batted .311 in 1945, helped lead the Tigers to a come-from-behind American League pennant, clinching it with a grand slam home run in the dark—there were no lights in Sportsman's Park in St. Louis—ninth inning of the final game of the season. The ump—former Yankee pitching star of the 1920s Murderers Row team George Pipgras—supposedly said, "Sorry Hank, but I'm gonna have to call the game. I can't see the ball." Greenberg replied, "Don't worry, George, I can see it just fine", so the game continued. It ended with Greenberg's grand slam on the next pitch, clinching Hal Newhouser's 25th victory of the season. His home run allowed the Tigers to clinch the pennant and avoid a one-game playoff (that would have been necessary without the win) against the now-second-place Washington Senators. The Tigers went on to beat the Cubs in the World Series in seven games. Only three home runs were hit in that World Series. Phil Cavarretta hit a home run for the Cubs in Game One, Greenberg hit a homer in Game Two, where he batted in three runs in a 4–1 Tigers win, and he hit a two-run homer in Game Six in the eighth inning that tied the score 8–8; the Cubs went on to win that game with a run in the bottom of the 12th.

In 1946, he returned to peak form and playing at first base. He led the AL in home runs (44) and RBIs (127), both for the fourth time. He was second in slugging percentage (.604) and total bases (316) behind Ted Williams.

In 1947, Greenberg and the Tigers had a lengthy salary dispute. When Greenberg decided to retire rather than play for less, Detroit sold his contract to the Pittsburgh Pirates. To persuade him not to retire, Pittsburgh made Greenberg the first baseball player to earn over $80,000 ($ today) in a season as pure salary (though the exact amount is a matter of some dispute). Team co-owner Bing Crosby recorded a song, "Goodbye, Mr. Ball, Goodbye" with Groucho Marx and Greenberg to celebrate Greenberg's arrival. The Pirates also reduced the size of Forbes Field's cavernous left field, renaming the section "Greenberg Gardens" to accommodate Greenberg's pull-hitting style. Greenberg played first base for the Pirates in 1947 and was one of the few opposing players to publicly welcome Jackie Robinson to the majors.
That year he also had a chance to mentor a young future Hall-of-Famer, the 24-year-old Ralph Kiner. Said Greenberg, "Ralph had a natural home run swing. All he needed was somebody to teach him the value of hard work and self-discipline. Early in the morning on off-days, every chance we got, we worked on hitting." Kiner would go on to hit 51 home runs that year to lead the National League.

In his final season of 1947, Greenberg tied for the league lead in walks with 104, with a .408 on-base percentage and finished eighth in the league in home runs and tenth in slugging percentage. Greenberg became the first major league player to hit 25 or more home runs in a season in each league. Johnny Mize became the second in 1950. 

Nevertheless, Greenberg retired as a player to take a front-office post with the Cleveland Indians. No player had ever retired after a final season in which they hit so many home runs. Since then, only Ted Williams (1960, 29), Dave Kingman (1986; 35), Mark McGwire (2001; 29), Barry Bonds (2007; 28) and David Ortiz (2016; 38) have hit as many or more homers in their final season.

Through 2010, he was first in career home runs and RBIs (ahead of Shawn Green) and batting average (ahead of Ryan Braun), and fourth in hits (behind Lou Boudreau), among all-time Jewish major league baseball players.

As a fielder, the 193-cm (6-foot-4-inch) Greenberg was awkward and unsure of himself early in his career, but mastered first base through countless hours of practice. Over the course of his career he demonstrated a higher-than-average fielding percentage and range at first base. When asked to move to left field in 1940 to make room for Rudy York, he worked tirelessly to conquer that position as well, reducing his errors in the outfield from 15 in 1940 to 0 in 1945.

Greenberg felt that runs batted in were more important than home runs. He would tell his teammates, "just get on base", or "just get the runner to third", and he would do the rest.

Greenberg would likely have approached 500 home runs and 1,800 RBIs had he not served in the military. As it was, he compiled 331 home runs and 1,276 RBI in a 1,394-game career. Greenberg also hit for average, earning a lifetime batting average of .313. Starring as a first baseman and outfielder with the Tigers (1930, 1933–46) and doing duty only briefly with the Pirates (1947), Greenberg played only nine full seasons. He missed all but 19 games of the 1941 season, the three full seasons that followed, and most of 1945 to World War II military service and missed most of another season with a broken wrist.

After the 1947 season, Greenberg retired from the field to become the Cleveland Indians' farm system director and two years later, their General Manager and part-owner along with Bill Veeck. During his tenure, he sponsored more African American players than any other major league executive. Greenberg's contributions to the Cleveland farm system led to the team's successes throughout the 1950s, although Bill James once wrote that the Indians' late 1950s collapse should also be attributed to him. In 1949, Larry Doby also recommended Greenberg scout three players Doby used to play with in the Negro leagues: Hank Aaron, Ernie Banks, and Willie Mays. The next offseason Doby asked what Indians' scouts said about his recommendations. Said Greenberg, "Our guys checked 'em out and their reports were not good. They said that Aaron has a hitch in his swing and will never hit good pitching. Banks is too slow and didn't have enough range [at shortstop], and Mays can't hit a curveball." When Veeck sold his interest, Greenberg remained as general manager and part-owner until 1957. He was the mastermind behind a potential move of the club to Minneapolis that was vetoed by the rest of ownership at the last minute. Greenberg was furious and sold his share soon afterwards.

In 1959, Greenberg and Veeck teamed up for a second time when their syndicate purchased the Chicago White Sox; Veeck served as team president with Greenberg as vice president and general manager. During Veeck and Greenberg's first season, the White Sox won their first AL pennant since 1919. Veeck would sell his shares in the White Sox in 1961, and Greenberg stepped down as general manager on August 26 of that season.

After the 1960 season, the American League announced plans to put a team in Los Angeles. Greenberg immediately became the favorite to become the new team's first owner and persuaded Veeck to join him as his partner. However, when Dodgers owner Walter O'Malley got wind of these developments, he threatened to scuttle the whole deal by invoking his exclusive rights to operate a major league team in southern California. In truth, O'Malley wanted no part of competing against an expansion team owned by a master promoter such as Veeck, even if he was only a minority partner. Greenberg wouldn't budge and pulled out of the running for what became the Los Angeles Angels (now the Los Angeles Angels of Anaheim). Greenberg later became a successful investment banker, briefly returning to baseball as a minority partner with Veeck when the latter repurchased the White Sox in 1975.

Greenberg married Caral Gimbel (daughter of Bernard Gimbel of the Gimbel's New York department store family) on February 18, 1946, three days after signing a $60,000 ($ today) contract with the Tigers. The couple had three children—sons Glenn H. Greenberg and Stephen and a daughter, Alva—before divorcing in 1958. Their son, Stephen, played five years in the Washington Senators/Texas Rangers organization. In 1995, Stephen Greenberg co-founded Classic Sports Network with Brian Bedol, which was purchased by ESPN and became ESPN Classic. He also was the chairman of CSTV, the first cable network devoted exclusively to college sports.

Hank's grandson Spencer Greenberg is a machine learning scientist and Wall Street entrepreneur. In 1966, Greenberg married Mary Jo Tarola, a minor actress who appeared on-screen as Linda Douglas, and remained with her until his death. They had no children.



Incidents of anti-Semitism Greenberg faced included having players stare at him and having racial slurs thrown at him by spectators and sometimes opposing players. Examples of these imprecations were: "Hey Mo!" (referring to the Jewish prophet Moses) and "Throw a pork chop—he can't hit that!" (a reference to Judaic kosher laws). In the 1935 World Series umpire George Moriarty warned some Chicago Cubs players to stop yelling anti-Semitic slurs at Greenberg and eventually cleared the players from the Cubs bench. Moriarty was disciplined for this action by then-commissioner Kenesaw Mountain Landis.

Greenberg befriended Jackie Robinson after he signed with the Dodgers in 1947, and encouraged him; Robinson credited Greenberg with helping him through the difficulties of his rookie year.

Greenberg died of metastatic kidney cancer in Beverly Hills, California, in 1986, and his remains were entombed at Hillside Memorial Park Cemetery, in Culver City, California.

In an article in 1976 in "Esquire" magazine, sportswriter Harry Stein published an "All Time All-Star Argument Starter", consisting of five ethnic baseball teams. Greenberg was the first baseman on Stein's Jewish team.

In 2006, Greenberg was featured on a United States postage stamp. The stamp is one of a block of four honoring "baseball sluggers", the others being Mickey Mantle, Mel Ott, and Roy Campanella.







</doc>
<doc id="13628" url="https://en.wikipedia.org/wiki?curid=13628" title="Heinrich Schliemann">
Heinrich Schliemann

Heinrich Schliemann (; 6 January 1822 – 26 December 1890) was a German businessman and a pioneer in the field of archaeology. He was an advocate of the historicity of places mentioned in the works of Homer and an archaeological excavator of Hissarlik, now presumed to be the site of Troy, along with the Mycenaean sites Mycenae and Tiryns. His work lent weight to the idea that Homer's "Iliad" reflects historical events. Schliemann's excavation of nine levels of archaeological remains with dynamite has been criticized as destructive of significant historical artifacts, including the level that is believed to be the historical Troy.

Along with Arthur Evans, Schliemann was a pioneer in the study of Aegean civilization in the Bronze Age. The two men knew of each other, Evans having visited Schliemann's sites. Schliemann had planned to excavate at Knossos but died before fulfilling that dream. Evans bought the site and stepped in to take charge of the project, which was then still in its infancy.

Schliemann was born in Neubukow, Mecklenburg-Schwerin (part of the German Confederation), in 1822. His father, Ernst Schliemann, was a Lutheran minister. The family moved to Ankershagen in 1823 (today their home houses the "Heinrich Schliemann Museum").

Heinrich's father was a poor Pastor. His mother, Luise Therese Sophie Schliemann, died in 1831, when Heinrich was nine years old. After his mother's death, his father sent Heinrich to live with his uncle. When he was eleven years old, his father paid for him to enroll in the Gymnasium (grammar school) at Neustrelitz. Heinrich's later interest in history was initially encouraged by his father, who had schooled him in the tales of the Iliad and the Odyssey and had given him a copy of Ludwig Jerrer's "Illustrated History of the World" for Christmas in 1829. Schliemann later claimed that at the age of 7 he had declared he would one day excavate the city of Troy.

However, Heinrich had to transfer to the Realschule (vocational school) after his father was accused of embezzling church funds and had to leave that institution in 1836 when his father was no longer able to pay for it. His family's poverty made a university education impossible, so it was Schliemann's early academic experiences that influenced the course of his education as an adult. In his archaeological career, however, there was often a division between Schliemann and the educated professionals.

At age 14, after leaving Realschule, Heinrich became an apprentice at Herr Holtz's grocery in Fürstenberg. He later told that his passion for Homer was born when he heard a drunkard reciting it at the grocer's. He laboured for five years, until he was forced to leave because he burst a blood vessel lifting a heavy barrel. In 1841, Schliemann moved to Hamburg and became a cabin boy on the "Dorothea," a steamer bound for Venezuela. After twelve days at sea, the ship foundered in a gale. The survivors washed up on the shores of the Netherlands. Schliemann became a messenger, office attendant, and later, a bookkeeper in Amsterdam.

On March 1, 1844, 22-year-old Schliemann took a position with B. H. Schröder & Co., an import/export firm. In 1846, the firm sent him as a General Agent to St. Petersburg.

In time, Schliemann represented a number of companies. He learned Russian and Greek, employing a system that he used his entire life to learn languages; Schliemann claimed that it took him six weeks to learn a language and wrote his diary in the language of whatever country he happened to be in. By the end of his life, he could converse in English, French, Dutch, Spanish, Portuguese, Swedish, Polish, Italian, Greek, Latin, Russian, Arabic, and Turkish as well as German.

Schliemann's ability with languages was an important part of his career as a businessman in the importing trade. In 1850, he learned of the death of his brother, Ludwig, who had become wealthy as a speculator in the California gold fields.

Schliemann went to California in early 1851 and started a bank in Sacramento buying and reselling over a million dollars' worth of gold dust in just six months. When the local Rothschild agent complained about short-weight consignments he left California, pretending it was because of illness. While he was there, California became the 31st state in September 1850, and Schliemann acquired United States citizenship. While this story was propounded in Schliemann's autobiography of 1881, Christo Thanos and Wout Arentzen, state clearly that Schliemann was in St Petersburg that day, and "in actual fact, ...obtained his American citizenship only in 1869."

According to his memoirs, before arriving in California he dined in Washington, D.C. with President Millard Fillmore and his family, but Eric Cline says that Schliemann didn't attend but simply read about it in the papers.

Schliemann also published what he said was an eyewitness account of the San Francisco Fire of 1851, which he said was in June although it took place in May. At the time he was in Sacramento and used the report of the fire in the "Sacramento Daily Journal" to write his report.

On April 7, 1852, he sold his business and returned to Russia. There he attempted to live the life of a gentleman, which brought him into contact with Ekaterina Lyschin, the niece of one of his wealthy friends. Schliemann had previously learned that his childhood sweetheart, Minna, had married.

Heinrich and Ekaterina married on October 12, 1852. The marriage was troubled from the start.

Schliemann next cornered the market in indigo dye and then went into the indigo business itself, turning a good profit. Ekaterina and Heinrich had a son, Sergey, and two daughters, Natalya and Nadezhda, born in 1855, 1858, and 1861, respectively.

Schliemann made yet another quick fortune as a military contractor in the Crimean War, 1854–1856. He cornered the market in saltpeter, sulfur, and lead, constituents of ammunition, which he resold to the Russian government.

By 1858, Schliemann was 36 years old and wealthy enough to retire. In his memoirs, he claimed that he wished to dedicate himself to the pursuit of Troy.

As a consequence of his many travels, Schliemann was often separated from his wife and small children. He spent a month studying at the Sorbonne in 1866, while moving his assets from St. Petersburg to Paris to invest in real estate. He asked his wife to join him, but she refused.

Schliemann threatened to divorce Ekaterina twice before doing so. In 1869, he bought property and settled in Indianapolis for about three months to take advantage of Indiana's liberal divorce laws. although he obtained the divorce by lying about his residency in the U.S. and his intention to remain in the state. He moved to Athens as soon as an Indiana court granted him the divorce and married again three months later.

Schliemann's first interest of a classical nature seems to have been the location of Troy. At the time he began excavating in Turkey, the site commonly believed to be Troy was at Pınarbaşı, a hilltop at the south end of the Trojan Plain. The site had been previously excavated by archaeologist and local expert, Frank Calvert. Schliemann performed soundings at Pınarbaşı but was disappointed by his findings. It was Calvert who identified Hissarlik as Troy and suggested Schliemann dig there on land owned by Calvert's family.

In 1868, Schliemann visited sites in the Greek world, published "Ithaka, der Peloponnesus und Troja" in which he asserted that Hissarlik was the site of Troy, and submitted a dissertation in Ancient Greek proposing the same thesis to the University of Rostock. In 1869, he was awarded a PhD "in absentia" from the university of Rostock for that submission. David Traill wrote that the examiners gave him his PhD on the basis of his topographical analyses of Ithaca, which were in part simply translations of another author's work or drawn from poetic descriptions by the same author.

Schliemann was at first skeptical about the identification of Hissarlik with Troy but was persuaded by Calvert and took over Calvert's excavations on the eastern half of the Hissarlik site. The Turkish government owned the western half. Calvert became Schliemann's collaborator and partner.

Schliemann needed an assistant who was knowledgeable in matters pertaining to Greek culture. As he had divorced Ekaterina in 1869, he advertised for a wife in a newspaper in Athens. A friend, the Archbishop of Athens, suggested a relative of his, 17-year-old Sophia Engastromenos (1852–1932). Schliemann, age 47, married her in October 1869, despite the 30 year difference in age. They later had two children, Andromache and Agamemnon Schliemann; he reluctantly allowed them to be baptized, but solemnized the ceremony in his own way by placing a copy of the "Iliad" on the children's heads and reciting 100 hexameters.

Schliemann began work on Troy in 1871. His excavations began before archaeology had developed as a professional field. Thinking that Homeric Troy must be in the lowest level, Schliemann and his workers dug hastily through the upper levels, reaching fortifications that he took to be his target. In 1872, he and Calvert fell out over this method. Schliemann was angry when Calvert published an article stating that the Trojan War period was missing from the site's archaeological record.

Schliemann was elected a member of the American Antiquarian Society in 1880.

A cache of gold and other objects appeared on or around May 27, 1873; Schliemann named it "Priam's Treasure". He later wrote that he had seen the gold glinting in the dirt and dismissed the workmen so that he and Sophia could excavate it themselves; they removed it in her shawl. However, Schliemann's oft-repeated story of the treasure's being carried by Sophia in her shawl was untrue. Schliemann later admitted fabricating it; at the time of the discovery Sophia was in fact with her family in Athens, following the death of her father. Sophia later wore "the Jewels of Helen" for the public. Those jewels, taken from the Pergamon Museum in Berlin by the Soviet Army (Red Army) in 1945, are now in the Pushkin Museum in Moscow.

Schliemann published his findings in 1874, in "Trojanische Altertümer" ("Trojan Antiquities").

This publicity backfired when the Turkish government revoked Schliemann's permission to dig and sued him for a share of the gold. Collaborating with Calvert, Schliemann smuggled the treasure out of Turkey. He defended his "smuggling" in Turkey as an attempt to protect the items from corrupt local officials. Priam's Treasure today remains a subject of international dispute.

Schliemann published "Troja und seine Ruinen" ("Troy and Its Ruins") in 1875 and began excavation of the Treasury of Minyas at Orchomenus (Boeotia) in 1880. In 1876, he began digging at Mycenae. Upon discovering the Shaft Graves, with their skeletons and more regal gold (including the so-called Mask of Agamemnon), Schliemann cabled the king of Greece. The results were published in "Mykenai" in 1878.

Although he had received permission in 1876 to continue excavation, Schliemann did not reopen the dig site at Troy until 1878–1879, after another excavation in Ithaca designed to locate a site mentioned in the "Odyssey". This was his second excavation at Troy. Emile Burnouf and Rudolf Virchow joined him there in 1879. 

Schliemann made a third excavation at Troy in 1882–1883, an excavation of Tiryns with Wilhelm Dörpfeld in 1884, and a fourth excavation at Troy, also with Dörpfeld (who emphasized the importance of strata), in 1888–1890.

On August 1, 1890, Schliemann returned reluctantly to Athens, and in November travelled to Halle, where his chronic ear infection was operated upon, on November 13. The doctors deemed the operation a success, but his inner ear became painfully inflamed. Ignoring his doctors' advice, he left the hospital and travelled to Leipzig, Berlin, and Paris. From the latter, he planned to return to Athens in time for Christmas, but his ear condition became even worse. Too sick to make the boat ride from Naples to Greece, Schliemann remained in Naples but managed to make a journey to the ruins of Pompeii. On Christmas Day 1890, he collapsed into a coma; he died in a Naples hotel room the following day; the cause of death was cholesteatoma.

His corpse was then transported by friends to the First Cemetery in Athens. It was interred in a mausoleum shaped like a temple erected in ancient Greek style, designed by Ernst Ziller in the form of a pedimental sculpture. The frieze circling the outside of the mausoleum shows Schliemann conducting the excavations at Mycenae and other sites.

Schliemann's magnificent residence in the city centre of Athens, the "Iliou Melathron" (Ιλίου Μέλαθρον, "Palace of Ilium") houses today the Numismatic Museum of Athens.

Further excavation of the Troy site by others indicated that the level he named the Troy of the "Iliad" was inaccurate, although they retain the names given by Schliemann. In an article for "The Classical World," D.F. Easton wrote that Schliemann "was not very good at separating fact from interpretation" and claimed that, "Even in 1872 Frank Calvert could see from the pottery that Troy II had to be hundreds of years too early to be the Troy of the Trojan War, a point finally proved by the discovery of Mycenaean pottery in Troy VI in 1890."
"King Priam's Treasure" was found in the Troy II level, that of the Early Bronze Age, long before Priam's city of Troy VI or Troy VIIa in the prosperous and elaborate Mycenaean Age. Moreover, the finds were unique. The elaborate gold artifacts do not appear to belong to the Early Bronze Age.

His excavations were condemned by later archaeologists as having destroyed the main layers of the real Troy. Kenneth W. Harl, in the Teaching Company's "Great Ancient Civilizations of Asia Minor" lecture series, sarcastically claimed that Schliemann's excavations were carried out with such rough methods that he did to Troy what the Greeks couldn't do in their times, destroying and levelling down the entire city walls to the ground.

In 1972, Professor William Calder of the University of Colorado, speaking at a commemoration of Schliemann's birthday, claimed that he had uncovered several possible problems in Schliemann's work. Other investigators followed, such as Professor David Traill of the University of California.

An article published by the National Geographic Society called into question Schliemann's qualifications, his motives, and his methods:

Another article presented similar criticisms when reporting on a speech by University of Pennsylvania scholar C. Brian Rose:

Schliemann's methods have been described as "savage and brutal. He plowed through layers of soil and everything in them without proper record keeping—no mapping of finds, few descriptions of discoveries." Carl Blegen forgave his recklessness, saying "Although there were some regrettable blunders, those criticisms are largely colored by a comparison with modern techniques of digging; but it is only fair to remember that before 1876 very few persons, if anyone, yet really knew how excavations should properly be conducted. There was no science of archaeological investigation, and there was probably no other digger who was better than Schliemann in actual field work."

In 1874, Schliemann also initiated and sponsored the removal of medieval edifices from the Acropolis of Athens, including the great Frankish Tower. Despite considerable opposition, including from King George I of Greece, Schliemann saw the project through. The eminent historian of Frankish Greece William Miller later denounced this as "an act of vandalism unworthy of any people imbued with a sense of the continuity of history", and "pedantic barbarism".

Peter Ackroyd's novel "The Fall of Troy" (2006) is based on Schliemann's excavation of Troy. Schliemann is portrayed as "Heinrich Obermann".

Schliemann is also the subject of Chris Kuzneski's novel" The Lost Throne".

Schliemann is the subject of Irving Stone's novel "The Greek Treasure" (1975), which was the basis for the 2007 German television production "" ("Hunt for Troy").

Schliemann is a peripheral character in the historical mystery, "A Terrible Beauty". It is the 11th book in a series of novels featuring Lady Emily Hargreaves by Tasha Alexander. 







</doc>
<doc id="13629" url="https://en.wikipedia.org/wiki?curid=13629" title="Hypnos">
Hypnos

In Greek mythology, Hypnos (; , "sleep") is the personification of sleep; the Roman equivalent is known as Somnus.

In the Greek mythology, Hypnos is the son of Nyx ("The Night") and Erebus ("The Darkness"). His brother is Thanatos ("Death"). Both siblings live in the underworld ("Hades") or in Erebus, another valley of the Greek underworld. According to rumors, Hypnos lives in a big cave, which the river Lethe ("Forgetfulness") comes from and where night and day meet. His bed is made of ebony, on the entrance of the cave grow a number of poppies and other hypnotic plants. No light and no sound would ever enter his grotto. According to Homer, he lives on the island Lemnos, which later on has been claimed to be his very own dream-island. His children Morpheus ("Shape"), Phobetor ("Fear") and Phantasos ("Imagination, Fantasy") are the gods of the dream. It is claimed that he has many more children, which are also Oneiroi. He is said to be a calm and gentle god, as he helps humans in need and, due to their sleep, owns half of their lives.

Hypnos lived next to his twin brother, Thanatos (Θάνατος, "death personified") in the underworld.

Hypnos' mother was Nyx (Νύξ, "Night"), the deity of Night, and his father was Erebus, the deity of Darkness. Nyx was a dreadful and powerful goddess, and even Zeus feared to enter her realm.

His wife, Pasithea, was one of the youngest of the Graces and was promised to him by Hera, who is the goddess of marriage and birth. Pasithea is the deity of hallucination or relaxation.

Hypnos' three brothers (according to Hesiod and Hyginus) or sons (according to Ovid) were known as the Oneiroi, which is Greek for "dreams." Morpheus is the Winged God of Dreams and can take human form in dreams. Phobetor is the personification of nightmares and created frightening dreams, he could take the shape of any animal including bears and tigers. Phantasos was known for creating fake dreams full of illusions. Morpheus, Phobetor, and Phantasos appeared in the dreams of kings. The Oneiroi lived in a cave at the shores of the Ocean in the West. The cave had two gates with which to send people dreams; one made from ivory and the other from buckhorn. However, before they could do their work and send out the dreams, first Hypnos had to put the recipient to sleep.

Hypnos used his powers to trick Zeus. Hypnos was able to trick him and help the Danaans win the Trojan war. During the war, Hera loathed her brother and husband, Zeus, so she devised a plot to trick him. She decided that in order to trick him she needed to make him so enamoured with her that he would fall for the trick. So she washed herself with ambrosia and anointed herself with oil, made especially for her to make herself impossible to resist for Zeus. She wove flowers through her hair, put on three brilliant pendants for earrings, and donned a wondrous robe. She then called for Aphrodite, the goddess of love, and asked her for a charm that would ensure that her trick would not fail. In order to procure the charm, however, she lied to Aphrodite because they sided on opposites sides of the war. She told Aphrodite that she wanted the charm to help herself and Zeus stop fighting. Aphrodite willingly agreed. Hera was almost ready to trick Zeus, but she needed the help of Hypnos, who had tricked Zeus once before.

Hera called on Hypnos and asked him to help her by putting Zeus to sleep. Hypnos was reluctant because the last time he had put the god to sleep, he was furious when he awoke. It was Hera who had asked him to trick Zeus the first time as well. She was furious that Heracles, Zeus' son, sacked the city of the Trojans. So she had Hypnos put Zeus to sleep, and set blasts of angry winds upon the sea while Heracles was still sailing home. When Zeus awoke he was furious and went on a rampage looking for Hypnos. Hypnos managed to avoid Zeus by hiding with his mother, Nyx. This made Hypnos reluctant to accept Hera's proposal and help her trick Zeus again. Hera first offered him a beautiful golden seat that can never fall apart and a footstool to go with it. He refused this first offer, remembering the last time he tricked Zeus. Hera finally got him to agree by promising that he would be married to Pasithea, one of the youngest Graces, whom he had always wanted to marry. Hypnos made her swear by the river Styx and call on gods of the underworld to be witnesses so that he would be ensured that he would marry Pasithea.

Hera went to see Zeus on Gargarus, the topmost peak of Mount Ida. Zeus was extremely taken by her and suspected nothing as Hypnos was shrouded in a thick mist and hidden upon a pine tree that was close to where Hera and Zeus were talking. Zeus asked Hera what she was doing there and why she had come from Olympus, and she told him the same lie she told Aphrodite. She told him that she wanted to go help her parent stop quarrelling and she stopped there to consult him because she didn't want to go without his knowledge and have him be angry with her when he found out. Zeus said that she could go any time, and that she should postpone her visit and stay there with him so they could enjoy each other's company. He told her that he was never in love with anyone as much as he loved her at that moment. He took her in his embrace and Hypnos went to work putting him to sleep, with Hera in his arms. While this went on, Hypnos travelled to the ships of the Achaeans to tell Poseidon, God of the Sea, that he could now help the Danaans and give them a victory while Zeus was sleeping. This is where Hypnos leaves the story, leaving Poseidon eager to help the Danaans. Thanks to Hypnos helping to trick Zeus, the war changed its course to Hera's favour, and Zeus never found out that Hypnos had tricked him one more time.

According to a passage in "Deipnosophistae", the sophist and dithyrambic poet Licymnius of Chios tells a different tale about the Endymion myth, in which Hypnos, in awe of his beauty, causes him to sleep with his eyes open, so he can fully admire his face.

Hypnos appears in numerous works of art, most of which are vases. An example of one vase that Hypnos is featured on is called "Ariadne Abandoned by Theseus," which is part of the Museum of Fine Arts in Boston’s collection. In this vase, Hypnos is shown as a winged god dripping Lethean water upon the head of Ariadne as she sleeps. One of the most famous works of art featuring Hypnos is a bronze head of Hypnos himself, now kept in the British Museum in London. This bronze head has wings sprouting from his temples and the hair is elaborately arranged, some tying in knots and some hanging freely from his head.

The English word "hypnosis" is derived from his name, referring to the fact that when hypnotized, a person is put into a sleep-like state (hypnos "sleep" + -osis "condition"). The class of medicines known as "hypnotics" which induce sleep also take their name from Hypnos.

Additionally, the English word "insomnia" comes from the name of his Latin counterpart, Somnus. (in- "not" + somnus "sleep"), as well as a few less-common words such as "somnolent", meaning sleepy or tending to cause sleep and hypersomnia meaning excessive sleep, which can be caused by many conditions (known as secondary hypersomnia) or a rare sleep disorder causing excessive sleep with unknown cause, called Idiopathic Hypersomnia.




</doc>
<doc id="13631" url="https://en.wikipedia.org/wiki?curid=13631" title="Holy orders">
Holy orders

In the Christian churches, holy orders are ordained ministries such as bishop, priest or deacon. In the Roman Catholic (Latin: "sacri ordines"), Eastern Catholic, Eastern Orthodox (ιερωσύνη ["hierōsynē"], ιεράτευμα ["hierateuma"], Священство ["Svyashchenstvo"]), Oriental Orthodox, Anglican, Assyrian, Old Catholic, Independent Catholic and some Lutheran churches, holy orders are the three ministerial orders of bishop, priest and deacon, or the sacrament or rite by which candidates are ordained to those orders. Except for Lutherans and some Anglicans, these churches regard ordination as a sacrament (the "sacramentum ordinis"). The Anglo-Catholic tradition within Anglicanism identifies more with the Roman Catholic position about the sacramental nature of ordination.

Denominations have varied conceptions of holy orders. In the Anglican churches and some Lutheran churches the traditional orders of bishop, priest and deacon are bestowed using ordination rites. The extent to which ordination is considered sacramental in these traditions has, however, been a matter of some internal dispute. Many other denominations do not consider ministry as being sacramental in nature and would not think of it in terms of "holy orders" as such. Historically, the word "order" (Latin "ordo") designated an established civil body or corporation with a hierarchy, and "ordinatio" meant legal incorporation into an "ordo". The word "holy" refers to the Church. In context, therefore, a holy order is set apart for ministry in the Church. Other positions, such as pope, patriarch, cardinal, monsignor, archbishop, archimandrite, archpriest, protopresbyter, hieromonk, protodeacon and archdeacon, are not sacramental orders but particular ministry positions.

The Eastern Orthodox Church considers ordination (known as "Cheirotonia", "laying on of hands") to be a Sacred Mystery (what in the West is called a sacrament). Although all other mysteries may be performed by a presbyter, ordination may only be conferred by a bishop, and ordination of a bishop may only be performed by several bishops together. Cheirotonia always takes place during the Divine Liturgy.

It was the mission of the Apostles to go forth into all the world and preach the Gospel, baptizing those who believed in the name of the Holy Trinity (). In the Early Church those who presided over congregations were referred to variously as "episcopos" (bishop) or "presbyteros" (priest). These successors of the Apostles were ordained to their office by the laying on of hands, and according to Orthodox theology formed a living, organic link with the Apostles, and through them with Jesus Christ himself. This link is believed to continue in unbroken succession to this day. Over time, the ministry of bishops (who hold the fullness of the priesthood) and presbyters or priests (who hold a portion of the priesthood as bestowed by their bishop) came to be distinguished. In Orthodox terminology, "priesthood" or "sacerdotal" refers to the ministry of bishops and priests.

The Eastern Orthodox Church also has ordination to minor orders (known as "cheirothesia", "imposition of hands") which is performed outside of the Divine Liturgy, typically by a bishop, although certain archimandrites of stavropegial monasteries may bestow cheirothesia on members of their communities.

A bishop is the Teacher of the Faith, the carrier of Sacred Tradition, and the living Vessel of Grace through whom the "energeia" (divine grace) of the Holy Spirit flows into the rest of the church. A bishop is consecrated through the laying on of hands by several bishops. (With the consent of several other bishops, a single bishop has performed the ordination of another bishop, in emergency situations, such as times of persecution), The consecration of a bishop takes place near the beginning of the Liturgy, since a bishop can, in addition to performing the Mystery of the Eucharist, also ordain priests and deacons. Before the commencement of the Holy Liturgy, the bishop-elect professes, in the middle of the church before the seated bishops who will consecrate him, in detail the doctrines of the Orthodox Christian Faith and pledges to observe the canons of the Apostles and Councils, the Typikon and customs of the Orthodox Church and to obey ecclesiastical authority. After the Little Entrance, the arch-priest and arch-deacon conduct the bishop-elect before the Royal Gates where he is met by the bishops and kneels before the altar on both knees and the Gospel Book is laid over his head and the consecrating bishops lay their hands upon the Gospel Book, while the prayers of ordination are read by the eldest bishop; after this, the newly consecrated bishop ascends the "synthranon" (bishop's throne in the sanctuary) for the first time.

A priest may serve only at the pleasure of his bishop. A bishop bestows faculties (permission to minister within his diocese) giving a priest chrism and an antimins; he may withdraw faculties and demand the return of these items. The ordination of a priest occurs before the Anaphora (Eucharistic Prayer) in order that he may on the same day take part in the celebration of the Eucharist: During the Great Entrance, the candidate for ordination carries the Aër (chalice veil) over his head (rather than on his shoulder, as a deacon otherwise carries it then) as a symbol of giving up his diaconate, and comes last in the procession and stands at the end of the pair of lines of the priests. After the Aër is taken from the candidate to cover the chalice and diskos, a chair is brought for the bishop to sit on by the northeast corner of the Holy Table (altar). Two deacons go to priest-elect who, at that point, had been standing alone in the middle of the church, and bow him down to the west (to the people) and to the east (to the clergy), asking their consent by saying “Command ye!” and then lead him through the holy doors of the altar where the archdeacon asks the bishop’s consent, saying, “Command, most sacred master!” after which a priest escorts the candidate three times around the Holy Table, during which he kisses each corner of the Holy Table as well as the bishop's epigonation and right hand and prostrates himself before the holy table at each circuit. The candidate is then taken to the southeast corner of the Holy Table and kneels on both knees, resting his forehead on the edge of the Holy Table. The ordaining bishop then places his omophor and right hand over the ordinand's head and recites aloud the first "Prayer of Cheirotonia" and then prays silently the other two prayers of cheirotonia while a deacon quietly recites a litany and the clergy, then the congregation, chant “Lord, have mercy”. Afterwards, the bishop brings the newly ordained priest to stand in the Holy Doors and presents him to the faithful. He then clothes the priest in each of his sacerdotal vestments, at each of which the people sing, "Worthy!". Later, after the Epiklesis of the Liturgy, the bishop hands him a portion of the Lamb (Host) saying:
A deacon may not perform any Sacrament and performs no liturgical services on his own but serves only as an assistant to a priest and may not even vest without the blessing of a priest. The ordination of a deacon occurs after the Anaphora (Eucharistic Prayer) since his role is not in performing the Holy Mystery but consists only in serving; the ceremony is much the same as at the ordination of a priest, but the deacon-elect is presented to the people and escorted to the holy doors by two sub-deacons (his peers, analogous to the two deacons who so present a priest-elect) is escorted three times around the Holy Table by a deacon, and he kneels on only one knee during the "Prayer of Cheirotonia". After being vested as a deacon and given a "liturgical fan (ripidion or hexapterygion)", he is led to the side of the Holy Table where he uses the ripidion to gently fan the Holy Gifts (consecrated Body and Blood of Christ).

The Anglican churches hold their bishops to be in apostolic succession, although there is some difference of opinion with regard to whether ordination is to be regarded as a sacrament. The Anglican Articles of Religion hold that only Baptism and the Lord's Supper are to be counted as sacraments of the gospel, and assert that other rites "commonly called Sacraments", considered to be sacraments by such as the Roman Catholic and Eastern churches, were not ordained by Christ in the Gospel. They do not have the nature of a sacrament of the gospel in the absence of any physical matter such as the water in Baptism and the bread and wine in the Eucharist. The Book of Common Prayer provides rites for ordination of bishops, priests and deacons. Only bishops may ordain. Within Anglicanism, three bishops are normally required for ordination to the episcopate, while one bishop is sufficient for performing ordinations to the priesthood and diaconate.

Lutherans reject the Roman Catholic understanding of holy orders because they do not think sacerdotalism is supported by the Bible. Martin Luther taught that each individual was expected to fulfill his God-appointed task in everyday life. The modern usage of the term vocation as a life-task was first employed by Martin Luther. In Luther's Small Catechism, the holy orders include, but are not limited to the following: bishops, pastors, preachers, governmental offices, citizens, husbands, wives, children, employees, employers, young people, and widows. However, also according to the Book of Concord: "But if ordination be understood as applying to the ministry of the Word, we are not unwilling to call ordination a sacrament. For the ministry of the Word has God's command and glorious promises, Rom. 1:16: The Gospel is the power of God unto salvation to every one that believeth. Likewise, Isa. 55:11: So shall My Word be that goeth forth out of My mouth; it shall not return unto Me void, but it shall accomplish that which I please. 12.] If ordination be understood in this way, neither will we refuse to call the imposition of hands a sacrament. For the Church has the command to appoint ministers, which should be most pleasing to us, because we know that God approves this ministry, and is present in the ministry [that God will preach and work through men and those who have been chosen by men]."

The ministerial orders of the Catholic Church include the orders of bishops, deacons and presbyters, which in Latin is "sacerdos". The ordained priesthood and common priesthood (or priesthood of the all the baptized) are different in function and essence.

A distinction is made between "priest" and "presbyter". In the 1983 Code of Canon Law, "The Latin words "sacerdos" and "sacerdotium" are used to refer in general to the ministerial priesthood shared by bishops and presbyters. The words "presbyter, presbyterium and presbyteratus" refer to priests [in the English use of the word] and presbyters".

While the consecrated life is neither clerical nor lay by definition, clerics can be members of institutes of consecrated or secular (diocesan) life.

The sequence in which holy orders are received are: minor orders, deacon, priest, bishop.

For Catholics, it is typical in the year of seminary training that a man will be ordained to the diaconate, which Catholics since the Second Vatican Council sometimes call the "transitional diaconate" to distinguish men bound for priesthood from permanent deacons. They are licensed to preach sermons (under certain circumstances a permanent deacon may not receive faculties to preach), to perform baptisms, and to witness Catholic marriages, but to perform no other sacraments. They assist at the Eucharist or the Mass, but are not able to consecrate the bread and wine. Normally, after six months or more as a transitional deacon, a man will be ordained to the priesthood. Priests are able to preach, perform baptisms, confirm (with special dispensation from their ordinary), witness marriages, hear confessions and give absolutions, anoint the sick, and celebrate the Eucharist or the Mass.

Orthodox seminarians are typically tonsured as readers before entering seminary, and may later be made subdeacons or deacons; customs vary between seminaries and between Orthodox jurisdictions. Some deacons remain permanently in the diaconate while most subsequently are ordained as priests. Orthodox clergy are typically either married or monastic. Monastic deacons are called hierodeacons, monastic priests are called hieromonks. Orthodox clergy who marry must do so prior to ordination to the subdiaconate (or diaconate, according to local custom) and typically one is either tonsured a monk or married before ordination. A deacon or priest may not marry, or remarry if widowed, without abandoning his clerical office. Often, widowed priests take monastic vows. Orthodox bishops are always monks; a single or widowed man may be elected a bishop but he must be tonsured a monk before consecration as a bishop.

For Anglicans, a person is usually ordained a deacon once he (or she) has completed training at a theological college. The historic practice of a bishop tutoring a candidate himself ("reading for orders") is still to be found. The candidate then typically serves as an assistant curate and may later be ordained as a priest at the discretion of the bishop. Other deacons may choose to remain in this order. Anglican deacons can preach sermons, perform baptisms and conduct funerals, but, unlike priests, cannot celebrate the Eucharist. In most branches of the Anglican church, women can be ordained as priests, and in some of them, can also be ordained bishops.

Bishops are chosen from among priests in churches that adhere to Catholic usage.
In the Roman Catholic Church, bishops, like priests, are celibate and thus unmarried; further, a bishop is said to possess the fullness of the sacrament of holy orders, empowering him to ordain deacons, priests, and – with papal consent – other bishops. If a bishop, especially one acting as an ordinary – a head of a diocese or archdiocese – is to be ordained, three bishops must usually co-consecrate him with one bishop, usually an archbishop or the bishop of the place, being the chief consecrating prelate.

Among Eastern Rite Catholic and Eastern Orthodox churches, which permit married priests, bishops must either be unmarried or agree to abstain from contact with their wives. It is a common misconception that all such bishops come from religious orders; while this is generally true, it is not an absolute rule. In the case of both Catholics – (Western and) Eastern Catholic, Oriental Orthodox and Eastern Orthodox, they are usually leaders of territorial units called dioceses (or its equivalent in the east, an eparchy). Only bishops can validly administer the sacrament of holy orders.

The Roman Catholic Church unconditionally recognizes the validity of ordinations in the Eastern churches. Some Eastern Orthodox churches reordain Catholic priests who convert while others accept their Roman Catholic ordination using the concept of economia (church economy).

Anglican churches claim to have maintained apostolic succession. The succession of Anglican bishops is not universally recognized, however. The Roman Catholic Church judged Anglican orders invalid when Pope Leo XIII in 1896 wrote in "Apostolicae curae" that Anglican orders lack validity because the rite by which priests were ordained was not correctly worded from 1547 to 1553 and from 1559 to the time of Archbishop William Laud, (Archbishop of Canterbury 1633–1645). The Papacy claimed the form and matter was inadequate to make a Catholic bishop. The actual 'mechanical' succession, prayer and laying on hands was not disputed. Two of the four consecrators of Matthew Parker in 1559 had been consecrated using the English Ordinal and two using the Roman Pontifical. Nonetheless, they believed that this caused a break of continuity in apostolic succession, making all further ordinations null and void.

Eastern Orthodox bishops have, on occasion, granted "economy" when Anglican priests convert to Orthodoxy. Various Orthodox churches have also declared Anglican orders valid subject to a finding that the bishops in question did indeed maintain the true faith, the Orthodox concept of Apostolic Succession being one in which the faith must be properly adhered to and transmitted, not simply that the ceremony by which a man is made a bishop is conducted correctly.

Changes in the Anglican Ordinal since King Edward VI, and a fuller appreciation of the pre-Reformation ordinals, suggest that the correctness of the enduring dismissal of Anglican orders is questionable. To reduce doubt concerning Anglican apostolic succession, especially since the 1930 Bonn agreement between the Anglican and Old Catholic churches, some Anglican bishops have included among their consecrators bishops of the Old Catholic Church, whose holy orders are recognised as valid and regular by the Roman Catholic Church.

Neither Roman Catholics nor Anglicans recognize the validity of ordinations of ministers in Protestant churches that do not maintain Apostolic Succession; but some Anglicans, especially Low Church or Evangelical Anglicans, commonly treat Protestant ministers and their sacraments as valid. Rome also does not recognize the apostolic succession of those Lutheran bodies which retained Apostolic Succession.

Officially, the Anglican Communion accepts the ordinations of those denominations which are in full communion with their own churches, such as the Lutheran state churches of Scandinavia. Those clergy may preside at services requiring a priest if one is not otherwise available.

The rules discussed in this section are not considered to be among the infallible dogmas of the Catholic Church, but are mutable rules of discipline. See clerical celibacy for a more detailed discussion.

Married men may be ordained to the diaconate as Permanent Deacons, but in the Latin Rite of the Roman Catholic Church generally may not be ordained to the priesthood. In the Eastern Catholic Churches and in the Eastern Orthodox Church, married deacons may be ordained priests but may not become bishops. Bishops in the Eastern Rites and the Eastern Orthodox churches are almost always drawn from among monks, who have taken a vow of celibacy. They may be widowers, though; it is not required of them never to have been married.

In some cases, widowed permanent deacons have been ordained to the priesthood. There have been some situations in which men previously married and ordained to the priesthood in an Anglican church or in a Lutheran church have been ordained to the Catholic priesthood and allowed to function much as an Eastern Rite priest but in a Latin Rite setting. This is never "sub conditione" (conditionally), as there is in Catholic canon law no true priesthood in Protestant denominations. Such ordination may only happen with the approval of the priest's Bishop and a special permission by the Pope.

Anglican clergy may be married and/or may marry after ordination. In the Old Catholic Church and the Independent Catholic Churches there are no ordination restrictions related to marriage.

Ordination ritual and procedures vary by denomination. Different churches and denominations specify more or less rigorous requirements for entering into office, and the process of ordination is likewise given more or less ceremonial pomp depending on the group. Many Protestants still communicate authority and ordain to office by having the existing overseers physically lay hands on the candidates for office.

The American Methodist model is an episcopal system loosely based on the Anglican model, as the Methodist Church arose from the Anglican Church. It was first devised under the leadership of Bishops Thomas Coke and Francis Asbury of the Methodist Episcopal Church in the late 18th century. In this approach, an elder (or "presbyter") is ordained to word (preaching and teaching), sacrament (administering Baptism and the Lord's Supper), order (administering the life of the church and, in the case of bishops, ordaining others for mission and ministry), and service. A deacon is a person ordained only to word and service.

In the United Methodist Church, for instance, seminary graduates are examined and approved by the Conference Board of Ordained Ministry and then the Clergy Session. They are accepted as "probationary (provisional) members of the conference." The resident bishop may commission them to full-time ministry as "provisional" ministers. (Before 1996, the graduate was ordained as a transitional deacon at this point, a provisional role since eliminated. The order of deacon is now a separate and distinct clergy order in the United Methodist Church.) After serving the probationary period, of a minimum of two years, the probationer is then examined again and either continued on probation, discontinued altogether, or approved for ordination. Upon final approval by the Clergy Session of the Conference, the probationer becomes a full member of the Conference and is then ordained as an elder or deacon by the resident Bishop. Those ordained as elders are members of the Order of Elders, and those ordained deacons are members of the Order of Deacons.

The British Methodist Conference has two distinct orders of presbyter and deacon. It does not have bishops as a separate order of ministry.

John Wesley appointed Thomas Coke (above mentioned as bishop) as 'Superintendent', his translation of the Greek 'episcopos' – which is normally translated 'bishop' in English. The British Methodist Church has more than 500 Superintendents who are not a separate order of ministry but a role within the order of Presbyters.

In British Methodism the roles normally undertaken by bishops are expressed in ordaining presbyters and deacons by the annual Conference through its President (or a past president); in confirmation by all presbyters; in local oversight by Superintendents and in regional oversight by Chairs of District.

Presbyterian churches, following their Scottish forebears, reject the traditions surrounding overseers and instead identify the offices of bishop ("episkopos" in Greek) and elder ("presbuteros" in Greek, from which the term "presbyterian" comes). The two terms seem to be used interchangeably in the Bible (compare Titus 1.5–9 and I Tim. 3.2–7). Their form of church governance is known as presbyterian polity. While there is increasing authority with each level of gathering of elders ('Session' over a congregation or parish, then presbytery, then possibly a synod, then the General Assembly), there is no hierarchy of elders. Each elder has an equal vote at the court on which they stand.

Elders are usually chosen at their local level, either elected by the congregation and approved by the Session, or appointed directly by the Session. Some churches place limits on the term that the elders serve, while others ordain elders for life.

Presbyterians also ordain (by laying on of hands) ministers of Word and Sacrament (sometimes known as 'teaching elders'). These ministers are regarded simply as Presbyters ordained to a different function, but in practice they provide the leadership for local Session.

Some Presbyterians identify those appointed (by the laying on of hands) to serve in practical ways (Acts 6.1–7) as deacons ("diakonos" in Greek, meaning "servant"). In many congregations, a group of men or women is thus set aside to deal with matters such as congregational fabric and finance, releasing elders for more 'spiritual' work. These persons may be known as 'deacons', 'board members' or 'managers', depending on the local tradition. Unlike elders and minister, they are not usually 'ordained', and are often elected by the congregation for a set period of time.

Other Presbyterians have used an 'order of deacons' as full-time servants of the wider Church. Unlike ministers, they do not administer sacraments or routinely preach. The Church of Scotland has recently begun ordaining deacons to this role.

Unlike the Episcopalian system, but similar to the United Methodist system described above, the two Presbyterian offices are different in "kind" rather than in "degree", since one need not be a deacon before becoming an elder. Since there is no hierarchy, the two offices do not make up an "order" in the technical sense, but the terminology of holy orders is sometimes still developed.

Congregationalist churches implement different schemes, but the officers usually have less authority than in the presbyterian or episcopalian forms. Some ordain only ministers and rotate members on an advisory board (sometimes called a board of elders or a board of deacons). Because the positions are by comparison less powerful, there is usually less rigor or fanfare in how officers are ordained.

The Church of Jesus Christ of Latter-day Saints (LDS Church) accepts the legal authority of clergy to perform marriages but does not recognize any other sacraments performed by ministers not ordained to the Latter-day Saint priesthood. Although the Latter-day Saints do claim a doctrine of a certain spiritual "apostolic succession," it is significantly different from that claimed by Catholics and Protestants since there is no succession or continuity between the first century and the lifetime of Joseph Smith, the founder of the LDS church. Mormons teach that the priesthood was lost in ancient times not to be restored by Christ until the nineteenth century when it was given to Joseph Smith directly.

The Church of Jesus Christ of Latter-day Saints has a relatively open priesthood, ordaining nearly all worthy adult males and boys of the age of twelve and older. Latter-day Saint priesthood consists of two divisions: the Melchizedek Priesthood and Aaronic Priesthood. The Melchizedek Priesthood because Melchizedek was such a great high priest. Before his day it was called the Holy Priesthood, after the Order of the Son of God. But out of respect or reverence to the name of the Supreme Being, to avoid the too frequent repetition of his name, the church, in ancient days, called that priesthood after Melchizedek. The lesser priesthood is an appendage to the Melchizedek Priesthood. It is called the Aaronic Priesthood because it was conferred on Aaron and his sons throughout all their generations.
The offices, or ranks, of the Melchizedek order (in roughly descending order) include apostle, seventy, patriarch, high priest, and elder. The offices of the Aaronic order are bishop, priest, teacher, and deacon. The manner of ordination consists of the laying on of hands by two or more men holding at least the office being conferred while one acts as voice in conferring the priesthood and/or office and usually pronounces a blessing upon the recipient. Teachers and deacons do not have the authority to ordain others to the priesthood. All church members are authorized to teach and preach regardless of priesthood ordination so long as they maintain good standing within the church. The church does not use the term "holy orders."

Community of Christ has a largely volunteer priesthood, and all members of the priesthood are free to marry (as traditionally defined by the Christian community). The priesthood is divided into two orders, the Aaronic priesthood and the Melchisedec priesthood. The Aaronic order consists of the offices of deacon, teacher and priest. The Melchisedec Order consists of the offices of elder (including the specialized office of seventy) and high priest (including the specialized offices of evangelist, bishop, apostle, & prophet). Paid ministers include “appointees” and the general officers of the church, which include some specialized priesthood offices (such as the office of president, reserved for the three top members of the church leadership team). As of 1984, women have been eligible for priesthood, which is conferred through the sacrament of ordination by the laying-on-of-hands. While there is technically no age requirement for any office of priesthood, there is no automatic ordination or progression as in the LDS Church. Young people are occasionally ordained as deacon, and sometimes teacher or priest, but generally most priesthood members are called following completion of post secondary school education. In March 2007 a woman was ordained for the first time to the office of president.

The Roman Catholic Church, in accordance with its understanding of the theological tradition on the issue, and the definitive clarification found in the encyclical letter "Ordinatio Sacerdotalis" (1994) written by Pope John Paul II, officially teaches that it has no authority to ordain women as priests and thus there is no possibility of women becoming priests at any time in the future. "Ordaining" women as deaconesses is not a possibility in any sacramental sense of the diaconate, for a deaconess is not simply a woman who is a deacon but instead holds a position of lay service. As such, she does not receive the sacrament of holy orders. Many Anglican and Protestant churches ordain women, but in many cases, only to the office of deacon.

Various branches of the Orthodox churches, including the Greek Orthodox, currently set aside women as deaconesses. Some churches are internally divided on whether the Scriptures permit the ordination of women. When one considers the relative size of the churches (1.1 billion Roman Catholics, 300 million Orthodox, 590 million Anglicans and Protestants), it is a minority of Christian churches that ordain women. Protestants constitute about 27 percent of Christians worldwide, and most of their churches that do ordain women have only done so within the past century.

In some traditions women may be ordained to the same orders as men. In others women are restricted from certain offices. Women may be ordained bishop in the Old Catholic churches and in the Anglican/Episcopal churches in Scotland, Ireland, Wales, Cuba, Brazil, South Africa, Canada, US, Australia, Aotearoa New Zealand and Polynesia. The Church of Ireland had installed Pat Storey in 2013. On 19 September 2013, Storey was chosen by the House of Bishops to succeed Richard Clarke as Bishop of Meath and Kildare. She was consecrated to the episcopate at Christ Church Cathedral, Dublin, on 30 November 2013. She is the first woman to be elected as a bishop in the Church of Ireland and the first woman to be an Anglican Communion bishop in Ireland and Great Britain. The Church of England's General Synod voted in 2014 to allow women to be ordained to the episcopate, with Libby Lane being the first woman to be ordained bishop. Continuing Anglican churches of the world do not permit women to be ordained. In some Protestant denominations, women may serve as assistant pastors but not as pastors in charge of congregations. In some denominations, women can be ordained to be an elder or deacon. Some denominations allow for the ordination of women for certain religious orders. Within certain traditions, such as the Anglican and Lutheran, there is a diversity of theology and practice regarding ordination of women.

The ordination of lesbian, gay, bisexual or transgender clergy who are sexually active, and open about it, represents a fiercely contested subject within many mainline Protestant communities. The majority of churches are opposed to such ordinations because they view homosexuality as incompatible with Biblical teaching and traditional Christian practice. Yet there are an increasing number of Christian congregations and communities that are open to ordaining people who are gay or lesbian. These are liberal Protestant denominations, such as the Episcopal Church the United Church of Christ, and the Evangelical Lutheran Church in America, plus the small Metropolitan Community Church, founded as a church intending to minister primarily to LGBT people, and the Church of Sweden where such clergy may serve in senior clerical positions.

The issue of ordination has caused particular controversy in the worldwide Anglican Communion, following the approval of Gene Robinson to be Bishop of New Hampshire in the US Episcopal Church.





</doc>
<doc id="13633" url="https://en.wikipedia.org/wiki?curid=13633" title="Homer">
Homer

Homer (; , "Hómēros") is the legendary author of the "Iliad" and the "Odyssey", two epic poems that are the central works of ancient Greek literature. The "Iliad" is set during the Trojan War, the ten-year siege of the city of Troy by a coalition of Greek kingdoms. It focuses on a quarrel between King Agamemnon and the warrior Achilles lasting a few weeks during the last year of the war. The "Odyssey" focuses on the journey home of Odysseus, king of Ithaca, after the fall of Troy.

Many accounts of Homer's life circulated in classical antiquity, the most widespread being that he was a blind bard from Ionia, a region of central coastal Anatolia in present-day Turkey. Modern scholars consider them legends.

The Homeric Question—concerning by whom, when, where and under what circumstances the "Iliad" and "Odyssey" were composed—continues to be debated. Broadly speaking, modern scholarly opinion falls into two groups. One holds that most of the "Iliad" and (according to some) the "Odyssey" are the works of a single poet of genius. The other considers the Homeric poems to be the result of a process of working and re-working by many contributors, and that "Homer" is best seen as a label for an entire tradition.

It is generally accepted that the poems were composed at some point around the late 8th or early 7th century BC. The poems are in Homeric Greek, also known as Epic Greek, a literary language which shows a mixture of features of the Ionic and Aeolic dialects from different centuries; the predominant influence is Eastern Ionic. Most researchers believe that the poems were originally transmitted orally.

From antiquity until the present day, the influence of the Homeric epics on Western civilization has been great, inspiring many of its most famous works of literature, music, art and film. The Homeric epics were the greatest influence on ancient Greek culture and education; to Plato, Homer was simply the one who "has taught Greece" – "ten Hellada pepaideuken".

Today only the "Iliad" and "Odyssey" are associated with the name 'Homer'. In antiquity, a very large number of other works were sometimes attributed to him, including the "Homeric Hymns", the "Contest of Homer and Hesiod", the "Little Iliad", the "Nostoi", the "Thebaid", the "Cypria", the "Epigoni", the comic mini-epic "Batrachomyomachia" ("The Frog-Mouse War"), the "Margites", the "Capture of Oechalia", and the "Phocais". These claims are not considered authentic today and were by no means universally accepted in the ancient world. As with the multitude of legends surrounding Homer's life, they indicate little more than the centrality of Homer to ancient Greek culture.

Many traditions circulated in the ancient world concerning Homer, most of which are lost. Modern scholarly consensus is that they have no value as history. Some claims were established early and repeated often - that Homer was blind (taking as self-referential a passage describing the blind bard Demodocus), that he was born in Chios, that he was the son of the river Meles and the nymph Critheïs, that he was a wandering bard, that he composed a varying list of other works (the Homerica), that he died either in Ios or after failing to solve a riddle set by fishermen, and various explanations for the name 'Homer'. The two best known ancient biographies of Homer are the "Life of Homer" by the Pseudo-Herodotus and the "Contest of Homer and Hesiod".

 The study of Homer is one of the oldest topics in scholarship, dating back to antiquity. The aims of Homeric studies have changed over the course of the millennia. Ancient Greek scholars first sought to establish a canonical text of the poems and to explicate points that were difficult (whether linguistically or culturally).

The Byzantine scholars such as Eustathius of Thessalonica and John Tzetzes produced commentaries, extensions and scholia to Homer, especially in the 12th century. Virgil was more widely read during the Renaissance and Homer was often seen through a Virgilian lens.

Friedrich August Wolf's "Prolegomena ad Homerum" launched modern Homeric scholarship, arguing that the poems were assembled at a late date by literate authors from a large group of much shorter poems that were originally transmitted orally. Wolf and the 'Analyst' school, which led the field in the 19th century, sought to recover the original, authentic poems which were thought to be concealed by later excrescences. In contrast the 'Unitarians' saw the later additions as superior, the work of a single inspired poet.

In the 20th century, Milman Parry and Albert Lord, after their studies of folk bards in the Balkans, developed the "Oral-Formulaic Theory" that the Homeric poems were originally improvised. This theory found very wide scholarly acceptance. The 'Neoanalysts' sought to bridge the gap between the 'Analysts' and 'Unitarians'.

Today Homeric scholarship continues to develop. Most scholars, although disagreeing on other questions about the genesis of the poems, agree that the "Iliad" and the "Odyssey" were not produced by the same author, based on "the many differences of narrative manner, theology, ethics, vocabulary, and geographical perspective, and by the apparently imitative character of certain passages of the "Odyssey" in relation to the "Iliad"."

Some ancient scholars believed Homer to have been an eyewitness to the Trojan War; others thought he had lived up to 500 years afterwards. Contemporary scholars continue to debate the date of the poems; at one extreme Richard Janko has taken an 8th-century BC date, at the other scholars such as Gregory Nagy see 'Homer' as a continually evolving tradition which only ceased when the poems were written down in the 6th century BC. Martin West has argued that the "Iliad" echoes the poetry of Hesiod, and that it must have been composed around 660-650 BC at the earliest, with the "Odyssey" up to a generation later. A long history of oral transmission lies behind the composition of the poems, complicating the search for a precise date.

'Homer' is a name of unknown etymological origin, around which many theories were erected in antiquity; one such linkage was to the Greek ("hómēros"), "hostage" (or "surety"). The explanations suggested by modern scholars tend to mirror their position on the overall Homeric question. Nagy interprets it as "he who fits (the song) together". West has advanced both possible Greek and Phoenician etymologies.

Scholars continue to debate questions such as whether the Trojan War actually took place - and if so when and where - and to what extent the society depicted by Homer is based on his own or one which was, even at the time of the poems’ composition, known only as legend. The Homeric epics are largely set in the east and center of the Mediterranean, with some scattered references to Egypt, Ethiopia and other distant lands, in a warlike society that resembles that of the Greek world slightly before the hypothesized date of the poems' composition.

In ancient Greek chronology, the sack of Troy was dated to 1184 BC. By the nineteenth century there was widespread scholarly skepticism that Troy or the Trojan War had ever existed, but in 1873 Heinrich Schliemann announced to the world that he had discovered the ruins of Homer’s Troy at Hissarlik in modern Turkey. Some contemporary scholars think the destruction of Troy VIIa "circa" 1220 BC was the origin of the myth of the Trojan War, others that the poem was inspired by multiple similar sieges that took place over the centuries.

Homer depicts customs that are not characteristic of any one historical period. For instance, his heroes use bronze weapons, characteristic of the Bronze Age rather than the later Iron Age during which the poems were composed; yet they are cremated (an Iron Age practice) rather than buried (as they were in the Bronze Age).

In the "Iliad" 10.260-265, Odysseus is described as wearing a helmet made of boar's tusks. Such helmets were not worn in Homer's time, but were commonly worn by aristocratic warriors between 1600 and 1150 BC. The decipherment of Linear B in the 1950s by Michael Ventris and continued archaeological investigation has increased modern scholars' understanding of Aegean civilisation, which in many ways resembles the ancient Near East more than the society described by Homer.

The Homeric epics are written in an artificial literary language or 'Kunstsprache' only used in epic hexameteric poetry. Homeric Greek shows features of multiple regional Greek dialects and periods, but is fundamentally based on Ionic Greek, in keeping with the tradition that Homer was from Ionia. Linguistic analysis suggests that the "Iliad" was composed slightly before the "Odyssey", and that Homeric formulae preserve older features than other parts of the poems.

The Homeric poems were composed in unrhymed dactylic hexameter; ancient Greek metre was quantity rather than stress based. Homer frequently uses set phrases such as epithets ('crafty Odysseus', 'rosy-fingered Dawn', 'owl-eyed Athena', etc.), Homeric formulae ('and then answered [him/her], Agamemnon, king of men', 'when the early-born rose-fingered Dawn came to light', 'thus he/she spoke'), simile, type scenes, ring composition and repetition. These habits aid the extemporizing bard, and are characteristic of oral poetry. For instance, the main words of a Homeric sentence are generally placed towards the beginning, whereas literate poets like Virgil or Milton use longer and more complicated syntactical structures. Homer then expands on these ideas in subsequent clauses; this technique is called parataxis.

The so-called 'type scenes' ("typischen Scenen"), were named by Walter Arend in 1933. He noted that Homer often, when describing frequently recurring activities such as eating, praying, fighting and dressing, used blocks of set phrases in sequence that were then elaborated by the poet. The 'Analyst' school had considered these repetitions as un-Homeric, whereas Arend interpreted them philosophically. Parry and Lord noted that these conventions are found in many other cultures.

'Ring composition' or chiastic structure (when a phrase or idea is repeated at both the beginning and end of a story, or a series of such ideas first appears in the order A, B, C... before being reversed as ...C, B, A) has been observed in the Homeric epics. Opinion differs as to whether these occurrences are a conscious artistic device, a mnemonic aid or a spontaneous feature of human storytelling.

Both of the Homeric poems begin with an invocation to the Muse. In the "Iliad", the poet invokes her to sing of "the anger of Achilles", and, in the "Odyssey", he asks her to sing of "the man of many ways". A similar opening was later employed by Virgil in his "Aeneid".

The orally transmitted Homeric poems were put into written form at some point between the 8th and 6th centuries BC. Some scholars believe that they were dictated by the poet; Albert Lord noted that, in the process of dictating, the Balkan bards he recorded revised and extended their lays. Some scholars hypothesize that a similar process occurred when the Homeric poems were first written.

Other scholars such as Gregory Nagy hold that, after the poems were formed in the 8th century, they were orally transmitted with little deviation until they were written down in the 6th century. After textualisation, the poems were each divided into 24 rhapsodes, today referred to as books, and labelled by the letters of the Greek alphabet. These divisions probably date from before 200 BC, and may have been made by Homer.

In antiquity, it was widely held that the Homeric poems were collected and organised in Athens by the tyrant Pesistratos (died 528/7 BC), in the famed 'Pesistratean recension'. From around 150 BC the text seems to have become relatively established. After the establishment of the Library of Alexandria, Homeric scholars such as Zenodotus of Ephesus, Aristophanes of Byzantium and in particular Aristarchus of Samothrace helped establish a canonical text.

The first printed edition of Homer was produced in 1488 in Milan. Today scholars use medieval manuscripts, papyri and other sources; some argue for a 'multi-text' view, rather than seeking a single definitive text. The 19th century edition of Arthur Ludwich mainly follows Aristarchus's work, whereas van Thiel's (1991,1996) follows the medieval vulgate. Others, such as Martin West (1998-2000) or T.W. Allen fall somewhere between these two extremes.

</div>



This is a partial list of translations into English of Homer's "Iliad" and "Odyssey".







</doc>
<doc id="13635" url="https://en.wikipedia.org/wiki?curid=13635" title="Hugo Gernsback">
Hugo Gernsback

Hugo Gernsback (; born Hugo Gernsbacher, August 16, 1884 – August 19, 1967) was a Luxembourgish-American inventor, writer, editor, and magazine publisher, best known for publications including the first science fiction magazine. His contributions to the genre as publisher–although not as a writer–were so significant that, along with the novelists H. G. Wells and Jules Verne, he is sometimes called "The Father of Science Fiction". In his honour, annual awards presented at the World Science Fiction Convention are named the "Hugos".

Gernsback was born in 1884 in Luxembourg City, to Berta (Dürlacher), a housewife, and Moritz Gernsbacher, a winemaker. His family was Jewish. Gernsback emigrated to the United States in 1904 and later became a naturalized citizen. He married three times: to Rose Harvey in 1906, Dorothy Kantrowitz in 1921, and Mary Hancher in 1951. In 1925, he founded radio station WRNY, which broadcast from the 18th floor of the Roosevelt Hotel in New York City. In 1928, WRNY gave some of the first television broadcasts. During the show, audio stopped and each artist waved or bowed onscreen. When audio resumed, they performed. Gernsback is also considered a pioneer in amateur radio.

Before helping to create science fiction, Gernsback was an entrepreneur in the electronics industry, importing radio parts from Europe to the United States and helping to popularize amateur "wireless". In April 1908 he founded "Modern Electrics", the world's first magazine about both electronics and radio, called "wireless" at the time. While the cover of the magazine itself states it was a catalog, most historians note that it contained articles, features, and plotlines, qualifying it as a magazine.

Under its auspices, in January 1909, he founded the Wireless Association of America, which had 10,000 members within a year. In 1912, Gernsback said that he estimated 400,000 people in the U.S. were involved in amateur radio. In 1913, he founded a similar magazine, "The Electrical Experimenter", which became "Science and Invention" in 1920. It was in these magazines that he began including scientific fiction stories alongside science journalism—including his own novel "Ralph 124C 41+" which he ran for 12 months from April 1911 in "Modern Electrics".

He died at Roosevelt Hospital in New York City on August 19, 1967.

Gernsback provided a forum for the modern genre of science fiction in 1926 by founding the first magazine dedicated to it, "Amazing Stories". The inaugural April issue comprised a one-page editorial and reissues of six stories, three less than ten years old and three by Poe, Verne, and Wells. He said he became interested in the concept after reading a translation of the work of Percival Lowell as a child. His idea of a perfect science fiction story was "75 percent literature interwoven with 25 percent science". He also played an important role in starting science fiction fandom, by organizing the Science Fiction League and by publishing the addresses of people who wrote letters to his magazines. Fans began to organize, and became aware of themselves as a movement, a social force; this was probably decisive for the subsequent history of the genre. He also created the term "science fiction", though he preferred the term "scientifiction".

In 1929, he lost ownership of his first magazines after a bankruptcy lawsuit. There is some debate about whether this process was genuine, manipulated by publisher Bernarr Macfadden, or was a Gernsback scheme to begin another company. After losing control of "Amazing Stories", Gernsback founded two new science fiction magazines, "Science Wonder Stories" and "Air Wonder Stories". A year later, due to Depression-era financial troubles, the two were merged into "Wonder Stories", which Gernsback continued to publish until 1936, when it was sold to Thrilling Publications and renamed "Thrilling Wonder Stories". Gernsback returned in 1952–53 with "Science-Fiction Plus".

Gernsback was noted for sharp (and sometimes shady) business practices, and for paying his writers extremely low fees or not paying them at all. H. P. Lovecraft and Clark Ashton Smith referred to him as "Hugo the Rat".

As Barry Malzberg has said:

Gernsback's venality and corruption, his sleaziness and his utter disregard for the financial rights of authors, have been well documented and discussed in critical and fan literature. That the founder of genre science fiction who gave his name to the field's most prestigious award and who was the Guest of Honor at the 1952 Worldcon was pretty much a crook (and a contemptuous crook who stiffed his writers but paid himself $100K a year as President of Gernsback Publications) has been clearly established. 

Jack Williamson, who had to hire an attorney associated with the American Fiction Guild to force Gernsback to pay him, summed up his importance for the genre:

At any rate, his main influence in the field was simply to start Amazing and Wonder Stories and get SF out to the public newsstands—and to name the genre he had earlier called "scientifiction."

Frederik Pohl said in 1965 that Gernsback's "Amazing Stories" published "the kind of stories Gernsback himself used to write: a sort of animated catalogue of gadgets". Gernsback's fiction includes the novel "Ralph 124C 41+"; the title is a pun on the phrase "one to foresee for many" ("one plus"). Even though "Ralph 124C 41+" has been described as pioneering many ideas and themes found in later SF work, it has often been neglected due to most critics deem as poor artistic quality. Author Brian Aldiss called the story a "tawdry illiterate tale" and a "sorry concoction", while author and editor Lester del Rey called it "simply dreadful." While most other modern critics have little positive to say about the story's writing, "Ralph 124C 41+" is considered by science fiction critic Gary Westfahl as "essential text for all studies of science fiction."

Gernsback's second novel, "Baron Münchausen's Scientific Adventures", was serialized in "Amazing Stories" in 1928.

Gernsback's third (and final) novel, "Ultimate World", written c. 1958, was not published until 1971. Lester del Rey described it simply as "a bad book", marked more by routine social commentary than by scientific insight or extrapolation. James Blish, in a caustic review, described the novel as "incompetent, pedantic, graceless, incredible, unpopulated and boring" and concluded that its publication "accomplishes nothing but the placing of a blot on the memory of a justly honored man."

Gernsback combined his fiction and science into "Everyday Science and Mechanics" magazine, serving as the editor in the 1930s.

The Hugo Awards or "Hugos" are the annual achievement awards presented at the World Science Fiction Convention, selected in a process that ends with vote by current Convention members. They originated and acquired the "Hugo" nickname during the 1950s and were formally defined as a convention responsibility under the name "Science Fiction Achievement Awards" early in the 1960s. The nickname soon became almost universal and its use legally protected; "Hugo Award(s)" replaced the longer name in all official uses after the 1991 cycle.

In 1960 Gernsback received a special Hugo Award as "The Father of Magazine Science Fiction".

The Science Fiction and Fantasy Hall of Fame inducted him in 1996, its inaugural class of two deceased and two living persons.

Science fiction author Brian W. Aldiss held a contrary view about Gernsback's contributions: "It is easy to argue that Hugo Gernsback ... was one of the worst disasters to hit the science fiction field ... Gernsback himself was utterly without any literary understanding. He created dangerous precedents which many later editors in the field followed."

Gernsback made significant contributions to the growth of early broadcasting, mostly through his efforts as a publisher. He originated the industry of specialized publications for radio with "Modern Electrics" and "Electrical Experimenter". Later on, and more influentially, he published "Radio News", which would have the largest readership among radio magazines in radio broadcasting's formative years. He edited "Radio News" until 1929. For a short time he hired John F. Rider to be editor. Rider was a former engineer working with the US Army Signal Corps and a radio engineer for A. H. Grebe, a radio manufacturer. However Rider would soon leave Gernsback and form his own publishing company, John F. Rider Publisher, New York around 1931.

Gernsback made use of the magazine to promote his own interests, including having his radio station's call letters on the cover starting in 1925. WRNY and "Radio News" were used to cross-promote each other, with programs on his station often used to discuss articles he had published, and articles in the magazine often covering program activities at WRNY. He also advocated for future directions in innovation and regulation of radio. The magazine contained many drawings and diagrams, encouraging radio listeners of the 1920s to experiment themselves to improve the technology. WRNY was often used as a laboratory to see if various radio inventions were worthwhile.

Articles that were published about television were also tested in this manner when the radio station was used to send pictures to experimental television receivers in August 1928. The technology, however, required sending sight and sound one after the other rather than sending both at the same time, as WRNY only broadcast on one channel. Such experiments were expensive, eventually contributing to Gernsback's Experimenter Publishing Company going into bankruptcy in 1929. WRNY was sold to Aviation Radio, who maintained the channel part-time to broadcast aviation weather reports and related feature programs. It left the air in 1934.

Gernsback held 80 patents by the time of his death in New York City on August 19, 1967.

Novels:


Short stories:








</doc>
<doc id="13636" url="https://en.wikipedia.org/wiki?curid=13636" title="History of computing hardware">
History of computing hardware

The history of computing hardware covers the developments from early simple devices to aid calculation to modern day computers. Before the 20th century, most calculations were done by humans. Early mechanical tools to help humans with digital calculations, such as the abacus, were called "calculating machines", called by proprietary names, or referred to as calculators. The machine operator was called the computer.

The first aids to computation were purely mechanical devices which required the operator to set up the initial values of an elementary arithmetic operation, then manipulate the device to obtain the result. Later, computers represented numbers in a continuous form, for instance distance along a scale, rotation of a shaft, or a voltage. Numbers could also be represented in the form of digits, automatically manipulated by a mechanical mechanism. Although this approach generally required more complex mechanisms, it greatly increased the precision of results. A series of breakthroughs, such as miniaturized transistor computers, and the integrated circuit, caused digital computers to largely replace analog computers. The cost of computers gradually became so low that by the 1990s, personal computers, and then, in the 2000s, mobile computers, (smartphones and tablets) became ubiquitous in industrialized countries.

Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers. The use of counting rods is one example. The abacus was early used for arithmetic tasks. What we now call the Roman abacus was used in Babylonia as early as c. 2700–2300 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.

Several analog computers were constructed in ancient and medieval times to perform astronomical calculations. These included the south-pointing chariot (c. 1050–771 BC) from ancient China, and the astrolabe and Antikythera mechanism from the Hellenistic world (c. 150–100 BC). In Roman Egypt, Hero of Alexandria (c. 10–70 AD) made mechanical devices including automata and a programmable cart. Other early mechanical devices used to perform one or another type of calculations include the planisphere and other mechanical computing devices invented by Abu Rayhan al-Biruni (c. AD 1000); the equatorium and universal latitude-independent astrolabe by Abū Ishāq Ibrāhīm al-Zarqālī (c. AD 1015); the astronomical analog computers of other medieval Muslim astronomers and engineers; and the astronomical clock tower of Su Song (1094) during the Song dynasty. The castle clock, a hydropowered mechanical astronomical clock invented by Ismail al-Jazari in 1206, was the first programmable analog computer. Ramon Llull invented the Lullian Circle: a notional machine for calculating answers to philosophical questions (in this case, to do with Christianity) via logical combinatorics. This idea was taken up by Leibniz centuries later, and is thus one of the founding elements in computing and information science.

Scottish mathematician and physicist John Napier discovered that the multiplication and division of numbers could be performed by the addition and subtraction, respectively, of the logarithms of those numbers. While producing the first logarithmic tables, Napier needed to perform many tedious multiplications. It was at this point that he designed his 'Napier's bones', an abacus-like device that greatly simplified calculations that involved multiplication and division.
Since real numbers can be represented as distances or intervals on a line, the slide rule was invented in the 1620s, shortly after Napier's work, to allow multiplication and division operations to be carried out significantly faster than was previously possible. Edmund Gunter built a calculating device with a single logarithmic scale at the University of Oxford. His device greatly simplified arithmetic calculations, including multiplication and division. William Oughtred greatly improved this in 1630 with his circular slide rule. He followed this up with the modern slide rule in 1632, essentially a combination of two Gunter rules, held together with the hands. Slide rules were used by generations of engineers and other mathematically involved professional workers, until the invention of the pocket calculator.

Wilhelm Schickard, a German polymath, designed a calculating machine in 1623 which combined a mechanised form of Napier's rods with the world's first mechanical adding machine built into the base. Because it made use of a single-tooth gear there were circumstances in which its carry mechanism would jam. A fire destroyed at least one of the machines in 1624 and it is believed Schickard was too disheartened to build another.

In 1642, while still a teenager, Blaise Pascal started some pioneering work on calculating machines and after three years of effort and 50 prototypes he invented a mechanical calculator. He built twenty of these machines (called Pascal's calculator or Pascaline) in the following ten years. Nine Pascalines have survived, most of which are on display in European museums. A continuing debate exists over whether Schickard or Pascal should be regarded as the "inventor of the mechanical calculator" and the range of issues to be considered is discussed elsewhere.

Gottfried Wilhelm von Leibniz invented the stepped reckoner and his famous stepped drum mechanism around 1672. He attempted to create a machine that could be used not only for addition and subtraction but would utilise a moveable carriage to enable long multiplication and division. Leibniz once said "It is unworthy of excellent men to lose hours like slaves in the labour of calculation which could safely be relegated to anyone else if machines were used." However, Leibniz did not incorporate a fully successful carry mechanism. Leibniz also described the binary numeral system, a central ingredient of all modern computers. However, up to the 1940s, many subsequent designs (including Charles Babbage's machines of the 1822 and even ENIAC of 1945) were based on the decimal system.

Around 1820, Charles Xavier Thomas de Colmar created what would over the rest of the century become the first successful, mass-produced mechanical calculator, the Thomas Arithmometer. It could be used to add and subtract, and with a moveable carriage the operator could also multiply, and divide by a process of long multiplication and long division. It utilised a stepped drum similar in conception to that invented by Leibniz. Mechanical calculators remained in use until the 1970s.

In 1804, Joseph-Marie Jacquard developed a loom in which the pattern being woven was controlled by a paper tape constructed from punched cards. The paper tape could be changed without changing the mechanical design of the loom. This was a landmark achievement in programmability. His machine was an improvement over similar weaving looms. Punched cards were preceded by punch bands, as in the machine proposed by Basile Bouchon. These bands would inspire information recording for automatic pianos and more recently numerical control machine tools.
In the late 1880s, the American Herman Hollerith invented data storage on punched cards that could then be read by a machine. To process these punched cards he invented the tabulator, and the keypunch machine. His machines used electromechanical relays and counters. Hollerith's method was used in the 1890 United States Census. That census was processed two years faster than the prior census had been. Hollerith's company eventually became the core of IBM.

By 1920, electromechanical tabulating machines could add, subtract and print accumulated totals. Machine functions were directed by inserting dozens of wire jumpers into removable control panels. When the United States instituted Social Security in 1935, IBM punched card systems were used to process records of 26 million workers. Punched cards became ubiquitous in industry and government for accounting and administration.

Leslie Comrie's articles on punched card methods and W.J. Eckert's publication of "Punched Card Methods in Scientific Computation" in 1940, described punched card techniques sufficiently advanced to solve some differential equations or perform multiplication and division using floating point representations, all on punched cards and unit record machines. Such machines were used during World War II for cryptographic statistical processing, as well as a vast number of administrative uses. The Astronomical Computing Bureau, Columbia University, performed astronomical calculations representing the state of the art in computing.

The book "IBM and the Holocaust" by Edwin Black outlines the ways in which IBM's technology helped facilitate Nazi genocide through generation and tabulation of punch cards based upon national census data. "See also: Dehomag"

By the 20th century, earlier mechanical calculators, cash registers, accounting machines, and so on were redesigned to use electric motors, with gear position as the representation for the state of a variable. The word "computer" was a job title assigned to primarily women who used these calculators to perform mathematical calculations. By the 1920s, British scientist Lewis Fry Richardson's interest in weather prediction led him to propose human computers and numerical analysis to model the weather; to this day, the most powerful computers on Earth are needed to adequately model its weather using the Navier–Stokes equations.

Companies like Friden, Marchant Calculator and Monroe made desktop mechanical calculators from the 1930s that could add, subtract, multiply and divide. In 1948, the Curta was introduced by Austrian inventor Curt Herzstark. It was a small, hand-cranked mechanical calculator and as such, a descendant of Gottfried Leibniz's Stepped Reckoner and Thomas's Arithmometer.

The world's first "all-electronic desktop" calculator was the British Bell Punch ANITA, released in 1961. It used vacuum tubes, cold-cathode tubes and Dekatrons in its circuits, with 12 cold-cathode "Nixie" tubes for its display. The ANITA sold well since it was the only electronic desktop calculator available, and was silent and quick. The tube technology was superseded in June 1963 by the U.S. manufactured Friden EC-130, which had an all-transistor design, a stack of four 13-digit numbers displayed on a CRT, and introduced reverse Polish notation (RPN).

Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer", he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. It employed ordinary base-10 fixed-point arithmetic.

The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.

There was to be a store, or memory, capable of holding 1,000 numbers of 40 decimal digits each (ca. 16.7 kB). An arithmetical unit, called the "mill", would be able to perform all four arithmetic operations, plus comparisons and optionally square roots. Initially it was conceived as a difference engine curved back upon itself, in a generally circular layout, with the long store exiting off to one side. (Later drawings depict a regularized grid layout.) Like the central processing unit (CPU) in a modern computer, the mill would rely upon its own internal procedures, roughly equivalent to microcode in modern CPUs, to be stored in the form of pegs inserted into rotating drums called "barrels", to carry out some of the more complex instructions the user's program might specify.
The programming language to be employed by users was akin to modern day assembly languages. Loops and conditional branching were possible, and so the language as conceived would have been Turing-complete as later defined by Alan Turing. Three different types of punch cards were used: one for arithmetical operations, one for numerical constants, and one for load and store operations, transferring numbers from the store to the arithmetical unit or back. There were three separate readers for the three types of cards.

The machine was about a century ahead of its time. However, the project was slowed by various problems including disputes with the chief machinist building parts for it. All the parts for his machine had to be made by hand—this was a major problem for a machine with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Ada Lovelace, Lord Byron's daughter, translated and added notes to the ""Sketch of the Analytical Engine"" by Luigi Federico Menabrea. This appears to be the first published description of programming, so Ada Lovelace is widely regarded as the first computer programmer.

Following Babbage, although unaware of his earlier work, was Percy Ludgate, an accountant from Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.

In the first half of the 20th century, analog computers were considered by many to be the future of computing. These devices used the continuously changeable aspects of physical phenomena such as electrical, mechanical, or hydraulic quantities to model the problem being solved, in contrast to digital computers that represented varying quantities symbolically, as their numerical values change. As an analog computer does not use discrete values, but rather continuous values, processes cannot be reliably repeated with exact equivalence, as they can with Turing machines.

The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson, later Lord Kelvin, in 1872. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location and was of great utility to navigation in shallow waters. His device was the foundation for further developments in analog computing.

The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin. He explored the possible construction of such calculators, but was stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output.
An important advance in analog computing was the development of the first fire-control systems for long range ship gunlaying. When gunnery ranges increased dramatically in the late 19th century it was no longer a simple matter of calculating the proper aim point, given the flight times of the shells. Various spotters on board the ship would relay distance measures and observations to a central plotting station. There the fire direction teams fed in the location, speed and direction of the ship and its target, as well as various adjustments for Coriolis effect, weather effects on the air, and other adjustments; the computer would then output a firing solution, which would be fed to the turrets for laying. In 1912, British engineer Arthur Pollen developed the first electrically powered mechanical analogue computer (called at the time the Argo Clock). It was used by the Imperial Russian Navy in World War I. The alternative Dreyer Table fire control system was fitted to British capital ships by mid-1916.

Mechanical devices were also used to aid the accuracy of aerial bombing. Drift Sight was the first such aid, developed by Harry Wimperis in 1916 for the Royal Naval Air Service; it measured the wind speed from the air, and used that measurement to calculate the wind's effects on the trajectory of the bombs. The system was later improved with the Course Setting Bomb Sight, and reached a climax with World War II bomb sights, Mark XIV bomb sight (RAF Bomber Command) and the Norden (United States Army Air Forces).

The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927, which built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious; the most powerful was constructed at the University of Pennsylvania's Moore School of Electrical Engineering, where the ENIAC was built.

A fully electronic analog computer was built by Helmut Hölzer in 1942 at Peenemünde Army Research Center

By the 1950s the success of digital electronic computers had spelled the end for most analog computing machines, but hybrid analog computers, controlled by digital electronics, remained in substantial use into the 1950s and 1960s, and later in some specialized applications.

The principle of the modern computer was first described by computer scientist Alan Turing, who set out the idea in his seminal 1936 paper, "On Computable Numbers". Turing reformulated Kurt Gödel's 1931 results on the limits of proof and computation, replacing Gödel's universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. He proved that some such machine would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the "Entscheidungsproblem" by first showing that the halting problem for Turing machines is undecidable: in general, it is not possible to decide algorithmically whether a given Turing machine will ever halt.

He also introduced the notion of a 'Universal Machine' (now known as a Universal Turing machine), with the idea that such a machine could perform the tasks of any other machine, or in other words, it is provably capable of computing anything that is computable by executing a program stored on tape, allowing the machine to be programmable. Von Neumann acknowledged that the central concept of the modern computer was due to this paper. Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.

The era of modern computing began with a flurry of development before and during World War II. Most digital computers built in this period were electromechanical – electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes.

The Z2 was one of the earliest examples of an electromechanical relay computer, and was created by German engineer Konrad Zuse in 1940. It was an improvement on his earlier Z1; although it used the same mechanical memory, it replaced the arithmetic and control logic with electrical relay circuits.
In the same year, electro-mechanical devices called bombes were built by British cryptologists to help decipher German Enigma-machine-encrypted secret messages during World War II. The bombes' initial design was created in 1939 at the UK Government Code and Cypher School (GC&CS) at Bletchley Park by Alan Turing, with an important refinement devised in 1940 by Gordon Welchman. The engineering design and construction was the work of Harold Keen of the British Tabulating Machine Company. It was a substantial development from a device that had been designed in 1938 by Polish Cipher Bureau cryptologist Marian Rejewski, and known as the "cryptologic bomb" (Polish: ""bomba kryptologiczna"").

In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer. The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz. Program code and data were stored on punched film. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Replacement of the hard-to-implement decimal system (used in Charles Babbage's earlier design) by the simpler binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time. The Z3 was probably a complete Turing machine. In two 1936 patent applications, Zuse also anticipated that machine instructions could be stored in the same storage used for data—the key insight of what became known as the von Neumann architecture, first implemented in Britain in the Manchester Baby of 1948.

Zuse suffered setbacks during World War II when some of his machines were destroyed in the course of Allied bombing campaigns. Apparently his work remained largely unknown to engineers in the UK and US until much later, although at least IBM was aware of it as it financed his post-war startup company in 1946 in return for an option on Zuse's patents.

In 1944, the Harvard Mark I was constructed at IBM's Endicott laboratories; it was a similar general purpose electro-mechanical computer to the Z3, but was not quite Turing-complete.

The term digital is suggested by George Stibitz and refers to all applications based on signals with two states – low (0) and high (1). That is why the decimal and binary computing are two ways to implement digital computing. A mathematical basis of digital computing is Boolean algebra, developed by the British mathematician George Boole in his work "The Laws of Thought", published in 1854. His Boolean algebra was further refined in the 1860s by William Jevons and Charles Sanders Peirce, and was first presented systematically by Ernst Schröder and A. N. Whitehead. In 1879 Gottlob Frege develops the formal approach to logic and proposes the first logic language for logical equations.

In the 1930s and working independently, American electronic engineer Claude Shannon and Soviet logician Victor Shestakov both showed a one-to-one correspondence between the concepts of Boolean logic and certain electrical circuits, now called logic gates, which are now ubiquitous in digital computers. They showed that electronic relays and switches can realize the expressions of Boolean algebra. This thesis essentially founded practical digital circuit design.

Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. Machines such as the Z3, the Atanasoff–Berry Computer, the Colossus computers, and the ENIAC were built by hand, using circuits containing relays or valves (vacuum tubes), and often used punched cards or punched paper tape for input and as the main (non-volatile) storage medium.

The engineer Tommy Flowers joined the telecommunications branch of the General Post Office in 1926. While working at the research station in Dollis Hill in the 1930s, he began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation 5 years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.

In the US, in the period summer 1937 to the fall of 1939 Arthur Dickinson (IBM) invented the first digital electronic computer. This calculating device was fully electronic – control, calculations and output (the first electronic display). John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed the Atanasoff–Berry Computer (ABC) in 1942, the first binary electronic digital calculating device. This design was semi-electronic (electro-mechanical control and electronic calculations), and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory. However, its paper card writer/reader was unreliable and the regenerative drum contact system was mechanical. The machine's special-purpose nature and lack of changeable, stored program distinguish it from modern computers.

During World War II, the British at Bletchley Park (40 miles north of London) achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes. They ruled out possible Enigma settings by performing chains of logical deductions implemented electrically. Most possibilities led to a contradiction, and the few remaining could be tested by hand.

The Germans also developed a series of teleprinter encryption systems, quite different from Enigma. The Lorenz SZ 40/42 machine was used for high-level Army communications, termed "Tunny" by the British. The first intercepts of Lorenz messages began in 1941. As part of an attack on Tunny, Max Newman and his colleagues helped specify the Colossus.

Tommy Flowers, still a senior engineer at the Post Office Research Station was recommended to Max Newman by Alan Turing and spent eleven months from early February 1943 designing and building the first Colossus. After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944 and attacked its first message on 5 February.
Colossus was the world's first electronic digital programmable computer. It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1500 thermionic valves (tubes), but Mark II with 2400 valves, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process. Mark 2 was designed while Mark 1 was being constructed. Allen Coombs took over leadership of the Colossus Mark 2 project when Tommy Flowers moved on to other projects.

Colossus was able to process 5,000 characters per second with the paper tape moving at . Sometimes, two or more Colossus computers tried different possibilities simultaneously in what now is called parallel computing, speeding the decoding process by perhaps as much as double the rate of comparison.

Colossus included the first ever use of shift registers and systolic arrays, enabling five simultaneous tests, each involving up to 100 Boolean calculations, on each of the five channels on the punched tape (although in normal operation only one or two channels were examined in any run). Initially Colossus was only used to determine the initial wheel positions used for a particular message (termed wheel setting). The Mark 2 included mechanisms intended to help determine pin patterns (wheel breaking). Both models were programmable using switches and plug panels in a way their predecessors had not been.
Without the use of these machines, the Allies would have been deprived of the very valuable intelligence that was obtained from reading the vast quantity of encrypted high-level telegraphic messages between the German High Command (OKW) and their army commands throughout occupied Europe. Details of their existence, design, and use were kept secret well into the 1970s. Winston Churchill personally issued an order for their destruction into pieces no larger than a man's hand, to keep secret that the British were capable of cracking Lorenz SZ cyphers (from German rotor stream cipher machines) during the oncoming Cold War. Two of the machines were transferred to the newly formed GCHQ and the others were destroyed. As a result, the machines were not included in many histories of computing. A reconstructed working copy of one of the Colossus machines is now on display at Bletchley Park.

The US-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. It was unambiguously a Turing-complete device and could compute any problem that would fit into its memory. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches.

It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High-speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors. One of its major engineering feats was to minimize the effects of tube burnout, which was a common problem in machine reliability at that time. The machine was in almost constant use for the next ten years.

Early computing machines had fixed programs. For example, a desk calculator is a fixed program computer. It can do basic mathematics, but it cannot be used as a word processor or a gaming console. Changing the program of a fixed-program machine requires re-wiring, re-structuring, or re-designing the machine. The earliest computers were not so much "programmed" as they were "designed". "Reprogramming", when it was possible at all, was a laborious process, starting with flowcharts and paper notes, followed by detailed engineering designs, and then the often-arduous process of physically re-wiring and re-building the machine. With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation.

The theoretical basis for the stored-program computer had been composed by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began his work on developing an electronic stored-program digital computer. His 1945 report ‘Proposed Electronic Calculator’ was the first specification for such a device.

Meanwhile, John von Neumann at the Moore School of Electrical Engineering, University of Pennsylvania, circulated his "First Draft of a Report on the EDVAC" in 1945. Although substantially similar to Turing's design and containing comparatively little engineering detail, the computer architecture it outlined became known as the "von Neumann architecture". Turing presented a more detailed paper to the National Physical Laboratory (NPL) Executive Committee in 1946, giving the first reasonably complete design of a stored-program computer, a device he called the Automatic Computing Engine (ACE). However, the better-known EDVAC design of John von Neumann, who knew of Turing's theoretical work, received more publicity, despite its incomplete nature and questionable lack of attribution of the sources of some of the ideas.

Turing thought that the speed and the size of computer memory were crucial elements, so he proposed a high-speed memory of what would today be called 25 KB, accessed at a speed of 1 MHz. The ACE implemented subroutine calls, whereas the EDVAC did not, and the ACE also used "Abbreviated Computer Instructions," an early form of programming language.

The Manchester Baby was the world's first electronic stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.

The machine was not intended to be a practical computer but was instead designed as a testbed for the Williams tube, the first random-access digital storage device. Invented by Freddie Williams and Tom Kilburn at the University of Manchester in 1946 and 1947, it was a cathode ray tube that used an effect called secondary emission to temporarily store electronic binary data, and was used successfully in several early computers.

Although the computer was considered "small and primitive" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer. As soon as the Baby had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.

The Baby had a 32-bit word length and a memory of 32 words. As it was designed to be the simplest possible stored-program computer, the only arithmetic operations implemented in hardware were subtraction and negation; other arithmetic operations were implemented in software. The first of three programs written for the machine found the highest proper divisor of 2 (262,144), a calculation that was known would take a long time to run—and so prove the computer's reliability—by testing every integer from 2 - 1 downwards, as division was implemented by repeated subtraction of the divisor. The program consisted of 17 instructions and ran for 52 minutes before reaching the correct answer of 131,072, after the Baby had performed 3.5 million operations (for an effective CPU speed of 1.1 kIPS).

The Experimental machine led on to the development of the Manchester Mark 1 at the University of Manchester. Work began in August 1948, and the first version was operational by April 1949; a program written to search for Mersenne primes ran error-free for nine hours on the night of 16/17 June 1949.
The machine's successful operation was widely reported in the British press, which used the phrase "electronic brain" in describing it to their readers.

The computer is especially historically significant because of its pioneering inclusion of index registers, an innovation which made it easier for a program to read sequentially through an array of words in memory. Thirty-four patents resulted from the machine's development, and many of the ideas behind its design were incorporated in subsequent commercial products such as the and 702 as well as the Ferranti Mark 1. The chief designers, Frederic C. Williams and Tom Kilburn, concluded from their experiences with the Mark 1 that computers would be used more in scientific roles than in pure mathematics. In 1951 they started development work on Meg, the Mark 1's successor, which would include a floating point unit.

The other contender for being the first recognizably modern digital stored-program computer was the EDSAC, designed and constructed by Maurice Wilkes and his team at the University of Cambridge Mathematical Laboratory in England at the University of Cambridge in 1949. The machine was inspired by John von Neumann's seminal "First Draft of a Report on the EDVAC" and was one of the first usefully operational electronic digital stored-program computer.

EDSAC ran its first programs on 6 May 1949, when it calculated a table of squares and a list of prime numbers.The EDSAC also served as the basis for the first commercially applied computer, the LEO I, used by food manufacturing company J. Lyons & Co. Ltd. EDSAC 1 and was finally shut down on 11 July 1958, having been superseded by EDSAC 2 which stayed in use until 1965.

ENIAC inventors John Mauchly and J. Presper Eckert proposed the EDVAC's construction in August 1944, and design work for the EDVAC commenced at the University of Pennsylvania's Moore School of Electrical Engineering, before the ENIAC was fully operational. The design implemented a number of important architectural and logical improvements conceived during the ENIAC's construction, and a high speed serial access memory. However, Eckert and Mauchly left the project and its construction floundered.

It was finally delivered to the U.S. Army's Ballistics Research Laboratory at the Aberdeen Proving Ground in August 1949, but due to a number of problems, the computer only began operation in 1951, and then only on a limited basis.

The first commercial computer was the Ferranti Mark 1, built by Ferranti and delivered to the University of Manchester in February 1951. It was based on the Manchester Mark 1. The main improvements over the Manchester Mark 1 were in the size of the primary storage (using random access Williams tubes), secondary storage (using a magnetic drum), a faster multiplier, and additional instructions. The basic cycle time was 1.2 milliseconds, and a multiplication could be completed in about 2.16 milliseconds. The multiplier used almost a quarter of the machine's 4,050 vacuum tubes (valves). A second machine was purchased by the University of Toronto, before the design was revised into the Mark 1 Star. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.

In October 1947, the directors of J. Lyons & Company, a British catering company famous for its teashops but with strong interests in new office management techniques, decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951 and ran the world's first regular routine office computer job. On 17 November 1951, the J. Lyons company began weekly operation of a bakery valuations job on the LEO (Lyons Electronic Office). This was the first business to go live on a stored program computer.
In June 1951, the UNIVAC I (Universal Automatic Computer) was delivered to the U.S. Census Bureau. Remington Rand eventually sold 46 machines at more than US$1 million each ($ as of 2018). UNIVAC was the first "mass produced" computer. It used 5,200 vacuum tubes and consumed 125 kW of power. Its primary storage was serial-access mercury delay lines capable of storing 1,000 words of 11 decimal digits plus sign (72-bit words).

IBM introduced a smaller, more affordable computer in 1954 that proved very popular. The IBM 650 weighed over 900 kg, the attached power supply weighed around 1350 kg and both were held in separate cabinets of roughly 1.5 meters by 0.9 meters by 1.8 meters. It cost US$500,000 ($ as of 2018) or could be leased for US$3,500 a month ($ as of 2018). Its drum memory was originally 2,000 ten-digit words, later expanded to 4,000 words. Memory limitations such as this were to dominate programming for decades afterward. The program instructions were fetched from the spinning drum as the code ran. Efficient execution using drum memory was provided by a combination of hardware architecture: the instruction format included the address of the next instruction; and software: the Symbolic Optimal Assembly Program, SOAP, assigned instructions to the optimal addresses (to the extent possible by static analysis of the source program). Thus many instructions were, when needed, located in the next row of the drum to be read and additional wait time for drum rotation was not required.

In 1951, British scientist Maurice Wilkes developed the concept of microprogramming from the realisation that the central processing unit of a computer could be controlled by a miniature, highly specialised computer program in high-speed ROM. Microprogramming allows the base instruction set to be defined or extended by built-in programs (now called firmware or microcode). This concept greatly simplified CPU development. He first described this at the University of Manchester Computer Inaugural Conference in 1951, then published in expanded form in IEEE Spectrum in 1955.

It was widely used in the CPUs and floating-point units of mainframe and other computers; it was implemented for the first time in EDSAC 2, which also used multiple identical "bit slices" to simplify design. Interchangeable, replaceable tube assemblies were used for each bit of the processor.

Magnetic drum memories were developed for the US Navy during WW II with the work continuing at Engineering Research Associates (ERA) in 1946 and 1947. ERA, then a part of Univac included a drum memory in its 1103, announced in February 1953. The first mass-produced computer, the IBM 650, also announced in 1953 had about 8.5 kilobytes of drum memory.

The first use of magnetic core was demonstrated for the Whirlwind computer in August 1953. Commercialization followed quickly. Magnetic core was used in peripherals of the IBM 702 delivered in July 1955, and later in the 702 itself. The IBM 704 (1955) and the Ferranti Mercury (1957) used magnetic-core memory. It went on to dominate the field through the mid-1970s.

As late as 1980, PDP-11/45 machines using magnetic core main memory and drums for swapping were still in use at many of the original UNIX sites

The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers. Initially the only devices available were germanium point-contact transistors. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space. Transistors greatly reduced computers' size, initial cost, and operating cost.
Typically, second-generation computers were composed of large numbers of printed circuit boards such as the IBM Standard Modular System
each carrying one to four logic gates or flip-flops.

At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Initially the only devices available were germanium point-contact transistors, less reliable than the valves they replaced but which consumed far less power. Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. The 1955 version used 200 transistors, 1,300 solid-state diodes, and had a power consumption of 150 watts. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer.

That distinction goes to the Harwell CADET of 1955, built by the electronics division of the Atomic Energy Research Establishment at Harwell. The design featured a 64-kilobyte magnetic drum memory store with multiple moving heads that had been designed at the National Physical Laboratory, UK. By 1953 this team had transistor circuits operating to read and write on a smaller magnetic drum from the Royal Radar Establishment. The machine used a low clock speed of only 58 kHz to avoid having to use any valves to generate the clock waveforms.

CADET used 324 point-contact transistors provided by the UK company Standard Telephones and Cables; 76 junction transistors were used for the first stage amplifiers for data read from the drum, since point-contact transistors were too noisy. From August 1956 CADET was offering a regular computing service, during which it often executed continuous computing runs of 80 hours or more. Problems with the reliability of early batches of point contact and alloyed junction transistors meant that the machine's mean time between failures was about 90 minutes, but this improved once the more reliable bipolar junction transistors became available.

The Manchester University Transistor Computer's design was adopted by the local engineering firm of Metropolitan-Vickers in their Metrovick 950, the first commercial transistor computer anywhere. Six Metrovick 950s were built, the first completed in 1956. They were successfully deployed within various departments of the company and were in use for about five years. A second generation computer, the IBM 1401, captured about one third of the world market. IBM installed more than ten thousand 1401s between 1960 and 1964.

Transistorized electronics improved not only the CPU (Central Processing Unit), but also the peripheral devices. The second generation disk data storage units were able to store tens of millions of letters and digits. Next to the fixed disk storage units, connected to the CPU via high-speed data transmission, were removable disk data storage units. A removable disk pack can be easily exchanged with another pack in a few seconds. Even if the removable disks' capacity is smaller than fixed disks, their interchangeability guarantees a nearly unlimited quantity of data close at hand. Magnetic tape provided archival capability for this data, at a lower cost than disk.

Many second-generation CPUs delegated peripheral device communications to a secondary processor. For example, while the communication processor controlled card reading and punching, the main CPU executed calculations and binary branch instructions. One databus would bear data between the main CPU and core memory at the CPU's fetch-execute cycle rate, and other databusses would typically serve the peripheral devices. On the PDP-1, the core memory's cycle time was 5 microseconds; consequently most arithmetic instructions took 10 microseconds (100,000 operations per second) because most operations took at least two memory cycles; one for the instruction, one for the operand data fetch.

During the second generation remote terminal units (often in the form of Teleprinters like a Friden Flexowriter) saw greatly increased use. Telephone connections provided sufficient speed for early remote terminals and allowed hundreds of kilometers separation between remote-terminals and the computing center. Eventually these stand-alone computer networks would be generalized into an interconnected "network of networks"—the Internet.

The early 1960s saw the advent of supercomputing. The Atlas Computer was a joint development between the University of Manchester, Ferranti, and Plessey, and was first installed at Manchester University and officially commissioned in 1962 as one of the world's first supercomputers – considered to be the most powerful computer in the world at that time. It was said that whenever Atlas went offline half of the United Kingdom's computer capacity was lost. It was a second-generation machine, using discrete germanium transistors. Atlas also pioneered the Atlas Supervisor, "considered by many to be the first recognisable modern operating system".

In the US, a series of computers at Control Data Corporation (CDC) were designed by Seymour Cray to use innovative designs and parallelism to achieve superior computational peak performance. The CDC 6600, released in 1964, is generally considered the first supercomputer. The CDC 6600 outperformed its predecessor, the IBM 7030 Stretch, by about a factor of three. With performance of about 1 megaFLOPS, the CDC 6600 was the world's fastest computer from 1964 to 1969, when it relinquished that status to its successor, the CDC 7600.

The next great advance in computing power came with the advent of the integrated circuit.
The idea of the integrated circuit was conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952:

The first practical ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor. Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. In his patent application of 6 February 1959, Kilby described his new device as “a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated.” The first customer for the invention was the US Air Force.

Noyce also came up with his own idea of an integrated circuit half a year later than Kilby. His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium.

The explosion in the use of computers began with "third-generation" computers, making use of Jack St. Clair Kilby's and Robert Noyce's independent invention of the integrated circuit (or microchip). This led to the invention of the microprocessor. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term "microprocessor", it is largely undisputed that the first single-chip microprocessor was the Intel 4004, designed and realized by Ted Hoff, Federico Faggin, and Stanley Mazor at Intel.

While the earliest microprocessor ICs literally contained only the processor, i.e. the central processing unit, of a computer, their progressive development naturally led to chips containing most or all of the internal electronic parts of a computer. The integrated circuit in the image on the right, for example, an Intel 8742, is an 8-bit microcontroller that includes a CPU running at 12 MHz, 128 bytes of RAM, 2048 bytes of EPROM, and I/O in the same chip.

During the 1960s there was considerable overlap between second and third generation technologies. IBM implemented its IBM Solid Logic Technology modules in hybrid circuits for the IBM System/360 in 1964. As late as 1975, Sperry Univac continued the manufacture of second-generation machines such as the UNIVAC 494. The Burroughs large systems such as the B5000 were stack machines, which allowed for simpler programming. These pushdown automatons were also implemented in minicomputers and microprocessors later, which influenced programming language design. Minicomputers served as low-cost computer centers for industry, business and universities. It became possible to simulate analog circuits with the "simulation program with integrated circuit emphasis", or SPICE (1971) on minicomputers, one of the programs for electronic design automation ().
The microprocessor led to the development of the microcomputer, small, low-cost computers that could be owned by individuals and small businesses. Microcomputers, the first of which appeared in the 1970s, became ubiquitous in the 1980s and beyond.

While which specific system is considered the first microcomputer is a matter of debate, as there were several unique hobbyist systems developed based on the Intel 4004 and its successor, the Intel 8008, the first commercially available microcomputer kit was the Intel 8080-based Altair 8800, which was announced in the January 1975 cover article of "Popular Electronics". However, this was an extremely limited system in its initial stages, having only 256 bytes of DRAM in its initial package and no input-output except its toggle switches and LED register display. Despite this, it was initially surprisingly popular, with several hundred sales in the first year, and demand rapidly outstripped supply. Several early third-party vendors such as Cromemco and Processor Technology soon began supplying additional S-100 bus hardware for the Altair 8800.

In April 1975 at the Hannover Fair, Olivetti presented the P6060, the world's first complete, pre-assembled personal computer system. The central processing unit consisted of two cards, code named PUCE1 and PUCE2, and unlike most other personal computers was built with TTL components rather than a microprocessor. It had one or two 8" floppy disk drives, a 32-character plasma display, 80-column graphical thermal printer, 48 Kbytes of RAM, and BASIC language. It weighed . As a complete system, this was a significant step from the Altair, though it never achieved the same success. It was in competition with a similar product by IBM that had an external floppy disk drive.

From 1975 to 1977, most microcomputers, such as the MOS Technology KIM-1, the Altair 8800, and some versions of the Apple I, were sold as kits for do-it-yourselfers. Pre-assembled systems did not gain much ground until 1977, with the introduction of the Apple II, the Tandy TRS-80, the first SWTPC computers, and the Commodore PET. Computing has evolved with microcomputer architectures, with features added from their larger brethren, now dominant in most market segments.

A NeXT Computer and its object-oriented development tools and libraries were used by Tim Berners-Lee and Robert Cailliau at CERN to develop the world's first web server software, CERN httpd, and also used to write the first web browser, WorldWideWeb. These facts, along with the close association with Steve Jobs, secure the 68030 NeXT a place in history as one of the most significant computers of all time.

Systems as complicated as computers require very high reliability. ENIAC remained on, in continuous operation from 1947 to 1955, for eight years before being shut down. Although a vacuum tube might fail, it would be replaced without bringing down the system. By the simple strategy of never shutting down ENIAC, the failures were dramatically reduced. The vacuum-tube SAGE air-defense computers became remarkably reliable – installed in pairs, one off-line, tubes likely to fail did so when the computer was intentionally run at reduced power to find them. Hot-pluggable hard disks, like the hot-pluggable vacuum tubes of yesteryear, continue the tradition of repair during continuous operation. Semiconductor memories routinely have no errors when they operate, although operating systems like Unix have employed memory tests on start-up to detect failing hardware. Today, the requirement of reliable performance is made even more stringent when server farms are the delivery platform. Google has managed this by using fault-tolerant software to recover from hardware failures, and is even working on the concept of replacing entire server farms on-the-fly, during a service event.

In the 21st century, multi-core CPUs became commercially available. Content-addressable memory (CAM) has become inexpensive enough to be used in networking, and is frequently used for on-chip cache memory in modern microprocessors, although no computer system has yet implemented hardware CAMs for use in programming languages. Currently, CAMs (or associative arrays) in software are programming-language-specific. Semiconductor memory cell arrays are very regular structures, and manufacturers prove their processes on them; this allows price reductions on memory products. During the 1980s, CMOS logic gates developed into devices that could be made as fast as other circuit types; computer power consumption could therefore be decreased dramatically. Unlike the continuous current draw of a gate based on other logic types, a CMOS gate only draws significant current during the 'transition' between logic states, except for leakage.

This has allowed computing to become a commodity which is now ubiquitous, embedded in many forms, from greeting cards and telephones to satellites. The thermal design power which is dissipated during operation has become as essential as computing speed of operation. In 2006 servers consumed 1.5% of the total energy budget of the U.S. The energy consumption of computer data centers was expected to double to 3% of world consumption by 2011. The SoC (system on a chip) has compressed even more of the integrated circuitry into a single chip; SoCs are enabling phones and PCs to converge into single hand-held wireless mobile devices.

MIT Technology Review reported 10 November 2017 that IBM has created a 50-qubit computer; currently its quantum state lasts 50 microseconds. "See: Quantum supremacy"

Computing hardware and its software have even become a metaphor for the operation of the universe.

An indication of the rapidity of development of this field can be inferred from the history of the seminal 1947 article by Burks, Goldstine and von Neumann. By the time that anyone had time to write anything down, it was obsolete. After 1945, others read John von Neumann's "First Draft of a Report on the EDVAC", and immediately started implementing their own systems. To this day, the rapid pace of development has continued, worldwide.

A 1966 article in "Time" predicted that: "By 2000, the machines will be producing so much that everyone in the U.S. will, in effect, be independently wealthy. How to use leisure time will be a major problem."




</doc>
<doc id="13637" url="https://en.wikipedia.org/wiki?curid=13637" title="Hausdorff space">
Hausdorff space

In topology and related branches of mathematics, a Hausdorff space, separated space or T space is a topological space in which distinct points have disjoint neighbourhoods. Of the many separation axioms that can be imposed on a topological space, the "Hausdorff condition" (T) is the most frequently used and discussed. It implies the uniqueness of limits of sequences, nets, and filters.

Hausdorff spaces are named after Felix Hausdorff, one of the founders of topology. Hausdorff's original definition of a topological space (in 1914) included the Hausdorff condition as an axiom.

Points "x" and "y" in a topological space "X" can be "separated by neighbourhoods" if there exists a neighbourhood "U" of "x" and a neighbourhood "V" of "y" such that "U" and "V" are disjoint ().
"X" is a Hausdorff space if all distinct points in "X" are pairwise neighborhood-separable. This condition is the third separation axiom (after T and T), which is why Hausdorff spaces are also called "T spaces". The name "separated space" is also used.

A related, but weaker, notion is that of a preregular space. "X" is a preregular space if any two topologically distinguishable points can be separated by disjoint neighbourhoods. Preregular spaces are also called "R spaces".

The relationship between these two conditions is as follows. A topological space is Hausdorff if and only if it is both preregular (i.e. topologically distinguishable points are separated by neighbourhoods) and Kolmogorov (i.e. distinct points are topologically distinguishable). A topological space is preregular if and only if its Kolmogorov quotient is Hausdorff.

For a topological space "X", the following are equivalent:

Almost all spaces encountered in analysis are Hausdorff; most importantly, the real numbers (under the standard metric topology on real numbers) are a Hausdorff space. More generally, all metric spaces are Hausdorff. In fact, many spaces of use in analysis, such as topological groups and topological manifolds, have the Hausdorff condition explicitly stated in their definitions.

A simple example of a topology that is T but is not Hausdorff is the cofinite topology defined on an infinite set.

Pseudometric spaces typically are not Hausdorff, but they are preregular, and their use in analysis is usually only in the construction of Hausdorff gauge spaces. Indeed, when analysts run across a non-Hausdorff space, it is still probably at least preregular, and then they simply replace it with its Kolmogorov quotient, which is Hausdorff.

In contrast, non-preregular spaces are encountered much more frequently in abstract algebra and algebraic geometry, in particular as the Zariski topology on an algebraic variety or the spectrum of a ring. They also arise in the model theory of intuitionistic logic: every complete Heyting algebra is the algebra of open sets of some topological space, but this space need not be preregular, much less Hausdorff, and in fact usually is neither. The related concept of Scott domain also consists of non-preregular spaces.

While the existence of unique limits for convergent nets and filters implies that a space is Hausdorff, there are non-Hausdorff T spaces in which every convergent sequence has a unique limit.

Subspaces and products of Hausdorff spaces are Hausdorff, but quotient spaces of Hausdorff spaces need not be Hausdorff. In fact, "every" topological space can be realized as the quotient of some Hausdorff space.

Hausdorff spaces are T, meaning that all singletons are closed. Similarly, preregular spaces are R.

Another nice property of Hausdorff spaces is that compact sets are always closed. This may fail in non-Hausdorff spaces such as Sierpiński space. 

The definition of a Hausdorff space says that points can be separated by neighborhoods. It turns out that this implies something which is seemingly stronger: in a Hausdorff space every pair of disjoint compact sets can also be separated by neighborhoods, in other words there is a neighborhood of one set and a neighborhood of the other, such that the two neighborhoods are disjoint. This is an example of the general rule that compact sets often behave like points.

Compactness conditions together with preregularity often imply stronger separation axioms. For example, any locally compact preregular space is completely regular. Compact preregular spaces are normal, meaning that they satisfy Urysohn's lemma and the Tietze extension theorem and have partitions of unity subordinate to locally finite open covers. The Hausdorff versions of these statements are: every locally compact Hausdorff space is Tychonoff, and every compact Hausdorff space is normal Hausdorff.

The following results are some technical properties regarding maps (continuous and otherwise) to and from Hausdorff spaces.

Let "f" : "X" → "Y" be a continuous function and suppose "Y" is Hausdorff. Then the graph of "f", formula_1, is a closed subset of "X" × "Y".

Let "f" : "X" → "Y" be a function and let formula_2 be its kernel regarded as a subspace of "X" × "X".

If "f,g" : "X" → "Y" are continuous maps and "Y" is Hausdorff then the equalizer formula_3 is closed in "X". It follows that if "Y" is Hausdorff and "f" and "g" agree on a dense subset of "X" then "f" = "g". In other words, continuous functions into Hausdorff spaces are determined by their values on dense subsets.

Let "f" : "X" → "Y" be a closed surjection such that "f"("y") is compact for all "y" ∈ "Y". Then if "X" is Hausdorff so is "Y".

Let "f" : "X" → "Y" be a quotient map with "X" a compact Hausdorff space. Then the following are equivalent:

All regular spaces are preregular, as are all Hausdorff spaces. There are many results for topological spaces that hold for both regular and Hausdorff spaces.
Most of the time, these results hold for all preregular spaces; they were listed for regular and Hausdorff spaces separately because the idea of preregular spaces came later.
On the other hand, those results that are truly about regularity generally do not also apply to nonregular Hausdorff spaces.

There are many situations where another condition of topological spaces (such as paracompactness or local compactness) will imply regularity if preregularity is satisfied.
Such conditions often come in two versions: a regular version and a Hausdorff version.
Although Hausdorff spaces are not, in general, regular, a Hausdorff space that is also (say) locally compact will be regular, because any Hausdorff space is preregular.
Thus from a certain point of view, it is really preregularity, rather than regularity, that matters in these situations.
However, definitions are usually still phrased in terms of regularity, since this condition is better known than preregularity.

See History of the separation axioms for more on this issue.

The terms "Hausdorff", "separated", and "preregular" can also be applied to such variants on topological spaces as uniform spaces, Cauchy spaces, and convergence spaces.
The characteristic that unites the concept in all of these examples is that limits of nets and filters (when they exist) are unique (for separated spaces) or unique up to topological indistinguishability (for preregular spaces).

As it turns out, uniform spaces, and more generally Cauchy spaces, are always preregular, so the Hausdorff condition in these cases reduces to the T condition.
These are also the spaces in which completeness makes sense, and Hausdorffness is a natural companion to completeness in these cases.
Specifically, a space is complete if and only if every Cauchy net has at "least" one limit, while a space is Hausdorff if and only if every Cauchy net has at "most" one limit (since only Cauchy nets can have limits in the first place).

The algebra of continuous (real or complex) functions on a compact Hausdorff space is a commutative C*-algebra, and conversely by the Banach–Stone theorem one can recover the topology of the space from the algebraic properties of its algebra of continuous functions. This leads to noncommutative geometry, where one considers noncommutative C*-algebras as representing algebras of functions on a noncommutative space.





</doc>
<doc id="13644" url="https://en.wikipedia.org/wiki?curid=13644" title="Hawkwind">
Hawkwind

Hawkwind are an English rock band and one of the earliest space rock groups. Their lyrics favour urban and science fiction themes. Formed in November 1969, Hawkwind have gone through many incarnations and they have incorporated different styles into their music, including hard rock, progressive rock and psychedelic rock. They are also regarded as an influential proto-punk band.

Dozens of musicians, dancers and writers have worked with the band since their inception. Notable musicians to have performed in the band include Lemmy, Ginger Baker and Huw Lloyd-Langton, but the band are most closely associated with their founder, the singer, songwriter and guitarist Dave Brock, who is the only remaining original member.

They are best known for the song "Silver Machine", which became a number three UK hit single in 1972, but they scored further hit singles with "Urban Guerrilla" (another Top 40 hit) and "Shot Down in the Night." Twenty-two of their albums charted in the UK from 1971 to 1993. 

Dave Brock and Mick Slattery had been in the London-based psychedelic band Famous Cure, and a meeting with bassist John Harrison revealed a mutual interest in electronic music which led the trio to embark upon a new musical venture together. Seventeen-year-old drummer Terry Ollis replied to an advert in a music weekly, while Nik Turner and Michael "Dik Mik" Davies, old acquaintances of Brock, offered help with transport and gear, but were soon pulled into the band.

Gatecrashing a local talent night at the All Saints Hall, Notting Hill, they were so disorganised as to not even have a name, opting for "Group X" at the last minute, nor any songs, choosing to play an extended 20-minute jam on The Byrds' "Eight Miles High." BBC Radio 1 DJ John Peel was in the audience and was impressed enough to tell event organiser, Douglas Smith, to keep an eye on them. Smith signed them up and got them a deal with Liberty Records on the back of a deal he was setting up for Cochise.

The band settled on the name "Hawkwind" after briefly being billed as "Group X" and "Hawkwind Zoo".

An Abbey Road session took place recording demos of "Hurry on Sundown" and others (included on the remasters version of "Hawkwind"), after which Slattery left to be replaced by Huw Lloyd-Langton, who had known Brock from his days working in a music shop selling guitar strings to Brock, then a busker.

Pretty Things guitarist Dick Taylor was brought in to produce the 1970 debut album "Hawkwind". Although it was not a commercial success, it did bring them to the attention of the UK underground scene, which found them playing free concerts, benefit gigs, and festivals. Playing free outside the Bath Festival, they encountered another Ladbroke Grove based band, the Pink Fairies, who shared similar interests in music and recreational activities; a friendship developed which led to the two bands becoming running partners and performing as "Pinkwind". Their use of drugs, however, led to the departure of Harrison, who did not partake, to be replaced briefly by Thomas Crimble (about July 1970 – March 1971). Crimble played on a few BBC sessions before leaving to help organise the Glastonbury Free Festival 1971; he sat in during the band's performance there. Lloyd-Langton also quit, after a bad LSD trip at the Isle of Wight Festival led to a nervous breakdown.

Their follow-up album, 1971's "In Search of Space", brought greater commercial success, reaching number 18 on the UK album charts. This album offered a refinement of the band's image and philosophy courtesy of graphic artist Barney Bubbles and underground press writer Robert Calvert, as depicted in the accompanying "Hawklog" booklet, which would be further developed into the "Space Ritual" stage show. Science fiction author Michael Moorcock and dancer Stacia also started contributing to the band. Dik Mik had left the band, replaced by sound engineer Del Dettmar, but chose to return for this album giving the band two electronics players. Bass player Dave Anderson, who had been in the German band Amon Düül II, had also joined and played on the album but departed before its release because of personal tensions with some other members of the band. Anderson and Lloyd-Langton then formed the short-lived band Amon Din. Meanwhile, Ollis quit, unhappy with the commercial direction the band were heading in.
The addition of bassist Ian "Lemmy" Kilmister and drummer Simon King propelled the band to greater heights. One of the early gigs the band played was a benefit for the Greasy Truckers at The Roundhouse on 13 February 1972. A live album of the concert, "Greasy Truckers Party", was released, and after re-recording the vocal, a single, "Silver Machine", was also released, reaching number three in the UK charts. This generated sufficient funds for the subsequent album "Doremi Fasol Latido" Space Ritual tour. The show featured dancers Stacia and Miss Renee, mime artist Tony Carrera and a light show by Liquid Len and was recorded on the elaborate package "Space Ritual". At the height of their success, in 1973, the band released the single "Urban Guerrilla", which coincided with an IRA bombing campaign in London, so the BBC refused to play it and the band's management reluctantly decided to withdraw it fearing accusations of opportunism, despite the disc having already climbed to number 39 in the UK chart.

Dik Mik departed during 1973 and Calvert ended his association with the band to concentrate on solo projects. Dettmar also indicated that he was to leave the band, so Simon House was recruited as keyboardist and violinist playing live shows, a North America tour and recording the 1974 album "Hall of the Mountain Grill". Dettmar left after a European tour and emigrated to Canada, whilst Alan Powell deputised for an incapacitated King on that European tour, but remained giving the band two drummers.

At the beginning of 1975, the band recorded the album "Warrior on the Edge of Time" in collaboration with Michael Moorcock, loosely based on his Eternal Champion figure. However, during a North American tour in May, Lemmy was caught in possession of amphetamine crossing the border from the US into Canada. The border police mistook the powder for cocaine and he was jailed, forcing the band to cancel some shows. Fed up with his erratic behaviour, the band dismissed the bass player replacing him with their long-standing friend and former Pink Fairies guitarist Paul Rudolph. Lemmy then teamed up with another Pink Fairies guitarist, Larry Wallis, to form Motörhead, named after the last song he had written for Hawkwind.

Calvert made a guest appearance with the band for their headline set at the Reading Festival in August 1975, after which he chose to rejoin the band as a full-time lead vocalist. Stacia chose to relinquish her dancing duties and settle down to family life. The band changed record company to Tony Stratton-Smith's Charisma Records and, on Stratton-Smith's suggestion, band management from Douglas Smith to Tony Howard.

"Astounding Sounds, Amazing Music" is the first album of this era and highlights both Calvert's well-crafted lyrics written with stage performance in mind and a greater proficiency and scope in the music. But on the eve of recording the follow-up "Back on the Streets" single, Turner was dismissed for his erratic live playing and Powell was deemed surplus to requirements. After a tour to promote the single and during the recording of the next album, Rudolph was also dismissed, for allegedly trying to steer the band into a musical direction at odds with Calvert and Brock's vision.

Adrian "Ade" Shaw, who, as bass player for Magic Muscle, had supported Hawkwind on the "Space Ritual" tour, came in for the 1977 album "Quark, Strangeness and Charm". The band continued to enjoy moderate commercial success, but Calvert's mental illness often caused problems. A manic phase saw the band abandon a European tour in France, while a depression phase during a 1978 North American tour convinced Brock to disband the group. In between these two tours, the band had recorded the album "PXR5" in January 1978, but its release was delayed until 1979.

On 23 December 1977 in Barnstaple, Brock and Calvert had performed a one-off gig with Devon band Ark as the Sonic Assassins, and looking for a new project in 1978, bassist Harvey Bainbridge and drummer Martin Griffin were recruited from this event. Steve Swindells was recruited as keyboard player. The band was named Hawklords, (probably for legal reasons, the band having recently split from their management), and recording took place on a farm in Devon using a mobile studio, resulting in the album "25 Years On". King had originally been the drummer for the project but quit during recording sessions to return to London, while House, who had temporarily left the band to join a David Bowie tour, elected to remain with Bowie full-time, but nevertheless contributed violin to these sessions. At the end of the band's UK tour, Calvert, wanting King back in the band, dismissed Griffin, then promptly resigned himself, choosing to pursue a career in literature. Swindells left to record a solo album after an offer had been made to him by the record company ATCO.

In late 1979, Hawkwind reformed with Brock, Bainbridge and King being joined by Huw Lloyd-Langton (who had played on the debut album) and Tim Blake (formerly of Gong), embarking upon a UK tour despite not having a record deal or any product to promote. Some shows were recorded and a deal was made with Bronze Records, resulting in the "Live Seventy Nine" album, quickly followed by the studio album "Levitation". However, during the recording of "Levitation" King quit and Ginger Baker was drafted in for the sessions, but he chose to stay with the band for the tour, during which Blake left to be replaced by Keith Hale.

In 1981 Baker and Hale left after their insistence that Bainbridge should be dismissed was ignored, and Brock and Bainbridge elected to handle synthesisers and sequencers themselves, with drummer Griffin from the Hawklords rejoining. Three albums, which again saw Moorcock contributing lyrics and vocals, were recorded for RCA/Active: "Sonic Attack", the electronic "Church of Hawkwind" and "Choose Your Masques". This band headlined the 1981 Glastonbury Festival and made an appearance at the 1982 Donington Monsters of Rock Festival, as well as continuing to play the summer solstice at Stonehenge Free Festival.

In the early 1980s, Brock had started using drum machines for his home demos and became increasingly frustrated at the inability of drummers to keep perfect time, leading to a succession of drummers coming and going. First, Griffin was ousted and the band tried King again, but, unhappy with his playing at that time, he was rejected. Andy Anderson briefly joined while he was also playing for The Cure, and Robert Heaton also filled the spot briefly prior to the rise of New Model Army. Lloyd Langton Group drummer John Clark did some recording sessions, and in late 1983 Rick Martinez joined the band to play drums on the "Earth Ritual" tour in February and March 1984, later replaced by Clive Deamer.

Turner had returned as a guest for the 1982 "Choose Your Masques" tour and was invited back permanently. Further tours ensued with Phil "Dead Fred" Reeves augmenting the line-up on keyboards and violin, but neither Turner nor Reeves would appear on the only recording of 1983–84, "The Earth Ritual Preview", but there was a guest spot for Lemmy. The "Earth Ritual" tour was filmed for Hawkwind's first video release, "Night of the Hawk".

Alan Davey was a young fan of the band who had sent a tape of his playing to Brock, and Brock chose to oust Reeves moving Bainbridge from bass to keyboards to accommodate Davey. This experimental line-up played at the Stonehenge Free Festival in 1984, which was filmed and release as "Stonehenge 84". Subsequent personal and professional tensions between Brock and Turner led to the latter's expulsion at the beginning of 1985. Clive Deamer, who was deemed "too professional" for the band, was eventually replaced in 1985 by Danny Thompson Jr, a friend of bassist Alan Davey, and remained almost to the end of the decade.

Hawkwind's association with Moorcock climaxed in their most ambitious project, "The Chronicle of the Black Sword", based loosely around the Elric series of books and theatrically staged with Tony Crerar as the central character. Moorcock contributed lyrics, but only performed some spoken pieces on some live dates. The tour was recorded and issued as an album "Live Chronicles" and video "The Chronicle of the Black Sword". A headline appearance at the 1986 Reading Festival was followed by a UK tour to promote the "Live Chronicles" album which was filmed and released as "Chaos". In 1988 the band recorded the album "The Xenon Codex" with Guy Bidmead, but all was not well in the band and soon after, both Lloyd-Langton and Thompson departed.

Drummer Richard Chadwick, who joined in the summer of 1988, had been playing in small alternative free festival bands, most notably Bath's Smart Pils, for a decade and had frequently crossed paths with Hawkwind and Brock. He was initially invited simply to play with the band, but eventually replaced stand in drummer Mick Kirton to become the band's drummer to the present day.

To fill in the gap of lead sound, lost when Lloyd-Langton left, violinist House was re-instated into the line-up in 1989 (having previously been a member from 1974 until 1978), and, notably, Hawkwind embarked on their first North American visit in eleven years (since the somewhat disastrous 1978 tour), in which House did not partake. The successfully received tour was the first of several over the coming years, in an effort by the band to re-introduce themselves to the American market.

Bridget Wishart, an associate of Chadwick's from the festival circuit, also joined to become the band's one and only singing front-woman, the band had been fronted in earlier days by Stacia but only as a dancer. This band produced two albums, 1990's "Space Bandits" and 1991's "Palace Springs" and also filmed a 1-hour appearance for the "Bedrock TV" series.

1990 saw Hawkwind tour North America again, the second instalment in a series of American visits made at around this time in an effort to re-establish the Hawkwind brand in America. The original business plan was to hold three consecutive US tours, annually, from 1989–1991, with the first losing money, the second breaking even, and the third turning a profit, ultimately bringing Hawkwind back into recognition across the Atlantic. Progress, however, was somewhat stunted, due to ex-member Nik Turner touring the United States with his own band at the time, in which the shows were often marketed as Hawkwind.

In 1991 Bainbridge, House and Wishart departed and the band continued as a three piece relying heavily on synthesisers and sequencers to create a wall-of-sound. The 1992 album "Electric Tepee" combined hard rock and light ambient pieces, while "It is the Business of the Future to be Dangerous" is almost devoid of the rock leanings. "The Business Trip" is a record of the previous album's tour, but rockier as would be expected from a live outing. The "White Zone" album was released under the alias Psychedelic Warriors to distance itself entirely from the rock expectancy of Hawkwind.

A general criticism of techno music at that time was its facelessness and lack of personality, which the band were coming to feel also plagued them. Ron Tree had known the band on the festival circuit and offered his services as a front-man, and the band duly employed him for the album "Alien 4" and its accompanying tour which resulted in the album "Love in Space" and "video".

In 1996, unhappy with the musical direction of the band, bassist Davey left, forming his own Middle-Eastern flavoured hard-rock group Bedouin and a Motörhead tribute act named Ace of Spades. His bass playing role was reluctantly picked up by singer Tree and the band were joined full-time by lead guitarist Jerry Richards (another stalwart of the festival scene, playing for Tubilah Dog who had merged with Brock's Agents of Chaos during 1988) for the albums "Distant Horizons" and "In Your Area". Rasta chanter Captain Rizz also joined the band for guest spots during live shows.

Hawkestra — a re-union event featuring appearances from past and present members — had originally been intended to coincide with the band's 30th anniversary and the release of the career spanning "Epocheclipse – 30 Year Anthology" set, but logistical problems delayed it until 21 October 2000. It took place at the Brixton Academy with about 20 members taking part in a 3+ hour set which was filmed and recorded. Guests included Samantha Fox who sang "Master of the Universe". However, arguments and disputes over financial recompense and musical input resulted in the prospect of the event being re-staged unlikely, and any album or DVD release being indefinitely shelved.

The Hawkestra had set a template for Brock to assemble a core band of Tree, Brock, Richards, Davey, Chadwick and for the use of former members as guests on live shows and studio recordings. The 2000 Christmas Astoria show was recorded with contributions from House, Blake, Rizz, Moorcock, Jez Huggett and Keith Kniveton and released as "Yule Ritual" the following year. In 2001, Davey agreed to rejoin the band permanently, but only after the departure of Tree and Richards.

Meanwhile, having rekindled relationships with old friends at the Hawkestra, Turner organised further Hawkestra gigs resulting in the formation of xhawkwind.com, a band consisting mainly of ex-Hawkwind members and playing old Hawkwind songs. An appearance at Guilfest in 2002 led to confusion as to whether this actually was Hawkwind, sufficiently irking Brock into taking legal action to prohibit Turner from trading under the name Hawkwind. Turner lost the case and the band began performing as Space Ritual.

An appearance at the Canterbury Sound Festival in August 2001, resulting in another live album "Canterbury Fayre 2001", saw guest appearances from Lloyd-Langton, House, Kniveton with Arthur Brown on "Silver Machine". The band organised the first of their own weekend festivals, named Hawkfest, in Devon in the summer of 2002. Brown joined the band in 2002 for a Winter tour which featured some Kingdom Come songs and saw appearances from Blake and Lloyd-Langton, the Newcastle show being released on DVD as "Out of the Shadows" and the London show on CD as "Spaced Out in London".

In 2005 a new album "Take Me to Your Leader" was released. Recorded by the core band of Brock/Davey/Chadwick, contributors included new keyboardist Jason Stuart, Arthur Brown, tabloid writer and TV personality Matthew Wright, 1970s New Wave singer Lene Lovich, Simon House and Jez Huggett. This was followed in 2006 by the CD/DVD "Take Me to Your Future".

The band were the subject of an hour-long television documentary entitled "Hawkwind: Do Not Panic" that aired on BBC Four as part of the "Originals" series. It was broadcast on 30 March 2007 and repeated on 10 August 2007. Although Brock participated in its making he did not appear in the programme, it is alleged that he requested all footage of himself be removed after he was denied any artistic control over the documentary. In one of the documentary's opening narratives regarding Brock, it is stated that he declined to be interviewed for the programme because of Nik Turner's involvement, indicating that the two men have still not been reconciled over the xhawkwind.com incident.

December 2006 saw the official departure of Alan Davey, who left to perform and record with two new bands: Gunslinger and Thunor. He was replaced by Mr Dibs, a long-standing member of the road crew. The band performed at their annual Hawkfest festival and headlined the US festival Nearfest and played gigs in PA and NY. At the end of 2007, Tim Blake once again joined the band filling the lead role playing keyboards and theremin. The band played 5 Christmas dates, the London show being released as an audio CD and video DVD under the title "Knights of Space".

In January 2008 the band reversed its anti-taping policy, long a sore-point with many fans, announcing that it would allow audio recording and non-commercial distribution of such recordings, provided there was no competing official release. At the end of 2008, Atomhenge Records (a subsidiary of Cherry Red Records) commenced the re-issuing of Hawkwind's back catalogue from the years 1976 through to 1997 with the release of two triple CD anthologies "Spirit of the Age (anthology 1976–84)" and "The Dream Goes On (anthology 1985–97)".

On 8 September 2008 keyboard player Jason Stuart died due to a brain haemorrhage. In October 2008, Niall Hone (former Tribe of Cro) joined Hawkwind for their Winter 2008 tour playing guitar, along with returning synth/theremin player Tim Blake. In this period, Hone also occasionally played bass guitar alongside Mr Dibs and used laptops for live electronic improvisation.

In 2009, the band began occasionally featuring Jon Sevink, from The Levellers as guest violinist at some shows. Later that year, Hawkwind embarked on a winter tour to celebrate the band's 40th anniversary, including two gigs on 28 and 29 August marking the anniversary of their first live performances. In 2010, Hawkwind held their annual Hawkfest at the site of the original Isle of Wight Festival, marking the 40th anniversary of their appearance there.

On 21 June 2010, Hawkwind released a studio album entitled "Blood of the Earth" on Eastworld Records. During and since the "Blood of the Earth" support tours, Hone's primary on-stage responsibility shifted to bass, while Mr. Dibs moved to a more traditional lead singer/front man role.

In 2011, Hawkwind toured Australia for the second time.

April 2012 saw the release of a new album, "Onward", again on Eastworld. Keyboardist Dead Fred rejoined Hawkwind for the 2012 tour in support of "Onward" and has since remained with the band. In November 2012, Brock, Chadwick and Hone — credited as "Hawkwind Light Orchestra" — released "Stellar Variations" on Esoteric Recordings.

2013 marked the first Hawkeaster, a two-day festival held in Seaton, Devon during the Easter weekend. A US tour was booked for October 2013, but due to health issues, was postponed and later cancelled.

In February 2014, as part of a one-off Space Ritual performance, Hawkwind performed at the O2 Shepherd's Bush Empire featuring an appearance by Brian Blessed for the spoken word element of Sonic Attack; a studio recording of this performance was released as a single in September 2014. Later in the year, former Soft Machine guitarist John Etheridge joined the live line-up of the band, though he had departed again prior to early 2015 dates.

Following Hawkeaster 2015, Hawkwind made their debut visit to Japan, playing two sold-out shows in Tokyo. Hawkwind performed two Solstice Ritual shows in December 2015, with Steve Hillage guesting, and Haz Wheaton joining Hawkwind on bass guitar. Wheaton is a former member of the band's road crew who had previously appeared with Technicians of Spaceship Hawkwind, a "skeleton crew" spin off live band. Additionally, he had guested on bass for Dave Brock's solo album "Brockworld" released earlier in the year.

The band released The Machine Stops on 15 April 2016. The album marked Wheaton's first appearance on a Hawkwind studio album, and the first album without Tim Blake's involvement since he had rejoined the band in 2010 and appeared on Blood of the Earth. His departure was offset by increased synthesiser work by Hone and Brock.

Dead Fred's last live appearance with Hawkwind was at The Eastbourne Winter Gardens April 1, 2016. Hone took over keyboards and synth duties live until though Blake returned for shows in summer 2016.

It was announced in November 2016 that Hawkwind were recording a new studio album, entitled "Into The Woods". Keyboardist-guitarist Magnus Martin replaced both Hone and Blake in the lineup for the new album, leaving the 2017 core band composed of Brock, Chadwick, Mr Dibs, Wheaton and Martin.

In 2018, Hawkwind recorded an acoustic album "The Road to Utopia" consisting primarily of cover versions of their 1970s songs with production, arrangement and additional orchestrations by Mike Batt and a guest appearance from Eric Clapton. Batt is scheduled to conduct a series concerts of Hawkwind songs featuring the band and orchestra in October and November.

Hawkwind have been cited as an influence by artists such as Monster Magnet, the Sex Pistols (who covered "Silver Machine"), Henry Rollins and Dez Cadena of Black Flag, Ty Segall, The Mekano Set, and Ozric Tentacles.

Hard rock musician Lemmy of the band Motörhead gained a lot from his tenure in Hawkwind. He has remarked, "I really found myself as an instrumentalist in Hawkwind. Before that I was just a guitar player who was pretending to be good, when actually I was no good at all. In Hawkwind I became a good bass player. It was where I learned I was good at something."




There are three biographies of Hawkwind.


</doc>
<doc id="13645" url="https://en.wikipedia.org/wiki?curid=13645" title="Horse">
Horse

The horse ("Equus ferus caballus") is one of two extant subspecies of "Equus ferus". It is an odd-toed ungulate mammal belonging to the taxonomic family Equidae. The horse has evolved over the past 45 to 55 million years from a small multi-toed creature, "Eohippus", into the large, single-toed animal of today. Humans began to domesticate horses around 4000 BC, and their domestication is believed to have been widespread by 3000 BC. Horses in the subspecies "caballus" are domesticated, although some domesticated populations live in the wild as feral horses. These feral populations are not true wild horses, as this term is used to describe horses that have never been domesticated, such as the endangered Przewalski's horse, a separate subspecies, and the only remaining true wild horse. There is an extensive, specialized vocabulary used to describe equine-related concepts, covering everything from anatomy to life stages, size, colors, markings, breeds, locomotion, and behavior.

Horses' anatomy enables them to make use of speed to escape predators and they have a well-developed sense of balance and a strong fight-or-flight response. Related to this need to flee from predators in the wild is an unusual trait: horses are able to sleep both standing up and lying down, with younger horses tending to sleep significantly more than adults. Female horses, called mares, carry their young for approximately 11 months, and a young horse, called a foal, can stand and run shortly following birth. Most domesticated horses begin training under saddle or in harness between the ages of two and four. They reach full adult development by age five, and have an average lifespan of between 25 and 30 years.

Horse breeds are loosely divided into three categories based on general temperament: spirited "hot bloods" with speed and endurance; "cold bloods", such as draft horses and some ponies, suitable for slow, heavy work; and "warmbloods", developed from crosses between hot bloods and cold bloods, often focusing on creating breeds for specific riding purposes, particularly in Europe. There are more than 300 breeds of horse in the world today, developed for many different uses.

Horses and humans interact in a wide variety of sport competitions and non-competitive recreational pursuits, as well as in working activities such as police work, agriculture, entertainment, and therapy. Horses were historically used in warfare, from which a wide variety of riding and driving techniques developed, using many different styles of equipment and methods of control. Many products are derived from horses, including meat, milk, hide, hair, bone, and pharmaceuticals extracted from the urine of pregnant mares. Humans provide domesticated horses with food, water and shelter, as well as attention from specialists such as veterinarians and farriers.

Specific terms and specialized language are used to describe equine anatomy, different life stages, colors and breeds.

Depending on breed, management and environment, the modern domestic horse has a life expectancy of 25 to 30 years. Uncommonly, a few animals live into their 40s and, occasionally, beyond. The oldest verifiable record was "Old Billy", a 19th-century horse that lived to the age of 62. In modern times, Sugar Puff, who had been listed in "Guinness World Records" as the world's oldest living pony, died in 2007 at age 56.

Regardless of a horse or pony's actual birth date, for most competition purposes a year is added to its age each January 1 of each year in the Northern Hemisphere and each August 1 in the Southern Hemisphere. The exception is in endurance riding, where the minimum age to compete is based on the animal's actual calendar age.

The following terminology is used to describe horses of various ages:


In horse racing, these definitions may differ: For example, in the British Isles, Thoroughbred horse racing defines colts and fillies as less than five years old. However, Australian Thoroughbred racing defines colts and fillies as less than four years old.

The height of horses is usually measured at the highest point of the withers, where the neck meets the back. This point is used because it is a stable point of the anatomy, unlike the head or neck, which move up and down in relation to the body of the horse.

In English-speaking countries, the height of horses is often stated in units of hands and inches: one hand is equal to . The height is expressed as the number of full hands, followed by a point, then the number of additional inches, and ending with the abbreviation "h" or "hh" (for "hands high"). Thus, a horse described as "15.2 h" is 15 hands plus 2 inches, for a total of in height.
The size of horses varies by breed, but also is influenced by nutrition. Light riding horses usually range in height from and can weigh from . Larger riding horses usually start at about and often are as tall as , weighing from . Heavy or draft horses are usually at least high and can be as tall as high. They can weigh from about .

The largest horse in recorded history was probably a Shire horse named Mammoth, who was born in 1848. He stood high and his peak weight was estimated at . The current record holder for the world's smallest horse is Thumbelina, a fully mature miniature horse affected by dwarfism. She is tall and weighs .

Ponies are taxonomically the same animals as horses. The distinction between a horse and pony is commonly drawn on the basis of height, especially for competition purposes. However, height alone is not dispositive; the difference between horses and ponies may also include aspects of phenotype, including conformation and temperament.

The traditional standard for height of a horse or a pony at maturity is . An animal 14.2 h or over is usually considered to be a horse and one less than 14.2 h a pony, but there are many exceptions to the traditional standard. In Australia, ponies are considered to be those under . For competition in the Western division of the United States Equestrian Federation, the cutoff is . The International Federation for Equestrian Sports, the world governing body for horse sport, uses metric measurements and defines a pony as being any horse measuring less than at the withers without shoes, which is just over 14.2 h, and , or just over 14.2½ h, with shoes.

Height is not the sole criterion for distinguishing horses from ponies. Breed registries for horses that typically produce individuals both under and over 14.2 h consider all animals of that breed to be horses regardless of their height. Conversely, some pony breeds may have features in common with horses, and individual animals may occasionally mature at over 14.2 h, but are still considered to be ponies.

Ponies often exhibit thicker manes, tails, and overall coat. They also have proportionally shorter legs, wider barrels, heavier bone, shorter and thicker necks, and short heads with broad foreheads. They may have calmer temperaments than horses and also a high level of intelligence that may or may not be used to cooperate with human handlers. Small size, by itself, is not an exclusive determinant. For example, the Shetland pony which averages , is considered a pony. Conversely, breeds such as the Falabella and other miniature horses, which can be no taller than , are classified by their registries as very small horses, not ponies.

Horses have 64 chromosomes. The horse genome was sequenced in 2007. It contains 2.7 billion DNA base pairs, which is larger than the dog genome, but smaller than the human genome or the bovine genome. The map is available to researchers.

Horses exhibit a diverse array of coat colors and distinctive markings, described by a specialized vocabulary. Often, a horse is classified first by its coat color, before breed or sex. Horses of the same color may be distinguished from one another by white markings, which, along with various spotting patterns, are inherited separately from coat color.

Many genes that create horse coat colors and patterns have been identified. Current genetic tests can identify at least 13 different alleles influencing coat color, and research continues to discover new genes linked to specific traits. The basic coat colors of chestnut and black are determined by the gene controlled by the Melanocortin 1 receptor, also known as the "extension gene" or "red factor," as its recessive form is "red" (chestnut) and its dominant form is black. Additional genes control suppression of black color to point coloration that results in a bay, spotting patterns such as pinto or leopard, dilution genes such as palomino or dun, as well as graying, and all the other factors that create the many possible coat colors found in horses.

Horses that have a white coat color are often mislabeled; a horse that looks "white" is usually a middle-aged or older gray. Grays are born a darker shade, get lighter as they age, but usually keep black skin underneath their white hair coat (with the exception of pink skin under white markings). The only horses properly called white are born with a predominantly white hair coat and pink skin, a fairly rare occurrence. Different and unrelated genetic factors can produce white coat colors in horses, including several different alleles of dominant white and the sabino-1 gene. However, there are no "albino" horses, defined as having both pink skin and red eyes.

Gestation lasts approximately 340 days, with an average range 320–370 days, and usually results in one foal; twins are rare. Horses are a precocial species, and foals are capable of standing and running within a short time following birth. Foals are usually born in the spring. The estrous cycle of a mare occurs roughly every 19–22 days and occurs from early spring into autumn. Most mares enter an "anestrus" period during the winter and thus do not cycle in this period. Foals are generally weaned from their mothers between four and six months of age.

Horses, particularly colts, sometimes are physically capable of reproduction at about 18 months, but domesticated horses are rarely allowed to breed before the age of three, especially females. Horses four years old are considered mature, although the skeleton normally continues to develop until the age of six; maturation also depends on the horse's size, breed, sex, and quality of care. Larger horses have larger bones; therefore, not only do the bones take longer to form bone tissue, but the epiphyseal plates are larger and take longer to convert from cartilage to bone. These plates convert after the other parts of the bones, and are crucial to development.

Depending on maturity, breed, and work expected, horses are usually put under saddle and trained to be ridden between the ages of two and four. Although Thoroughbred race horses are put on the track as young as the age of two in some countries, horses specifically bred for sports such as dressage are generally not put under saddle until they are three or four years old, because their bones and muscles are not solidly developed. For endurance riding competition, horses are not deemed mature enough to compete until they are a full 60 calendar months (five years) old.

The horse skeleton averages 205 bones. A significant difference between the horse skeleton and that of a human is the lack of a collarbone—the horse's forelimbs are attached to the spinal column by a powerful set of muscles, tendons, and ligaments that attach the shoulder blade to the torso. The horse's legs and hooves are also unique structures. Their leg bones are proportioned differently from those of a human. For example, the body part that is called a horse's "knee" is actually made up of the carpal bones that correspond to the human wrist. Similarly, the hock contains bones equivalent to those in the human ankle and heel. The lower leg bones of a horse correspond to the bones of the human hand or foot, and the fetlock (incorrectly called the "ankle") is actually the proximal sesamoid bones between the cannon bones (a single equivalent to the human metacarpal or metatarsal bones) and the proximal phalanges, located where one finds the "knuckles" of a human. A horse also has no muscles in its legs below the knees and hocks, only skin, hair, bone, tendons, ligaments, cartilage, and the assorted specialized tissues that make up the hoof.

The critical importance of the feet and legs is summed up by the traditional adage, "no foot, no horse". The horse hoof begins with the distal phalanges, the equivalent of the human fingertip or tip of the toe, surrounded by cartilage and other specialized, blood-rich soft tissues such as the laminae. The exterior hoof wall and horn of the sole is made of keratin, the same material as a human fingernail. The end result is that a horse, weighing on average , travels on the same bones as would a human on tiptoe. For the protection of the hoof under certain conditions, some horses have horseshoes placed on their feet by a professional farrier. The hoof continually grows, and in most domesticated horses needs to be trimmed (and horseshoes reset, if used) every five to eight weeks, though the hooves of horses in the wild wear down and regrow at a rate suitable for their terrain.

Horses are adapted to grazing. In an adult horse, there are 12 incisors at the front of the mouth, adapted to biting off the grass or other vegetation. There are 24 teeth adapted for chewing, the premolars and molars, at the back of the mouth. Stallions and geldings have four additional teeth just behind the incisors, a type of canine teeth called "tushes". Some horses, both male and female, will also develop one to four very small vestigial teeth in front of the molars, known as "wolf" teeth, which are generally removed because they can interfere with the bit. There is an empty interdental space between the incisors and the molars where the bit rests directly on the gums, or "bars" of the horse's mouth when the horse is bridled.

An estimate of a horse's age can be made from looking at its teeth. The teeth continue to erupt throughout life and are worn down by grazing. Therefore, the incisors show changes as the horse ages; they develop a distinct wear pattern, changes in tooth shape, and changes in the angle at which the chewing surfaces meet. This allows a very rough estimate of a horse's age, although diet and veterinary care can also affect the rate of tooth wear.

Horses are herbivores with a digestive system adapted to a forage diet of grasses and other plant material, consumed steadily throughout the day. Therefore, compared to humans, they have a relatively small stomach but very long intestines to facilitate a steady flow of nutrients. A horse will eat of food per day and, under normal use, drink of water. Horses are not ruminants, they have only one stomach, like humans, but unlike humans, they can utilize cellulose, a major component of grass. Horses are hindgut fermenters. Cellulose fermentation by symbiotic bacteria occurs in the cecum, or "water gut", which food goes through before reaching the large intestine. Horses cannot vomit, so digestion problems can quickly cause colic, a leading cause of death.

The horses' senses are based on their status as prey animals, where they must be aware of their surroundings at all times. They have the largest eyes of any land mammal, and are lateral-eyed, meaning that their eyes are positioned on the sides of their heads. This means that horses have a range of vision of more than 350°, with approximately 65° of this being binocular vision and the remaining 285° monocular vision. Horses have excellent day and night vision, but they have two-color, or dichromatic vision; their color vision is somewhat like red-green color blindness in humans, where certain colors, especially red and related colors, appear as a shade of green.

Their sense of smell, while much better than that of humans, is not quite as good as that of a dog. It is believed to play a key role in the social interactions of horses as well as detecting other key scents in the environment. Horses have two olfactory centers. The first system is in the nostrils and nasal cavity, which analyze a wide range of odors. The second, located under the nasal cavity, are the Vomeronasal organs, also called Jacobson's organs. These have a separate nerve pathway to the brain and appear to primarily analyze pheromones.

A horse's hearing is good, and the pinna of each ear can rotate up to 180°, giving the potential for 360° hearing without having to move the head. Noise impacts the behavior of horses and certain kinds of noise may contribute to stress: A 2013 study in the UK indicated that stabled horses were calmest in a quiet setting, or if listening to country or classical music, but displayed signs of nervousness when listening to jazz or rock music. This study also recommended keeping music under a volume of 21 decibels. An Australian study found that stabled racehorses listening to talk radio had a higher rate of gastric ulcers than horses listening to music, and racehorses stabled where a radio was played had a higher overall rate of ulceration than horses stabled where there was no radio playing.

Horses have a great sense of balance, due partly to their ability to feel their footing and partly to highly developed proprioception—the unconscious sense of where the body and limbs are at all times. A horse's sense of touch is well developed. The most sensitive areas are around the eyes, ears, and nose. Horses are able to sense contact as subtle as an insect landing anywhere on the body.

Horses have an advanced sense of taste, which allows them to sort through fodder and choose what they would most like to eat, and their prehensile lips can easily sort even small grains. Horses generally will not eat poisonous plants, however, there are exceptions; horses will occasionally eat toxic amounts of poisonous plants even when there is adequate healthy food.

All horses move naturally with four basic gaits: the four-beat walk, which averages ; the two-beat trot or jog at (faster for harness racing horses); the canter or lope, a three-beat gait that is ; and the gallop. The gallop averages , but the world record for a horse galloping over a short, sprint distance is . Besides these basic gaits, some horses perform a two-beat pace, instead of the trot. There also are several four-beat "ambling" gaits that are approximately the speed of a trot or pace, though smoother to ride. These include the lateral rack, running walk, and tölt as well as the diagonal fox trot. Ambling gaits are often genetic in some breeds, known collectively as gaited horses. Often, gaited horses replace the trot with one of the ambling gaits.

Horses are prey animals with a strong fight-or-flight response. Their first reaction to threat is to startle and usually flee, although they will stand their ground and defend themselves when flight is impossible or if their young are threatened. They also tend to be curious; when startled, they will often hesitate an instant to ascertain the cause of their fright, and may not always flee from something that they perceive as non-threatening. Most light horse riding breeds were developed for speed, agility, alertness and endurance; natural qualities that extend from their wild ancestors. However, through selective breeding, some breeds of horses are quite docile, particularly certain draft horses.

Horses are herd animals, with a clear hierarchy of rank, led by a dominant individual, usually a mare. They are also social creatures that are able to form companionship attachments to their own species and to other animals, including humans. They communicate in various ways, including vocalizations such as nickering or whinnying, mutual grooming, and body language. Many horses will become difficult to manage if they are isolated, but with training, horses can learn to accept a human as a companion, and thus be comfortable away from other horses. However, when confined with insufficient companionship, exercise, or stimulation, individuals may develop stable vices, an assortment of bad habits, mostly stereotypies of psychological origin, that include wood chewing, wall kicking, "weaving" (rocking back and forth), and other problems.

Studies have indicated that horses perform a number of cognitive tasks on a daily basis, meeting mental challenges that include food procurement and identification of individuals within a social system. They also have good spatial discrimination abilities. They are naturally curious and apt to investigate things they have not seen before. Studies have assessed equine intelligence in areas such as problem solving, speed of learning, and memory. Horses excel at simple learning, but also are able to use more advanced cognitive abilities that involve categorization and concept learning. They can learn using habituation, desensitization, classical conditioning, and operant conditioning, and positive and negative reinforcement. One study has indicated that horses can differentiate between "more or less" if the quantity involved is less than four.

Domesticated horses may face greater mental challenges than wild horses, because they live in artificial environments that prevent instinctive behavior whilst also learning tasks that are not natural. Horses are animals of habit that respond well to regimentation, and respond best when the same routines and techniques are used consistently. One trainer believes that "intelligent" horses are reflections of intelligent trainers who effectively use response conditioning techniques and positive reinforcement to train in the style that best fits with an individual animal's natural inclinations.

Horses are mammals, and as such are warm-blooded, or endothermic creatures, as opposed to cold-blooded, or poikilothermic animals. However, these words have developed a separate meaning in the context of equine terminology, used to describe temperament, not body temperature. For example, the "hot-bloods", such as many race horses, exhibit more sensitivity and energy, while the "cold-bloods", such as most draft breeds, are quieter and calmer. Sometimes "hot-bloods" are classified as "light horses" or "riding horses", with the "cold-bloods" classified as "draft horses" or "work horses".
"Hot blooded" breeds include "oriental horses" such as the Akhal-Teke, Arabian horse, Barb and now-extinct Turkoman horse, as well as the Thoroughbred, a breed developed in England from the older oriental breeds. Hot bloods tend to be spirited, bold, and learn quickly. They are bred for agility and speed. They tend to be physically refined—thin-skinned, slim, and long-legged. The original oriental breeds were brought to Europe from the Middle East and North Africa when European breeders wished to infuse these traits into racing and light cavalry horses.

Muscular, heavy draft horses are known as "cold bloods", as they are bred not only for strength, but also to have the calm, patient temperament needed to pull a plow or a heavy carriage full of people. They are sometimes nicknamed "gentle giants". Well-known draft breeds include the Belgian and the Clydesdale. Some, like the Percheron, are lighter and livelier, developed to pull carriages or to plow large fields in drier climates. Others, such as the Shire, are slower and more powerful, bred to plow fields with heavy, clay-based soils. The cold-blooded group also includes some pony breeds.

"Warmblood" breeds, such as the Trakehner or Hanoverian, developed when European carriage and war horses were crossed with Arabians or Thoroughbreds, producing a riding horse with more refinement than a draft horse, but greater size and milder temperament than a lighter breed. Certain pony breeds with warmblood characteristics have been developed for smaller riders. Warmbloods are considered a "light horse" or "riding horse".

Today, the term "Warmblood" refers to a specific subset of sport horse breeds that are used for competition in dressage and show jumping. Strictly speaking, the term "warm blood" refers to any cross between cold-blooded and hot-blooded breeds. Examples include breeds such as the Irish Draught or the Cleveland Bay. The term was once used to refer to breeds of light riding horse other than Thoroughbreds or Arabians, such as the Morgan horse.

Horses are able to sleep both standing up and lying down. In an adaptation from life in the wild, horses are able to enter light sleep by using a "stay apparatus" in their legs, allowing them to doze without collapsing. Horses sleep better when in groups because some animals will sleep while others stand guard to watch for predators. A horse kept alone will not sleep well because its instincts are to keep a constant eye out for danger.

Unlike humans, horses do not sleep in a solid, unbroken period of time, but take many short periods of rest. Horses spend four to fifteen hours a day in standing rest, and from a few minutes to several hours lying down. Total sleep time in a 24-hour period may range from several minutes to a couple of hours, mostly in short intervals of about 15 minutes each. The average sleep time of a domestic horse is said to be 2.9 hours per day.

Horses must lie down to reach REM sleep. They only have to lie down for an hour or two every few days to meet their minimum REM sleep requirements. However, if a horse is never allowed to lie down, after several days it will become sleep-deprived, and in rare cases may suddenly collapse as it involuntarily slips into REM sleep while still standing. This condition differs from narcolepsy, although horses may also suffer from that disorder.

The horse adapted to survive in areas of wide-open terrain with sparse vegetation, surviving in an ecosystem where other large grazing animals, especially ruminants, could not. Horses and other equids are odd-toed ungulates of the order Perissodactyla, a group of mammals that was dominant during the Tertiary period. In the past, this order contained 14 families, but only three—Equidae (the horse and related species), Tapiridae (the tapir), and Rhinocerotidae (the rhinoceroses)—have survived to the present day.

The earliest known member of the family Equidae was the "Hyracotherium", which lived between 45 and 55 million years ago, during the Eocene period. It had 4 toes on each front foot, and 3 toes on each back foot. The extra toe on the front feet soon disappeared with the "Mesohippus", which lived 32 to 37 million years ago. Over time, the extra side toes shrank in size until they vanished. All that remains of them in modern horses is a set of small vestigial bones on the leg below the knee, known informally as splint bones. Their legs also lengthened as their toes disappeared until they were a hooved animal capable of running at great speed. By about 5 million years ago, the modern "Equus" had evolved. Equid teeth also evolved from browsing on soft, tropical plants to adapt to browsing of drier plant material, then to grazing of tougher plains grasses. Thus proto-horses changed from leaf-eating forest-dwellers to grass-eating inhabitants of semi-arid regions worldwide, including the steppes of Eurasia and the Great Plains of North America.

By about 15,000 years ago, "Equus ferus" was a widespread holarctic species. Horse bones from this time period, the late Pleistocene, are found in Europe, Eurasia, Beringia, and North America. Yet between 10,000 and 7,600 years ago, the horse became extinct in North America and rare elsewhere. The reasons for this extinction are not fully known, but one theory notes that extinction in North America paralleled human arrival. Another theory points to climate change, noting that approximately 12,500 years ago, the grasses characteristic of a steppe ecosystem gave way to shrub tundra, which was covered with unpalatable plants.

A truly wild horse is a species or subspecies with no ancestors that were ever domesticated. Therefore, most "wild" horses today are actually feral horses, animals that escaped or were turned loose from domestic herds and the descendants of those animals. Only two never-domesticated subspecies, the Tarpan and the Przewalski's Horse, survived into recorded history and only the latter survives today.

The Przewalski's horse ("Equus ferus przewalskii"), named after the Russian explorer Nikolai Przhevalsky, is a rare Asian animal. It is also known as the Mongolian wild horse; Mongolian people know it as the "taki", and the Kyrgyz people call it a "kirtag". The subspecies was presumed extinct in the wild between 1969 and 1992, while a small breeding population survived in zoos around the world. In 1992, it was reestablished in the wild due to the conservation efforts of numerous zoos. Today, a small wild breeding population exists in Mongolia. There are additional animals still maintained at zoos throughout the world.

The tarpan or European wild horse ("Equus ferus ferus") was found in Europe and much of Asia. It survived into the historical era, but became extinct in 1909, when the last captive died in a Russian zoo. Thus, the genetic line was lost. Attempts have been made to recreate the tarpan, which resulted in horses with outward physical similarities, but nonetheless descended from domesticated ancestors and not true wild horses.

Periodically, populations of horses in isolated areas are speculated to be relict populations of wild horses, but generally have been proven to be feral or domestic. For example, the Riwoche horse of Tibet was proposed as such, but testing did not reveal genetic differences from domesticated horses. Similarly, the Sorraia of Portugal was proposed as a direct descendant of the Tarpan based on shared characteristics, but genetic studies have shown that the Sorraia is more closely related to other horse breeds and that the outward similarity is an unreliable measure of relatedness.

Besides the horse, there are seven other species of genus "Equus" in the Equidae family. These are the ass or donkey, "Equus asinus"; the mountain zebra, "Equus zebra"; plains zebra, "Equus quagga"; Grévy's zebra, "Equus grevyi"; the kiang, "Equus kiang"; and the onager, "Equus hemionus".

Horses can crossbreed with other members of their genus. The most common hybrid is the mule, a cross between a "jack" (male donkey) and a mare. A related hybrid, a hinny, is a cross between a stallion and a jenny (female donkey). Other hybrids include the zorse, a cross between a zebra and a horse. With rare exceptions, most hybrids are sterile and cannot reproduce.

Domestication of the horse most likely took place in central Asia prior to 3500 BC. Two major sources of information are used to determine where and when the horse was first domesticated and how the domesticated horse spread around the world. The first source is based on palaeological and archaeological discoveries; the second source is a comparison of DNA obtained from modern horses to that from bones and teeth of ancient horse remains.

The earliest archaeological evidence for the domestication of the horse comes from sites in Ukraine and Kazakhstan, dating to approximately 3500–4000 BC. By 3000 BC, the horse was completely domesticated and by 2000 BC there was a sharp increase in the number of horse bones found in human settlements in northwestern Europe, indicating the spread of domesticated horses throughout the continent. The most recent, but most irrefutable evidence of domestication comes from sites where horse remains were interred with chariots in graves of the Sintashta and Petrovka cultures c. 2100 BC.

Domestication is also studied by using the genetic material of present-day horses and comparing it with the genetic material present in the bones and teeth of horse remains found in archaeological and palaeological excavations. The variation in the genetic material shows that very few wild stallions contributed to the domestic horse, while many mares were part of early domesticated herds. This is reflected in the difference in genetic variation between the DNA that is passed on along the paternal, or sire line (Y-chromosome) versus that passed on along the maternal, or dam line (mitochondrial DNA). There are very low levels of Y-chromosome variability, but a great deal of genetic variation in mitochondrial DNA. There is also regional variation in mitochondrial DNA due to the inclusion of wild mares in domestic herds. Another characteristic of domestication is an increase in coat color variation. In horses, this increased dramatically between 5000 and 3000 BC.

Before the availability of DNA techniques to resolve the questions related to the domestication of the horse, various hypotheses were proposed. One classification was based on body types and conformation, suggesting the presence of four basic prototypes that had adapted to their environment prior to domestication. Another hypothesis held that the four prototypes originated from a single wild species and that all different body types were entirely a result of selective breeding after domestication. However, the lack of a detectable substructure in the horse has resulted in a rejection of both hypotheses.

Feral horses are born and live in the wild, but are descended from domesticated animals. Many populations of feral horses exist throughout the world. Studies of feral herds have provided useful insights into the behavior of prehistoric horses, as well as greater understanding of the instincts and behaviors that drive horses that live in domesticated conditions.

There are also semi-feral horses in many parts of the world, such as Dartmoor and the New Forest in the UK, where the animals are all privately owned but live for significant amounts of time in "wild" conditions on undeveloped, often public, lands. Owners of such animals often pay a fee for grazing rights.

The concept of purebred bloodstock and a controlled, written breed registry has come to be particularly significant and important in modern times. Sometimes purebred horses are incorrectly or inaccurately called "thoroughbreds". Thoroughbred is a specific breed of horse, while a "purebred" is a horse (or any other animal) with a defined pedigree recognized by a breed registry. Horse breeds are groups of horses with distinctive characteristics that are transmitted consistently to their offspring, such as conformation, color, performance ability, or disposition. These inherited traits result from a combination of natural crosses and artificial selection methods. Horses have been selectively bred since their domestication. An early example of people who practiced selective horse breeding were the Bedouin, who had a reputation for careful practices, keeping extensive pedigrees of their Arabian horses and placing great value upon pure bloodlines. These pedigrees were originally transmitted via an oral tradition. In the 14th century, Carthusian monks of southern Spain kept meticulous pedigrees of bloodstock lineages still found today in the Andalusian horse.

Breeds developed due to a need for "form to function", the necessity to develop certain characteristics in order to perform a particular type of work. Thus, a powerful but refined breed such as the Andalusian developed as riding horses with an aptitude for dressage. Heavy draft horses developed out of a need to perform demanding farm work and pull heavy wagons. Other horse breeds developed specifically for light agricultural work, carriage and road work, various sport disciplines, or simply as pets. Some breeds developed through centuries of crossing other breeds, while others descended from a single foundation sire, or other limited or restricted foundation bloodstock. One of the earliest formal registries was General Stud Book for Thoroughbreds, which began in 1791 and traced back to the foundation bloodstock for the breed. There are more than 300 horse breeds in the world today.

Worldwide, horses play a role within human cultures and have done so for millennia. The genetic makeup of the human population in a geographical area is affected by the presence or absence of horses (more variation in Africa, less in Eurasian steppes). Societies where horse riding is an integral part of life have developed traditional attires specially suited for horse riding such as tightly wrapping waistbands or Cummerbunds giving wide support useful for protecting the spine during long journeys, and voluminous headgear such as turban to protect the skull during falls from the horse. Horses are used for leisure activities, sports, and working purposes. The Food and Agriculture Organization (FAO) estimates that in 2008, there were almost 59,000,000 horses in the world, with around 33,500,000 in the Americas, 13,800,000 in Asia and 6,300,000 in Europe and smaller portions in Africa and Oceania. There are estimated to be 9,500,000 horses in the United States alone. The American Horse Council estimates that horse-related activities have a direct impact on the economy of the United States of over $39 billion, and when indirect spending is considered, the impact is over $102 billion. In a 2004 "poll" conducted by Animal Planet, more than 50,000 viewers from 73 countries voted for the horse as the world's 4th favorite animal.

Communication between human and horse is paramount in any equestrian activity; to aid this process horses are usually ridden with a saddle on their backs to assist the rider with balance and positioning, and a bridle or related headgear to assist the rider in maintaining control. Sometimes horses are ridden without a saddle, and occasionally, horses are trained to perform without a bridle or other headgear. Many horses are also driven, which requires a harness, bridle, and some type of vehicle.

Historically, equestrians honed their skills through games and races. Equestrian sports provided entertainment for crowds and honed the excellent horsemanship that was needed in battle. Many sports, such as dressage, eventing and show jumping, have origins in military training, which were focused on control and balance of both horse and rider. Other sports, such as rodeo, developed from practical skills such as those needed on working ranches and stations. Sport hunting from horseback evolved from earlier practical hunting techniques. Horse racing of all types evolved from impromptu competitions between riders or drivers. All forms of competition, requiring demanding and specialized skills from both horse and rider, resulted in the systematic development of specialized breeds and equipment for each sport. The popularity of equestrian sports through the centuries has resulted in the preservation of skills that would otherwise have disappeared after horses stopped being used in combat.

Horses are trained to be ridden or driven in a variety of sporting competitions. Examples include show jumping, dressage, three-day eventing, competitive driving, endurance riding, gymkhana, rodeos, and fox hunting. Horse shows, which have their origins in medieval European fairs, are held around the world. They host a huge range of classes, covering all of the mounted and harness disciplines, as well as "In-hand" classes where the horses are led, rather than ridden, to be evaluated on their conformation. The method of judging varies with the discipline, but winning usually depends on style and ability of both horse and rider.
Sports such as polo do not judge the horse itself, but rather use the horse as a partner for human competitors as a necessary part of the game. Although the horse requires specialized training to participate, the details of its performance are not judged, only the result of the rider's actions—be it getting a ball through a goal or some other task. Examples of these sports of partnership between human and horse include jousting, in which the main goal is for one rider to unseat the other, and buzkashi, a team game played throughout Central Asia, the aim being to capture a goat carcass while on horseback.

Horse racing is an equestrian sport and major international industry, watched in almost every nation of the world. There are three types: "flat" racing; steeplechasing, i.e. racing over jumps; and harness racing, where horses trot or pace while pulling a driver in a small, light cart known as a sulky. A major part of horse racing's economic importance lies in the gambling associated with it.

There are certain jobs that horses do very well, and no technology has yet developed to fully replace them. For example, mounted police horses are still effective for certain types of patrol duties and crowd control. Cattle ranches still require riders on horseback to round up cattle that are scattered across remote, rugged terrain. Search and rescue organizations in some countries depend upon mounted teams to locate people, particularly hikers and children, and to provide disaster relief assistance. Horses can also be used in areas where it is necessary to avoid vehicular disruption to delicate soil, such as nature reserves. They may also be the only form of transport allowed in wilderness areas. Horses are quieter than motorized vehicles. Law enforcement officers such as park rangers or game wardens may use horses for patrols, and horses or mules may also be used for clearing trails or other work in areas of rough terrain where vehicles are less effective.

Although machinery has replaced horses in many parts of the world, an estimated 100 million horses, donkeys and mules are still used for agriculture and transportation in less developed areas. This number includes around 27 million working animals in Africa alone. Some land management practices such as cultivating and logging can be efficiently performed with horses. In agriculture, less fossil fuel is used and increased environmental conservation occurs over time with the use of draft animals such as horses. Logging with horses can result in reduced damage to soil structure and less damage to trees due to more selective logging.

Horses have been used in warfare for most of recorded history. The first archaeological evidence of horses used in warfare dates to between 4000 and 3000 BC, and the use of horses in warfare was widespread by the end of the Bronze Age. Although mechanization has largely replaced the horse as a weapon of war, horses are still seen today in limited military uses, mostly for ceremonial purposes, or for reconnaissance and transport activities in areas of rough terrain where motorized vehicles are ineffective. Horses have been used in the 21st century by the Janjaweed militias in the War in Darfur.

Modern horses are often used to reenact many of their historical work purposes. Horses are used, complete with equipment that is authentic or a meticulously recreated replica, in various live action historical reenactments of specific periods of history, especially recreations of famous battles. Horses are also used to preserve cultural traditions and for ceremonial purposes. Countries such as the United Kingdom still use horse-drawn carriages to convey royalty and other VIPs to and from certain culturally significant events. Public exhibitions are another example, such as the Budweiser Clydesdales, seen in parades and other public settings, a team of draft horses that pull a beer wagon similar to that used before the invention of the modern motorized truck.

Horses are frequently used in television, films and literature. They are sometimes featured as a major character in films about particular animals, but also used as visual elements that assure the accuracy of historical stories. Both live horses and iconic images of horses are used in advertising to promote a variety of products. The horse frequently appears in coats of arms in heraldry, in a variety of poses and equipment. The mythologies of many cultures, including Greco-Roman, Hindu, Islamic, and Norse, include references to both normal horses and those with wings or additional limbs, and multiple myths also call upon the horse to draw the chariots of the Moon and Sun. The horse also appears in the 12-year cycle of animals in the Chinese zodiac related to the Chinese calendar.

People of all ages with physical and mental disabilities obtain beneficial results from association with horses. Therapeutic riding is used to mentally and physically stimulate disabled persons and help them improve their lives through improved balance and coordination, increased self-confidence, and a greater feeling of freedom and independence. The benefits of equestrian activity for people with disabilities has also been recognized with the addition of equestrian events to the Paralympic Games and recognition of para-equestrian events by the International Federation for Equestrian Sports (FEI). Hippotherapy and therapeutic horseback riding are names for different physical, occupational, and speech therapy treatment strategies that utilize equine movement. In hippotherapy, a therapist uses the horse's movement to improve their patient's cognitive, coordination, balance, and fine motor skills, whereas therapeutic horseback riding uses specific riding skills.

Horses also provide psychological benefits to people whether they actually ride or not. "Equine-assisted" or "equine-facilitated" therapy is a form of experiential psychotherapy that uses horses as companion animals to assist people with mental illness, including anxiety disorders, psychotic disorders, mood disorders, behavioral difficulties, and those who are going through major life changes. There are also experimental programs using horses in prison settings. Exposure to horses appears to improve the behavior of inmates and help reduce recidivism when they leave.

Horses are raw material for many products made by humans throughout history, including byproducts from the slaughter of horses as well as materials collected from living horses.

Products collected from living horses include mare's milk, used by people with large horse herds, such as the Mongols, who let it ferment to produce kumis. Horse blood was once used as food by the Mongols and other nomadic tribes, who found it a convenient source of nutrition when traveling. Drinking their own horses' blood allowed the Mongols to ride for extended periods of time without stopping to eat. The drug Premarin is a mixture of estrogens extracted from the urine of pregnant mares (pregnant mares' urine), and was previously a widely used drug for hormone replacement therapy. The tail hair of horses can be used for making bows for string instruments such as the violin, viola, cello, and double bass.

Horse meat has been used as food for humans and carnivorous animals throughout the ages. It is eaten in many parts of the world, though consumption is taboo in some cultures, and a subject of political controversy in others. Horsehide leather has been used for boots, gloves, jackets, baseballs, and baseball gloves. Horse hooves can also be used to produce animal glue. Horse bones can be used to make implements. Specifically, in Italian cuisine, the horse tibia is sharpened into a probe called a "spinto", which is used to test the readiness of a (pig) ham as it cures. In Asia, the saba is a horsehide vessel used in the production of kumis.

Horses are grazing animals, and their major source of nutrients is good-quality forage from hay or pasture. They can consume approximately 2% to 2.5% of their body weight in dry feed each day. Therefore, a adult horse could eat up to of food. Sometimes, concentrated feed such as grain is fed in addition to pasture or hay, especially when the animal is very active. When grain is fed, equine nutritionists recommend that 50% or more of the animal's diet by weight should still be forage.

Horses require a plentiful supply of clean water, a minimum of to per day. Although horses are adapted to live outside, they require shelter from the wind and precipitation, which can range from a simple shed or shelter to an elaborate stable.

Horses require routine hoof care from a farrier, as well as vaccinations to protect against various diseases, and dental examinations from a veterinarian or a specialized equine dentist. If horses are kept inside in a barn, they require regular daily exercise for their physical health and mental well-being. When turned outside, they require well-maintained, sturdy fences to be safely contained. Regular grooming is also helpful to help the horse maintain good health of the hair coat and underlying skin.




</doc>
<doc id="13647" url="https://en.wikipedia.org/wiki?curid=13647" title="Hermann Ebbinghaus">
Hermann Ebbinghaus

Hermann Ebbinghaus (January 24, 1850 – February 26, 1909) was a German psychologist who pioneered the experimental study of memory, and is known for his discovery of the forgetting curve and the spacing effect. He was also the first person to describe the learning curve. He was the father of the eminent neo-Kantian philosopher Julius Ebbinghaus.

Ebbinghaus was born in Barmen, in the Rhine Province of the Kingdom of Prussia, as the son of a wealthy merchant, Carl Ebbinghaus. Little is known about his infancy except that he was brought up in the Lutheran faith and was a pupil at the town Gymnasium. At the age of 17 (1867), he began attending the University of Bonn, where he had planned to study history and philology. However, during his time there he developed an interest in philosophy. In 1870, his studies were interrupted when he served with the Prussian Army in the Franco-Prussian War. Following this short stint in the military, Ebbinghaus finished his dissertation on Eduard von Hartmann's "" (philosophy of the unconscious) and received his doctorate on August 16, 1873, when he was 23 years old. During the next three years, he spent time at Halle and Berlin.

After acquiring his PhD, Ebbinghaus moved around England and France, tutoring students to support himself. In England, he may have taught in two small schools in the south of the country (Gorfein, 1885). In London, in a used bookstore, he came across Gustav Fechner's book "Elemente der Psychophysik" ("Elements of Psychophysics"), which spurred him to conduct his famous memory experiments. After beginning his studies at the University of Berlin, he founded the third psychological testing lab in Germany (third to Wilhelm Wundt and Georg Elias Müller). He began his memory studies here in 1879. In 1885 — the same year that he published his monumental work, "Über das Gedächtnis. Untersuchungen zur experimentellen Psychologie", later published in English under the title "Memory: A Contribution to Experimental Psychology" — he was made a professor at the University of Berlin, most likely in recognition of this publication. In 1890, along with Arthur König, he founded the psychological journal "Zeitschrift für Physiologie und Psychologie der Sinnesorgane" ("The Psychology and Physiology of the Sense Organs'").

In 1894, he was passed over for promotion to head of the philosophy department at Berlin, most likely due to his lack of publications. Instead, Carl Stumpf received the promotion. As a result of this, Ebbinghaus left to join the University of Breslau (now Wrocław, Poland), in a chair left open by Theodor Lipps (who took over Stumpf's position when he moved to Berlin). While in Breslau, he worked on a commission that studied how children's mental ability declined during the school day. While the specifics on how these mental abilities were measured have been lost, the successes achieved by the commission laid the groundwork for future intelligence testing. At Breslau, he again founded a psychological testing laboratory.

In 1902, Ebbinghaus published his next piece of writing entitled "Die Grundzüge der Psychologie" ("Fundamentals of Psychology"). It was an instant success and continued to be long after his death. In 1904, he moved to Halle where he spent the last few years of his life. His last published work, "Abriss der Psychologie" ("Outline of Psychology") was published six years later, in 1908. This, too, continued to be a success, being re-released in eight different editions. Shortly after this publication, on February 26, 1909, Ebbinghaus died from pneumonia at the age of 59.

Ebbinghaus was determined to show that higher mental processes could actually be studied using experimentation, which was in opposition to the popularly held thought of the time. To control for most potentially confounding variables, Ebbinghaus wanted to use simple acoustic encoding and maintenance rehearsal for which a list of words could have been used. As learning would be affected by prior knowledge and understanding, he needed something that could be easily memorized but which had no prior cognitive associations. Easily formable associations with regular words would interfere with his results, so he used items that would later be called "nonsense syllables" (also known as the CVC trigram). A nonsense syllable is a consonant-vowel-consonant combination, where the consonant does not repeat and the syllable does not have prior meaning. BOL (sounds like "Ball") and DOT (already a word) would then not be allowed. However, syllables such as DAX, BOK, and YAT would all be acceptable (though Ebbinghaus left no examples). After eliminating the meaning-laden syllables, Ebbinghaus ended up with 2,300 resultant syllables. Once he had created his collection of syllables, he would pull out a number of random syllables from a box and then write them down in a notebook. Then, to the regular sound of a metronome, and with the same voice inflection, he would read out the syllables, and attempt to recall them at the end of the procedure. One investigation alone required 15,000 recitations.

It was later determined that humans impose meaning even on nonsense syllables to make them more meaningful. The nonsense syllable PED (which is the first three letters of the word "pedal") turns out to be less nonsensical than a syllable such as KOJ; the syllables are said to differ in association value. It appears that Ebbinghaus recognized this, and only referred to the strings of syllables as "nonsense" in that the syllables might be less likely to have a specific meaning and he should make no attempt to make associations with them for easier retrieval.

There are several limitations to his work on memory. The most important one was that Ebbinghaus was the only subject in his study. This limited the study's generalizability to the population. Although he attempted to regulate his daily routine to maintain more control over his results, his decision to avoid the use of participants sacrificed the external validity of the study despite sound internal validity. In addition, although he tried to account for his personal influences, there is an inherent bias when someone serves as researcher as well as participant. Also, Ebbinghaus's memory research halted research in other, more complex matters of memory such as semantic and procedural memory and mnemonics.

In 1885, he published his groundbreaking "Über das Gedächtnis" ("On Memory", later translated to English as "Memory. A Contribution to Experimental Psychology") in which he described experiments he conducted on himself to describe the processes of learning and forgetting.

Ebbinghaus made several findings that are still relevant and supported to this day. First, arguably his most famous finding, the forgetting curve. The forgetting curve describes the exponential loss of information that one has learned. The sharpest decline occurs in the first twenty minutes and the decay is significant through the first hour. The curve levels off after about one day.

The learning curve described by Ebbinghaus refers to how fast one learns information. The sharpest increase occurs after the first try and then gradually evens out, meaning that less and less new information is retained after each repetition. Like the forgetting curve, the learning curve is exponential. Ebbinghaus had also documented the serial position effect, which describes how the position of an item affects recall. The two main concepts in the serial position effect are recency and primacy. The recency effect describes the increased recall of the most recent information because it is still in the short-term memory. The primacy effect causes better memory of the first items in a list due to increased rehearsal and commitment to long-term memory.

Another important discovery is that of savings. This refers to the amount of information retained in the subconscious even after this information cannot be consciously accessed. Ebbinghaus would memorize a list of items until perfect recall and then would not access the list until he could no longer recall any of its items. He then would relearn the list, and compare the new learning curve to the learning curve of his previous memorization of the list. The second list was generally memorized faster, and this difference between the two learning curves is what Ebbinghaus called "savings". Ebbinghaus also described the difference between involuntary and voluntary memory, the former occurring "with apparent spontaneity and without any act of the will" and the latter being brought "into consciousness by an exertion of the will".

Prior to Ebbinghaus, most contributions to the study of memory were undertaken by philosophers and centered on observational description and speculation. For example, Immanuel Kant used pure description to discuss recognition and its components and Sir Francis Bacon claimed that the simple observation of the rote recollection of a previously learned list was "no use to the art" of memory. This dichotomy between descriptive and experimental study of memory would resonate later in Ebbinghaus's life, particularly in his public argument with former colleague Wilhelm Dilthey. However, more than a century before Ebbinghaus, Johann Andreas Segner invented the "Segner-wheel" to see the length of after-images by seeing how fast a wheel with a hot coal attached had to move for the red ember circle from the coal to appear complete. (see iconic memory)

Ebbinghaus's effect on memory research was almost immediate. With very few works published on memory in the previous two millennia, Ebbinghaus's works spurred memory research in the United States in the 1890s, with 32 papers published in 1894 alone. This research was coupled with the growing development of mechanized mnemometers, or devices that aided in the recording and study of memory.

The reaction to his work in his day was mostly positive. Noted psychologist William James called the studies "heroic" and said that they were "the single most brilliant investigation in the history of psychology". Edward B. Titchener also mentioned that the studies were the greatest undertaking in the topic of memory since Aristotle.

Ebbinghaus can also be credited with pioneering sentence completion exercises, which he developed in studying the abilities of schoolchildren. It was these same exercises that Alfred Binet had borrowed and incorporated into the Binet-Simon intelligence scale. Sentence completion had since then also been used extensively in memory research, especially in tapping into measures of implicit memory, and also has been used in psychotherapy as a tool to help tap into the motivations and drives of the patient. He had also influenced Charlotte Bühler, who along with Lev Vygotsky and others went on to study language meaning and society.

Ebbinghaus is also largely credited with drafting the first standard research report. In his paper on memory, Ebbinghaus arranged his research into four sections: the introduction, the methods, the results, and a discussion section. The clarity and organization of this format was so impressive to contemporaries that it has now become standard in the discipline, and all research reports follow the same standards laid out by Ebbinghaus.

Unlike notable contemporaries like Titchener and James, Ebbinghaus did not promote any specific school of psychology nor was he known for extensive lifetime research, having done only three works. He never attempted to bestow upon himself the title of the pioneer of experimental psychology, did not seek to have any "disciples", and left the exploitation of the new field to others.

In addition to pioneering experimental psychology, Ebbinghaus was also a strong defender of this direction of the new science, as is illustrated by his public dispute with University of Berlin colleague, Wilhelm Dilthey. Shortly after Ebbinghaus left Berlin in 1893, Dilthey published a paper extolling the virtues of descriptive psychology, and condemning experimental psychology as boring, claiming that the mind was too complex, and that introspection was the desired method of studying the mind. The debate at the time had been primarily whether psychology should aim to explain or understand the mind and whether it belonged to the natural or human sciences. Many had seen Dilthey's work as an outright attack on experimental psychology, Ebbinghaus included, and he responded to Dilthey with a personal letter and also a long scathing public article. Amongst his counterarguments against Dilthey he mentioned that it is inevitable for psychology to do hypothetical work and that the kind of psychology that Dilthey was attacking was the one that existed before Ebbinghaus's "experimental revolution". Charlotte Bühler echoed his words some forty years later, stating that people like Ebbinghaus "buried the old psychology in the 1890s". Ebbinghaus explained his scathing review by saying that he could not believe that Dilthey was advocating the status quo of structuralists like Wilhelm Wundt and Titchener and attempting to stifle psychology's progress.

Some contemporary texts still describe Ebbinghaus as a philosopher rather than a psychologist and he had also spent his life as a professor of philosophy. However, Ebbinghaus himself would probably describe himself as a psychologist considering that he fought to have psychology viewed as a separate discipline from philosophy.

There has been some speculation as to what influenced Ebbinghaus in his undertakings. None of his professors seem to have influenced him, nor are there suggestions that his colleagues affected him. Von Hartmann's work, on which Ebbinghaus based his doctorate, did suggest that higher mental processes were hidden from view, which may have spurred Ebbinghaus to attempt to prove otherwise. The one influence that has always been cited as having inspired Ebbinghaus was Gustav Fechner's two-volume "Elemente der Psychophysik." ("Elements of Psychophysics", 1860), a book which he purchased second-hand in England. It is said that the meticulous mathematical procedures impressed Ebbinghaus so much that he wanted to do for psychology what Fechner had done for psychophysics. This inspiration is also evident in that Ebbinghaus dedicated his second work "Principles of Psychology" to Fechner, signing it "I owe everything to you."




</doc>
<doc id="13648" url="https://en.wikipedia.org/wiki?curid=13648" title="Hilbert (disambiguation)">
Hilbert (disambiguation)

David Hilbert (1862–1943) was a German mathematician.

Hilbert may also refer to:

People:

Places:

Other uses:



</doc>
<doc id="13652" url="https://en.wikipedia.org/wiki?curid=13652" title="Hindi">
Hindi

Hindi (Devanagari: हिन्दी, "Hindī"), or Modern Standard Hindi (Devanagari: मानक हिन्दी, "Mānak Hindī") is a standardised and Sanskritised register of the Hindustani language. Along with the English language, Hindi written in the Devanagari script is the official language of India. 

On 14 September 1949, the Constituent Assembly of India adopted Hindi written in the Devanagari script as the official language of the Republic of India replacing Urdu's previous usage in British India. To this end, several stalwarts rallied and lobbied pan-India in favor of Hindi, most notably along with Hazari Prasad Dwivedi, Kaka Kalelkar, Maithili Sharan Gupt and Seth Govind Das who even debated in Parliament on this issue. As such, on the 50th birthday of on 14 September 1949, the efforts came to fruition following the adoption of Hindi as the official language. It is one of the 22 scheduled languages of the Republic of India. However, it is not the national language of India because no language was given such a status in the Indian constitution.

Hindi is the "lingua franca" of the "Hindi belt", and to a lesser extent the whole of India (usually in a simplified or pidginized variety such as Bazaar Hindustani or Haflong Hindi). Outside India, several other languages are recognized officially as "Hindi" but do not refer to the Standard Hindi language described here and instead descend from other dialects of Hindustani, such as Awadhi and Bhojpuri. Such languages include "Fiji Hindi", which is official in Fiji, and Caribbean Hindustani, which is a recognized language in Trinidad and Tobago, Guyana, and Suriname. Apart from specialized vocabulary, Hindi is mutually intelligible with spoken Urdu, another recognized register of Hindustani.

As a linguistic variety (including the Hindi belt languages except for Urdu), Hindi is the fourth most-spoken first language in the world, after Mandarin, Spanish and English. Alongside Urdu as Hindustani, it is the third most-spoken language in the world, after Mandarin and English.

The term "Hindī" originally was used to refer to inhabitants of the region east of the Indus. It was borrowed from Classical Persian "Hindī" (Iranian Persian "Hendi"), meaning "Indian", from the proper noun "Hind" "India".

The name "Hindavī" was used by Amir Khusrow in his poetry.

Like other Indo-Aryan languages, Hindi is a direct descendant of an early form of Vedic Sanskrit, through Sauraseni Prakrit and Śauraseni Apabhraṃśa (from Sanskrit "apabhraṃśa" "corrupted"), which emerged in the 7th century A.D.

Modern Standard Hindi is based on the Khariboli dialect, the vernacular of Delhi and the surrounding region, which came to replace earlier prestige dialects such as Awadhi, Maithili (sometimes regarded as separate from the Hindi dialect continuum) and Braj. "Urdu" – another form of Hindustani – acquired linguistic prestige in the later Mughal period (1800s), and underwent significant Persian influence. Modern Hindi and its literary tradition evolved towards the end of the 18 century. However, modern Hindi's earlier literary stages before standardization can be traced to the 16th century. In the late 19th century, a movement to further develop Hindi as a standardised form of Hindustani separate from Urdu took form. In 1881, Bihar accepted Hindi as its sole official language, replacing Urdu, and thus became the first state of India to adopt Hindi. Modern Standard Hindi is one of the youngest Indian languages in this regard.

After independence, the government of India instituted the following conventions:

The Constituent Assembly adopted Hindi as an official language of India on 14 September 1949. Now, it is celebrated as Hindi Day.

In Northeast India a pidgin known as Haflong Hindi has developed as a "lingua franca" for various tribes in Assam that speak other languages natively. In Arunachal Pradesh, Hindi emerged as a lingua franca among locals who speak over 50 dialects natively.

Part XVII of the Indian Constitution deals with the official language of the Indian Commonwealth. Under Article 343, the official languages of the Union has been prescribed, which includes Hindi in Devanagari script and English:

(1) The official language of the Union shall be Hindi in Devanagari script. The form of numerals to be used for the official purposes of the Union shall be the international form of Indian numerals.
(2) Notwithstanding anything in clause (1), for a period of fifteen years from the commencement of this Constitution, the English language shall continue to be used for all the official purposes of the Union for which it was being used immediately before such commencement: Provided that the President may, during the said period, by order authorize the use of the Hindi language in addition to the English language and of the Devanagari form of numerals in addition to the international form of Indian numerals for any of the official purposes of the Union

It shall be the duty of the Union to promote the spread of the Hindi language, to develop it so that it may serve as a medium of expression for all the elements of the composite culture of India and to secure its enrichment by assimilating without interfering with its genius, the forms, style and expressions used in Hindustani and in the other languages of India specified in the Eighth Schedule, and by drawing, wherever necessary or desirable, for its vocabulary, primarily on Sanskrit and secondarily on other languages. 

It was envisioned that Hindi would become the sole working language of the Union Government by 1965 (per directives in Article 344 (2) and Article 351), with state governments being free to function in the language of their own choice. However, widespread resistance to the imposition of Hindi on non-native speakers, especially in South India (such as the those in Tamil Nadu) led to the passage of the Official Languages Act of 1963, which provided for the continued use of English indefinitely for all official purposes, although the constitutional directive for the Union Government to encourage the spread of Hindi was retained and has strongly influenced its policies.

At the state level, Hindi is the official language of the following Indian states: Bihar, Chhattisgarh, Haryana, Himachal Pradesh, Jharkhand, Madhya Pradesh, Mizoram, Rajasthan, Uttar Pradesh, Uttarakhand and West Bengal. Each may also designate a "co-official language"; in Uttar Pradesh, for instance, depending on the political formation in power, this language is generally Urdu. Similarly, Hindi is accorded the status of official language in the following Union Territories: Andaman & Nicobar Islands, Chandigarh, Dadra & Nagar Haveli, Daman & Diu, National Capital Territory.

National language status for Hindi is a long-debated theme. In 2010, the Gujarat High Court clarified that Hindi is not the national language of India because the constitution does not mention it as such.

Outside Asia, the Awadhi language (A Hindi dialect) is an official language in Fiji as per the 1997 Constitution of Fiji, where it referred to it as "Hindustani", however in the 2013 Constitution of Fiji, it is simply called "Fiji Hindi". It is spoken by 380,000 people in Fiji.

Hindi is also spoken by a large population of Madheshis (people having roots in north-India but have migrated to Nepal over hundreds of years) of Nepal. Apart from specialized vocabulary, Hindi is mutually intelligible with Standard Urdu, another recognized register of Hindustani. Hindi is quite easy to understand for some Pakistanis, who speak Urdu, which, like Hindi, is part of Hindustani. Apart from this, Hindi is spoken by the large Indian diaspora which hails from, or has its origin from the "Hindi Belt" of India. A substantially large North Indian diaspora lives in countries like The United States of America, the United Kingdom, the United Arab Emirates, Trinidad and Tobago, Guyana, Suriname, South Africa, Fiji and Mauritius, where it is natively spoken at home and among their own Hindustani-speaking communities. Outside India, Hindi speakers are 8 million in Nepal; 649,000 in United States of America; 450,170 in Mauritius; 380,000 in Fiji; 250,292 in South Africa; 150,000 in Suriname; 100,000 in Uganda; 45,800 in United Kingdom; 20,000 in New Zealand; 20,000 in Germany; 16,000 in Trinidad and Tobago; 3,000 in Singapore.

Linguistically, Hindi and Urdu are two registers of the same language and are mutually intelligble. Hindi is written in the Devanagari script and uses more Sanskrit words, whereas Urdu is written in the Perso-Arabic script and uses more Arabic and Persian words. Hindi is the most commonly used official language in India. Urdu is the and "lingua franca" of Pakistan and is one of 22 official languages of India.

The splitting of Hindi and Urdu into separate languages is largely motivated by politics, namely the Indo-Pakistani rivalry.

Hindi is written in the Devanagari script, an abugida. Devanagari consists of 11 vowels and 33 consonants and is written from left to right. Unlike for Sanskrit, Devanagari is not entirely phonetic for Hindi, especially failing to mark schwa dropping in spoken Standard Hindi.

The Government of India uses Hunterian transliteration as its official system of writing Hindi in the Latin script. Various other systems also exist, such as IAST, ITRANS and ISO 15919.

Traditionally, Hindi words are divided into five principal categories according to their etymology:


Hindi also makes extensive use of loan translation (calqueing) and occasionally phono-semantic matching of English.

Hindi has naturally inherited a large portion of its vocabulary from Śaurasenī Prākṛt, in the form of "tadbhava" words. This process usually involves compensatory lengthening of vowels preceding consonant clusters in Prakrit, e.g. Sanskrit "tīkṣṇa" > Prakrit "tikkha" > Hindi "tīkhā".

Much of Modern Standard Hindi's vocabulary is borrowed from Sanskrit as "tatsam" borrowings, especially in technical and academic fields. The formal Hindi standard, from which much of the Persian, Arabic and English vocabulary has been replaced by neologisms compounding "tatsam" words, is called "Śuddh Hindi" (pure Hindi), and is viewed as a more prestigious dialect over other more colloquial forms of Hindi.

Excessive use of "tatsam" words sometimes creates problems for native speakers. They may have Sanskrit consonant clusters which do not exist in native Hindi, causing difficulties in pronunciation.

As a part of the process of Sanskritization, new words are coined using Sanskrit components to be used as replacements for supposedly foreign vocabulary. Usually these neologisms are calques of English words already adopted into spoken Hindi. Some terms such as "dūrbhāṣ" "telephone", literally "far-speech" and "dūrdarśan" "television", literally "far-sight" have even gained some currency in formal Hindi in the place of the English borrowings "(ṭeli)fon" and "ṭīvī".

Hindi literature is broadly divided into four prominent forms or styles, being "Bhakti" (devotional – Kabir, Raskhan); "Śṛṇgār" (beauty – Keshav, Bihari); "Vīgāthā" (epic); and "Ādhunik" (modern).

Medieval Hindi literature is marked by the influence of Bhakti movement and the composition of long, epic poems. It was primarily written in other varieties of Hindi, particularly Avadhi and Braj Bhasha, but to a degree also in Khariboli, the basis for Modern Standard Hindi. During the British Raj, Hindustani became the prestige dialect.

"Chandrakanta", written by Devaki Nandan Khatri in 1888, is considered the first authentic work of prose in modern Hindi. The person who brought realism in the Hindi prose literature was Munshi Premchand, who is considered as the most revered figure in the world of Hindi fiction and progressive movement. Literary, or "Sāhityik", Hindi was popularised by the writings of Swami Dayananda Saraswati, Bhartendu Harishchandra and others. The rising numbers of newspapers and magazines made Hindustani popular with the educated people.

The "Dvivedī Yug" ("Age of Dwivedi") in Hindi literature lasted from 1900 to 1918. It is named after Mahavir Prasad Dwivedi, who played a major role in establishing Modern Standard Hindi in poetry and broadening the acceptable subjects of Hindi poetry from the traditional ones of religion and romantic love.

In the 20th century, Hindi literature saw a romantic upsurge. This is known as "Chāyāvād" ("shadow-ism") and the literary figures belonging to this school are known as "Chāyāvādī". Jaishankar Prasad, Suryakant Tripathi 'Nirala', Mahadevi Varma and Sumitranandan Pant, are the four major "Chāyāvādī" poets.

"Uttar Ādhunik" is the post-modernist period of Hindi literature, marked by a questioning of early trends that copied the West as well as the excessive ornamentation of the "Chāyāvādī" movement, and by a return to simple language and natural themes.

The Hindi Wikipedia was the first Indic-language wiki to reach 100,000 articles. Hindi literature, music, and film have all been disseminated via the internet. In 2015, Google reported a 94% increase in Hindi-content consumption year-on-year, adding that 21% of users in India prefer content in Hindi.

Many Hindi newspapers also offer digital editions.

The following is a sample text in High Hindi, of the Article 1 of the Universal Declaration of Human Rights (by the United Nations):









</doc>
<doc id="13653" url="https://en.wikipedia.org/wiki?curid=13653" title="Huginn and Muninn">
Huginn and Muninn

In Norse mythology, Huginn (from Old Norse "thought") and Muninn (Old Norse "memory" or "mind") are a pair of ravens that fly all over the world, Midgard, and bring information to the god Odin. Huginn and Muninn are attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources: the "Prose Edda" and "Heimskringla", written in the 13th century by Snorri Sturluson; in the "Third Grammatical Treatise", compiled in the 13th century by Óláfr Þórðarson; and in the poetry of skalds. The names of the ravens are sometimes modernly anglicized as Hugin and Munin.

In the "Poetic Edda", a disguised Odin expresses that he fears that they may not return from their daily flights. The "Prose Edda" explains that Odin is referred to as "raven-god" due to his association with Huginn and Muninn. In the "Prose Edda" and the "Third Grammatical Treatise", the two ravens are described as perching on Odin's shoulders. "Heimskringla" details that Odin gave Huginn and Muninn the ability to speak.

Migration Period golden bracteates, Vendel era helmet plates, a pair of identical Germanic Iron Age bird-shaped brooches, Viking Age objects depicting a moustached man wearing a helmet, and a portion of the 10th or 11th century may depict Odin with one of the ravens. Huginn and Muninn's role as Odin's messengers has been linked to shamanic practices, the Norse raven banner, general raven symbolism among the Germanic peoples, and the Norse concepts of the fylgja and the hamingja.

In the "Poetic Edda" poem "Grímnismál", the god Odin (disguised as "Grímnir") provides the young Agnarr with information about Odin's companions. He tells the prince about Odin's wolves Geri and Freki, and, in the next stanza of the poem, states that Huginn and Muninn fly daily across the entire world, Midgard. Grímnir says that he worries Huginn may not come back, yet more does he fear for Muninn:

In the "Prose Edda" book "Gylfaginning" (chapter 38), the enthroned figure of High tells Gangleri (king Gylfi in disguise) that two ravens named Huginn and Muninn sit on Odin's shoulders. The ravens tell Odin everything they see and hear. Odin sends Huginn and Muninn out at dawn, and the birds fly all over the world before returning at dinner-time. As a result, Odin is kept informed of many events. High adds that it is from this association that Odin is referred to as "raven-god". The above-mentioned stanza from "Grímnismál" is then quoted.

In the "Prose Edda" book "Skáldskaparmál" (chapter 60), Huginn and Muninn appear in a list of poetic names for ravens. In the same chapter, excerpts from a work by the skald Einarr Skúlason are provided. In these excerpts Muninn is referenced in a common noun for 'raven' and Huginn is referenced in a kenning for 'carrion'.

In the "Heimskringla" book "Ynglinga saga", an euhemerized account of the life of Odin is provided. Chapter 7 describes that Odin had two ravens, and upon these ravens he bestowed the gift of speech. These ravens flew all over the land and brought him information, causing Odin to become "very wise in his lore."

In the "Third Grammatical Treatise" an anonymous verse is recorded that mentions the ravens flying from Odin's shoulders; Huginn seeking hanged men, and Muninn slain bodies. The verse reads:

Migration Period (5th and 6th centuries CE) gold bracteates (types A, B, and C) feature a depiction of a human figure above a horse, holding a spear and flanked by one or more often two birds. The presence of the birds has led to the iconographic identification of the human figure as the god Odin, flanked by Huginn and Muninn. Like Snorri's "Prose Edda" description of the ravens, a bird is sometimes depicted at the ear of the human, or at the ear of the horse. Bracteates have been found in Denmark, Sweden, Norway and, in smaller numbers, England and areas south of Denmark. Austrian Germanist Rudolf Simek states that these bracteates may depict Odin and his ravens healing a horse and may indicate that the birds were originally not simply his battlefield companions but also "Odin's helpers in his veterinary function."

Vendel era helmet plates (from the 6th or 7th century) found in a grave in Sweden depict a helmeted figure holding a spear and a shield while riding a horse, flanked by two birds. The plate has been interpreted as Odin accompanied by two birds: his ravens.

A pair of identical Germanic Iron Age bird-shaped brooches from Bejsebakke in northern Denmark may be depictions of Huginn and Muninn. The back of each bird features a mask motif, and the feet of the birds are shaped like the heads of animals. The feathers of the birds are also composed of animal heads. Together, the animal heads on the feathers form a mask on the back of the bird. The birds have powerful beaks and fan-shaped tails, indicating that they are ravens. The brooches were intended to be worn on each shoulder, after Germanic Iron Age fashion. Archaeologist Peter Vang Petersen comments that while the symbolism of the brooches is open to debate, the shape of the beaks and tail feathers confirm that the brooch depictions are ravens. Petersen notes that "raven-shaped ornaments worn as a pair, after the fashion of the day, one on each shoulder, makes one's thoughts turn towards Odin's ravens and the cult of Odin in the Germanic Iron Age." Petersen says that Odin is associated with disguise and that the masks on the ravens may be portraits of Odin.

The Oseberg tapestry fragments, discovered within the Viking Age Oseberg ship burial in Norway, feature a scene containing two black birds hovering over a horse, possibly originally leading a wagon (as a part of a procession of horse-led wagons on the tapestry). In her examination of the tapestry, scholar Anne Stine Ingstad interprets these birds as Huginn and Muninn flying over a covered cart containing an image of Odin, drawing comparison with the images of Nerthus attested by Tacitus in 1 CE.

Excavations in Ribe in Denmark have recovered a Viking Age lead metal-caster's mould and 11 identical casting-moulds. These objects depict a moustached man wearing a helmet that features two head-ornaments. Archaeologist Stig Jensen proposes that these ornaments should be interpreted as Huginn and Muninn, and the wearer as Odin. He notes that "similar depictions occur everywhere the Vikings went—from eastern England to Russia and naturally also in the rest of Scandinavia."

A portion of (a partly surviving runestone erected at Kirk Andreas on the Isle of Man) depicts a bearded human holding a spear downward at a wolf, his right foot in its mouth, and a large bird on his shoulder. Andy Orchard comments that this bird may be either Huginn or Muninn. Rundata dates the cross to 940, while Pluskowski dates it to the 11th century. This depiction has been interpreted as Odin, with a raven or eagle at his shoulder, being consumed by the monstrous wolf Fenrir during the events of Ragnarök.

In November 2009, the Roskilde Museum announced the discovery and subsequent display of a niello-inlaid silver figurine found in Lejre, Denmark, which they dubbed "Odin from Lejre". The silver object depicts a person sitting on a throne. The throne features the heads of animals and is flanked by two birds. The Roskilde Museum identifies the figure as Odin sitting on his throne Hliðskjálf, flanked by the ravens Huginn and Muninn.

Scholars have linked Odin's relation to Huginn and Muninn to shamanic practice. John Lindow relates Odin's ability to send his "thought" (Huginn) and "mind" (Muninn) to the trance-state journey of shamans. Lindow says the "Grímnismál" stanza where Odin worries about the return of Huginn and Muninn "would be consistent with the danger that the shaman faces on the trance-state journey."

Rudolf Simek is critical of the approach, stating that "attempts have been made to interpret Odin's ravens as a personification of the god's intellectual powers, but this can only be assumed from the names Huginn and Muninn themselves which were unlikely to have been invented much before the 9th or 10th centuries" yet that the two ravens, as Odin's companions, appear to derive from much earlier times. Instead, Simek connects Huginn and Muninn with wider raven symbolism in the Germanic world, including the raven banner (described in English chronicles and Scandinavian sagas), a banner which was woven in a method that allowed it, when fluttering in the wind, to appear as if the raven depicted upon it was beating its wings.

Anthony Winterbourne connects Huginn and Muninn to the Norse concepts of the fylgja—a concept with three characteristics; shape-shifting abilities, good fortune, and the guardian spirit—and the hamingja—the ghostly double of a person that may appear in the form of an animal. Winterbourne states that "The shaman's journey through the different parts of the cosmos is symbolized by the "hamingja" concept of the shape-shifting soul, and gains another symbolic dimension for the Norse soul in the account of Oðin's ravens, Huginn and Muninn." In response to Simek's criticism of attempts to interpret the ravens "philosophically", Winterbourne says that "such speculations [...] simply strengthen the conceptual significance made plausible by other features of the mythology" and that the names "Huginn" and "Muninn" "demand more explanation than is usually provided."

The "Heliand", an Old Saxon adaptation of the New Testament from the 9th century, differs from the New Testament in that an explicit reference is made to a dove sitting on the shoulder of Christ. Regarding this, G. Ronald Murphy says "In placing the powerful white dove not just above Christ, but right on his shoulder, the "Heliand" author has portrayed Christ, not only as the Son of the All-Ruler, but also as a new Woden. This deliberate image of Christ triumphantly astride the land with the magnificent bird on his shoulders (the author is perhaps a bit embarrassed that the bird is an unwarlike dove!) is an image intended to calm the fears and longings of those who mourn the loss of Woden and who want to return to the old religion's symbols and ways. With this image, Christ becomes a Germanic god, one into whose ears the Spirit of the Almighty whispers".

Bernd Heinrich theorizes that Huginn and Muninn, along with Odin and his wolves Geri and Freki, reflect a symbiosis observed in the natural world among ravens, wolves, and humans on the hunt:




</doc>
<doc id="13654" url="https://en.wikipedia.org/wiki?curid=13654" title="Heat engine">
Heat engine

In thermodynamics, a heat engine is a system that converts heat or thermal energy—and chemical energy—to mechanical energy, which can then be used to do mechanical work. It does this by bringing a working substance from a higher state temperature to a lower state temperature. A heat source generates thermal energy that brings the working substance to the high temperature state. The working substance generates work in the working body of the engine while transferring heat to the colder sink until it reaches a low temperature state. During this process some of the thermal energy is converted into work by exploiting the properties of the working substance. The working substance can be any system with a non-zero heat capacity, but it usually is a gas or liquid. During this process, a lot of heat is lost to the surroundings and so cannot be converted to work.

In general an engine converts energy to mechanical work. Heat engines distinguish themselves from other types of engines by the fact that their efficiency is fundamentally limited by Carnot's theorem. Although this efficiency limitation can be a drawback, an advantage of heat engines is that most forms of energy can be easily converted to heat by processes like exothermic reactions (such as combustion), absorption of light or energetic particles, friction, dissipation and resistance. Since the heat source that supplies thermal energy to the engine can thus be powered by virtually any kind of energy, heat engines are very versatile and have a wide range of applicability.

Heat engines are often confused with the cycles they attempt to implement. Typically, the term "engine" is used for a physical device and "cycle" for the model.

In thermodynamics, heat engines are often modeled using a standard engineering model such as the Otto cycle. The theoretical model can be refined and augmented with actual data from an operating engine, using tools such as an indicator diagram. Since very few actual implementations of heat engines exactly match their underlying thermodynamic cycles, one could say that a thermodynamic cycle is an ideal case of a mechanical engine. In any case, fully understanding an engine and its efficiency requires gaining a good understanding of the (possibly simplified or idealized) theoretical model, the practical nuances of an actual mechanical engine, and the discrepancies between the two.

In general terms, the larger the difference in temperature between the hot source and the cold sink, the larger is the potential thermal efficiency of the cycle. On Earth, the cold side of any heat engine is limited to being close to the ambient temperature of the environment, or not much lower than 300 kelvins, so most efforts to improve the thermodynamic efficiencies of various heat engines focus on increasing the temperature of the source, within material limits. The maximum theoretical efficiency of a heat engine (which no engine ever attains) is equal to the temperature difference between the hot and cold ends divided by the temperature at the hot end, all expressed as absolute temperatures (in kelvins).

The efficiency of various heat engines proposed or used today has a large range: 

All these processes gain their efficiency (or lack thereof) from the temperature drop across them. Significant energy may be used for auxiliary equipment, such as pumps, which effectively reduces efficiency.

Heat engines can be characterized by their specific power, which is typically given in kilowatts per litre of engine displacement (in the U.S. also horsepower per cubic inch). The result offers an approximation of the peak power output of an engine. This is not to be confused with fuel efficiency, since high efficiency often requires a lean fuel-air ratio, and thus lower power density. A modern high-performance car engine makes in excess of 75 kW/l (1.65 hp/in).

Examples of everyday heat engines include the steam engine and the internal combustion engine. The stirling engine is also heat engine, as well as the drinking bird toy. All of these heat engines are powered by the expansion of heated gases. The general surroundings are the heat sink, which provides relatively cool gases that, when heated, expand rapidly to drive the mechanical motion of the engine.

It is important to note that although some cycles have a typical combustion location (internal or external), they often can be implemented with the other. For example, John Ericsson developed an external heated engine running on a cycle very much like the earlier Diesel cycle. In addition, externally heated engines can often be implemented in open or closed cycles.

Earth's atmosphere and hydrosphere—Earth’s heat engine—are coupled processes that constantly even out solar heating imbalances through evaporation of surface water, convection, rainfall, winds, and ocean circulation, when distributing heat around the globe.

The Hadley system provides an example of a heat engine. The Hadley circulation is identified with rising of warm and moist air in the equatorial region with descent of colder air in the subtropics corresponding to a thermally driven direct circulation, with consequent net production of kinetic energy.

In these cycles and engines, the working fluids are gases and liquids. The engine converts the working fluid from a gas to a liquid, from liquid to gas, or both, generating work from the fluid expansion or compression.

In these cycles and engines the working fluid is always a gas (i.e., there is no phase change):

In these cycles and engines the working fluid are always like liquid:



A domestic refrigerator is an example of a heat pump: a heat engine in reverse. Work is used to create a heat differential. Many cycles can run in reverse to move heat from the cold side to the hot side, making the cold side cooler and the hot side hotter. Internal combustion engine versions of these cycles are, by their nature, not reversible.

Refrigeration cycles include:

The Barton evaporation engine is a heat engine based on a cycle producing power and cooled moist air from the evaporation of water into hot dry air.

Mesoscopic heat engines are nanoscale devices that may serve the goal of processing heat fluxes and perform useful work at small scales. Potential applications include e.g. electric cooling devices.
In such mesoscopic heat engines, work per cycle of operation fluctuates due to thermal noise.
There is exact equality that relates average of exponents of work performed by any heat engine and the heat transfer from the hotter heat bath. This relation transforms the Carnot's inequality into exact equality.

The efficiency of a heat engine relates how much useful work is output for a given amount of heat energy input.

From the laws of thermodynamics, after a completed cycle:

In other words, a heat engine absorbs heat energy from the high temperature heat source, converting part of it to useful work and delivering the rest to the cold temperature heat sink.

In general, the efficiency of a given heat transfer process (whether it be a refrigerator, a heat pump or an engine) is defined informally by the ratio of "what you get out" to "what you put in".

In the case of an engine, one desires to extract work and puts in a heat transfer.

The "theoretical" maximum efficiency of any heat engine depends only on the temperatures it operates between. This efficiency is usually derived using an ideal imaginary heat engine such as the Carnot heat engine, although other engines using different cycles can also attain maximum efficiency. Mathematically, this is because in reversible processes, the change in entropy of the cold reservoir is the negative of that of the hot reservoir (i.e., formula_7), keeping the overall change of entropy zero. Thus:

where formula_9 is the absolute temperature of the hot source and formula_10 that of the cold sink, usually measured in kelvins. Note that formula_11 is positive while formula_12 is negative; in any reversible work-extracting process, entropy is overall not increased, but rather is moved from a hot (high-entropy) system to a cold (low-entropy one), decreasing the entropy of the heat source and increasing that of the heat sink.

The reasoning behind this being the maximal efficiency goes as follows. It is first assumed that if a more efficient heat engine than a Carnot engine is possible, then it could be driven in reverse as a heat pump. Mathematical analysis can be used to show that this assumed combination would result in a net decrease in entropy. Since, by the second law of thermodynamics, this is statistically improbable to the point of exclusion, the Carnot efficiency is a theoretical upper bound on the reliable efficiency of "any" process.

Empirically, no heat engine has ever been shown to run at a greater efficiency than a Carnot cycle heat engine.

Figure 2 and Figure 3 show variations on Carnot cycle efficiency. Figure 2 indicates how efficiency changes with an increase in the heat addition temperature for a constant compressor inlet temperature. Figure 3 indicates how the efficiency changes with an increase in the heat rejection temperature for a constant turbine inlet temperature.

The most Carnot efficiency as a criterion of heat engine performance is the fact that by its nature, any maximally efficient Carnot cycle must operate at an infinitesimal temperature gradient. This is because "any" transfer of heat between two bodies at differing temperatures is irreversible, and therefore the Carnot efficiency expression only applies in the infinitesimal limit. The major problem with that is that the object of most heat engines is to output some sort of power, and infinitesimal power is usually not what is being sought.

A different measure of ideal heat engine efficiency is given by considerations of endoreversible thermodynamics, where the cycle is identical to the Carnot cycle except in that the two processes of heat transfer are "not" reversible (Callen 1985):

This model does a better job of predicting how well real-world heat engines can do (Callen 1985, see also endoreversible thermodynamics):

As shown, the endoreversible efficiency much more closely models the observed data.

Heat engines have been known since antiquity but were only made into useful devices at the time of the industrial revolution in the 18th century. They continue to be developed today.

Engineers have studied the various heat engine cycles extensively in effort to improve the amount of usable work they could extract from a given power source. The Carnot cycle limit cannot be reached with any gas-based cycle, but engineers have worked out at least two ways to possibly go around that limit, and one way to get better efficiency without bending any rules.

Each process is one of the following:




</doc>
<doc id="13655" url="https://en.wikipedia.org/wiki?curid=13655" title="Heimdallr">
Heimdallr

In Norse mythology, Heimdallr is a god who possesses the resounding horn Gjallarhorn, owns the golden-maned horse Gulltoppr, has gold teeth, and is the son of Nine Mothers (who may represent personified waves). Heimdallr is attested as possessing foreknowledge, keen eyesight and hearing, and keeps watch for the onset of Ragnarök while drinking fine mead in his dwelling Himinbjörg, located where the burning rainbow bridge Bifröst meets heaven. Heimdallr is said to be the originator of social classes among humanity and once regained Freyja's treasured possession Brísingamen while doing battle in the shape of a seal with Loki. Heimdallr and Loki are foretold to kill one another during the events of Ragnarök. Heimdallr is additionally referred to as Rig, Hallinskiði, Gullintanni, and Vindlér or Vindhlér.

Heimdallr is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional material; in the "Prose Edda" and "Heimskringla", both written in the 13th century by Snorri Sturluson; in the poetry of skalds; and on an Old Norse runic inscription found in England. Two lines of an otherwise lost poem about the god, "Heimdalargaldr", survive. Due to the problematic and enigmatic nature of these attestations, scholars have produced various theories about the nature of the god, including his apparent relation to rams, that he may be a personification of or connected to the world tree Yggdrasil, and potential Indo-European cognates.

The etymology of the name is obscure, but 'the one who illuminates the world' has been proposed. "Heimdallr" may be connected to "Mardöll", one of Freyja's names. "Heimdallr" and its variants are sometimes modernly anglicized as Heimdall (; with the nominative "-r" dropped).

Heimdallr is attested as having three other names; "Hallinskiði", "Gullintanni", and "Vindlér" or "Vindhlér". The name "Hallinskiði" is obscure, but has resulted in a series of attempts at deciphering it. "Gullintanni" literally means 'the one with the golden teeth'. "Vindlér" (or "Vindhlér") translates as either 'the one protecting against the wind' or 'wind-sea'. All three have resulted in numerous theories about the god.

A lead spindle whorl bearing an Old Norse Younger Futhark inscription that mentions Heimdallr was discovered in Saltfleetby, England on September 1, 2010. The spindle whorl itself is dated from the year 1000 to 1100 AD. On the inscription, the god Heimdallr is mentioned alongside the god Odin and Þjálfi, a name of one of the god Thor's servants. Regarding the inscription reading, John Hines of Cardiff University comments that there is "quite an essay to be written over the uncertainties of translation and identification here; what are clear, and very important, are the names of two of the Norse gods on the side, Odin and Heimdallr, while Þjalfi (masculine, not the feminine in -a) is the recorded name of a servant of the god Thor."

In the "Poetic Edda", Heimdallr is attested in six poems; "Völuspá", "Grímnismál", "Lokasenna", "Þrymskviða", "Rígsþula", and "Hrafnagaldr Óðins".

Heimdallr is mentioned thrice in "Völuspá". In the first stanza of the poem, the undead völva reciting the poem calls out for listeners to be silent and refers to Heimdallr:

This stanza has led to various scholarly interpretations. The "holy races" have been considered variously as either humanity or the gods. The notion of humanity as "Heimdallr's sons" is otherwise unattested and has also resulted in various interpretations. Some scholars have pointed to the prose introduction to the poem "Rígsþula", where Heimdallr is said to have once gone about mankind, slept between couples, and so doled out classes among them (see "Rígsthula" section below).

Later in "Völuspá", the völva foresees the events of Ragnarök and the role in which Heimdallr and Gjallarhorn will play at its onset; Heimdallr will raise his horn and blow loudly. Due to manuscript differences, translations of the stanza vary:

Regarding this stanza, scholar Andy Orchard comments that the name "Gjallarhorn" may here mean "horn of the river Gjöll" as "Gjöll is the name of one of the rivers of the Underworld, whence much wisdom is held to derive", but notes that in the poem "Grímnismál" Heimdallr is said to drink fine mead in his heavenly home Himinbjörg.

Earlier in the same poem, the völva mentions a scenario involving the hearing or horn (depending on translation of the Old Norse noun "hljóð"—translations bolded below for the purpose of illustration) of the god Heimdallr:

Scholar Paul Schach comments that the stanzas in this section of " Völuspá" are "all very mysterious and obscure, as it was perhaps meant to be". Schach details that ""Heimdallar hljóð" has aroused much speculation. Snorri [in the "Prose Edda"] seems to have confused this word with "gjallarhorn", but there is otherwise no attestation of the use of "hljóð" in the sense of 'horn' in Icelandic. Various scholars have read this as "hearing" rather than "horn".

Scholar Carolyne Larrington comments that if "hearing" rather than "horn" is understood to appear in this stanza, the stanza indicates that Heimdallr, like Odin, has left a body part in the well; his ear. Larrington says that "Odin exchanged one of his eyes for wisdom from Mimir, guardian of the well, while Heimdall seems to have forfeited his ear."

In the poem "Grímnismál", Odin (disguised as "Grímnir"), tortured, starved and thirsty, tells the young Agnar of a number of mythological locations. The eighth location he mentions is Himinbjörg, where he says that Heimdallr drinks fine mead:

Regarding the above stanza, Henry Adams Bellows comments that "in this stanza the two functions of Heimdall—as father of mankind [ . . . ] and as warder of the gods—seem both to be mentioned, but the second line in the manuscripts is apparently in bad shape, and in the editions it is more or less conjecture".

In the poem "Lokasenna", Loki flyts with various gods who have met together to feast. At one point during the exchanges, the god Heimdallr says that Loki is drunk and witless, and asks Loki why he won't stop speaking. Loki tells Heimdallr to be silent, that he was fated a "hateful life", that Heimdallr must always have a muddy back, and that he must serve as watchman of the gods. The goddess Skaði interjects and the flyting continues in turn.

The poem "Þrymskviða" tells of Thor's loss of his hammer, Mjöllnir, to the jötnar and quest to get it back. At one point in the tale, the gods gather at the thing and debate how to get Thor's hammer back from the jötnar, who demand the beautiful goddess Freyja in return for it. Heimdallr advises that they simply dress Thor up as Freyja, during which he is described as "hvítastr ása" (translations of the phrase vary below) and is said to have foresight like the Vanir, a group of gods:

Regarding Heimdallr's whiteness and the comparison to the Vanir, scholar John Lindow comments that there are no other indications of Heimdallr being considered among the Vanir, and that Heimdallr's status as "whitest of the gods" has not been explained.

The introductory prose to the poem "Rígsþula" says that "people say in the old stories" that Heimdallr, described as a god among the Æsir, once fared on a journey. Heimdallr wandered along a seashore, and referred to himself as "Rígr". In the poem, Rígr, who is described as a wise and powerful god, walks in the middle of roads on his way to steads, where he meets a variety of couples and dines with them, giving them advice and spending three nights at a time between them in their bed. The wives of the couples become pregnant, and from them come the various classes of humanity. Eventually a warrior home produces a promising boy, and as the boy grows older, Rígr comes out of a thicket, teaches the boy runes, gives him a name, and proclaims him to be his son. Rígr tells him to strike out and get land for himself. The boy does so, and so becomes a great war leader with many estates. He marries a beautiful woman and the two have many children and are happy. One of the children eventually becomes so skilled that he is able to share in runic knowledge with Heimdallr, and so earns the title of "Rígr" himself. The poem continues without further mention of the god.

In the "Prose Edda", Heimdallr is mentioned in the books "Gylfaginning", "Skáldskaparmál", and "Háttatal". In "Gylfaginning", the enthroned figure of High tells the disguised mythical king Gangleri of various gods, and, in chapter 25, mentions Heimdallr. High says that Heimdallr is known "the white As", is "great and holy", and that nine maidens, all sisters, gave birth to him. Heimdallr is called "Hallinskiði" and "Gullintanni", and he has gold teeth. High continues that Heimdallr lives in "a place" called Himinbjörg and that it is near Bifröst. Heimdallr is the watchman of the gods, and he sits on the edge of heaven to guard the Bifröst bridge from the berg jötnar. Heimdallr requires less sleep than a bird, can see at night just as well as if it were day, and for over a hundred leagues. Heimdallr's hearing is also quite keen; he can hear grass as it grows on the earth, wool as it grows on sheep, and anything louder. Heimdallr possesses a trumpet, Gjallarhorn, that, when blown, can be heard in all worlds, and "the head is referred to as Heimdall's sword". High then quotes the above-mentioned "Grímnismál" stanza about Himinbjörg and provides two lines from the otherwise lost poem about Heimdallr, "Heimdalargaldr", in which Heimdallr proclaims himself to be the son of Nine Mothers.

In chapter 49, High tells of the god Baldr's funeral procession. Various deities are mentioned as having attended, including Heimdallr, who there rode his horse Gulltopr.

In chapter 51, High foretells the events of Ragnarök. After the enemies of the gods will gather at the plain Vígríðr, Heimdallr will stand and mightily blow into Gjallarhorn. The gods will awake and assemble together at the thing. At the end of the battle between various gods and their enemies, Heimdallr will face Loki and they will kill one another. After, the world will be engulfed in flames. High then quotes the above-mentioned stanza regarding Heimdallr raising his horn in "Völuspá".

At the beginning of "Skáldskaparmál", Heimdallr is mentioned as having attended a banquet in Asgard with various other deities. Later in the book, "Húsdrápa", a poem by 10th century skald Úlfr Uggason, is cited, during which Heimdallr is described as having ridden to Baldr's funeral pyre.

In chapter 8, means of referring to Heimdallr are provided; "son of nine mothers", "guardian of the gods", "the white As" (see "Poetic Edda" discussion regarding "hvítastr ása" above), "Loki's enemy", and "recoverer of Freyja's necklace". The section adds that the poem "Heimdalargaldr" is about him, and that, since the poem, "the head has been called Heimdall's doom: man's doom is an expression for sword". Hiemdallr is the owner of Gulltoppr, is also known as Vindhlér, and is a son of Odin. Heimdallr visits Vágasker and Singasteinn and there vied with Loki for Brísingamen. According to the chapter, the skald Úlfr Uggason composed a large section of his "Húsdrápa" about these events and that "Húsdrápa" says that the two were in the shape of seals. A few chapters later, ways of referring to Loki are provided, including "wrangler with Heimdall and Skadi", and section of Úlfr Uggason's "Húsdrápa" is then provided in reference:

The chapter points out that in the above "Húsdrápa" section Heimdallr is said to be the son of nine mothers.

Heimdallr is mentioned once in "Háttatal". There, in a composition by Snorri Sturluson, a sword is referred to as "Vindhlér's helmet-filler", meaning "Heimdallr's head".

In "Ynglinga saga" compiled in "Heimskringla", Snorri presents a euhemerized origin of the Norse gods and rulers descending from them. In chapter 5, Snorri asserts that the Æsir settled in what is now Sweden and built various temples. Snorri writes that Odin settled in Lake Logrin "at a place which formerly was called Sigtúnir. There he erected a large temple and made sacrifices according to the custom of the Æsir. He took possession of the land as far as he had called it Sigtúnir. He gave dwelling places to the temple priests." Snorri adds that, after this, Njörðr dwelt in Nóatún, Freyr dwelt in Uppsala, Heimdall at Himinbjörg, Thor at Þrúðvangr, Baldr at Breiðablik and that to everyone Odin gave fine estates.

A figure holding a large horn to his lips and clasping a sword on his hip appears on a stone cross from the Isle of Man. Some scholars have theorized that this figure is a depiction of Heimdallr with Gjallarhorn.

A 9th or 10th century Gosforth Cross in Cumbria, England depicts a figure holding a horn and a sword standing defiantly before two open-mouthed beasts. This figure has been often theorized as depicting Heimdallr with Gjallarhorn.

Heimdallr's attestations have proven troublesome and enigmatic to interpret for scholars. Scholar Georges Dumézil summarizes the difficulties as follows:




</doc>
<doc id="13658" url="https://en.wikipedia.org/wiki?curid=13658" title="House of Lords">
House of Lords

The House of Lords of the United Kingdom, also known as the House of Peers, is the upper house of the Parliament of the United Kingdom. Like the House of Commons, it meets in the Palace of Westminster. Officially, the full name of the house is the Right Honourable the Lords Spiritual and Temporal of the United Kingdom of Great Britain and Northern Ireland in Parliament assembled.

Unlike the elected House of Commons, all members of the House of Lords (excluding 90 hereditary peers elected among themselves and two peers who are "ex officio" members) are appointed. The membership of the House of Lords is drawn from the peerage and is made up of Lords Spiritual and Lords Temporal. The Lords Spiritual are 26 bishops in the established Church of England. Of the Lords Temporal, the majority are life peers who are appointed by the monarch on the advice of the Prime Minister, or on the advice of the House of Lords Appointments Commission. However, they also include some hereditary peers including four dukes.

Membership was once an entitlement of all hereditary peers, other than those in the peerage of Ireland, but under the House of Lords Act 1999, the right to membership was restricted to 92 hereditary peers. As of 2018, only one of them is female (the Countess of Mar); most hereditary peerages can be inherited only by men.

While the House of Commons has a defined 650-seat membership, the number of members in the House of Lords is not fixed. There are currently sitting Lords. The House of Lords is the only upper house of any bicameral parliament to be larger than its lower house.

The House of Lords scrutinises bills that have been approved by the House of Commons. It regularly reviews and amends Bills from the Commons. While it is unable to prevent Bills passing into law, except in certain limited circumstances, it can delay Bills and force the Commons to reconsider their decisions. In this capacity, the House of Lords acts as a check on the House of Commons that is independent from the electoral process. Bills can be introduced into either the House of Lords or the House of Commons. While members of the Lords may also take on roles as government ministers, high-ranking officials such as cabinet ministers are usually drawn from the Commons. The House of Lords has its own support services, separate from the Commons, including the House of Lords Library.

The Queen's Speech is delivered in the House of Lords during the State Opening of Parliament. In addition to its role as the upper house, until the establishment of the Supreme Court in 2009, the House of Lords, through the Law Lords, acted as the final court of appeal in the United Kingdom judicial system. The House also has a Church of England role, in that Church Measures must be tabled within the House by the Lords Spiritual.

Today's Parliament of the United Kingdom largely descends, in practice, from the Parliament of England, though the Treaty of Union of 1706 and the Acts of Union that ratified the Treaty in 1707 created a new Parliament of Great Britain to replace the Parliament of England and the Parliament of Scotland. This new parliament was, in effect, the continuation of the Parliament of England with the addition of 45 MPs and 16 Peers to represent Scotland.

The House of Lords developed from the "Great Council" ("Magnum Concilium") that advised the King during medieval times. This royal council came to be composed of ecclesiastics, noblemen, and representatives of the counties of England and Wales (afterwards, representatives of the boroughs as well). The first English Parliament is often considered to be the "Model Parliament" (held in 1295), which included archbishops, bishops, abbots, earls, barons, and representatives of the shires and boroughs of it.

The power of Parliament grew slowly, fluctuating as the strength of the monarchy grew or declined. For example, during much of the reign of Edward II (1307–1327), the nobility was supreme, the Crown weak, and the shire and borough representatives entirely powerless. In 1569, the authority of Parliament was for the first time recognised not simply by custom or royal charter, but by an authoritative statute, passed by Parliament itself.

During the reign of Edward II's successor, Edward III, Parliament clearly separated into two distinct chambers: the House of Commons (consisting of the shire and borough representatives) and the House of Lords (consisting of the bishops, abbots and peers). The authority of Parliament continued to grow, and during the early 15th century both Houses exercised powers to an extent not seen before. The Lords were far more powerful than the Commons because of the great influence of the great landowners and the prelates of the realm.

The power of the nobility declined during the civil wars of the late 15th century, known as the Wars of the Roses. Much of the nobility was killed on the battlefield or executed for participation in the war, and many aristocratic estates were lost to the Crown. Moreover, feudalism was dying, and the feudal armies controlled by the barons became obsolete. Henry VII (1485–1509) clearly established the supremacy of the monarch, symbolised by the "Crown Imperial". The domination of the Sovereign continued to grow during the reigns of the Tudor monarchs in the 16th century. The Crown was at the height of its power during the reign of Henry VIII (1509–1547).

The House of Lords remained more powerful than the House of Commons, but the Lower House continued to grow in influence, reaching a zenith in relation to the House of Lords during the middle 17th century. Conflicts between the King and the Parliament (for the most part, the House of Commons) ultimately led to the English Civil War during the 1640s. In 1649, after the defeat and execution of King Charles I, the Commonwealth of England was declared, but the nation was effectively under the overall control of Oliver Cromwell, Lord Protector of England, Scotland and Ireland.

The House of Lords was reduced to a largely powerless body, with Cromwell and his supporters in the Commons dominating the Government. On 19 March 1649, the House of Lords was abolished by an Act of Parliament, which declared that "The Commons of England [find] by too long experience that the House of Lords is useless and dangerous to the people of England." The House of Lords did not assemble again until the Convention Parliament met in 1660 and the monarchy was restored. It returned to its former position as the more powerful chamber of Parliament—a position it would occupy until the 19th century.

The 19th century was marked by several changes to the House of Lords. The House, once a body of only about 50 members, had been greatly enlarged by the liberality of George III and his successors in creating peerages. The individual influence of a Lord of Parliament was thus diminished.

Moreover, the power of the House as a whole decreased, whilst that of the House of Commons grew. Particularly notable in the development of the Lower House's superiority was the Reform Bill of 1832. The electoral system of the House of Commons was far from democratic: property qualifications greatly restricted the size of the electorate, and the boundaries of many constituencies had not been changed for centuries.

Entire cities such as Manchester were not represented by a single individual in the House of Commons, but the 11 voters of Old Sarum retained their ancient right to elect two MPs. A small borough was susceptible to bribery, and was often under the control of a patron, whose nominee was guaranteed to win an election. Some aristocrats were patrons of numerous "pocket boroughs", and therefore controlled a considerable part of the membership of the House of Commons.

When the House of Commons passed a Reform Bill to correct some of these anomalies in 1831, the House of Lords rejected the proposal. The popular cause of reform, however, was not abandoned by the ministry, despite a second rejection of the bill in 1832. Prime Minister Charles Grey, 2nd Earl Grey advised the King to overwhelm opposition to the bill in the House of Lords by creating about 80 new pro-Reform peers. William IV originally balked at the proposal, which effectively threatened the opposition of the House of Lords, but at length relented.

Before the new peers were created, however, the Lords who opposed the bill admitted defeat and abstained from the vote, allowing the passage of the bill. The crisis damaged the political influence of the House of Lords but did not altogether end it. A vital reform was effected by the Lords themselves in 1868, when they changed their standing orders to abolish proxy voting, preventing Lords from voting without taking the trouble to attend. Over the course of the century the power of the Upper House were further eroded, and the Commons gradually became the stronger House of Parliament.

The status of the House of Lords returned to the forefront of debate after the election of a Liberal Government in 1906. In 1909, the Chancellor of the Exchequer, David Lloyd George, introduced into the House of Commons the "People's Budget", which proposed a land tax targeting wealthy landowners. The popular measure, however, was defeated in the heavily Conservative House of Lords.

Having made the powers of the House of Lords a primary campaign issue, the Liberals were narrowly re-elected in January 1910. Prime Minister H. H. Asquith then proposed that the powers of the House of Lords be severely curtailed. After a further general election in December 1910, and with an undertaking by King George V to create sufficient new Liberal peers to overcome Lords' opposition to the measure if necessary, the Asquith Government secured the passage of a bill to curtail the powers of the House of Lords.

The Parliament Act 1911 effectively abolished the power of the House of Lords to reject legislation, or to amend it in a way unacceptable to the House of Commons: most bills could be delayed for no more than three parliamentary sessions or two calendar years. It was not meant to be a permanent solution; more comprehensive reforms were planned. Neither party, however, pursued the matter with much enthusiasm, and the House of Lords remained primarily hereditary. In 1949, the Parliament Act reduced the delaying power of the House of Lords further to two sessions or one year.

In 1958, the predominantly hereditary nature of the House of Lords was changed by the Life Peerages Act 1958, which authorised the creation of life baronies, with no numerical limits. The number of Life Peers then gradually increased, though not at a constant rate.

The Labour Party had for most of the 20th century a commitment, based on the party's historic opposition to class privilege, to abolish the House of Lords, or at least expel the hereditary element. In 1968, the Labour Government of Harold Wilson attempted to reform the House of Lords by introducing a system under which hereditary peers would be allowed to remain in the House and take part in debate, but would be unable to vote. This plan, however, was defeated in the House of Commons by a coalition of traditionalist Conservatives (such as Enoch Powell), and Labour members who continued to advocate the outright abolition of the Upper House (such as Michael Foot).

When Michael Foot became leader of the Labour Party in 1980, abolition of the House of Lords became a part of the party's agenda; under his successor, Neil Kinnock, however, a reformed Upper House was proposed instead. In the meantime, the creation of hereditary peerages (except for members of the Royal Family) has been arrested, with the exception of three creations during the administration of the Conservative Margaret Thatcher in the 1980s.

Whilst some hereditary peers were at best apathetic, the Labour Party's clear commitments were not lost on Merlin Hanbury-Tracy, 7th Baron Sudeley, who for decades was considered an expert on the House of Lords. In December 1979 the Conservative Monday Club published his extensive paper entitled "Lords Reform – Why tamper with the House of Lords?" and in July 1980 "The Monarchist" carried another article by Sudeley entitled "Why Reform or Abolish the House of Lords?". In 1990 he wrote a further booklet for the Monday Club entitled "The Preservation of the House of Lords".

There were no women sitting in the House of Lords until 1958, when a small number came into the chamber as a result of the Life Peerages Act 1958. One of these was Irene Curzon, 2nd Baroness Ravensdale, who had inherited her father's peerage in 1925 and was made a life peer to enable her to sit. After a campaign stretching back in some cases to the 1920s, another twelve women who held hereditary peerages in their own right were finally admitted by the Peerage Act 1963.

The Labour Party included in its 1997 general election Manifesto a commitment to remove the hereditary peerage from the House of Lords. Their subsequent election victory in 1997 under Tony Blair led to the denouement of the traditional House of Lords. The Labour Government introduced legislation to expel all hereditary peers from the Upper House as a first step in Lords reform. As a part of a compromise, however, it agreed to permit 92 hereditary peers to remain until the reforms were complete. Thus all but 92 hereditary peers were expelled under the House of Lords Act 1999 (see below for its provisions), making the House of Lords predominantly an appointed house.

Since 1999, however, no further reform has taken place. The Wakeham Commission proposed introducing a 20% elected element to the Lords, but this plan was widely criticised. A Joint Committee was established in 2001 to resolve the issue, but it reached no conclusion and instead gave Parliament seven options to choose from (fully appointed, 20% elected, 40% elected, 50% elected, 60% elected, 80%, and fully elected). In a confusing series of votes in February 2003, all of these options were defeated, although the 80% elected option fell by just three votes in the Commons. Socialist MPs favouring outright abolition voted against all the options.

In 2005, a cross-party group of senior MPs (Kenneth Clarke, Paul Tyler, Tony Wright, George Young and Robin Cook) published a report proposing that 70% of members of the House of Lords should be elected — each member for a single long term — by the single transferable vote system. Most of the remainder were to be appointed by a Commission to ensure a mix of "skills, knowledge and experience". This proposal was also not implemented. A cross-party campaign initiative called "Elect the Lords" was set up to make the case for a predominantly elected Second Chamber in the run up to the 2005 general election.

At the 2005 election, the Labour Party proposed further reform of the Lords, but without specific details. The Conservative Party, which had, prior to 1997, opposed any tampering with the House of Lords, favoured an 80% elected Second Chamber, while the Liberal Democrats called for a fully elected Senate. During 2006, a cross-party committee discussed Lords reform, with the aim of reaching a consensus: its findings were published in early 2007.

On 7 March 2007, members of the House of Commons voted ten times on a variety of alternative compositions for the upper chamber. Outright abolition, a wholly appointed house, a 20% elected house, a 40% elected house, a 50% elected house and a 60% elected house were all defeated in turn. Finally the vote for an 80% elected chamber was won by 305 votes to 267, and the vote for a wholly elected chamber was won by an even greater margin: 337 to 224. Significantly this last vote represented an overall majority of MPs.

Furthermore, examination of the names of MPs voting at each division shows that, of the 305 who voted for the 80% elected option, 211 went on to vote for the 100% elected option. Given that this vote took place after the vote on 80% – whose result was already known when the vote on 100% took place – this showed a clear preference for a fully elected upper house among those who voted for the only other option that passed. But this was nevertheless only an indicative vote and many political and legislative hurdles remained to be overcome for supporters of an elected second chamber. The House of Lords, soon after, rejected this proposal and voted for an entirely appointed House of Lords.

In July 2008, Jack Straw, the Secretary of State for Justice and Lord Chancellor, introduced a white paper to the House of Commons proposing to replace the House of Lords with an 80–100% elected chamber, with one third being elected at each general election, for a term of approximately 12–15 years. The white paper states that as the peerage would be totally separated from membership of the upper house, the name "House of Lords" would no longer be appropriate: It goes on to explain that there is cross-party consensus for the new chamber to be titled the "Senate of the United Kingdom"; however, to ensure the debate remains on the role of the upper house rather than its title, the white paper is neutral on the title of the new house.

On 30 November 2009, a "Code of Conduct for Members of the House of Lords" was agreed by them; certain amendments were agreed by them on 30 March 2010 and on 12 June 2014. The scandal over expenses in the Commons was at its highest pitch only six months before, and the Labourite leadership under Janet Royall determined that something sympathetic should be done.

In Meg Russell's article "Is the House of Lords already reformed?", she states three essential features of a legitimate House of Lords. The first is that it must have adequate powers over legislation to make the government think twice before making a decision. The House of Lords, she argues, currently has enough power to make it relevant. During Tony Blair’s first year, he was defeated 38 times in the Lords. Secondly, as to the composition of the Lords, Meg Russell suggests that the composition must be distinct from the Commons, otherwise it would render the Lords useless. The third feature is the perceived legitimacy of the Lords. She writes, "In general legitimacy comes with election."

If the Lords have a distinct and elected composition, this would probably come about through fixed term proportional representation. If this happens, then the perceived legitimacy of the Lords could arguably outweigh the legitimacy of the Commons. This would especially be the case if the House of Lords had been elected more recently than the House of Commons as it could be said to reflect the will of the people better than the Commons.

In this scenario, there may well come a time when the Lords twice reject a Bill from the Commons and it is forced through. This would in turn trigger questions about the amount of power the Lords should have and there would be pressure for it to increase. This hypothetical process is known as the "circumnavigation of power theory". It implies that it would never be in any government's interest to legitimise the Lords, as they would be forfeiting their own power.

The Conservative–Liberal Democrat coalition agreed, after the 2010 general election, to outline clearly a provision for a wholly or mainly elected second chamber, elected by proportional representation. These proposals sparked a debate on 29 June 2010. As an interim measure, appointment of new peers would reflect the shares of the vote secured by the political parties in the last general election.

Detailed proposals for Lords reform, including a draft House of Lords Reform Bill, were published on 17 May 2011. These included a 300-member hybrid house, of whom 80% would be elected. A further 20% would be appointed, and reserve space would be included for some Church of England bishops. Under the proposals, members would also serve single non-renewable terms of 15 years. Former MPs would be allowed to stand for election to the Upper House, but members of the Upper House would not be immediately allowed to become MPs.

The details of the proposal were:

The proposals were considered by a Joint Committee on House of Lords Reform made up of both MPs and Peers, which issued its final report on 23 April 2012, making the following suggestions:

Deputy Prime Minister Nick Clegg introduced the House of Lords Reform Bill 2012 on 27 June 2012 which built on proposals published on 17 May 2011. However, this Bill was abandoned by the Government on 6 August 2012 following opposition from within the Conservative Party.

A private members bill to introduce some reforms was introduced by Dan Byles in 2013. The House of Lords Reform Act 2014 received the Royal Assent in 2014. Under the new law:


The House of Lords (Expulsion and Suspension) Act 2015 authorised the House to expel or suspend members.

This act makes provision to preferentially admit bishops of the Church of England who are women to the Lords Spiritual in the 10 years following its commencement.

In 2015, Rachel Treweek, Bishop of Gloucester, became the first woman to sit as a Lord Spiritual in the House of Lords.

The size of the House of Lords has varied greatly throughout its history. From about 220 members in the early 1700s, it increased to a record size of 1,330 in October 1999, before Lords reform reduced it to 669 by March 2000.

In April 2011, a cross-party group of former leading politicians, including many senior members of the House of Lords, called on the Prime Minister David Cameron to stop creating new peers. He had created 117 new peers since becoming prime minister in May 2010, a faster rate of elevation than any PM in British history. The expansion occurred while his government had tried (in vain) to reduce the size of the House of Commons by 50 members, from 650 to 600.

In August 2014, despite there being a seating capacity of only around 230 to 400 on the benches in the Lords chamber, the House had 774 active members (plus 54 who were not entitled to attend or vote, having been suspended or granted leave of absence). This made the House of Lords the largest parliamentary chamber in any democracy. In August 2014, former Speaker of the House of Commons Betty Boothroyd requested that “older peers should retire gracefully” to ease the overcrowding in the House of Lords. She also criticised successive prime ministers for filling the second chamber with “lobby fodder” in an attempt to help their policies become law. She made her remarks days before a new batch of peers were due to be created.

In August 2015, following the creation of a further 45 peers in the Dissolution Honours, the total number of eligible members of the Lords increased to 826. In a report entitled "Does size matter?" the BBC said: "Increasingly, yes. Critics argue the House of Lords is the second largest legislature after the Chinese National People's Congress and dwarfs Upper Houses in other bi-cameral democracies such as the United States (100 senators), France (348 senators), Australia (76 senators) and India (250 members). The Lords is also larger than the Supreme People's Assembly of North Korea (687 members). [… ] Peers grumble that there is not enough room to accommodate all of their colleagues in the Chamber, where there are only about 400 seats, and say they are constantly jostling for space – particularly during high-profile sittings", but added, "On the other hand, defenders of the Lords say that it does a vital job scrutinising legislation, a lot of which has come its way from the Commons in recent years".

The House of Lords does not control the term of the Prime Minister or of the Government. Only the Lower House may force the Prime Minister to resign or call elections by passing a motion of no-confidence or by withdrawing supply. Thus, the House of Lords' oversight of the government is limited.

Most Cabinet ministers are from the House of Commons rather than the House of Lords. In particular, all Prime Ministers since 1902 have been members of the Lower House. (Alec Douglas-Home, who became Prime Minister in 1963 whilst still an Earl, disclaimed his peerage and was elected to the Commons soon after his term began.) In recent history, it has been very rare for major cabinet positions (except Lord Chancellor and Leader of the House of Lords) to have been filled by peers.

Exceptions include Lord Carrington, who was the Foreign Secretary between 1979 and 1982, Lord Young of Graffham (Minister without Portfolio, then Secretary of State for Employment and then Secretary of State for Trade and Industry from 1984 to 1989), Baroness Amos, who served as Secretary of State for International Development and Lord Mandelson, who served as First Secretary of State, Secretary of State for Business, Innovation and Skills and President of the Board of Trade. Lord Robertson of Port Ellen was briefly a peer whilst serving as Secretary of State for Defence before resigning to take up the post of Secretary General of NATO. From 1999 to 2010 the Attorney General for England and Wales was a Member of the House of Lords; the most recent was Baroness Scotland of Asthal.

The House of Lords remains a source for junior ministers and members of government. Like the House of Commons, the Lords also has a Government Chief Whip as well as several Junior Whips. Where a government department is not represented by a minister in the Lords or one is not available, government whips will act as spokesmen for them.

Legislation, with the exception of money bills, may be introduced in either House.

The House of Lords debates legislation, and has power to amend or reject bills. However, the power of the Lords to reject a bill passed by the House of Commons is severely restricted by the Parliament Acts. Under those Acts, certain types of bills may be presented for the Royal Assent without the consent of the House of Lords (i.e. the Commons can override the Lords' veto). The House of Lords cannot delay a money bill (a bill that, in the view of the Speaker of the House of Commons, solely concerns national taxation or public funds) for more than one month.

Other public bills cannot be delayed by the House of Lords for more than two parliamentary sessions, or one calendar year. These provisions, however, only apply to public bills that originate in the House of Commons, and cannot have the effect of extending a parliamentary term beyond five years. A further restriction is a constitutional convention known as the Salisbury Convention, which means that the House of Lords does not oppose legislation promised in the Government's election manifesto.

By a custom that prevailed even before the Parliament Acts, the House of Lords is further restrained insofar as financial bills are concerned. The House of Lords may neither originate a bill concerning taxation or Supply (supply of treasury or exchequer funds), nor amend a bill so as to insert a taxation or Supply-related provision. (The House of Commons, however, often waives its privileges and allows the Upper House to make amendments with financial implications.) Moreover, the Upper House may not amend any Supply Bill. The House of Lords formerly maintained the absolute power to reject a bill relating to revenue or Supply, but this power was curtailed by the Parliament Acts, as aforementioned.

Historically, the House of Lords held several judicial functions. Most notably, until 2009 the House of Lords served as the court of last resort for most instances of UK law. Since 1 October 2009 this role is now held by the Supreme Court of the United Kingdom.

The Lords' judicial functions originated from the ancient role of the Curia Regis as a body that addressed the petitions of the King's subjects. The functions were exercised not by the whole House, but by a committee of "Law Lords". The bulk of the House's judicial business was conducted by the twelve Lords of Appeal in Ordinary, who were specifically appointed for this purpose under the Appellate Jurisdiction Act 1876.

The judicial functions could also be exercised by Lords of Appeal (other members of the House who happened to have held high judicial office). No Lord of Appeal in Ordinary or Lord of Appeal could sit judicially beyond the age of seventy-five. The judicial business of the Lords was supervised by the Senior Lord of Appeal in Ordinary and their deputy, the Second Senior Lord of Appeal in Ordinary.

The jurisdiction of the House of Lords extended, in civil and in criminal cases, to appeals from the courts of England and Wales, and of Northern Ireland. From Scotland, appeals were possible only in civil cases; Scotland's High Court of Justiciary is the highest court in criminal matters. The House of Lords was not the United Kingdom's only court of last resort; in some cases, the Judicial Committee of the Privy Council performs such a function. The jurisdiction of the Privy Council in the United Kingdom, however, is relatively restricted; it encompasses appeals from ecclesiastical courts, disputes under the House of Commons Disqualification Act 1975, and a few other minor matters. Issues related to devolution were transferred from the Privy Council to the Supreme Court in 2009.

The twelve Law Lords did not all hear every case; rather, after World War II cases were heard by panels known as Appellate Committees, each of which normally consisted of five members (selected by the Senior Lord). An Appellate Committee hearing an important case could consist of more than five members. Though Appellate Committees met in separate committee rooms, judgement was given in the Lords Chamber itself. No further appeal lay from the House of Lords, although the House of Lords could refer a "preliminary question" to the European Court of Justice in cases involving an element of European Union law, and a case could be brought at the European Court of Human Rights if the House of Lords did not provide a satisfactory remedy in cases where the European Convention on Human Rights was relevant.

A distinct judicial function—one in which the whole House used to participate—is that of trying impeachments. Impeachments were brought by the House of Commons, and tried in the House of Lords; a conviction required only a majority of the Lords voting. Impeachments, however, are to all intents and purposes obsolete; the last impeachment was that of Henry Dundas, 1st Viscount Melville, in 1806.

Similarly, the House of Lords was once the court that tried peers charged with high treason or felony. The House would be presided over not by the Lord Chancellor, but by the Lord High Steward, an official especially appointed for the occasion of the trial. If Parliament was not in session, then peers could be tried in a separate court, known as the Lord High Steward's Court. Only peers, their wives, and their widows (unless remarried) were entitled to such trials; the Lords Spiritual were tried in ecclesiastical courts. In 1948, the right of peers to be tried in such special courts was abolished; now, they are tried in the regular courts. The last such trial in the House was of Edward Russell, 26th Baron de Clifford, in 1935. An illustrative dramatisation circa 1928 of a trial of a peer (the fictional Duke of Denver) on a charge of murder (a felony) is portrayed in the 1972 BBC Television adaption of Dorothy L. Sayers' Lord Peter Wimsey mystery "Clouds of Witness".

The Constitutional Reform Act 2005 resulted in the creation of a separate Supreme Court of the United Kingdom, to which the judicial function of the House of Lords, and some of the judicial functions of the Judicial Committee of the Privy Council, were transferred. In addition, the office of Lord Chancellor was reformed by the act, removing his ability to act as both a government minister and a judge. This was motivated in part by concerns about the historical admixture of legislative, judicial, and executive power. The new Supreme Court is located at Middlesex Guildhall.

Members of the House of Lords who sit by virtue of their ecclesiastical offices are known as Lords Spiritual. Formerly, the Lords Spiritual were the majority in the English House of Lords, comprising the church's archbishops, (diocesan) bishops, abbots, and those priors who were entitled to wear a mitre. After the English Reformation's highpoint in 1539, only the archbishops and bishops continued to attend, as the Dissolution of the Monasteries had just disproved of and suppressed the positions of abbot and prior. In 1642, during the few Lords' gatherings convened during English Interregnum which saw periodic war, the Lords Spiritual were excluded altogether, but they returned under the Clergy Act 1661.

The number of Lords Spiritual was further restricted by the Bishopric of Manchester Act 1847, and by later Acts. The Lords Spiritual can now number no more than 26; these are the Archbishop of Canterbury, the Archbishop of York, the Bishop of London, the Bishop of Durham, the Bishop of Winchester (who sit by right regardless of seniority) and the 21 longest-serving bishops from other dioceses in the Church of England (excluding the dioceses of Sodor and Man and Gibraltar in Europe, as these lie entirely outside the United Kingdom). Following a change to the law in 2014 to allow women to be ordained bishops, the Lords Spiritual (Women) Act 2015 was passed, which provides that whenever a vacancy arises among the Lords Spiritual during the ten years following the Act coming into force, the vacancy has to be filled by a woman, if one is eligible. This does not apply to the five bishops who sit by right.

The current Lords Spiritual represent only the Church of England. Bishops of the Church of Scotland historically sat in the Parliament of Scotland but were finally excluded in 1689 (after a number of previous exclusions) when the Church of Scotland became permanently Presbyterian. There are no longer bishops in the Church of Scotland in the traditional sense of the word, and that Church has never sent members to sit in the Westminster House of Lords. The Church of Ireland did obtain representation in the House of Lords after the union of Ireland and Great Britain in 1801.

Of the Church of Ireland's ecclesiastics, four (one archbishop and three bishops) were to sit at any one time, with the members rotating at the end of every parliamentary session (which normally lasted about one year). The Church of Ireland, however, was disestablished in 1871, and thereafter ceased to be represented by Lords Spiritual. Bishops of Welsh sees in the Church of England originally sat in the House of Lords (after 1847, only if their seniority within the Church entitled them to), but the Church in Wales ceased to be a part of the Church of England in 1920 and was simultaneously disestablished in Wales. Accordingly, bishops of the Church in Wales were no longer eligible to be appointed to the House as bishops of the Church of England, but those already appointed remained.

Other ecclesiastics have sat in the House of Lords as Lords Temporal in recent times: Chief Rabbi Immanuel Jakobovits was appointed to the House of Lords (with the consent of the Queen, who acted on the advice of Prime Minister Margaret Thatcher), as was his successor Chief Rabbi Jonathan Sacks. Julia Neuberger is the Senior Rabbi to the West London Synagogue. In recognition of his work at reconciliation and in the peace process in Northern Ireland, the Archbishop of Armagh (the senior Anglican bishop in Northern Ireland), Robin Eames, was appointed to the Lords by John Major. Other clergymen appointed include Donald Soper, Timothy Beaumont, and some Scottish clerics.

There have been no Roman Catholic clergymen appointed, though it was rumoured that Cardinal Basil Hume and his successor Cormac Murphy O'Connor were offered peerages, by James Callaghan, Margaret Thatcher and Tony Blair respectively, but declined. Hume later accepted the Order of Merit, a personal appointment of the Queen, shortly before his death. O'Connor said he had his maiden speech ready, but Roman Catholics who have received Holy Orders are prohibited by Canon Law from holding major offices connected with any government other than the Holy See.

Former Archbishops of Canterbury, having reverted to the status of bishop but who are no longer diocesans, are invariably given life peerages and sit as Lords Temporal.

By custom at least one of the Bishops reads prayers in each legislative day (a role taken by the chaplain in the Commons). They often speak in debates; in 2004 Rowan Williams, the Archbishop of Canterbury, opened a debate into sentencing legislation. Measures (proposed laws of the Church of England) must be put before the Lords, and the Lords Spiritual have a role in ensuring that this takes place.

Since the Dissolution of the Monasteries, the Lords Temporal have been the most numerous group in the House of Lords. Unlike the Lords Spiritual, they may be publicly partisan, aligning themselves with one or another of the political parties that dominate the House of Commons. Publicly non-partisan Lords are called crossbenchers. Originally, the Lords Temporal included several hundred hereditary peers (that is, those whose peerages may be inherited), who ranked variously as dukes, marquesses, earls, viscounts, and barons (as well as Scottish Lords of Parliament). Such hereditary dignities can be created by the Crown; in modern times this is done on the advice of the Prime Minister of the day (except in the case of members of the Royal Family).

Holders of Scottish and Irish peerages were not always permitted to sit in the Lords. When Scotland united with England to form Great Britain in 1707, it was provided that the Scottish hereditary peers would only be able to elect 16 representative peers to sit in the House of Lords; the term of a representative was to extend until the next general election. A similar provision was enacted when Ireland merged with Great Britain in 1801 to form the United Kingdom; the Irish peers were allowed to elect 28 representatives, who were to retain office for life. Elections for Irish representatives ended in 1922, when most of Ireland became an independent state; elections for Scottish representatives ended with the passage of the Peerage Act 1963, under which all Scottish peers obtained seats in the Upper House.

In 1999, the Labour government brought forward the House of Lords Act removing the right of several hundred hereditary peers to sit in the House. The Act provided, as a measure intended to be temporary, that 92 people would continue to sit in the Lords by virtue of hereditary peerages, and this is still in effect.

Of the 92, two remain in the House of Lords because they hold royal offices connected with Parliament: the Earl Marshal and the Lord Great Chamberlain. Of the remaining ninety peers sitting in the Lords by virtue of a hereditary peerage, 15 are elected by the whole House and 75 are chosen by fellow hereditary peers in the House of Lords, grouped by party. (If a hereditary peerage holder is given a life peerage, he or she becomes a member of the House of Lords without a need for a by-election.) The exclusion of other hereditary peers removed Charles, Prince of Wales (who is also Earl of Chester) and all other Royal Peers, including Prince Philip, Duke of Edinburgh; Prince Andrew, Duke of York; Prince Edward, Earl of Wessex; Prince Richard, Duke of Gloucester; and Prince Edward, Duke of Kent.

The number of peers to be chosen by a political group reflects the proportion of hereditary peers that belonged to that group (see current composition below) in 1999. When an elected hereditary peer dies, a by-election is held, with a variant of the Alternative Vote system being used. If the recently deceased hereditary peer had been elected by the whole House, then so is his or her replacement; a hereditary peer elected by a specific political group (including the non-aligned crossbenchers) is replaced by a vote of the hereditary peers already elected to the Lords belonging to that political group (whether elected by that group or by the whole house).

Until 2009, the Lords Temporal also included the Lords of Appeal in Ordinary, a group of individuals appointed to the House of Lords so that they could exercise its judicial functions. Lords of Appeal in Ordinary, more commonly known as Law Lords, were first appointed under the Appellate Jurisdiction Act 1876. They were selected by the Prime Minister of the day, but were formally appointed by the Sovereign. A Lord of Appeal in Ordinary had to retire at the age of 70, or, if his or her term was extended by the government, at the age of 75; after reaching such an age, the Law Lord could not hear any further cases in the House of Lords.

The number of Lords of Appeal in Ordinary (excluding those who were no longer able to hear cases because of age restrictions) was limited to twelve, but could be changed by statutory instrument. By a convention of the House, Lords of Appeal in Ordinary did not take part in debates on new legislation, so as to maintain judicial independence. Lords of Appeal in Ordinary held their seats in the House of Lords for life, remaining as members even after reaching the judicial retirement age of 70 or 75. Former Lord Chancellors and holders of other high judicial office could also sit as Law Lords under the Appellate Jurisdiction Act, although in practice this right was only rarely exercised.

Under the Constitutional Reform Act 2005, the Lords of Appeal in Ordinary when the Act came into effect in 2009 became judges of the new Supreme Court of the United Kingdom and were then barred from sitting or voting in the House of Lords until they had retired as judges. One of the main justifications for the new Supreme Court was to establish a separation of powers between the judiciary and the legislature. It is therefore unlikely that future appointees to the Supreme Court of the United Kingdom will be made Lords of Appeal in Ordinary.

The largest group of Lords Temporal, and indeed of the whole House, are life peers. Life peerages rank only as barons or baronesses, and are created under the Life Peerages Act 1958. Like all other peers, life peers are created by the Sovereign, who acts on the advice of the Prime Minister or the House of Lords Appointments Commission. By convention, however, the Prime Minister allows leaders of other parties to nominate some life peers, so as to maintain a political balance in the House of Lords. Moreover, some non-party life peers (the number being determined by the Prime Minister) are nominated by the independent House of Lords Appointments Commission.

In 2000, the government announced it would set up an Independent Appointments Commission, under Dennis, Lord Stevenson of Coddenham, to select fifteen so-called "people's peers" for life peerages. However, when the choices were announced in April 2001, from a list of 3,000 applicants, the choices were treated with criticism in the media, as all were distinguished in their field, and none were "ordinary people" as some had originally hoped.

Several different qualifications apply for membership of the House of Lords. No person may sit in the House of Lords if under the age of 21. Furthermore, only United Kingdom, Irish and Commonwealth citizens may sit in the House of Lords. The nationality restrictions were previously more stringent: under the Act of Settlement 1701, and prior to the British Nationality Act 1948, only natural-born subjects qualified.

Additionally, some bankruptcy-related restrictions apply to members of the Upper House. A person may not sit in the House of Lords if he or she is the subject of a Bankruptcy Restrictions Order (applicable in England and Wales only), or if he or she is adjudged bankrupt (in Northern Ireland), or if his or her estate is sequestered (in Scotland). A final restriction bars an individual convicted of high treason from sitting in the House of Lords until completing his or her full term of imprisonment. An exception applies, however, if the individual convicted of high treason receives a full pardon. Note that an individual serving a prison sentence for an offence other than high treason is "not" automatically disqualified.

Women were excluded from the House of Lords until the Life Peerages Act 1958, passed to address the declining number of active members, made possible the creation of peerages for life. Women were immediately eligible and four were among the first life peers appointed. However, hereditary peeresses continued to be excluded until the passage of the Peerage Act 1963. Since the passage of the House of Lords Act 1999, hereditary peeresses remain eligible for election to the Upper House; there is one (Margaret of Mar, 31st Countess of Mar) among the 90 hereditary peers who continue to sit.

The Honours (Prevention of Abuses) Act 1925 made it illegal for a peerage, or other honour, to be bought or sold. Nonetheless, there have been repeated allegations that life peerages (and thus membership of the House of Lords) have been made available to major political donors in exchange for donations. The most prominent case, the 2006 Cash for Honours scandal, saw a police investigation, with no charges being brought. A 2015 study found that of 303 people nominated for peerages in the period 2005–14, a total of 211 were former senior figures within politics (including former MPs), or were non-political appointments. Of the remaining 92 political appointments from outside public life, 27 had made significant donations to political parties. The authors concluded firstly that nominees from outside public life were much more likely to have made large gifts than peers nominated after prior political or public service. They also found that significant donors to parties were far more likely to be nominated for peerages than other party members.

Traditionally there was no mechanism by which members could resign or be removed from the House of Lords (compare the situation as regards resignation from the House of Commons). The Peerage Act 1963 permitted a person to disclaim their newly inherited peerage (within certain time limits); this meant that such a person could effectively renounce their membership of the Lords. This might be done in order to remain or become qualified to sit in the House of Commons, as in the case of Tony Benn (formerly the second Viscount Stansgate), who had campaigned for such a change.

The House of Lords Reform Act 2014 made provision for members' resignation from the House, removal for non-attendance, and automatic expulsion upon conviction for a serious criminal offence (if resulting in a jail sentence of at least one year). In June 2015, under the House of Lords (Expulsion and Suspension) Act 2015, the House's Standing Orders may provide for the expulsion or suspension of a member upon a resolution of the House.

Traditionally the House of Lords did not elect its own speaker, unlike the House of Commons; rather, the "ex officio" presiding officer was the Lord Chancellor. With the passage of the Constitutional Reform Act 2005, the post of Lord Speaker was created, a position to which a peer is elected by the House and subsequently appointed by the Crown. The first Lord Speaker, elected on 4 May 2006, was Helene Hayman, a former Labour peer. As the Speaker is expected to be an impartial presiding officer, Hayman resigned from the Labour Party. In 2011, Frances D'Souza was elected as the second Lord Speaker, replacing Hayman in September 2011. D'Souza was in turn succeeded by Norman Fowler in September 2016, the incumbent Lord Speaker.

This reform of the post of Lord Chancellor was made due to the perceived constitutional anomalies inherent in the role. The Lord Chancellor was not only the Speaker of the House of Lords, but also a member of the Cabinet; his or her department, formerly the Lord Chancellor's Department, is now called the Ministry of Justice. The Lord Chancellor is no longer the head of the judiciary of England and Wales. Hitherto, the Lord Chancellor was part of all three branches of government: the legislative, the executive, and the judicial.

The overlap of the legislative and executive roles is a characteristic of the Westminster system, as the entire cabinet consists of members of the House of Commons or the House of Lords; however, in June 2003, the Blair Government announced its intention to abolish the post of Lord Chancellor because of the office's mixed executive and judicial responsibilities. The abolition of the office was rejected by the House of Lords, and the Constitutional Reform Act 2005 was thus amended to preserve the office of Lord Chancellor. The Act no longer guarantees that the office holder of Lord Chancellor is the presiding officer of the House of Lords, and therefore allows the House of Lords to elect a speaker of their own.
The Lord Speaker may be replaced as presiding officer by one of his or her deputies. The Chairman of Committees, the Principal Deputy Chairman of Committees, and several Chairmen are all deputies to the Lord Speaker, and are all appointed by the House of Lords itself at the beginning of each session. By custom, the Crown appoints each Chairman, Principal Deputy Chairman and Deputy Chairman to the additional office of Deputy Speaker of the House of Lords. There was previously no legal requirement that the Lord Chancellor or a Deputy Speaker be a member of the House of Lords (though the same has long been customary).

Whilst presiding over the House of Lords, the Lord Chancellor traditionally wore ceremonial black and gold robes. Robes of black and gold are now worn by the Lord Chancellor and Secretary of State for Justice in the House of Commons, on ceremonial occasions. This is no longer a requirement for the Lord Speaker except for State occasions outside of the chamber. The Speaker or Deputy Speaker sits on the Woolsack, a large red seat stuffed with wool, at the front of the Lords Chamber.

When the House of Lords resolves itself into committee (see below), the Chairman of Committees or a Deputy Chairman of Committees presides, not from the Woolsack, but from a chair at the Table of the House. The presiding officer has little power compared to the Speaker of the House of Commons. He or she only acts as the mouthpiece of the House, performing duties such as announcing the results of votes. This is because, unlike in the House of Commons where all statements are directed to "Mr/Madam Speaker", in the House of Lords they are directed to "My Lords"; i.e., the entire body of the House.

The Lord Speaker or Deputy Speaker cannot determine which members may speak, or discipline members for violating the rules of the House; these measures may be taken only by the House itself. Unlike the politically neutral Speaker of the House of Commons, the Lord Chancellor and Deputy Speakers originally remained members of their respective parties, and were permitted to participate in debate; however, this is no longer true of the new role of Lord Speaker.

Another officer of the body is the Leader of the House of Lords, a peer selected by the Prime Minister. The Leader of the House is responsible for steering Government bills through the House of Lords, and is a member of the Cabinet. The Leader also advises the House on proper procedure when necessary, but such advice is merely informal, rather than official and binding. A Deputy Leader is also appointed by the Prime Minister, and takes the place of an absent or unavailable leader.

The Clerk of the Parliaments is the chief clerk and officer of the House of Lords (but is not a member of the House itself). The Clerk, who is appointed by the Crown, advises the presiding officer on the rules of the House, signs orders and official communications, endorses bills, and is the keeper of the official records of both Houses of Parliament. Moreover, the Clerk of the Parliaments is responsible for arranging by-elections of hereditary peers when necessary. The deputies of the Clerk of the Parliaments (the Clerk Assistant and the Reading Clerk) are appointed by the Lord Speaker, subject to the House's approval.

The Gentleman Usher of the Black Rod is also an officer of the House; he takes his title from the symbol of his office, a black rod. Black Rod (as the Gentleman Usher is normally known) is responsible for ceremonial arrangements, is in charge of the House's doorkeepers, and may (upon the order of the House) take action to end disorder or disturbance in the Chamber. Black Rod also holds the office of Serjeant-at-Arms of the House of Lords, and in this capacity attends upon the Lord Speaker. The Gentleman Usher of the Black Rod's duties may be delegated to the Yeoman Usher of the Black Rod or to the Assistant Serjeant-at-Arms.

The House of Lords and the House of Commons assemble in the Palace of Westminster. The Lords Chamber is lavishly decorated, in contrast with the more modestly furnished Commons Chamber. Benches in the Lords Chamber are coloured red. The Woolsack is at the front of the Chamber; the Government sit on benches on the right of the Woolsack, while members of the Opposition sit on the left. Crossbenchers, sit on the benches immediately opposite the Woolsack.

The Lords Chamber is the site of many formal ceremonies, the most famous of which is the State Opening of Parliament, held at the beginning of each new parliamentary session. During the State Opening, the Sovereign, seated on the Throne in the Lords Chamber and in the presence of both Houses of Parliament, delivers a speech outlining the Government's agenda for the upcoming parliamentary session.

In the House of Lords, members need not seek the recognition of the presiding officer before speaking, as is done in the House of Commons. If two or more Lords simultaneously rise to speak, the House decides which one is to be heard by acclamation, or, if necessary, by voting on a motion. Often, however, the Leader of the House will suggest an order, which is thereafter generally followed. Speeches in the House of Lords are addressed to the House as a whole ("My Lords") rather than to the presiding officer alone (as is the custom in the Lower House). Members may not refer to each other in the second person (as "you"), but rather use third person forms such as "the noble Duke", "the noble Earl", "the noble Lord", "my noble friend", "The most Reverend Primate", etc.

Each member may make no more than one speech on a motion, except that the mover of the motion may make one speech at the beginning of the debate and another at the end. Speeches are not subject to any time limits in the House; however, the House may put an end to a speech by approving a motion "that the noble Lord be no longer heard". It is also possible for the House to end the debate entirely, by approving a motion "that the Question be now put". This procedure is known as Closure, and is extremely rare.

Once all speeches on a motion have concluded, or Closure invoked, the motion may be put to a vote. The House first votes by voice vote; the Lord Speaker or Deputy Speaker puts the question, and the Lords respond either "content" (in favour of the motion) or "not content" (against the motion). The presiding officer then announces the result of the voice vote, but if his assessment is challenged by any Lord, a recorded vote known as a division follows.

Members of the House enter one of two lobbies (the "content" lobby or the "not-content" lobby) on either side of the Chamber, where their names are recorded by clerks. At each lobby are two Tellers (themselves members of the House) who count the votes of the Lords. The Lord Speaker may not take part in the vote. Once the division concludes, the Tellers provide the results thereof to the presiding officer, who then announces them to the House.

If there is an equality of votes, the motion is decided according to the following principles: legislation may proceed in its present form, unless there is a majority in favour of amending or rejecting it; any other motions are rejected, unless there is a majority in favour of approving it. The quorum of the House of Lords is just three members for a general or procedural vote, and 30 members for a vote on legislation. If fewer than three or 30 members (as appropriate) are present, the division is invalid.

By contrast with the House of Commons, the House of Lords has not until recently had an established procedure for putting sanctions on its members. When a cash for influence scandal was referred to the Committee of Privileges in January 2009, the Leader of the House of Lords also asked the Privileges Committee to report on what sanctions the House had against its members. After seeking advice from the Attorney General for England and Wales and the former Lord Chancellor James, Lord Mackay of Clashfern, the committee decided that the House "possessed an inherent power" to suspend errant members, although not to withhold a writ of summons nor to expel a member permanently. When the House subsequently suspended Peter, Lord Truscott and Tom, Lord Taylor of Blackburn for their role in the scandal, they were the first to meet this fate since 1642.

Recent changes have expanded the disciplinary powers of the House. Section 3 of the House of Lords Reform Act 2014 now provides that any member of the House of Lords convicted of a crime and sentenced to imprisonment for more than one year loses their seat. The House of Lords (Expulsion and Suspension) Act 2015 allows the House to set up procedures to suspend, and to expel, its members.

There are two motions which have grown up through custom and practice and which govern questionable conduct within the House. They are brought into play by a member standing up, possibly intervening on another member, and moving the motion without notice. When the debate is getting excessively heated, it is open to a member to move "that the Standing Order on Asperity of Speech be read by the Clerk". The motion can be debated, but if agreed by the House, the Clerk of the Parliaments will read out Standing Order 33 which provides "That all personal, sharp, or taxing speeches be forborn". The Journals of the House of Lords record only four instances on which the House has ordered the Standing Order to be read since the procedure was invented in 1871.

For more serious problems with an individual Lord, the option is available to move "That the noble Lord be no longer heard". This motion also is debatable, and the debate which ensues has sometimes offered a chance for the member whose conduct has brought it about to come to order so that the motion can be withdrawn. If the motion is passed, its effect is to prevent the member from continuing their speech on the motion then under debate. The Journals identify eleven occasions on which this motion has been moved since 1884; four were eventually withdrawn, one was voted down, and six were passed.

In 1958, to counter criticism that some peers only appeared at major decisions in the House and thereby particular votes were swayed, the Standing Orders of the House of Lords were enhanced. Peers who did not wish to attend meetings regularly or were prevented by ill health, age or further reasons, were now able to request Leave of Absence. During the granted time a peer is expected not to visit the House's meetings until either its expiration or termination, announced at least a month prior to their return.

Members of the House of Lords can, since 2010, opt to receive a £300 per day attendance allowance, plus limited travel expenses. Peers can elect to receive a reduced attendance allowance of £150 per day instead. Prior to 2010 peers from outside London could claim an overnight allowance of £174.

Unlike in the House of Commons, when the term committee is used to describe a stage of a bill, this committee does not take the form of a public bill committee, but what is described as Committee of the Whole House. It is made up of all Members of the House of Lords allowing any Member to contribute to debates if he or she chooses to do so and allows for more flexible rules of procedure. It is presided over by the Chairman of Committees.

The term committee is also used to describe Grand Committee, where the same rules of procedure apply as in the main chamber, except that no divisions may take place. For this reason, business that is discussed in Grand Committee is usually uncontroversial and likely to be agreed unanimously.

Public bills may also be committed to pre-legislative committees. A pre-legislative Committee is specifically constituted for a particular bill. These committees are established in advance of the bill being laid before either the House of Lords or the House of Commons and can take evidence from the public. Such committees are rare and do not replace any of the usual stages of a bill, including committee stage.

The House of Lords also has 15 Select Committees. Typically, these are "sessional committees", meaning that their members are appointed by the House at the beginning of each session, and continue to serve until the next parliamentary session begins. In practice, these are often permanent committees, which are re-established during every session. These committees are typically empowered to make reports to the House "from time to time", that is, whenever they wish. Other committees are "ad-hoc committees", which are set up to investigate a specific issue. When they are set up by a motion in the House, the motion will set a deadline by which the Committee must report. After this date, the Committee will cease to exist unless it is granted an extension. One example of this is the Committee on Public Service and Demographic Change. The House of Lords may appoint a chairman for a committee; if it does not do so, the Chairman of Committees or a Deputy Chairman of Committees may preside instead. Most of the Select Committees are also granted the power to co-opt members, such as the European Union Committee. The primary function of Select Committees is to scrutinise and investigate Government activities; to fulfil these aims, they are permitted to hold hearings and collect evidence. Bills may be referred to Select Committees, but are more often sent to the Committee of the Whole House and Grand Committees.

The committee system of the House of Lords also includes several Domestic Committees, which supervise or consider the House's procedures and administration. One of the Domestic Committees is the Committee of Selection, which is responsible for assigning members to many of the House's other committees.

There are currently sitting members of the House of Lords. An additional Lords are ineligible from participation, including eight peers who are constitutionally disqualified as members of the Judiciary.

The House of Lords Act 1999 allocated 75 of the 92 hereditary peers to the parties based on the proportion of hereditary peers that belonged to that party in 1999:

Of the initial 42 hereditary peers elected as Conservatives, one, Leonard, Lord Willoughby de Broke, now sits as a member of UKIP.

Fifteen hereditary peers are elected by the whole House, and the remaining hereditary peers are the two royal office-holders, the Earl Marshal and the Lord Great Chamberlain, both of whom are currently on leave of absence.

A report in 2007 stated that many members of the Lords (particularly the life peers) do not attend regularly; the average daily attendance was around 408.

While the number of hereditary peers is limited to 92, and that of Lords spiritual to 26, there is no maximum limit to the number of life peers who may be members of the House of Lords at any time.











</doc>
<doc id="13660" url="https://en.wikipedia.org/wiki?curid=13660" title="Homeomorphism">
Homeomorphism

In the mathematical field of topology, a homeomorphism or topological isomorphism or bi continuous function is a continuous function between topological spaces that has a continuous inverse function. Homeomorphisms are the isomorphisms in the category of topological spaces—that is, they are the mappings that preserve all the topological properties of a given space. Two spaces with a homeomorphism between them are called homeomorphic, and from a topological viewpoint they are the same. The word "homeomorphism" comes from the Greek words "ὅμοιος" ("homoios") = similar or same and "μορφή" ("morphē") = shape, form, introduced to mathematics by Henri Poincaré in 1895. 

Very roughly speaking, a topological space is a geometric object, and the homeomorphism is a continuous stretching and bending of the object into a new shape. Thus, a square and a circle are homeomorphic to each other, but a sphere and a torus are not. However, this description can be misleading. Some continuous deformations are not homeomorphisms, such as the deformation of a line into a point. Some homeomorphisms are not continuous deformations, such as the homeomorphism between a trefoil knot and a circle.

An often-repeated mathematical joke is that topologists can't tell the difference between a coffee cup and a donut, since a sufficiently pliable donut could be reshaped to the form of a coffee cup by creating a dimple and progressively enlarging it, while preserving the donut hole in a cup's handle.

A function formula_1 between two topological spaces formula_2 and formula_3 is called a homeomorphism if it has the following properties:


A function with these three properties is sometimes called bicontinuous. If such a function exists, we say formula_2 and formula_9 are homeomorphic. A self-homeomorphism is a homeomorphism of a topological space and itself. The homeomorphisms form an equivalence relation on the class of all topological spaces. The resulting equivalence classes are called homeomorphism classes.



The third requirement, that formula_22 be continuous, is essential. Consider for instance the function formula_23 (the unit circle in formula_24) defined byformula_25. This function is bijective and continuous, but not a homeomorphism (formula_26 is compact but formula_27 is not). The function formula_22 is not continuous at the point formula_29, because although formula_22 maps formula_29 to formula_32, any neighbourhood of this point also includes points that the function maps close to formula_33, but the points it maps to numbers in between lie outside the neighbourhood.

Homeomorphisms are the isomorphisms in the category of topological spaces. As such, the composition of two homeomorphisms is again a homeomorphism, and the set of all self-homeomorphisms formula_34 forms a group, called the homeomorphism group of "X", often denoted formula_35. This group can be given a topology, such as the compact-open topology, which under certain assumptions makes it a topological group.

For some purposes, the homeomorphism group happens to be too big, but by means of the isotopy relation, one can reduce this group to the mapping class group.

Similarly, as usual in category theory, given two spaces that are homeomorphic, the space of homeomorphisms between them, formula_36, is a torsor for the homeomorphism groups formula_35 and formula_38, and, given a specific homeomorphism between formula_39 and formula_40, all three sets are identified.


The intuitive criterion of stretching, bending, cutting and gluing back together takes a certain amount of practice to apply correctly—it may not be obvious from the description above that deforming a line segment to a point is impermissible, for instance. It is thus important to realize that it is the formal definition given above that counts. In this case, for example, the line segment possesses infinitely many points, and therefore cannot be put into a bijection with a set containing only a finite number of points, including a single point.

This characterization of a homeomorphism often leads to a confusion with the concept of homotopy, which is actually "defined" as a continuous deformation, but from one "function" to another, rather than one space to another. In the case of a homeomorphism, envisioning a continuous deformation is a mental tool for keeping track of which points on space "X" correspond to which points on "Y"—one just follows them as "X" deforms. In the case of homotopy, the continuous deformation from one map to the other is of the essence, and it is also less restrictive, since none of the maps involved need to be one-to-one or onto. Homotopy does lead to a relation on spaces: homotopy equivalence.

There is a name for the kind of deformation involved in visualizing a homeomorphism. It is (except when cutting and regluing are required) an isotopy between the identity map on "X" and the homeomorphism from "X" to "Y".




</doc>
<doc id="13661" url="https://en.wikipedia.org/wiki?curid=13661" title="Hvergelmir">
Hvergelmir

In Norse mythology, Hvergelmir (Old Norse "bubbling boiling spring") is a major spring. Hvergelmir is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In the "Poetic Edda", Hvergelmir is mentioned in a single stanza, which details that it is the location where liquid from the antlers of the stag Eikþyrnir flow, and that the spring, "whence all waters rise", is the source of numerous rivers. The "Prose Edda" repeats this information and adds that the spring is located in Niflheim, that it is one of the three major springs at the primary roots of the cosmic tree Yggdrasil (the other two are Urðarbrunnr and Mímisbrunnr), and that within the spring are a vast amount of snakes and the dragon Níðhöggr.

Hvergelmir is attested in the following works:

Hvergelmir receives a single mention in the "Poetic Edda", found in the poem "Grímnismál":

This stanza is followed by three stanzas consisting mainly of the names of 42 rivers. Some of these rivers lead to the dwelling of the gods (such as Gömul and Geirvimul), while at least two (Gjöll and Leipt), reach to Hel.

Hvergelmir is mentioned several times in the "Prose Edda". In "Gylfaginning", Just-as-High explains that the spring Hvergelmir is located in the foggy realm of Niflheim: "It was many ages before the earth was created that Niflheim was made, and in its midst lies a spring called Hvergelmir, and from it flows the rivers called Svol, Gunnthra, Fiorm, Fimbulthul, Slidr and Hrid, Sylg and Ylg, Vid, Leiptr; Gioll is next to Hell-gates."

Later in "Gylfaginning", Just-as-High describes the central tree Yggdrasil. Just-as-High says that three roots of the tree support it and "extend very, very far" and that the third of these three roots extends over Niflheim. Beneath this root, says Just-as-High, is the spring Hvergelmir, and that the base of the root is gnawed on by the dragon Níðhöggr. Additionally, High says that Hvergelmir contains not only Níðhöggr but also so many snakes that "no tongue can enumerate them".

The spring is mentioned a third time in "Gylfaginning" where High recounts its source: the stag Eikþyrnir stands on top of the afterlife hall Valhalla feeding branches of Yggdrasil, and from the stag's antlers drips great amounts of liquid down into Hvergelmir. High tallies 26 rivers here.

Hvergelmir is mentioned a final time in the "Prose Edda" where Third discusses the unpleasantries of Náströnd. Third notes that Hvergelmir yet worse than the venom-filled Náströnd because—by way of quoting a portion of a stanza from the "Poetic Edda" poem "Völuspá"—"There Nidhogg torments the bodies of the dead".



</doc>
<doc id="13665" url="https://en.wikipedia.org/wiki?curid=13665" title="Hausdorff maximal principle">
Hausdorff maximal principle

In mathematics, the Hausdorff maximal principle is an alternate and earlier formulation of Zorn's lemma proved by Felix Hausdorff in 1914 (Moore 1982:168). It states that in any partially ordered set, every totally ordered subset is contained in a maximal totally ordered subset.

The Hausdorff maximal principle is one of many statements equivalent to the axiom of choice over ZF (Zermelo–Fraenkel set theory without the axiom of choice). The principle is also called the Hausdorff maximality theorem or the Kuratowski lemma (Kelley 1955:33).

The Hausdorff maximal principle states that, in any partially ordered set, every totally ordered subset is contained in a maximal totally ordered subset. Here a maximal totally ordered subset is one that, if enlarged in any way, does not remain totally ordered. The maximal set produced by the principle is not unique, in general; there may be many maximal totally ordered subsets containing a given totally ordered subset.

An equivalent form of the principle is that in every partially ordered set there exists a maximal totally ordered subset.

To prove that it follows from the original form, let "A" be a poset. Then formula_1 is a totally ordered subset of "A", hence there exists a maximal totally ordered subset containing formula_1, in particular "A" contains a maximal totally ordered subset.

For the converse direction, let "A" be a partially ordered set and "T" a totally ordered subset of "A". Then
is partially ordered by set inclusion formula_4, therefore it contains a maximal totally ordered subset "P". Then the set formula_5 satisfies the desired properties.

The proof that the Hausdorff maximal principle is equivalent to Zorn's lemma is very similar to this proof.

EXAMPLE 1. If "A" is any collection of sets, the relation "is a proper subset of" is a strict partial order on "A". Suppose that "A" is the collection of all circular regions (interiors of circles) in the plane. One maximal totally ordered sub-collection of "A" consists of all circular regions with centers at the origin. Another maximal totally ordered sub-collection consists of all circular regions bounded by circles tangent from the right to the y-axis at the origin.

EXAMPLE 2. If (x, y) and (x, y) are two points of the plane ℝ, define (x, y) < (x, y)

if y = y and x < x. This is a partial ordering of ℝ under which two points are comparable only if they lie on the same horizontal line. The maximal totally ordered sets are horizontal lines in ℝ.




</doc>
<doc id="13666" url="https://en.wikipedia.org/wiki?curid=13666" title="Hel (being)">
Hel (being)

In Norse mythology, Hel is a being who presides over a realm of the same name, where she receives a portion of the dead. Hel is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In addition, she is mentioned in poems recorded in "Heimskringla" and "Egils saga" that date from the 9th and 10th centuries, respectively. An episode in the Latin work "Gesta Danorum", written in the 12th century by Saxo Grammaticus, is generally considered to refer to Hel, and Hel may appear on various Migration Period bracteates.

In the "Poetic Edda", "Prose Edda", and "Heimskringla", Hel is referred to as a daughter of Loki. In the "Prose Edda" book "Gylfaginning", Hel is described as having been appointed by the god Odin as ruler of a realm of the same name, located in Niflheim. In the same source, her appearance is described as half blue and half flesh-coloured and further as having a gloomy, downcast appearance. The "Prose Edda" details that Hel rules over vast mansions with many servants in her underworld realm and plays a key role in the attempted resurrection of the god Baldr.

Scholarly theories have been proposed about Hel's potential connections to figures appearing in the 11th-century "Old English Gospel of Nicodemus" and Old Norse "Bartholomeus saga postola", that she may have been considered a goddess with potential Indo-European parallels in Bhavani, Kali, and Mahakali or that Hel may have become a being only as a late personification of the location of the same name.

The Old Norse feminine proper noun "Hel" is identical to the name of the location over which she rules, Old Norse "Hel". The word has cognates in all branches of the Germanic languages, including Old English "hell" (and thus Modern English "hell"), Old Frisian "helle", Old Saxon "hellia", Old High German "hella", and Gothic "halja". All forms ultimately derive from the reconstructed Proto-Germanic feminine noun *"xaljō" or *"haljō" ('concealed place, the underworld'). In turn, the Proto-Germanic form derives from the o-grade form of the Proto-Indo-European root *"kel-", *"kol"-: 'to cover, conceal, save'.

The term is etymologically related to Modern English "hall" and therefore also "Valhalla", an afterlife 'hall of the slain' in Norse Mythology. "Hall" and its numerous Germanic cognates derive from Proto-Germanic *"hallō" 'covered place, hall', from Proto-Indo-European *"kol-".

Related early Germanic terms and concepts include Proto-Germanic *"xalja-rūnō(n)", a feminine compound noun, and *"xalja-wītjan", a neutral compound noun. This form is reconstructed from the Latinized Gothic plural noun *"haliurunnae" (attested by Jordanes; according to philologist Vladimir Orel, meaning 'witches'), Old English "helle-rúne" ('sorceress, necromancer', according to Orel), and Old High German "helli-rūna" 'magic'. The compound is composed of two elements: *"xaljō" (*"haljō") and *"rūnō", the Proto-Germanic precursor to Modern English "rune". The second element in the Gothic "haliurunnae" may however instead be an agent noun from the verb "rinnan" ("to run, go"), which would make its literal meaning "one who travels to the netherworld".)

Proto-Germanic *"xalja-wītjan" (or *"halja-wītjan") is reconstructed from Old Norse "hel-víti" 'hell', Old English "helle-wíte" 'hell-torment, hell', Old Saxon "helli-wīti" 'hell', and the Middle High German feminine noun "helle-wīze". The compound is a compound of *"xaljō" (discussed above) and *"wītjan" (reconstructed from forms such as Old English "witt" 'right mind, wits', Old Saxon "gewit" 'understanding', and Gothic "un-witi" 'foolishness, understanding').

The "Poetic Edda", compiled in the 13th century from earlier traditional sources, features various poems that mention Hel. In the "Poetic Edda" poem "Völuspá", Hel's realm is referred to as the "Halls of Hel." In stanza 31 of "Grímnismál", Hel is listed as living beneath one of three roots growing from the world tree Yggdrasil. In "Fáfnismál", the hero Sigurd stands before the mortally wounded body of the dragon Fáfnir, and states that Fáfnir lies in pieces, where "Hel can take" him. In "Atlamál", the phrases "Hel has half of us" and "sent off to Hel" are used in reference to death, though it could be a reference to the location and not the being, if not both. In stanza 4 of "Baldrs draumar", Odin rides towards the "high hall of Hel."

Hel may also be alluded to in "Hamðismál". Death is periphrased as "joy of the troll-woman" (or "ogress") and ostensibly it is Hel being referred to as the troll-woman or the ogre ("flagð"), although it may otherwise be some unspecified "dís". The Poetic Edda also mentions that travelers to Hel must pass by her guardian hound Garmr.

Hel is referred to in the "Prose Edda", written in the 13th century by Snorri Sturluson. In chapter 34 of the book "Gylfaginning", Hel is listed by High as one of the three children of Loki and Angrboða; the wolf Fenrir, the serpent Jörmungandr, and Hel. High continues that, once the gods found that these three children are being brought up in the land of Jötunheimr, and when the gods "traced prophecies that from these siblings great mischief and disaster would arise for them" then the gods expected a lot of trouble from the three children, partially due to the nature of the mother of the children, yet worse so due to the nature of their father.

High says that Odin sent the gods to gather the children and bring them to him. Upon their arrival, Odin threw Jörmungandr into "that deep sea that lies round all lands," Odin threw Hel into Niflheim, and bestowed upon her authority over nine worlds, in that she must "administer board and lodging to those sent to her, and that is those who die of sickness or old age." High details that in this realm Hel has "great Mansions" with extremely high walls and immense gates, a hall called Éljúðnir, a dish called "Hunger," a knife called "Famine," the servant Ganglati (Old Norse "lazy walker"), the serving-maid Ganglöt (also "lazy walker"), the entrance threshold "Stumbling-block," the bed "Sick-bed," and the curtains "Gleaming-bale." High describes Hel as "half black and half flesh-coloured," adding that this makes her easily recognizable, and furthermore that Hel is "rather downcast and fierce-looking."

In chapter 49, High describes the events surrounding the death of the god Baldr. The goddess Frigg asks who among the Æsir will earn "all her love and favour" by riding to Hel, the location, to try to find Baldr, and offer Hel herself a ransom. The god Hermóðr volunteers and sets off upon the eight-legged horse Sleipnir to Hel. Hermóðr arrives in Hel's hall, finds his brother Baldr there, and stays the night. The next morning, Hermóðr begs Hel to allow Baldr to ride home with him, and tells her about the great weeping the Æsir have done upon Baldr's death. Hel says the love people have for Baldr that Hermóðr has claimed must be tested, stating:

If all things in the world, alive or dead, weep for him, then he will be allowed to return to the Æsir. If anyone speaks against him or refuses to cry, then he will remain with Hel.
Later in the chapter, after the female jötunn Þökk refuses to weep for the dead Baldr, she responds in verse, ending with "let Hel hold what she has." In chapter 51, High describes the events of Ragnarök, and details that when Loki arrives at the field Vígríðr "all of Hel's people" will arrive with him.

In chapter 5 of the "Prose Edda" book "Skáldskaparmál", Hel is mentioned in a kenning for Baldr ("Hel's companion"). In chapter 16, "Hel's [...] relative or father" is given as a kenning for Loki. In chapter 50, Hel is referenced ("to join the company of the quite monstrous wolf's sister") in the skaldic poem "Ragnarsdrápa".

In the "Heimskringla" book "Ynglinga saga", written in the 13th century by Snorri Sturluson, Hel is referred to, though never by name. In chapter 17, the king Dyggvi dies of sickness. A poem from the 9th-century "Ynglingatal" that forms the basis of "Ynglinga saga" is then quoted that describes Hel's taking of Dyggvi:

In chapter 45, a section from "Ynglingatal" is given which refers to Hel as "howes'-warder" (meaning "guardian of the graves") and as taking King Halfdan Hvitbeinn from life. In chapter 46, King Eystein Halfdansson dies by being knocked overboard by a sail yard. A section from "Ynglingatal" follows, describing that Eystein "fared to" Hel (referred to as "Býleistr's-brother's-daughter"). In chapter 47, the deceased Eystein's son King Halfdan dies of an illness, and the excerpt provided in the chapter describes his fate thereafter, a portion of which references Hel:

In a stanza from "Ynglingatal" recorded in chapter 72 of the "Heimskringla" book "Saga of Harald Sigurdsson", "given to Hel" is again used as a phrase to referring to death.

The Icelanders' saga "Egils saga" contains the poem "Sonatorrek". The saga attributes the poem to 10th century skald Egill Skallagrímsson, and writes that it was composed by Egill after the death of his son Gunnar. The final stanza of the poem contains a mention of Hel, though not by name:

In the account of Baldr's death in Saxo Grammaticus' early 13th century work "Gesta Danorum", the dying Baldr has a dream visitation from Proserpina (here translated as "the goddess of death"):

The following night the goddess of death appeared to him in a dream standing at his side, and declared that in three days time she would clasp him in her arms. It was no idle vision, for after three days the acute pain of his injury brought his end.

Scholars have assumed that Saxo used Proserpina as a goddess equivalent to the Norse Hel.

It has been suggested that several imitation medallions and bracteates of the Migration Period (ca. first centuries AD) feature depictions of Hel. In particular the bracteates IK 14 and IK 124 depict a rider traveling down a slope and coming upon a female being holding a scepter or a staff. The downward slope may indicate that the rider is traveling towards the realm of the dead and the woman with the scepter may be a female ruler of that realm, corresponding to Hel.

Some B-class bracteates showing three godly figures have been interpreted as depicting Baldr's death, the best known of these is the Fakse bracteate. Two of the figures are understood to be Baldr and Odin while both Loki and Hel have been proposed as candidates for the third figure. If it is Hel she is presumably greeting the dying Baldr as he comes to her realm.

The "Old English Gospel of Nicodemus", preserved in two manuscripts from the 11th century, contains a female figure referred to as "Seo hell" who engages in flyting with Satan and tells him to leave her dwelling (Old English "ut of mynre onwununge"). Regarding Seo Hell in the "Old English Gospel of Nicodemus", Michael Bell states that "her vivid personification in a dramatically excellent scene suggests that her gender is more than grammatical, and invites comparison with the Old Norse underworld goddess Hel and the Frau Holle of German folklore, to say nothing of underworld goddesses in other cultures" yet adds that "the possibility that these genders "are" merely grammatical is strengthened by the fact that an Old Norse version of Nicodemus, possibly translated under English influence, personifies Hell in the neutral (Old Norse "þat helvíti")."

The Old Norse "Bartholomeus saga postola", an account of the life of Saint Bartholomew dating from the 13th century, mentions a "Queen Hel." In the story, a devil is hiding within a pagan idol, and bound by Bartholomew's spiritual powers to acknowledge himself and confess, the devil refers to Jesus as the one which "made war on Hel our queen" (Old Norse "heriaði a Hel drottning vara"). "Queen Hel" is not mentioned elsewhere in the saga.

Michael Bell says that while Hel "might at first appear to be identical with the well-known pagan goddess of the Norse underworld" as described in chapter 34 of "Gylfaginning", "in the combined light of the Old English and Old Norse versions of "Nicodemus" she casts quite a different a shadow," and that in "Bartholomeus saga postola" "she is clearly the queen of the Christian, not pagan, underworld."

Jacob Grimm theorized that Hel (whom he refers to here as "Halja", the theorized Proto-Germanic form of the term) is essentially an "image of a greedy, unrestoring, female deity" and that "the higher we are allowed to penetrate into our antiquities, the less hellish and more godlike may "Halja" appear. Of this we have a particularly strong guarantee in her affinity to the Indian Bhavani, who travels about and bathes like Nerthus and Holda, but is likewise called "Kali" or "Mahakali", the great "black" goddess. In the underworld she is supposed to sit in judgment on souls. This office, the similar name and the black hue [...] make her exceedingly like "Halja". And "Halja" is one of the oldest and commonest conceptions of our heathenism."

Grimm theorizes that the Helhest, a three legged-horse that roams the countryside "as a harbinger of plague and pestilence" in Danish folklore, was originally the steed of the goddess Hel, and that on this steed Hel roamed the land "picking up the dead that were her due." In addition, Grimm says that a wagon was once ascribed to Hel, with which Hel made journeys. Grimm says that Hel is an example of a "half-goddess;" "one who cannot be shown to be either wife or daughter of a god, and who stands in a dependent relation to higher divinities" and that "half-goddesses" stand higher than "half-gods" in Germanic mythology.

Hilda Ellis Davidson (1948) states that Hel "as a goddess" in surviving sources seems to belong to a genre of literary personification, that the word "hel" is generally "used simply to signify death or the grave," and that the word often appears as the equivalent to the English 'death,' which Davidson states "naturally lends itself to personification by poets." Davidson explains that "whether this personification has originally been based on a belief in a goddess of death called Hel is another question," but that she does not believe that the surviving sources give any reason to believe so. Davidson adds that, on the other hand, various other examples of "certain supernatural women" connected with death are to be found in sources for Norse mythology, that they "seem to have been closely connected with the world of death, and were pictured as welcoming dead warriors," and that the depiction of Hel "as a goddess" in "Gylfaginning" "might well owe something to these."

In a later work (1998), Davidson states that the description of Hel found in chapter 33 of "Gylfaginning" "hardly suggests a goddess." Davidson adds that "yet this is not the impression given in the account of Hermod's ride to Hel later in "Gylfaginning" (49)" and points out that here Hel "[speaks] with authority as ruler of the underworld" and that from her realm "gifts are sent back to Frigg and Fulla by Balder's wife Nanna as from a friendly kingdom." Davidson posits that Snorri may have "earlier turned the goddess of death into an allegorical figure, just as he made Hel, the underworld of shades, a place 'where wicked men go,' like the Christian Hell ("Gylfaginning" 3)." Davidson continues that:

On the other hand, a goddess of death who represents the horrors of slaughter and decay is something well known elsewhere; the figure of Kali in India is an outstanding example. Like Snorri's Hel, she is terrifying to in appearance, black or dark in colour, usually naked, adorned with severed heads or arms or the corpses of children, her lips smeared with blood. She haunts the battlefield or cremation ground and squats on corpses. Yet for all this she is "the recipient of ardent devotion from countless devotees who approach her as their mother" [...].

Davidson further compares to early attestations of the Irish goddesses Badb (Davidson points to the description of Badb from "The Destruction of Da Choca's Hostel" where Badb is wearing a dusky mantle, has a large mouth, is dark in color, and has gray hair falling over her shoulders, or, alternatively, "as a red figure on the edge of the ford, washing the chariot of a king doomed to die") and The Morrígan. Davidson concludes that, in these examples, "here we have the fierce destructive side of death, with a strong emphasis on its physical horrors, so perhaps we should not assume that the gruesome figure of Hel is wholly Snorri's literary invention."

John Lindow states that most details about Hel, as a figure, are not found outside of Snorri's writing in "Gylfaginning", and says that when older skaldic poetry "says that people are 'in' rather than 'with' Hel, we are clearly dealing with a place rather than a person, and this is assumed to be the older conception," that the noun and place "Hel" likely originally simply meant "grave," and that "the personification came later." He also draws a parallel between the personified Hel's banishment to the underworld and the binding of Fenrir as part of a recurring theme of the bound monster, where an enemy of the gods is bound but destined to break free at Ragnarok. Rudolf Simek theorizes that the figure of Hel is "probably a very late personification of the underworld Hel," and says that "the first scriptures using the goddess Hel are found at the end of the 10th and in the 11th centuries." Simek states that the allegorical description of Hel's house in "Gylfaginning" "clearly stands in the Christian tradition," and that "on the whole nothing speaks in favour of there being a belief in Hel in pre-Christian times." However, Simek also cites Hel as possibly appearing as one of three figures appearing together on Migration Period B-bracteates.

In January 2017, the Icelandic Naming Committee ruled that parents could not name their child "Hel" "on the grounds that the name would cause the child significant distress and trouble as it grows up". 





</doc>
<doc id="13667" url="https://en.wikipedia.org/wiki?curid=13667" title="Hawar Islands">
Hawar Islands

The Hawar Islands (; transliterated: Juzur Ḩawār) are an archipelago of desert islands owned by Bahrain, situated off the west coast of Qatar in the Gulf of Bahrain of the Persian Gulf.

The islands used to be one of the settlements of the Bahraini branch of the Dawasir who settled there in the early 19th century. The islands were first surveyed in 1820, when they were called the Warden’s Islands, and two villages were recorded. They are now uninhabited, other than a police garrison and a hotel on the main island; access to all but Hawar island itself is severely restricted. Local fishermen are allowed to fish in adjacent waters and there is some recreational fishing and tourism on and around the islands. Fresh water has always been scarce; historically it was obtained by surface collection and even today, with the desalinisation plant, additional supplies have to be brought in.

Despite their proximity to Qatar (they are only about from the Qatari mainland whilst being about from the main islands of Bahrain), most of the islands belong to Bahrain, having been a part of a dispute between Bahrain and Qatar which was resolved in 2001. The islands were formerly coincident with municipality or "Minţaqat" Juzur Ḩawār (مِنْطَقَة جُزُر حَوَار) and are now administered as part of the Southern Governorate of Bahrain. The land area of the islands is approximately 52 km.

Although there are 36 islands in the group, many of the smaller islands are little more than sand or shingle accumulations on areas of exposed bedrock molded by the ongoing processes of sedimentation and accretion.
The application named 8 major islands (see table hereafter), which conforms to the description of the islands when first surveyed as consisting of 8 or 9 islands. It has often been described as an archipelago of 16 islands. Janan island, to the south of Hawar island, is not legally considered to be a part of the group and is owned by Qatar.

The islands are home to many bird species, notably Socotra cormorants. There are small herds of Arabian oryx and sand gazelle on Hawar island, and the seas around support a large population of dugong.

The islands were listed as a Ramsar site in 1997. In 2002, the Bahraini government applied to have the islands recognised as a World Heritage Site due to their unique environment and habitat for endangered species; the application was ultimately unsuccessful.

The islands were formerly coincident with municipality or "Minţaqat" Juzur Ḩawār (مِنْطَقَة جُزُر حَوَار) and are now administered as part of the Southern Governorate of Bahrain.

In 2014, a Best Western hotel with 140 rooms replaced a much smaller Hawar Islands Resort. However, the resort closed in mid-2016.

By far the largest island is Hawar, which accounts for more than 41 km of the 54.5 km land area. Following in size are Suwād al Janūbīyah, Suwād ash Shamālīyah, Rubud Al Sharqiyah, Rubud Al Gharbiyah, and Muhazwarah (Umm Hazwarah).

The following were not considered as part of the Hawar islands in the International Court of Justice (ICJ) judgment, being located between Hawar and the Bahrain Islands and not disputed by Qatar, but have been included in the Hawar archipelago by the Bahrain government as part of the 2002 World Heritage Site application.

Janan island, a small island south of Hawar island, was also considered in the 2001 judgment. Based on a previous agreement when both Qatar and Bahrain were under British protection, it was judged to be separate from the Hawar islands and so considered by the court separately. It was awarded to Qatar.





</doc>
<doc id="13669" url="https://en.wikipedia.org/wiki?curid=13669" title="Hans-Dietrich Genscher">
Hans-Dietrich Genscher

Hans-Dietrich Genscher (21 March 1927 – 31 March 2016) was a German statesman and a member of the liberal Free Democratic Party (FDP), who served as the Minister of the Interior of West Germany from 1969 to 1974, and as the Foreign Minister and Vice Chancellor of West Germany and then the reunified Germany from 1974 to 1992 (except for a two-week break in 1982), making him the longest-serving occupant of either post. In 1991 he was chairman of the Organization for Security and Co-operation in Europe (OSCE).

A proponent of Realpolitik, Genscher has been called "a master of diplomacy." He is widely regarded as having been a principal "architect of German Reunification." In 1991, he played a pivotal role in the breakup of Yugoslavia by successfully pushing for international recognition of Croatia, Slovenia and other republics declaring independence, in an effort to halt "a trend towards a so called; Greater Serbia." After leaving office, he worked as a lawyer and international consultant. He was President of the German Council on Foreign Relations and was involved with several international organisations, and with former Czech President Václav Havel, he called for a Cold War museum to be built in Berlin.

Genscher was born on 21 March 1927 in Reideburg (Province of Saxony-Anhalt), now a part of Halle, in what later became East Germany. He was the son of Hilda Kreime and Kurt Genscher. His father, a lawyer, died when Genscher was nine years old. In 1943, he was drafted to serve as a member of the Air Force Support Personnel ("Luftwaffenhelfer") at the age of 16. At age 17, close to the end of the war, he and his fellow soldiers became members of the Nazi Party due to a collective application ("Sammelantrag") by his Wehrmacht unit. He later said he was unaware of it at the time.
Late in the war, Genscher was deployed as a soldier in General Walther Wenck's 12th Army, which ostensibly was directed to relieve the siege of Berlin. After the German surrender he was an American and British prisoner of war, but was released after two months. Following World War II, he studied law and economics at the universities of Halle and Leipzig (1946–1949) and joined the East German Liberal Democratic Party (LDPD) in 1946.

In 1952, Genscher fled to West Germany, where he joined the Free Democratic Party (FDP). He passed his second state examination in law in Hamburg in 1954 and became a solicitor in Bremen. During these early years after the war, Genscher continuously struggled with illness. From 1956 to 1959 he was a research assistant of the FDP parliamentary group in Bonn. From 1959 to 1965 he was the FDP group managing director, while from 1962 to 1964 he was National Secretary of the FDP.

In 1965 Genscher was elected on the North Rhine-Westphalian FDP list to the West German parliament and remained a member of parliament until his retirement in 1998. He was elected deputy national chairman in 1968. From 1969 he served as minister of the interior in the SPD-FDP coalition government led by Chancellor Willy Brandt.

In 1974 he became foreign minister and vice chancellor, both posts he would hold for 18 years. From 1 October 1974 to 23 February 1985 he was Chairman of the FDP. It was during his tenure as party chairman that the FDP switched from being the junior member of social-liberal coalition to being the junior member of the 1982 coalition with the CDU/CSU. In 1985 he gave up the post of national chairman. After his resignation as Foreign Minister, Genscher was appointed honorary chairman of the FDP in 1992.

After the federal election of 1969 Genscher was instrumental in the formation of the social-liberal coalition of chancellor Willy Brandt and was on 22 October 1969 appointed as federal minister of the interior. 
In 1972, while minister for the interior, Genscher rejected Israel's offer to send an Israeli special forces unit to Germany to deal with the Munich Olympics hostage crisis. A flawed rescue attempt by German police forces at Fürstenfeldbruck air base resulted in a bloody shootout, which left all eleven hostages, five terrorists, and one German policeman dead. Genscher's popularity with Israel declined further when he endorsed the release of the three captured attackers following the hijacking of a Lufthansa aircraft on 29 October 1972.

In the SPD–FDP coalition, Genscher helped shape Brandt's policy of deescalation with the communist East, commonly known as "Ostpolitik", which was continued under chancellor Helmut Schmidt after Brandt's resignation in 1974. He would later be a driving factor in continuing this policy in the new conservative-liberal coalition under Helmut Kohl.

In the negotiations on a coalition government of SPD and FDP following the 1976 elections, it took Genscher 73 days to reach agreement with Chancellor Helmut Schmidt.

As Foreign Minister, Genscher stood for a policy of compromise between East and West, and developed strategies for an active policy of détente and the continuation of the East-West dialogue with the USSR. He was widely regarded a strong advocate of negotiated settlements to international problems. As a popular story on Genscher's preferred method of shuttle diplomacy has it, "two Lufthansa jets crossed over the Atlantic, and Genscher was on both."

Genscher was a major player in the negotiations on the text of the Helsinki Accords. In December 1976, the General Assembly of the United Nations in New York City accepted Genscher's proposal of an anti-terrorism convention in New York, which was set among other things, to respond to demands from hostage-takers under any circumstances.

Genscher was one of the FDP's driving forces when, in 1982, the party switched sides from its coalition with the SPD to support the CDU/CSU in their Constructive vote of no confidence to have incumbent Helmut Schmidt replaced with opposition leader Helmut Kohl as Chancellor. The reason for this was the increase in the differences between the coalition partners, particularly in economic and social policy. The switch was controversial, not least in his own party.

At several points in his tenure, he irritated the governments of the United States and other allies of Germany by appearing not to support Western initiatives fully. "During the Cold War, his penchant to seek the middle ground at times exasperated United States policy-makers who wanted a more decisive, less equivocal Germany," according to Tyler Marshall. Genscher's perceived quasi-neutralism was dubbed "Genscherism". "Fundamental to "Genscherism" was said to be the belief that Germany could play a role as a bridge between East and West without losing its status as a reliable NATO ally." In the 1980s, Genscher opposed the deployment of new short-range NATO missiles in Germany. At the time, the Reagan Administration questioned whether Germany was straying from the Western alliance and following a program of its own.

In 1984, Genscher became the first Western foreign minister to visit Tehran since the Iranian Revolution of 1979. In 1988, he appointed Jürgen Hellner as West Germany's new ambassador to Libya, a post that had been vacant since the 1986 Berlin discotheque bombing, a tragedy which U.S. officials blamed on the government of Muammar Gaddafi.

Genscher's proposals frequently set the tone and direction of foreign affairs among Western Europe's democracies. He was also an active participant in the further development of the European Union, taking an active part in the Single European Act Treaty negotiations in the mid-1980s, as well as the joint publication of the Genscher-Colombo plan with Italian Minister of Foreign Affairs Emilio Colombo which advocated further integration and deepening of relations in the European Union towards a more federal Europe. He later was among the politicians who pushed hard for monetary union alongside Edouard Balladur, France's finance minister, and Giuliano Amato, circulating a memorandum to that effect.

Genscher retained his posts as foreign minister and vice chancellor through German reunification and until 1992 when he stepped down for health reasons.

Genscher is most respected for his efforts that helped spell the end of the Cold War, in the late 1980s when Communist eastern European governments toppled, and which led to German reunification. During his time in office, he focused on maintaining stability and balance between the West and the Soviet bloc. From the beginning, he argued that the West should seek cooperation with Communist governments rather than treat them as implacably hostile; this policy was embraced by many Germans and other Europeans.

Genscher had great interest in European integration and the success of German reunification. He soon pushed for effective support of political reform processes in Poland and Hungary. For this purpose, he visited Poland to meet the chairman of Solidarity Lech Wałęsa as early as January 1980. Especially from 1987 he campaigned for an "active relaxation" policy response by the West to the Soviet efforts. In the years before German reunification, he made a point of maintaining strong ties with his birthplace Halle, which was regarded as significant by admirers and critics alike.

When thousands of East Germans sought refuge in West German embassies in Czechoslovakia and Poland, Genscher held discussions on the refugee crisis at the United Nations in New York with the foreign ministers of Czechoslovakia, Poland, East Germany and the Soviet Union in September 1989. Genscher's 30 September 1989 speech from the balcony of the German embassy in Prague was an important milestone on the road to the end of the GDR. In the embassy courtyard thousands of East German citizens had assembled. They were trying to travel to West Germany, but were being denied permission to travel by the Czechoslovak government at the request of East Germany. He announced that he had reached an agreement with the Communist Czechoslovak government that the refugees could leave: "We have come to you to tell you that today, your departure ..." (German: "Wir sind zu Ihnen gekommen, um Ihnen mitzuteilen, dass heute Ihre Ausreise ..."). After these words, the speech was drowned in cheers.

With his fellow foreign ministers James Baker of the United States and Eduard Shevardnadze of the Soviet Union, Genscher is widely credited with securing Germany's subsequent peaceful unification and the withdrawal of Soviet forces. He negotiated the German reunification in 1990 with his counterpart from the GDR, Markus Meckel. In November 1990, Genscher and his Polish counterpart Krzysztof Skubiszewski signed the German-Polish Border Treaty on the establishment of the Oder–Neisse line as Poland's western border. Meanwhile, he strongly endorsed the plans of the Bush Administration to assure continued U.S. influence in a post-Cold War Europe.

In 1991, Genscher successfully pushed for Germany's recognition of the Republic of Croatia in the Croatian War of Independence shortly after the Yugoslav attack on Vukovar. After Croatia and Slovenia had declared independence, Genscher concluded that Yugoslavia could not be held together, and that republics that wanted to break from the Serbian-dominated federation deserved quick diplomatic recognition. He hoped that such recognition would stop the fighting. The rest of the European Union was subsequently pressured to follow suit soon afterward. The UN Secretary-General Javier Pérez de Cuéllar had warned the German Government, that a recognition of Slovenia and Croatia would lead to an increase in aggression in the former Yugoslavia.

At a meeting of the European Community's foreign ministers in 1991, Genscher proposed to press for a war crimes trial for President Saddam Hussein of Iraq, accusing him of aggression against Kuwait, using chemical weapons against civilians and condoning genocide against the Kurds.

During the Gulf War, Genscher sought to deal with Iraq after other Western leaders had decided to go to war to force it out of Kuwait. Germany made a substantial financial contribution to the allied cause but, citing constitutional restrictions on the use of its armed forces, provided almost no military assistance. In January 1991, Germany sent Genscher on a state visit to Israel and followed up with an agreement to provide the Jewish state with $670 million in military aid, including financing for two submarines long coveted by Israel, a battery of Patriot missiles to defend against Iraqi missiles, 58 armored vehicles specially fitted to detect chemical and biological attacks, and a shipment of gas masks. When, in the aftermath of the war, a far-reaching political debate broke out over how Germany should fulfill its global responsibilities, Genscher responded that if foreign powers expect Germany to assume greater responsibility in the world, they should give it a chance to express its views "more strongly" in the United Nations Security Council. He also famously held that "whatever floats is fine, whatever rolls is not" to sum up Germany's military export policy for restless countries – based on a navy's unsuitability for use against a country's own people.

In 1992, Genscher, together with his Danish colleague Uffe Ellemann-Jensen, took the initiative to create the Council of the Baltic Sea States (CBSS) and the EuroFaculty.

More than half a century after Nazi leaders assembled their infamous exhibition "Degenerate Art," a sweeping condemnation of the work of the avant-garde, Genscher opened a re-creation of the show at the Altes Museum in March 1992, describing Nazi attempts to restrict artistic expression as "a step toward the catastrophe that produced the mass murder of European Jews and the war of extermination against Germany's neighbors." "The paintings in this exhibition have survived oppression and censorship," he asserted in his opening remarks. "They are not only a monument but also a sign of hope. They stand for the triumph of creative freedom over barbarism."

On 18 May 1992 Genscher retired at his own request from the federal government, which he had been member of for a total of 23 years. At the time, he was the world's longest-serving foreign minister and Germany's most popular politician. He had announced his decision three weeks earlier, on 27 April 1992. At that time he was Europe's longest-serving foreign minister. Genscher did not specify his reasons for quitting; however, he had suffered two heart attacks by that time. His resignation took effect in May, but he remained a member of parliament and continued to be influential in the Free Democratic Party.

Following Genscher's resignation, Chancellor Helmut Kohl and FDP chairman Otto Graf Lambsdorff named Irmgard Schwaetzer, a former aide to Genscher, to be the new Foreign Minister. In a surprise decision, however, a majority of the FDP parliamentary group rejected her nomination and voted instead to name Justice Minister Klaus Kinkel to head the Foreign Ministry.

Ahead of the German presidential election in 1994, Genscher proclaimed his lack of interest in the position, but was nonetheless widely considered a leading contender. After a poll taken for "Stern" magazine showed him to be the favored candidate of 48 percent of German voters, he reiterated in 1993 that he would "in no case" accept the presidency.

Having finished his political career, Genscher remained active as a lawyer and in international organizations. In late 1992, Genscher was appointed chairman of a newly established donors' board of the Berlin State Opera. Between 1997 and 2010, Genscher was affiliated with the law firm Büsing, Müffelmann & Theye. He founded his own consulting firm, Hans-Dietrich Genscher Consult GmbH, in 2000. Between 2001 and 2003, he served as president of the German Council on Foreign Relations. In 2001, Genscher headed an arbitration that ended a monthlong battle between German airline Lufthansa and its pilots' union and resulted in an agreement on increasing wages by more than 15 percent by the end of the following year.

In 2008, Genscher joined former Czech President Václav Havel, former United States Ambassador to Germany John Kornblum and several other well-known political figures in calling for a Cold War museum to be built at Checkpoint Charlie in Berlin. In 2009 Genscher expressed public concern at Pope Benedict XVI's lifting of excommunication of the bishops of the Society of Saint Pius X. Genscher wrote in the "Mitteldeutsche Zeitung": "Poles can be proud of Pope John Paul II. At the last papal election, we said We are the pope! But please—not like this." He argued that Pope Benedict XVI was making a habit of offending non-Catholics. "This is a deep moral and political question. It is about respect for the victims of crimes against humanity", Genscher said.

On 20 December 2013, it was revealed that Genscher played a key role in coordinating the release and flight to Germany of Mikhail Khodorkovsky, the former head of Yukos. Genscher had first met Khodorkovsky in 2002 and had chaired a conference at which Khodorkovsky blasted Russian President Vladimir Putin's pursuit of his oil company. Khodorkovsky asked his lawyers during a 2011 prison visit to let Genscher help mediate early release. Once Putin was re-elected in 2012, German Chancellor Angela Merkel instructed her officials to lobby for the president to meet Genscher. The subsequent negotiations involved two meetings between Genscher and Putin — one at Berlin Tegel Airport at the end of Putin's first visit to Germany after he was re-elected in 2012, the other in Moscow. While keeping the chancellor informed, Khodorkovsky's attorneys and Genscher spent the ensuing months developing a variety of legal avenues that could allow Putin to release his former rival early, ranging from amendments to existing laws to clemency. When Khodorkovsky's mother was in a Berlin hospital with cancer in November 2013, Genscher passed a message to Khodorkovsky suggesting the prisoner should write a pardon letter to Putin emphasizing his mother's ill health. Following Putin's pardoning of Khodorkovsky "for humanitarian reasons" in December 2013, a private plane provided by Genscher brought Khodorkovsky to Berlin for a family reunion at the Hotel Adlon.

Genscher signed on in 2014 to be a member of the Southern Corridor Advisory Panel, a BP-led consortium which includes former British Prime Minister Tony Blair and Peter Sutherland, chairman of Goldman Sachs International. The panel's purpose is to facilitate the expansion of a vast natural-gas field in the Caspian Sea and the building of two pipelines across Europe. The $45 billion enterprise, championed by the Azerbaijani president, Ilham Aliyev, has been called by critics "the Blair Rich Project."

Genscher died at his home outside Bonn in Wachtberg on 31 March 2016 from heart failure, one week and three days after his 89th birthday.


Genscher has been awarded honorary citizenship by his birthplace Halle (Saale) (in 1991) and the city of Berlin (in 1993).







 


</doc>
<doc id="13675" url="https://en.wikipedia.org/wiki?curid=13675" title="Henry Ainsworth">
Henry Ainsworth

Henry Ainsworth (1571–1622) was an English Nonconformist clergyman and scholar.

He was born of a farming family of Swanton Morley, Norfolk. He was educated at St John's College, Cambridge, later moving to Caius College, and, after associating with the Puritan party in the Church, eventually joined the Brownists.

Driven abroad to Holland in about 1593 due to the government of Queen Elizabeth's dissatisfaction with his non-conformist views, he found a home in "a blind lane at Amsterdam", acting as "porter" to a bookseller, who, on discovering his knowledge of Hebrew, introduced him to other scholars. When part of the London church, of which Francis Johnson (then in prison) was pastor, reassembled in Amsterdam, Ainsworth was chosen as their doctor or teacher. In 1596 he drew up a confession of their faith, reissued in Latin in 1598 and dedicated to the various universities of Europe (including St Andrews, Scotland). Johnson joined his flock in 1597, and in 1604 he and Ainsworth composed "An Apology or Defence of such true Christians as are commonly but unjustly called Brownists".

Organizing the church was not easy and dissension was rife. Though often involved in controversy, Ainsworth was not arrogant, but was a steadfast and cultured champion of the principles represented by the early Congregationalists. Amid all the controversy, he steadily pursued his studies. The combination was so unique that some have mistaken him for two different individuals. Confusion has also been occasioned through his friendly controversy with one John Ainsworth, who left the Anglican for the Roman Catholic church.

In 1610 Ainsworth was forced reluctantly to withdraw, with a large part of their church, from Johnson and those who adhered to him. A difference of principle as to the church's right to revise its officers' decisions had been growing between them; Ainsworth taking the more Congregational view. In spirit he remained a man of peace.

He died in 1622 in Amsterdam.

In 1608 Ainsworth answered Richard Bernard's "The Separatist Schisme", but his greatest minor work in this field was his reply to John Smyth (commonly called "the Se-Baptist"), entitled "Defence of Holy Scripture, Worship and Ministry used in the Christian Churches separated from Antichrist, against the Challenges, Cavils and Contradictions of Mr Smyth" (1609).

His scholarly works include his "Annotations"—on "Genesis" (1616); "Exodus" (1617); "Leviticus" (1618); "Numbers" (1619); "Deuteronomy" (1619); "Psalms" (including a metrical version, 1612); and the "Song of Solomon" (1623). These were collected in folio in 1627. From the outset the "Annotations" took a commanding place, especially among continental scholars, establishing a scholarly tradition for English nonconformity.

His publication of Psalms, "The Book of Psalmes: Englished both in Prose and Metre with Annotations" (Amsterdam, 1612), which includes thirty-nine separate monophonic psalm tunes, constituted the Ainsworth Psalter, the only book of music brought to New England in 1620 by the Pilgrim settlers. Although its content was later reworked into the Bay Psalm Book, it had an important influence on the early development of American psalmody.

Ainsworth died in 1622, or early in 1623, for in that year was published his "Seasonable Discourse, or a Censure upon a Dialogue of the Anabaptists", in which the editor speaks of him as a departed worthy.


</doc>
<doc id="13677" url="https://en.wikipedia.org/wiki?curid=13677" title="Hindu">
Hindu

Hindu () refers to any person who regards themselves as culturally, ethnically, or religiously adhering to aspects of Hinduism. It has historically been used as a geographical, cultural, and later religious identifier for people indigenous to the Indian subcontinent.

The historical meaning of the term "Hindu" has evolved with time. Starting with the Persian and Greek references to the land of the Indus in the 1st millennium BCE through the texts of the medieval era, the term Hindu implied a geographic, ethnic or cultural identifier for people living in the Indian subcontinent around or beyond the Sindhu (Indus) river. By the 16th century, the term began to refer to residents of the subcontinent who were not Turkic or Muslims.

The historical development of Hindu self-identity within the local South Asian population, in a religious or cultural sense, is unclear. Competing theories state that Hindu identity developed in the British colonial era, or that it developed post-8th century CE after the Islamic invasion and medieval Hindu-Muslim wars. A sense of Hindu identity and the term "Hindu" appears in some texts dated between the 13th and 18th century in Sanskrit and regional languages. The 14th- and 18th-century Indian poets such as Vidyapati, Kabir and Eknath used the phrase "Hindu dharma" (Hinduism) and contrasted it with "Turaka dharma" (Islam). The Christian friar Sebastiao Manrique used the term 'Hindu' in religious context in 1649. In the 18th century, the European merchants and colonists began to refer to the followers of Indian religions collectively as "Hindus", in contrast to "Mohamedans" for Mughals and Arabs following Islam. By the mid-19th century, colonial orientalist texts further distinguished Hindus from Buddhists, Sikhs and Jains, but the colonial laws continued to consider all of them to be within the scope of the term "Hindu" until about mid-20th century. Scholars state that the custom of distinguishing between Hindus, Buddhists, Jains and Sikhs is a modern phenomenon. --> Hindoo is an archaic spelling variant, whose use today may be considered derogatory.
At more than 1.03 billion, Hindus are the world's third largest group after Christians and Muslims. The vast majority of Hindus, approximately 966 million, live in India, according to India's 2011 census. After India, the next 9 countries with the largest Hindu populations are, in decreasing order: Nepal, Bangladesh, Indonesia, Pakistan, Sri Lanka, United States, Malaysia, United Kingdom and Myanmar. These together accounted for 99% of the world's Hindu population, and the remaining nations of the world together had about 6 million Hindus in 2010.

The word "Hindu" is derived from the Indo-Aryan and Sanskrit word "Sindhu", which means "a large body of water", covering "river, ocean". It was used as the name of the Indus river and also referred to its tributaries. The actual term " first occurs, states Gavin Flood, as "a Persian geographical term for the people who lived beyond the river Indus (Sanskrit: "Sindhu")", more specifically in the 6th-century BCE inscription of Darius I. The Punjab region, called Sapta Sindhava in the Vedas, is called "Hapta Hindu" in Zend Avesta. The 6th-century BCE inscription of Darius I mentions the province of "Hi[n]dush", referring to northwestern India. The people of India were referred to as "Hinduvān" (Hindus) and "hindavī" was used as the adjective for Indian in the 8th century text "Chachnama". The term 'Hindu' in these ancient records is an ethno-geographical term and did not refer to a religion. The Arabic equivalent "Al-Hind" likewise referred to the country of India.

Among the earliest known records of 'Hindu' with connotations of religion may be in the 7th-century CE Chinese text "Record of the Western Regions" by the Buddhist scholar Xuanzang. Xuanzang uses the transliterated term "In-tu" whose "connotation overflows in the religious" according to Arvind Sharma. While Xuanzang suggested that the term refers to the country named after the moon, another Buddhist scholar I-tsing contradicted the conclusion saying that "In-tu" was not a common name for the country.

Al-Biruni's 11th-century text "Tarikh Al-Hind", and the texts of the Delhi Sultanate period use the term 'Hindu', where it includes all non-Islamic people such as Buddhists, and retains the ambiguity of being "a region or a religion". The 'Hindu' community occurs as the amorphous 'Other' of the Muslim community in the court chronicles, according to Romila Thapar. Wilfred Cantwell Smith notes that 'Hindu' retained its geographical reference initially: 'Indian', 'indigenous, local', virtually 'native'. Slowly, the Indian groups themselves started using the term, differentiating themselves and their "traditional ways" from those of the invaders.
The text "Prithviraj Raso", by Chanda Baradai, about the 1192 CE defeat of Prithviraj Chauhan at the hands of Muhammad Ghori, is full of references to "Hindus" and "Turks", and at one stage, says "both the religions have drawn their curved swords;" however, the date of this text is unclear and considered by most scholars to be more recent. In Islamic literature, 'Abd al-Malik Isami's Persian work, "Futuhu's-salatin", composed in the Deccan in 1350, uses the word " to mean Indian in the ethno-geographical sense and the word " to mean 'Hindu' in the sense of a follower of the Hindu religion". The poet Vidyapati's poem "Kirtilata" contrasts the cultures of Hindus and Turks (Muslims) in a city and concludes "The Hindus and the Turks live close together; Each makes fun of the other's religion ("dhamme")." One of the earliest uses of word 'Hindu' in religious context in a European language (Spanish), was the publication in 1649 by Sebastiao Manrique.

Other prominent mentions of 'Hindu' include the epigraphical inscriptions from Andhra Pradesh kingdoms who battled military expansion of Muslim dynasties in the 14th century, where the word 'Hindu' partly implies a religious identity in contrast to 'Turks' or Islamic religious identity. The term "Hindu" was later used occasionally in some Sanskrit texts such as the later Rajataranginis of Kashmir (Hinduka, c. 1450) and some 16th- to 18th-century Bengali Gaudiya Vaishnava texts, including "Chaitanya Charitamrita" and "Chaitanya Bhagavata". These texts used it to contrast Hindus from Muslims who are called Yavanas (foreigners) or Mlecchas (barbarians), with the 16th-century "Chaitanya Charitamrita" text and the 17th-century "Bhakta Mala" text using the phrase "Hindu dharma".

One of the earliest but ambiguous uses of the word Hindu is, states Arvind Sharma, in the 'Brahmanabad settlement' which Muhammad ibn Qasim made with non-Muslims after the Arab invasion of northwestern Sindh region of India, in 712 CE. The term 'Hindu' meant people who were non-Muslims, and it included Buddhists of the region. In the 11th-century text of Al Biruni, Hindus are referred to as "religious antagonists" to Islam, as those who believe in rebirth, presents them to hold a diversity of beliefs, and seems to oscillate between Hindus holding a centralist and pluralist religious views. In the texts of Delhi Sultanate era, states Sharma, the term Hindu remains ambiguous on whether it means people of a region or religion, giving the example of Ibn Battuta's explanation of the name "Hindu Kush" for a mountain range in Afghanistan. It was so called, wrote Ibn Battuta, because many Indian slaves died there of snow cold, as they were marched across that mountain range. The term "Hindu" there is ambivalent and could mean geographical region or religion.

The term Hindu appears in the texts from the Mughal Empire era. It broadly refers to non-Muslims. Pashaura Singh states, "in Persian writings, Sikhs were regarded as Hindu in the sense of non-Muslim Indians". Jahangir, for example, called the Sikh Guru Arjan a Hindu:

During the colonial era, the term Hindu had connotations of native religions of India, that is religions other than Christianity and Islam. In early colonial era Anglo-Hindu laws and British India court system, the term Hindu referred to people of all Indian religions and two non-Indian religions:

The 20th-century colonial laws of British India segregated people's rights by their religion, evolving to provide Muslims with Sharia law, Christians, Jews and Parsis of British India with their own religious laws. The British government created a compendium of religious laws for Hindus, and the term 'Hindu' in these colonial 'Hindu laws', decades before India's independence, applied to Buddhists, Jains and Sikhs.

Beyond the stipulations of British law, colonial orientalists and particularly the influential Asiatick Researches founded in the 18th century, later called The Asiatic Society, initially identified just two religions in India – Islam, and Hinduism. These orientalists included all Indian religions such as Buddhism as a subgroup of Hinduism in the 18th century. These texts called followers of Islam as "Mohamedans", and all others as "Hindus". The text, by the early 19th century, began dividing Hindus into separate groups, for chronology studies of the various beliefs. Among the earliest terms to emerge were "Seeks and their College" (later spelled Sikhs by Charles Wilkins), "Boudhism" (later spelled Buddhism), and in the 9th volume of Asiatick Researches report on religions in India, the term "Jainism" received notice.

According to Pennington, the terms Hindu and Hinduism were thus constructed for colonial studies of India. The various sub-divisions and separation of subgroup terms were assumed to be result of "communal conflict", and Hindu was constructed by these orientalists to imply people who adhered to "ancient default oppressive religious substratum of India", states Pennington. Followers of other Indian religions so identified were later referred Buddhists, Sikhs or Jains and distinguished from Hindus, in an antagonistic two-dimensional manner, with Hindus and Hinduism stereotyped as irrational traditional and others as rational reform religions. However, these mid-19th-century reports offered no indication of doctrinal or ritual differences between Hindu and Buddhist, or other newly constructed religious identities. These colonial studies, states Pennigton, "puzzled endlessly about the Hindus and intensely scrutinized them, but did not interrogate and avoided reporting the practices and religion of Mughal and Arabs in South Asia", and often relied on Muslim scholars to characterise Hindus.

In contemporary era, the term Hindus are individuals who identify with one or more aspects of Hinduism, whether they are practising or non-practicing or "Laissez-faire". The term does not include those who identify with other Indian religions such as Buddhism, Jainism, Sikhism or various animist tribal religions found in India such as "Sarnaism". The term Hindu, in contemporary parlance, includes people who accept themselves as culturally or ethnically Hindu rather than with a fixed set of religious beliefs within Hinduism. One need not be religious in the minimal sense, states Julius Lipner, to be accepted as Hindu by Hindus, or to describe oneself as Hindu.

Hindus subscribe to a diversity of ideas on spirituality and traditions, but have no ecclesiastical order, no unquestionable religious authorities, no governing body, nor a single foudning prophet; Hindus can choose to be polytheistic, pantheistic, monotheistic, monistic, agnostic, atheistic or humanist. Because of the wide range of traditions and ideas covered by the term Hinduism, arriving at a comprehensive definition is difficult. The religion "defies our desire to define and categorize it". A Hindu may, by his or her choice, draw upon ideas of other Indian or non-Indian religious thought as a resource, follow or evolve his or her personal beliefs, and still identify as a Hindu.

In 1995, Chief Justice P. B. Gajendragadkar was quoted in an Indian Supreme Court ruling:

Although Hinduism contains a broad range of philosophies, Hindus share philosophical concepts, such as but not limiting to dharma, karma, kama, artha, moksha and samsara, even if each subscribes to a diversity of views. Hindus also have shared texts such as the Vedas with embedded Upanishads, and common ritual grammar (Sanskara (rite of passage)) such as rituals during a wedding or when a baby is born or cremation rituals. Some Hindus go on pilgrimage to shared sites they consider spiritually significant, practice one or more forms of bhakti or puja, celebrate mythology and epics, major festivals, love and respect for guru and family, and other cultural traditions. A Hindu could:

In the Constitution of India, the word "Hindu" has been used in some places to denote persons professing any of these religions: Hinduism, Jainism, Buddhism or Sikhism. This however has been challenged by the Sikhs and by neo-Buddhists who were formerly Hindus. According to Sheen and Boyle, Jains have not objected to being covered by personal laws termed under 'Hindu', but Indian courts have acknowledged that Jainism is a distinct religion.

The Republic of India is in the peculiar situation that the Supreme Court of India has repeatedly been called upon to define "Hinduism" because the Constitution of India, while it prohibits "discrimination of any citizen" on grounds of religion in article 15, article 30 foresees special rights for "All minorities, whether based on religion or language". As a consequence, religious groups have an interest in being recognised as distinct from the Hindu majority in order to qualify as a "religious minority". Thus, the Supreme Court was forced to consider the question whether Jainism is part of Hinduism in 2005 and 2006.

Starting after the 10th century and particularly after the 12th century Islamic invasion, states Sheldon Pollock, the political response fused with the Indic religious culture and doctrines. Temples dedicated to deity Rama were built from north to south India, and textual records as well as hagiographic inscriptions began comparing the Hindu epic of Ramayana to regional kings and their response to Islamic attacks. The Yadava king of Devagiri named "Ramacandra", for example states Pollock, is described in a 13th-century record as, "How is this Rama to be described.. who freed Varanasi from the "mleccha" (barbarian, Turk Muslim) horde, and built there a golden temple of Sarngadhara". Pollock notes that the Yadava king "Ramacandra" is described as a devotee of deity Shiva (Shaivism), yet his political achievements and temple construction sponsorship in Varanasi, far from his kingdom's location in the Deccan region, is described in the historical records in Vaishnavism terms of Rama, a deity Vishnu avatar. Pollock presents many such examples and suggests an emerging Hindu political identity that was grounded in the Hindu religious text of Ramayana, one that has continued into the modern times, and suggests that this historic process began with the arrival of Islam in India.

Brajadulal Chattopadhyaya has questioned the Pollock theory and presented textual and inscriptional evidence. According to Chattopadhyaya, the Hindu identity and religious response to Islamic invasion and wars developed in different kingdoms, such as wars between Islamic Sultanates and the Vijayanagara kingdom (Karnataka), and Islamic raids on the kingdoms in Tamil Nadu. These wars were described not just using the mythical story of Rama from Ramayana, states Chattopadhyaya, the medieval records used a wide range of religious symbolism and myths that are now considered as part of Hindu literature. This emergence of religious with political terminology began with the first Muslim invasion of Sindh in the 8th century CE, and intensified 13th century onwards. The 14th-century Sanskrit text, "Madhuravijayam", a memoir written by "Gangadevi", the wife of Vijayanagara prince, for example describes the consequences of war using religious terms,

The historiographic writings in Telugu language from the 13th- and 14th-century Kakatiya dynasty period presents a similar "alien other (Turk)" and "self-identity (Hindu)" contrast. Chattopadhyaya, and other scholars, state that the military and political campaign during the medieval era wars in Deccan peninsula of India, and in the north India, were no longer a quest for sovereignty, they embodied a political and religious animosity against the "otherness of Islam", and this began the historical process of Hindu identity formation.

Andrew Nicholson, in his review of scholarship on Hindu identity history, states that the vernacular literature of Bhakti movement sants from 15th to 17th century, such as Kabir, Anantadas, Eknath, Vidyapati, suggests that distinct religious identities, between Hindus and Turks (Muslims), had formed during these centuries. The poetry of this period contrasts Hindu and Islamic identities, states Nicholson, and the literature vilifies the Muslims coupled with a "distinct sense of a Hindu religious identity".

Scholars state that Hindu, Buddhist and Jain identities are retrospectively-introduced modern constructions. Inscriptional evidence from the 8th century onwards, in regions such as South India, suggests that medieval era India, at both elite and folk religious practices level, likely had a "shared religious culture", and their collective identities were "multiple, layered and fuzzy". Even among Hinduism denominations such as Shaivism and Vaishnavism, the Hindu identities, states Leslie Orr, lacked "firm definitions and clear boundaries".

Overlaps in Jain-Hindu identities have included Jains worshipping Hindu deities, intermarriages between Jains and Hindus, and medieval era Jain temples featuring Hindu religious icons and sculpture. Beyond India, on Java island of Indonesia, historical records attest to marriages between Hindus and Buddhists, medieval era temple architecture and sculptures that simultaneously incorporate Hindu and Buddhist themes, where Hinduism and Buddhism merged and functioned as "two separate paths within one overall system", according to Ann Kenney and other scholars. Similarly, there is an organic relation of Sikhs to Hindus, states Zaehner, both in religious thought and their communities, and virtually all Sikhs' ancestors were Hindus. Marriages between Sikhs and Hindus, particularly among "Khatris", were frequent. Some Hindu families brought up a son as a Sikh, and some Hindus view Sikhism as a tradition within Hinduism, even though the Sikh faith is a distinct religion.

Julius Lipner states that the custom of distinguishing between Hindus, Buddhists, Jains, and Sikhs is a modern phenomena, but one that is a convenient abstraction. Distinguishing Indian traditions is a fairly recent practice, states Lipner, and is the result of "not only Western preconceptions about the nature of religion in general and of religion in India in particular, but also with the political awareness that has arisen in India" in its people and a result of Western influence during its colonial history.

Scholars such as Fleming and Eck state that the post-Epic era literature from the 1st millennium CE amply demonstrate that there was a historic concept of the Indian subcontinent as a sacred geography, where the sacredness was a shared set of religious ideas. For example, the twelve "Jyotirlingas" of Shaivism and fifty-one "Shaktipithas" of Shaktism are described in the early medieval era Puranas as pilgrimage sites around a theme. This sacred geography and Shaiva temples with same iconography, shared themes, motifs and embedded legends are found across India, from the Himalayas to hills of South India, from Ellora Caves to Varanasi by about the middle of 1st millennium. Shakti temples, dated to a few centuries later, are verifiable across the subcontinent. Varanasi as a sacred pilgrimage site is documented in the "Varanasimahatmya" text embedded inside the "Skanda Purana", and the oldest versions of this text are dated to 6th to 8th-century CE.

The idea of twelve sacred sites in Shiva Hindu tradition spread across the Indian subcontinent appears not only in the medieval era temples but also in copper plate inscriptions and temple seals discovered in different sites. According to Bhardwaj, non-Hindu texts such as the memoirs of Chinese Buddhist and Persian Muslim travellers attest to the existence and significance of the pilgrimage to sacred geography among Hindus by later 1st millennium CE.

According to Fleming, those who question whether the term Hindu and Hinduism are a modern construction in a religious context present their arguments based on some texts that have survived into the modern era, either of Islamic courts or of literature published by Western missionaries or colonial-era Indologists aiming for a reasonable construction of history. However, the existence of non-textual evidence such as cave temples separated by thousands of kilometers, as well as lists of medieval era pilgrimage sites, is evidence of a shared sacred geography and existence of a community that was self-aware of shared religious premises and landscape. Further, it is a norm in evolving cultures that there is a gap between the "lived and historical realities" of a religious tradition and the emergence of related "textual authorities". The tradition and temples likely existed well before the medieval era Hindu manuscripts appeared that describe them and the sacred geography. This, states Fleming, is apparent given the sophistication of the architecture and the sacred sites along with the variance in the versions of the Puranic literature. According to Diana L. Eck and other Indologists such as André Wink, Muslim invaders were aware of Hindu sacred geography such as Mathura, Ujjain, and Varanasi by the 11th-century. These sites became a target of their serial attacks in the centuries that followed.

The Hindus have been persecuted during the medieval and modern era. The medieval persecution included waves of plunder, killing, destruction of temples and enslavement by Turk-Mongol Muslim armies from central Asia. This is documented in Islamic literature such as those relating to 8th century Muhammad bin-Qasim, 11th century Mahmud of Ghazni, the Persian traveler Al Biruni, the 14th century Islamic army invasion led by Timur, and various Sunni Islamic rulers of the Delhi Sultanate and Mughal Empire. There were occasional exceptions such as Akbar who stopped the persecution of Hindus, and occasional severe persecution such as under Aurangzeb, who destroyed temples, forcibly converted non-Muslims to Islam and banned the celebration of Hindu festivals such as Holi and Diwali.

Other recorded persecution of Hindus include those under the reign of 18th century Tipu Sultan in south India, and during the colonial era. In the modern era, religious persecution of Hindus have been reported outside India.

Christophe Jaffrelot states that modern Hindu nationalism was born in Maharashtra, in the 1920s, as a reaction to the Islamic Khilafat Movement wherein Indian Muslims championed and took the cause of the Turkish Ottoman sultan as the Caliph of all Muslims, at the end of the World War I. Hindus viewed this development as one of divided loyalties of Indian Muslim population, of pan-Islamic hegemony, and questioned whether Indian Muslims were a part of an inclusive anti-colonial Indian nationalism. The Hindu nationalism ideology that emerged, states Jeffrelot, was codified by Savarkar while he was a political prisoner of the British colonial empire.

Chris Bayly traces the roots of Hindu nationalism to the Hindu identity and political independence achieved by the Maratha confederacy, that overthrew the Islamic Mughal empire in large parts of India, allowing Hindus the freedom to pursue any of their diverse religious beliefs and restored Hindu holy places such as Varanasi. A few scholars view Hindu mobilisation and consequent nationalism to have emerged in the 19th century as a response to British colonialism by Indian nationalists and neo-Hinduism gurus. Jaffrelot states that the efforts of Christian missionaries and Islamic proselytizers, during the British colonial era, each of whom tried to gain new converts to their own religion, by stereotyping and stigmatising Hindus to an identity of being inferior and superstitious, contributed to Hindus re-asserting their spiritual heritage and counter cross examining Islam and Christianity, forming organisations such as the "Hindu Sabhas" (Hindu associations), and ultimately a Hindu-identity driven nationalism in the 1920s.

The colonial era Hindu revivalism and mobilisation, along with Hindu nationalism, states Peter van der Veer, was primarily a reaction to and competition with Muslim separatism and Muslim nationalism. The successes of each side fed the fears of the other, leading to the growth of Hindu nationalism and Muslim nationalism in the Indian subcontinent. In the 20th century, the sense of religious nationalism grew in India, states van der Veer, but only Muslim nationalism succeeded with the formation of the West and East Pakistan (later split into Pakistan and Bangladesh), as "an Islamic state" upon independence. Religious riots and social trauma followed as millions of Hindus, Jains, Buddhists and Sikhs moved out of the newly created Islamic states and resettled into the Hindu-majority post-British India. After the separation of India and Pakistan in 1947, the Hindu nationalism movement developed the concept of Hindutva in second half of the 20th century.

The Hindu nationalism movement has sought to reform Indian laws, that critics say attempts to impose Hindu values on India's Islamic minority. Gerald Larson states, for example, that Hindu nationalists have sought a uniform civil code, where all citizens are subject to the same laws, everyone has equal civil rights, and individual rights do not depend on the individual's religion. In contrast, opponents of Hindu nationalists remark that eliminating religious law from India poses a threat to the cultural identity and religious rights of Muslims, and people of Islamic faith have a constitutional right to Islamic shariah-based personal laws. A specific law, contentious between Hindu nationalists and their opponents in India, relates to the legal age of marriage for girls. Hindu nationalists seek that the legal age for marriage be eighteen that is universally applied to all girls regardless of their religion and that marriages be registered with local government to verify the age of marriage. Muslim clerics consider this proposal as unacceptable because under the shariah-derived personal law, a Muslim girl can be married at any age after she reaches puberty.

Hindu nationalism in India, states Katharine Adeney, is a controversial political subject, with no consensus about what it means or implies in terms of the form of government and religious rights of the minorities.

According to Pew Research, there are over 1 billion Hindus worldwide (15% of world's population). Along with Christians (31.5%), Muslims (23.2%) and Buddhists (7.1%), Hindus are one of the four major religious groups of the world.

Most Hindus are found in Asian countries. The countries with most Hindu residents and citizens include (in decreasing order) are India, Nepal, Bangladesh, Indonesia, Pakistan, Sri Lanka, United States, Malaysia, United Kingdom, Myanmar, Canada, Mauritius, Guyana, South Africa, Trinidad and Tobago, Fiji, Suriname.

The fertility rate, that is children per woman, for Hindus is 2.4, which is less than the world average of 2.5. Pew Research projects that there will be 1.161 billion Hindus by 2020.

In more ancient times, Hindu kingdoms arose and spread the religion and traditions across Southeast Asia, particularly Thailand, Nepal, Burma, Malaysia, Indonesia, Cambodia, Laos, Philippines, and what is now central Vietnam.

Over 3 million Hindus are found in Bali Indonesia, a culture whose origins trace back to ideas brought by Tamil Hindu traders to Indonesian islands in the 1st millennium CE. Their sacred texts are also the Vedas and the Upanishads. The Puranas and the Itihasa (mainly "Ramayana" and the "Mahabharata") are enduring traditions among Indonesian Hindus, expressed in community dances and shadow puppet ("wayang") performances. As in India, Indonesian Hindus recognises four paths of spirituality, calling it "Catur Marga". Similarly, like Hindus in India, Balinese Hindu believe that there are four proper goals of human life, calling it "Catur Purusartha" - dharma (pursuit of moral and ethical living), artha (pursuit of wealth and creative activity), kama (pursuit of joy and love) and moksha (pursuit of self-knowledge and liberation).




</doc>
<doc id="13678" url="https://en.wikipedia.org/wiki?curid=13678" title="Hernando de Alarcón">
Hernando de Alarcón

Hernando de Alarcón (born 1500) was a Spanish explorer and navigator of the 16th century, noted for having led an early expedition to the Baja California Peninsula, during which he became one of the first Europeans to ascend the Colorado River from its mouth and perhaps the first to reach Alta California.

Little is known about Alarcón's life outside of his exploits in New Spain. He was probably born in the town of Trujillo, in present-day Extremadura, Spain, in the first years of the 16th century and traveled to the Spanish colonies in the Americas as a young man.

By 1540, Mexico had been conquered and state-sponsored expeditions were being sent north in search of new wealth and the existence of a water passage between the Atlantic and Pacific oceans. Viceroy of New Spain Antonio de Mendoza commissioned Francisco Vázquez de Coronado to undertake a massive overland expedition with the purpose of finding the Seven Cities of Cibola, which were rumored to exist in the unexplored northern interior. The expedition was to be resupplied with stores and provisions delivered by ships traveling up the Sea of Cortés, the commander of which would be Alarcón.

Alarcón set sail from Acapulco with two ships, the "San Pedro" and the "Santa Catalina", on May 9, 1540, and was later joined by the "San Gabriel" at St. Jago de Buena Esperanza, in Colima. His orders from Mendoza were to await the arrival of Coronado's land expedition at a certain latitude along the coast. The meeting with Coronado was never effected, though Alarcón reached the appointed place and left letters, which were soon afterwards discovered by Melchior Diaz, another explorer.

Alarcón eventually sailed to the northern terminus of the Gulf of California and completed the explorations begun by Francisco de Ulloa the preceding year. During this voyage Alarcón proved to his satisfaction that no open-water passage existed between the Gulf of California and the South Sea. Subsequently, on September 26, he entered the mouth of the Colorado River, which he named the "Buena Guia". He was the first European to ascend the river for a distance considerable enough to make important observations. On a second voyage, he probably proceeded past the present-day site of Yuma, Arizona. A map drawn by one of Alarcón's pilots is the earliest accurately detailed representation of the Gulf of California and the lower course of the Colorado River.

Alarcón is almost unique among 16th-century "conquistadores" in that he reportedly treated the Indians he met humanely, as opposed to the often reckless and cruel behavior known from accounts of his contemporaries. Bernard de Voto, in his 1953 "Westward the Course of Empire", observed: "The Indians had an experience they were never to repeat: they were sorry to see these white men leave." Alarcón wrote of his contact with the Yuma-speaking Indians along the Colorado. The information he compiled consisted of their practices in warfare, religion, curing and even sexual customs.

California Historical Landmark No. 568, on the west bank of the Colorado River near Andrade in Imperial County, California, commemorates Alarcón's expedition having been the first non-Indians to sight land within the present-day state of California.




</doc>
<doc id="13679" url="https://en.wikipedia.org/wiki?curid=13679" title="Hakka cuisine">
Hakka cuisine

Hakka cuisine, or Kuh-chia cuisine, is the cooking style of the Hakka people, who may also be found in other parts of Taiwan and in countries with significant overseas Hakka communities. There are numerous restaurants in Taiwan, Hong Kong, Indonesia, Malaysia, Singapore and Thailand serving Hakka cuisine. Hakka cuisine was listed in 2014 on the first Hong Kong Inventory of Intangible Cultural Heritage.

The Hakka people have a marked cuisine and style of Chinese cooking which is little known outside the Hakka home. It concentrates on the texture of food – the hallmark of Hakka cuisine. Whereas preserved meats feature in Hakka delicacy, stewed, braised, roast meats – 'texturised' contributions to the Hakka palate – have a central place in their repertoire. Preserved vegetables (梅菜) are commonly used for steamed and braised dishes such as steamed minced pork with preserved vegetables and braised pork with salted vegetables. In fact, the raw materials for Hakka food are no different from raw materials for any other type of regional Chinese cuisine where what is cooked depends on what is available in the market. Hakka cuisine may be described as outwardly simple but tasty. The skill in Hakka cuisine lies in the ability to cook meat thoroughly without hardening it, and to naturally bring out the proteinous flavour (umami taste) of meat.

The Hakka who settled in the harbour and port areas of Hong Kong placed great emphasis on seafood cuisine. Hakka cuisine in Hong Kong is less dominated by expensive meats; instead, emphasis is placed on an abundance of vegetables. Pragmatic and simple, Hakka cuisine is garnished lightly with sparse or little flavouring. Modern Hakka cooking in Hong Kong favours offal, an example being deep-fried intestines (). Others include tofu with preservatives, along with their signature dish, salt baked chicken (). Another specialty is the poon choi (). While it may be difficult to prove these were the actual diets of the old Hakka community, it is at present a commonly accepted view. The above dishes and their variations are in fact found and consumed throughout China, including Guangdong Province, and are not particularly unique or confined to the Hakka population.

Besides meat as source of protein, there is a unique vegan dish called lei cha (). It comprises combinations of vegetables and beans. Although not specifically unique for all Hakka people but are definitely famous among the Hakka-Hopo families. This vegetable-based rice tea dish is gaining momentum in some multicultural countries like Malaysia. Cooking of this dish requires the help from other family members to complete all eight combinations. It helps foster the relationship between family members in return.

Steamed bun (茶果) is a popular snack for Hakka people. It is mainly made from glutinous rice and is available in sweet or salty options. Sweet version consists of sweetened black-eyed pea pastes or peanuts. Salty version consists of preserved radish.

Hakka food also includes other traditional Taiwanese dishes, just as other Taiwanese ethnic groups do. Some of the more notable dishes in Hakka cuisine are listed as follow:

In India and other regions with significant Indian populations, the locally known "Hakka cuisine" is actually an Indian adaptation of original Hakka dishes. This variation of Hakka cuisine is in reality, mostly Indian Chinese cuisine. It is called "Hakka cuisine" because in India, many owners of restaurants who serve this cuisine are of Hakka origin. Typical dishes include 'chilli chicken' and 'Dongbei (northeastern) chow mein' (an Indianised version of real Northeastern Chinese cuisine), and these restaurants also serve traditional Indian dishes such as pakora. Being very popular in these areas, this style of cuisine is often mistakenly credited as being representative of Hakka cuisine in general, whereas the authentic style of Hakka cuisine is rarely known in these regions.

In Thailand, Bangkok's Chinatown is Yaowarat and including neighboring areas such as Sampheng, Charoen Chai, Charoen Krung, Suan Mali, Phlapphla Chai or Wong Wian Yi Sip Song Karakadakhom (July 22nd Circle). In the past, many Hakka restaurants are located in the Suan Mali near Bangkok Metropolitan Administration General Hospital. But now they had moved into many places, such as Talad Phlu, which is also one of the Chinatown as well.




</doc>
<doc id="13680" url="https://en.wikipedia.org/wiki?curid=13680" title="Hunan cuisine">
Hunan cuisine

Hunan cuisine, also known as Xiang cuisine, consists of the cuisines of the Xiang River region, Dongting Lake and western Hunan Province in China. It is one of the Eight Great Traditions of Chinese cuisine and is well known for its hot and spicy flavours, fresh aroma and deep colours. Common cooking techniques include stewing, frying, pot-roasting, braising and smoking. Due to the high agricultural output of the region, ingredients for Hunan dishes are many and varied.

The history of the cooking skills employed in Hunan cuisine dates back to the 17th century. During the course of its history, Hunan cuisine assimilated a variety of local forms, eventually evolving into its own style. Some well-known dishes include fried chicken with Sichuan spicy sauce () and smoked pork with dried long green beans ().

Hunan cuisine consists of three primary styles:

Known for its liberal use of chili peppers, shallots and garlic, Hunan cuisine is known for being "gan la" () or purely hot, as opposed to Sichuan cuisine, to which it is often compared. Sichuan cuisine is known for its distinctive "ma la" () seasoning and other complex flavour combinations, frequently employs Sichuan pepper along with chilies which are often dried. It also utilises more dried or preserved ingredients and condiments. Hunan cuisine, on the other hand, is often spicier by pure chili content and contains a larger variety of fresh ingredients. Both Hunan and Sichuan cuisine are perhaps significantly oilier than the other cuisines in China, but Sichuan dishes are generally oilier than Hunan dishes. Another characteristic distinguishing Hunan cuisine from Sichuan cuisine is that, in general, Hunan cuisine uses smoked and cured goods in its dishes much more frequently.
Another feature of Hunan cuisine is that the menu changes with the seasons. In a hot and humid summer, a meal will usually start with cold dishes or a platter holding a selection of cold meats with chilies for opening the pores and keeping cool in the summer. In winter, a popular choice is the hot pot, thought to heat the blood in the cold months. A special hot pot called "yuanyang huoguo" () is notable for splitting the pot into two sides – a spicy one and a mild one. One of the classic dishes in Hunan cuisine served in restaurants and at home is farmer pepper fried pork. It is made with several common ingredients: pork belly, green pepper, fermented black beans and other spices.

Chilies are an entire class of flavourings in Hunan cuisine.



</doc>
<doc id="13681" url="https://en.wikipedia.org/wiki?curid=13681" title="Hyperinflation">
Hyperinflation

In economics, hyperinflation is very high and typically accelerating inflation. It quickly erodes the real value of the currency, as the prices of most or all goods increase. This causes people to minimize their holdings in that currency as they usually switch to more stable foreign currencies. Prices typically remain stable in terms of other currencies.

Unlike low inflation, where the process of rising prices is protracted and not generally noticeable except by studying past market prices, hyperinflation sees a rapid and continuing increase in nominal prices, the nominal cost of goods, and in the supply of money.
Typically, however, the general price level rises even more rapidly than the money supply as people try ridding themselves of the devaluing currency as quickly as possible. As this happens, the real stock of money (i.e., the amount of circulating money divided by the price level) decreases considerably.

Economists believe that hyperinflations are caused by large persistent government deficits financed primarily by money creation (rather than by borrowing or by increasing taxation). As such, hyperinflation is often associated with some stress to the government budget, such as wars or their aftermath, sociopolitical upheavals, a collapse in export prices, or other crises that make it difficult for the government to collect tax revenue. A sharp decrease in real tax revenue coupled with a strong need to maintain government spending, together with an inability or unwillingness to borrow, can lead a country into hyperinflation.

In 1956, Phillip Cagan wrote "The Monetary Dynamics of Hyperinflation", the book often regarded as the first serious study of hyperinflation and its effects (though "The Economics of Inflation" by C. Bresciani-Turroni on the German hyperinflation was published in Italian in 1931). In his book, Cagan defined a hyperinflationary episode as starting in the month that the monthly inflation rate exceeds 50%, and as ending when the monthly inflation rate drops below 50% and stays that way for at least a year. Economists usually follow Cagan’s description that hyperinflation occurs when the monthly inflation rate exceeds 50%.

The International Accounting Standards Board has issued guidance on accounting rules in a hyperinflationary environment. It does not establish an absolute rule on when hyperinflation arises. Instead, it lists factors that indicate the existence of hyperinflation:

While there can be a number of causes of high inflation, most hyperinflations have been caused by government budget deficits financed by money creation. Peter Bernholz analysed 29 hyperinflations (following Cagan's definition) and concludes that at least 25 of them have been caused in this way. A necessary condition for hyperinflation is the use of paper money, instead of gold or silver coins. Most hyperinflations in history, with some exceptions, such as the French hyperinflation of 1789-1796, occurred after the use of fiat currency became widespread in the late 19th century. The French hyperinflation took place after the introduction of a non-convertible paper currency, the assignats.

Hyperinflation occurs when there is a continuing (and often accelerating) rapid increase in the amount of money that is not supported by a corresponding growth in the output of goods and services.

The price increases that result from the rapid money creation creates a vicious circle, requiring ever growing amounts of new money creation to fund government deficits. Hence both monetary inflation and price inflation proceed at a rapid pace. Such rapidly increasing prices cause widespread unwillingness of the local population to hold the local currency as it rapidly loses its buying power. Instead they quickly spend any money they receive, which increases the velocity of money flow; this in turn causes further acceleration in prices. This means that the increase in the price level is greater than that of the money supply. The real stock of money, M/P, decreases. Here M refers to the money stock and P to the price level.

This results in an imbalance between the supply and demand for the money (including currency and bank deposits), causing rapid inflation. Very high inflation rates can result in a loss of confidence in the currency, similar to a bank run. Usually, the excessive money supply growth results from the government being either unable or unwilling to fully finance the government budget through taxation or borrowing, and instead it finances the government budget deficit through the printing of money.

Governments have sometimes resorted to excessively loose monetary policy, as it allows a government to devalue its debts and reduce (or avoid) a tax increase. Inflation is effectively a regressive tax on the users of money, but less overt than levied taxes and is therefore harder to understand by ordinary citizens. Inflation can obscure quantitative assessments of the true cost of living, as published price indices only look at data in retrospect, so may increase only months later. Monetary inflation can become hyperinflation if monetary authorities fail to fund increasing government expenses from taxes, government debt, cost cutting, or by other means, because either

Theories of hyperinflation generally look for a relationship between seigniorage and the inflation tax. In both Cagan's model and the neo-classical models, a tipping point occurs when the increase in money supply or the drop in the monetary base makes it impossible for a government to improve its financial position. Thus when fiat money is printed, government obligations that are not denominated in money increase in cost by more than the value of the money created.

From this, it might be wondered why any rational government would engage in actions that cause or continue hyperinflation. One reason for such actions is that often the alternative to hyperinflation is either depression or military defeat. The root cause is a matter of more dispute. In both classical economics and monetarism, it is always the result of the monetary authority irresponsibly borrowing money to pay all its expenses. These models focus on the unrestrained seigniorage of the monetary authority, and the gains from the inflation tax.

In neo-classical economic theory, hyperinflation is rooted in a deterioration of the monetary base, that is the confidence that there is a store of value that the currency will be able to command later. In this model, the perceived risk of holding currency rises dramatically, and sellers demand increasingly high premiums to accept the currency. This in turn leads to a greater fear that the currency will collapse, causing even higher premiums. One example of this is during periods of warfare, civil war, or intense internal conflict of other kinds: governments need to do whatever is necessary to continue fighting, since the alternative is defeat. Expenses cannot be cut significantly since the main outlay is armaments. Further, a civil war may make it difficult to raise taxes or to collect existing taxes. While in peacetime the deficit is financed by selling bonds, during a war it is typically difficult and expensive to borrow, especially if the war is going poorly for the government in question. The banking authorities, whether central or not, "monetize" the deficit, printing money to pay for the government's efforts to survive. The hyperinflation under the Chinese Nationalists from 1939 to 1945 is a classic example of a government printing money to pay civil war costs. By the end, currency was flown in over the Himalayas, and then old currency was flown out to be destroyed.

Hyperinflation is a complex phenomenon and one explanation may not be applicable to all cases. In both of these models, however, whether loss of confidence comes first, or central bank seigniorage, the other phase is ignited. In the case of rapid expansion of the money supply, prices rise rapidly in response to the increased supply of money relative to the supply of goods and services, and in the case of loss of confidence, the monetary authority responds to the risk premiums it has to pay by "running the printing presses."

Nevertheless, the immense acceleration process that occurs during hyperinflation (such as during the German hyperinflation of 1922/23) still remains unclear and unpredictable. The transformation of an inflationary development into the hyperinflation has to be identified as a very complex phenomenon, which could be a further advanced research avenue of the complexity economics in conjunction with research areas like mass hysteria, bandwagon effect, social brain, and mirror neurons.

A number of hyperinflations were caused by some sort of extreme negative supply shock, often but not always associated with wars, the breakdown of the communist system or natural disasters.

Since hyperinflation is visible as a monetary effect, models of hyperinflation center on the demand for money. Economists see both a rapid increase in the money supply and an increase in the velocity of money if the (monetary) inflating is not stopped. Either one, or both of these together are the root causes of inflation and hyperinflation. A dramatic increase in the velocity of money as the cause of hyperinflation is central to the "crisis of confidence" model of hyperinflation, where the risk premium that sellers demand for the paper currency over the nominal value grows rapidly. The second theory is that there is first a radical increase in the amount of circulating medium, which can be called the "monetary model" of hyperinflation. In either model, the second effect then follows from the first—either too little confidence forcing an increase in the money supply, or too much money destroying confidence.

In the "confidence model", some event, or series of events, such as defeats in battle, or a run on stocks of the specie that back a currency, removes the belief that the authority issuing the money will remain solvent—whether a bank or a government. Because people do not want to hold notes that may become valueless, they want to spend them. Sellers, realizing that there is a higher risk for the currency, demand a greater and greater premium over the original value. Under this model, the method of ending hyperinflation is to change the backing of the currency, often by issuing a completely new one. War is one commonly cited cause of crisis of confidence, particularly losing in a war, as occurred during Napoleonic Vienna, and capital flight, sometimes because of "contagion" is another. In this view, the increase in the circulating medium is the result of the government attempting to buy time without coming to terms with the root cause of the lack of confidence itself.

In the "monetary model", hyperinflation is a positive feedback cycle of rapid monetary expansion. It has the same cause as all other inflation: money-issuing bodies, central or otherwise, produce currency to pay spiraling costs, often from lax fiscal policy, or the mounting costs of warfare. When business people perceive that the issuer is committed to a policy of rapid currency expansion, they mark up prices to cover the expected decay in the currency's value. The issuer must then accelerate its expansion to cover these prices, which pushes the currency value down even faster than before. According to this model the issuer cannot "win" and the only solution is to abruptly stop expanding the currency. Unfortunately, the end of expansion can cause a severe financial shock to those using the currency as expectations are suddenly adjusted. This policy, combined with reductions of pensions, wages, and government outlays, formed part of the Washington consensus of the 1990s.

Whatever the cause, hyperinflation involves both the supply and velocity of money. Which comes first is a matter of debate, and there may be no universal story that applies to all cases. But once the hyperinflation is established, the pattern of increasing the money stock, by whichever agencies are allowed to do so, is universal. Because this practice increases the supply of currency without any matching increase in demand for it, the price of the currency, that is the exchange rate, naturally falls relative to other currencies. Inflation becomes hyperinflation when the increase in money supply turns specific areas of pricing power into a general frenzy of spending quickly before money becomes worthless. The purchasing power of the currency drops so rapidly that holding cash for even a day is an unacceptable loss of purchasing power. As a result, no one holds currency, which increases the velocity of money, and worsens the crisis.

Because rapidly rising prices undermine the role of money as a store of value, people try to spend it on real goods or services as quickly as possible. Thus, the monetary model predicts that the velocity of money will increase as a result of an excessive increase in the money supply. At the point when money velocity and prices rapidly accelerate in a vicious circle, hyperinflation is out of control, because ordinary policy mechanisms, such as increasing reserve requirements, raising interest rates, or cutting government spending will be ineffective and be responded to by shifting away from the rapidly devalued money and towards other means of exchange.

During a period of hyperinflation, bank runs, loans for 24-hour periods, switching to alternate currencies, the return to use of gold or silver or even barter become common. Many of the people who hoard gold today expect hyperinflation, and are hedging against it by holding specie. There may also be extensive capital flight or flight to a "hard" currency such as the US dollar. This is sometimes met with capital controls, an idea that has swung from standard, to anathema, and back into semi-respectability. All of this constitutes an economy that is operating in an "abnormal" way, which may lead to decreases in real production. If so, that intensifies the hyperinflation, since it means that the amount of goods in "too much money chasing too few goods" formulation is also reduced. This is also part of the vicious circle of hyperinflation.

Once the vicious circle of hyperinflation has been ignited, dramatic policy means are almost always required. Simply raising interest rates is insufficient. Bolivia, for example, underwent a period of hyperinflation in 1985, where prices increased 12,000% in the space of less than a year. The government raised the price of gasoline, which it had been selling at a huge loss to quiet popular discontent, and the hyperinflation came to a halt almost immediately, since it was able to bring in hard currency by selling its oil abroad. The crisis of confidence ended, and people returned deposits to banks. The German hyperinflation (1919–November 1923) was ended by producing a currency based on assets loaned against by banks, called the Rentenmark. Hyperinflation often ends when a civil conflict ends with one side winning.

Although wage and price controls are sometimes used to control or prevent inflation, no episode of hyperinflation has been ended by the use of price controls alone, because price controls that force merchants to sell at prices far below their restocking costs result in shortages that cause prices to rise still further.

Nobel prize winner Milton Friedman said "We economists don't know much, but we do know how to create a shortage. If you want to create a shortage of tomatoes, for example, just pass a law that retailers can't sell tomatoes for more than two cents per pound. Instantly you'll have a tomato shortage. It's the same with oil or gas."

Hyperinflation effectively wipes out the purchasing power of private and public savings; distorts the economy in favor of the hoarding of real assets; causes the monetary base, whether specie or hard currency, to flee the country; and makes the afflicted area anathema to investment.

One of the most important characteristics of hyperinflation is the accelerating substitution of the inflating money by stable money—gold and silver in former times, then relatively stable foreign currencies after the breakdown of the gold or silver standards (Thiers' Law). If inflation is high enough, government regulations like heavy penalties and fines, often combined with exchange controls, cannot prevent this currency substitution. As a consequence, the inflating currency is usually heavily undervalued compared to stable foreign money in terms of purchasing power parity. So foreigners can live cheaply and buy at low prices in the countries hit by high inflation. It follows that governments that do not succeed in engineering a successful currency reform in time must finally legalize the stable foreign currencies (or, formerly, gold and silver) that threaten to fully substitute the inflating money. Otherwise, their tax revenues, including the inflation tax, will approach zero. The last episode of hyperinflation in which this process could be observed was in Zimbabwe in the first decade of the 21st century. In this case, the local money was mainly driven out by the US dollar and the South African rand.

Enactment of price controls to prevent discounting the value of paper money relative to gold, silver, hard currency, or other commodities fail to force acceptance of a paper money that lacks intrinsic value. If the entity responsible for printing a currency promotes excessive money printing, with other factors contributing a reinforcing effect, hyperinflation usually continues. Hyperinflation is generally associated with paper money, which can easily be used to increase the money supply: add more zeros to the plates and print, or even stamp old notes with new numbers. Historically, there have been numerous episodes of hyperinflation in various countries followed by a return to "hard money". Older economies would revert to hard currency and barter when the circulating medium became excessively devalued, generally following a "run" on the store of value.

Much attention on hyperinflation centers on the effect on savers whose investments become worthless. Interest rate changes often cannot keep up with hyperinflation or even high inflation, certainly with contractually fixed interest rates. For example, in the 1970s in the United Kingdom inflation reached 25% per annum, yet interest rates did not rise above 15%—and then only briefly—and many fixed interest rate loans existed. Contractually, there is often no bar to a debtor clearing his long term debt with "hyperinflated cash", nor could a lender simply somehow suspend the loan. Contractual "early redemption penalties" were (and still are) often based on a penalty of "n" months of interest/payment; again no real bar to paying off what had been a large loan. In interwar Germany, for example, much private and corporate debt was effectively wiped out—certainly for those holding fixed interest rate loans.

Ludwig von Mises used the term "crack-up boom" (German: Katastrophenhausse) to describe the economic consequences of an unmitigated increasing in the base-money supply. As more and more money is provided, interest rates decline towards zero. Realizing that fiat money is losing value, investors will try to place money in assets such as real estate, stocks, even art; as these appear to represent "real" value. Asset prices are thus becoming inflated. This potentially spiraling process will ultimately lead to the collapse of the monetary system. The Cantillon effect says that those institutions that receive the new money first are the beneficiaries of the policy.

Hyperinflation is ended by drastic remedies, such as imposing the shock therapy of slashing government expenditures or altering the currency basis. One form this may take is dollarization, the use of a foreign currency (not necessarily the U.S. dollar) as a national unit of currency. An example was dollarization in Ecuador, initiated in September 2000 in response to a 75% loss of value of the Ecuadorian sucre in early 2000. But usually the "dollarization" takes place in spite of all efforts of the government to prevent it by exchange controls, heavy fines and penalties. The government has thus to try to engineer a successful currency reform stabilizing the value of the money. If it does not succeed with this reform the substitution of the inflating by stable money goes on. Thus it is not surprising that there have been at least seven historical cases in which the good (foreign) money did fully drive out the use of the inflating currency. In the end the government had to legalize the former, for otherwise its revenues would have fallen to zero.

Hyperinflation has always been a traumatic experience for the people who suffer it, and the next political regime almost always enacts policies to try to prevent its recurrence. Often this means making the central bank very aggressive about maintaining price stability, as was the case with the German Bundesbank, or moving to some hard basis of currency, such as a currency board. Many governments have enacted extremely stiff wage and price controls in the wake of hyperinflation, but this does not prevent further inflation of the money supply by the central bank, and always leads to widespread shortages of consumer goods if the controls are rigidly enforced.

In countries experiencing hyperinflation, the central bank often prints money in larger and larger denominations as the smaller denomination notes become worthless. This can result in the production of unusually large demoninations of banknotes, including those denominated in amounts of 1,000,000,000 or more.

One way to avoid the use of large numbers is by declaring a new unit of currency. (As an example, instead of 10,000,000,000 dollars, a central bank might set 1 new dollar = 1,000,000,000 old dollars, so the new note would read "10 new dollars".) A recent example of this is Turkey's revaluation of the Lira on 1 January 2005, when the old Turkish lira (TRL) was converted to the New Turkish lira (TRY) at a rate of 1,000,000 old to 1 new Turkish Lira. While this does not lessen the actual value of a currency, it is called redenomination or revaluation and also occasionally happens in countries with lower inflation rates. During hyperinflation, currency inflation happens so quickly that bills reach large numbers before revaluation.

Some banknotes were stamped to indicate changes of denomination, as it would have taken too long to print new notes. By the time new notes were printed, they would be obsolete (that is, they would be of too low a denomination to be useful).

Metallic coins were rapid casualties of hyperinflation, as the scrap value of metal enormously exceeded its face value. Massive amounts of coinage were melted down, usually illicitly, and exported for hard currency.

Governments will often try to disguise the true rate of inflation through a variety of techniques. None of these actions addresses the root causes of inflation; and if discovered, they tend to further undermine trust in the currency, causing further increases in inflation. Price controls will generally result in shortages and hoarding and extremely high demand for the controlled goods, causing disruptions of supply chains. Products available to consumers may diminish or disappear as businesses no longer find it economic to continue producing and/or distributing such goods at the legal prices, further exacerbating the shortages.

There are also issues with computerized money-handling systems. In Zimbabwe, during the hyperinflation of the Zimbabwe dollar, many automated teller machines and payment card machines struggled with arithmetic overflow errors as customers required many billions and trillions of dollars at one time.

During the Crisis of the Third Century, Rome underwent hyper-inflation caused by years of coinage devaluation.

In 1922, inflation in Austria reached 1,426%, and from 1914 to January 1923, the consumer price index rose by a factor of 11,836, with the highest banknote in denominations of 500,000 Austrian krones. After World War I essentially all State enterprises ran at a loss. The number of state employees in the capital, Vienna, was greater than in the earlier monarchy, even though the new republic was almost eight times smaller.

Observing the Austrian response to developing hyperinflation, which included the hoarding of food and the speculation in foreign currencies, Owen S. Phillpotts, the Commercial Secretary at the British Legation in Vienna wrote: "The Austrians are like men on a ship who cannot manage it, and are continually signalling for help. While waiting, however, most of them begin to cut rafts, each for himself, out of the sides and decks. The ship has not yet sunk despite the leaks so caused, and those who have acquired stores of wood in this way may use them to cook their food, while the more seamanlike look on cold and hungry. The population lack courage and energy as well as patriotism."


As the first user of fiat currency, China was also the first country to experience hyperinflation. Paper currency was introduced during the Tang Dynasty, and was generally welcomed. It maintained its value, as successive Chinese governments put in place strict controls on issuance. The convenience of paper currency for trade purposes led to strong demand for paper currency. It was only when discipline on quantity supplied broke down that hyperinflation emerged. The Yuan Dynasty was the first to print large amounts of fiat paper money to fund their wars, resulting in hyperinflation. Much later, the Republic of China went through hyperinflation from 1948–49. In 1947, the highest denomination bill was 50,000 yuan. By mid-1948, the highest denomination was 180,000,000 yuan. The 1948 currency reform replaced the yuan by the gold yuan at an exchange rate of 1 gold yuan = 3,000,000 yuan. In less than a year, the highest denomination was 10,000,000 gold yuan. In the final days of the civil war, the Silver Yuan was briefly introduced at the rate of 500,000,000 Gold Yuan. Meanwhile, the highest denomination issued by a regional bank was 6,000,000,000 yuan (issued by Xinjiang Provincial Bank in 1949). After the renminbi was instituted by the new communist government, hyperinflation ceased, with a revaluation of 1:10,000 old Renminbi in 1955.


During the French Revolution and first Republic, the National Assembly issued bonds, some backed by seized church property, called assignats. Napoleon replaced them with the franc in 1803, at which time the assignats were basically worthless.
Stephen D. Dillaye pointed out that one of the reasons for the failure was massive counterfeiting of the paper currency, "the Assignats" – largely through London – where, according to Dillaye: "Seventeen manufacturing establishments were in full operation in London, with a force of four hundred men devoted to the production of false and forged Assignats." 


By November 1922, the value in gold of money in circulation had fallen from £300 million before World War I to £20 million. The Reichsbank responded by the unlimited printing of notes, thereby accelerating the devaluation of the mark. In his report to London, Lord D'Abernon wrote: "In the whole course of history, no dog has ever run after its own tail with the speed of the Reichsbank." Germany went through its worst inflation in 1923. In 1922, the highest denomination was 50,000 Marks. By 1923, the highest denomination was 100,000,000,000,000 (10) Marks. In December 1923 the exchange rate was 4,200,000,000,000 (4.2 × 10) Marks to 1 US dollar. In 1923, the rate of inflation hit 3.25 × 10 percent per month (prices double every two days). Beginning on 20 November 1923, 1,000,000,000,000 old Marks were exchanged for 1 Rentenmark, so that 4.2 Rentenmarks were worth 1 US dollar, exactly the same rate the Mark had in 1914.


With the German invasion in April 1941, there was an abrupt increase in prices. This was due to psychological factors related to the fear of shortages and to the hoarding of goods. During the German and Italian Axis occupation of Greece (1941-1944), the agricultural, mineral, industrial etc. production of Greece were used to sustain the occupation forces, but also to secure provisions for the Afrika Korps. One part of these "sales" of provisions was settled with bilateral clearing through the German DEGRIGES and the Italian Sagic companies at very low prices. As the value of Greek exports in drachmas fell, the demand for drachmas followed suit and so did its forex rate.
While shortages started due to naval blockades and hoarding, the prices of commodities soared. The other part of the "purchases" was settled with drachmas secured from the Bank of Greece and printed for this purpose by private printing presses. As prices soared, the Germans and Italians started requesting more and more drachmas from the Bank of Greece to offset price increases; each time prices increased, the note circulation followed suit soon afterwards. For the year November 1943 - November 1944, the inflation rate was 2.5 × 10%, the circulation was 6.28 × 10 drachmae and one gold sovereign cost 43,167 billion drachmas.
The hyperinflation started subsiding immediately after the departure of the German occupation forces, but inflation rates took several years before they fell below 50%.

The Treaty of Trianon and political instability between 1919 and 1924 led to a major inflation of Hungary's currency. In 1921, in an attempt to stop inflation, the national assembly of Hungary passed the Hegedűs reforms, including a 20% levy on bank deposits. This action precipitated a mistrust of banks by the public, especially the peasants, and resulted in a reduction in savings and in the amount of currency in circulation. Unable to tax adequately, the government resorted to printing money, and in 1923 inflation in Hungary reached 98% per month.

Between the end of 1945 and July 1946, Hungary went through the worst inflation ever recorded. In 1944, the highest denomination was 1,000 pengő. By the end of 1945, it was 10,000,000 pengő. The highest denomination in mid-1946 was 100,000,000,000,000,000,000 (10) pengő. A special currency, the adópengő – or tax pengő – was created for tax and postal payments. The value of the adópengő was adjusted each day, by radio announcement. On 1 January 1946 one adópengő equaled one pengő. By late July, one adópengő equaled 2,000,000,000,000,000,000,000 or 2×10 (2 sextillion) pengő. When the pengő was replaced in August 1946 by the forint, the total value of all Hungarian banknotes in circulation amounted to / of one US dollar. This is the most severe known incident of inflation recorded, peaking at 1.3 × 10 percent per month (prices double every 15 hours). The overall impact of hyperinflation: On 18 August 1946, 400,000,000,000,000,000,000,000,000,000 or 4 (four hundred quadrilliard on the long scale used in Hungary; four hundred octillion on short scale) pengő became 1 forint.


North Korea has most likely experienced hyperinflation from December 2009 to mid-January 2011. Based on the price of rice, North Korea's hyperinflation peaked in mid-January 2010, but according to black market exchange-rate data, and calculations based on purchasing power parity, North Korea experienced its peak month of inflation in early March 2010. These data are unofficial, however, and therefore must be treated with a degree of caution.

Poland has gone through two episodes of hyperinflation since the country regained independence following the first world war: the first one in 1923, the second one in 1989-1990. Both of these events resulted in introduction of new currencies. In 1924, the "złoty" replaced the original currency of post-war Poland, the mark. The currency was then replaced by a new one of the same name in 1950, which was assigned the ISO code of PLZ. As a result of the second hyperinflation crisis, the current "new złoty" was introduced in 1990 (ISO code: PLN). See the article on Polish złoty for more information about the currency's history.

The newly independent Poland had been struggling with a large budget deficit since its inception in 1918 but it was in 1923 when inflation reached its peak. The exchange rate to the American dollar went up from 9 Polish marks for a dollar in 1918 to 6,375,000 marks per dollar at the end of 1923. A new personal 'inflation tax' was introduced. The resolution of the crisis is attributed to the person of Władysław Grabski, who became the prime minister of Poland in December 1923. Having nominated an all-new government and been granted extraordinary lawmaking powers by the Sejm for a period of six months, he went on to introduce a new currency, establish a new national bank and scrap the inflation tax, which took place throughout 1924.

The economic crisis in Poland in 1980s was accompanied by rising inflation when new money was printed to cover a budget deficit. Although the inflation was not as acute as in 1920s, it is estimated that its annual rate reached around 600% in a period of over a year spanning parts of 1989 and 1990. The economy was stabilised by the adoption of the Balcerowicz Plan in 1989, named so after the main author of the reforms, the minister of finance Leszek Balcerowicz. The plan was largely inspired by the previous Grabski's reforms.

The Japanese government occupying the Philippines during World War II issued fiat currencies for general circulation. The Japanese-sponsored Second Philippine Republic government led by Jose P. Laurel at the same time outlawed possession of other currencies, most especially "guerilla money." The fiat money's lack of value earned it the derisive nickname "Mickey Mouse money". Survivors of the war often tell tales of bringing suitcase or "bayong" (native bags made of woven coconut or buri leaf strips) overflowing with Japanese-issued bills. In the early times, 75 Mickey Mouse pesos could buy one duck egg. In 1944, a box of matches cost more than 100 Mickey Mouse pesos.

In 1942, the highest denomination available was 10 pesos. Before the end of the war, because of inflation, the Japanese government was forced to issue 100, 500, and 1000 peso notes.


Malaya and Singapore were under Japanese occupation from 1942 until 1945. The Japanese issued banana money as the official currency to replace the Straits currency issued by the British. During that time, the cost of basic necessities increased drastically. As the occupation proceeded, the Japanese authorities printed more money to fund their wartime activities, which resulted in hyperinflation and a severe depreciation in value of the banana note.

From February to December 1942, $100 of Straits currency was worth $100 in Japanese scrip, after which the value of Japanese scrip began to erode, reaching $385 on December 1943 and $1,850 one year later. By 1 August 1945, this had inflated to $10,500, and 11 days later it had reached $95,000. After 13 August 1945, Japanese scrip had become valueless.

A seven-year period of uncontrollable spiralling inflation occurred in the early Soviet Union, running from the earliest days of the Bolshevik Revolution in November 1917 to the reestablishment of the gold standard with the introduction of the chervonets as part of the New Economic Policy. The inflationary crisis effectively ended in March 1924 with the introduction of the so-called "gold ruble" as the country's standard currency.

The early Soviet hyperinflationary period was marked by three successive redenominations of its currency, in which "new rubles" replaced old at the rates of 10,000-to-1 (1 January 1922), 100-to-1 (1 January 1923), and 50,000-to-1 (7 March 1924), respectively.

Between 1921 and 1922, inflation in the Soviet Union reached 213%.

Yugoslavia went through a period of hyperinflation and subsequent currency reforms from 1989–1994. One of several regional conflicts accompanying the dissolution of Yugoslavia was the Bosnian War (1992–1995). The Belgrade government of Slobodan Milošević backed ethnic Serbian secessionist forces in the conflict, resulting in a United Nations boycott of Yugoslavia. The UN boycott collapsed an economy already weakened by regional war, with the projected monthly inflation rate accelerating to one million percent by December 1993 (prices double every 2.3 days).

The highest denomination in 1988 was 50,000 dinars. By 1989 it was 2,000,000 dinars. In the 1990 currency reform, 1 new dinar was exchanged for 10,000 old dinars. In the 1992 currency reform, 1 new dinar was exchanged for 10 old dinars. The highest denomination in 1992 was 50,000 dinars. By 1993, it was 10,000,000,000 dinars. In the 1993 currency reform, 1 new dinar was exchanged for 1,000,000 old dinars. Before the year was over, however, the highest denomination was 500,000,000,000 dinars. In the 1994 currency reform, 1 new dinar was exchanged for 1,000,000,000 old dinars. In another currency reform a month later, 1 novi dinar was exchanged for 13 million dinars (1 novi dinar = 1 German mark at the time of exchange). The overall impact of hyperinflation was that 1 novi dinar was equal to 1 × 10~1.3 × 10 pre-1990 dinars. Yugoslavia's rate of inflation hit 5 × 10 percent cumulative inflation over the time period 1 October 1993 and 24 January 1994. 

Inflation of Venezuela's bolivar fuerte (VEF) in 2014 reached 69% and was the highest in the world. In 2015, inflation was 181%, the highest in the world and the highest in the country's history at that time, 800% in 2016 and over 4,000% in 2017. with Venezuela spiraling into hyperinflation. While the Venezuelan government "has essentially stopped" producing official inflation estimates as of early 2018, one estimation of the rate at that time was 5,220%, according to inflation economist Steve Hanke of Johns Hopkins University.

Inflation has affected Venezuelans so much that in 2017, some people became video game gold farmers and could be seen playing games such as "RuneScape" to sell in-game currency or characters for real currency. In many cases, these gamers made more money than salaried workers in Venezuela even though they were earning just a few dollars per day. In the Christmas season of 2017, some shops would no longer use price tags since prices would inflate so quickly, so customers were required to ask staff at stores how much each item was.

The International Monetary Fund estimated in 2018 that Venezuela's inflation rate would reach 1,000,000% by the end of the year.


Hyperinflation in Zimbabwe was one of the few instances that resulted in the abandonment of the local currency. At independence in 1980, the Zimbabwe dollar (ZWD) was worth about USD 1.25. Afterwards, however, rampant inflation and the collapse of the economy severely devalued the currency. Inflation was steady before Robert Mugabe began confiscating land from the white farming community in 1998, resulting in a near total collapse in food production and the decline of foreign investment. The result was that to pay its expenditures Mugabe's government and Gideon Gono's Reserve Bank printed more and more notes with higher face values.

Hyperinflation began early in the 21st century, reaching 624% in 2004. It fell back to low triple digits before surging to a new high of 1,730% in 2006. The Reserve Bank of Zimbabwe revalued on 1 August 2006 at a ratio of 1,000 ZWD to each second dollar (ZWN), but year-to-year inflation rose by June 2007 to 11,000% (versus an earlier estimate of 9,000%). Larger denominations were progressively issued in 2008:

Inflation by 16 July officially surged to 2,200,000% with some analysts estimating figures surpassing 9,000,000%. As of 22 July 2008 the value of the ZWN fell to approximately 688 billion per 1 USD, or 688 trillion pre-August 2006 Zimbabwean dollars.

On 1 August 2008, the Zimbabwe dollar was redenominated at the ratio of ZWN to each third dollar (ZWR). On 19 August 2008, official figures announced for June estimated the inflation over 11,250,000%. Zimbabwe's annual inflation was 231,000,000% in July (prices doubling every 17.3 days). By October 2008 Zimbabwe was mired in hyperinflation with wages falling far behind inflation. In this dysfunctional economy hospitals and schools had chronic staffing problems, because many nurses and teachers could not afford bus fare to work. Most of the capital of Harare was without water because the authorities had stopped paying the bills to buy and transport the treatment chemicals. Desperate for foreign currency to keep the government functioning, Zimbabwe's central bank governor, Gideon Gono, sent runners into the streets with suitcases of Zimbabwean dollars to buy up American dollars and South African rand.

For periods after July 2008, no official inflation statistics were released. Prof. Steve H. Hanke overcame the problem by estimating inflation rates after July 2008 and publishing the Hanke Hyperinflation Index for Zimbabwe. Prof. Hanke's HHIZ measure indicated that the inflation peaked at an annual rate of 89.7 sextillion percent (89,700,000,000,000,000,000,000%) in mid-November 2008. The peak monthly rate was 79.6 billion percent, which is equivalent to a 98% daily rate, or around percent yearly rate. At that rate, prices were doubling every 24.7 hours. Note that many of these figures should be considered mostly theoretic, since the hyperinflation did not proceed at that rate a whole year.

At its November 2008 peak, Zimbabwe's rate of inflation approached, but failed to surpass, Hungary's July 1946 world record. On 2 February 2009, the dollar was redenominated for the third time at the ratio of ZWR to 1 ZWL, only three weeks after the $100 trillion banknote was issued on 16 January, but hyperinflation waned by then as official inflation rates in USD were announced and foreign transactions were legalised, and on 12 April the dollar was abandoned in favour of using only foreign currencies. The overall impact of hyperinflation was 1 ZWL = ZWD.


Some countries experienced very high inflation, but did not reach hyperinflation, as defined as a "monthly" inflation rate of 50%.

Between 1987 and 1995 the Iraqi Dinar went from an official value of 0.306 Dinars/USD (or $3.26 USD per dinar, though the black market rate is thought to have been substantially lower) to 3,000 Dinars/USD due to government printing of 10s of trillions of dinars starting with a base of only 10s of billions. That equates to approximately 315% inflation per year averaged over that eight-year period.

In spite of increased oil prices in the late 1970s (Mexico is a producer and exporter), Mexico defaulted on its external debt in 1982. As a result, the country suffered a severe case of capital flight and several years of acute inflation and peso devaluation, leading to an accumulated inflation rate of almost 27,000% between December 1975 and late 1988. On 1 January 1993, Mexico created a new currency, the "nuevo peso" ("new peso", or MXN), which chopped three zeros off the old peso (One new peso was equal to 1,000 old MXP pesos).

In Roman Egypt, where the best documentation on pricing has survived, the price of a measure of wheat was 200 drachmae in 276 AD, and increased to more than 2,000,000 drachmae in 334 AD, roughly 1,000,000% inflation in a span of 58 years.

Although the price increased by a factor of 10,000 over 58 years, the annual rate of inflation was only 17.2% compounded.

Romania experienced high inflation in the 1990s. The highest denomination in 1990 was 100 lei and in 1998 was 100,000 lei. By 2000 it was 500,000 lei. In early 2005 it was 1,000,000 lei. In July 2005 the lei was replaced by the new leu at 10,000 old lei = 1 new leu. Inflation in 2005 was 9%. In July 2005 the highest denomination became 500 lei (= 5,000,000 old lei).

During the Revolutionary War, when the Continental Congress authorized the printing of paper currency called continental currency, the monthly inflation rate reached a peak of 47 percent in November 1779 (Bernholz 2003: 48). These notes depreciated rapidly, giving rise to the expression "not worth a continental."
One cause of the inflation was counterfeiting by the British, who ran a press on HMS "Phoenix", moored in New York Harbour. The counterfeits were advertised and sold almost for the price of the paper they were printed on.

A second close encounter occurred during the U.S. Civil War, between January 1861 and April 1865, the Lerner Commodity Price Index of leading cities in the eastern Confederacy states increased from 100 to over 9,000. As the Civil War dragged on, the Confederate dollar had less and less value, until it was almost worthless by the last few months of the war. Similarly, the Union government inflated its greenbacks, with the monthly rate peaking at 40 percent in March 1864 (Bernholz 2003: 107).

Vietnam went through a period of chaos and hyperinflation in the late 1980s, with inflation peaking at 774% in 1988, after the country's "price-wage-currency" reform package, led by then-Deputy Prime Minister Trần Phương, had failed. Hyperinflation also occurred in the early stages of the socialist-oriented market economic reforms commonly referred to as the Đổi Mới.

Inflation rate is usually measured in percent per year. It can also be measured in percent per month or in price doubling time.

formula_1

formula_2

formula_3

formula_4

Often, at redenominations, three zeroes are cut from the bills. It can be read from the table that if the (annual) inflation is for example 100%, it takes 3.32 years to produce one more zero on the price tags, or 3 × 3.32 = 9.96 years to produce three zeroes. Thus can one expect a redenomination to take place about 9.96 years after the currency was introduced.





</doc>
<doc id="13682" url="https://en.wikipedia.org/wiki?curid=13682" title="Herbert Hoover">
Herbert Hoover

Herbert Clark Hoover (August 10, 1874 – October 20, 1964) was an American engineer, businessman and politician who served as the 31st President of the United States from 1929 to 1933 during the Great Depression. A Republican, as Secretary of Commerce in the 1920s he introduced themes of efficiency in the business community and provided government support for standardization, efficiency and international trade. As president from 1929 to 1933, his domestic programs were overshadowed by the onset of the Great Depression. Hoover was defeated in a landslide election in 1932 by Democratic Franklin D. Roosevelt. After this loss, Hoover became staunchly conservative, and advocated against Roosevelt's New Deal policies.

A lifelong Quaker, he became a successful mining engineer with a global perspective. He built an international reputation as a humanitarian by leading international relief efforts in Belgium during World War I, 1914-1917. When the U.S. entered the war in 1917 he became "food czar" as head of the U.S. Food Administration with charge of much of the nation's food supply and a massive advertising campaign to help consumers adjust and save. He worked well with President Woodrow Wilson and the cabinet, and gained a large national audience. After the war, he led the American Relief Administration, which provided food to the inhabitants of Central Europe and Eastern Europe. Hoover was popular among progressives as a potential candidate in the 1920 presidential election, but his candidacy quickly petered out. Republican Warren G. Harding won and appointed Hoover as Secretary of Commerce. Hoover was an unusually active and visible cabinet member, becoming known as "Secretary of Commerce and Under-Secretary of all other departments." Hoover won the Republican nomination in 1928, and defeated Democrat Al Smith in a landslide. Hoover avoided the anti-Catholicism that hurt Smith, but in a time of peace and prosperity his success was highly likely.

The Great Depression was the central issue of his presidency, starting with the Wall Street Crash of October 1929. There were occasional upswings but more frequent downswings until the economy verged on disaster in 1931-33, along with most of the industrial world. Hoover pursued a variety of policies in an attempt to lift the economy, but opposed direct federal relief efforts until late in his tenure. He asked business and labor leaders to avoid wage cuts and work stoppages, and raised taxes in the hope of balancing the budget. In 1930, he reluctantly approved the Smoot–Hawley Tariff, which sent foreign trade spiraling down. The economy kept falling, and the unemployment rate rose to 25%, with heavy industry, mining, and wheat and cotton farming hit especially hard. In 1932, Hoover signed a major public works bill and established the Reconstruction Finance Corporation, which was designed to provide government loans to banks, railroads and big businesses in danger of failing. The downward economic spiral, along with violent dispersal of the Bonus Army, set the stage for Hoover's overwhelming defeat by Roosevelt, who promised a New Deal.

Hoover became a conservative spokesman in opposition to the domestic and foreign policies of Roosevelt. He opposed entry into the Second World War and was not called on to serve in any public role during the war. He had better relations with President Harry S. Truman, and Hoover helped produce a number of reports that changed U.S. occupation policy in Germany. Truman also appointed Hoover to head the Hoover Commission, intended to foster greater efficiency throughout the federal bureaucracy, and Hoover served on a similar commission under President Dwight D. Eisenhower. By the time of his death in 1964, he had rehabilitated his image. Nevertheless, Hoover is generally not ranked highly in historical rankings of Presidents of the United States.

Herbert Hoover was born on August 10, 1874 in West Branch, Iowa. He is the only president born in that state, and the first born west of the Mississippi River. His father, Jesse Hoover (1849–1880), was a blacksmith and farm implement store owner of German (Pfautz, Wehmeyer), German-Swiss (Huber, Burkhart) and English ancestry. Jesse Hoover and his father Eli had moved to Iowa from Ohio twenty years previously. Hoover's mother, Hulda Randall Minthorn (1849–1884), was born in Norwich, Ontario, Canada and was of English and Irish ancestry. Both of his parents were Quakers.

Hoover's family figured prominently in the town's public prayer life due almost entirely to Hulda's role in her church. His father, noted by the local paper for his "pleasant, sunshiny disposition", died in 1880. After working to retire her husband's debts, retain their life insurance, and care for the children, his mother died in 1884, leaving Hoover (age nine), his older brother, and his younger sister as orphans. Fellow Quaker Lawrie Tatum was appointed as Hoover's guardian.

After a brief stay with one of his grandmothers in Kingsley, Iowa, Hoover lived the next 18 months with his uncle Allen Hoover in West Branch. In November 1885, he went to Newberg, Oregon, to live with his uncle Dr. John Minthorn, a physician and businessman whose own son had died the year before. The Minthorn household was considered cultured and educational, and imparted a strong work ethic. Observers, including Minthorn himself, describe Hoover as being unhappy with the long days of toil he experienced while staying with the Minthorn household. Hoover attended Friends Pacific Academy (now George Fox University), but dropped out at the age of thirteen to become an office assistant for his uncle's real estate office in Salem, Oregon. Though he did not attend high school, Hoover attended night school and learned bookkeeping, typing, and mathematics.

Hoover entered Stanford University in 1891, its inaugural year, after failing all the entrance exams (except mathematics) and then being tutored for the summer in Palo Alto. Hoover claimed to be the very first student at Stanford, by virtue of having been the first person in the first class to sleep in the dormitory. Hoover worked a variety of odd jobs to support himself, and struggled in many of his classes, especially English. But he found a happiness at Stanford that previously eluded him.

Hoover had been unsure of his major before arriving at Stanford, but a position working for geologist John Casper Branner convinced him to switch his major to geology, and Hoover interned for Branner and the United States Geological Survey during the summer. Though he was shy among fellow students at first, Hoover won election as student treasurer and became known for his distaste for fraternities and sororities. In his senior year, he became smitten with a classmate named Lou Henry, but his financial situation precluded marriage. Hoover graduated from Stanford in 1895, in the midst of the Panic of 1893, and initially struggled to find a job.

While at the university he was the student manager of both the baseball and football teams and was a part of the inaugural Big Game versus rival the University of California and friend (Cal Manager) Herbert Lang. Only 10,000 tickets were printed for the inaugural game and 20,000 people showed up. Both Hoover and Lang had to find pots, bowls and any other available receptacles to collect admission fees. Stanford won the game. In 1892 Hoover invited Polish composer Ignacy Jan Paderewski to give a benefit concert. Hoover and his associates were unable to pay Paderewski the entire honorarium. The musician after hearing their story returned them the money so they could pay for rental of the concert hall. In 1919 Paderewski, now prime minister of Poland, traveled to America to thank the head of the American Relief Administration for helping Poland. "That's all right, Mr. Paderewski," Hoover replied. "Besides, you don't remember it, but you helped me once when I was a student at college and I was in a hole."

After graduation, Hoover worked in the gold mining districts of Nevada City and Grass Valley, California, before landing a job with the mining engineering firm of Louis Janin. Hoover went to Western Australia in 1897 as an employee of Bewick, Moreing & Co., a London-based gold mining company. His geological training and work experience were well suited for the firm's objectives. He worked at gold mines in Big Bell, Cue, Gwalia, Menzies, and Coolgardie.

Hoover first went to Coolgardie, then the center of the Eastern Goldfields, where he worked under Edward Hooper, a company partner. Conditions were harsh in these goldfields even though he got a $5,000 salary (equivalent to $100,000 today). In the Coolgardie and Murchison rangelands on the edge of the Great Victoria Desert, Hoover described the region as a land of "black flies, red dust and white heat". He served as a geologist and mining engineer while searching the Western Australian goldfields for investments. A poem (written to a barmaid he met) and antique mirror gifted by Hoover can still be found today at The Palace Hotel, in Kalgoorlie-Boulder, Western Australia. After being appointed as mine manager at the age of 23, he led a major program of expansion for the Sons of Gwalia gold mine at Gwalia, and brought in many Italian immigrants to cut costs and counter the labour movement of the Australian miners. Hoover became opposed to measures such as the minimum wage and workers' compensation, feeling that they were unfair to owners. In 1898, Hoover was promoted to junior partner by his employers, who were pleased with Hoover's talent and devotion to his work. After earning his promotion, he cabled his college sweetheart, Lou Henry, asking her to marry him. After she cabled back her acceptance of the proposal, Hoover briefly returned to the United States to marry her. Hoover and his wife had two children: Herbert Charles Hoover (1903–1969) and Allan Henry Hoover (1907–1993).

Rather than returning to Australia, Hoover and his new wife traveled to China. An open feud had developed between Hoover and his boss Ernest Williams, with Hoover persuading four other mine managers to conspire against his rival. Defusing the situation, the firm's principals offered Hoover a compelling promotion that relocated him to China. During his time at Gwalia, Hoover first met Fleury James Lyster, a pioneering metallurgist. In Western Australia friends called Hoover "H.C." or the old nickname "Hail Columbia".

The Hoovers lived in China from April 1899 until August 1900. Hoover's work in China revolved around the huge Kaiping Mines. Hoover worked as chief engineer for the Chinese Bureau of Mines, and as general manager for the Chinese Engineering and Mining Corporation. Later he worked for Bewick, Moreing & Co. as the company's lead engineer. Hoover's wife studied Mandarin Chinese and he also learned some of the language while he worked in China; it is said that they used it during his tenure at the White House when they wanted to foil eavesdroppers.

Hoover made recommendations to improve the lot of the Chinese worker, seeking to end the practice of imposing long term servitude contracts and to institute reforms for workers based on merit. The Boxer Rebellion trapped the Hoovers in Tianjin in June 1900. For almost a month, the settlement was under fire, and both dedicated themselves to defense of their city. Hoover himself guided U.S. Marines around Tianjin during the battle, using his knowledge of the local terrain. Mrs. Hoover, meanwhile, devoted her efforts at the various hospitals and even wielded, and willingly and accurately deployed, a .38-caliber pistol.
Hoover was made a partner in Bewick, Moreing & Co. on December 18, 1901 and assumed responsibility for various Australian operations and investments. His initial compensation rose to $12,500 annually in addition to a 20% share of profits. The company eventually controlled at one point approximately 50% of gold production in Western Australia. In 1901, Hoover no longer lived in Australia, but he visited the country in 1902, 1903, 1905, and 1907 as an overseas investor.

Hoover was also a director of Chinese Engineering and Mining Corporation (CEMC) when it became a supplier of immigrant labor from Southeast Asia for South African mines. The first shipment of almost 2,000 workers arrived in Durban from Qinhuangdao in July 1904. By 1906, the total number of immigrant workers increased to 50,000, almost entirely recruited and shipped by CEMC. When the living and working conditions of the laborers became known, public opposition to the scheme grew and questions were asked in the British Parliament. The scheme was abandoned in 1911.

Acting as a main investor, financier, mining speculator, and organizer of men, Hoover played a major role in the important metallurgical developments that occurred in Broken Hill, New South Wales in the first decade of the twentieth century, developments that had a great impact on the mining and production of silver, lead, and zinc. In August–September 1905, he founded the Zinc Corporation (eventually part of the Rio Tinto Group) with William Baillieu and others. The lead-silver ore produced at Broken Hill was rich in zinc. But the zinc could not be recovered due to "the Sulphide Problem", and was left in the tailings that remained after the silver and lead was extracted.

Zinc Corporation proposed to buy the tailings and extract the zinc by a new process. The froth flotation process was then being developed at Broken Hill, although the Zinc Corporation struggled to apply it. F.J. Lyster, originally a carpenter before becoming a foreman in the gravity mill, perfected the "Lyster Process", which enabled the Zinc Corporation to operate the world's first selective or differential flotation plant. According to Geoffrey Blainey, though the process was not fully understood, a patent was applied for in May 1912. Hoover wrote, "Broken Hill was one of the dreariest places in the world at this time. It lay in the middle of the desert, was unbelievably hot in summer, had no fresh water, no vegetation, and mountains of tailings blew into every crack with every wisp of wind.' Despite these miserable conditions, Hoover and his associates became suppliers to world industry of zinc and other vital base minerals.

In 1908, Hoover became an independent mining consultant, traveling worldwide until the outbreak of World War I in 1914. He left Bewick, Moreing & Co and, setting out on his own, eventually ended up with investments on every continent and offices in San Francisco, London, New York City, St. Petersburg, Paris and Mandalay, Burma. He specialized in rejuvenating troubled mining operations, taking a share of the profits in exchange for his technical and financial expertise. In later years Hoover thought of himself and his associates as "engineering doctors to sick concerns", hence his reputation as the "Doctor of sick mines".

He had his second successful venture with the British firm Burma Corporation, again producing silver, lead, and zinc in large quantities at the Namtu Bawdwin Mine, where he caught malaria in 1907. He also helped increase copper production in Kyshtym, Russia, through the use of pyritic smelting. Then he agreed to manage one of the Russian Czar's Cabinet Mines, located in the Altai Mountains. The oxidized lead-zinc-silver ore contained copper and gold as well. According to Hoover, "It developed probably the greatest and richest single body of ore known in the world" before the Communist Revolution.

In his spare time, Hoover wrote. His lectures at Columbia and Stanford universities were published in 1909 as "Principles of Mining", which became a standard textbook. The book reflects his move towards Progressive ideals, as Hoover came to endorse eight-hour workdays and organized labor. Hoover and his wife also published their English translation of the 1556 mining classic "De re metallica" in 1912. This translation from the Latin of Renaissance author Georgius Agricola is still the most important scholarly version and provides its historical context.

By 1914, Hoover was a very wealthy man, with an estimated personal fortune of $4 million. Hoover also stood eventually to obtain what he later described as "a large fortune from these Russian industries, probably more than is good for anybody." Sixty-six years after opening the mine in 1897, Hoover still had a partial share in the Sons of Gwalia mine when it finally closed in 1963, just one year before the former president's death in New York City in 1964. The successful mine had yielded $55m in gold and $10m in dividends for investors.

When World War I began in August 1914, Hoover helped organize the return of around 120,000 Americans from Europe. He led 500 volunteers in distributing food, clothing, steamship tickets and cash. "I did not realize it at the moment, but on August 3, 1914, my career was over forever. I was on the slippery road of public life." Hoover liked to say that the difference between dictatorship and democracy was simple: dictators organize from the top down, democracies from the bottom up.

When Belgium faced a food crisis after being invaded by Germany in 1914, Hoover undertook an unprecedented relief effort with the Commission for Relief in Belgium (CRB). As chairman of the CRB, Hoover worked with the leader of the Belgian "Comité National de Secours et d'Alimentation" (CNSA), Émile Francqui, to feed the entire nation for the duration of the war. The CRB obtained and imported millions of tons of foodstuffs for the CNSA to distribute, and watched over the CNSA to make sure the German army did not appropriate the food. The CRB became a veritable independent republic of relief, with its own flag, navy, factories, mills, and railroads. Private donations and government grants (78%) supplied an $11-million-a-month budget.

For the next two years, Hoover worked 14-hour days from London, administering the distribution of over two million tons of food to nine million war victims. In an early form of shuttle diplomacy, he crossed the North Sea forty times to meet with German authorities and persuade them to allow food shipments, becoming an international hero. The Belgian city of Leuven named a prominent square "Hooverplein" after him. At its peak, Hoover's American Relief Administration (ARA) fed 10.5 million people daily. Great Britain grew reluctant to support the CRB, preferring instead to emphasize Germany's obligation to supply the relief; Winston Churchill, whom Hoover intensely disliked, led a military faction that considered the Belgian relief effort "a positive military disaster".

Hoover greatly impressed American diplomat Walter Page, who said:
He is probably the only man living who has privately (i.e., without holding office) negotiated understandings with the British, French, German, Dutch, and Belgian governments. He personally knows and has had direct dealings with these governments, and his transactions with them have involved several hundred million dollars. He is a man of very considerable fortune—less than when the war began, for this relief work has cost him much.

After the United States entered the war in April 1917, President Woodrow Wilson appointed Hoover to head the U.S. Food Administration, which was created under the Lever Food Control Act in 1917 to ensure the nation's food needs during the war. This was a position he actively sought, though he later claimed it was thrust upon him. He was convinced from his Belgian work that centralization of authority was essential to any relief effort; he demanded, and got, great power albeit not as much as he sought. Hoover believed "food will win the war"; and beginning on September 29, this slogan was introduced and put into frequent use. Earning the appellation of "food czar," Hoover recruited a volunteer force of hundreds of thousands of women and deployed propaganda in movie theaters, schools, and churches.

He carefully selected men to assist in the agency leadership – Alonzo Taylor (technical abilities), Robert Taft (political associations), Gifford Pinchot (agricultural influence) and Julius Barnes (business acumen). Determined to avoid rationing, Hoover established set days for people to avoid eating specified foods and save them for soldiers' rations: meatless Mondays, wheatless Wednesdays, and "when in doubt, eat potatoes." These policies were dubbed "Hooverizing" by government publicists, in spite of Hoover's continual orders that publicity should not mention him by name. The agency employed a system of price controls and licensing requirements for suppliers to maximize production. Despite efforts to prevent it, some companies reaped great profits.

Days after the end of World War I in November 1918, Hoover sailed to Europe. The United States Food Administration became the American Relief Administration (ARA), and Hoover was charged with providing food to Central and Eastern Europe. The ARA fed millions, including the inhabitants of Germany and the former Habsburg Empire. U.S. government funding for the ARA expired in the summer of 1919, and Hoover transformed the ARA into a private organization, raising millions of dollars from private donors. Under the auspices of the ARA, the European Children's Fund fed millions of starving children. In addition to nourishing millions, the ARA also helped the United States avoid a potentially problematic domestic food surplus. Hoover provided aid to the defeated German nation after the war, as well as relief to famine-stricken Bolshevik-controlled areas of Russia in 1921, despite the opposition of Senator Henry Cabot Lodge and other Republicans. The Russian famine of 1921–22 claimed 6 million people. When asked if he was not thus helping Bolshevism, Hoover stated,

Reflecting the gratitude of many Europeans, in July 1922, Soviet author Maxim Gorky wrote to Hoover:

As one of the most powerful individuals in Europe, Hoover became involved in continental politics. He was broadly supportive of Wilson's Fourteen Points, and was appointed by Wilson to the U.S. delegation at the peace conference in Versailles. He urged ratification of the Treaty of Versailles, but opposed harsh reparations for Germany. He opposed a monarchist coup in Hungary and demanded the appointment of the moderate Ignacy Jan Paderewski in Poland. He condemned Bolshevism, but warned President Wilson against an intervention in Russia, viewing the White Russian forces as little better and fearing the possibility of a protracted U.S. involvement.

Hoover had been little known among the American public before 1914, but emerged as perhaps the second-most famous person in the United States after President Wilson. In the lead-up to the 1920 presidential election, Hoover was often mentioned as a potential candidate, but his partisan affiliation was unclear. Hoover particularly appealed to progressives of both parties, who commended his wartime push for higher taxes, criticism of Attorney General A. Mitchell Palmer, and advocacy for measures such as the minimum wage, forty-eight-hour workweek, and elimination of child labor. Yet Hoover's "rags to riches" story and war-time leadership made him appealing to many others as well.

In March 1920, Hoover publicly declared his allegiance with the GOP, stating that he would not run for but would not refuse the 1920 Republican nomination. Hoover had various reasons for choosing the Republican Party, including a split with Wilson over the Versailles Treaty (Hoover had come to accept the Lodge Reservations to the treaty), and his view that the Democrats would likely lose the 1920 election. However, the conservative Old Guard of the GOP viewed Hoover warily, and his time as Food Czar had also made him numerous enemies among farmers, an important bloc in the GOP. Hoover's candidacy for the GOP nomination fizzled out after his defeat in the California primary by favorite son Hiram Johnson. In the general election, Hoover supported the Republican nominee, Warren G. Harding, who emerged victorious.

In 1919, Hoover established the Hoover War Collection at Stanford University. He donated all the files of the Commission for Relief in Belgium, the U.S. Food Administration, and the American Relief Administration, and pledged $50,000 as an endowment. Scholars were sent to Europe to collect pamphlets, society publications, government documents, newspapers, posters, proclamations, and other ephemeral materials related to the war and the revolutions that followed it. The collection was renamed the Hoover War Library in 1922 and is now known as the Hoover Institution.

After his election as president in 1920, Harding rewarded Hoover for his support, offering to appoint him as either Secretary of the Interior or Secretary of Commerce. Expecting opposition from the Republican-controlled Senate, Hoover initially decided to refuse a Cabinet position, but Harding paired Hoover's nomination with that of Andrew Mellon, who many Republicans hoped would become Secretary of the Treasury. Secretary of Commerce was considered a minor Cabinet post, with limited and vaguely defined responsibilities, but Hoover decided to accept the position. Hoover remained in office until 1928, serving in both the Harding and Coolidge administrations.

Hoover envisioned the Commerce Department as the hub of the nation's growth and stability. From Harding he demanded, and received, authority to coordinate economic affairs throughout the government. He created many sub-departments and committees, overseeing and regulating everything from manufacturing statistics, the census and radio, to air travel. In some instances he "seized" control of responsibilities from other Cabinet departments when he deemed that they were not carrying out their responsibilities well. He became known as the "Secretary of Commerce and Under-Secretary of all other departments."

Many of Hoover's efforts as Commerce Secretary centered on eliminating waste and increasing efficiency in business and industry. This included reducing labor losses from trade disputes and seasonal fluctuations, reducing industrial losses from accident and injury, and reducing the amount of crude oil spilled during extraction and shipping. One major achievement was to promote product standardizations. He promoted international trade by opening overseas offices to advise businessmen. Hoover was especially eager to promote Hollywood films overseas.

As secretary and later as president, Hoover revolutionized relations between business and government. Rejecting the adversarial stance of Theodore Roosevelt, William Howard Taft, and Woodrow Wilson, he sought to make the Commerce Department a powerful service organization, empowered to forge cooperative voluntary partnerships between government and business. This philosophy is often called "associationalism". Both the U.S. Department of Justice and the Federal Trade Commission opposed Hoover's goals, but the U.S. Supreme Court ruled in "Maple Flooring Manufacturers' Assn. v. United States 268 U.S. 563 (1925)" that Hoover's policies served the public interest by producing "fairer price levels" and "avoid[ing] waste."

His "Own Your Own Home" campaign was a collaboration to promote ownership of single-family dwellings, with groups such as the Better Houses in America movement, the Architects' Small House Service Bureau, and the Home Modernizing Bureau. He worked with bankers and the savings and loan industry to promote the new long-term home mortgage, which dramatically stimulated home construction.

Hoover's radio conferences played a key role in the early organization, development and regulation of radio broadcasting. Prior to the Radio Act of 1927, the Secretary of Commerce was unable to deny radio licensing or reassign broadcast frequencies. With help from supporters Senator Dill and Representative White, Hoover brought the issue of radio control to the Senate floor. Hoover fought for more power to control the proliferation of licensed radio stations (which in 1927, stood at 732 stations). With help from Dill and White, Hoover promoted the Dill-White Bill which eventually would become the Radio Act of 1927. This act allowed the government to intervene and abolish radio stations that were deemed "non-useful" to the public. Hoover's attempts at regulating radio were not supported by all Congressmen, and he received much opposition from the Senate and from radio station owners. However, Hoover's contributions to regulate radio in its infancy heavily influenced the modern radio system.

Hoover was also influential in the early development of air travel. He sought to create a thriving private industry boosted by indirect government subsidies. He encouraged the development of emergency landing fields and required all runways to be equipped with lights and radio beams. He also encouraged farmers to make use of planes for crop dusting. In light of his efforts, Washington, D.C. named its first airport Hoover Field.

As Commerce Secretary, Hoover also hosted two national conferences on street traffic, in 1924 and 1926 (a third convened in 1930, during Hoover's presidency). Collectively the meetings were called the National Conference on Street and Highway Safety. Hoover's chief objective was to address the growing casualty toll of traffic accidents, but the scope grew and soon embraced motor vehicle standards, rules of the road, and urban traffic control. He left the invited interest groups to negotiate agreements among themselves, which were then presented for adoption by states and localities. Because automotive trade associations were the best organized, many of the positions taken by the conferences reflected their interests. The conferences issued a model Uniform Vehicle Code for adoption by the states, and a Model Municipal Traffic Ordinance for adoption by cities. Both were widely influential, promoting greater uniformity between jurisdictions and tending to promote the automobile's priority in city streets.

The Great Mississippi Flood of 1927 broke the banks and levees of the lower Mississippi River in early 1927, resulting in flooding of millions of acres and leaving 1.5 million people displaced from their homes. Although such a disaster did not fall under the duties of the Commerce Department, the governors of six states along the Mississippi specifically asked for Herbert Hoover in the emergency. President Coolidge appointed Hoover to coordinate the response to the flood. Hoover personally crisscrossed the Mississippi Valley, giving speeches and coordinating the response. He established over one hundred tent cities and a fleet of more than six hundred vessels, and raised $17 million. In large part due to his leadership during the flood crisis, by 1928, Hoover had begun to overshadow President Coolidge himself.

The treatment of African-Americans during the disaster endangered Hoover's reputation as a humanitarian. Local officials brutalized black farmers and prevented them from leaving relief camps, aid intended for African-American sharecroppers was often given instead to the landowners, and black men often were conscripted by locals into forced labor, sometimes at gun point. Knowing the potential damage to his presidential hopes if this became public, Hoover struck a deal with Robert Russa Moton, the prominent African-American successor to Booker T. Washington as president of the Tuskegee Institute. In exchange for keeping the sufferings of African-Americans quiet, Hoover promised unprecedented influence for African-Americans should he become president. Moton agreed, and following the accommodationist philosophy of Washington, he worked actively to conceal the information from the media.

When President Calvin Coolidge announced in August 1927 that he would not seek a second full term of office in the 1928 presidential election, Hoover became the leading Republican candidate, despite the fact Coolidge was lukewarm on Hoover, often deriding his ambitious and popular Commerce Secretary as "Wonder Boy". Coolidge had been reluctant to choose Hoover as his successor; on one occasion he remarked that

Even so, Coolidge had no desire to split the party by publicly opposing the popular Commerce Secretary's nomination.

Prior to the 1928 Republican National Convention, many wary Republican leaders cast about for an alternative candidate such as Treasury Secretary Andrew Mellon, former Secretary of State Charles Evans Hughes, or Coolidge. When no challenger emerged, Hoover won the presidential nomination on the first ballot of the convention. The delegates considered nominating incumbent Vice President Charles Dawes to be Hoover's running mate. However, Coolidge (who hated Dawes) remarked that this would be "a personal affront" to him. The convention instead selected Senator Charles Curtis of Kansas. Hoover accepted the nomination at Stanford Stadium, telling a huge crowd that he would continue the policies of the Harding and Coolidge administration.

Hoover campaigned for efficiency and the Republican record of prosperity against Democrat Alfred E. Smith. Smith likewise was a proponent of efficiency earned as governor of New York. Both candidates were pro-business, and each promised to improve conditions for farmers, reform immigration laws, and maintain America's isolationist foreign policy.

Where they differed was on the Volstead Act which outlawed the sale of liquor and beer. Smith was a "wet" who called for its repeal, whereas Hoover gave limited support for prohibition, calling it an "experiment noble in purpose." His use of ""experiment"" suggested it was not permanent. While Smith won extra support among Catholics in the big cities, he was also the target of intense anti-Catholicism from some Protestant communities, especially amongst Southern Baptists and German Lutherans. Overall the religious factor worked to the advantage of Hoover, although he took no part in it.

Historians agree that Hoover's national reputation and the booming economy, combined with deep splits in the Democratic Party over religion and prohibition, guaranteed his landslide victory with 58 percent of the popular vote. Hoover's appeal to southern white voters succeeded in cracking the "Solid South", winning the Democratic strongholds of Florida, North Carolina, Virginia, Texas and Tennessee, and nearly taking Alabama on support from Appalachian counties; the Deep South continued to support Smith as the Democratic candidate. This was the first time that a Republican candidate for president had carried Texas. Hoover and the national party had pursued a "lily-white southern strategy" to resuscitate the Republican Party in the South, "purging black Republicans from leadership positions in the southern wing of the G.O.P." This outraged the black leadership, which largely broke from the Republican Party, and began seeking candidates within the Democratic Party who supported civil rights. In 1956, W. E. B. Du Bois, a leader in the NAACP in the 1920s, would recall that, "In 1928, Negroes faced absolute dilemma. Neither Hoover nor Smith wanted the Negro vote and both publicly insulted us."

Following his inauguration, Hoover held a press conference on his first day in office, promising a "new phase of press relations". He asked the group of journalists to elect a committee to recommend improvements to the White House press conference. Hoover declined to use a spokesman, instead asking reporters to directly quote him and giving them handouts with his statements ahead of time. In his first 120 days in office, he held more regular and frequent press conferences than any other president, before or since. However, he changed his press policies after the 1929 stock market crash, screening reporters and greatly reducing his availability.

Hoover entered office with a plan to reform the nation's regulatory system, believing that a federal bureaucracy should have limited regulation over a country's economic system. Hoover sought a balance among labor, capital, and the government, and he has been variously labeled a corporatist or associationalist.

Hoover saw the presidency as a vehicle for improving the conditions of all Americans by encouraging public-private cooperation—what he termed "volunteerism". He tended to oppose governmental coercion or intervention, as he thought they infringed on American ideals of individualism and self-reliance. Hoover made extensive use of commissions to study issues and propose solutions, and many of those commissions were sponsored by private donors rather than by the government. One of the commissions started by Hoover, the Research Committee on Social Trends, was tasked with surveying the entirety of American society.

Lou Henry Hoover was an activist First Lady. She typified the new woman of the post–World War I era: intelligent, robust, and aware of multiple female possibilities.

White House physician Admiral Joel T. Boone invented the sport Hooverball to keep Hoover fit while in the White House. Hooverball is a combination of volleyball and tennis, played with a medicine ball. Hoover and several staff members played it each morning, earning them the nickname "Medicine Ball Cabinet".

Hoover seldom mentioned civil rights while he was president. He believed that African-Americans and other races could improve themselves with education and individual initiative. He opposed federal anti-lynching laws, and when lynchings occurred in the South, including one incident linked to his party's efforts to "Republicanize" southern states, he offered only verbal condemnation.

First Lady Lou Hoover defied custom and invited the wife of Republican Oscar De Priest, the only African-American member in Congress, to tea at the White House. Booker T. Washington was the only previous African-American to have dined at the White House, with Theodore Roosevelt in 1901.

Charles Curtis, the nation's first Native American Vice President, was from the Kaw tribe in Kansas. Hoover's humanitarian and Quaker reputation, along with Curtis as a vice president, gave special meaning to his Indian policies. His Quaker upbringing influenced his views that Native Americans needed to achieve economic self-sufficiency. As president, he appointed Charles J. Rhoads as commissioner of Indian affairs. Hoover supported Rhoads' commitment to Indian assimilation and sought to minimize the federal role in Indian affairs. His goal was to have Indians acting as individuals (not as tribes) and to assume the responsibilities of citizenship granted with the Indian Citizenship Act of 1924.

On taking office, Hoover said that "given the chance to go forward with the policies of the last eight years, we shall soon with the help of God, be in sight of the day when poverty will be banished from this nation." Having seen the fruits of prosperity brought by technological progress, many shared Hoover's optimism, and the already bullish stock market climbed even higher on Hoover's accession. But within months of taking office, the Stock Market Crash of 1929 (also known as Black Tuesday) occurred, and the worldwide economy began to spiral downward into the Great Depression.

The causes of the Great Depression remain a matter of debate, but Hoover viewed a lack of confidence in the financial system as the fundamental economic problem facing the nation. He sought to avoid direct federal intervention, believing that the best way to bolster the economy was through the strengthening of businesses such as banks and railroads. He also feared that allowing individuals on the "dole" would permanently weaken the country. Instead, Hoover strongly believed that local governments and private giving should address the needs of individuals. A reserved man with a fear of public speaking, Hoover allowed his political enemies to define him as cold, incompetent, reactionary, and out-of-touch.

Hoover pursued many policies in an attempt to pull the country out of depression, while attempting to restrain the federal government from becoming directly involved in commercial affairs. In the days following Black Tuesday, Hoover gathered business and labor leaders, asking them to avoid wage cuts and work stoppages while the country faced what he believed would be a short recession similar to the Depression of 1920–21. Some economists, such as Lee Ohanian, point to the resulting wage rigidity as a key cause of the severity of the Great Depression.

Hoover blamed Mexicans for the economic downturn, and championed a mass deportation which became known as the Mexican Repatriation program. This forced migration of approximately 500,000 to 2 million people to Mexico continued until 1936, after Hoover had left office. An estimated sixty percent of those deported were birthright citizens. Because the forced movement was based on race, and ignored citizenship, the process is seen as meeting modern standards for ethnic cleansing.

In the spring of 1930, Hoover acquired from Congress an additional $100 million to continue the Federal Farm Board lending and purchasing policies. At the end of 1929, the FFB established the National Wool Marketing Corporation (NWMC), a national wool cooperative made up of 30 state associations. Hoover also supported new public works projects, although his fear of budget deficits led him to oppose expansive projects such as that contemplated by the Muscle Shoals Bill, which sought to establish government production and distribution of power in the Tennessee Valley. In autumn 1930, Hoover established the President's Organization for Unemployment Relief, which issued press releases urging companies to hire.

Hoover had taken office hoping to raise agricultural tariffs in order to help farmers reeling from the farm crisis of the 1920s, but his attempt to raise agricultural tariffs became connected with attempts to raise tariffs for other goods. In June 1930, over the objection of many economists, Congress approved and Hoover reluctantly signed into law the Smoot–Hawley Tariff Act. The legislation raised tariffs on thousands of imported items.

The intent of the act was to encourage the purchase of American-made products by increasing the cost of imported goods, while raising revenue for the federal government and protecting farmers. However, economic depression had spread worldwide, and Canada, France and other nations retaliated by raising tariffs on imports from the U.S. The result was to contract international trade, and worsen the Depression. Progressive Republicans such as Senator Borah were outraged when Hoover signed the bill, and Hoover's relations with that wing of the party never recovered.

For much of his presidency, Hoover opposed congressional proposals to provide federal relief, and he feared that Congress would impose a federal relief program that would infringe on the prerogatives of state and local governments and philanthropic organizations. Hoover created the National Credit Corporation, a voluntary association of bankers, but the organization did not manage to save banks or ease credit as Hoover had hoped it would.

As the Great Depression continued, Hoover finally heeded calls for more direct federal intervention, though he vetoed a bill that would have allowed direct federal lending to individuals. In January 1932, Hoover signed a bill creating the Reconstruction Finance Corporation (RFC). The RFC's initial goal was to provide government-secured loans to financial institutions, railroads, and local governments to continue relief programs. The RFC saved numerous businesses from failure, but it failed to stimulate commercial lending as Hoover had hoped, partly because it was run by conservative bankers unwilling to make riskier loans. The RFC would be adopted by Roosevelt and greatly expanded as part of his New Deal. With the RFC failing to stem the economic crisis, Hoover signed the Emergency Relief and Construction Act, a major public works bill, in July 1932.

Throughout his presidency, Hoover defended the gold standard, and derided any other monetary system as "collectivism." Hoover and Senator Carter Glass, another gold standard proponent, recognized that they needed to stop deflation by encouraging the lending of credit. Hoover was instrumental in passing the Glass–Steagall Act of 1932, which allowed for prime rediscounting at the Federal Reserve, in turn allowing further inflation of credit and bank reserves. In July 1932, Hoover signed the Federal Home Loan Bank Act, establishing 12 district banks overseen by a Federal Home Loan Bank Board in a manner similar to the Federal Reserve System.

In 1930, unemployment stood at 8.9%, and many assumed that the United States was just in another recession. But by 1932, unemployment had reached 24.9%, businesses had defaulted on record numbers of loans, and more than 5,000 banks had failed, especially small rural banks. The homeless lived in shantytowns they called Hoovervilles.

Hoover was a firm believer in balanced budgets, and sought to avoid a budget deficit by greatly increasing tax rates on the wealthy. Nonetheless, the total Federal deficit in 1932 was 4.5% of GDP. Along with falling tax revenue, this was caused by the increase in total spending from about $2.9 billion in 1929 to about $4.6 billion in 1932, a total increase of about 58 percent.

To pay for government programs and to make up for revenue lost due to the Depression, Hoover signed the Revenue Act of 1932. The act increased taxes across the board, so that top earners were taxed at 63% on their net income – up from 25% when Herbert Hoover took office. The 1932 Act also increased the tax on the net income of corporations from 12% to 13.75%. Additionally, under Hoover, the estate tax was doubled and corporate taxes were raised by almost 15%. Also, a "check tax" took effect, placing a 2-cent tax (over 30 cents in today's economy) on all bank checks. Economists William D. Lastrapes and George Selgin conclude that the check tax was "an important contributing factor to that period's severe monetary contraction". Despite the passage of the Revenue Act, the federal government continued to run a budget deficit.
According to Leuchtenburg, Hoover was "the last American president to take office with no conspicuous need to pay attention to the rest of the world." Nevertheless, during Hoover's term, the world order established with the 1919 Treaty of Versailles began to crumble.

As president, Hoover largely made good on his pledge made prior to assuming office not to interfere in Latin America's internal affairs. In 1930, he released the Clark Memorandum, a rejection of the Roosevelt Corollary and a move towards non-interventionism in Latin America. Hoover did not completely refrain from the use of the military in Latin American affairs; he thrice threatened intervention in the Dominican Republic, and he sent warships to El Salvador to support the government against a left-wing revolution. But he wound down the Banana Wars, ending the occupation of Nicaragua and nearly bringing an end to the occupation of Haiti. Franklin Roosevelt's Good Neighbor policy would continue the trend towards non-interventionism in Latin America. 

Though the United States remained outside of the League of Nations, Hoover showed a willingness to work within multilateral structures. Hoover pursued United States membership in the Permanent Court of International Justice, but the Senate never voted on his proposal. The Senate also defeated Hoover's proposed Saint Lawrence Seaway Treaty with Canada.

In 1930, the United States and other major naval powers signed the London Naval Treaty, an extension of the 1922 Washington Naval Treaty, which sought to prevent a naval arms race. The treaty represented the first time that the naval powers had agreed to cap their tonnage of auxiliary vessels (previous agreements had focused on capital ships), but the treaty failed to include France or Italy. The treaty provoked a nationalist backlash in Japan due to its reconfirmation of the "5-5-3" ratio which limited Japan to a smaller fleet than the United States or the United Kingdom. At the 1932 World Disarmament Conference, Hoover urged worldwide cutbacks in armaments and the outlawing of tanks and bombers, but his proposals were not adopted.

In 1931, Japan invaded Manchuria, defeating the Republic of China's military forces and establishing Manchukuo, a puppet state. The Hoover administration deplored the invasion, but also sought to avoid antagonizing the Japanese, fearing that taking too strong a stand would weaken the moderate forces in the Japanese government. In response to the Japanese invasion, Hoover and Secretary of State Henry Stimson outlined the Stimson Doctrine, which held that the United States would not recognize territories gained by force. The Hoover administration based this declaration on the 1928 Kellogg-Briand Pact, in which several nations (including Japan and the United States) renounced war and promised to peacefully solve disputes. In the aftermath of invasion of Manchuria, Stimson and other members of the Cabinet came to believe that war with Japan might be inevitable, though Hoover continued to push for disarmament among the world powers.

In 1931, Hoover issued the Hoover Moratorium, calling for a one-year halt in World War I reparation payments. Hoover also made U.S. bankers agree to refrain from demanding payment on private loans from Germans. Hoover hoped that the moratorium would help stabilize the European economy, which he viewed as a major cause of economic troubles in the United States. As the moratorium neared its expiration the following year, an attempt to find a permanent solution was made at the Lausanne Conference of 1932. A working compromise was never established, and reparations payments virtually stopped.

Thousands of World War I veterans and their families demonstrated and camped out in Washington, DC, during June 1932, calling for immediate payment of a bonus that had been promised by the World War Adjusted Compensation Act in 1924 for payment in 1945. Although offered money by Congress to return home, some members of the "Bonus Army" remained. Washington police attempted to remove the demonstrators from their camp, but they were outnumbered and unsuccessful. Shots were fired by the police in a futile attempt to attain order, and two protesters were killed while many officers were injured.

Hoover sent U.S. Army forces led by General Douglas MacArthur and helped by lower ranking officers Dwight D. Eisenhower and George S. Patton to stop a march. MacArthur, believing he was fighting a communist revolution, chose to clear out the camp with military force. In the ensuing clash, hundreds of civilians were injured. Hoover had sent orders that the Army was not to move on the encampment, but MacArthur chose to ignore the command. Hoover was incensed, but refused to reprimand MacArthur. The entire incident was another devastating negative for Hoover in the 1932 election. That led New York governor and Democratic presidential candidate Franklin Roosevelt to declare of Hoover: "There is nothing inside the man but jelly!"

Despite the economic calamity facing the nation and his dim hopes for re-election, Hoover faced little opposition for re-nomination at the 1932 Republican National Convention. Some Republicans talked of nominating Coolidge, former Vice President Charles Dawes, or Senator Hiram Johnson, but all passed on the opportunity to challenge Hoover. Curtis was re-nominated as Hoover's running mate. Franklin D. Roosevelt won the presidential nomination on the third ballot of the 1932 Democratic National Convention, defeating the 1928 Democratic nominee, Al Smith. Speaker of the House John Nance Garner was nominated as Roosevelt's running mate. By 1932, the radio was in 12 million homes, changing the nature of presidential campaigns. No longer could presidents change the content of their speeches for each audience; anyone with a radio could listen to every major speech.

Hoover originally planned to make only one or two major speeches, and to leave the rest of the campaigning to proxies, as sitting presidents had traditionally done. However, encouraged by Republican pleas and outraged by Democratic claims, Hoover entered the public fray. In his nine major radio addresses Hoover primarily defended his administration and his philosophy of government. Hoover urged voters to hold to the "foundations of experience," rejecting the notion that government interventionism could save the country from the Depression.

In his campaign trips around the country, Hoover was faced with perhaps the most hostile crowds of any sitting president. Besides having his train and motorcades pelted with eggs and rotten fruit, he was often heckled while speaking, and on several occasions, the Secret Service halted attempts to kill Hoover by disgruntled citizens, including capturing one man nearing Hoover carrying sticks of dynamite, and another already having removed several spikes from the rails in front of the President's train.

The Democrats attacked Hoover as the cause of the Great Depression, and for being indifferent to the suffering of millions. As Governor of New York, Roosevelt had called on the New York legislature to provide aid for the needy, establishing Roosevelt's reputation for being more favorable toward government interventionism during the economic crisis. Fausold rejects the notion that the two nominees were similar ideologically, pointing to differences between the two on federal spending on public works, agricultural issues, Prohibition, and the tariff.

Hoover's attempts to vindicate his administration fell on deaf ears, as much of the public blamed his administration for the depression. Roosevelt won 57.4 percent of the popular vote compared to Hoover's 39.7 percent. Hoover's popular vote was reduced by nineteen percent from his result in the 1928 election, and he carried just five Northeastern states and Delaware. Roosevelt became the first Democratic presidential nominee to win a majority of the popular vote and the presidency since Franklin Pierce in 1852.

Hoover departed from Washington in March 1933, bitter at his election loss and continuing unpopularity. Upon leaving office, Hoover was the only living ex-president for nearly 20 years, until Harry Truman left office in 1953. The Hoovers went first to New York City, where they stayed for a while in the Waldorf Astoria hotel. Later that spring, they returned to California to their Stanford residence. Hoover enjoyed returning to the men's clubs that he had long been involved with, including the Bohemian Club, the Pacific-Union Club, and the University Club in San Francisco. Hoover and his wife lived in Palo Alto until her death in 1944, at which point Hoover began to live permanently at the Waldorf Astoria.

Hoover continued to closely follow national events after his retirement, becoming a constant critic of Franklin Roosevelt. In response to continued attacks on his character and presidency, Hoover wrote more than two dozen books, including "The Challenge to Liberty" (1934), which harshly criticized the New Deal. Hoover feared that the country had surrendered its "freedom of mind and spirit" to the New Deal. He described the National Recovery Administration and Agricultural Adjustment Administration as "fascistic," and the 1933 Banking Act as a "move to gigantic socialism."

Only 58 when he left office, Hoover held out hope for another term during the 1930s. At the 1936 Republican National Convention, Hoover's speech attacking the New Deal was well received, but the nomination went to Kansas Governor Alf Landon. In the general election, Hoover campaigned for Landon's unsuccessful campaign with numerous well-publicized speeches attacking New Deal liberalism. Though Hoover was eager to oppose Roosevelt at every turn, Senator Arthur Vandenberg and other Republicans urged the still-unpopular Hoover to remain out of the fray during the debate over Roosevelt's proposed Judiciary Reorganization Bill of 1937. At the 1940 Republican National Convention, Hoover again hoped for the presidential nomination, and was disheartened when it went to the internationalist Wendell Willkie.

Hoover remained popular in Europe and was honored in France and Belgium. During a 1938 trip to Europe, Hoover met with Adolf Hitler and stayed at Hermann Göring's hunting lodge. Hoover expressed dismay at the persecution of Jews in Germany and believed that Hitler was mad, but did not present a threat to the U.S. Instead, Hoover believed that Roosevelt posed the biggest threat to peace, as he believed that Roosevelt provoked Japan and discouraged France and the United Kingdom from reaching an "accommodation" with Germany. After the September 1939 invasion of Poland by Germany, Hoover opposed United States intervention in World War II, including the Lend-Lease policy.

Hoover became a vocal supporter of providing relief to countries in Nazi-occupied Europe. He was instrumental in creating the Commission for Polish Relief and Finnish Relief Fund. In 1939, Roosevelt asked Hoover to the White House for advice on getting aid to Poland, but Hoover turned down the offer. Much to his own frustration, Hoover was not called upon to serve after the United States entered World War II due to his differences with Roosevelt and his continuing unpopularity.

During a radio broadcast on June 29, 1941, one week after the Nazi invasion of the Soviet Union, Hoover disparaged any "tacit alliance" between the U.S. and the USSR by saying:

Following World War II, Hoover became friends with President Harry S. Truman despite their ideological differences. Hoover joked that they were for many years the sole members of the "trade union" of former Presidents. Because of Hoover's experience with Germany at the end of World War I, in 1946 President Truman selected the former president to tour Germany to ascertain the food needs of the occupied nation. Hoover toured what was to become West Germany in Hermann Göring's old train coach and produced a number of reports critical of U.S. occupation policy. He said that the economy of Germany had "sunk to the lowest level in a hundred years". He stated in one report:

On Hoover's initiative, a school meals program in the American and British occupation zones of Germany was begun on April 14, 1947. The program served 3,500,000 children aged six through 18. A total of 40,000 tons of American food was provided during the "Hooverspeisung" (Hoover meals).

In 1947, Truman appointed Hoover to a commission, which elected him chairman, to reorganize the executive departments. This became known as the Hoover Commission. Led by Hoover, the commission recommended changes designed to strengthen the president's ability to manage the federal government. Though Hoover had opposed FDR's concentration of power in the 1930s, he believed that a stronger presidency was required with the advent of the Atomic Age. In 1953, Hoover was appointed to a similar commission by President Dwight D. Eisenhower. Despite the appointment, Hoover disliked Eisenhower, faulting the latter's failure to roll back the New Deal.

Despite his advancing years, Hoover continued to work nearly full-time both on writing (among his literary works is "The Ordeal of Woodrow Wilson", a bestseller, and the first time one former president had ever written a biography about another), as well as overseeing the Hoover Institution at Stanford University, which housed not only his own professional papers, but also those of a number of other former high-ranking governmental and military servants. He also threw himself into fund-raising for the Boys Clubs (now the Boys & Girls Clubs of America), which became his pet charity.

In 1958, Congress passed the Former Presidents Act, offering a $25,000 yearly pension to each former president. Hoover took the pension even though he did not need the money; reportedly, he did so to avoid embarrassing Truman, who was the only other living former president and whose precarious financial status played a role in the law's enactment.

Hoover was the only living former Republican president between his last day in office in 1933 and Eisenhower's last day in office in 1961. Starting with the 1948 convention, Hoover was feted as the guest of "farewell" ceremonies, with the unspoken assumption that the aging former president might not survive until the next convention. In 1960, Hoover appeared at his final Republican National Convention. Joking to the delegates, he said, "Apparently, my last three good-byes didn't take." Although he lived to see the 1964 convention, ill health prevented him from attending, and his absence was acknowledged in presidential nominee Barry Goldwater's acceptance speech. In 1962, Hoover had a malignant intestinal tumor removed. Ten months later he had severe gastrointestinal bleeding and seemed terminally ill and frail, but his mind was clear and he maintained a great deal of correspondence. Although the illness would get worse over time, he refused to be hospitalized.
Hoover died following massive internal bleeding at the age of 90 in his New York City suite at 11:35 a.m. on October 20, 1964, 31 years, seven months, and sixteen days after leaving office. At the time of his death, he had the longest retirement of any president. Former President Jimmy Carter surpassed the length of Hoover's retirement on September 7, 2012. At the time of Hoover's death he was the second longest-lived president after John Adams; both were since surpassed by George H. W. Bush, Gerald Ford, Ronald Reagan, and Jimmy Carter. He had outlived by 20 years his wife, Lou Henry Hoover, who had died in 1944, and he was the last living member of both the Harding and Coolidge administrations. He had also outlived John F. Kennedy, who was assassinated nearly a year prior.

By the time of his death, he had rehabilitated his image. His birthplace in Iowa and an Oregon home where he lived as a child, became National Landmarks during his lifetime. His Rapidan fishing camp in Virginia, which he had donated to the government in 1933, is now a National Historic Landmark within the Shenandoah National Park. Hoover and his wife are buried at the Herbert Hoover Presidential Library and Museum in West Branch, Iowa. Hoover was honored with a state funeral, the last of three in a span of 12 months, coming as it did just after the deaths of President John F. Kennedy and General Douglas MacArthur. Former Chaplain of the Senate Frederick Brown Harris officiated. All three had two things in common: the commanding general of the Military District of Washington during those funerals was Army Major General Philip C. Wehle and the riderless horse was Black Jack, who also served in that role during Lyndon B. Johnson's funeral.

In 1912 Hoover published the first English edition of the medieval mining compendium "De Re Metallica" ("On the Nature of Metals"). It was translated from Latin by himself and his wife, who was a geologist and proficient in Latin. It remains the standard English translation.

After working with the Food Administration, Hoover became president of the Federated American Engineering Societies. Hoover's design was set on increasing efficiency and reducing waste. As president of the FAES, Hoover constructed a plan to study waste in the industrial sector, and he had a strong focus on labor matters. The result was a 400-page report titled "Waste in Industry", which was a highly publicized report at the time. When Hoover became Commerce Secretary, he resigned from the FAES. "Waste in Industry" served as a blueprint for the Department of Commerce for the next several years.

Hoover began his magnum opus "Freedom Betrayed" in 1944 as part of a proposed autobiography. This turned into a significant work critiquing the foreign policy of the United States during the period from the 1930s to 1945. Essentially an attack on the statesmanship of Franklin D. Roosevelt, Hoover completed this work in his 90th year but it was not published until the historian George H. Nash took on the task of editing it. Significant themes are his belief that the western democratic powers should have let Nazi Germany and Soviet Russia assail and weaken each other, and opposition to the British guarantee of Poland's independence. Other works include:

The Herbert Hoover Presidential Library and Museum is located in West Branch, Iowa next to the Herbert Hoover National Historic Site. The library is one of thirteen presidential libraries run by the National Archives and Records Administration. The Hoover-Minthorn House, where Hoover lived from 1885 to 1891, is located in Newberg, Oregon.

The Lou Henry and Herbert Hoover House, built in 1919 in Stanford, California, is now the official residence of the president of Stanford University, and a National Historic Landmark. Also located at Stanford is the Hoover Institution, a think tank and research institution started by Hoover.

Hoover's rustic rural presidential retreat, Rapidan Camp (also known as Camp Hoover) in the Shenandoah National Park, Virginia, has been restored and opened to the public. The Hoover Dam is named in his honor, as are numerous elementary, middle, and high schools across the United States.

The Polish capital of Warsaw also has a square named after Hoover alongside the Royal Route leading to the Old Town.

The historic townsite of Gwalia, Western Australia contains the Sons of Gwalia Museum and the Hoover House Bed and Breakfast, the renovated and restored Mining Engineers residence that was the original residence of Herbert Hoover and where he stayed in subsequent visits to the mine during the first decade of the twentieth century.

Hoover was made an honorary citizen of Tallinn, the capital of Estonia. (1921).

Works cited




</doc>
<doc id="13684" url="https://en.wikipedia.org/wiki?curid=13684" title="Hildegard of Bingen">
Hildegard of Bingen

Hildegard of Bingen (; ; 1098 – 17 September 1179), also known as Saint Hildegard and Sibyl of the Rhine, was a German Benedictine abbess, writer, composer, philosopher, Christian mystic, visionary, and polymath. She is considered to be the founder of scientific natural history in Germany.

Hildegard was elected "magistra" by her fellow nuns in 1136; she founded the monasteries of Rupertsberg in 1150 and Eibingen in 1165. One of her works as a composer, the "Ordo Virtutum", is an early example of liturgical drama and arguably the oldest surviving morality play. She wrote theological, botanical, and medicinal texts, as well as letters, liturgical songs, and poems, while supervising miniature illuminations in the Rupertsberg manuscript of her first work, "Scivias". She is also noted for the invention of a constructed language known as "Lingua Ignota".

Although the history of her formal consideration is complicated, she has been recognized as a saint by branches of the Roman Catholic Church for centuries. On 7 October 2012, Pope Benedict XVI named her a Doctor of the Church.

Hildegard was born around the year 1098, although the exact date is uncertain. Her parents were Mechtild of Merxheim-Nahet and Hildebert of Bermersheim, a family of the free lower nobility in the service of the Count Meginhard of Sponheim. Sickly from birth, Hildegard is traditionally considered their youngest and tenth child, although there are records of seven older siblings. In her "Vita", Hildegard states that from a very young age she had experienced visions.

Perhaps because of Hildegard's visions, or as a method of political positioning (or both), Hildegard's parents offered her as an oblate to the Benedictine monastery at the Disibodenberg, which had been recently reformed in the Palatinate Forest. The date of Hildegard's enclosure at the monastery is the subject of debate. Her "Vita" says she was professed with an older woman, Jutta, the daughter of Count Stephan II of Sponheim, at the age of eight. However, Jutta's date of enclosure is known to have been in 1112, when Hildegard would have been fourteen. Their vows were received by Bishop Otto Bamberg on All Saints' Day, 1112. Some scholars speculate that Hildegard was placed in the care of Jutta at the age of eight, and the two women were then enclosed together six years later.

In any case, Hildegard and Jutta were enclosed together at the Disibodenberg, and formed the core of a growing community of women attached to the male monastery. Jutta was also a visionary and thus attracted many followers who came to visit her at the cloister. Hildegard tells us that Jutta taught her to read and write, but that she was unlearned and therefore incapable of teaching Hildegard sound biblical interpretation. The written record of the "Life of Jutta" indicates that Hildegard probably assisted her in reciting the psalms, working in the garden and other handiwork, and tending to the sick. This might have been a time when Hildegard learned how to play the ten-stringed psaltery. Volmar, a frequent visitor, may have taught Hildegard simple psalm notation. The time she studied music could have been the beginning of the compositions she would later create.

Upon Jutta's death in 1136, Hildegard was unanimously elected as "magistra" of the community by her fellow nuns. Abbot Kuno of Disibodenberg asked Hildegard to be Prioress, which would be under his authority. Hildegard, however, wanted more independence for herself and her nuns, and asked Abbot Kuno to allow them to move to Rupertsberg. This was to be a move towards poverty, from a stone complex that was well established to a temporary dwelling place. When the abbot declined Hildegard's proposition, Hildegard went over his head and received the approval of Archbishop Henry I of Mainz. Abbot Kuno did not relent until Hildegard was stricken by an illness that kept her paralyzed and unable to move from her bed, an event that she attributed to God's unhappiness at her not following his orders to move her nuns to Rupertsberg. It was only when the Abbot himself could not move Hildegard that he decided to grant the nuns their own monastery. Hildegard and about twenty nuns thus moved to the St. Rupertsberg monastery in 1150, where Volmar served as provost, as well as Hildegard's confessor and scribe. In 1165 Hildegard founded a second monastery for her nuns at Eibingen.

Hildegard said that she first saw "The Shade of the Living Light" at the age of three, and by the age of five she began to understand that she was experiencing visions. She used the term 'visio' to this feature of her experience, and recognized that it was a gift that she could not explain to others. Hildegard explained that she saw all things in the light of God through the five senses: sight, hearing, taste, smell, and touch. Hildegard was hesitant to share her visions, confiding only to Jutta, who in turn told Volmar, Hildegard's tutor and, later, secretary. Throughout her life, she continued to have many visions, and in 1141, at the age of 42, Hildegard received a vision she believed to be an instruction from God, to "write down that which you see and hear." Still hesitant to record her visions, Hildegard became physically ill. The illustrations recorded in the book of Scivias were visions that Hildegard experienced, causing her great suffering and tribulations. In her first theological text, "Scivias" ("Know the Ways"), Hildegard describes her struggle within:

But I, though I saw and heard these things, refused to write for a long time through doubt and bad opinion and the diversity of human words, not with stubbornness but in the exercise of humility, until, laid low by the scourge of God, I fell upon a bed of sickness; then, compelled at last by many illnesses, and by the witness of a certain noble maiden of good conduct [the nun Richardis von Stade] and of that man whom I had secretly sought and found, as mentioned above, I set my hand to the writing. While I was doing it, I sensed, as I mentioned before, the deep profundity of scriptural exposition; and, raising myself from illness by the strength I received, I brought this work to a close – though just barely – in ten years. (...) And I spoke and wrote these things not by the invention of my heart or that of any other person, but as by the secret mysteries of God I heard and received them in the heavenly places. And again I heard a voice from Heaven saying to me, 'Cry out therefore, and write thus!'

It was between November 1147 and February 1148 at the synod in Trier that Pope Eugenius heard about Hildegard's writings. It was from this that she received Papal approval to document her visions as revelations from the Holy Spirit giving her instant credence.

Before Hildegard's death, a problem arose with the clergy of Mainz. A man buried in Rupertsburg had died after excommunication from the Church. Therefore, the clergy wanted to remove his body from the sacred ground. Hildegard did not accept this idea, replying that it was a sin and that the man had been reconciled to the church at the time of his death.

On 17 September 1179, when Hildegard died, her sisters claimed they saw two streams of light appear in the skies and cross over the room where she was dying.

Hildegard's hagiography, "Vita Sanctae Hildegardis", was compiled by the monk Theoderic of Echternach after Hildegard's death. He included the hagiographical work "Libellus" or "Little Book" begun by Godfrey of Disibodenberg. Godfrey had died before he was able to complete his work. Guibert of Gembloux was invited to finish the work; however, he had to return to his monastery with the project unfinished. Theoderic utilized sources Guibert had left behind to complete the "Vita".

Hildegard's works include three great volumes of visionary theology; a variety of musical compositions for use in liturgy, as well as the musical morality play "Ordo Virtutum"; one of the largest bodies of letters (nearly 400) to survive from the Middle Ages, addressed to correspondents ranging from popes to emperors to abbots and abbesses, and including records of many of the sermons she preached in the 1160s and 1170s; two volumes of material on natural medicine and cures; an invented language called the "Lingua ignota" ("unknown language"); and various minor works, including a gospel commentary and two works of hagiography.

Several manuscripts of her works were produced during her lifetime, including the illustrated Rupertsberg manuscript of her first major work, "Scivias" (lost since 1945); the Dendermonde Codex, which contains one version of her musical works; and the Ghent manuscript, which was the first fair-copy made for editing of her final theological work, the "Liber Divinorum Operum". At the end of her life, and probably under her initial guidance, all of her works were edited and gathered into the single Riesenkodex manuscript.

Hildegard's most significant works were her three volumes of visionary theology: "Scivias" ("Know the Ways", composed 1142–1151), "Liber Vitae Meritorum" ("Book of Life's Merits" or "Book of the Rewards of Life", composed 1158–1163); and "Liber Divinorum Operum" ("Book of Divine Works", also known as "De operatione Dei", "On God's Activity", composed 1163/4–1172 or 1174). In these volumes, the last of which was completed when she was well into her seventies, Hildegard first describes each vision, whose details are often strange and enigmatic; and then interprets their theological contents in the words of the "voice of the Living Light."

The composition of the first work, "Scivias", was triggered by the insistence of her visionary experiences in about 1142, when she was already forty-three years old. Perceiving a divine command to "write down what you see and hear", Hildegard began to record her visionary experiences. "Scivias" is structured into three parts of unequal length. The first part (six visions) chronicles the order of God's creation: the Creation and Fall of Adam and Eve, the structure of the universe (famously described as the shape of an "egg"), the relationship between body and soul, God's relationship to his people through the Synagogue, and the choirs of angels. The second part (seven visions) describes the order of redemption: the coming of Christ the Redeemer, the Trinity, the Church as the Bride of Christ and the Mother of the Faithful in baptism and confirmation, the orders of the Church, Christ's sacrifice on the Cross and the Eucharist, and the fight against the devil. Finally, the third part (thirteen visions) recapitulates the history of salvation told in the first two parts, symbolized as a building adorned with various allegorical figures and virtues. It concludes with the Symphony of Heaven, an early version of Hildegard's musical compositions.

Portions of the uncompleted work were read aloud to Pope Eugenius III at the Synod of Trier in 1148, after which he sent Hildegard a letter with his blessing. This blessing was later construed as papal approval for all of Hildegard's wide-ranging theological activities. Towards the end of her life, Hildegard commissioned a richly decorated manuscript of "Scivias" (the Rupertsberg Codex); although the original has been lost since its evacuation to Dresden for safekeeping in 1945, its images are preserved in a hand-painted facsimile from the 1920s.

In her second volume of visionary theology, composed between 1158 and 1163, after she had moved her community of nuns into independence at the Rupertsberg in Bingen, Hildegard tackled the moral life in the form of dramatic confrontations between the virtues and the vices. She had already explored this area in her musical morality play, "Ordo Virtutum", and the "Book of the Rewards of Life" takes up that play's characteristic themes. Each vice, although ultimately depicted as ugly and grotesque, nevertheless offers alluring, seductive speeches that attempt to entice the unwary soul into their clutches. Standing in our defense, however, are the sober voices of the Virtues, powerfully confronting every vicious deception.

Amongst the work's innovations is one of the earliest descriptions of purgatory as the place where each soul would have to work off its debts after death before entering heaven. Hildegard's descriptions of the possible punishments there are often gruesome and grotesque, which emphasize the work's moral and pastoral purpose as a practical guide to the life of true penance and proper virtue.

Hildegard's last and grandest visionary work had its genesis in one of the few times she experienced something like an ecstatic loss of consciousness. As she described it in an autobiographical passage included in her Vita, sometime in about 1163, she received "an extraordinary mystical vision" in which was revealed the "sprinkling drops of sweet rain" that John the Evangelist experienced when he wrote, "In the beginning was the Word..." (John 1:1). Hildegard perceived that this Word was the key to the "Work of God", of which humankind is the pinnacle. The "Book of Divine Works", therefore, became in many ways an extended explication of the Prologue to John's Gospel.

The ten visions of this work's three parts are cosmic in scale, often populated by the grand allegorical female figures representing Divine Love ("Caritas") or Wisdom ("Sapientia"). The first of these opens the work with a salvo of poetic and visionary images, swirling about to characterize the dynamic activity of God within the scope of his salvation-historical work. The remaining three visions of the first part introduce the famous image of a human being standing astride the spheres that make up the universe, and detail the intricate relationships between the human as microcosm and the universe as macrocosm. This culminates in the final chapters of Part One, Vision Four with Hildegard's direct rumination on the meaning of "In the beginning was the Word..." (John 1:1). The single vision that comprises the whole of Part Two stretches that rumination back to the opening of Genesis, and forms an extended meditation on the six days of the creation of the world. Finally, the five visions of the third part take up again the building imagery of "Scivias" to describe the course of salvation history.

Attention in recent decades to women of the medieval Church has led to a great deal of popular interest in Hildegard's music. In addition to the "Ordo Virtutum," sixty-nine musical compositions, each with its own original poetic text, survive, and at least four other texts are known, though their musical notation has been lost. This is one of the largest repertoires among medieval composers.
One of her better known works, "Ordo Virtutum" ("Play of the Virtues"), is a morality play. It is uncertain when some of Hildegard's compositions were composed, though the "Ordo Virtutum" is thought to have been composed as early as 1151. The morality play consists of monophonic melodies for the Anima (human soul) and 16 Virtues. There is also one speaking part for the Devil. Scholars assert that the role of the Devil would have been played by Volmar, while Hildegard's nuns would have played the parts of Anima and the Virtues.

In addition to the "Ordo Virtutum", Hildegard composed many liturgical songs that were collected into a cycle called the "Symphonia armoniae celestium revelationum". The songs from the Symphonia are set to Hildegard's own text and range from antiphons, hymns, and sequences, to responsories. Her music is described as monophonic, that is, consisting of exactly one melodic line. Its style is characterized by soaring melodies that can push the boundaries of the more staid ranges of traditional Gregorian chant. Though Hildegard's music is often thought to stand outside the normal practices of monophonic monastic chant, current researchers are also exploring ways in which it may be viewed in comparison with her contemporaries, such as Hermannus Contractus. Another feature of Hildegard's music that both reflects twelfth-century evolutions of chant and pushes those evolutions further is that it is highly melismatic, often with recurrent melodic units. Scholars such as Margot Fassler, Marianne Richert Pfau, and Beverly Lomer also note the intimate relationship between music and text in Hildegard's compositions, whose rhetorical features are often more distinct than is common in twelfth-century chant. As with all medieval chant notation, Hildegard's music lacks any indication of tempo or rhythm; the surviving manuscripts employ late German style notation, which uses very ornamental neumes. The reverence for the Virgin Mary reflected in music shows how deeply influenced and inspired Hildegard of Bingen and her community were by the Virgin Mary and the saints.

The definition of viriditas or "greenness" is an earthly expression of the heavenly in an integrity that overcomes dualisms. This greenness or power of life appears frequently in Hildegard's works.

Despite Hildegard's self-professed view that her compositions have as object the praise of God, one scholar has asserted that Hildegard made a close association between music and the female body in her musical compositions. According to him, the poetry and music of Hildegard's Symphonia would therefore be concerned with the anatomy of female desire thus described as Sapphonic, or pertaining to Sappho, connecting her to a history of female rhetoricians.

Hildegard's medicinal and scientific writings, though thematically complementary to her ideas about nature expressed in her visionary works, are different in focus and scope. Neither claim to be rooted in her visionary experience and its divine authority. Rather, they spring from her experience helping in and then leading the monastery's herbal garden and infirmary, as well as the theoretical information she likely gained through her wide-ranging reading in the monastery's library. As she gained practical skills in diagnosis, prognosis, and treatment, she combined physical treatment of physical diseases with holistic methods centered on "spiritual healing." She became well known for her healing powers involving practical application of tinctures, herbs, and precious stones. She combined these elements with a theological notion ultimately derived from Genesis: all things put on earth are for the use of humans. In addition to her hands-on experience, she also gained medical knowledge, including elements of her humoral theory, from traditional Latin texts.

Hildegard catalogued both her theory and practice in two works. The first, "Physica," contains nine books that describe the scientific and medicinal properties of various plants, stones, fish, reptiles, and animals. The second, "Causae et Curae", is an exploration of the human body, its connections to the rest of the natural world, and the causes and cures of various diseases. Hildegard documented various medical practices in these books, including the use of bleeding and home remedies for many common ailments. She also explains remedies for common agricultural injuries such as burns, fractures, dislocations, and cuts. Hildegard may have used the books to teach assistants at the monastery. These books are historically significant because they show areas of medieval medicine that were not well documented because their practitioners (mainly women) rarely wrote in Latin. Her writings were commentated on by Mélanie Lipinska, a Polish scientist

In addition to its wealth of practical evidence, "Causae et Curae" is also noteworthy for its organizational scheme. Its first part sets the work within the context of the creation of the cosmos and then humanity as its summit, and the constant interplay of the human person as microcosm both physically and spiritually with the macrocosm of the universe informs all of Hildegard's approach. Her hallmark is to emphasize the vital connection between the "green" health of the natural world and the holistic health of the human person. "Viriditas", or greening power, was thought to sustain human beings and could be manipulated by adjusting the balance of elements within a person. Thus, when she approached medicine as a type of gardening, it was not just as an analogy. Rather, Hildegard understood the plants and elements of the garden as direct counterparts to the humors and elements within the human body, whose imbalance led to illness and disease.

Thus, the nearly three hundred chapters of the second book of "Causae et Curae" "explore the etiology, or causes, of disease as well as human sexuality, psychology, and physiology." In this section, she give specific instructions for bleeding based on various factors, including gender, the phase of the moon (bleeding is best done when moon is waning), the place of disease (use veins near diseased organ of body part) or prevention (big veins in arms), and how much blood to take (described in imprecise measurements, like "the amount that a thirsty person can swallow in one gulp"). She even includes bleeding instructions for animals to keep them healthy. In the third and fourth sections, Hildegard describes treatments for malignant and minor problems and diseases according to the humoral theory, again including information on animal health. The fifth section is about diagnosis and prognosis, which includes instructions to check the patient's blood, pulse, urine and stool. Finally, the sixth section documents a lunar horoscope to provide an additional means of prognosis for both disease and other medical conditions, such as conception and the outcome of pregnancy. For example, she indicates that a waxing moon is good for human conception and is also good for sowing seeds for plants (sowing seeds is the plant equivalent of conception). Elsewhere, Hildegard is even said to have stressed the value of boiling drinking water in an attempt to prevent infection.

As Hildegard elaborates the medical and scientific relationship between the human microcosm and the macrocosm of the universe, she often focuses on interrelated patterns of four: "the four elements (fire, air, water, and earth), the four seasons, the four humors, the four zones of the earth, and the four major winds." Although she inherited the basic framework of humoral theory from ancient medicine, Hildegard's conception of the hierarchical inter-balance of the four humors (blood, phlegm, black bile, and yellow bile) was unique, based on their correspondence to "superior" and "inferior" elements—blood and phlegm corresponding to the "celestial" elements of fire and air, and the two biles corresponding to the "terrestrial" elements of water and earth. Hildegard understood the disease-causing imbalance of these humors to result from the improper dominance of the subordinate humors. This disharmony reflects that introduced by Adam and Eve in the Fall, which for Hildegard marked the indelible entrance of disease and humoral imbalance into humankind. As she writes in "Causae et Curae" c. 42:

It happens that certain men suffer diverse illnesses. This comes from the phlegm which is superabundant within them. For if man had remained in paradise, he would not have had the "flegmata" within his body, from which many evils proceed, but his flesh would have been whole and without dark humor ["livor"]. However, because he consented to evil and relinquished good, he was made into a likeness of the earth, which produces good and useful herbs, as well as bad and useless ones, and which has in itself both good and evil moistures. From tasting evil, the blood of the sons of Adam was turned into the poison of semen, out of which the sons of man are begotten. And therefore their flesh is ulcerated and permeable [to disease]. These sores and openings create a certain storm and smoky moisture in men, from which the "flegmata" arise and coagulate, which then introduce diverse infirmities to the human body. All this arose from the first evil, which man began at the start, because if Adam had remained in paradise, he would have had the sweetest health, and the best dwelling-place, just as the strongest balsam emits the best odor; but on the contrary, man now has within himself poison and phlegm and diverse illnesses.

Hildegard also invented an alternative alphabet. The text of her writing and compositions reveals Hildegard's use of this form of modified medieval Latin, encompassing many invented, conflated and abridged words. Because of her inventions of words for her lyrics and use of a constructed script, many conlangers look upon her as a medieval precursor. Scholars believe that Hildegard used her "Lingua Ignota" to increase solidarity among her nuns.

Maddocks claims that it is likely Hildegard learned simple Latin and the tenets of the Christian faith but was not instructed in the Seven Liberal Arts, which formed the basis of all education for the learned classes in the Middle Ages: the "Trivium" of grammar, dialectic, and rhetoric plus the "Quadrivium" of arithmetic, geometry, astronomy, and music. The correspondence she kept with the outside world, both spiritual and social, transcended the cloister as a space of spiritual confinement and served to document Hildegard's grand style and strict formatting of medieval letter writing.

Contributing to Christian European rhetorical traditions, Hildegard "authorized herself as a theologian" through alternative rhetorical arts. Hildegard was creative in her interpretation of theology. She believed that her monastery should exclude novices who were not from the nobility because she did not want her community to be divided on the basis of social status. She also stated that "woman may be made from man, but no man can be made without a woman."
Because of church limitation on public, discursive rhetoric, the medieval rhetorical arts included preaching, letter writing, poetry, and the encyclopedic tradition. Hildegard's participation in these arts speaks to her significance as a female rhetorician, transcending bans on women's social participation and interpretation of scripture. The acceptance of public preaching by a woman, even a well-connected abbess and acknowledged prophet, does not fit the stereotype of this time. Her preaching was not limited to the monasteries; she preached publicly in 1160 in Germany. (New York: Routledge, 2001, 9). She conducted four preaching tours throughout Germany, speaking to both clergy and laity in chapter houses and in public, mainly denouncing clerical corruption and calling for reform.

Many abbots and abbesses asked her for prayers and opinions on various matters. She traveled widely during her four preaching tours. She had several fanatical followers, including Guibert of Gembloux, who wrote to her frequently and became her secretary after Volmar's death in 1173. Hildegard also influenced several monastic women, exchanging letters with Elisabeth of Schönau, a nearby visionary.

Hildegard corresponded with popes such as Eugene III and Anastasius IV, statesmen such as Abbot Suger, German emperors such as Frederick I Barbarossa, and other notable figures such as Saint Bernard of Clairvaux, who advanced her work, at the behest of her abbot, Kuno, at the Synod of Trier in 1147 and 1148. Hildegard of Bingen's correspondence is an important component of her literary output.

Hildegard was one of the first persons for whom the Roman canonization process was officially applied, but the process took so long that four attempts at canonization were not completed and she remained at the level of her beatification. Her name was nonetheless taken up in the Roman Martyrology at the end of the 16th century. Her feast day is 17 September. Numerous popes have referred to Hildegard as a saint, including Pope John Paul II and Pope Benedict XVI.

On 10 May 2012, Pope Benedict XVI extended the liturgical cult of St. Hildegard to the entire Catholic Church in a process known as "equivalent canonization," thus laying the groundwork for naming her a Doctor of the Church. On 7 October 2012, the feast of the Holy Rosary, the pope named her a Doctor of the Church, the fourth woman of 35 saints given that title by the Roman Catholic Church. He called her "perennially relevant" and "an authentic teacher of theology and a profound scholar of natural science and music."

Hildegard of Bingen also appears in the calendar of saints of various Anglican churches, such as that of the Church of England, in which she is commemorated on 17 September.

Hildegard's parish and pilgrimage church in Eibingen near Rüdesheim houses her relics.

In recent years, Hildegard has become of particular interest to feminist scholars. They note her reference to herself as a member of the weaker sex and her rather constant belittling of women. Hildegard frequently referred to herself as an unlearned woman, completely incapable of Biblical exegesis. Such a statement on her part, however, worked to her advantage because it made her statements that all of her writings and music came from visions of the Divine more believable, therefore giving Hildegard the authority to speak in a time and place where few women were permitted a voice. Hildegard used her voice to amplify the Church's condemnation of institutional corruption, in particular simony.

Hildegard has also become a figure of reverence within the contemporary New Age movement, mostly because of her holistic and natural view of healing, as well as her status as a mystic. Though her medical writings were long neglected, and then studied without reference to their context, she was the inspiration for Dr. Gottfried Hertzka's "Hildegard-Medicine", and is the namesake for June Boyce-Tillman's Hildegard Network, a healing center that focuses on a holistic approach to wellness and brings together people interested in exploring the links between spirituality, the arts, and healing. Her reputation as a medicinal writer and healer was also used by early feminists to argue for women's rights to attend medical schools. Hildegard's reincarnation has been debated since 1924 when Austrian mystic Rudolf Steiner lectured that a nun of her description was the past life of Russian poet philosopher Vladimir Soloviev, whose Sophianic visions are often compared to Hildegard's. Sophiologist Robert Powell writes that hermetic astrology proves the match, while mystical communities in Hildegard's lineage include that of artist Carl Schroeder as studied by Columbia sociologist Courtney Bender and supported by reincarnation researchers Walter Semkiw and Kevin Ryerson.

Recordings and performances of Hildegard's music have gained critical praise and popularity since 1979. See Discography listed below.

The following modern musical works are directly linked to Hildegard and her music or texts:

The artwork "The Dinner Party" features a place setting for Hildegard.

In space, the minor planet 898 Hildegard is named for her.

In film, Hildegard has been portrayed by Patricia Routledge in a BBC documentary called "Hildegard of Bingen" (1994), by Ángela Molina in "Barbarossa" (2009) and by Barbara Sukowa in the film "Vision", directed by Margarethe von Trotta.

Hildegard was the subject of a 2012 fictionalized biographic novel "Illuminations" by Mary Sharratt.

The plant genus "Hildegardia" is named after her because of her contributions to herbal medicine.

Hildegard makes an appearance in "The Baby-Sitters Club #101: Claudia Kishi, Middle School Drop-Out" by Ann M. Martin, when Anna Stevenson dresses as Hildegard for Halloween. 

A feature documentary film, "," was released by American director Michael M. Conti in 2014.





Primary Sources (in translation):
Hildegard of Bingen. 

Secondary Sources:



</doc>
<doc id="13686" url="https://en.wikipedia.org/wiki?curid=13686" title="Hilversum">
Hilversum

Hilversum () is a city and municipality in the province of North Holland, Netherlands. Located in the heart of the Gooi, it is the largest urban centre in that area. It is surrounded by heathland, woods, meadows, lakes, and smaller towns. Hilversum is part of the Randstad, one of the largest conurbations in Europe.

Hilversum lies south-east of Amsterdam and north of Utrecht. The town is known for its architecturally important Town Hall (Raadhuis Hilversum), designed by Willem Marinus Dudok and built in 1931.

Hilversum has one public library, two swimming pools (Van Hellemond Sport and De Lieberg), a number of sporting halls and several shopping centres (such as Hilvertshof, De Gijsbrecht, Kerkelanden, Riebeeck Galerij and Seinhorst.) Locally, the town centre is known as "het dorp", which means "the village".

Hilversum is often called "media city", since it is the principal centre for radio and television broadcasting in the Netherlands, and is home to an extensive complex of radio and television studios and to the administrative headquarters of the multiple broadcasting organizations which make up the Netherlands Public Broadcasting system. Hilversum is also home to many newer commercial TV production companies. Radio Netherlands, which has been broadcasting worldwide via shortwave radio since the 1920s, is also based here.

The following is a list of organizations that have, or are continuing to, broadcast from studios in Hilversum:


One result of the town's history as an important radio transmission centre is that many older radio sets throughout Europe featured "Hilversum" as a pre-marked dial position on their tuning scales.

Dutch national voting in the Eurovision Song Contest is normally co-ordinated from Hilversum.

Earthenware found in Hilversum gives its name to the Hilversum culture, which is an early- to mid-Bronze Age, or 800–1200 BCE material culture. Artifacts from this prehistoric civilization bear similarities to the Wessex Culture of southern Britain and may indicate that the first Hilversum residents emigrated from that area. The first brick settlements formed around 900, but it was not until 1305 that the first official mention of Hilversum ("Hilfersheem" from "Hilvertshem" meaning "houses between the hills") is found. At that point it was a part of Naarden, the oldest town in the Gooi area.
Farming, raising sheep and some wool manufacturing were the means of life for the Gooi in the Middle Ages. In 1424, on 21 March at 6:30 am (the hour at which people got up, as the farm was full of restless and loud animals), Hilversum received its first official independent status. This made possible further growth in the village because permission from Naarden was no longer needed for new industrial development. The town grew further in the 17th century when the Dutch economy as a whole entered its age of prosperity, and several canals were built connecting it indirectly to Amsterdam. In 1725 and 1766 large fires destroyed most of the town, leveling parts of the old townhouse and the church next to it. The town overcame these setbacks and the textile industry continued to develop, among other ways by devising a way to weave cows' hair. In the 19th century a substantial textile and tapestry industry emerged, aided by a railway link to Amsterdam in 1874. From that time the town grew quickly with rich commuters from Amsterdam moving in, building themselves large villas in the wooded surroundings, and gradually starting to live in Hilversum permanently. Despite this growth, Hilversum was never granted city rights so it is still referred to by many locals as "het dorp," or "the village."

For the 1928 Summer Olympics in neighboring Amsterdam, it hosted all of the non-jumping equestrian and the running part of the modern pentathlon event. The city was the headquarters of the German ground forces (Heer) in the Netherlands .

The "Nederlandse Seintoestellen Fabriek" (NSF) company established a professional transmitter and radio factory in Hilversum in the early 1920s, growing into the largest of its kind in the Netherlands, and in 1948 being taken over by Philips. By then the textile industry had started its decline; only one factory, Veneta, managed to continue into the 1960s, when it also had to close its doors. Another major industry, the chemical factory IFF, also closed by the end of the 1960s. In the meantime, almost all Dutch radio broadcasting organizations (followed by television broadcasters in the 1950s) established their headquarters in Hilversum and provided a source of continuing economic growth. The concentration of broadcasters in Hilversum has given it its enduring status as the media city for the Netherlands.

In 1964, the population reached a record high – over 103,000 people called Hilversum home. The current population hovers around 85,000. Several factors figure into the decline: one is the fact that the average family nowadays consists of fewer people, so fewer people live in each house; second, the town is virtually unable to expand because all the surrounding lands were sold by city architect W.M. Dudok to the Goois Natuurreservaat (""). The third reason for this decline of the population was because the property values were increasing rapidly in that moment of time, and many people were forced to move to less expensive areas in the Netherlands.

Some sources blame connections in the television world for attracting crime to Hilversum; the town has had to cope with mounting drug-related issues in a community with higher than average unemployment and ongoing housing shortage.

Hilversum was one of the first towns to have a local party of the populist movement called "Leefbaar" ("liveable"). Founded by former social-democrat party strongman Jan Nagel, it was initially held at bay for alderman positions. In 2001, Nagel from 'Leefbaar Hilversum' teamed up with 'Leefbaar Utrecht' leaders to found a national 'Leefbaar Nederland' party. By strange coincidence, in 2002 the most vocal 'Leefbaar Rotterdam' politician Pim Fortuyn was shot and killed by an animal rights activist at Hilversum Media Park just after finishing a radio interview. This happened, however, after a break between Fortuyn and Nagel during a Leefbaar Nederland board meeting in Hilversum on Fortuyn's anti-Islamic viewpoints.

The town of Hilversum has put a great deal of effort into improvements, including a recent renovation to its central train station, thorough renovation of the main shopping centre (Hilvertshof), and development of new dining and retail districts downtown including the "vintage" district in the Leeuwenstraat. Several notable architectural accomplishments include the Institute for Sound and Vision, and Zanderij Crailoo (""), the largest man-made wildlife crossing in the world.

The nearby Media Park was the scene of the 2002 assassination of politician Pim Fortuyn; in 2015, a gunman carrying a false pistol stormed into Nederlandse Omroep Stichting's headquarters, demanding airtime on the evening news.

The population declined from 103,000 in 1964 to 84,000 in 2006.

The large Catholic neo-gothic St. Vitus church (P.J.H. Cuypers, 1892, bell tower 96 metres).

The city played host to many landscape artists during the 19th century, including Barend Cornelis Koekkoek.

The 1958 Eurovision Song Contest took place in Hilversum.

Hilversum is well connected to the Dutch railway network, and contains three stations: 

One can get the best connections from the station Hilversum, as this is an Intercity station.

The municipal council of Hilversum consists of 37 seats, which are divided as followed since the last local election of 2014:


Government

After the 2014 elections, the municipal government was made up of eldermen from the political parties D66, VVD, SP and CDA. Hart voor Hilversum originally joined in the negotiations to form the local government, but an agreement could not be reached. Hart voor Hilversum is now the largest party in opposition.

The mayor of Hilversum is Pieter Broertjes, former lead editor of the Volkskrant, a nationwide distributed newspaper.

It was the first city with a "Leefbaar" party (which was intended as just a local party). Today, Leefbaar Hilversum has been reduced to only 1 seat, but some other parties have their origins in Leefbaar Hilversum:

Notable people born in Hilversum:




</doc>
<doc id="13688" url="https://en.wikipedia.org/wiki?curid=13688" title="The Hound of Heaven">
The Hound of Heaven

"The Hound of Heaven" is a 182-line poem written by English poet Francis Thompson (1859–1907). The poem became famous and was the source of much of Thompson's posthumous reputation. The poem was first published in Thompson's first volume of poems in 1893. It was included in the "Oxford Book of English Mystical Verse" (1917). Thompson's work was praised by G. K. Chesterton, and it was also an influence on J. R. R. Tolkien, who presented a paper on Thompson in 1914.

This Christian poem has been described as follows:
"The name is strange. It startles one at first. It is so bold, so new, so fearless. It does not attract, rather the reverse. But when one reads the poem this strangeness disappears. The meaning is understood. As the hound follows the hare, never ceasing in its running, ever drawing nearer in the chase, with unhurrying and imperturbed pace, so does God follow the fleeing soul by His Divine grace. And though in sin or in human love, away from God it seeks to hide itself, Divine grace follows after, unwearyingly follows ever after, till the soul feels its pressure forcing it to turn to Him alone in that never ending pursuit." J.F.X. O'Conor, S.J.





</doc>
<doc id="13692" url="https://en.wikipedia.org/wiki?curid=13692" title="History of the Internet">
History of the Internet

The history of the Internet begins with the development of electronic computers in the 1950s. Initial concepts of wide area networking originated in several computer science laboratories in the United States, United Kingdom, and France. The US Department of Defense awarded contracts as early as the 1960s, including for the development of the ARPANET project, directed by Robert Taylor and managed by Lawrence Roberts. The first message was sent over the ARPANET in 1969 from computer science Professor Leonard Kleinrock's laboratory at University of California, Los Angeles (UCLA) to the second network node at Stanford Research Institute (SRI).

Packet switching networks such as the NPL network, ARPANET, Tymnet, Merit Network, CYCLADES, and Telenet, were developed in the late 1960s and early 1970s using a variety of communications protocols. Donald Davies first demonstrated packet switching in 1967 at the National Physics Laboratory (NPL) in the UK, which became a testbed for UK research for almost two decades. The ARPANET project led to the development of protocols for internetworking, in which multiple separate networks could be joined into a network of networks.

The Internet protocol suite (TCP/IP) was developed by Robert E. Kahn and Vint Cerf in the 1970s and became the standard networking protocol on the ARPANET, incorporating concepts from the French CYCLADES project directed by Louis Pouzin. In the early 1980s the NSF funded the establishment for national supercomputing centers at several universities, and provided interconnectivity in 1986 with the NSFNET project, which also created network access to the supercomputer sites in the United States from research and education organizations. Commercial Internet service providers (ISPs) began to emerge in the very late 1980s. The ARPANET was decommissioned in 1990. Limited private connections to parts of the Internet by officially commercial entities emerged in several American cities by late 1989 and 1990, and the NSFNET was decommissioned in 1995, removing the last restrictions on the use of the Internet to carry commercial traffic.

In the 1980s, research at CERN in Switzerland by British computer scientist Tim Berners-Lee resulted in the World Wide Web, linking hypertext documents into an information system, accessible from any node on the network. Since the mid-1990s, the Internet has had a revolutionary impact on culture, commerce, and technology, including the rise of near-instant communication by electronic mail, instant messaging, voice over Internet Protocol (VoIP) telephone calls, two-way interactive video calls, and the World Wide Web with its discussion forums, blogs, social networking, and online shopping sites. The research and education community continues to develop and use advanced networks such as JANET in the United Kingdom and Internet2 in the United States. Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1-Gbit/s, 10-Gbit/s, or more. The Internet's takeover of the global communication landscape was almost instant in historical terms: it only communicated 1% of the information flowing through two-way telecommunications networks in the year 1993, already 51% by 2000, and more than 97% of the telecommunicated information by 2007. Today the Internet continues to grow, driven by ever greater amounts of online information, commerce, entertainment, and social networking.

The concept of data communication – transmitting data between two different places through an electromagnetic medium such as radio or an electric wire – pre-dates the introduction of the first computers. Such communication systems were typically limited to point to point communication between two end devices. Telegraph systems and telex machines can be considered early precursors of this kind of communication. The Telegraph in the late 19th century was the first fully digital communication system.

Fundamental theoretical work in data transmission and information theory was developed by Claude Shannon, Harry Nyquist, and Ralph Hartley in the early 20th century.

Early computers had a central processing unit and remote terminals. As the technology evolved, new systems were devised to allow communication over longer distances (for terminals) or with higher speed (for interconnection of local devices) that were necessary for the mainframe computer model. These technologies made it possible to exchange data (such as files) between remote computers. However, the point-to-point communication model was limited, as it did not allow for direct communication between any two arbitrary systems; a physical link was necessary. The technology was also considered unsafe for strategic and military use because there were no alternative paths for the communication in case of an enemy attack.

With limited exceptions, the earliest computers were connected directly to terminals used by individual users, typically in the same building or site. Such networks became known as local area networks (LANs). Networking beyond this scope, known as wide area networks (WANs), emerged during the 1950s and became established during the 1960s.

J. C. R. Licklider, Vice President at Bolt Beranek and Newman, Inc., proposed a global network in his January 1960 paper "Man-Computer Symbiosis":

In August 1962, Licklider and Welden Clark published the paper "On-Line Man-Computer Communication" which was one of the first descriptions of a networked future.

In October 1962, Licklider was hired by Jack Ruina as director of the newly established Information Processing Techniques Office (IPTO) within DARPA, with a mandate to interconnect the United States Department of Defense's main computers at Cheyenne Mountain, the Pentagon, and SAC HQ. There he formed an informal group within DARPA to further computer research. He began by writing memos describing a distributed network to the IPTO staff, whom he called "Members and Affiliates of the Intergalactic Computer Network". As part of the information processing office's role, three network terminals had been installed: one for System Development Corporation in Santa Monica, one for Project Genie at University of California, Berkeley, and one for the Compatible Time-Sharing System project at Massachusetts Institute of Technology (MIT). Licklider's identified need for inter-networking would become obvious by the apparent waste of resources this caused.

For each of these three terminals, I had three different sets of user commands. So if I was talking online with someone at S.D.C. and I wanted to talk to someone I knew at Berkeley or M.I.T. about this, I had to get up from the S.D.C. terminal, go over and log into the other terminal and get in touch with them... <br><br>I said, oh man, it's obvious what to do: If you have these three terminals, there ought to be one terminal that goes anywhere you want to go where you have interactive computing. That idea is the ARPAnet.

Although he left the IPTO in 1964, five years before the ARPANET went live, it was his vision of universal networking that provided the impetus for one of his successors, Robert Taylor, to initiate the ARPANET development. Licklider later returned to lead the IPTO in 1973 for two years.

The issue of connecting separate physical networks to form one logical network was the first of many problems. Early networks used message switched systems that required rigid routing structures prone to single point of failure. In the 1960s, Paul Baran of the RAND Corporation produced a study of survivable networks for the U.S. military in the event of nuclear war. Information transmitted across Baran's network would be divided into what he called "message blocks". Independently, Donald Davies (National Physical Laboratory, UK), proposed and was the first to put into practice a local area network based on what he called packet switching, the term that would ultimately be adopted. Larry Roberts applied Davies' concepts of packet switching for the ARPANET wide area network, and sought input from Paul Baran and Leonard Kleinrock. Kleinrock subsequently developed the mathematical theory behind the performance of this technology building on his earlier work on queueing theory.

Packet switching is a rapid store and forward networking design that divides messages up into arbitrary packets, with routing decisions made per-packet. It provides better bandwidth utilization and response times than the traditional circuit-switching technology used for telephony, particularly on resource-limited interconnection links.

Following discussions with J. C. R. Licklider, Donald Davies became interested in data communications for computer networks. At the National Physical Laboratory (United Kingdom) in 1965, Davies designed and proposed a national data network based on packet switching. The following year, he described the use of an "Interface computer" to act as a router. The proposal was not taken up nationally but by 1967, a pilot experiment had demonstrated the feasibility of packet switched networks.

By 1969 he had begun building the Mark I packet-switched network to meet the needs of the multidisciplinary laboratory and prove the technology under operational conditions. In 1976, 12 computers and 75 terminal devices were attached, and more were added until the network was replaced in 1986. NPL, followed by ARPANET, were the first two networks in the world to use packet switching, and were interconnected in the early 1970s.

Robert Taylor was promoted to the head of the information processing office at Defense Advanced Research Projects Agency (DARPA) in June 1966. He intended to realize Licklider's ideas of an interconnected networking system. Bringing in Larry Roberts from MIT, he initiated a project to build such a network. The first ARPANET link was established between the University of California, Los Angeles (UCLA) and the Stanford Research Institute at 22:30 hours on October 29, 1969.

By December 5, 1969, a 4-node network was connected by adding the University of Utah and the University of California, Santa Barbara. Building on ideas developed in ALOHAnet, the ARPANET grew rapidly. By 1981, the number of hosts had grown to 213, with a new host being added approximately every twenty days.

ARPANET development was centered around the Request for Comments (RFC) process, still used today for proposing and distributing Internet Protocols and Systems. RFC 1, entitled "Host Software", was written by Steve Crocker from the University of California, Los Angeles, and published on April 7, 1969. These early years were documented in the 1972 film .

ARPANET became the technical core of what would become the Internet, and a primary tool in developing the technologies used. The early ARPANET used the Network Control Program (NCP, sometimes Network Control Protocol) rather than TCP/IP. On January 1, 1983, known as flag day, NCP on the ARPANET was replaced by the more flexible and powerful family of TCP/IP protocols, marking the start of the modern Internet.

International collaborations on ARPANET were sparse. For various political reasons, European developers were concerned with developing the X.25 networks. Notable exceptions were the "Norwegian Seismic Array" (NORSAR) in 1972, followed in 1973 by Sweden with satellite links to the Tanum Earth Station and Peter Kirstein's research group in the UK, initially at the Institute of Computer Science, London University and later at University College London.

The Merit Network was formed in 1966 as the Michigan Educational Research Information Triad to explore computer networking between three of Michigan's public universities as a means to help the state's educational and economic development. With initial support from the State of Michigan and the National Science Foundation (NSF), the packet-switched network was first demonstrated in December 1971 when an interactive host to host connection was made between the IBM mainframe computer systems at the University of Michigan in Ann Arbor and Wayne State University in Detroit. In October 1972 connections to the CDC mainframe at Michigan State University in East Lansing completed the triad. Over the next several years in addition to host to host interactive connections the network was enhanced to support terminal to host connections, host to host batch connections (remote job submission, remote printing, batch file transfer), interactive file transfer, gateways to the Tymnet and Telenet public data networks, X.25 host attachments, gateways to X.25 data networks, Ethernet attached hosts, and eventually TCP/IP and additional public universities in Michigan join the network. All of this set the stage for Merit's role in the NSFNET project starting in the mid-1980s.

The CYCLADES packet switching network was a French research network designed and directed by Louis Pouzin. First demonstrated in 1973, it was developed to explore alternatives to the early ARPANET design and to support network research generally. It was the first network to make the hosts responsible for reliable delivery of data, rather than the network itself, using unreliable datagrams and associated end-to-end protocol mechanisms. Concepts of this network influenced later ARPANET architecture.

Based on ARPA's research, packet switching network standards were developed by the International Telecommunication Union (ITU) in the form of X.25 and related standards. While using packet switching, X.25 is built on the concept of virtual circuits emulating traditional telephone connections. In 1974, X.25 formed the basis for the SERCnet network between British academic and research sites, which later became JANET. The initial ITU Standard on X.25 was approved in March 1976.

The British Post Office, Western Union International and Tymnet collaborated to create the first international packet switched network, referred to as the International Packet Switched Service (IPSS), in 1978. This network grew from Europe and the US to cover Canada, Hong Kong, and Australia by 1981. By the 1990s it provided a worldwide networking infrastructure.

Unlike ARPANET, X.25 was commonly available for business use. Telenet offered its Telemail electronic mail service, which was also targeted to enterprise use rather than the general email system of the ARPANET.

The first public dial-in networks used asynchronous TTY terminal protocols to reach a concentrator operated in the public network. Some networks, such as CompuServe, used X.25 to multiplex the terminal sessions into their packet-switched backbones, while others, such as Tymnet, used proprietary protocols. In 1979, CompuServe became the first service to offer electronic mail capabilities and technical support to personal computer users. The company broke new ground again in 1980 as the first to offer real-time chat with its CB Simulator. Other major dial-in networks were America Online (AOL) and Prodigy that also provided communications, content, and entertainment features. Many bulletin board system (BBS) networks also provided on-line access, such as FidoNet which was popular amongst hobbyist computer users, many of them hackers and amateur radio operators.

In 1979, two students at Duke University, Tom Truscott and Jim Ellis, originated the idea of using Bourne shell scripts to transfer news and messages on a serial line UUCP connection with nearby University of North Carolina at Chapel Hill. Following public release of the software in 1980, the mesh of UUCP hosts forwarding on the Usenet news rapidly expanded. UUCPnet, as it would later be named, also created gateways and links between FidoNet and dial-up BBS hosts. UUCP networks spread quickly due to the lower costs involved, ability to use existing leased lines, X.25 links or even ARPANET connections, and the lack of strict use policies compared to later networks like CSNET and Bitnet. All connects were local. By 1981 the number of UUCP hosts had grown to 550, nearly doubling to 940 in 1984. – Sublink Network, operating since 1987 and officially founded in Italy in 1989, based its interconnectivity upon UUCP to redistribute mail and news groups messages throughout its Italian nodes (about 100 at the time) owned both by private individuals and small companies. Sublink Network represented possibly one of the first examples of the Internet technology becoming progress through popular diffusion.

With so many different network methods, something was needed to unify them. Robert E. Kahn of DARPA and ARPANET recruited Vinton Cerf of Stanford University to work with him on the problem. By 1973, they had worked out a fundamental reformulation, where the differences between network protocols were hidden by using a common internetwork protocol, and instead of the network being responsible for reliability, as in the ARPANET, the hosts became responsible. Cerf credits Hubert Zimmermann, Gerard LeLann and Louis Pouzin (designer of the CYCLADES network) with important work on this design.

The specification of the resulting protocol, "RFC 675 – Specification of Internet Transmission Control Program", by Vinton Cerf, Yogen Dalal and Carl Sunshine, Network Working Group, December 1974, contains the first attested use of the term "internet", as a shorthand for "internetworking"; later RFCs repeat this use, so the word started out as an adjective rather than the noun it is today.
With the role of the network reduced to the bare minimum, it became possible to join almost any networks together, no matter what their characteristics were, thereby solving Kahn's initial problem. DARPA agreed to fund development of prototype software, and after several years of work, the first demonstration of a gateway between the Packet Radio network in the SF Bay area and the ARPANET was conducted by the Stanford Research Institute. On November 22, 1977 a three network demonstration was conducted including the ARPANET, the SRI's Packet Radio Van on the Packet Radio Network and the Atlantic Packet Satellite network.

Stemming from the first specifications of TCP in 1974, TCP/IP emerged in mid-late 1978 in nearly its final form, as used for the first decades of the Internet, known as "IPv4". which is described in IETF publication RFC 791 (September 1981).
IPv4 uses 32-bit addresses which limits the address space to 2 addresses, i.e. addresses. The last available IPv4 address was assigned in January 2011. IPv4 is being replaced by its successor, called "IPv6", which uses 128 bit addresses, providing 2 addresses, i.e. . This is a vastly increased address space. The shift to IPv6 is expected to take many years, decades, or perhaps longer, to complete, since there were four billion machines with IPv4 when the shift began.

The associated standards for IPv4 were published by 1981 as RFCs 791, 792 and 793, and adopted for use. DARPA sponsored or encouraged the development of TCP/IP implementations for many operating systems and then scheduled a migration of all hosts on all of its packet networks to TCP/IP. On January 1, 1983, known as flag day, TCP/IP protocols became the only approved protocol on the ARPANET, replacing the earlier NCP protocol.

After the ARPANET had been up and running for several years, ARPA looked for another agency to hand off the network to; ARPA's primary mission was funding cutting edge research and development, not running a communications utility. Eventually, in July 1975, the network had been turned over to the Defense Communications Agency, also part of the Department of Defense. In 1983, the U.S. military portion of the ARPANET was broken off as a separate network, the MILNET. MILNET subsequently became the unclassified but military-only NIPRNET, in parallel with the SECRET-level SIPRNET and JWICS for TOP SECRET and above. NIPRNET does have controlled security gateways to the public Internet.

The networks based on the ARPANET were government funded and therefore restricted to noncommercial uses such as research; unrelated commercial use was strictly forbidden. This initially restricted connections to military sites and universities. During the 1980s, the connections expanded to more educational institutions, and even to a growing number of companies such as Digital Equipment Corporation and Hewlett-Packard, which were participating in research projects or providing services to those who were.

Several other branches of the U.S. government, the National Aeronautics and Space Administration (NASA), the National Science Foundation (NSF), and the Department of Energy (DOE) became heavily involved in Internet research and started development of a successor to ARPANET. In the mid-1980s, all three of these branches developed the first Wide Area Networks based on TCP/IP. NASA developed the NASA Science Network, NSF developed CSNET and DOE evolved the Energy Sciences Network or ESNet.
NASA developed the TCP/IP based NASA Science Network (NSN) in the mid-1980s, connecting space scientists to data and information stored anywhere in the world. In 1989, the DECnet-based Space Physics Analysis Network (SPAN) and the TCP/IP-based NASA Science Network (NSN) were brought together at NASA Ames Research Center creating the first multiprotocol wide area network called the NASA Science Internet, or NSI. NSI was established to provide a totally integrated communications infrastructure to the NASA scientific community for the advancement of earth, space and life sciences. As a high-speed, multiprotocol, international network, NSI provided connectivity to over 20,000 scientists across all seven continents.

In 1981 NSF supported the development of the Computer Science Network (CSNET). CSNET connected with ARPANET using TCP/IP, and ran TCP/IP over X.25, but it also supported departments without sophisticated network connections, using automated dial-up mail exchange.

In 1986, the NSF created NSFNET, a 56 kbit/s backbone to support the NSF-sponsored supercomputing centers. The NSFNET also provided support for the creation of regional research and education networks in the United States, and for the connection of university and college campus networks to the regional networks. The use of NSFNET and the regional networks was not limited to supercomputer users and the 56 kbit/s network quickly became overloaded. NSFNET was upgraded to 1.5 Mbit/s in 1988 under a cooperative agreement with the Merit Network in partnership with IBM, MCI, and the State of Michigan. The existence of NSFNET and the creation of Federal Internet Exchanges (FIXes) allowed the ARPANET to be decommissioned in 1990. NSFNET was expanded and upgraded to 45 Mbit/s in 1991, and was decommissioned in 1995 when it was replaced by backbones operated by several commercial Internet Service Providers.

The term "internet" was adopted in the first RFC published on the TCP protocol (RFC 675: Internet Transmission Control Program, December 1974) as an abbreviation of the term "internetworking" and the two terms were used interchangeably. In general, an internet was any network using TCP/IP. It was around the time when ARPANET was interlinked with NSFNET in the late 1980s, that the term was used as the name of the network, Internet, being the large and global TCP/IP network.

As interest in networking grew and new applications for it were developed, the Internet's technologies spread throughout the rest of the world. The network-agnostic approach in TCP/IP meant that it was easy to use any existing network infrastructure, such as the IPSS X.25 network, to carry Internet traffic. In 1982, one year earlier than ARPANET, University College London replaced its transatlantic satellite links with TCP/IP over IPSS.

Many sites unable to link directly to the Internet created simple gateways for the transfer of electronic mail, the most important application of the time. Sites with only intermittent connections used UUCP or FidoNet and relied on the gateways between these networks and the Internet. Some gateway services went beyond simple mail peering, such as allowing access to File Transfer Protocol (FTP) sites via UUCP or mail.

Finally, routing technologies were developed for the Internet to remove the remaining centralized routing aspects. The Exterior Gateway Protocol (EGP) was replaced by a new protocol, the Border Gateway Protocol (BGP). This provided a meshed topology for the Internet and reduced the centric architecture which ARPANET had emphasized. In 1994, Classless Inter-Domain Routing (CIDR) was introduced to support better conservation of address space which allowed use of route aggregation to decrease the size of routing tables.

Between 1984 and 1988 CERN began installation and operation of TCP/IP to interconnect its major internal computer systems, workstations, PCs and an accelerator control system. CERN continued to operate a limited self-developed system (CERNET) internally and several incompatible (typically proprietary) network protocols externally. There was considerable resistance in Europe towards more widespread use of TCP/IP, and the CERN TCP/IP intranets remained isolated from the Internet until 1989.

In 1988, Daniel Karrenberg, from Centrum Wiskunde & Informatica (CWI) in Amsterdam, visited Ben Segal, CERN's TCP/IP Coordinator, looking for advice about the transition of the European side of the UUCP Usenet network (much of which ran over X.25 links) over to TCP/IP. In 1987, Ben Segal had met with Len Bosack from the then still small company Cisco about purchasing some TCP/IP routers for CERN, and was able to give Karrenberg advice and forward him on to Cisco for the appropriate hardware. This expanded the European portion of the Internet across the existing UUCP networks, and in 1989 CERN opened its first external TCP/IP connections. This coincided with the creation of Réseaux IP Européens (RIPE), initially a group of IP network administrators who met regularly to carry out coordination work together. Later, in 1992, RIPE was formally registered as a cooperative in Amsterdam.

At the same time as the rise of internetworking in Europe, ad hoc networking to ARPA and in-between Australian universities formed, based on various technologies such as X.25 and UUCPNet. These were limited in their connection to the global networks, due to the cost of making individual international UUCP dial-up or X.25 connections. In 1989, Australian universities joined the push towards using IP protocols to unify their networking infrastructures. AARNet was formed in 1989 by the Australian Vice-Chancellors' Committee and provided a dedicated IP based network for Australia.

The Internet began to penetrate Asia in the 1980s. In May 1982 South Korea became the second country to successfully set up TCP/IP IPv4 network. Japan, which had built the UUCP-based network JUNET in 1984, connected to NSFNET in 1989. It hosted the annual meeting of the Internet Society, INET'92, in Kobe. Singapore developed TECHNET in 1990, and Thailand gained a global Internet connection between Chulalongkorn University and UUNET in 1992.

While developed countries with technological infrastructures were joining the Internet, developing countries began to experience a digital divide separating them from the Internet. On an essentially continental basis, they are building organizations for Internet resource administration and sharing operational experience, as more and more transmission facilities go into place.

At the beginning of the 1990s, African countries relied upon X.25 IPSS and 2400 baud modem UUCP links for international and internetwork computer communications.

In August 1995, InfoMail Uganda, Ltd., a privately held firm in Kampala now known as InfoCom, and NSN Network Services of Avon, Colorado, sold in 1997 and now known as Clear Channel Satellite, established Africa's first native TCP/IP high-speed satellite Internet services. The data connection was originally carried by a C-Band RSCC Russian satellite which connected InfoMail's Kampala offices directly to NSN's MAE-West point of presence using a private network from NSN's leased ground station in New Jersey. InfoCom's first satellite connection was just 64 kbit/s, serving a Sun host computer and twelve US Robotics dial-up modems.

In 1996, a USAID funded project, the Leland Initiative, started work on developing full Internet connectivity for the continent. Guinea, Mozambique, Madagascar and Rwanda gained satellite earth stations in 1997, followed by Ivory Coast and Benin in 1998.

Africa is building an Internet infrastructure. AfriNIC, headquartered in Mauritius, manages IP address allocation for the continent. As do the other Internet regions, there is an operational forum, the Internet Community of Operational Networking Specialists.

There are many programs to provide high-performance transmission plant, and the western and southern coasts have undersea optical cable. High-speed cables join North Africa and the Horn of Africa to intercontinental cable systems. Undersea cable development is slower for East Africa; the original joint effort between New Partnership for Africa's Development (NEPAD) and the East Africa Submarine System (Eassy) has broken off and may become two efforts.

The Asia Pacific Network Information Centre (APNIC), headquartered in Australia, manages IP address allocation for the continent. APNIC sponsors an operational forum, the Asia-Pacific Regional Internet Conference on Operational Technologies (APRICOT).

South Korea’s first Internet system, the System Development Network (SDN) began operation on 15 May 1982. SDN was connected to the rest of the world in August 1983 using UUCP (Unixto-Unix-Copy); connected to CSNET in December 1984; and formally connected to the U.S. Internet in 1990.

In 1991, the People's Republic of China saw its first TCP/IP college network, Tsinghua University's TUNET. The PRC went on to make its first global Internet connection in 1994, between the Beijing Electro-Spectrometer Collaboration and Stanford University's Linear Accelerator Center. However, China went on to implement its own digital divide by implementing a country-wide content filter.

As with the other regions, the Latin American and Caribbean Internet Addresses Registry (LACNIC) manages the IP address space and other resources for its area. LACNIC, headquartered in Uruguay, operates DNS root, reverse DNS, and other key services.

Initially, as with its predecessor networks, the system that would evolve into the Internet was primarily for government and government body use.

However, interest in commercial use of the Internet quickly became a commonly debated topic. Although commercial use was forbidden, the exact definition of commercial use was unclear and subjective. UUCPNet and the X.25 IPSS had no such restrictions, which would eventually see the official barring of UUCPNet use of ARPANET and NSFNET connections. (Some UUCP links still remained connecting to these networks however, as administrators cast a blind eye to their operation.)
As a result, during the late 1980s, the first Internet service provider (ISP) companies were formed. Companies like PSINet, UUNET, Netcom, and Portal Software were formed to provide service to the regional research networks and provide alternate network access, UUCP-based email and Usenet News to the public. The first commercial dialup ISP in the United States was The World, which opened in 1989.

In 1992, the U.S. Congress passed the Scientific and Advanced-Technology Act, , which allowed NSF to support access by the research and education communities to computer networks which were not used exclusively for research and education purposes, thus permitting NSFNET to interconnect with commercial networks. This caused controversy within the research and education community, who were concerned commercial use of the network might lead to an Internet that was less responsive to their needs, and within the community of commercial network providers, who felt that government subsidies were giving an unfair advantage to some organizations.

By 1990, ARPANET's goals had been fulfilled and new networking technologies exceeded the original scope and the project came to a close. New network service providers including PSINet, Alternet, CERFNet, ANS CO+RE, and many others were offering network access to commercial customers. NSFNET was no longer the de facto backbone and exchange point of the Internet. The Commercial Internet eXchange (CIX), Metropolitan Area Exchanges (MAEs), and later Network Access Points (NAPs) were becoming the primary interconnections between many networks. The final restrictions on carrying commercial traffic ended on April 30, 1995 when the National Science Foundation ended its sponsorship of the NSFNET Backbone Service and the service ended. NSF provided initial support for the NAPs and interim support to help the regional research and education networks transition to commercial ISPs. NSF also sponsored the very high speed Backbone Network Service (vBNS) which continued to provide support for the supercomputing centers and research and education in the United States.

The World Wide Web (sometimes abbreviated "www" or "W3") is an information space where documents and other web resources are identified by URIs, interlinked by hypertext links, and can be accessed via the Internet using a web browser and (more recently) web-based applications. It has become known simply as "the Web". As of the 2010s, the World Wide Web is the primary tool billions use to interact on the Internet, and it has changed people's lives immeasurably.

Precursors to the web browser emerged in the form of hyperlinked applications during the mid and late 1980s (the bare concept of hyperlinking had by then existed for some decades). Following these, Tim Berners-Lee is credited with inventing the World Wide Web in 1989 and developing in 1990 both the first web server, and the first web browser, called WorldWideWeb (no spaces) and later renamed Nexus. Many others were soon developed, with Marc Andreessen's 1993 Mosaic (later Netscape), being particularly easy to use and install, and often credited with sparking the internet boom of the 1990s. Today, the major web browsers are Firefox, Internet Explorer, Google Chrome, Opera and Safari.

A boost in web users was triggered in September 1993 by NCSA Mosaic, a graphical browser which eventually ran on several popular office and home computers. This was the first web browser aiming to bring multimedia content to non-technical users, and therefore included images and text on the same page, unlike previous browser designs; its founder, Marc Andreessen, also established the company that in 1994, released Netscape Navigator, which resulted in one of the early browser wars, when it ended up in a competition for dominance (which it lost) with Microsoft Windows' Internet Explorer. Commercial use restrictions were lifted in 1995. The online service America Online (AOL) offered their users a connection to the Internet via their own internal browser.

During the first decade or so of the public internet, the immense changes it would eventually enable in the 2000s were still nascent. In terms of providing context for this period, mobile cellular devices ("smartphones" and other cellular devices) which today provide near-universal access, were used for business and not a routine household item owned by parents and children worldwide. Social media in the modern sense had yet to come into existence, laptops were bulky and most households did not have computers. Data rates were slow and most people lacked means to video or digitize video; media storage was transitioning slowly from analog tape to digital optical discs (DVD and to an extent still, floppy disc to CD). Enabling technologies used from the early 2000s such as PHP, modern Javascript and Java, technologies such as AJAX, HTML 4 (and its emphasis on CSS), and various software frameworks, which enabled and simplified speed of web development, largely awaited invention and their eventual widespread adoption.

The Internet was widely used for mailing lists, emails, e-commerce and early popular online shopping (Amazon and eBay for example), online forums and bulletin boards, and personal websites and blogs, and use was growing rapidly, but by more modern standards the systems used were static and lacked widespread social engagement. It awaited a number of events in the early 2000s to change from a communications technology to gradually develop into a key part of global society's infrastructure.

Typical design elements of these "Web 1.0" era websites included: Static pages instead of dynamic HTML; content served from filesystems instead of relational databases; pages built using Server Side Includes or CGI instead of a web application written in a dynamic programming language; HTML 3.2-era structures such as frames and tables to create page layouts; online guestbooks; overuse of GIF buttons and similar small graphics promoting particular items; and HTML forms sent via email. (Support for server side scripting was rare on shared servers so the usual feedback mechanism was via email, using mailto forms and their email program.

During the period 1997 to 2001, the first speculative investment bubble related to the Internet took place, in which "dot-com" companies (referring to the ".com" top level domain used by businesses) were propelled to exceedingly high valuations as investors rapidly stoked stock values, followed by a market crash; the first dot-com bubble. However this only temporarily slowed enthusiasm and growth, which quickly recovered and continued to grow.

The changes that would propel the Internet into its place as a social system took place during a relatively short period of no more than five years, starting from around 2004. They included:

and shortly after (approximately 2007–2008 onward):
With the call to Web 2.0, the period up to around 2004–2005 was retrospectively named and described by some as Web 1.0.

The term "Web 2.0" describes websites that emphasize user-generated content (including user-to-user interaction), usability, and interoperability. It first appeared in a January 1999 article called "Fragmented Future" written by Darcy DiNucci, a consultant on electronic information design, where she wrote:

The term resurfaced during 2002 – 2004, and gained prominence in late 2004 following presentations by Tim O'Reilly and Dale Dougherty at the first Web 2.0 Conference. In their opening remarks, John Battelle and Tim O'Reilly outlined their definition of the "Web as Platform", where software applications are built upon the Web as opposed to upon the desktop. The unique aspect of this migration, they argued, is that "customers are building your business for you". They argued that the activities of users generating content (in the form of ideas, text, videos, or pictures) could be "harnessed" to create value.

Web 2.0 does not refer to an update to any technical specification, but rather to cumulative changes in the way Web pages are made and used. Web 2.0 describes an approach, in which sites focus substantially upon allowing users to interact and collaborate with each other in a social media dialogue as creators of user-generated content in a virtual community, in contrast to Web sites where people are limited to the passive viewing of content. Examples of Web 2.0 include social networking sites, blogs, wikis, folksonomies, video sharing sites, hosted services, Web applications, and mashups. Terry Flew, in his 3rd Edition of "New Media" described what he believed to characterize the differences between Web 1.0 and Web 2.0:
This era saw several household names gain prominence through their community-oriented operation – YouTube, Twitter, Facebook, Reddit and Wikipedia being some examples.

The process of change generally described as "Web 2.0" was itself greatly accelerated and transformed only a short time later by the increasing growth in mobile devices. This mobile revolution meant that computers in the form of smartphones became something many people used, took with them everywhere, communicated with, used for photographs and videos they instantly shared or to shop or seek information "on the move" – and used socially, as opposed to items on a desk at home or just used for work.

Location-based services, services using location and other sensor information, and crowdsourcing (frequently but not always location based), became common, with posts tagged by location, or websites and services becoming location aware. Mobile-targeted websites (such as "m.website.com") became common, designed especially for the new devices used. Netbooks, ultrabooks, widespread 4G and Wi-Fi, and mobile chips capable or running at nearly the power of desktops from not many years before on far lower power usage, became enablers of this stage of Internet development, and the term "App" emerged (short for "Application program" or "Program") as did the "App store".

The first Internet link into low earth orbit was established on January 22, 2010 when astronaut T. J. Creamer posted the first unassisted update to his Twitter account from the International Space Station, marking the extension of the Internet into space. (Astronauts at the ISS had used email and Twitter before, but these messages had been relayed to the ground through a NASA data link before being posted by a human proxy.) This personal Web access, which NASA calls the Crew Support LAN, uses the space station's high-speed Ku band microwave link. To surf the Web, astronauts can use a station laptop computer to control a desktop computer on Earth, and they can talk to their families and friends on Earth using Voice over IP equipment.

Communication with spacecraft beyond earth orbit has traditionally been over point-to-point links through the Deep Space Network. Each such data link must be manually scheduled and configured. In the late 1990s NASA and Google began working on a new network protocol, Delay-tolerant networking (DTN) which automates this process, allows networking of spaceborne transmission nodes, and takes the fact into account that spacecraft can temporarily lose contact because they move behind the Moon or planets, or because space weather disrupts the connection. Under such conditions, DTN retransmits data packages instead of dropping them, as the standard TCP/IP Internet Protocol does. NASA conducted the first field test of what it calls the "deep space internet" in November 2008. Testing of DTN-based communications between the International Space Station and Earth (now termed Disruption-Tolerant Networking) has been ongoing since March 2009, and is scheduled to continue until March 2014.

This network technology is supposed to ultimately enable missions that involve multiple spacecraft where reliable inter-vessel communication might take precedence over vessel-to-earth downlinks. According to a February 2011 statement by Google's Vint Cerf, the so-called "Bundle protocols" have been uploaded to NASA's EPOXI mission spacecraft (which is in orbit around the Sun) and communication with Earth has been tested at a distance of approximately 80 light seconds.

As a globally distributed network of voluntarily interconnected autonomous networks, the Internet operates without a central governing body. It has no centralized governance for either technology or policies, and each constituent network chooses what technologies and protocols it will deploy from the voluntary technical standards that are developed by the Internet Engineering Task Force (IETF). However, throughout its entire history, the Internet system has had an "Internet Assigned Numbers Authority" (IANA) for the allocation and assignment of various technical identifiers needed for the operation of the Internet. The Internet Corporation for Assigned Names and Numbers (ICANN) provides oversight and coordination for two principal name spaces in the Internet, the Internet Protocol address space and the Domain Name System.

The IANA function was originally performed by USC Information Sciences Institute (ISI), and it delegated portions of this responsibility with respect to numeric network and autonomous system identifiers to the Network Information Center (NIC) at Stanford Research Institute (SRI International) in Menlo Park, California. ISI's Jonathan Postel managed the IANA, served as RFC Editor and performed other key roles until his premature death in 1998.

As the early ARPANET grew, hosts were referred to by names, and a HOSTS.TXT file would be distributed from SRI International to each host on the network. As the network grew, this became cumbersome. A technical solution came in the form of the Domain Name System, created by ISI's Paul Mockapetris in 1983. The Defense Data Network—Network Information Center (DDN-NIC) at SRI handled all registration services, including the top-level domains (TLDs) of .mil, .gov, .edu, .org, .net, .com and .us, root nameserver administration and Internet number assignments under a United States Department of Defense contract. In 1991, the Defense Information Systems Agency (DISA) awarded the administration and maintenance of DDN-NIC (managed by SRI up until this point) to Government Systems, Inc., who subcontracted it to the small private-sector Network Solutions, Inc.

The increasing cultural diversity of the Internet also posed administrative challenges for centralized management of the IP addresses. In October 1992, the Internet Engineering Task Force (IETF) published RFC 1366, which described the "growth of the Internet and its increasing globalization" and set out the basis for an evolution of the IP registry process, based on a regionally distributed registry model. This document stressed the need for a single Internet number registry to exist in each geographical region of the world (which would be of "continental dimensions"). Registries would be "unbiased and widely recognized by network providers and subscribers" within their region.
The RIPE Network Coordination Centre (RIPE NCC) was established as the first RIR in May 1992. The second RIR, the Asia Pacific Network Information Centre (APNIC), was established in Tokyo in 1993, as a pilot project of the Asia Pacific Networking Group.

Since at this point in history most of the growth on the Internet was coming from non-military sources, it was decided that the Department of Defense would no longer fund registration services outside of the .mil TLD. In 1993 the U.S. National Science Foundation, after a competitive bidding process in 1992, created the InterNIC to manage the allocations of addresses and management of the address databases, and awarded the contract to three organizations. Registration Services would be provided by Network Solutions; Directory and Database Services would be provided by AT&T; and Information Services would be provided by General Atomics.

Over time, after consultation with the IANA, the IETF, RIPE NCC, APNIC, and the Federal Networking Council (FNC), the decision was made to separate the management of domain names from the management of IP numbers. Following the examples of RIPE NCC and APNIC, it was recommended that management of IP address space then administered by the InterNIC should be under the control of those that use it, specifically the ISPs, end-user organizations, corporate entities, universities, and individuals. As a result, the American Registry for Internet Numbers (ARIN) was established as in December 1997, as an independent, not-for-profit corporation by direction of the National Science Foundation and became the third Regional Internet Registry.

In 1998, both the IANA and remaining DNS-related InterNIC functions were reorganized under the control of ICANN, a California non-profit corporation contracted by the United States Department of Commerce to manage a number of Internet-related tasks. As these tasks involved technical coordination for two principal Internet name spaces (DNS names and IP addresses) created by the IETF, ICANN also signed a memorandum of understanding with the IAB to define the technical work to be carried out by the Internet Assigned Numbers Authority. The management of Internet address space remained with the regional Internet registries, which collectively were defined as a supporting organization within the ICANN structure. ICANN provides central coordination for the DNS system, including policy coordination for the split registry / registrar system, with competition among registry service providers to serve each top-level-domain and multiple competing registrars offering DNS services to end-users.

The Internet Engineering Task Force (IETF) is the largest and most visible of several loosely related ad-hoc groups that provide technical direction for the Internet, including the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF).

The IETF is a loosely self-organized group of international volunteers who contribute to the engineering and evolution of Internet technologies. It is the principal body engaged in the development of new Internet standard specifications. Much of the work of the IETF is organized into "Working Groups". Standardization efforts of the Working Groups are often adopted by the Internet community, but the IETF does not control or patrol the Internet.

The IETF grew out of quarterly meeting of U.S. government-funded researchers, starting in January 1986. Non-government representatives were invited by the fourth IETF meeting in October 1986. The concept of Working Groups was introduced at the fifth meeting in February 1987. The seventh meeting in July 1987 was the first meeting with more than one hundred attendees. In 1992, the Internet Society, a professional membership society, was formed and IETF began to operate under it as an independent international standards body. The first IETF meeting outside of the United States was held in Amsterdam, The Netherlands, in July 1993. Today, the IETF meets three times per year and attendance has been as high as ca. 2,000 participants. Typically one in three IETF meetings are held in Europe or Asia. The number of non-US attendees is typically ca. 50%, even at meetings held in the United States.

The IETF is not a legal entity, has no governing board, no members, and no dues. The closest status resembling membership is being on an IETF or Working Group mailing list. IETF volunteers come from all over the world and from many different parts of the Internet community. The IETF works closely with and under the supervision of the Internet Engineering Steering Group (IESG) and the Internet Architecture Board (IAB). The Internet Research Task Force (IRTF) and the Internet Research Steering Group (IRSG), peer activities to the IETF and IESG under the general supervision of the IAB, focus on longer term research issues.

Request for Comments (RFCs) are the main documentation for the work of the IAB, IESG, IETF, and IRTF. RFC 1, "Host Software", was written by Steve Crocker at UCLA in April 1969, well before the IETF was created. Originally they were technical memos documenting aspects of ARPANET development and were edited by Jon Postel, the first RFC Editor.

RFCs cover a wide range of information from proposed standards, draft standards, full standards, best practices, experimental protocols, history, and other informational topics. RFCs can be written by individuals or informal groups of individuals, but many are the product of a more formal Working Group. Drafts are submitted to the IESG either by individuals or by the Working Group Chair. An RFC Editor, appointed by the IAB, separate from IANA, and working in conjunction with the IESG, receives drafts from the IESG and edits, formats, and publishes them. Once an RFC is published, it is never revised. If the standard it describes changes or its information becomes obsolete, the revised standard or updated information will be re-published as a new RFC that "obsoletes" the original.

The Internet Society (ISOC) is an international, nonprofit organization founded during 1992 "to assure the open development, evolution and use of the Internet for the benefit of all people throughout the world". With offices near Washington, DC, USA, and in Geneva, Switzerland, ISOC has a membership base comprising more than 80 organizational and more than 50,000 individual members. Members also form "chapters" based on either common geographical location or special interests. There are currently more than 90 chapters around the world.

ISOC provides financial and organizational support to and promotes the work of the standards settings bodies for which it is the organizational home: the Internet Engineering Task Force (IETF), the Internet Architecture Board (IAB), the Internet Engineering Steering Group (IESG), and the Internet Research Task Force (IRTF). ISOC also promotes understanding and appreciation of the Internet model of open, transparent processes and consensus-based decision-making.

Since the 1990s, the Internet's governance and organization has been of global importance to governments, commerce, civil society, and individuals. The organizations which held control of certain technical aspects of the Internet were the successors of the old ARPANET oversight and the current decision-makers in the day-to-day technical aspects of the network. While recognized as the administrators of certain aspects of the Internet, their roles and their decision-making authority are limited and subject to increasing international scrutiny and increasing objections. These objections have led to the ICANN removing themselves from relationships with first the University of Southern California in 2000, and in September 2009, gaining autonomy from the US government by the ending of its longstanding agreements, although some contractual obligations with the U.S. Department of Commerce continued. Finally, on October 1, 2016 ICANN ended its contract with the United States Department of Commerce National Telecommunications and Information Administration (NTIA), allowing oversight to pass to the global Internet community.

The IETF, with financial and organizational support from the Internet Society, continues to serve as the Internet's ad-hoc standards body and issues Request for Comments.

In November 2005, the World Summit on the Information Society, held in Tunis, called for an Internet Governance Forum (IGF) to be convened by United Nations Secretary General. The IGF opened an ongoing, non-binding conversation among stakeholders representing governments, the private sector, civil society, and the technical and academic communities about the future of Internet governance. The first IGF meeting was held in October/November 2006 with follow up meetings annually thereafter. Since WSIS, the term "Internet governance" has been broadened beyond narrow technical concerns to include a wider range of Internet-related policy issues.

Due to its prominence and immediacy as an effective means of mass communication, the Internet has also become more politicized as it has grown. This has led in turn, to discourses and activities that would once have taken place in other ways, migrating to being mediated by internet.

Examples include political activities such as public protest and canvassing of support and votes, but also –

On April 23, 2014, the Federal Communications Commission (FCC) was reported to be considering a new rule that would permit Internet service providers to offer content providers a faster track to send content, thus reversing their earlier net neutrality position. A possible solution to net neutrality concerns may be municipal broadband, according to Professor Susan Crawford, a legal and technology expert at Harvard Law School. On May 15, 2014, the FCC decided to consider two options regarding Internet services: first, permit fast and slow broadband lanes, thereby compromising net neutrality; and second, reclassify broadband as a telecommunication service, thereby preserving net neutrality. On November 10, 2014, President Obama recommended the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality. On January 16, 2015, Republicans presented legislation, in the form of a U.S. Congress H. R. discussion draft bill, that makes concessions to net neutrality but prohibits the FCC from accomplishing the goal or enacting any further regulation affecting Internet service providers (ISPs). On January 31, 2015, AP News reported that the FCC will present the notion of applying ("with some caveats") Title II (common carrier) of the Communications Act of 1934 to the internet in a vote expected on February 26, 2015. Adoption of this notion would reclassify internet service from one of information to one of telecommunications and, according to Tom Wheeler, chairman of the FCC, ensure net neutrality. The FCC is expected to enforce net neutrality in its vote, according to "The New York Times".

On February 26, 2015, the FCC ruled in favor of net neutrality by applying Title II (common carrier) of the Communications Act of 1934 and Section 706 of the Telecommunications act of 1996 to the Internet. The FCC Chairman, Tom Wheeler, commented, "This is no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept."

On March 12, 2015, the FCC released the specific details of the net neutrality rules. On April 13, 2015, the FCC published the final rule on its new "Net Neutrality" regulations.

On December 14, 2017, the F.C.C Repealed their March 12, 2015 decision by a 3-2 vote regarding net neutrality rules.

E-mail has often been called the killer application of the Internet. It predates the Internet, and was a crucial tool in creating it. Email started in 1965 as a way for multiple users of a time-sharing mainframe computer to communicate. Although the history is undocumented, among the first systems to have such a facility were the System Development Corporation (SDC) Q32 and the Compatible Time-Sharing System (CTSS) at MIT.

The ARPANET computer network made a large contribution to the evolution of electronic mail. An experimental inter-system transferred mail on the ARPANET shortly after its creation. In 1971 Ray Tomlinson created what was to become the standard Internet electronic mail addressing format, using the @ sign to separate mailbox names from host names.

A number of protocols were developed to deliver messages among groups of time-sharing computers over alternative transmission systems, such as UUCP and IBM's VNET email system. Email could be passed this way between a number of networks, including ARPANET, BITNET and NSFNET, as well as to hosts connected directly to other sites via UUCP. See the history of SMTP protocol.

In addition, UUCP allowed the publication of text files that could be read by many others. The News software developed by Steve Daniel and Tom Truscott in 1979 was used to distribute news and bulletin board-like messages. This quickly grew into discussion groups, known as newsgroups, on a wide range of topics. On ARPANET and NSFNET similar discussion groups would form via mailing lists, discussing both technical issues and more culturally focused topics (such as science fiction, discussed on the sflovers mailing list).

During the early years of the Internet, email and similar mechanisms were also fundamental to allow people to access resources that were not available due to the absence of online connectivity. UUCP was often used to distribute files using the 'alt.binary' groups. Also, FTP e-mail gateways allowed people that lived outside the US and Europe to download files using ftp commands written inside email messages. The file was encoded, broken in pieces and sent by email; the receiver had to reassemble and decode it later, and it was the only way for people living overseas to download items such as the earlier Linux versions using the slow dial-up connections available at the time. After the popularization of the Web and the HTTP protocol such tools were slowly abandoned.

As the Internet grew through the 1980s and early 1990s, many people realized the increasing need to be able to find and organize files and information. Projects such as Archie, Gopher, WAIS, and the FTP Archive list attempted to create ways to organize distributed data. In the early 1990s, Gopher, invented by Mark P. McCahill offered a viable alternative to the World Wide Web. However, in 1993 the World Wide Web saw many advances to indexing and ease of access through search engines, which often neglected Gopher and Gopherspace. As popularity increased through ease of use, investment incentives also grew until in the middle of 1994 the WWW's popularity gained the upper hand. Then it became clear that Gopher and the other projects were doomed fall short.

One of the most promising user interface paradigms during this period was hypertext. The technology had been inspired by Vannevar Bush's "Memex" and developed through Ted Nelson's research on Project Xanadu and Douglas Engelbart's research on NLS. Many small self-contained hypertext systems had been created before, such as Apple Computer's HyperCard (1987). Gopher became the first commonly used hypertext interface to the Internet. While Gopher menu items were examples of hypertext, they were not commonly perceived in that way.
In 1989, while working at CERN, Tim Berners-Lee invented a network-based implementation of the hypertext concept. By releasing his invention to public use, he ensured the technology would become widespread. For his work in developing the World Wide Web, Berners-Lee received the Millennium technology prize in 2004. One early popular web browser, modeled after HyperCard, was ViolaWWW.

A turning point for the World Wide Web began with the introduction of the Mosaic web browser in 1993, a graphical browser developed by a team at the National Center for Supercomputing Applications at the University of Illinois at Urbana–Champaign (NCSA-UIUC), led by Marc Andreessen. Funding for Mosaic came from the High-Performance Computing and Communications Initiative, a funding program initiated by the High Performance Computing and Communication Act of 1991, also known as the "Gore Bill". Mosaic's graphical interface soon became more popular than Gopher, which at the time was primarily text-based, and the WWW became the preferred interface for accessing the Internet. (Gore's reference to his role in "creating the Internet", however, was ridiculed in his presidential election campaign. See the full article Al Gore and information technology).

Mosaic was superseded in 1994 by Andreessen's Netscape Navigator, which replaced Mosaic as the world's most popular browser. While it held this title for some time, eventually competition from Internet Explorer and a variety of other browsers almost completely displaced it. Another important event held on January 11, 1994, was "The Superhighway Summit" at UCLA's Royce Hall. This was the "first public conference bringing together all of the major industry, government and academic leaders in the field [and] also began the national dialogue about the "Information Superhighway" and its implications."

"24 Hours in Cyberspace", "the largest one-day online event" (February 8, 1996) up to that date, took place on the then-active website, "cyber24.com." It was headed by photographer Rick Smolan. A photographic exhibition was unveiled at the Smithsonian Institution's National Museum of American History on January 23, 1997, featuring 70 photos from the project.

Even before the World Wide Web, there were search engines that attempted to organize the Internet. The first of these was the Archie search engine from McGill University in 1990, followed in 1991 by WAIS and Gopher. All three of those systems predated the invention of the World Wide Web but all continued to index the Web and the rest of the Internet for several years after the Web appeared. There are still Gopher servers as of 2006, although there are a great many more web servers.

As the Web grew, search engines and Web directories were created to track pages on the Web and allow people to find things. The first full-text Web search engine was WebCrawler in 1994. Before WebCrawler, only Web page titles were searched. Another early search engine, Lycos, was created in 1993 as a university project, and was the first to achieve commercial success. During the late 1990s, both Web directories and Web search engines were popular—Yahoo! (founded 1994) and Altavista (founded 1995) were the respective industry leaders. By August 2001, the directory model had begun to give way to search engines, tracking the rise of Google (founded 1998), which had developed new approaches to relevancy ranking. Directory features, while still commonly available, became after-thoughts to search engines.

Database size, which had been a significant marketing feature through the early 2000s, was similarly displaced by emphasis on relevancy ranking, the methods by which search engines attempt to sort the best results first. Relevancy ranking first became a major issue circa 1996, when it became apparent that it was impractical to review full lists of results. Consequently, algorithms for relevancy ranking have continuously improved. Google's PageRank method for ordering the results has received the most press, but all major search engines continually refine their ranking methodologies with a view toward improving the ordering of results. As of 2006, search engine rankings are more important than ever, so much so that an industry has developed ("search engine optimizers", or "SEO") to help web-developers improve their search ranking, and an entire body of case law has developed around matters that affect search engine rankings, such as use of trademarks in metatags. The sale of search rankings by some search engines has also created controversy among librarians and consumer advocates.

On June 3, 2009, Microsoft launched its new search engine, Bing. The following month Microsoft and Yahoo! announced a deal in which Bing would power Yahoo! Search.

Resource or file sharing has been an important activity on computer networks from well before the Internet was established and was supported in a variety of ways including bulletin board systems (1978), Usenet (1980), Kermit (1981), and many others. The File Transfer Protocol (FTP) for use on the Internet was standardized in 1985 and is still in use today. A variety of tools were developed to aid the use of FTP by helping users discover files they might want to transfer, including the Wide Area Information Server (WAIS) in 1991, Gopher in 1991, Archie in 1991, Veronica in 1992, Jughead in 1993, Internet Relay Chat (IRC) in 1988, and eventually the World Wide Web (WWW) in 1991 with Web directories and Web search engines.

In 1999, Napster became the first peer-to-peer file sharing system. Napster used a central server for indexing and peer discovery, but the storage and transfer of files was decentralized. A variety of peer-to-peer file sharing programs and services with different levels of decentralization and anonymity followed, including: Gnutella, eDonkey2000, and Freenet in 2000, FastTrack, Kazaa, Limewire, and BitTorrent in 2001, and Poisoned in 2003.

All of these tools are general purpose and can be used to share a wide variety of content, but sharing of music files, software, and later movies and videos are major uses. And while some of this sharing is legal, large portions are not. Lawsuits and other legal actions caused Napster in 2001, eDonkey2000 in 2005, Kazaa in 2006, and Limewire in 2010 to shut down or refocus their efforts. The Pirate Bay, founded in Sweden in 2003, continues despite a trial and appeal in 2009 and 2010 that resulted in jail terms and large fines for several of its founders. File sharing remains contentious and controversial with charges of theft of intellectual property on the one hand and charges of censorship on the other.

Suddenly the low price of reaching millions worldwide, and the possibility of selling to or hearing from those people at the same moment when they were reached, promised to overturn established business dogma in advertising, mail-order sales, customer relationship management, and many more areas. The web was a new killer app—it could bring together unrelated buyers and sellers in seamless and low-cost ways. Entrepreneurs around the world developed new business models, and ran to their nearest venture capitalist. While some of the new entrepreneurs had experience in business and economics, the majority were simply people with ideas, and did not manage the capital influx prudently. Additionally, many dot-com business plans were predicated on the assumption that by using the Internet, they would bypass the distribution channels of existing businesses and therefore not have to compete with them; when the established businesses with strong existing brands developed their own Internet presence, these hopes were shattered, and the newcomers were left attempting to break into markets dominated by larger, more established businesses. Many did not have the ability to do so.

The dot-com bubble burst in March 2000, with the technology heavy NASDAQ Composite index peaking at 5,048.62 on March 10 (5,132.52 intraday), more than double its value just a year before. By 2001, the bubble's deflation was running full speed. A majority of the dot-coms had ceased trading, after having burnt through their venture capital and IPO capital, often without ever making a profit. But despite this, the Internet continues to grow, driven by commerce, ever greater amounts of online information and knowledge and social networking.

The first mobile phone with Internet connectivity was the Nokia 9000 Communicator, launched in Finland in 1996. The viability of Internet services access on mobile phones was limited until prices came down from that model, and network providers started to develop systems and services conveniently accessible on phones. NTT DoCoMo in Japan launched the first mobile Internet service, i-mode, in 1999 and this is considered the birth of the mobile phone Internet services. In 2001, the mobile phone email system by Research in Motion (now BlackBerry Limited) for their BlackBerry product was launched in America. To make efficient use of the small screen and tiny keypad and one-handed operation typical of mobile phones, a specific document and networking model was created for mobile devices, the Wireless Application Protocol (WAP). Most mobile device Internet services operate using WAP. The growth of mobile phone services was initially a primarily Asian phenomenon with Japan, South Korea and Taiwan all soon finding the majority of their Internet users accessing resources by phone rather than by PC. Developing countries followed, with India, South Africa, Kenya, the Philippines, and Pakistan all reporting that the majority of their domestic users accessed the Internet from a mobile phone rather than a PC. The European and North American use of the Internet was influenced by a large installed base of personal computers, and the growth of mobile phone Internet access was more gradual, but had reached national penetration levels of 20–30% in most Western countries. The cross-over occurred in 2008, when more Internet access devices were mobile phones than personal computers. In many parts of the developing world, the ratio is as much as 10 mobile phone users to one PC user.

Web pages were initially conceived as structured documents based upon Hypertext Markup Language (HTML) which can allow access to images, video, and other content. Hyperlinks in the page permit users to navigate to other pages. In the earliest browsers, images opened in a separate "helper" application. Marc Andreessen's 1993 Mosaic and 1994 Netscape introduced mixed text and images for non-technical users. HTML evolved during the 1990s, leading to HTML 4 which introduced large elements of CSS styling and, later, extensions to allow browser code to make calls and ask for content from servers in a structured way (AJAX).

There are nearly insurmountable problems in supplying a historiography of the Internet's development. The process of digitization represents a twofold challenge both for historiography in general and, in particular, for historical communication research. A sense of the difficulty in documenting early developments that led to the internet can be gathered from the quote:




</doc>
<doc id="13693" url="https://en.wikipedia.org/wiki?curid=13693" title="Horace">
Horace

Quintus Horatius Flaccus (December 8, 65 BC – November 27, 8 BC), known in the English-speaking world as Horace (), was the leading Roman lyric poet during the time of Augustus (also known as Octavian). The rhetorician Quintilian regarded his "Odes" as just about the only Latin lyrics worth reading: "He can be lofty sometimes, yet he is also full of charm and grace, versatile in his figures, and felicitously daring in his choice of words."

Horace also crafted elegant hexameter verses ("Satires" and "Epistles") and caustic iambic poetry ("Epodes"). The hexameters are amusing yet serious works, friendly in tone, leading the ancient satirist Persius to comment: "as his friend laughs, Horace slyly puts his finger on his every fault; once let in, he plays about the heartstrings".

His career coincided with Rome's momentous change from a republic to an empire. An officer in the republican army defeated at the Battle of Philippi in 42 BC, he was befriended by Octavian's right-hand man in civil affairs, Maecenas, and became a spokesman for the new regime. For some commentators, his association with the regime was a delicate balance in which he maintained a strong measure of independence (he was "a master of the graceful sidestep") but for others he was, in John Dryden's phrase, "a well-mannered court slave".

Horace can be regarded as the world's first autobiographer – In his writings, he tells us far more about himself, his character, his development, and his way of life than any other great poet in antiquity. Some of the biographical writings contained in his writings can be supplemented from the short but valuable "Life of Horace" by Suetonius (in his "Lives of the Poets").

He was born on 8 December 65 BC in the Samnite south of Italy. His home town, Venusia, lay on a trade route in the border region between Apulia and Lucania (Basilicata). Various Italic dialects were spoken in the area and this perhaps enriched his feeling for language. He could have been familiar with Greek words even as a young boy and later he poked fun at the jargon of mixed Greek and Oscan spoken in neighbouring Canusium. One of the works he probably studied in school was the "Odyssia" of Livius Andronicus, taught by teachers like the 'Orbilius' mentioned in one of his poems. Army veterans could have been settled there at the expense of local families uprooted by Rome as punishment for their part in the Social War (91–88 BC). Such state-sponsored migration must have added still more linguistic variety to the area. According to a local tradition reported by Horace, a colony of Romans or Latins had been installed in Venusia after the Samnites had been driven out early in the third century. In that case, young Horace could have felt himself to be a Roman though there are also indications that he regarded himself as a Samnite or Sabellus by birth. Italians in modern and ancient times have always been devoted to their home towns, even after success in the wider world, and Horace was no different. Images of his childhood setting and references to it are found throughout his poems.

Horace's father was probably a Venutian taken captive by Romans in the Social War, or possibly he was descended from a Sabine captured in the Samnite Wars. Either way, he was a slave for at least part of his life. He was evidently a man of strong abilities however and managed to gain his freedom and improve his social position. Thus Horace claimed to be the free-born son of a prosperous 'coactor'. The term 'coactor' could denote various roles, such as tax collector, but its use by Horace was explained by scholia as a reference to 'coactor argentareus' i.e. an auctioneer with some of the functions of a banker, paying the seller out of his own funds and later recovering the sum with interest from the buyer.

The father spent a small fortune on his son's education, eventually accompanying him to Rome to oversee his schooling and moral development. The poet later paid tribute to him in a poem that one modern scholar considers the best memorial by any son to his father. The poem includes this passage:
If my character is flawed by a few minor faults, but is otherwise decent and moral, if you can point out only a few scattered blemishes on an otherwise immaculate surface, if no one can accuse me of greed, or of prurience, or of profligacy, if I live a virtuous life, free of defilement (pardon, for a moment, my self-praise), and if I am to my friends a good friend, my father deserves all the credit... As it is now, he deserves from me unstinting gratitude and praise. I could never be ashamed of such a father, nor do I feel any need, as many people do, to apologize for being a freedman's son. "Satires 1.6.65–92"
He never mentioned his mother in his verses and he might not have known much about her. Perhaps she also had been a slave.

Horace left Rome, possibly after his father's death, and continued his formal education in Athens, a great centre of learning in the ancient world, where he arrived at nineteen years of age, enrolling in The Academy. Founded by Plato, The Academy was now dominated by Epicureans and Stoics, whose theories and practises made a deep impression on the young man from Venusia. Meanwhile, he mixed and lounged about with the elite of Roman youth, such as Marcus, the idle son of Cicero, and the Pompeius to whom he later addressed a poem. It was in Athens too that he probably acquired deep familiarity with the ancient tradition of Greek lyric poetry, at that time largely the preserve of grammarians and academic specialists (access to such material was easier in Athens than in Rome, where the public libraries had yet to be built by Asinius Pollio and Augustus).

Rome's troubles following the assassination of Julius Caesar were soon to catch up with him. Marcus Junius Brutus came to Athens seeking support for the republican cause. Brutus was fêted around town in grand receptions and he made a point of attending academic lectures, all the while recruiting supporters among the young men studying there, including Horace. An educated young Roman could begin military service high in the ranks and Horace was made tribunus militum (one of six senior officers of a typical legion), a post usually reserved for men of senatorial or equestrian rank and which seems to have inspired jealousy among his well-born confederates. He learned the basics of military life while on the march, particularly in the wilds of northern Greece, whose rugged scenery became a backdrop to some of his later poems. It was there in 42 BC that Octavian (later Augustus) and his associate Mark Antony crushed the republican forces at the Battle of Philippi. Horace later recorded it as a day of embarrassment for himself, when he fled without his shield, but allowance should be made for his self-deprecating humour. Moreover, the incident allowed him to identify himself with some famous poets who had long ago abandoned their shields in battle, notably his heroes Alcaeus and Archilochus. The comparison with the latter poet is uncanny: Archilochus lost his shield in a part of Thrace near Philippi, and he was deeply involved in the Greek colonization of Thasos, where Horace's die-hard comrades finally surrendered.

Octavian offered an early amnesty to his opponents and Horace quickly accepted it. On returning to Italy, he was confronted with yet another loss: his father's estate in Venusia was one of many throughout Italy to be confiscated for the settlement of veterans (Virgil lost his estate in the north about the same time). Horace later claimed that he was reduced to poverty and this led him to try his hand at poetry. In reality, there was no money to be had from versifying. At best, it offered future prospects through contacts with other poets and their patrons among the rich. Meanwhile, he obtained the sinecure of "scriba quaestorius", a civil service position at the "aerarium" or Treasury, profitable enough to be purchased even by members of the "ordo equester" and not very demanding in its work-load, since tasks could be delegated to "scribae" or permanent clerks. It was about this time that he began writing his "Satires" and "Epodes".

The "Epodes" belong to iambic poetry. Iambic poetry features insulting and obscene language; sometimes, it is referred to as "blame poetry". "Blame poetry", or "shame poetry", is poetry written to blame and shame fellow citizens into a sense of their social obligations. Horace modelled these poems on the poetry of Archilochus. Social bonds in Rome had been decaying since the destruction of Carthage a little more than a hundred years earlier, due to the vast wealth that could be gained by plunder and corruption. These social ills were magnified by rivalry between Julius Caesar, Mark Antony and confederates like Sextus Pompey, all jockeying for a bigger share of the spoils. One modern scholar has counted a dozen civil wars in the hundred years leading up to 31 BC, including the Spartacus rebellion, eight years before Horace's birth. As the heirs to Hellenistic culture, Horace and his fellow Romans were not well prepared to deal with these problems:
Horace's Hellenistic background is clear in his Satires, even though the genre was unique to Latin literature. He brought to it a style and outlook suited to the social and ethical issues confronting Rome but he changed its role from public, social engagement to private meditation. Meanwhile, he was beginning to interest Octavian's supporters, a gradual process described by him in one of his satires. The way was opened for him by his friend, the poet Virgil, who had gained admission into the privileged circle around Maecenas, Octavian's lieutenant, following the success of his "Eclogues". An introduction soon followed and, after a discreet interval, Horace too was accepted. He depicted the process as an honourable one, based on merit and mutual respect, eventually leading to true friendship, and there is reason to believe that his relationship was genuinely friendly, not just with Maecenas but afterwards with Augustus as well. On the other hand, the poet has been unsympathetically described by one scholar as "a sharp and rising young man, with an eye to the main chance." There were advantages on both sides: Horace gained encouragement and material support, the politicians gained a hold on a potential dissident. His republican sympathies, and his role at Philippi, may have caused him some pangs of remorse over his new status. However most Romans considered the civil wars to be the result of "contentio dignitatis", or rivalry between the foremost families of the city, and he too seems to have accepted the principate as Rome's last hope for much needed peace.

In 37 BC, Horace accompanied Maecenas on a journey to Brundisium, described in one of his poems as a series of amusing incidents and charming encounters with other friends along the way, such as Virgil. In fact the journey was political in its motivation, with Maecenas en route to negotiatie the Treaty of Tarentum with Antony, a fact Horace artfully keeps from the reader (political issues are largely avoided in the first book of satires). Horace was probably also with Maecenas on one of Octavian's naval expeditions against the piratical Sextus Pompeius, which ended in a disastrous storm off Palinurus in 36 BC, briefly alluded to by Horace in terms of near-drowning. There are also some indications in his verses that he was with Maecenas at the Battle of Actium in 31 BC, where Octavian defeated his great rival, Antony. By then Horace had already received from Maecenas the famous gift of his Sabine farm, probably not long after the publication of the first book of "Satires". The gift, which included income from five tenants, may have ended his career at the Treasury, or at least allowed him to give it less time and energy. It signalled his identification with the Octavian regime yet, in the second book of "Satires" that soon followed, he continued the apolitical stance of the first book. By this time, he had attained the status of "eques Romanus", perhaps as a result of his work at the Treasury.

"Odes" 1–3 were the next focus for his artistic creativity. He adapted their forms and themes from Greek lyric poetry of the seventh and sixth centuries BC. The fragmented nature of the Greek world had enabled his literary heroes to express themselves freely and his semi-retirement from the Treasury in Rome to his own estate in the Sabine hills perhaps empowered him to some extent also yet even when his lyrics touched on public affairs they reinforced the importance of private life. Nevertheless, his work in the period 30–27 BC began to show his closeness to the regime and his sensitivity to its developing ideology. In "Odes" 1.2, for example, he eulogized Octavian in hyperboles that echo Hellenistic court poetry. The name "Augustus", which Octavian assumed in January 27 BC, is first attested in "Odes" 3.3 and 3.5. In the period 27–24 BC, political allusions in the "Odes" concentrated on foreign wars in Britain (1.35), Arabia (1.29) Spain (3.8) and Parthia (2.2). He greeted Augustus on his return to Rome in 24 BC as a beloved ruler upon whose good health he depended for his own happiness (3.14).

The public reception of "Odes" 1–3 disappointed him however. He attributed the lack of success to jealousy among imperial courtiers and to his isolation from literary cliques. Perhaps it was disappointment that led him to put aside the genre in favour of verse letters. He addressed his first book of "Epistles" to a variety of friends and acquaintances in an urbane style reflecting his new social status as a knight. In the opening poem, he professed a deeper interest in moral philosophy than poetry but, though the collection demonstrates a leaning towards stoic theory, it reveals no sustained thinking about ethics. Maecenas was still the dominant confidante but Horace had now begun to assert his own independence, suavely declining constant invitations to attend his patron. In the final poem of the first book of "Epistles", he revealed himself to be forty-four years old in the consulship of Lollius and Lepidus i.e. 21 BC, and "of small stature, fond of the sun, prematurely grey, quick-tempered but easily placated".

According to Suetonius, the second book of "Epistles" was prompted by Augustus, who desired a verse epistle to be addressed to himself. Augustus was in fact a prolific letter-writer and he once asked Horace to be his personal secretary. Horace refused the secretarial role but complied with the emperor's request for a verse letter. The letter to Augustus may have been slow in coming, being published possibly as late as 11 BC. It celebrated, among other things, the 15 BC military victories of his stepsons, Drusus and Tiberius, yet it and the following letter were largely devoted to literary theory and criticism. The literary theme was explored still further in "Ars Poetica", published separately but written in the form of an epistle and sometimes referred to as "Epistles" 2.3 (possibly the last poem he ever wrote). He was also commissioned to write odes commemorating the victories of Drusus and Tiberius and one to be sung in a temple of Apollo for the Secular Games, a long abandoned festival that Augustus revived in accordance with his policy of recreating ancient customs ("Carmen Saeculare").

Suetonius recorded some gossip about Horace's sexual activities late in life, claiming that the walls of his bedchamber were covered with obscene pictures and mirrors, so that he saw erotica wherever he looked. The poet died at 56 years of age, not long after his friend Maecenas, near whose tomb he was laid to rest. Both men bequeathed their property to Augustus, an honour that the emperor expected of his friends.

The dating of Horace's works isn't known precisely and scholars often debate the exact order in which they were first 'published'. There are persuasive arguments for the following chronology:

Horace composed in traditional metres borrowed from Archaic Greece, employing hexameters in his "Satires" and "Epistles", and iambs in his "Epodes", all of which were relatively easy to adapt into Latin forms. His "Odes" featured more complex measures, including alcaics and sapphics, which were sometimes a difficult fit for Latin structure and syntax. Despite these traditional metres, he presented himself as a partisan in the development of a new and sophisticated style. He was influenced in particular by Hellenistic aesthetics of brevity, elegance and polish, as modeled in the work of Callimachus.
In modern literary theory, a distinction is often made between immediate personal experience ("Urerlebnis") and experience mediated by cultural vectors such as literature, philosophy and the visual arts ("Bildungserlebnis"). The distinction has little relevance for Horace however since his personal and literary experiences are implicated in each other. "Satires" 1.5, for example, recounts in detail a real trip Horace made with Virgil and some of his other literary friends, and which parallels a Satire by Lucilius, his predecessor. Unlike much Hellenistic-inspired literature, however, his poetry was not composed for a small coterie of admirers and fellow poets, nor does it rely on abstruse allusions for many of its effects. Though elitist in its literary standards, it was written for a wide audience, as a public form of art. Ambivalence also characterizes his literary persona, since his presentation of himself as part of a small community of philosophically aware people, seeking true peace of mind while shunning vices like greed, was well adapted to Augustus's plans to reform public morality, corrupted by greedhis personal plea for moderation was part of the emperor's grand message to the nation.

Horace generally followed the examples of poets established as classics in different genres, such as Archilochus in the "Epodes", Lucilius in the "Satires" and Alcaeus in the "Odes", later broadening his scope for the sake of variation and because his models weren't actually suited to the realities confronting him. Archilochus and Alcaeus were aristocratic Greeks whose poetry had a social and religious function that was immediately intelligible to their audiences but which became a mere artifice or literary motif when transposed to Rome. However, the artifice of the "Odes" is also integral to their success, since they could now accommodate a wide range of emotional effects, and the blend of Greek and Roman elements adds a sense of detachment and universality. Horace proudly claimed to introduce into Latin the spirit and iambic poetry of Archilochus but (unlike Archilochus) without persecuting anyone ("Epistles" 1.19.23–5). It was no idle boast. His "Epodes" were modeled on the verses of the Greek poet, as 'blame poetry', yet he avoided targeting real scapegoats. Whereas Archilochus presented himself as a serious and vigorous opponent of wrong-doers, Horace aimed for comic effects and adopted the persona of a weak and ineffectual critic of his times (as symbolized for example in his surrender to the witch Canidia in the final epode). He also claimed to be the first to introduce into Latin the lyrical methods of Alcaeus ("Epistles" 1.19.32–3) and he actually was the first Latin poet to make consistent use of Alcaic meters and themes: love, politics and the symposium. He imitated other Greek lyric poets as well, employing a 'motto' technique, beginning each ode with some reference to a Greek original and then diverging from it.

The satirical poet Lucilius was a senator's son who could castigate his peers with impunity. Horace was a mere freedman's son who had to tread carefully. Lucilius was a rugged patriot and a significant voice in Roman self-awareness, endearing himself to his countrymen by his blunt frankness and explicit politics. His work expressed genuine freedom or libertas. His style included 'metrical vandalism' and looseness of structure. Horace instead adopted an oblique and ironic style of satire, ridiculing stock characters and anonymous targets. His libertas was the private freedom of a philosophical outlook, not a political or social privilege. His "Satires" are relatively easy-going in their use of meter (relative to the tight lyric meters of the "Odes") but formal and highly controlled relative to the poems of Lucilius, whom Horace mocked for his sloppy standards ("Satires" 1.10.56–61)

The "Epistles" may be considered among Horace's most innovative works. There was nothing like it in Greek or Roman literature. Occasionally poems had had some resemblance to letters, including an elegiac poem from Solon to Mimnermus and some lyrical poems from Pindar to Hieron of Syracuse. Lucilius had composed a satire in the form of a letter, and some epistolary poems were composed by Catullus and Propertius. But nobody before Horace had ever composed an entire collection of verse letters, let alone letters with a focus on philosophical problems. The sophisticated and flexible style that he had developed in his "Satires" was adapted to the more serious needs of this new genre. Such refinement of style was not unusual for Horace. His craftsmanship as a wordsmith is apparent even in his earliest attempts at this or that kind of poetry, but his handling of each genre tended to improve over time as he adapted it to his own needs. Thus for example it is generally agreed that his second book of "Satires", where human folly is revealed through dialogue between characters, is superior to the first, where he propounds his ethics in monologues. Nevertheless, the first book includes some of his most popular poems.

Horace developed a number of inter-related themes throughout his poetic career, including politics, love, philosophy and ethics, his own social role, as well as poetry itself. His "Epodes" and "Satires" are forms of 'blame poetry' and both have a natural affinity with the moralising and diatribes of Cynicism. This often takes the form of allusions to the work and philosophy of Bion of Borysthenes but it is as much a literary game as a philosophical alignment. By the time he composed his "Epistles", he was a critic of Cynicism along with all impractical and "high-falutin" philosophy in general. The "Satires" also include a strong element of Epicureanism, with frequent allusions to the Epicurean poet Lucretius. So for example the Epicurean sentiment "carpe diem" is the inspiration behind Horace's repeated punning on his own name ("Horatius ~ hora") in "Satires" 2.6. The "Satires" also feature some Stoic, Peripatetic and Platonic ("Dialogues") elements. In short, the "Satires" present a medley of philosophical programs, dished up in no particular ordera style of argument typical of the genre. The "Odes" display a wide range of topics. Over time, he becomes more confident about his political voice. Although he is often thought of as an overly intellectual lover, he is ingenuous in representing passion. The "Odes" weave various philosophical strands together, with allusions and statements of doctrine present in about a third of the "Odes" Books 1–3, ranging from the flippant (1.22, 3.28) to the solemn (2.10, 3.2, 3.3). Epicureanism is the dominant influence, characterizing about twice as many of these odes as Stoicism. A group of odes combines these two influences in tense relationships, such as "Odes" 1.7, praising Stoic virility and devotion to public duty while also advocating private pleasures among friends. While generally favouring the Epicurean lifestyle, the lyric poet is as eclectic as the satiric poet, and in "Odes" 2.10 even proposes Aristotle's golden mean as a remedy for Rome's political troubles. Many of Horace's poems also contain much reflection on genre, the lyric tradition, and the function of poetry. "Odes" 4, thought to be composed at the emperor's request, takes the themes of the first three books of "Odes" to a new level. This book shows greater poetic confidence after the public performance of his "Carmen saeculare" or "Century hymn" at a public festival orchestrated by Augustus. In it, Horace addresses the emperor Augustus directly with more confidence and proclaims his power to grant poetic immortality to those he praises. It is the least philosophical collection of his verses, excepting the twelfth ode, addressed to the dead Virgil as if he were living. In that ode, the epic poet and the lyric poet are aligned with Stoicism and Epicureanism respectively, in a mood of bitter-sweet pathos. The first poem of the "Epistles" sets the philosophical tone for the rest of the collection: "So now I put aside both verses and all those other games: What is true and what befits is my care, this my question, this my whole concern." His poetic renunciation of poetry in favour of philosophy is intended to be ambiguous. Ambiguity is the hallmark of the "Epistles". It is uncertain if those being addressed by the self-mocking poet-philosopher are being honoured or criticized. Though he emerges as an Epicurean, it is on the understanding that philosophical preferences, like political and social choices, are a matter of personal taste. Thus he depicts the ups and downs of the philosophical life more realistically than do most philosophers.

The reception of Horace's work has varied from one epoch to another and varied markedly even in his own lifetime. "Odes" 1–3 were not well received when first 'published' in Rome, yet Augustus later commissioned a ceremonial ode for the Centennial Games in 17 BC and also encouraged the publication of "Odes" 4, after which Horace's reputation as Rome's premier lyricist was assured. His Odes were to become the best received of all his poems in ancient times, acquiring a classic status that discouraged imitation: no other poet produced a comparable body of lyrics in the four centuries that followed (though that might also be attributed to social causes, particularly the parasitism that Italy was sinking into). In the seventeenth and eighteenth centuries, ode-writing became highly fashionable in England and a large number of aspiring poets imitated Horace both in English and in Latin.

In a verse epistle to Augustus (Epistle 2.1), in 12 BC, Horace argued for classic status to be awarded to contemporary poets, including Virgil and apparently himself. In the final poem of his third book of Odes he claimed to have created for himself a monument more durable than bronze ("Exegi monumentum aere perennius", "Carmina" 3.30.1). For one modern scholar, however, Horace's personal qualities are more notable than the monumental quality of his achievement:

Yet for men like Wilfred Owen, scarred by experiences of World War I, his poetry stood for discredited values:

The same motto, Dulce et decorum est pro patria mori, had been adapted to the ethos of martyrdom in the lyrics of early Christian poets like Prudentius.

These preliminary comments touch on a small sample of developments in the reception of Horace's work. More developments are covered epoch by epoch in the following sections.

Horace's influence can be observed in the work of his near contemporaries, Ovid and Propertius. Ovid followed his example in creating a completely natural style of expression in hexameter verse, and Propertius cheekily mimicked him in his third book of elegies. His "Epistles" provided them both with a model for their own verse letters and it also shaped Ovid's exile poetry.

His influence had a perverse aspect. As mentioned before, the brilliance of his "Odes" may have discouraged imitation. Conversely, they may have created a vogue for the lyrics of the archaic Greek poet Pindar, due to the fact that Horace had neglected that style of lyric (see Pindar#Influence and legacy). The iambic genre seems almost to have disappeared after publication of Horace's "Epodes". Ovid's "Ibis" was a rare attempt at the form but it was inspired mainly by Callimachus, and there are some iambic elements in Martial but the main influence there was Catullus. A revival of popular interest in the satires of Lucilius may have been inspired by Horace's criticism of his unpolished style. Both Horace and Lucilius were considered good role-models by Persius, who critiqued his own satires as lacking both the acerbity of Lucillius and the gentler touch of Horace. Juvenal's caustic satire was influenced mainly by Lucilius but Horace by then was a school classic and Juvenal could refer to him respectfully and in a round-about way as ""the Venusine lamp"".

Statius paid homage to Horace by composing one poem in Sapphic and one in Alcaic meter (the verse forms most often associated with "Odes"), which he included in his collection of occasional poems, "Silvae". Ancient scholars wrote commentaries on the lyric meters of the "Odes", including the scholarly poet Caesius Bassus. By a process called "derivatio", he varied established meters through the addition or omission of syllables, a technique borrowed by Seneca the Younger when adapting Horatian meters to the stage.

Horace's poems continued to be school texts into late antiquity. Works attributed to Helenius Acro and Pomponius Porphyrio are the remnants of a much larger body of Horatian scholarship. Porphyrio arranged the poems in non-chronological order, beginning with the "Odes", because of their general popularity and their appeal to scholars (the "Odes" were to retain this privileged position in the medieval manuscript tradition and thus in modern editions also). Horace was often evoked by poets of the fourth century, such as Ausonius and Claudian. Prudentius presented himself as a Christian Horace, adapting Horatian meters to his own poetry and giving Horatian motifs a Christian tone. On the other hand, St Jerome, modelled an uncompromising response to the pagan Horace, observing: ""What harmony can there be between Christ and the Devil? What has Horace to do with the Psalter?"" By the early sixth century, Horace and Prudentius were both part of a classical heritage that was struggling to survive the disorder of the times. Boethius, the last major author of classical Latin literature, could still take inspiration from Horace, sometimes mediated by Senecan tragedy. It can be argued that Horace's influence extended beyond poetry to dignify core themes and values of the early Christian era, such as self-sufficiency, inner contentment and courage.

Classical texts almost ceased being copied in the period between the mid sixth century and the Middle Ages. Horace's work probably survived in just two or three books imported into northern Europe from Italy. These became the ancestors of six extant manuscripts dated to the ninth century. Two of those six manuscripts are French in origin, one was produced in Alsace, and the other three show Irish influence but were probably written in continental monasteries (Lombardy for example). By the last half of the ninth century, it was not uncommon for literate people to have direct experience of Horace's poetry. His influence on the Carolingian Renaissance can be found in the poems of Heiric of Auxerre and in some manuscripts marked with neumes, mysterious notations that may have been an aid to the memorization and discussion of his lyric meters. "Ode" is neumed with the melody of a hymn to John the Baptist, "Ut queant laxis", composed in Sapphic stanzas. This hymn later became the basis of the solfege system ("Do, re, mi...")an association with western music quite appropriate for a lyric poet like Horace, though the language of the hymn is mainly Prudentian. Lyons argues that the melody in question was linked with Horace's Ode well before Guido d'Arezzo fitted Ut queant laxis to it. However, the melody is unlikely to be a survivor from classical times, although Ovid testifies to Horace's use of the lyre while performing his Odes.

The German scholar, Ludwig Traube, once dubbed the tenth and eleventh centuries "The age of Horace" ("aetas Horatiana"), and placed it between the "aetas Vergiliana" of the eighth and ninth centuries, and the "aetas Ovidiana" of the twelfth and thirteenth centuries, a distinction supposed to reflect the dominant classical Latin influences of those times. Such a distinction is over-schematized since Horace was a substantial influence in the ninth century as well. Traube had focused too much on Horace's "Satires". Almost all of Horace's work found favor in the Medieval period. In fact medieval scholars were also guilty of over-schematism, associating Horace's different genres with the different ages of man. A twelfth century scholar encapsulated the theory: "...Horace wrote four different kinds of poems on account of the four ages, the "Odes" for boys, the "Ars Poetica" for young men, the "Satires" for mature men, the "Epistles" for old and complete men." It was even thought that Horace had composed his works in the order in which they had been placed by ancient scholars. Despite its naivety, the schematism involved an appreciation of Horace's works as a collection, the "Ars Poetica", "Satires" and "Epistles" appearing to find favour as well as the "Odes". The later Middle Ages however gave special significance to "Satires" and "Epistles", being considered Horace's mature works. Dante referred to Horace as "Orazio satiro", and he awarded him a privileged position in the first circle of Hell, with Homer, Ovid and Lucan.

Horace's popularity is revealed in the large number of quotes from all his works found in almost every genre of medieval literature, and also in the number of poets imitating him in quantitative Latin meter . The most prolific imitator of his "Odes" was the Bavarian monk, Metellus of Tegernsee, who dedicated his work to the patron saint of Tegernsee Abbey, St Quirinus, around the year 1170. He imitated all Horace's lyrical meters then followed these up with imitations of other meters used by Prudentius and Boethius, indicating that variety, as first modelled by Horace, was considered a fundamental aspect of the lyric genre. The content of his poems however was restricted to simple piety. Among the most successful imitators of "Satires" and "Epistles" was another Germanic author, calling himself Sextus Amarcius, around 1100, who composed four books, the first two exemplifying vices, the second pair mainly virtues.

Petrarch is a key figure in the imitation of Horace in accentual meters. His verse letters in Latin were modelled on the "Epistles" and he wrote a letter to Horace in the form of an ode. However he also borrowed from Horace when composing his Italian sonnets. One modern scholar has speculated that authors who imitated Horace in accentual rhythms (including stressed Latin and vernacular languages) may have considered their work a natural sequel to Horace's metrical variety. In France, Horace and Pindar were the poetic models for a group of vernacular authors called the Pléiade, including for example Pierre de Ronsard and Joachim du Bellay. Montaigne made constant and inventive use of Horatian quotes. The vernacular languages were dominant in Spain and Portugal in the sixteenth century, where Horace's influence is notable in the works of such authors as Garcilaso de la Vega, Juan Boscán Sá de Miranda, Antonio Ferreira and Fray Luis de León, the latter for example writing odes on the Horatian theme "beatus ille" ("happy the man"). The sixteenth century in western Europe was also an age of translations (except in Germany, where Horace wasn't translated until well into the seventeenth century). The first English translator was Thomas Drant, who placed translations of Jeremiah and Horace side by side in "Medicinable Morall", 1566. That was also the year that the Scot George Buchanan paraphrased the Psalms in a Horatian setting. Ben Jonson put Horace on the stage in 1601 in "Poetaster", along with other classical Latin authors, giving them all their own verses to speak in translation. Horace's part evinces the independent spirit, moral earnestness and critical insight that many readers look for in his poems.

During the seventeenth and eighteenth centuries, or the Age of Enlightenment, neo-classical culture was pervasive. English literature in the middle of that period has been dubbed Augustan. It is not always easy to distinguish Horace's influence during those centuries (the mixing of influences is shown for example in one poet's pseudonym, "Horace Juvenal"). However a measure of his influence can be found in the diversity of the people interested in his works, both among readers and authors.

New editions of his works were published almost yearly. There were three new editions in 1612 (two in Leiden, one in Frankfurt) and again in 1699 (Utrecht, Barcelona, Cambridge). Cheap editions were plentiful and fine editions were also produced, including one whose entire text was engraved by John Pine in copperplate. The poet James Thomson owned five editions of Horace's work and the physician James Douglas had five hundred books with Horace-related titles. Horace was often commended in periodicals such as The Spectator, as a hallmark of good judgement, moderation and manliness, a focus for moralising. His verses offered a fund of mottoes, such as "simplex munditiis", (elegance in simplicity) "splendide mendax" (nobly untruthful.), "sapere aude", "nunc est bibendum", "carpe diem" (the latter perhaps being the only one still in common use today), quoted even in works as prosaic as Edmund Quincy's "A treatise of hemp-husbandry" (1765). The fictional hero Tom Jones recited his verses with feeling. His works were also used to justify commonplace themes, such as patriotic obedience, as in James Parry's English lines from an Oxford University collection in 1736:

Horatian-style lyrics were increasingly typical of Oxford and Cambridge verse collections for this period, most of them in Latin but some like the previous ode in English. John Milton's Lycidas first appeared in such a collection. It has few Horatian echoes yet Milton's associations with Horace were lifelong. He composed a controversial version of "Odes" 1.5, and Paradise Lost includes references to Horace's 'Roman' "Odes" 3.1–6 (Book 7 for example begins with echoes of "Odes" 3.4). Yet Horace's lyrics could offer inspiration to libertines as well as moralists, and neo-Latin sometimes served as a kind of discrete veil for the risqué. Thus for example Benjamin Loveling authored a catalogue of Drury Lane and Covent Garden prostitutes, in Sapphic stanzas, and an encomium for a dying lady "of salacious memory". Some Latin imitations of Horace were politically subversive, such as a marriage ode by Anthony Alsop that included a rallying cry for the Jacobite cause. On the other hand, Andrew Marvell took inspiration from Horace's "Odes" 1.37 to compose his English masterpiece Horatian Ode upon Cromwell's Return from Ireland, in which subtly nuanced reflections on the execution of Charles I echo Horace's ambiguous response to the death of Cleopatra (Marvell's ode was suppressed in spite of its subtlety and only began to be widely published in 1776). Samuel Johnson took particular pleasure in reading "The Odes". Alexander Pope wrote direct "Imitations" of Horace (published with the original Latin alongside) and also echoed him in "Essays" and The Rape of the Lock. He even emerged as "a quite Horatian Homer" in his translation of the "Iliad". Horace appealed also to female poets, such as Anna Seward ("Original sonnets on various subjects, and odes paraphrased from Horace", 1799) and Elizabeth Tollet, who composed a Latin ode in Sapphic meter to celebrate her brother's return from overseas, with tea and coffee substituted for the wine of Horace's sympotic settings:

Horace's "Ars Poetica" is second only to Aristotle's "Poetics" in its influence on literary theory and criticism. Milton recommended both works in his treatise "of Education". Horace's "Satires" and "Epistles" however also had a huge impact, influencing theorists and critics such as John Dryden. There was considerable debate over the value of different lyrical forms for contemporary poets, as represented on one hand by the kind of four-line stanzas made familiar by Horace's Sapphic and Alcaic "Odes" and, on the other, the loosely structured Pindarics associated with the odes of Pindar. Translations occasionally involved scholars in the dilemmas of censorship. Thus Christopher Smart entirely omitted "Odes" and re-numbered the remaining odes. He also removed the ending of "Odes" . Thomas Creech printed "Epodes" and in the original Latin but left out their English translations. Philip Francis left out both the English and Latin for those same two epodes, a gap in the numbering the only indication that something was amiss. French editions of Horace were influential in England and these too were regularly bowdlerized.

Most European nations had their own 'Horaces': thus for example Friedrich von Hagedorn was called "The German Horace" and Maciej Kazimierz Sarbiewski "The Polish Horace" (the latter was much imitated by English poets such as Henry Vaughan and Abraham Cowley). Pope Urban VIII wrote voluminously in Horatian meters, including an ode on gout.

Horace maintained a central role in the education of English-speaking elites right up until the 1960s. A pedantic emphasis on the formal aspects of language-learning at the expense of literary appreciation may have made him unpopular in some quarters yet it also confirmed his influencea tension in his reception that underlies Byron's famous lines from "Childe Harold" (Canto iv, 77):
William Wordsworth's mature poetry, including the preface to Lyrical Ballads, reveals Horace's influence in its rejection of false ornament and he once expressed "a wish / to meet the shade of Horace...". John Keats echoed the opening of Horace's "Epodes" 14 in the opening lines of "Ode to a Nightingale".

The Roman poet was presented in the nineteenth century as an honorary English gentleman. William Thackeray produced a version of "Odes" in which Horace's questionable 'boy' became 'Lucy', and Gerard Manley Hopkins translated the boy innocently as 'child'. Horace was translated by Sir Theodore Martin (biographer of Prince Albert) but minus some ungentlemanly verses, such as the erotic "Odes" and "Epodes" 8 and 12. Lord Lytton produced a popular translation and William Gladstone also wrote translations during his last days as Prime Minister.

Edward FitzGerald's "Rubaiyat of Omar Khayyam", though formally derived from the Persian "ruba'i", nevertheless shows a strong Horatian influence, since, as one modern scholar has observed,""...the quatrains inevitably recall the stanzas of the 'Odes', as does the narrating first person of the world-weary, ageing Epicurean Omar himself, mixing sympotic exhortation and 'carpe diem' with splendid moralising and 'memento mori' nihilism."" Matthew Arnold advised a friend in verse not to worry about politics, an echo of "Odes" , yet later became a critic of Horace's inadequacies relative to Greek poets, as role models of Victorian virtues, observing: ""If human life were complete without faith, without enthusiasm, without energy, Horace...would be the perfect interpreter of human life."" Christina Rossetti composed a sonnet depicting a woman willing her own death steadily, drawing on Horace's depiction of 'Glycera' in "Odes" and Cleopatra in "Odes" . A. E. Housman considered "Odes" , in Archilochian couplets, the most beautiful poem of antiquity and yet he generally shared Horace's penchant for quatrains, being readily adapted to his own elegiac and melancholy strain. The most famous poem of Ernest Dowson took its title and its heroine's name from a line of "Odes" , "Non sum qualis eram bonae sub regno Cynarae", as well as its motif of nostalgia for a former flame. Kipling wrote a famous parody of the "Odes", satirising their stylistic idiosyncrasies and especially the extraordinary syntax, but he also used Horace's Roman patriotism as a focus for British imperialism, as in the story "Regulus" in the school collection "Stalky & Co.", which he based on "Odes" . Wilfred Owen's famous poem, quoted above, incorporated Horatian text to question patriotism while ignoring the rules of Latin scansion. However, there were few other echoes of Horace in the war period, possibly because war is not actually a major theme of Horace's work.
Both W.H.Auden and Louis MacNeice began their careers as teachers of classics and both responded as poets to Horace's influence. Auden for example evoked the fragile world of the 1930s in terms echoing "Odes" , where Horace advises a friend not to let worries about frontier wars interfere with current pleasures.
The American poet, Robert Frost, echoed Horace's "Satires" in the conversational and sententious idiom of some of his longer poems, such as "The Lesson for Today" (1941), and also in his gentle advocacy of life on the farm, as in "Hyla Brook" (1916), evoking Horace's "fons Bandusiae" in "Ode" . Now at the start of the third millennium, poets are still absorbing and re-configuring the Horatian influence, sometimes in translation (such as a 2002 English/American edition of the "Odes" by thirty-six poets) and sometimes as inspiration for their own work (such as a 2003 collection of odes by a New Zealand poet).

Horace's "Epodes" have largely been ignored in the modern era, excepting those with political associations of historical significance. The obscene qualities of some of the poems have repulsed even scholars yet more recently a better understanding of the nature of Iambic poetry has led to a re-evaluation of the "whole" collection. A re-appraisal of the "Epodes" also appears in creative adaptations by recent poets (such as a 2004 collection of poems that relocates the ancient context to a 1950s industrial town).







</doc>
