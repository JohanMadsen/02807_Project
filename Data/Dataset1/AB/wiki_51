<doc id="18885" url="https://en.wikipedia.org/wiki?curid=18885" title="Morton Downey Jr.">
Morton Downey Jr.

Sean Morton Downey (December 9, 1932 – March 12, 2001), better known by his stage name Morton Downey Jr., was an American television talk show host of the late-1980s who pioneered the "trash TV" format on his program "The Morton Downey Jr. Show".

American independent film company Ironbound Films produced a documentary film about Downey titled "", which premiered April 19, 2012, at the 2012 Tribeca Film Festival.

Downey's parents were also in show business; his father, Morton Downey, was a popular singer, and his mother, Barbara Bennett, was a singer and dancer. Downey did not use his legal first name (Sean) in his stage name. His aunts included Hollywood film stars Constance and Joan Bennett, from whom he was estranged, and his maternal grandfather was the celebrated matinée idol Richard Bennett. Born into a wealthy family, he was raised during the summers next door to the Kennedy compound in Hyannis Port, Massachusetts. Downey attended New York University.

He was a program director and announcer at radio station WPOP in Hartford, Connecticut in the 1950s. He went on to work as a disc jockey, sometimes using the moniker "Doc" Downey, in various markets around the U.S., including Phoenix (KRIZ), Miami (WFUN), Kansas City (KUDL), San Diego (KDEO) and Seattle (KJR). Like his father, Downey pursued a career in music, recording in both pop and country styles. He sang on a few records and then began to write songs, several of which were popular in the 1950s and 1960s. He joined ASCAP as a result. In 1958, he recorded "Boulevard of Broken Dreams", which he sang on national television on a set that resembled a dark street with one street light. In 1981, "Green Eyed Girl" charted on the "Billboard Magazine" country chart, peaking at #95.

In the 1980s, Downey was a talk show host at KFBK-AM in Sacramento, California, where he employed his abrasive style. He was fired in 1984. He was replaced on KFBK by Rush Limbaugh, who has held the time slot ever since, later via his national syndication. Downey also had a stint on WMAQ-AM in Chicago where he unsuccessfully tried to get other on air radio personalities to submit to drug testing. Downey's largest effect on American culture came from his popular, yet short-lived, syndicated late 1980s television talk show, "The Morton Downey Jr. Show".

On January 22, 1980, Downey, a devoted pro-life activist, hosted the California State Rally for Life at the invitation of the California ProLife Council and United Students for Life. At that time, he was also running for President of the United States, as a Democrat. The United Students for Life, at California State University, Sacramento helped organize his California presidential rallies. Downey worked to help promote pro-life candidates in California and around the country.

Downey headed to Secaucus, New Jersey, where his highly controversial television program "The Morton Downey Jr. Show" was taped. Starting as a local program on New York-New Jersey superstation WWOR-TV in October 1987, it expanded into national syndication in early 1988. The program featured screaming matches among Downey, his guests, and audience members. Using a large silver bowl for an ashtray, he would chainsmoke during the show and blow smoke in his guests' faces. Downey's fans became known as "Loudmouths," patterned after the studio lecterns decorated with gaping cartoon mouths, from which Downey's guests would go head-to-head against each other on their respective issues.

Downey's signature phrases "pablum puking liberal" (in reference to left-liberals) and "zip it!" briefly enjoyed some popularity in the contemporary vernacular. He particularly enjoyed making his guests angry with each other, which on a few occasions resulted in physical confrontations. One such incident occurred on a 1988 show taped at the Apollo Theater, involving Al Sharpton and CORE National Chairman Roy Innis. The exchange between the two men culminated in Innis shoving Sharpton into his chair, knocking him to the floor and Downey intervening to separate the pair.

Because of the controversial format and content of the show, distributor MCA Television had problems selling the show to a number of stations and advertisers. Even Downey's affiliates, many of which were low-rated independent television stations in small to medium markets, were so fearful of advertiser and viewer backlash that they would air one or even two local disclaimers during the broadcast. 

During one controversial episode Downey introduced his gay brother, Tony Downey, to his studio audience and informed them Tony was HIV positive. During the episode Downey stated he was afraid his audience would abandon him if they knew he had a gay brother, but then said he did not care.

"The Washington Post" wrote about him, "Suppose a maniac got hold of a talk show. Or need we suppose?" David Letterman said, "I'm always amazed at what people will fall for. We see this every ten or twelve years, an attempt at this, and I guess from that standpoint I don't quite understand why everybody's falling over backwards over the guy."

The success of the show made Downey a pop culture celebrity, leading to appearances on "Saturday Night Live" in 1988, WrestleMania V in 1989 in which he traded insults with Roddy Piper and Brother Love on "Piper's Pit", and later roles in movies such as "Predator 2" and "". He was also cast in several television roles, often playing tabloid TV hosts or other obnoxious media types. Downey notably starred in the "Tales from the Crypt" episode "Television Terror" which utilized several scenes shot by characters within the story, a format which became popular in horror films a decade later with the found footage genre.

In 1989, Downey released an album of songs based on his show entitled "Morton Downey Jr. Sings". The album's single, "Zip It!" (a catch-phrase from the TV show, used to quiet an irate guest), became a surprise hit on some college radio stations. Over the course of the 1988–89 television season, his TV show suffered a decline in viewership, resulting from many markets downgrading its time slot; even flagship station WWOR moved Downey's program from its original 9:00 PM slot to 11:30 PM in the fall of 1988. Beginning in January 1989, the time slot immediately following Downey's program was given to the then-new "Arsenio Hall Show". Following Hall's strong early ratings, however, the two series swapped time slots several weeks later, thus relegating Downey to 12:30 AM in the number-one television market. 

In late April 1989, he was involved in an incident in a San Francisco International Airport restroom in which he claimed to have been attacked by neo-Nazis who painted a swastika on his face and attempted to shave his head. Some inconsistencies in Downey's account (e.g., the swastika was painted in reverse, suggesting that Downey had drawn it himself in a mirror), and the failure of the police to find supportive evidence, led many to suspect the incident was a hoax and a ploy for attention. In July 1989, his show was canceled, with the owners of the show announcing that the last episode had been taped on June 30, and that no new shows would air after September 15, 1989.

At the time of its cancellation, the show was airing on a total of 70 stations across the country, and its advertisers had been reduced primarily to "direct-response" ads (such as 900 chat line and phone sex numbers). In February 1990, Downey filed for bankruptcy in the US Bankruptcy Court for the District of New Jersey.

In 1990, Downey resurfaced on CNBC with an interview program called "Showdown", which was followed by three attempted talk radio comebacks: first in 1992 on Washington, D.C. radio station WWRC; then in 1993 on Dallas radio station KGBS, where he would scream insults at his callers. He was also hired as the station's VP of Operations. The following year, he returned to CNBC with a short-lived television show, "Downey"; in one episode, Downey claimed to have had a psychic communication with O.J. Simpson's murdered ex-wife, Nicole Brown Simpson.

His third – and final – attempt at a talk radio comeback occurred in 1997 on Cleveland radio station WTAM in a late evening time slot. It marked his return to the Cleveland market, where Downey had been a host for crosstown radio station WERE in the early 1980s prior to joining KFBK. This stint came shortly after the surgery for lung cancer that removed one of his lungs. At WTAM, Downey abandoned the confrontational schtick of his TV and previous radio shows, and conducted this program in a much more conversational and jovial manner.

On August 30, 1997, Downey quit his WTAM show to focus on pursuing legal action against Howard Stern. Downey had accused Stern of spreading rumors that he had resumed his smoking habit, to which publicist Les Schecter retorted, "He hasn't picked up a cigarette." His replacement was former WERE host Rick Gilmour.

Following his death, news reports and obituaries incorrectly (according to the "Orange County Register") credited him as the composer of "Wipe Out." As of 2008, Downey's official website (and others) continue to make this claim. Prior to Downey's death, "Spin" in April 1989 had identified the "Wipe Out" authorship as a myth.

In 1984, at KFBK radio, Downey used the word "Chinaman" while telling a joke. His use of the word upset portions of the sizable Asian community in Sacramento. One Asian-American city councilman called for an apology and pressured the station for Downey's resignation. Downey refused to apologize and was forced to resign with Rush Limbaugh taking his place.

Downey was sued for allegedly appropriating the words and music to his theme song from two songwriters. He was sued for $40 million after bringing then-stripper Kellie Everts onto the show and calling her a "slut," a "pig," a "hooker," and a "tramp," claiming that she had venereal diseases, and banging his pelvis against hers.

In April 1988, he was arraigned on criminal charges for allegedly attacking a gay guest on his show, in a never-aired segment. In another lawsuit, he was accused of slandering a newscaster (a former colleague), and of indecently exposing himself to her and slapping her. Downey punched Stuttering John during an interview done for "The Howard Stern Show", while also shouting verbal insults at John, referring to him as an "uneducated slob". The situation then began to evolve into a brawl between the two until Downey had to be pulled off of John by security; the entire incident was caught on camera. When an "Inside Edition" camera crew approached Downey in 1989 to question him about his involvement in an alleged business scam, Downey grabbed the boom mike and struck the soundman's head with it.

In his later years, Downey expressed remorse for some of the extreme theatrics of his TV show, as well as various incidents outside the studio, including the "Inside Edition" confrontation. However, he also claimed that his show was of a higher quality and not as "sleazy" as Jerry Springer's show.

Released in 2012, the documentary film "" touches upon Downey's upbringing and formative years in radio and politics before launching into the history of "The Morton Downey Jr. Show" and Downey's influence on trash TV. The film also looks at Downey's relationship with Al Sharpton and other important 80s figures, as well as Downey's role as a predecessor for commentators like Glenn Beck and Rush Limbaugh.

Downey was married four times and had four children from three of those marriages. With wife Helen, he had daughter Melissa; with Joan, he had daughters Tracey and Kelli; and, with fourth wife Lori, he had daughter Seanna Micaela. He and Lori met when she appeared as a dancer in a show he attended in Atlantic City. According to Terry Pluto's book, "Loose Balls", Downey was one of the owners of the New Orleans Buccaneers basketball team in the American Basketball Association in the late 1960s. Downey was also president and co-founder of the proposed World Baseball Association in 1974.

In 1998, a Golden Palm Star on the Palm Springs, California, Walk of Stars was dedicated to him.

In the "Super Mario" video game series, the Koopaling Morton Koopa Jr. was named after him and has a large mouth, in reference to Downey's penchant for shouting during his talk show.

In "The Simpsons" season 1 episode "Some Enchanted Evening", a reporter resembling Downey is seen hosting a show called "America's Most Armed & Dangerous".

In June 1996, Downey was diagnosed with lung cancer while being treated for pneumonia, and had one of his lungs removed. He did a complete about-face on the issue of tobacco use, going from a one-time member of the National Smokers Alliance to a staunch anti-smoking activist. He continued to speak against smoking until his death from lung cancer and pneumonia on March 12, 2001.

After being diagnosed with lung cancer, he commented, "I had spawned a generation of kids to think it was cool to smoke a cigarette. Kids walked up to me until a matter of weeks ago, they'd have a cigarette in their hand and they'd say, 'Hey, Mort,' or, 'Hey, Mouth, autograph my cigarette.' And I'd do it." He also blamed tobacco companies for lying to consumers about cigarettes.




</doc>
<doc id="18886" url="https://en.wikipedia.org/wiki?curid=18886" title="List of male singles tennis players">
List of male singles tennis players

This is a list of top international male singles tennis players, both past and present.

It includes players who have been officially ranked among the top 25 singles players in the world during the "Open Era"; been ranked in the top 10 prior to the Open Era; have been a singles quarterfinalist or better at a Grand Slam tournament; have reached the finals of or won the season-ending event; or have been singles medalists at the Olympics.

Players who have won more than one Grand Slam singles title or have been ranked world no. 1 in singles have been put in bold font. Players who are still active on the tour have been put in "italics".

See also: Tyler "Bone" Jackson



</doc>
<doc id="18887" url="https://en.wikipedia.org/wiki?curid=18887" title="Metaphilosophy">
Metaphilosophy

Metaphilosophy (sometimes called philosophy of philosophy) is "the investigation of the nature of philosophy". Its subject matter includes the aims of philosophy, the boundaries of philosophy, and its methods. Thus, while philosophy characteristically inquires into the nature of being, the reality of objects, the possibility of knowledge, the nature of truth, and so on, metaphilosophy is the self-reflective inquiry into the nature, aims, and methods of the activity that makes these kinds of inquiries, by asking what "is" philosophy itself, what sorts of questions it should ask, how it might pose and answer them, and what it can achieve in doing so. It is considered by some to be a subject prior and preparatory to philosophy, while others see it as inherently a part of philosophy, or automatically a part of philosophy while others adopt some combination of these views. The interest in metaphilosophy led to the establishment of the journal "Metaphilosophy" in January 1970.

Although the "term" metaphilosophy and explicit attention to metaphilosophy as a specific domain within philosophy arose in the 20th century, the topic is likely as old as philosophy itself, and can be traced back at least as far as the works of Plato and Aristotle.

Some philosophers consider metaphilosophy to be a subject apart from philosophy, above or beyond it, while others object to that idea. Timothy Williamson argues that the philosophy of philosophy is "automatically part of philosophy", as is the philosophy of anything else. Nicholas Bunnin and Jiyuan Yu write that the separation of first- from second-order study has lost popularity as philosophers find it hard to observe the distinction. As evidenced by these contrasting opinions, debate persists as to whether the evaluation of the nature of philosophy is 'second order philosophy' or simply 'plain philosophy'.

Many philosophers have expressed doubts over the value of metaphilosophy. Among them is Gilbert Ryle : "preoccupation with questions about methods tends to distract us from prosecuting the methods themselves. We run as a rule, worse, not better, if we think a lot about our feet. So let us... not speak of it all but just do it."

The designations "metaphilosophy" and "philosophy of philosophy" have a variety of meanings, sometimes taken to be synonyms, and sometimes seen as distinct.

Morris Lazerowitz claims to have coined the term 'metaphilosophy' around 1940 and used it in print in 1942. Lazerowitz proposed that metaphilosophy is 'the investigation of the nature of philosophy'. Earlier uses have been found in translations from the French. The term is derived from Greek word "meta" μετά ("after", "beyond", "with") and "philosophía" φιλοσοφία ("love of wisdom").

The term 'metaphilosophy' is used by Paul Moser in the sense of a 'second-order' or more fundamental undertaking than philosophy itself, in the manner suggested by Charles Griswold:

This usage was considered nonsense by Ludwig Wittgenstein, who rejected the analogy between metalanguage and a metaphilosophy. As expressed by Martin Heidegger:
Some other philosophers treat the prefix "meta" as simply meaning '"about..."', rather than as referring to a metatheoretical 'second-order' form of philosophy, among them Rescher and Double. Others, such as Williamson, prefer the term "'philosophy of philosophy"' instead of 'metaphilosophy' as it avoids the connotation of a 'second-order' discipline that looks down on philosophy, and instead denotes something that is a part of it. Joll suggests that to take metaphilosophy as 'the application of the methods of philosophy to philosophy itself' is too vague, while the view that sees metaphilosophy as a 'second-order' or more abstract discipline, outside philosophy, "is narrow and tendentious".

In the analytical tradition, the term "metaphilosophy" is mostly used to tag commenting and research on previous works as opposed to original contributions towards solving philosophical problems.

Ludwig Wittgenstein wrote about the nature of philosophical puzzles and philosophical understanding. He suggested philosophical errors arose from confusions about the nature of philosophical inquiry. In the "Philosophical Investigations", Wittgenstein wrote that there is not a metaphilosophy in the sense of a metatheory of philosophy.

C. D. Broad distinguished Critical from Speculative philosophy in his "The Subject-matter of Philosophy, and its Relations to the special Sciences," in "Introduction to Scientific Thought", 1923. Curt Ducasse, in "Philosophy as a Science", examines several views of the nature of philosophy, and concludes that philosophy has a distinct subject matter: appraisals. Ducasse's view has been among the first to be described as 'metaphilosophy'.

Henri Lefebvre in "Metaphilosophie" (1965) argued, from a Marxian standpoint, in favor of an "ontological break", as a necessary methodological approach for critical social theory (whilst criticizing Louis Althusser's "epistemological break" with subjective Marxism, which represented a fundamental theoretical tool for the school of Marxist structuralism).

Paul Moser writes that typical metaphilosophical discussion includes determining the conditions under which a claim can be said to be a philosophical one. He regards meta-ethics, the study of ethics, to be a form of metaphilosophy, as well as meta-epistemology, the study of epistemology.

Many sub-disciplines of philosophy have their own branch of 'metaphilosophy', examples being Meta-aesthetics, Meta-epistemology, Meta-ethics, Meta-ontology, and so forth. However, some topics within 'metaphilosophy' cut across the various subdivisions of philosophy to consider fundamentals important to all its sub-disciplines. Some of these are mentioned below.

Cognitivity

Systematicity

Methodology

Historicity

Self-reference and Self-application

Immanence and non-immanence

Disagreement and diversity

Primacy of the practical

Philosophy good and bad

Philosophy and expertise

Ends of philosophy

Death of philosophy

Anti-philosophies

Philosophy and assertion

Philosophy and exposition
Philosophy and style

Philosophy as literature
Literature as philosophy
Philosophical beauty
Philosophy as science
Philosophy and related fields and activities
Philosophy and argument

Philosophy and wisdom

Philosophy and metaphilosophy

Philosophy and the folk

Philosophy and 'primitive' life

Philosophy and philosophers

Philosophy and pedagogy https://legacy.earlham.edu/~peters/courses/meta/topics.htm

Some philosophers (e.g. existentialists, pragmatists) think philosophy is ultimately a practical discipline that should help us lead meaningful lives by showing us who we are, how we relate to the world around us and what we should do. Others (e.g. analytic philosophers) see philosophy as a technical, formal, and entirely theoretical discipline, with goals such as "the disinterested pursuit of knowledge for its own sake". Other proposed goals of philosophy include "discover[ing] the absolutely fundamental reason of everything it investigates", "making explicit the nature and significance of ordinary and scientific beliefs", and unifying and transcending the insights given by science and religion. Others proposed that philosophy is a complex discipline because it has 4 or 6 different dimensions.

Defining philosophy and its boundaries is itself problematic; Nigel Warburton has called it "notoriously difficult". There is no straightforward definition, and most interesting definitions are controversial. As Bertrand Russell wrote:

While there is some agreement that philosophy involves general or fundamental topics, there is no clear agreement about a series of demarcation issues, including:



Philosophical method (or philosophical methodology) is the study of how to do philosophy. A common view among philosophers is that philosophy is distinguished by the ways that philosophers follow in addressing philosophical questions. There is not just one method that philosophers use to answer philosophical questions.

Recently, some philosophers have cast doubt about intuition as a basic tool in philosophical inquiry, from Socrates up to contemporary philosophy of language. In "Rethinking Intuition" various thinkers discard intuition as a valid source of knowledge and thereby call into question 'a priori' philosophy. Experimental philosophy is a form of philosophical inquiry that makes at least partial use of empirical research—especially "opinion polling"—in order to address persistent philosophical questions. This is in contrast with the methods found in analytic philosophy, whereby some say a philosopher will sometimes begin by appealing to his or her intuitions on an issue and then form an argument with those intuitions as premises. However, disagreement about what experimental philosophy can accomplish is widespread and several philosophers have offered criticisms. One claim is that the empirical data gathered by experimental philosophers can have an indirect effect on philosophical questions by allowing for a better understanding of the underlying psychological processes which lead to philosophical intuitions.

A prominent question in metaphilosophy is that of whether or not philosophical progress occurs and more so, whether such progress in philosophy is even possible. It has even been disputed, most notably by Ludwig Wittgenstein, whether genuine philosophical problems actually exist. The opposite has also been claimed, for example by Karl Popper, who held that such problems do exist, that they are solvable, and that he had actually found definite solutions to some of them.

David Chalmers divides inquiry into philosophical progress in metaphilosophy into three questions.





</doc>
<doc id="18888" url="https://en.wikipedia.org/wiki?curid=18888" title="Mandolin">
Mandolin

A mandolin ( ; literally "small mandola") is a stringed musical instrument in the lute family and is usually plucked with a plectrum or "pick". It commonly has four courses of doubled metal strings tuned in unison (8 strings), although five (10 strings) and six (12 strings) course versions also exist. The courses are normally tuned in a succession of perfect fifths. It is the soprano member of a family that includes the mandola, octave mandolin, mandocello and mandobass.

There are many styles of mandolin, but three are common, the "Neapolitan" or "round-backed" mandolin, the "carved-top" mandolin and the "flat-backed" mandolin. The round-back has a deep bottom, constructed of strips of wood, glued together into a bowl. The carved-top or "arch-top" mandolin has a much shallower, arched back, and an arched top—both carved out of wood. The flat-backed mandolin uses thin sheets of wood for the body, braced on the inside for strength in a similar manner to a guitar. Each style of instrument has its own sound quality and is associated with particular forms of music. Neapolitan mandolins feature prominently in European classical music and traditional music. Carved-top instruments are common in American folk music and bluegrass music. Flat-backed instruments are commonly used in Irish, British and Brazilian folk music. Some modern Brazilian instruments feature an extra fifth course tuned a fifth lower than the standard fourth course.

Other mandolin varieties differ primarily in the number of strings and include four-string models (tuned in fifths) such as the Brescian and Cremonese, six-string types (tuned in fourths) such as the Milanese, Lombard and the Sicilian and 6 course instruments of 12 strings (two strings per course) such as the Genoese. There has also been a twelve-string (three strings per course) type and an instrument with sixteen-strings (four strings per course).

Much of mandolin development revolved around the soundboard (the top). Pre-mandolin instruments were quiet instruments, strung with as many as six courses of gut strings, and were plucked with the fingers or with a quill. However, modern instruments are louder—using four courses of metal strings, which exert more pressure than the gut strings. The modern soundboard is designed to withstand the pressure of metal strings that would break earlier instruments. The soundboard comes in many shapes—but generally round or teardrop-shaped, sometimes with scrolls or other projections. There is usually one or more "sound holes" in the soundboard, either round, oval, or shaped like a calligraphic (f-hole). A round or oval sound hole may be covered or bordered with decorative rosettes or purfling.

Mandolins evolved from the lute family in Italy during the 17th and 18th centuries, and the deep bowled mandolin, produced particularly in Naples, became common in the 19th century.

Dating to c. 13,000 BC, a cave painting in the Trois Frères cave in France depicts what some believe is a musical bow, a hunting bow used as a single-stringed musical instrument. From the musical bow, families of stringed instruments developed; since each string played a single note, adding strings added new notes, creating bow harps, harps and lyres. In turn, this led to being able to play dyads and chords. Another innovation occurred when the bow harp was straightened out and a bridge used to lift the strings off the stick-neck, creating the lute.

This picture of musical bow to harp bow is theory and has been contested. In 1965 Franz Jahnel wrote his criticism stating that the early ancestors of plucked instruments are not currently known. He felt that the harp bow was a long cry from the sophistication of the 4th-century BC civilization that took the primitive technology and created "technically and artistically well made harps, lyres, citharas and lutes."

Musicologists have put forth examples of that 4th-century BC technology, looking at engraved images that have survived. The earliest image showing a lute-like instrument came from Mesopotamia prior to 3000 BC. A cylinder seal from c. 3100 BC or earlier (now in the possession of the British Museum) shows what is thought to be a woman playing a stick lute. From the surviving images, theororists have categorized the Mesopotamian lutes, showing that they developed into a long variety and a short. The line of long lutes may have developed into the tamburs and pandura. The line of short lutes was further developed to the east of Mesopotamia, in Bactria, Gandhara, and Northwest India, and shown in sculpture from the 2nd century BC through the 4th or 5th centuries AD.

Bactria and Gandhara became part of the Sasanian Empire (224–651 AD). Under the Sasanians, a short almond shaped lute from Bactria came to be called the barbat or barbud, which was developed into the later Islamic world's "oud" or "ud". When the Moors conquered Andalusia in 711 AD, they brought their ud along, into a country that had already known a lute tradition under the Romans, the pandura.

During the 8th and 9th centuries, many musicians and artists from across the Islamic world flocked to Iberia. Among them was Abu l-Hasan ‘Ali Ibn Nafi‘ (789–857), a prominent musician who had trained under Ishaq al-Mawsili (d. 850) in Baghdad and was exiled to Andalusia before 833 AD. He taught and has been credited with adding a fifth string to his oud and with establishing one of the first schools of music in Córdoba.

By the 11th century, Muslim Iberia had become a center for the manufacture of instruments. These goods spread gradually to Provence, influencing French troubadours and trouvères and eventually reaching the rest of Europe.

Beside the introduction of the lute to Spain (Andalusia) by the Moors, another important point of transfer of the lute from Arabian to European culture was Sicily, where it was brought either by Byzantine or later by Muslim musicians. There were singer-lutenists at the court in Palermo following the Norman conquest of the island from the Muslims, and the lute is depicted extensively in the ceiling paintings in the Palermo’s royal Cappella Palatina, dedicated by the Norman King Roger II of Sicily in 1140. His Hohenstaufen grandson Frederick II, Holy Roman Emperor (1194 - 1250) continued integrating Muslims into his court, including Moorish musicians. By the 14th century, lutes had disseminated throughout Italy and, probably because of the cultural influence of the Hohenstaufen kings and emperor, based in Palermo, the lute had also made significant inroads into the German-speaking lands.

A distinct European tradition of lute development is noticeable in pictures and sculpture from the 13th century onward. As early as the beginning of the 14th century, strings were doubled into courses on the miniature lute or gittern, used throughout Europe. The small soundhole shaped like a "3" or a "W", typical of Muslim-made instruments and seen in the Cantigas de Santa Maria illustrations on instruments played by Europeans, were not typical of European instruments. Instead the European instruments largely used a "C", "D" or "B"-shaped soundhole, or a round soundhole, which might be covered with a rose decoration. The "mandore", appeared in the late 16th century and although known here under a French name, was used elsewhere as indicated by the names in other European languages (German "mandoer", Spanish "vandola", and Italian "mandola").

The mandore was not a final form, and the design was tinkered with wherever it was built. The Italians, for instance, redesigned their "mandola" and produced the "mandolino" or "Baroque mandolin", a small catgut-strung mandola, strung in 4, 5 or 6 courses tuned in fourths: e′–a′–d″–g″, b–e′–a′–d″–g″ or g–b–e′–a′–d″–g″, and played finger-style.

The first evidence of modern metal-string mandolins is from literature regarding popular Italian players who travelled through Europe teaching and giving concerts. Notable are Signor Gabriele Leone, Giovanni Battista Gervasio, Pietro Denis, who travelled widely between 1750 and 1810. This, with the records gleaned from the Italian Vinaccia family of luthiers in Naples, Italy, led some musicologists to believe that the modern steel-string mandolins were developed in Naples by the Vinaccia family.

Not limited to mandolins, the Vinaccias made stringed instruments, including violins, cellos, guitars, mandolas and mandolins. Noted members of the family who made mandolins are known today from labels inside of surviving instruments and include Vincenzo, Giovanni, Domenico, and Antonio (and his sons Gaetano and Gennaro, grandson Pasquale and great-grandsons Gennaro and Achille). The mandolins they made changed over generations, from mandolinos with flat soundboards and gut-strings, through mandolins with a bent soundboard and bronze or bronze-and-gut strings, into mandolins with bent soundboards that used steel or steel-and-bronze strings.

Pasquale Vinaccia (1806–c. 1882), modernized the mandolin, adding features, creating the "Neapolitan" mandolin c. 1835. Pasquale remodeled, raised and extended the fingerboard to 17 frets, introduced stronger wire strings made of high-tension steel and substituted a machine head for the friction tuning pegs, then standard. The new wire strings required that he strengthen the mandolin's body, and he deepened the mandolin's bowl, giving the tonal quality more resonance. He didn't introduce the bent soundboard, as it was present in some of the instruments made by the previous generation for bronze strings.

Other luthiers who built mandolins included Raffaele Calace (1863 onwards) in Naples, Luigi Embergher (1856–1943) in Rome and Arpino, the Ferrari family (1716 onwards, also originally mandolino makers) in Rome, and De Santi (1834–1916) in Rome. The Neapolitan style of mandolin construction was adopted and developed by others, notably in Rome, giving two distinct but similar types of mandolin – Neapolitan and Roman.

The transition from the mandolino to the mandolin began around 1744 with the designing of the metal-string mandolin by the Vinaccia family, 3 brass strings and one of gut, using friction tuning pegs on a fingerboard that sat "flush" with the sound table. Names of other mandolin luthiers from this era include Carlo Guadagnini (son of Giovanni Battista Guadagnini) and Gaspare Ferrari, both of whom have instruments in the collection of the Music Museum in Venice. The mandolin grew in popularity over the next 60 years, in the streets where it was used by young men courting and by street musicians, and in the concert hall. After the Napoleonic Wars of 1815, however, its popularity began to fall. The 19th century produced some prominent players, including Bartolomeo Bortolazzi of Venice and Pietro Vimercati. However, professional virtuosity was in decline, and the mandolin music changed as the mandolin became a folk instrument; "the large repertoire of notated instrumental music for the mandolino and the mandoline was completely forgotten". The export market for mandolins from Italy dried up around 1815, and when Carmine de Laurentiis wrote a mandolin method in 1874, the "Music World" magazine wrote that the mandolin was "out of date." Salvador Léonardi mentioned this decline in his 1921 book, "Méthode pour Banjoline ou Mandoline-Banjo", saying that the mandolin had been declining in popularity from previous times.

It was during this slump in popularity (specifically in 1835) that Pasquale Vinaccia made his modifications to the instrument that his family made for generations, creating the Neapolitan mandolin. The mandolin was largely forgotten outside Italy by that point, but the stage was set for it to become known again, starting with the Paris Exposition in 1878.

Beginning with the Paris Exposition of 1878, the instrument's popularity began to rebound. The Exposition was one of many stops for the "Estudiantes Españoles" ("Spanish Students"). There has been confusion regarding this group.

The original Estudiantes Española or Estudiantina Española was a group of 64 students formed by 26 February 1878, principally from Madrid colleges. They dressed in historical clothing, representing ancient sophists of Salamanca and Alcala and traveled to Paris for Carnival staying from March 2 through March 15. This early group of students played flutes, guitars violins, bandurrias, flutes and tambourines. This early group was led by Ildefonso de Zabaleta (president) and Joaquin de Castañeda (vice president). The group performed before large audiences in Paris (reports of 10,000 and 56,000 people showing up for a night's entertainment were reported).

Their success in Paris preceded a second group of Spanish performers, known as the Esudiantina Figaro or Esudiantina Española Figaroa (Figaro Band of Spanish Students). This group was founded by Dionisio Granados and toured Europe dancing and playing guitars, violins and the bandurria, which became confused with the mandolin.

Along with their energy and the newfound awareness of the instrument created by the day's hit sensation, a wave of Italian mandolinists travelled Europe in the 1880s and 1890s and in the United States by the mid-1880s, playing and teaching their instrument. The instrument's popularity continued to increase during the 1890s and mandolin popularity was at its height in the "early years of the 20th century." Thousands were taking up the instrument as a pastime, and it became an instrument of "society", taken up by young men and women. Mandolin orchestras were formed worldwide, incorporating not only the mandolin family of instruments, but also guitars, double basses and zithers.

That era (from the late 19th century into the early 20th century) has come to be known as the "Golden Age" of the mandolin. The term is used online by mandolin enthusiasts to name the time period when the mandolin had become popular, when mandolin orchestras were being organized worldwide, and new and high-quality instruments were increasingly common.

After the First World War, the instrument's popularity again fell, though gradually. Reasons cited include the rise of Jazz, for which the instrument was too quiet. Also, modern conveniences (phonograph records, bicycle and automobiles, outdoor sports) competed with learning to play an instrument for fun.

The second decline was not as complete as the first. Thousands of people had learned to play the instrument. Even as the second wave of mandolin popularity declined in the early 20th century, players began using new versions of the mandolin in new forms of music. Luthiers created the resonator mandolin, the flatback mandolin, the carved-top or arched-top mandolin, the mandolin-banjo and the electric mandolin. Musicians began playing it in Celtic, Bluegrass, Jazz and Rock-n-Roll styles — and "Classical" too.

Mandolins have a body that acts as a resonator, attached to a neck. The resonating body may be shaped as a bowl () or a box (). Traditional Italian mandolins, such as the Neapolitan mandolin, meet the necked bowl description. The necked box instruments include the carved top mandolins and the flatback mandolins.

Strings run between mechanical tuning machines at the top of the neck to a tailpiece that anchors the other end of the strings. The strings are suspended over the neck and soundboard and pass over a floating bridge. The bridge is kept in contact with the soundboard by the downward pressure from the strings. The neck is either flat or has a slight radius, and is covered with a fingerboard with frets. The action of the strings on the bridge causes the soundboard to vibrate, producing sound.

Like any plucked instrument, mandolin notes decay to silence rather than sound out continuously as with a bowed note on a violin, and mandolin notes decay faster than larger stringed instruments like the guitar. This encourages the use of tremolo (rapid picking of one or more pairs of strings) to create sustained notes or chords. The mandolin's paired strings facilitate this technique: the plectrum (pick) strikes each of a pair of strings alternately, providing a more full and continuous sound than a single string would.

Various design variations and amplification techniques have been used to make mandolins comparable in volume with louder instruments and orchestras, including the creation of mandolin-banjo hybrid with the louder banjo, adding metal resonators (most notably by Dobro and the National String Instrument Corporation) to make a resonator mandolin, and amplifying electric mandolins through amplifiers.

A variety of different tunings are used. Usually, courses of 2 adjacent strings are tuned in unison. By far the most common tuning is the same as violin tuning, in scientific pitch notation G–D–A–E, or in Helmholtz pitch notation: g–d′–a′–e″.


Note that the numbers of Hz shown above assume a 440 Hz A, standard in most parts of the western world. Some players use an A up to 10 Hz above or below a 440, mainly outside the United States.

Other tunings exist, including "cross-tunings", in which the usually doubled string runs are tuned to different pitches. Additionally, guitarists may sometimes tune a mandolin to mimic a portion of the intervals on a standard guitar tuning to achieve familiar fretting patterns.

The mandolin is the soprano member of the mandolin family, as the violin is the soprano member of the violin family. Like the violin, its scale length is typically about . Modern American mandolins modelled after Gibsons have a longer scale, about . The strings in each of its double-strung courses are tuned in unison, and the courses use the same tuning as the violin: G–D–A–E.

The piccolo or sopranino mandolin is a rare member of the family, tuned one octave above the mandola and one fourth above the mandolin (C–G–D–A); the same relation as that of the piccolo or sopranino violin to the violin and viola. One model was manufactured by the Lyon & Healy company under the Leland brand. A handful of contemporary luthiers build piccolo mandolins. Its scale length is typically about .





The mandolone was a Baroque member of the mandolin family in the bass range that was surpassed by the mandocello. Built as part of the Neapolitan mandolin family.

The Greek laouto or laghouto (long-necked lute) is similar to a mandocello, ordinarily tuned C/C–G/G–D/D–A/A with half of each pair of the lower two courses being tuned an octave high on a lighter gauge string. The body is a staved bowl, the saddle-less bridge glued to the flat face like most ouds and lutes, with mechanical tuners, steel strings, and tied gut frets. Modern laoutos, as played on Crete, have the entire lower course tuned to C, a reentrant octave above the expected low C. Its scale length is typically about .

The Algerian mandole was developed by an Italian luthier in the early 1930s, scaled up from a mandola until it reached a scale length of approximately 25-27 inches. It is a flatback instrument, with a wide neck and 4 courses (8 strings), 5 courses (10 strings) or 6 courses (12 strings). Used in music in Algeria and Morocco. The instrument can be tuned as a guitar, oud or mandocello, depending on the music it will be used to play and player preference. When tuning it as a guitar the strings will be tuned (E) (E) A A D D G G B B (E) (E). Strings in parenthesis are dropped for a five or four course instrument. Using a common Arabic oud tuning D D G G A A D D (G) (G) (C) (C). For a mandocello tuning using fifths C C G G D D A A (E) (E).


Bowlback mandolins (also known as roundbacks), are used worldwide. They are most commonly manufactured in Europe, where the long history of mandolin development has created local styles. However, Japanese luthiers also make them.

Owing to the shape and to the common construction from wood strips of alternating colors, in the United States these are sometimes colloquially referred to as the "potato bug" or "potato beetle" mandolin.

The Neapolitan style has an almond-shaped body resembling a bowl, constructed from curved strips of wood. It usually has a bent sound table, canted in two planes with the design to take the tension of the eight metal strings arranged in four courses. A hardwood fingerboard sits on top of or is flush with the sound table. Very old instruments may use wooden tuning pegs, while newer instruments tend to use geared metal tuners. The bridge is a movable length of hardwood. A pickguard is glued below the sound hole under the strings. European roundbacks commonly use a scale instead of the common on archtop Mandolins.

Intertwined with the Neapolitan style is the Roman style mandolin, which has influenced it. The Roman mandolin had a fingerboard that was more curved and narrow. The fingerboard was lengthened over the sound hole for the E strings, the high pitched strings. The shape of the back of the neck was different, less rounded with an edge, the bridge was curved making the G strings higher. The Roman mandolin had mechanical tuning gears before the Neapolitan.

Prominent Italian manufacturers include Vinaccia (Naples), Embergher (Rome) and Calace (Naples). Other modern manufacturers include Lorenzo Lippi (Milan), Hendrik van den Broek (Netherlands), Brian Dean (Canada), Salvatore Masiello and Michele Caiazza (La Bottega del Mandolino) and Ferrara, Gabriele Pandini.

Another family of bowlback mandolins came from Milan and Lombardy. These mandolins are closer to the mandolino or mandore than other modern mandolins. They are shorter and wider than the standard Neapolitan mandolin, with a shallow back. The instruments have 6 strings, 3 wire treble-strings and 3 gut or wire-wrapped-silk bass-strings. The strings ran between the tuning pegs and a bridge that was glued to the soundboard, as a guitar's. The Lombardic mandolins were tuned g–b–e′–a′–d″–g″ (shown in Helmholtz pitch notation). A developer of the Milanese stye was Antonio Monzino (Milan) and his family who made them for 6 generations.

Samuel Adelstein described the Lombardi mandolin in 1893 as wider and shorter than the Neapolitan mandolin, with a shallower back and a shorter and wider neck, with six single strings to the regular mandolin's set of 4. The Lombardi was tuned C–D–A–E–B–G. The strings were fastened to the bridge like a guitar's. There were 20 frets, covering three octaves, with an additional 5 notes. When Adelstein wrote, there were no nylon strings, and the gut and single strings "do not vibrate so clearly and sweetly as the double steel string of the Neapolitan."

Brescian mandolins that have survived in museums have four gut strings instead of six. The mandolin was tuned in fifths, like the Neapolitan mandolin.

In his 1805 mandolin method, "Anweisung die Mandoline von selbst zu erlernen nebst einigen Uebungsstucken von Bortolazzi", Bartolomeo Bortolazzi popularised the Cremonese mandolin, which had four single-strings and a fixed bridge, to which the strings were attached. Bortolazzi said in this book that the new wire strung mandolins were uncomfortable to play, when compared with the gut-string instruments. Also, he felt they had a "less pleasing...hard, zither-like tone" as compared to the gut string's "softer, full-singing tone."
He favored the four single strings of the Cremonese instrument, which were tuned the same as the Neapolitan.

In the United States, when the bowlback was being made in numbers, Lyon and Healy was a major manufacturer, especially under the "Washburn" brand. Other American manufacturers include Martin, Vega, and Larson Brothers.

In Canada, Brian Dean has manufactured instruments in Neapolitan, Roman, German and American styles but is also known for his original 'Grand Concert' design created for American virtuoso Joseph Brent.

German manufacturers include Albert & Mueller, Dietrich, Klaus Knorr, Reinhold Seiffert and Alfred Woll. The German bowlbacks use a style developed by Seiffert, with a larger and rounder body.

Japanese brands include Kunishima and Suzuki. Other Japanese manufacturers include Oona, Kawada, Noguchi, Toichiro Ishikawa, Rokutaro Nakade, Otiai Tadao, Yoshihiko Takusari, Nokuti Makoto, Watanabe, Kanou Kadama and Ochiai.

At the very end of the 19th century, a new style, with a carved top and back construction inspired by violin family instruments began to supplant the European-style bowl-back instruments in the United States. This new style is credited to mandolins designed and built by Orville Gibson, a Kalamazoo, Michigan luthier who founded the "Gibson Mandolin-Guitar Manufacturing Co., Limited" in 1902. Gibson mandolins evolved into two basic styles: the Florentine or F-style, which has a decorative scroll near the neck, two points on the lower body and usually a scroll carved into the headstock; and the A-style, which is pear shaped, has no points and usually has a simpler headstock.

These styles generally have either two f-shaped soundholes like a violin (F-5 and A-5), or an oval sound hole (F-4 and A-4 and lower models) directly under the strings. Much variation exists between makers working from these archetypes, and other variants have become increasingly common. Generally, in the United States, Gibson F-hole F-5 mandolins and mandolins influenced by that design are strongly associated with bluegrass, while the A-style is associated other types of music, although it too is most often used for and associated with bluegrass. The F-5's more complicated woodwork also translates into a more expensive instrument.

Internal bracing to support the top in the F-style mandolins is usually achieved with parallel tone bars, similar to the bass bar on a violin. Some makers instead employ "X-bracing," which is two tone bars mortised together to form an X. Some luthiers now using a "modified x-bracing" that incorporates both a tone bar and X-bracing.

Numerous modern mandolin makers build instruments that largely replicate the Gibson F-5 Artist models built in the early 1920s under the supervision of Gibson acoustician Lloyd Loar. Original Loar-signed instruments are sought after and extremely valuable. Other makers from the Loar period and earlier include Lyon and Healy, Vega and Larson Brothers.

Flatback mandolins use a thin sheet of wood with bracing for the back, as a guitar uses, rather than the bowl of the bowlback or the arched back of the carved mandolins.

Like the bowlback, the flatback has a round sound hole. This has been sometimes modified to an elongated hole, called a D-hole. The body has a rounded almond shape with flat or sometimes canted soundboard.

The type was developed in Europe in the 1850s. The French and Germans called it a Portuguese mandolin, although they also developed it locally. The Germans used it in Wandervogel.

The bandolim is commonly used wherever the Spanish and Portuguese took it: in South America, in Brazil (Choro) and in the Philippines.

In the early 1970s English luthier Stefan Sobell developed a large-bodied, flat-backed mandolin with a carved soundboard, based on his own cittern design; this is often called a 'Celtic' mandolin.

American forms include the Army-Navy mandolin, the flatiron and the pancake mandolins.

The tone of the flatback is described as warm or mellow, suitable for folk music and smaller audiences. The instrument sound does not punch through the other players' sound like a carved top does.

The double top is a feature that luthiers are experimenting with in the 21st century, to get better sound. 
However, mandolinists and luthiers have been experimenting with them for at least since the early 1900s.

Back in the early 1900s, mandolinist Ginislao Paris approached Luigi Embergher to build custom mandolins. The sticker inside one of the four surviving instruments indicates the build was called after him, the "Sistema Ginislao Paris"). Paris' round-back double-top mandolins use a false back below the soundboard to create a second hollow space within the instrument.

Modern mandolinists such as Avi Avital and Joseph Brent use instruments customized, either by the luthier's choice or at the request of player. Joseph Brent's mandolin, made by Brian Dean also uses what Brent calls a false back. Brent's mandolin was the luthier's solution to Brent's request for a loud mandolin in which the wood was clearly audible, with less metallic sound from the strings. The type used by Avital is variation of the flatback, with a double top that encloses a resonating chamber, sound holes on the side, and a convex back. It is made by one manufacturer in Israel, luthier Arik Kerman. Other players of Kerman mandolins include Alon Sariel, Jacob Reuven, and Tom Cohen.

Other American-made variants include the mandolinetto or Howe-Orme guitar-shaped mandolin (manufactured by the Elias Howe Company between 1897 and roughly 1920), which featured a cylindrical bulge along the top from fingerboard end to tailpiece and the Vega mando-lute (more commonly called a cylinder-back mandolin manufactured by the Vega Company between 1913 and roughly 1927), which had a similar longitudinal bulge but on the back rather than the front of the instrument.

The mandolin was given a banjo body in an 1882 patent by Benjamin Bradbury of Brooklyn and given the name "banjolin" by John Farris in 1885. Today "banjolin" describes an instrument with four strings, while the version with the four courses of double strings is called a "mandolin-banjo".

A resonator mandolin or "resophonic mandolin" is a mandolin whose sound is produced by one or more metal cones (resonators) instead of the customary wooden soundboard (mandolin top/face). Historic brands include Dobro and National.

As with almost every other contemporary string instrument, another modern variant is the electric mandolin. These mandolins can have four or five individual or double courses of strings.

They have been around since the late 1920s or early 1930s depending on the brand. They come in solid body and acoustic electric forms.

Instruments have been designed that overcome the mandolin's lack of sustain with its plucked notes. Fender released a model in 1992 with an additional string (a high a, above the e string), a tremolo bridge and extra humbucker pickup (total of two). The result was an instrument capable of playing heavy metal style guitar riffs or violin-like passages with sustained notes that can be adjusted as with an electric guitar.

The international repertoire of music for mandolin is almost unlimited, and musicians use it to play various types of music. This is especially true of violin music, since the mandolin has the same tuning as the violin. Following its invention and early development in Italy the mandolin spread throughout the European continent. The instrument was primarily used in a classical tradition with Mandolin orchestras, so called "Estudiantinas" or in Germany "Zupforchestern" appearing in many cities. Following this continental popularity of the mandolin family local traditions appeared outside Europe in the Americas and in Japan. Travelling mandolin virtuosi like Giuseppe Pettine, Raffaele Calace and Silvio Ranieri contributed to the mandolin becoming a "fad" instrument in the early 20th century. This "mandolin craze" was fading by the 1930s, but just as this practice was falling into disuse, the mandolin found a new niche in American country, old-time music, bluegrass and folk music. More recently, the Baroque and Classical mandolin repertory and styles have benefited from the raised awareness of and interest in Early music, with media attention to classical players such as Israeli Avi Avital, Italian Carlo Aonzo and American Joseph Brent.

Algeria was colonized by the French in the 19th century and there were large numbers of Europeans living there during the mandolin's golden age. Mandolins and larger members of the mandolin family were used in orchestras, including orchestras playing native Algerian music. With the decline of the mandolin worldwide, the mandolin became less common until by the 21st century it was rare. However, mandolins and mandolas are still occasionally made by luthiers. The major mandolin-family instrument in use today is the mandocello-sized mondol or "mandole" (French word for mandola applied to the new instrument). The flatback instrument was the result of a corroboration between an Italian luthier and an Algerian musician and was used initially for Chaabi. It has since spread to other music forms. Prominent players today include Mohamed Rouane, Takfarinas, Mohamed Abdennour (P'tit Moh), Abderrahmane Abdelli and Kader Fahem. Past players include El Hadj M'Hamed El Anka, Boudjemaa El Ankis, El Hachemi Guerouabi, Amar Ezzahi, Cheikh El Hasnaoui, and Lounès Matoub. The mondol received some international attention on the movie "El Gusto" a featuring the reunion of some chaabi players (including a mondol player) years after the turmoil in Algeria came out in the 2010s.

The earliest references to the mandolin in Australia come from Phil Skinner (1903–1991). In his article "Recollections" he mentions a Walter Stent, who was "active in the early part of the century and organised possibly the first Mandolin Orchestra in Sydney."

Phil Skinner played a key role in 20th century development of the mandolin movement in Australia, and was awarded an MBE in 1979 for services to music and the community. He was born Harry Skinner in Sydney in 1903 and started learning music at age 10 when his uncle tutored him on the banjo. Skinner began teaching part-time at age 18, until the Great Depression forced him to begin teaching full-time and learn a broader range of instruments. Skinner founded the Sydney Mandolin Orchestra, the oldest surviving mandolin orchestra in Australia.

The Sydney Mandolins (Artistic Director: Adrian Hooper) have contributed greatly to the repertoire through commissioning over 200 works by Australian and International composers. Most of these works have been released on Compact Disks and can regularly be heard on radio stations on the ABC and MBS networks. One of their members, mandolin virtuoso Paul Hooper, has had a number of Concertos written for him by composers such as Eric Gross. He has performed and recorded these works with the Sydney Symphony Orchestra and the Tasmanian Symphony Orchestra. As well, Paul Hooper has had many solo works dedicated to him by Australian composers e.g., Caroline Szeto, Ian Shanahan, Larry Sitsky and Michael Smetanin.

In January 1979, the Federation of Australian Mandolin Ensembles (FAME) Inc. formed. Bruce Morey from Melbourne is the first FAME President. An Australian Mandolin Orchestra toured Germany in May 1980.

Australian popular groups such as My Friend The Chocolate Cake use the mandolin extensively. The McClymonts also use the mandolin, as do Mic Conway's National Junk Band and the Blue Tongue Lizards. Nevertheless, in folk and traditional styles, the mandolin remains more popular in Irish Music and other traditional repertoires.

In the early 20th century several mandolin orchestras (Estudiantinas) were active in Belgium. Today only a few groups remain: Royal Estudiantina la Napolitaine (founded in 1904) in Antwerp, Brasschaats mandoline orkest in Brasschaat and an orchestra in Mons (Bergen). Gerda Abts is a well known mandolin virtuoso in Belgium. She is also mandolin teacher and gives lessons in the music academies of Lier, Wijnegem and Brasschaat. She is now also professor mandolin at the music high school “Koninklijk Conservatorium Artesis Hogeschool Antwerpen”. She also gives various concerts each year in different ensembles. She is in close contact to the Brasschaat mandolin Orchestra. Her site is www.gevoeligesnaar.be

The mandolin has a particular shape in Brazilian music and is known as the "bandolim" in the Portuguese language, which is spoken there. The Portuguese have a long tradition of mandolins and mandolin-like instruments and brought their music to their colonies.

In modern Brazilian music, the bandolim is almost exclusively a melody instrument, often accompanied by the chordal accompaniment of the cavaquinho, a steel stringed instrument similar to a ukulele. The bandolim's popularity has risen and fallen with instrumental folk music styles, especially choro. The later part of the 20th century saw a renaissance of choro in Brazil, and with it, a revival of the country's mandolin tradition. Composer and mandolin virtuoso, Jacob do Bandolim, did much to popularize the instrument through many recordings, and his influence continues to the present day. Some contemporary mandolin players in Brazil include Jacob's disciple Déo Rian, and Hamilton de Holanda (the former, a traditional choro-style player, the latter an eclectic innovator.) Another is Joel Nascimento.

Western instruments such as the guitar, mandolin and ukulele were "associated with youth and freedom", "sophistication and cosmopolitanism" in Shanghai and Hong Kong in the 1920s and 1930s. They appeared in the hands of movie stars and in magazines like Ling Long with "fashionable young ladies," where they became part of the modern girl image. Modern girls were college-age women, perceived as active and modern. Independent and westernized, they engaged in western recreational activities such as "driving motorcycles, swimming, horse riding, horse racing, rowing competitions," and playing musical instruments.

Artist Guan Zilan was one such modern girl, educated in Chinese and Japanese colleges. Her artwork was displayed in "The Young Companion" magazine, and she herself was on the cover with her mandolin.

Mandolins were also seen in Shanghai in an advertisement poster of the East Asia Cigarette Company, which showed a woman, well-dressed, holding a mandolin. A variation of this image was used elsewhere as a calendar illustration.

Although prominent in media in the hands of women, the instruments were not solely women's. Young men in western-run schools could choose them up as part of their education. Trying to be like his brother, Chou Wen-Chung was able to "dabble in" the mandolin at a school in Nanjing.

During the 1920s and 30s when the mandolin came to China, its popularity had peaked worldwide. Although the imported western instruments never became common in China, they were advertised for sale in Shanghai in 1920, in English-language publications, such as "Our Navy, the Standard Publication of the U.S. Navy". When popularity for the fad began to fade after World War 1, China's neighbor Japan was an exception to that; mandolins were still popular and their use growing. Japan had commercial ties to coastal China, the very area from which the magazines and advertisements arose.

During the Chinese Cultural Revolution (1966-1976) it became dangerous in China to stick out from other people. The establishment feared "the taint of foreign ideas" and tried to root out the “representatives of the bourgeoisie who have sneaked into ... various spheres of culture.” Western classical music largely came to an end in China during the Cultural Revolution, when "listening to Beethoven became a political crime." At the Shanghai Conservatory of Music, western musical instruments were destroyed.

Mandolin is staple of folk and traditional music on the Croatian coast.

From Italy mandolin music extended in popularity throughout Europe in the early 20th century, with mandolin orchestras appearing throughout the continent.

In the 21st century an increased interest in bluegrass music, especially in Central European countries such as the Czech Republic and Slovak Republic, has inspired many new mandolin players and builders. These players often mix traditional folk elements with bluegrass. Radim Zenkl came from this tradition, emigrating to the United States, where he has played with U.S. stars, including David Grisman and Béla Fleck.

Finland has mandolin players rooted in the folk music scene. Prominent names include Petri Hakala, Seppo Sillanpää and Heikki Lahti, who have taught and recorded albums. Classical music is also represented with players such as Risto Aalto. A Finnish immigrant to the United States Arthur Kylander became a recording contract there in 1927, releasing 20 recordings with Victor Records, playing mandolin.

Prior to the "Golden Age of Mandolins", France had a history with the mandolin, with mandolinists playing in Paris until the Napoleonic Wars. The players, teachers and composers included Giovanni Fouchetti, Eduardo Mezzacapo, Gabriele Leon, and Gervasio. During the Golden age itself (1880s-1920s), the mandolin had a strong presence in France. Prominent mandolin players or composers included Jules Cottin and his sister Madeleine Cottin, Jean Pietrapertosa, and Edgar Bara. Paris had dozens of "estudiantina" mandolin orchestras in the early 1900s. Mandolin magazines included "L'Estudiantina", "Le Plectre", "École de la mandolie".

Today, French mandolinists include Patrick Vaillant, a prominent modern player, composer and recording artist for the mandolin, who also organises courses for aspiring players.

The mandore was known in Germany, prior to the invention of the Neapolitan mandolin. The mandolin spread to Germany with the visits of Italian mandolin virtuosi, including Achille Coronati. The mandolin gained popularity as a folk instrument, especially with the groups of youth participating in the German youth movement, which began in 1897. In the early 20th century the mandolin was popular in the Wandervogel movement (groups of young men and women, hiking and camping, singing and playing instruments), due to the instrument's small size.

The mandolin became increasingly popular in contemporary music during the 20th century. An important German composer for mandolin and mandolin orchestra of the 20th century was Konrad Wölki; He was instrumental in bringing musicological recognition of the mandolin and the orchestra. Mandolin orchestras are still playing in Germany, in various chamber music ensembles and as a solo instrument. The instrument is popular enough today that there is an increasing number of professional mandolin players and composers writing new works for the mandolin.

At the college level the mandolin has a presence, in the professorial chair for mandolins, chaired by Caterina Lichtenberg. She successed Marga Wilden-Hüsgen at the Hochschule für Musik und Tanz Köln, in Wuppertal. Another program with specialized training for students offering a diploma in this instrument takes place by Gertrud Weyhofen at the Music Academy Kassel and more recently at the University of Music Saar and by Steffen Trekel at Hamburg Conservatory.

The instrument was present in the folk revival of the 1970s. One mandolin player was Erich Schmeckenbecher in the duo "Zupfgeigenhansel". Comedian Hans Süper played a modified mandolin, which he called his "Bovec," in the duo "Colonia".

The mandolin has a long tradition in the Ionian islands (the "Heptanese") and Crete. It has long been played in the Aegean islands outside the control of the Ottoman Empire. It is common to see choirs accompanied by mandolin players (the "mandolinátes") in the Ionian islands and especially in the cities of Corfu, Zakynthos, Lefkada and Kefalonia. The evolution of the repertoire for choir and mandolins ("kantádes") occurred during Venetian rule over the islands.

On the island of Crete, along with the lyra and the laouto (lute), the mandolin is one of the main instruments used in Cretan Music. It appeared on Crete around the time of the Venetian rule of the island. Different variants of the mandolin, such as the "mantola," were used to accompany the lyra, the violin, and the laouto. Stelios Foustalierakis reported that the mandolin and the "mpoulgari" were used to accompany the lyra in the beginning of the 20th century in the city of Rethimno. There are also reports that the mandolin was mostly a woman's musical instrument. Nowadays it is played mainly as a solo instrument in personal and family events on the Ionian islands and Crete.

Nikolaos Lavdas was a significant composer and mandolinists for Greece, who was the founder and director of the "Athenian Mandolinata", one of the oldest music associations and music schools in Greece.

Mandolin music was used in Indian Movies as far back as the 1940s by the Raj Kapoor Studios in movies such as Barsaat. The movie Dilwale Dulhania Le Jayenge (1995) used mandolin in several places.

Adoption of the mandolin in Carnatic music is recent and involves an electric instrument. U. Srinivas (1969-2014) has, over the last couple of decades, made his version of the mandolin very popular in India and abroad.

Legendary Bollywood music composer Ustad Sajjad Hussain (June 15, 1917 – July 21, 1995) is often regarded as the 'Wizard of mandolin'. He was the first to effectively use it in Indian Classical Hindustani music. His unique style of playing deserves much more than the reputation he has achieved as a mandolinist.

Many adaptations of the instrument have been done to cater to the special needs of Indian Carnatic music.
In Indian classical music and Indian light music, the mandolin, which bears little resemblance to the European mandolin, is usually tuned E–B–E–B. As there is no concept of absolute pitch in Indian classical music, any convenient tuning maintaining these relative pitch intervals between the strings can be used. Another prevalent tuning with these intervals is C–G–C–G, which corresponds to "sa–pa–sa–pa" in the Indian carnatic classical music style. This tuning corresponds to the way violins are tuned for carnatic classical music. This type of mandolin is also used in Bhangra, dance music popular in Punjabi culture.

The mandolin has become a more common instrument amongst Irish traditional musicians. Fiddle tunes are readily accessible to the mandolin player because of the equivalent tuning and range of the two instruments, and the practically identical (allowing for the lack of frets on the fiddle) left hand fingerings.

Though almost any variety of acoustic mandolin might be adequate for Irish traditional music, virtually all Irish players prefer flat-backed instruments with oval sound holes to the Italian-style bowl-back mandolins or the carved-top mandolins with f-holes favoured by bluegrass mandolinists. The former are often too soft-toned to hold their own in a session (as well as having a tendency to not stay in place on the player's lap), whilst the latter tend to sound harsh and overbearing to the traditional ear. The f-hole mandolin, however, does come into its own in a traditional session, where its brighter tone cuts through the sonic clutter of a pub. Greatly preferred for formal performance and recording are flat-topped "Irish-style" mandolins (reminiscent of the WWI-era Martin Army-Navy mandolin) and carved (arch) top mandolins with oval soundholes, such as the Gibson A-style of the 1920s.

Noteworthy Irish mandolinists include Andy Irvine (who, like Johnny Moynihan, almost always tunes the top E down to D, to achieve an open tuning of G–D–A–D), Paul Brady, Mick Moloney, Paul Kelly and Claudine Langille. John Sheahan and the late Barney McKenna, respectively fiddle player and tenor banjo player with the Dubliners, are also accomplished Irish mandolin players. The instruments used are either flat-backed, oval hole examples as described above (made by UK luthier Roger Bucknall of Fylde Guitars), or carved-top, oval hole instruments with arched back (made by Stefan Sobell in Northumberland). The Irish guitarist Rory Gallagher often played the mandolin on stage, and he most famously used it in the song "Going To My Hometown."

The re-founding of Israel after World War II drew people from all across the world. Among them were people from Eastern European countries, where mandolin orchestras had been in use before the war. Researchers, such as Jeff Warschauer have gone to Israel, seeking to preserve klesmer music and Yiddish songs.

Mandolin orchestras were founded in the 1920s in the pioneering settlement in the Jezreel Valley and in the Jordan Valley by a teacher named Goldman. The orchestra in the Jordan Valley continues to operate today.

The oldest of the mandolin orchestras, which has been continuously active since the 1940s, is the Shfeya Orchestra, founded by Moshe Medalia, and today made up of graduates from the Shfeya Youth Village. The village is a boarding school for at-risk junior and senior high-school students. Part of the education includes students working together in small groups, teaching music to one another. There wasn't much in the way of learning materials when Moshe Jacobson, founder of the music program arrived in 1942. So he wrote text books for the instruments and composed music and songs.

Shimon Peres was an example of someone who brought his musical background. Peres, (born Szymon Perski), grew up in the then Polish town of Vishnyeva, was part of a mandolin orchestra when he was a child. The Vishnyeva orchestra of which he was part of was immortalized in a black-and-white photography, a group of young people dressed up holding their mandolins. Although busy with his political career, Peres remained musical for the rest of his life, composing songs in his home in Israel.

Israel today has four especially prominent mandolinists: Avi Avital, Alon Sariel, Jacob Reuven, and Tom Cohen. Another is composer Shaul Bustan, whose works include a "Concerto for Mandolin and Accordion" (2010) and an arrangement of Mozart's "Symphony No 40 in G minor" for mandolin, mandola, guitar and double bass (2014).

Important performers in the Italitan tradition include Raffaele Calace (luthier, virtuoso and composer of 180 works for many instruments including mandolin), Pietro Denis (whole also composed "Sonata for mandolin & continuo No. 1 in D major" and "Sonata No. 3"), Giovanni Fouchetti, Gabriele Leone, Carlo Munier (1859–1911), Giuseppe Branzoli (1835–1909), Giovanni Gioviale (1885–1949) and Silvio Ranieri (1882–1956).

Antonio Vivaldi composed a mandolin concerto ("Concerto in C major op.3 no.6") and two concertos for two mandolins and orchestra. Wolfgang Amadeus Mozart placed it in his 1787 work "Don Giovanni" and Beethoven created four variations of it. Antonio Maria Bononcini composed "La conquista delle Spagne di Scipione Africano il giovane" in 1707 and George Frideric Handel composed "Alexander Balus" in 1748. Others include Giovani Battista Gervasio ("Sonata in D major for Mandolin and Basso Continuo"), Giuseppe Giuliano ("Sonata in D major for Mandolin and Basso Continuo"), Emanuele Barbella ("Sonata in D major for Mandolin and Basso Continuo"), Domenico Scarlatti ("Sonata no.54 (K.89) in D minor for Mandolin and Basso Continuo"), and Addiego Guerra ("Sonata in G major for Mandolin and Basso Continuo").

More contemporary composers for the mandolin include Giuseppe Anedda (a virtuoso performer and teacher of the first chair of the Conservatory of Italian mandolin), Carlo Aonzo and Dorina Frati.

As the mandolin went out of style, information on the instrument, especially its history became hard to find in Naples, where there instrument was born. The term cultural ghettoization has been used to describe the situation where no information is readily available about the mandolin's history, its luthiers, composers, musicians, and the relationship of the instrument to folk music and the "classic Neapolitan song". Tourists arriving in the city were more likely to find such information elsewhere. The city had moved on from mandolins and didn't place particular emphasis on remembering their history or place in Neapolitan culture.

Furthermore, throughout Italy there was a similar slide. By 1992 there was only one active chair remaining for mandolins at the country's conservatories, the Pollini Conservatory of Padua.

Since then there has been some progress made to revive the mandolin and knowledge of it.

Projects to address the lack of visibility of the mandolin and its history include the formation of a Neapolitan Mandolin Association in 1988, the restarting of the Neapolitan Mandolin Academy in 1992, and the creation of the Mandolin House.

The Neapolitan Mandolin Academy is a school in Naples offering a specialized course of study in the mandolin.

The Mandolin House is a restored house in via Duomo that has room for concerts, and which offers classes to international visitors, as well as serving as an information point for tourists. The crowdfunded project was in part begun by the efforts of Mauro Squillante as he tried to revive the mandolin tradition. He wanted a place in which to give concerts, do training and give exhibitions.

Another sign of progress is the growth of the mandolin within the college education system. While in 1992 there was only one active conservatory chair for mandolins, at Padua (instructor Dorina Frati), today there are also the Music conservatories of Naples, the Conservatorio di Milan (mandolin professor Ugo Orlandi), the Conservatorio di Musica "Niccolò Piccinni" in Bari (instructor Mauro Squillante), the Conservatorio di Musica "Nicola Sala" in Benevento (instructor Nunzio Reina), and conservatories in Palermo, and Salerno as well.

Instruments of the mandolin family are popular in Japan, particularly Neapolitan (round-back) style instruments, and Roman-Embergher style mandolins are still being made there. Japan became seriously interested in mandolins at the beginning of the 20th century during a process of becoming westernized. In 1901, musical educator Kimihachi Hiruma (1867-1936) returned home to Japan after studying the mandolin. He opened a school and founded a mandolin ensemble in 1905.

Where interest in the mandolin declined in the United States and parts of Europe after World War I, in Japan there was a boom, with orchestras being formed all over the country.

Connections to the West, including cultural connections with World War II ally Italy, were forming. One musical connection that encouraged mandolin music growth was a visit by mandolin virtuoso Raffaele Calace, who toured extensively at the end of 1924, into 1925, and who gave a performance for the Japanese emperor. Another visiting mandolin virtuoso, Samuel Adelstein, toured from his home in the United States.

The expansion of mandolin use continued after World War II through the late 1960s, and Japan still maintains a strong classical music tradition using mandolins, with active orchestras and university music programs. New orchestras were founded and new orchestral compositions composed. Japanese mandolin orchestras today may consist of up to 40 or 50 members, and can include woodwind, percussion, and brass sections. Japan also maintains an extensive collection of 20th-century mandolin music from Europe and one of the most complete collections of mandolin magazines from mandolin's golden age, purchased by Morishige Takei.

Morishige Takei (1890–1949), a member of the court of Emperor Hirohito, established a mandolin orchestra in the Italian style before World War II. He was able to maintain his mandolin-guitar orchestra until 1943, in spite of the National Mobilization Law of 1938 that allowed the government to assert control of music, and ban western music and instruments, including the electric guitar, banjo and ukulele. He was also a major composer, with 114 compositions for mandolin.

Another composer, Jiro Nakano (1902–2000), arranged many of the Italian works for regular orchestras or winds composed before World War II as new repertoires for Japanese mandolin orchestras.

Original compositions for mandolin orchestras were composed increasingly after World War II. Seiichi Suzuki (1901–1980) composed music for early Kurosawa films. Others include Tadashi Hattori (1908–2008), and Hiroshi Ohguri (1918–1982). Ohguri was influenced by Béla Bartók and composed many symphonic works for Japanese mandolin orchestras. Yasuo Kuwahara (1946–2003) used German techniques. Many of his works were published in Germany.

In Jewish communities in Eastern Europe and North America the mandolin was important in the years before World War II. The modern Ger Mandolin Orchestra website explained the importance, calling mandolins a "quintessential Jewish musical form... Mandolin clubs and orchestras were at one time ubiquitous in Jewish Eastern Europe and in North American immigrant communities." In the Holocaust Museum in Washington D.C, recorded memories recall the mandolin's place in Eastern Europe, in Poland, Latvia, Lithuania, Croatia and Macedonia. For some, the mandolin was progressive. Individuals, once from Eastern Europe, speak of playing music in different languages, of family members who were able to make the instruments needed for an entire orchestra, of playing or having family members play in mandolin orchestras, of parents who used to teach music.

The instrument was inexpensive and fairly easy for beginners to learn and became a first instrument for children. In the United States, it was embraced by left-wing "Socialists and Unionists" as the "Instrument of the people" and was part of the curriculum in Yiddish schools. There were Jewish mandolin orchestras and the instrument was popular with apartment dwellers as a quiet instrument that wouldn't disturb the neighbors. When Avner Yonai started researching the likely pieces that an Eastern European mandolin orchestra would have played, he found that little could be found about the repertoire his ancestor had played Poland; it was likely to have been light classical pieces, tango, and folk songs, possibly ethnic music from Jewish theater, Yiddish songs, Hasidic music and Klezmer.

Jewish mandolinists were prominent on the American mandolin. The earliest were performing during the Golden Era of the Mandolin, from the 1880s through the 1920s. They include Vaudeville musician Samuel Siegel of Des Moines (parents from Baden, Germany), Charles J. Levin of Baltimore, Samuel Adelstein of San Francisco, C.H. Pomeroy of Salt Lake City, and Valentine Abt of Pittsburgh. 
Another player who helped to keep the mandolin in the public eye after the Golden Era passed was Dave Apollon (1897-1972). Apollon grew up in Kiev and performed in Vaudeville for 20 years, played with Russian, Latin, Gypsy and Ragtime material. His long lifetime playing let people hear virtuoso performances of the mandolin, and he contributed to the instrument's revival.

Modern players play a variety of music, as they explore their roots. Avner Yonai, descendant of people from Gora Kalwaria, Poland, put together an orchestra in Berkeley, California in honor of the original Ger Mandolin Orchestra. Examples of the New Ger Mandolin Orchestra's reperatoire include "Russian Rag" (a mandolin orchestra piece) and the 
"Abe Schwartz Freylekh" (Klezmer adapted for mandolins). Klezmer and mandolins came together in the 1970s in a "revival" in New York City, where "the overwhelmingly Jewish folk music scene would gather for Jam sessions – fiddles, banjos, and mandolins", with Klezmer and Bluegrass musician Andy Statman being credited for the success of the revival. Statman mixed jazz with klezmer.

Jeff Warschauer (a cantor since 2015) started the Klezmer Conservatory Band (1990-2003) and is part of The Strauss/Warschauer Duo (1995 to present) and has led Jewish mandolin orchestras "Klez Camp" and "New York Arbeiter Ring". He has become a modern link to the older Jewish tradition, interviewing older Jewish musicians to collect songs in Yiddish and klesmer music. Coming from a background in Bluegrass, Swing, R&B and Folk music, he adapted his skills to use the mandolin in Klezmer, which doesn't use mandolin as a traditional instrument In one example on his album "The Singing Waltz", Warschauer mixed mandolin with trombones for the song "Dem Helfand's Tants" ("The Elephant's Dance"), a combination that was also used for the "Odessa Swing" (composed by Steve Netsky). He mixed accordion, drums and piano with mandolin on the song "Ot Azoy" ("That's the Way!").

Another bandleader is Eric Stein of Canada; his "Beyond the Pale" band plays Klezmer and Eastern European music.

Although information today is sparse, evidence of the musical culture remains, collected at The United States Holocaust Memorial Museum in Washington D.C., where there are pictures and recorded memories of Eastern European Jewish people, which recall the culture that existed in the first decades of the 20th century. Those that remained, emigrants to other countries took their music and memories with them, and gradually assimilated. Drawing on them for their memories has been part of the revival of the cultures that passed.

The Holocaust Museum has online artifacts dealing with musician Elfa Heifecs, a Jewish musician from Riga, Latvia, who organized mandolin orchestra there, about 1930.

Macedonia has a mandolin tradition that dates back before World War II; The United States Holocaust Museum has a 1929 photo of a Jewish mandolin orchestra on display online. In modern times, the Skopje Mandolin orchestra was formed in 2001 and has performed in international competitions. Mandolinists associated with the Skopje Mandolin Orchestra include Ramadan Shukri, Suzana Turundzieva, Mustafa Imeri, Serafina Fantauzo, Gligor Popovski and Lazar Sandev.

The Auckland Mandolinata mandolin orchestra was formed in 1969 by Doris Flameling (1932–2004). Soon after arriving from the Netherlands with her family, Doris started teaching guitar and mandolin in West Auckland. In 1969, she formed a small ensemble for her pupils. This ensemble eventually developed into a full size mandolin orchestra, which survives today. Doris was the musical director and conductor of this orchestra for many years. The orchestra is currently led by Bryan Holden (conductor).

The early history of the mandolin in New Zealand is currently being researched by members of the Auckland Mandolinata.

The mandolin was used as a folk instrument throughout eastern part of European continent, including Poland, Ukraine and Slovakia in the early part of the 20th century. Mandolin orchestras were present as well. One example was the Mandolin Orchestra of Ger (Gora Kalwaria, Poland). A photo exists of the orchestra, made up of Jewish residents who were in the orchestra in the 1930s before the Jewish population was sent to the Treblinka extermination camp. A similar photo exists of a mandolin orchestra in the community of Grodno.

The "bandolim" (Portuguese for "mandolin") was a favourite instrument within the Portuguese bourgeoisie of the 19th century, but its rapid spread took it to other places, joining other instruments. Today you can see mandolins as part of the traditional and folk culture of Portuguese singing groups and the majority of the mandolin scene in Portugal is in Madeira Island. Madeira has over 17 active mandolin Orchestras and Tunas. The mandolin virtuoso Fabio Machado is one of Portugal's most accomplished mandolin players. The Portuguese influence brought the mandolin to Brazil.

The mandolin was popular in Russia in the pre-Soviet era and after as a folk instrument. Catalogs from the early 1900s offered a variety of bowlback mandolins, imports made by Italian companies. Locally made mandolins existed as well.

The first mandolin orchestra in Russia was put together in the early 1880s by an Italian immigrant, Ginislao Paris, called "The Society of Amateur Mandolinists and Guitarists". Paris designed double-top mandolins, which he had Luigi Embergher build. At least one member of the royal family of Russia played mandolin during this period, Tsarina Maria Fyodorovna who owned a Number 8 Embergher, a high-end instrument that would have been highly decorated.

There were talented mandolinists, in various parts the country. Dave Apollon, who became a well known (U.S.-based) player, was born in Kiev, then part of Russia. After emigration from Russia, he was able to land a job in Vaudeville, exhibiting virtuosity.

After the rise of the Soviet Union, bowlback mandolins were not imported in the numbers they had been. Although possibly that had much to with the turmoil starting in 1917, of the country being reorganized into a communist state, the timing also coincides with the drop in popularity worldwide, as the mandolin's "Golden Era" peaked. The instrument had competition as a folk instrument as well, from the domra and balalaika. Vasily Andreyev who founded the first balalaika orchestra in Russia and resurrected the domra was inspired by a performance of Russia's first mandolin orchestra. The bowlback didn't just disappear, as they are common in photos of mandolin orchestras in Eastern Europe in the 1930s. However, the mandolins produced in Soviet factories were of the cheap, flatback, "Portuguese" style—widespread throughout the Soviet Union.

In the 1970s and 80s, bluegrass music spread into Russia from Czechoslovakia, and through American performers touring in Russia, including Roy Clark and the Nitty Gritty Dirt Band. Russian bands were started with bluegrass elements that included a mandolin, including Kukuruza (with mandolinist Georgi Palmov) and Bering Strait (mandolinist Sergei Passov).

The Mandolin was used in movies, including "The Adventures of Pinocchio" (Приключения Буратино).

Mandolin has been a prominent instrument in the recordings of Johnny Clegg and his bands Juluka and Savuka. Since 1992, Andy Innes has been the mandolinist for Johnny Clegg and Savuka.

The mandolin was brought to Sri Lanka by the Portuguese, who colonized Sri Lanka from 1505 to 1658. The instrument has been heavily used in baila, a genre of Sri Lankan music formed from a mixture of Portuguese, African and Sinhala music. For example, the mandolin features prominently in M.S. Fernando's baila song, "Bola Bola Meti". Victor Rathnayaka's 'Thaniwennata Mage Lowe, Gypsies Sunil Perera's 'Saima Cut wela' and Sujatha Athanayaka's 'Hathara watin Kalukaragena' are among other examples of songs with mandolin. Modern day musicians include Antony Surendra, V. Hemapala Perera, Ananda Perera, Dinesh Subasinghe, Susantha Kariyakarawana, Sarath Fernando, Nalaka Sajee, Haasriza Imran and Buddhika Perera

The mandolin has a history on Tobago and Trinidad as the bandolin, dating back before World War I. It was a small instrument, approximately 20 x 40 centimeters, strung with 8 strings in four courses of two each. Before the war, it was commonly a round-backed instrument, made of strips of wood. The flat-backed version appeared after the war. Occasionally a turtle shell would be used for the back of the instrument. It is used in Trinidad's "parang" music, accompanied by "cuatro and maracas". There is another member of the mandolin family, the Trinidadian version of the bandol, different from the mainland bandola. It is the tenor of the mandolin family on the island, also strung with 8 strings in 4 courses; however, the lower two string courses are strung differently, each having a metal and a nylon string.

Turkey has been the home of a mandolin-banjo manufacturer, Cümbüs, since the early 20th century. The country had what may be its first Mandolin festival in June 2015, and has at least one Mandolin Orchestra.

One professional musician to use the mandolin is Sumru Ağıryürüyen who is known for singing and playing many styles of music including world music, Klezmer, Turkish folk, Balkan folk, blues, jazz, krautrock, protest rock and maqam.

The Mandolin has been played in Ukraine, and pictures of it being played today can be found online. One famous former resident of Kiev was Dave Apollon. Other photos dating back to the early 20th century show Ukrainian Mandolin orchestras in Canada and the United States.
One of the offshoots of Ukrainian immigrants to North America is the Toronto Mandolin Orchestra (possibly the oldest mandolin orchestra in North America), which claims Ukrainians as among its founders. The orchestra's web site said of mandolins in Ukraine, that the instruments were popular in the early 20th century, but never reached folk-instrument status there. Ukrainian immigrants of the period took the instruments with them to their new countries.

The instrument has had to compete in Ukraine with native instruments that have been revived, such as the kobza. The orchestral variant of the kobza is similar to the Mandolin, having four strings and being tuned in fifths.

The mandolin has been used extensively in the traditional music of England and Scotland for generations. Simon Mayor is a prominent British player who has produced six solo albums, instructional books and DVDs, as well as recordings with his mandolin quartet the Mandolinquents. The instrument has also found its way into British rock music. The mandolin was played by Mike Oldfield (and introduced by Vivian Stanshall) on Oldfield's album "Tubular Bells", as well as on a number of his subsequent albums (particularly prominently on "Hergest Ridge" (1974) and "Ommadawn" (1975)). It was used extensively by the British folk-rock band Lindisfarne, who featured two members on the instrument, Ray Jackson and Simon Cowe, and whose "Fog on the Tyne" was the biggest selling UK album of 1971–72. The instrument was also used extensively in the UK folk revival of the 1960s and 1970s with bands such as Fairport Convention and Steeleye Span taking it on as the lead instrument in many of their songs. "Maggie May" by Rod Stewart, which hit No. 1 on both the British charts and the Billboard Hot 100, also featured Jackson's playing. It has also been used by other British rock musicians. Led Zeppelin's bassist John Paul Jones is an accomplished mandolin player and has recorded numerous songs on mandolin including "Going to California" and "That's the Way"; the mandolin part on "The Battle of Evermore" is played by Jimmy Page, who composed the song. Other Led Zeppelin songs featuring mandolin are "Hey Hey What Can I Do", and "Black Country Woman." Pete Townshend of the Who played mandolin on the track "Mike Post Theme", along with many other tracks on Endless Wire. McGuinness Flint, for whom Graham Lyle played the mandolin on their most successful single, "When I'm Dead And Gone", is another example. Lyle was also briefly a member of Ronnie Lane's Slim Chance, and played mandolin on their hit "How Come." One of the more prominent early mandolin players in popular music was Robin Williamson in the Incredible String Band. Ian Anderson of Jethro Tull is a highly accomplished mandolin player (beautiful track "Pussy Willow"), as is his guitarist Martin Barre. The popular song "Please Please Please Let Me Get What I Want" by the Smiths featured a mandolin solo played by Johnny Marr. More recently, the Glasgow-based band Sons and Daughters featured the mandolin, played by Ailidh Lennon, on tracks such as "Fight," "Start to End," and "Medicine." British folk-punk icons the Levellers also regularly use the mandolin in their songs. Current bands are also beginning to use the Mandolin and its unique sound - such as South London's Indigo Moss who use it throughout their recordings and live gigs. The mandolin has also featured in the playing of Matthew Bellamy in the rock band Muse. It also forms the basis of Paul McCartney's 2007 hit "Dance Tonight." That was not the first time a Beatle played a mandolin, however; that distinction goes to George Harrison on Gone Troppo, the title cut from the 1982 album of the same name. The mandolin is taught in Lanarkshire by the Lanarkshire Guitar and Mandolin Association to over 100 people. Also more recently hard rock supergroup Them Crooked Vultures have been playing a song, "Highway One" in which Jones "morphs an effected mandolin bluegrass run into a slick rock riff." This song was left off their debut album, and features former Led Zeppelin bassist John Paul Jones.

In the Classical style, performers such as Hugo D'Alton, Alison Stephens and Michael Hooper have continued to play music by British composers such as Michael Finnissy, James Humberstone and Elspeth Brooke.

See also: mandolin orchestras
The mandolin's popularity in the United States was spurred by the success of a group of touring young European musicians known as the Estudiantina Figaro, or in the United States, simply the "Spanish Students." The group landed in the U.S. on January 2, 1880 in New York City, and played in Boston and New York to wildly enthusiastic crowds. Ironically, this ensemble did not play mandolins but rather bandurrias, which are also small, double-strung instruments that resemble the mandolin.

The success of the Figaro Spanish Students spawned other groups who imitated their musical style and costumes. An Italian musician, Carlo Curti, hastily started a musical ensemble after seeing the Figaro Spanish Students perform; his group of Italian born Americans called themselves the "Original Spanish Students," counting on the American public to not know the difference between the Spanish bandurrias and Italian mandolins. The imitators' use of mandolins helped to generate enormous public interest in an instrument previously relatively unknown in the United States.

Another imitation group was Zerega's Spanish Troubadours, a quintet of three mandolins and two guitars. They weren't Spanish, and Zerega was the stage name of Indiana-born Edgar E. Hill who played with his wife May, both Americans, who had eloped to London, but toured America in 1887.

Mandolin awareness in the United States blossomed in the 1880s, as the instrument became part of a fad that continued into the mid-1920s. According to Clarence L. Partee a publisher in the BMG movement (banjo, mandolin and guitar), the first mandolin made in the United States was made in 1883 or 1884 by Joseph Bohmann, who was an established maker of violins in Chicago. Partee characterized the early instrument as being larger than the European instruments he was used to, with a "peculiar shape" and "crude construction," and said that the quality improved, until American instruments were "superior" to imported instruments. At the time, Partee was using an imported French-made mandolin.

Instruments were marketed by teacher-dealers, much as the title character in the popular musical "The Music Man". Often, these teacher-dealers conducted mandolin orchestras: groups of four to fifty musicians who played various mandolin family instruments. However, alongside the teacher-dealers were serious musicians, working to create a spot for the instrument in classical music, ragtime and jazz. Like the teacher-dealers, they traveled the U.S., recording records, giving performances and teaching individuals and mandolin orchestras. Samuel Siegel played mandolin in Vaudeville and became one of America's preeminent mandolinists. Seth Weeks was an African American who not only taught and performed in the United States, but also in Europe, where he recorded records. Another pioneering African American musician and director who made his start with a mandolin orchestra was composer James Reese Europe. W. Eugene Page toured the country with a group, and was well known for his mandolin and mandola performances. Other names include Valentine Abt, Bob Yosco, Samuel Adelstein, William Place, Jr., Bernardo De Pace, and Aubrey Stauffer.

The instrument was primarily used in an ensemble setting well into the 1930s, and although the fad died out at the beginning of the 1930s, the instruments that were developed for the orchestra found a new home in bluegrass. The famous Lloyd Loar Master Model from Gibson (1923) was designed to boost the flagging interest in mandolin ensembles, with little success. However, the "Loar" became the defining instrument of bluegrass music when Bill Monroe purchased F-5 S/N 73987 in a Florida barbershop in 1943 and popularized it as his main instrument.

The mandolin orchestras never completely went away, however. In fact, along with all the other musical forms the mandolin is involved with, the mandolin ensemble (groups usually arranged like the string section of a modern symphony orchestra, with first mandolins, second mandolins, mandolas, mandocellos, mando-basses, and guitars, and sometimes supplemented by other instruments) continues to grow in popularity. Since the mid-nineties, several public-school mandolin-based guitar programs have blossomed around the country, including Fretworks Mandolin and Guitar Orchestra, the first of its kind. The national organization, Classical Mandolin Society of America, founded by Norman Levine, represents these groups. Prominent modern mandolinists and composers for mandolin in the classical music tradition include Samuel Firstman, Howard Fry, Rudy Cipolla, Dave Apollon, Neil Gladd, Evan Marshall, Marilynn Mair and Mark Davis (the Mair-Davis Duo), Brian Israel, David Evans, Emanuil Shynkman, Radim Zenkl, David Del Tredici and Ernst Krenek.

When Cowan Powers and his family recorded their old-time music from 1924 to 1926, his daughter Orpha Powers was one of the earliest known southern-music artists to record with the mandolin. By the 1930s, single mandolins were becoming more commonly used in southern string band music, most notably by brother duets such as the sedate Blue Sky Boys (Bill Bolick and Earl Bolick), the Armstrong Twins (Lloyd and Floyd Armstrong) and the more hard-driving Monroe Brothers (Bill Monroe and Charlie Monroe). However, the mandolin's modern popularity in country music can be directly traced to one man: Bill Monroe, the father of bluegrass music. After the Monroe Brothers broke up in 1939, Bill Monroe formed his own group, after a brief time called the Blue Grass Boys, and completed the transition of mandolin styles from a "parlor" sound typical of brother duets to the modern "bluegrass" style. He joined the Grand Ole Opry in 1939 and its powerful clear-channel broadcast signal on WSM-AM spread his style throughout the South, directly inspiring many musicians to take up the mandolin. Monroe famously played Gibson F-5 mandolin, signed and dated July 9, 1923, by Lloyd Loar, chief acoustic engineer at Gibson. The F-5 has since become the most imitated tonally and aesthetically by modern builders.

Monroe's style involved playing lead melodies in the style of a fiddler, and also a percussive chording sound referred to as "the chop" for the sound made by the quickly struck and muted strings. He also perfected a sparse, percussive blues style, especially up the neck in keys that had not been used much in country music, notably B and E. He emphasized a powerful, syncopated right hand at the expense of left-hand virtuosity. Monroe's most influential follower of the second generation is Frank Wakefield and nowadays Mike Compton of the Nashville Bluegrass Band and David Long, who often tour as a duet. Tiny Moore of the Texas Playboys developed an electric five-string mandolin and helped popularize the instrument in Western Swing music.

Other major bluegrass mandolinists who emerged in the early 1950s and are still active include Jesse McReynolds (of Jim and Jesse) who invented a syncopated banjo-roll-like style called crosspicking—and Bobby Osborne of the Osborne Brothers, who is a master of clarity and sparkling single-note runs. Highly respected and influential modern bluegrass players include Herschel Sizemore, Doyle Lawson, and the multi-genre Sam Bush, who is equally at home with old-time fiddle tunes, rock, reggae, and jazz. Ronnie McCoury of the Del McCoury Band has won numerous awards for his Monroe-influenced playing. John Duffey of the original Country Gentlemen and later the Seldom Scene did much to popularize the bluegrass mandolin among folk and urban audiences, especially on the east coast and in the Washington, D.C. area.

Jethro Burns, best known as half of the comedy duo Homer and Jethro, was also the first important jazz mandolinist. Tiny Moore popularized the mandolin in Western swing music. He initially played an 8-string Gibson but switched after 1952 to a 5-string solidbody electric instrument built by Paul Bigsby. Modern players David Grisman, Sam Bush, and Mike Marshall, among others, have worked since the early 1970s to demonstrate the mandolin's versatility for all styles of music. Chris Thile of California is a well-known player, and has accomplished many feats of traditional bluegrass, classical, contemporary pop and rock; the band Nickel Creek featured his playing in its blend of traditional and pop styles, and he now plays in his band Punch Brothers. Most commonly associated with bluegrass, mandolin has been used a lot in country music over the years. Some well-known players include Marty Stuart, Vince Gill, and Ricky Skaggs.

It saw some use in jug band music, since that craze began as the mandolin fad was waning, and there were plenty of instruments available at relatively low cost.

Mandolin has also been used in blues music, most notably by Ry Cooder, who performed outstanding covers on his very first recordings, Yank Rachell, Johnny "Man" Young, Carl Martin, and Gerry Hundt. Howard Armstrong, who is famous for blues violin, got his start with his father's mandolin and played in string bands similar to the other Tennessee string bands he came into contact with, with band makeup including "mandolins and fiddles and guitars and banjos. And once in a while they would ease a little ukulele in there and a bass fiddle." Other blues players from the era's string bands include Willie Black (Whistler And His Jug Band), Joe Evans (The Two Poor Boys), Dink Brister, Jim Hill, Charles Johnson, Coley Jones (Dallas String Band), Bobby Leecan (Need More Band), Alfred Martin, Charlie McCoy (1909–1950), Al Miller, Matthew Prater, and Herb Quinn.

The mandolin has been used occasionally in rock music, first appearing in the psychedelic era of the late 1960s. Levon Helm of the Band occasionally moved from his drum kit to play mandolin, most notably on "Rag Mama Rag", "Rockin' Chair", and "Evangeline". Ian Anderson of Jethro Tull played mandolin on "Fat Man", from their second album, "Stand Up", and also occasionally on later releases. Rod Stewarts 1971 No. 1 hit "Maggie May" features a significant mandolin riff. David Grisman played mandolin on two Grateful Dead songs on the "American Beauty" album, "Friend of the Devil" and "Ripple", which became instant favorites among amateur pickers at jam sessions and campground gatherings. John Paul Jones and Jimmy Page both played mandolin on Led Zeppelin songs. Dash Crofts of the soft rock duo Seals and Crofts extensively used mandolin in their repertoire during the 1970s. Styx released the song "Boat on the River" in 1980, which featured Tommy Shaw on vocals and mandolin. The song didn't chart in the United States but was popular in much of Europe and the Philippines.

Some rock musicians today use mandolins, often single-stringed electric models rather than double-stringed acoustic mandolins. One example is Tim Brennan of the Irish-American punk rock band Dropkick Murphys. In addition to electric guitar, bass, and drums, the band uses several instruments associated with traditional Celtic music, including mandolin, tin whistle, and Great Highland bagpipes. The band explains that these instruments accentuate the growling sound they favor. The 1991 R.E.M. hit "Losing My Religion" was driven by a few simple mandolin licks played by guitarist Peter Buck, who also played the mandolin in nearly a dozen other songs. The single peaked at No. 4 on the Billboard Hot 100 chart (#1 on the rock and alternative charts). The track "I Will Dare", by alternative rock band The Replacements, which features a Peter Buck guitar solo, also features songwriter Paul Westerberg on mandolin. Luther Dickinson of North Mississippi Allstars and the Black Crowes has made frequent use of the mandolin, most notably on the Black Crowes song "Locust Street." Armenian American rock group System of A Down makes extensive use of the mandolin on their 2005 double album Mezmerize/Hypnotize. Pop punk band Green Day has used a mandolin in several occasions, especially on their 2000 album, "Warning". Boyd Tinsley, violin player of the Dave Matthews Band has been using an electric mandolin since 2005. Frontman Colin Meloy and guitarist Chris Funk of the Decemberists regularly employ the mandolin in the band's music. Nancy Wilson, rhythm guitarist of Heart, uses a mandolin in Heart's song "Dream of the Archer" from the album "Little Queen", as well as in Heart's cover of Led Zeppelin's song "The Battle of Evermore." "Show Me Heaven" by Maria McKee, the theme song to the film "Days of Thunder", prominently features a mandolin. The popular alt rock group Imagine Dragons feature the mandolin on a few of their songs, most prominently being "It's Time". Folk rock band the Lumineers use a mandolin in the background of their 2012 hit "Ho Hey".

Many folk punk bands also feature the mandolin. One such band is Days N' Daze, who make use of the mandolin, banjo, ukulele, as well as several other acoustic plucked string instruments. Other folk punk acts include Blackbird Raum, and Johnny Hobo and the Freight Trains.

As in Brazil, the mandolin has played an important role in the Music of Venezuela. It has enjoyed a privileged position as the main melodic instrument in several different regions of the country. Specifically, the eastern states of Sucre, Nueva Esparta, Anzoategui and Monagas have made the mandolin the main instrument in their versions of Joropo as well as Puntos, Jotas, Polos, Fulias, Merengues and Malagueñas. Also, in the west of the country the sound of the mandolin is intrinsically associated with the regional genres of the Venezuelan Andes: Bambucos, Pasillos, Pasodobles, and Waltzes. In the western city of Maracaibo the Mandolin has been played in Decimas, Danzas and Contradanzas Zulianas; in the capital, Caracas, the Merengue Rucaneao, Pasodobles and Waltzes have also been played with mandolin for almost a century. Today, Venezuelan mandolists include an important group of virtuoso players and ensembles such as Alberto Valderrama, Jesus Rengel, Ricardo Sandoval, Saul Vera, and Cristobal Soto.

The French ruled Vietnam completely by 1884 and set up a system of modern education. The population was exposed to French culture and music, which included the mandolin. The influence of French culture was strong enough to affect Vietnamese music. They began writing lyrics to French pop music in Vietnamese and teaching themselves other western instruments including mandolin, guitar, Hawaiian guitar and banjo. Through the Catholics and later the conservatories, European classical music and instruments were taught, including piano and bowed instruments.

The mandolin and guitar were played in both classical music and pop music. Although used in popular music, both instruments were also taught in the Saigon National Conservatory (now the Conservatory of Ho Chi Minh City). However, the mandolin has disappeared from the conservatory where it had been taught, apparently dropped from the curriculum. Instead of Guitar-Mandolin department, the school lists Guitar-Accordion.

Though probably never a dominant instrument, the mandolin was learned by enough people to have a presence among the settlers who left Vietnam for the United States, and who continued to play mandolins in their new home. There are still people in Vietnam playing as well, although the cost of a new instrument is prohibitive. There is at least one mandolin orchestra still playing, a group of aging players teaching newcomers.

Vietnamese luthiers have worked with mandolin design. Mandolins being made in Vietnam today for the international market use the French flatback style. However, some Vietnamese luthiers have added their own innovation, putting sound holes in the mandolins' sides.

The luthiers also worked with the idea of modifying the mandolin to meet local musical styles, making experimental changes to the necks of violins, guitars and mandolins to suit them to Cai luong opera music. In the mid-1930s, they made concave frets, scooping out extra wood between the frets, making the space between them deep and to allow the musicians to bend the notes, and stringing them with four single strings. Musicians Hai Nén and Hai Nhành were associated with the instruments, possibly being the first to use them. The "sunken-fret mandolin" (mandolin phím lõm) did not meet the musical needs as well as the sunken-fret guitar, because the mandolin's rigidity made it painful to get the same effects from the strings. Also the mandolin's narrow fretboard made it difficult to hit the notes. Another reason to move to a bigger instrument was the pitch range of the mandolin, high pitched and not as useful to accompany singing as the instruments that replaced it in the opera.

The tradition of so-called "classical music" for the mandolin has been somewhat spotty, due to its being widely perceived as a "folk" instrument. Significant composers did write music specifically for the mandolin, but few "large" works were composed for it by the most widely regarded composers. The total number of works these works is rather small in comparison to—say—those composed for violin. One result of this dearth being that there were few positions for mandolinists in regular orchestras. To fill this gap in the literature, mandolin orchestras have traditionally played many arrangements of music written for regular orchestras or other ensembles. Some players have sought out contemporary composers to solicit new works.

Furthermore, of the works that have been written for mandolin from the 18th century onward, many have been lost or forgotten. Some of these await discovery in museums and libraries and archives. One example of rediscovered 18th-century music for mandolin and ensembles with mandolins is the "Gimo collection", collected in the first half of 1762 by Jean Lefebure. Lefebure collected the music in Italy, and it was forgotten until manuscripts were rediscovered.

Vivaldi created some concertos for mandolinos and orchestra: one for 4-chord mandolino, string bass & continuous in C major, (RV 425), and one for two 5-chord mandolinos, bass strings & continuous in G major, (RV 532), and concerto for two mandolins, 2 violons "in Tromba"—2 flûtes à bec, 2 salmoe, 2 théorbes, violoncelle, cordes et basse continuein in C major (P. 16).

Beethoven composed mandolin music and enjoyed playing the mandolin. His 4 small pieces date from 1796: Sonatine WoO 43a; Adagio ma non troppo WoO 43b; Sonatine WoO 44a and Andante con Variazioni WoO 44b.

The opera "Don Giovanni" by Mozart (1787) includes mandolin parts, including the accompaniment to the famous aria "Deh vieni alla finestra", and Verdi's opera Otello calls for guzla accompaniment in the aria "Dove guardi splendono raggi", but the part is commonly performed on mandolin.

Gustav Mahler used the mandolin in his Symphony No. 7, Symphony No. 8 and Das Lied von der Erde.

Parts for mandolin are included in works by Schoenberg (Variations op. 31), Stravinsky (Agon), Prokofiev (Romeo and Juliet) and Webern (opus Parts 10)

Some 20th century composers also used the mandolin as their instrument of choice (amongst these are: Schoenberg, Webern, Stravinsky and Prokofiev).

Among the most important European mandolin composers of the 20th century are Raffaele Calace (composer, performer and luthier) and Giuseppe Anedda (virtuoso concert pianist and professor of the first chair of the Conservatory of Italian Mandolin, Padua, 1975). Today representatives of Italian classical music and Italian classical-contemporary music include Ugo Orlandi, Carlo Aonzo, Dorina Frati, Mauro Squillante and Duilio Galfetti.

Japanese composers also produced orchestral music for mandolin in the 20th century, but these are not well known outside Japan.

Traditional mandolin orchestras remain especially popular in Japan and Germany, but also exist throughout the United States, Europe and the rest of the world. They perform works composed for mandolin family instruments, or re-orchestrations of traditional pieces. The structure of a contemporary traditional mandolin orchestra consists of: first and second mandolins, mandolas (either octave mandolas, tuned an octave below the mandolin, or tenor mandolas, tuned like the viola), mandocellos (tuned like the cello), and bass instruments (conventional string bass or, rarely, mandobasses). Smaller ensembles, such as quartets composed of two mandolins, mandola, and mandocello, may also be found.

















































































Chord dictionaries

Method and instructional guides



</doc>
<doc id="18889" url="https://en.wikipedia.org/wiki?curid=18889" title="Microphotonics">
Microphotonics

Microphotonics is a branch of technology that deals with directing light on a microscopic scale. It is used in optical networking.

Microphotonics employs at least two different materials with a large differential index of refraction to squeeze the light down to a small size. Generally speaking, virtually all of microphotonics relies on Fresnel reflection to guide the light. If the photons reside mainly in the higher index material, the confinement is due to total internal reflection. If the confinement is due many distributed Fresnel reflections, the device is termed a photonic crystal. There are many different types of geometries used in microphotonics including optical waveguides, optical microcavities, and Arrayed waveguide gratings.

Photonic crystals are non-conducting materials that reflect various wavelengths of light almost perfectly. Such a crystal can be referred to as a perfect mirror. Other devices employed in microphotonics include micromirrors and photonic wire waveguides. These tools are used to "mold the flow of light", a famous phrase for describing the goal of microphotonics.

Currently, microphotonics technology is being developed to replace electronics devices. For instance, the long-standing goal of an all-optical router would eliminate electronic bottlenecks, speeding up the network. Perfect mirrors are being developed for use in fiber optic cables.

An optical microdisk, optical microtoroid, or optical microsphere uses internal reflection in a circular geometry to hold on to the photons. This type of circularly symmetric optical resonance is called a Whispering gallery mode, after Lord Rayleigh coined the term.



</doc>
<doc id="18890" url="https://en.wikipedia.org/wiki?curid=18890" title="Microsoft Windows">
Microsoft Windows

Microsoft Windows is a group of several graphical operating system families, all of which are developed, marketed, and sold by Microsoft. Each family caters to a certain sector of the computing industry. Active Windows families include Windows NT and Windows Embedded; these may encompass subfamilies, e.g. Windows Embedded Compact (Windows CE) or Windows Server. Defunct Windows families include Windows 9x, Windows Mobile and Windows Phone.

Microsoft introduced an operating environment named "Windows" on November 20, 1985, as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs). Microsoft Windows came to dominate the world's personal computer (PC) market with over 90% market share, overtaking Mac OS, which had been introduced in 1984. Apple came to see Windows as an unfair encroachment on their innovation in GUI development as implemented on products such as the Lisa and Macintosh (eventually settled in court in Microsoft's favor in 1993). On PCs, Windows is still the most popular operating system. However, in 2014, Microsoft admitted losing the majority of the overall operating system market to Android, because of the massive growth in sales of Android smartphones. In 2014, the number of Windows devices sold was less than 25% that of Android devices sold. This comparison however may not be fully relevant, as the two operating systems traditionally target different platforms. Still, numbers for server use of Windows (that are comparable to competitors) show one third market share, similar to for end user use. 

, the most recent version of Windows for PCs, tablets, smartphones and embedded devices is Windows 10. The most recent versions for server computers is Windows Server 2016. A specialized version of Windows runs on the Xbox One video game console.

Microsoft, the developer of Windows, has registered several trademarks each of which denote a family of Windows operating systems that target a specific sector of the computing industry. As of 2014, the following Windows families are being actively developed:


The following Windows families are no longer being developed:


The term "Windows" collectively describes any or all of several generations of Microsoft operating system products. These products are generally categorized as follows:

The history of Windows dates back to 1981, when Microsoft started work on a program called "Interface Manager". It was announced in November 1983 (after the Apple Lisa, but before the Macintosh) under the name "Windows", but Windows 1.0 was not released until November 1985. Windows 1.0 was to compete with Apple's operating system, but achieved little popularity. Windows 1.0 is not a complete operating system; rather, it extends MS-DOS. The shell of Windows 1.0 is a program known as the MS-DOS Executive. Components included Calculator, Calendar, Cardfile, Clipboard viewer, Clock, Control Panel, Notepad, Paint, Reversi, Terminal and Write. Windows 1.0 does not allow overlapping windows. Instead all windows are tiled. Only modal dialog boxes may appear over other windows.

Windows 2.0 was released in December 1987, and was more popular than its predecessor. It features several improvements to the user interface and memory management. Windows 2.03 changed the OS from tiled windows to overlapping windows. The result of this change led to Apple Computer filing a suit against Microsoft alleging infringement on Apple's copyrights. Windows 2.0 also introduced more sophisticated keyboard shortcuts and could make use of expanded memory.

Windows 2.1 was released in two different versions: Windows/286 and Windows/386. Windows/386 uses the virtual 8086 mode of the Intel 80386 to multitask several DOS programs and the paged memory model to emulate expanded memory using available extended memory. Windows/286, in spite of its name, runs on both Intel 8086 and Intel 80286 processors. It runs in real mode but can make use of the high memory area. 

In addition to full Windows-packages, there were runtime-only versions that shipped with early Windows software from third parties and made it possible to run their Windows software on MS-DOS and without the full Windows feature set.

The early versions of Windows are often thought of as graphical shells, mostly because they ran on top of MS-DOS and use it for file system services. However, even the earliest Windows versions already assumed many typical operating system functions; notably, having their own executable file format and providing their own device drivers (timer, graphics, printer, mouse, keyboard and sound). Unlike MS-DOS, Windows allowed users to execute multiple graphical applications at the same time, through cooperative multitasking. Windows implemented an elaborate, segment-based, software virtual memory scheme, which allows it to run applications larger than available memory: code segments and resources are swapped in and thrown away when memory became scarce; data segments moved in memory when a given application had relinquished processor control.

Windows 3.0, released in 1990, improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between multi-tasked DOS applications. Windows 3.0 applications can run in protected mode, which gives them access to several megabytes of memory without the obligation to participate in the software virtual memory scheme. They run inside the same address space, where the segmented memory provides a degree of protection. Windows 3.0 also featured improvements to the user interface. Microsoft rewrote critical operations from C into assembly. Windows 3.0 is the first Microsoft Windows version to achieve broad commercial success, selling 2 million copies in the first six months.

Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, Windows for Workgroups, a special version with integrated peer-to-peer networking features and a version number of 3.11, was released. It was sold along Windows 3.1. Support for Windows 3.1 ended on December 31, 2001.

Windows 3.2, released 1994, is an updated version of the Chinese version of Windows 3.1. The update was limited to this language version, as it fixed only issues related to the complex writing system of the Chinese language. Windows 3.2 was generally sold by computer manufacturers with a ten-disk version of MS-DOS that also had Simplified Chinese characters in basic output and some translated utilities.

The next major consumer-oriented release of Windows, Windows 95, was released on August 24, 1995. While still remaining MS-DOS-based, Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft; Ina Fried of CNET remarked that "by the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world." Microsoft published four OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a service pack. The first OSR of Windows 95 was also the first version of Windows to be bundled with Microsoft's web browser, Internet Explorer. Mainstream support for Windows 95 ended on December 31, 2000, and extended support for Windows 95 ended on December 31, 2001.

Windows 95 was followed up with the release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configurations. Windows 98 also included integration with Internet Explorer 4 through Active Desktop and other aspects of the Windows Desktop Update (a series of enhancements to the Explorer shell which were also made available for Windows 95). In May 1999, Microsoft released Windows 98 Second Edition, an updated version of Windows 98. Windows 98 SE added Internet Explorer 5.0 and Windows Media Player 6.2 amongst other upgrades. Mainstream support for Windows 98 ended on June 30, 2002, and extended support for Windows 98 ended on July 11, 2006.

On September 14, 2000, Microsoft released Windows ME (Millennium Edition), the last DOS-based version of Windows. Windows ME incorporated visual interface enhancements from its Windows NT-based counterpart Windows 2000, had faster boot times than previous versions (which however, required the removal of the ability to access a real mode DOS environment, removing compatibility with some older programs), expanded multimedia functionality (including Windows Media Player 7, Windows Movie Maker, and the Windows Image Acquisition framework for retrieving images from scanners and digital cameras), additional system utilities such as System File Protection and System Restore, and updated home networking tools. However, Windows ME was faced with criticism for its speed and instability, along with hardware compatibility issues and its removal of real mode DOS support. "PC World" considered Windows ME to be one of the worst operating systems Microsoft had ever released, and the 4th worst tech product of all time.

In November 1988, a new development team within Microsoft (which included former Digital Equipment Corporation developers Dave Cutler and Mark Lucovsky) began work on a revamped version of IBM and Microsoft's OS/2 operating system known as "NT OS/2". NT OS/2 was intended to be a secure, multi-user operating system with POSIX compatibility and a modular, portable kernel with preemptive multitasking and support for multiple processor architectures. However, following the successful release of Windows 3.0, the NT development team decided to rework the project to use an extended 32-bit port of the Windows API known as Win32 instead of those of OS/2. Win32 maintained a similar structure to the Windows APIs (allowing existing Windows applications to easily be ported to the platform), but also supported the capabilities of the existing NT kernel. Following its approval by Microsoft's staff, development continued on what was now Windows NT, the first 32-bit version of Windows. However, IBM objected to the changes, and ultimately continued OS/2 development on its own.

The first release of the resulting operating system, Windows NT 3.1 (named to associate it with Windows 3.1) was released in July 1993, with versions for desktop workstations and servers. Windows NT 3.5 was released in September 1994, focusing on performance improvements and support for Novell's NetWare, and was followed up by Windows NT 3.51 in May 1995, which included additional improvements and support for the PowerPC architecture. Windows NT 4.0 was released in June 1996, introducing the redesigned interface of Windows 95 to the NT series. On February 17, 2000, Microsoft released Windows 2000, a successor to NT 4.0. The Windows NT name was dropped at this point in order to put a greater focus on the Windows brand.

The next major version of Windows NT, Windows XP, was released on October 25, 2001. The introduction of Windows XP aimed to unify the consumer-oriented Windows 9x series with the architecture introduced by Windows NT, a change which Microsoft promised would provide better performance over its DOS-based predecessors. Windows XP would also introduce a redesigned user interface (including an updated Start menu and a "task-oriented" Windows Explorer), streamlined multimedia and networking features, Internet Explorer 6, integration with Microsoft's .NET Passport services, modes to help provide compatibility with software designed for previous versions of Windows, and Remote Assistance functionality.

At retail, Windows XP was now marketed in two main editions: the "Home" edition was targeted towards consumers, while the "Professional" edition was targeted towards business environments and power users, and included additional security and networking features. Home and Professional were later accompanied by the "Media Center" edition (designed for home theater PCs, with an emphasis on support for DVD playback, TV tuner cards, DVR functionality, and remote controls), and the "Tablet PC" edition (designed for mobile devices meeting its specifications for a tablet computer, with support for stylus pen input and additional pen-enabled applications). Mainstream support for Windows XP ended on April 14, 2009. Extended support ended on April 8, 2014.

After Windows 2000, Microsoft also changed its release schedules for server operating systems; the server counterpart of Windows XP, Windows Server 2003, was released in April 2003. It was followed in December 2005, by Windows Server 2003 R2.

After a lengthy development process, Windows Vista was released on November 30, 2006, for volume licensing and January 30, 2007, for consumers. It contained a number of new features, from a redesigned shell and user interface to significant technical changes, with a particular focus on security features. It was available in a number of different editions, and has been subject to some criticism, such as drop of performance, longer boot time, criticism of new UAC, and stricter license agreement. Vista's server counterpart, Windows Server 2008 was released in early 2008.

On July 22, 2009, Windows 7 and Windows Server 2008 R2 were released as RTM (release to manufacturing) while the former was released to the public 3 months later on October 22, 2009. Unlike its predecessor, Windows Vista, which introduced a large number of new features, Windows 7 was intended to be a more focused, incremental upgrade to the Windows line, with the goal of being compatible with applications and hardware with which Windows Vista was already compatible. Windows 7 has multi-touch support, a redesigned Windows shell with an updated taskbar, a home networking system called HomeGroup, and performance improvements.

Windows 8, the successor to Windows 7, was released generally on October 26, 2012. A number of significant changes were made on Windows 8, including the introduction of a user interface based around Microsoft's Metro design language with optimizations for touch-based devices such as tablets and all-in-one PCs. These changes include the Start screen, which uses large tiles that are more convenient for touch interactions and allow for the display of continually updated information, and a new class of apps which are designed primarily for use on touch-based devices. Other changes include increased integration with cloud services and other online platforms (such as social networks and Microsoft's own OneDrive (formerly SkyDrive) and Xbox Live services), the Windows Store service for software distribution, and a new variant known as Windows RT for use on devices that utilize the ARM architecture. An update to Windows 8, called Windows 8.1, was released on October 17, 2013, and includes features such as new live tile sizes, deeper OneDrive integration, and many other revisions. Windows 8 and Windows 8.1 has been subject to some criticism, such as removal of the Start menu.

On September 30, 2014, Microsoft announced Windows 10 as the successor to Windows 8.1. It was released on July 29, 2015, and addresses shortcomings in the user interface first introduced with Windows 8. Changes include the return of the Start Menu, a virtual desktop system, and the ability to run Windows Store apps within windows on the desktop rather than in full-screen mode. Windows 10 is said to be available to update from qualified Windows 7 with SP1 and Windows 8.1 computers from the Get Windows 10 Application (for Windows 7, Windows 8.1) or Windows Update (Windows 7).

On November 12, 2015, an update to Windows 10, version 1511, was released.
This update can be activated with a Windows 7, 8 or 8.1 product key as well as Windows 10 product keys. Features include new icons and right-click context menus, default printer management, four times as many tiles allowed in the Start menu, Find My Device, and Edge updates.

In February 2017, Microsoft announced the migration of its Windows source code repository from Perforce to Git. This migration involved 3.5 million separate files in a 300 gigabyte repository. By May 2017, 90 percent of its engineering team now uses Git, in about 8500 commits and 1760 Windows builds per day.

Multilingual support is built into Windows since Windows 3. The language for both the keyboard and the interface can be changed through the Region and Language Control Panel. Components for all supported input languages, such as Input Method Editors, are automatically installed during Windows installation (in Windows XP and earlier, files for East Asian languages, such as Chinese, and right-to-left scripts, such as Arabic, may need to be installed separately, also from the said Control Panel). Third-party IMEs may also be installed if a user feels that the provided one is insufficient for their needs.

Interface languages for the operating system are free for download, but some languages are limited to certain editions of Windows. Language Interface Packs (LIPs) are redistributable and may be downloaded from Microsoft's Download Center and installed for any edition of Windows (XP or later) they translate most, but not all, of the Windows interface, and require a certain base language (the language which Windows originally shipped with). This is used for most languages in emerging markets. Full Language Packs, which translates the complete operating system, are only available for specific editions of Windows (Ultimate and Enterprise editions of Windows Vista and 7, and all editions of Windows 8, 8.1 and RT except Single Language). They do not require a specific base language, and are commonly used for more popular languages such as French or Chinese. These languages cannot be downloaded through the Download Center, but available as optional updates through the Windows Update service (except Windows 8).

The interface language of installed applications are not affected by changes in the Windows interface language. Availability of languages depends on the application developers themselves.

Windows 8 and Windows Server 2012 introduces a new Language Control Panel where both the interface and input languages can be simultaneously changed, and language packs, regardless of type, can be downloaded from a central location. The PC Settings app in Windows 8.1 and Windows Server 2012 R2 also includes a counterpart settings page for this. Changing the interface language also changes the language of preinstalled Windows Store apps (such as Mail, Maps and News) and certain other Microsoft-developed apps (such as Remote Desktop). The above limitations for language packs are however still in effect, except that full language packs can be installed for any edition except Single Language, which caters to emerging markets.

Windows NT included support for several different platforms before the x86-based personal computer became dominant in the professional world. Windows NT 4.0 and its predecessors supported PowerPC, DEC Alpha and MIPS R4000. (Although some these platforms implement 64-bit computing, the operating system treated them as 32-bit.) However, Windows 2000, the successor of Windows NT 4.0, dropped support for all platforms except the third generation x86 (known as IA-32) or newer in 32-bit mode. The client line of Windows NT family still runs on IA-32, although the Windows Server line has ceased supporting this platform with the release of Windows Server 2008 R2.

With the introduction of the Intel Itanium architecture (IA-64), Microsoft released new versions of Windows to support it. Itanium versions of Windows XP and Windows Server 2003 were released at the same time as their mainstream x86 counterparts. Windows XP 64-Bit Edition, released in 2005, is the last Windows client operating systems to support Itanium. Windows Server line continues to support this platform until Windows Server 2012; Windows Server 2008 R2 is the last Windows operating system to support Itanium architecture.

On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003 x64 Editions to support the x86-64 (or simply x64), the eighth generation of x86 architecture. Windows Vista was the first client version of Windows NT to be released simultaneously in IA-32 and x64 editions. x64 is still supported.

An edition of Windows 8 known as Windows RT was specifically created for computers with ARM architecture and while ARM is still used for Windows smartphones with Windows 10, tablets with Windows RT will not be updated.

Windows CE (officially known as "Windows Embedded Compact"), is an edition of Windows that runs on minimalistic computers, like satellite navigation systems and some mobile phones. Windows Embedded Compact is based on its own dedicated kernel, dubbed Windows CE kernel. Microsoft licenses Windows CE to OEMs and device makers. The OEMs and device makers can modify and create their own user interfaces and experiences, while Windows CE provides the technical foundation to do so.

Windows CE was used in the Dreamcast along with Sega's own proprietary OS for the console. Windows CE was the core from which Windows Mobile was derived. Its successor, Windows Phone 7, was based on components from both Windows CE 6.0 R3 and Windows CE 7.0. Windows Phone 8 however, is based on the same NT-kernel as Windows 8.

Windows Embedded Compact is not to be confused with Windows XP Embedded or Windows NT 4.0 Embedded, modular editions of Windows based on Windows NT kernel.

Xbox OS is an unofficial name given to the version of Windows that runs on the Xbox One. It is a more specific implementation with an emphasis on virtualization (using Hyper-V) as it is three operating systems running at once, consisting of the core operating system, a second implemented for games and a more Windows-like environment for applications.
Microsoft updates Xbox One's OS every month, and these updates can be downloaded from the Xbox Live service to the Xbox and subsequently installed, or by using offline recovery images downloaded via a PC. The Windows 10-based Core had replaced the Windows 8-based one in this update, and the new system is sometimes referred to as "Windows 10 on Xbox One" or "OneCore".
Xbox One's system also allows backward compatibility with Xbox 360, and the Xbox 360's system is backwards compatible with the original Xbox.

In 2017 Microsoft announced that it would start using Git, an open source version control system created by Linus Torvalds. Microsoft has previously used a proprietary version control system called "Source Depot". Microsoft began to integrate Git into Team Foundation Server in 2013, but Windows continued to rely to Source Depot. However, this decision came with some complexity. The Windows codebase is not especially well suited to the decentralized nature of Linux development that Git was originally created to manage. Each Git repository contains a complete history of all the files, which proved unworkable for Windows developers because cloning the repository takes several hours. Microsoft has been working on a new project called the Git Virtual File system (GVFS) to address these challenges.

According to Net Applications, that tracks use based on web use, Windows is the most-used operating system family for personal computers as of July 2017 with close to 90% usage share. When including both personal computers of all kinds, e.g. mobile devices, in July 2017, according to StatCounter, that also tracks use based on web use, Windows OSes accounted for 35.24% of usage share, compared to highest ranked Android at 41.24%, 13.22% for iOS, and 4.64% for macOS. The below 50% usage share of Windows, also applies to developed countries, such as the US (where desktop, with Windows the largest part of, is down to 46.18%), the UK and Ireland. These numbers are easiest (monthly numbers) to find that track real use, but they may not mirror installed base or sales numbers (in recent years) of devices. They are consistent with server numbers in next section.

In terms of the number of devices shipped with the operating system installed, on smartphones, Windows Phone was the third-most-shipped OS (2.6%) after Android (82.8%) and iOS (13.9%) in the second quarter of 2015 according to IDC. Across both PCs and mobile devices, in 2014, Windows OSes were the second-most-shipped (333 million devices, or 14%) after Android (1.2 billion, 49%) and ahead of iOS and macOS combined (263 million, 11%).

Use of the latest version Windows 10 has exceeded Windows 7 globally since early 2018. In most developed countries, such as Japan, Australia and the US, Windows 10 already was the most popular version since early 2017.

Usage share of Windows on serversthose running a web servers that is (there are also other kinds of servers) is at 33.6%.

Consumer versions of Windows were originally designed for ease-of-use on a single-user PC without a network connection, and did not have security features built in from the outset. However, Windows NT and its successors are designed for security (including on a network) and multi-user PCs, but were not initially designed with Internet security in mind as much, since, when it was first developed in the early 1990s, Internet use was less prevalent.

These design issues combined with programming errors (e.g. buffer overflows) and the popularity of Windows means that it is a frequent target of computer worm and virus writers. In June 2005, Bruce Schneier's "Counterpane Internet Security" reported that it had seen over 1,000 new viruses and worms in the previous six months. In 2005, Kaspersky Lab found around 11,000 malicious programs—viruses, Trojans, back-doors, and exploits written for Windows.

Microsoft releases security patches through its Windows Update service approximately once a month (usually the second Tuesday of the month), although critical updates are made available at shorter intervals when necessary. In versions of Windows after and including Windows 2000 SP3 and Windows XP, updates can be automatically downloaded and installed if the user selects to do so. As a result, Service Pack 2 for Windows XP, as well as Service Pack 1 for Windows Server 2003, were installed by users more quickly than it otherwise might have been.

While the Windows 9x series offered the option of having profiles for multiple users, they had no concept of access privileges, and did not allow concurrent access; and so were not true multi-user operating systems. In addition, they implemented only partial memory protection. They were accordingly widely criticised for lack of security.

The Windows NT series of operating systems, by contrast, are true multi-user, and implement absolute memory protection. However, a lot of the advantages of being a true multi-user operating system were nullified by the fact that, prior to Windows Vista, the first user account created during the setup process was an administrator account, which was also the default for new accounts. Though Windows XP did have limited accounts, the majority of home users did not change to an account type with fewer rights – partially due to the number of programs which unnecessarily required administrator rights – and so most home users ran as administrator all the time.

Windows Vista changes this by introducing a privilege elevation system called User Account Control. When logging in as a standard user, a logon session is created and a token containing only the most basic privileges is assigned. In this way, the new logon session is incapable of making changes that would affect the entire system. When logging in as a user in the Administrators group, two separate tokens are assigned. The first token contains all privileges typically awarded to an administrator, and the second is a restricted token similar to what a standard user would receive. User applications, including the Windows shell, are then started with the restricted token, resulting in a reduced privilege environment even under an Administrator account. When an application requests higher privileges or "Run as administrator" is clicked, UAC will prompt for confirmation and, if consent is given (including administrator credentials if the account requesting the elevation is not a member of the administrators group), start the process using the unrestricted token.

All Windows versions from Windows NT 3 have been based on a file system permission system referred to as AGDLP (Accounts, Global, Local, Permissions) in which file permissions are applied to the file/folder in the form of a 'local group' which then has other 'global groups' as members. These global groups then hold other groups or users depending on different Windows versions used. This system varies from other vendor products such as Linux and NetWare due to the 'static' allocation of permission being applied directory to the file or folder. However using this process of AGLP/AGDLP/AGUDLP allows a small number of static permissions to be applied and allows for easy changes to the account groups without reapplying the file permissions on the files and folders.

On January 6, 2005, Microsoft released a Beta version of Microsoft AntiSpyware, based upon the previously released Giant AntiSpyware. On February 14, 2006, Microsoft AntiSpyware became Windows Defender with the release of Beta 2. Windows Defender is a freeware program designed to protect against spyware and other unwanted software. Windows XP and Windows Server 2003 users who have genuine copies of Microsoft Windows can freely download the program from Microsoft's web site, and Windows Defender ships as part of Windows Vista and 7. In Windows 8, Windows Defender and Microsoft Security Essentials have been combined into a single program, named Windows Defender. It is based on Microsoft Security Essentials, borrowing its features and user interface. Although it is enabled by default, it can be turned off to use another anti-virus solution. Windows Malicious Software Removal Tool and the optional Microsoft Safety Scanner are two other free security products offered by Microsoft. In the Windows 10 Anniversary Update, Microsoft introduced the Limited Periodic Scanning feature, which allows Windows Defender to scan, detect, and remove any threats that third-party anti-virus software missed. The Advanced Threat Protection service is introduced for enterprise users. The new service uses cloud service to detect and take actions on advanced network attacks.

In an article based on a report by Symantec, internetnews.com has described Microsoft Windows as having the "fewest number of patches and the shortest average patch development time of the five operating systems it monitored in the last six months of 2006."

A study conducted by Kevin Mitnick and marketing communications firm Avantgarde in 2004, found that an unprotected and unpatched Windows XP system with Service Pack 1 lasted only four minutes on the Internet before it was compromised, and an unprotected and also unpatched Windows Server 2003 system was compromised after being connected to the internet for 8 hours. The computer that was running Windows XP Service Pack 2 was not compromised. The AOL National Cyber Security Alliance Online Safety Study of October 2004, determined that 80% of Windows users were infected by at least one spyware/adware product. Much documentation is available describing how to increase the security of Microsoft Windows products. Typical suggestions include deploying Microsoft Windows behind a hardware or software firewall, running anti-virus and anti-spyware software, and installing patches as they become available through Windows Update.

Owing to the operating system's popularity, a number of applications have been released that aim to provide compatibility with Windows applications, either as a compatibility layer for another operating system, or as a standalone system that can run software written for Windows out of the box. These include:





</doc>
<doc id="18892" url="https://en.wikipedia.org/wiki?curid=18892" title="Mojo (African-American culture)">
Mojo (African-American culture)

Mojo , in the African-American folk belief called hoodoo, is an amulet consisting of a flannel bag containing one or more magical items. It is a "prayer in a bag", or a spell that can be carried with or on the host's body.

Alternative American names for the mojo bag include hand, mojo hand, conjure hand, lucky hand, conjure bag, trick bag, root bag, toby, jomo, and gris-gris bag.

The term "mojo" is now commonly used in the English language to mean one's personal talent or gift. For example, a person might say that they are "getting their mojo on" when trying to get the attention of a possible mate.

The most common synonym for the word mojo is "gris-gris", which literally means "fetish" or "charm"; thus a gris-gris bag is a charm bag. In the Caribbean, an almost identical African-derived bag is called a "wanga" or "oanga" bag, but that term is uncommon in the United States. The word "conjure" is an ancient alternative to "hoodoo", which is a direct variation of African-American folklore. Because of this, a conjure hand is also considered a hoodoo bag, usually made by a respected community conjure doctor.

The word "hand" in this context is defined as a combination of ingredients. The term may derive from the use of finger and hand bones from the dead in mojo bags, or from ingredients such as the lucky hand root (favored by gamblers). The latter suggests an analogy between the varied bag ingredients and the several cards that make up a hand in card games. Mojo reaches as far back as West African culture, where it is said to drive away evil spirits, keep good luck in the household, manipulate a fortune, and lure and persuade lovers. The ideology of the ancestors and the descendants of the mojo hand used this "prayer in a bag" based on their belief of spiritual inheritance, by which the omniscient forefathers of their families would provide protection and favor, especially when they used the mojo. Through this, a strong belief was placed in the idealism of whomever used mojo, creating a spiritual trust in the magic itself.

Although most Southern-style conjure bags are made of red flannel material, most seasoned conjurers use color symbolism. This practice embodies itself in the practice of hoodoo, in which green flannel is used for a money mojo, white flannel is used for a baby-blessing mojo, red flannel is used for love mojo, and so on. West Indians also use mojo bags but often use leather instead of flannel.

The contents of each bag vary directly with the aim of the conjurer. For example, a mojo carried for love-drawing will contain different ingredients than one for gambling luck or magical protection. Ingredients can include roots, herbs, animal parts, minerals, coins, crystals, good luck tokens, and carved amulets. The more personalized objects are used to add extra power because of their symbolic value.

There is a process to fixing a proper mojo. A ritual must be put in place in order to successfully prepare a mojo by being filled and awakened to life. This can be done by smoking incense and candles, or it may be breathed upon to bring it to life. Prayers may be said, and other methods may be used to accomplish this essential step. Once prepared, the mojo is "dressed" or "fed" with a liquid such as alcohol, perfume, water, or bodily fluids. The reason it is said to feed the mojo to keep it working is that it is alive with spirit. One story from the work entitled "From My People" describes a slave who went out and sought a mojo conjurer that gave him a mojo to run away from home. The story describes the slave's mojo as fixing him into many formations, and he ultimately dies because he misuses its power. Had he fixed and believed in the specific mojo himself, he might have escaped the plantation alive.

Mojos are traditionally made for an individual and so must be concealed on the person at all times. Men usually keep the trinkets hidden in the pants pocket, while women are more prone to clip it to the bra. They are also commonly pinned to clothes below the waist. Depending on the type of mojo, the hiding place will be crucial to its success, as those who make conjure bags to carry love spells sometimes specify that the mojo must be worn next to the skin. A story from the book "From My People" described the story of Moses and the task he went through to bring his people out of slavery. It described how "Hoodoo Lost his Hand", as Moses's mojo was hidden through his staff. When he turned it into a snake, the pharaoh made his soothsayers and magicians create the same effect. As a result, the Pharaoh's snake was killed by Moses's snake, and that is how Hoodoo lost his hand.


</doc>
<doc id="18894" url="https://en.wikipedia.org/wiki?curid=18894" title="Matt Groening">
Matt Groening

Matthew Abraham Groening ( ; born February 15, 1954) is an American cartoonist, writer, producer, animator, and voice actor. He is the creator of the comic strip "Life in Hell" (1977–2012) and the television series "The Simpsons" (1989–present), "Futurama" (1999–2003, 2008–2013), and the upcoming "Disenchantment" (2018). "The Simpsons" is the longest-running U.S. primetime-television series in history and the longest-running U.S. animated series and sitcom.

Groening made his first professional cartoon sale of "Life in Hell" to the avant-garde "Wet" magazine in 1978. At its peak, the cartoon was carried in 250 weekly newspapers. "Life in Hell" caught the attention of James L. Brooks. In 1985, Brooks contacted Groening with the proposition of working in animation for the Fox variety show "The Tracey Ullman Show". Originally, Brooks wanted Groening to adapt his "Life in Hell" characters for the show. Fearing the loss of ownership rights, Groening decided to create something new and came up with a cartoon family, the Simpson family, and named the members after his own parents and sisters—while Bart was an anagram of the word brat. The shorts would be spun off into their own series "The Simpsons", which has since aired episodes. In 1997, Groening and former "Simpsons" writer David X. Cohen developed "Futurama", an animated series about life in the year 3000, which premiered in 1999, running for four years on Fox, then picked up by Comedy Central for additional seasons. Groening is currently developing a new series for Netflix titled "Disenchantment", which is set to premiere in 2018.

Groening has won 12 Primetime Emmy Awards, ten for "The Simpsons" and two for "Futurama" as well as a British Comedy Award for "outstanding contribution to comedy" in 2004. In 2002, he won the National Cartoonist Society Reuben Award for his work on "Life in Hell". He received a star on the Hollywood Walk of Fame on February 14, 2012.

Groening was born on February 15, 1954 in Portland, Oregon, the middle of five children (older brother Mark and sister Patty were born in 1950 and 1952, while the younger sisters Lisa and Maggie in 1956 and 1958, respectively). His Norwegian American mother, Margaret Ruth (née Wiggum; March 23, 1919 – April 22, 2013), was once a teacher, and his German Canadian father, Homer Philip Groening (December 30, 1919 – March 15, 1996), was a filmmaker, advertiser, writer and cartoonist. Homer, born in Main Centre, Saskatchewan, Canada, grew up in a Mennonite, Plautdietsch-speaking family.

Matt's grandfather, Abraham Groening, was a professor at Tabor College, a Mennonite Brethren liberal arts college in Hillsboro, Kansas before moving to Albany College (now known as Lewis and Clark College) in Oregon in 1930.

Groening grew up in Portland, and attended Ainsworth Elementary School and Lincoln High School. From 1972 to 1977, Groening attended The Evergreen State College in Olympia, Washington, a liberal arts school that he described as "a hippie college, with no grades or required classes, that drew every weirdo in the Northwest." He served as the editor of the campus newspaper, "The Cooper Point Journal", for which he also wrote articles and drew cartoons. He befriended fellow cartoonist Lynda Barry after discovering that she had written a fan letter to Joseph Heller, one of Groening's favorite authors, and had received a reply. Groening has credited Barry with being "probably [his] biggest inspiration." He first became interested in cartoons after watching the Disney animated film "One Hundred and One Dalmatians", and he has also cited "Peanuts" and its creator Charles M. Schulz as inspirations.

In 1977, at the age of 23, Groening moved to Los Angeles to become a writer. He went through what he described as "a series of lousy jobs," including being an extra in the television movie "When Every Day Was the Fourth of July", busing tables, washing dishes at a nursing home, clerking at the Hollywood Licorice Pizza record store, landscaping in a sewage treatment plant, and chauffeuring and ghostwriting for a retired Western director.

Groening described life in Los Angeles to his friends in the form of the self-published comic book "Life in Hell", which was loosely inspired by the chapter "How to Go to Hell" in Walter Kaufmann's book "Critique of Religion and Philosophy". Groening distributed the comic book in the book corner of Licorice Pizza, a record store in which he worked. He made his first professional cartoon sale to the avant-garde "Wet" magazine in 1978. The strip, titled "Forbidden Words," appeared in the September/October issue of that year.

Groening had gained employment at the "Los Angeles Reader", a newly formed alternative newspaper, delivering papers, typesetting, editing and answering phones. He showed his cartoons to the editor, James Vowell, who was impressed and eventually gave him a spot in the paper. "Life in Hell" made its official debut as a comic strip in the "Reader" on April 25, 1980. Vowell also gave Groening his own weekly music column, "Sound Mix," in 1982. However, the column would rarely actually be about music, as he would often write about his "various enthusiasms, obsessions, pet peeves and problems" instead. In an effort to add more music to the column, he "just made stuff up," concocting and reviewing fictional bands and nonexistent records. In the following week's column, he would confess to fabricating everything in the previous column and swear that everything in the new column was true. Eventually, he was finally asked to give up the "music" column. Among the fans of the column was Harry Shearer, who would later become a voice on "The Simpsons".

"Life in Hell" became popular almost immediately. In November 1984, Deborah Caplan, Groening's then-girlfriend and co-worker at the "Reader", offered to publish "Love is Hell", a series of relationship-themed "Life in Hell" strips, in book form. Released a month later, the book was an underground success, selling 22,000 copies in its first two printings. "Work is Hell" soon followed, also published by Caplan. Soon afterward, Caplan and Groening left and put together the Life in Hell Co., which handled merchandising for "Life in Hell". Groening also started Acme Features Syndicate, which syndicated "Life in Hell", Lynda Barry and John Callahan, but now only syndicates "Life in Hell". At the end of its run, "Life in Hell" was carried in 250 weekly newspapers and has been anthologized in a series of books, including "School is Hell", "Childhood is Hell", "The Big Book of Hell", and "The Huge Book of Hell". Although Groening has stated, "I'll never give up the comic strip. It's my foundation," he announced that the June 16, 2012 strip would mark "Life in Hell"s conclusion. After Groening ended the strip, the Center for Cartoon Studies commissioned a poster that was presented to Groening in honor of his work. The poster contained tribute cartoons by 22 of Groening's cartoonist friends who were influenced by "Life in Hell".

"Life in Hell" caught the eye of Hollywood writer-producer and Gracie Films founder James L. Brooks, who had been shown the strip by fellow producer Polly Platt. In 1985, Brooks contacted Groening with the proposition of working in animation on an undefined future project, which would turn out to be developing a series of short animated skits, called "bumpers," for the Fox variety show "The Tracey Ullman Show". Originally, Brooks wanted Groening to adapt his "Life in Hell" characters for the show. Groening feared that he would have to give up his ownership rights, and that the show would fail and would take down his comic strip with it. Groening conceived of the idea for The Simpsons in the lobby of James L. Brooks's office and hurriedly sketched out his version of a dysfunctional family: Homer, the overweight father; Marge, the slim mother; Bart, the bratty oldest child; Lisa, the intelligent middle child; and Maggie, the baby. Groening famously named the main Simpson characters after members of his own family: his parents, Homer and Margaret (Marge or Marjorie in full), and his younger sisters, Lisa and Margaret (Maggie). Claiming that it was a bit too obvious to name a character after himself, he chose the name "Bart," an anagram of brat. However, he stresses that aside from some of the sibling rivalry, his family is nothing like the Simpsons. Groening also has an older brother and sister, Mark and Patty, and in a 1995 interview Groening divulged that Mark "is the actual inspiration for Bart."

Maggie Groening has co-written a few "Simpsons" books featuring her cartoon namesake.

The family was crudely drawn, because Groening had submitted basic sketches to the animators, assuming they would clean them up; instead, they just traced over his drawings. The entire Simpson family was designed so that they would be recognizable in silhouette. When Groening originally designed Homer, he put his own initials into the character's hairline and ear: the hairline resembled an 'M', and the right ear resembled a 'G'. Groening decided that this would be too distracting though, and redesigned the ear to look normal. He still draws the ear as a 'G' when he draws pictures of Homer for fans. Marge's distinct beehive hairstyle was inspired by "Bride of Frankenstein" and the style that Margaret Groening wore during the 1960s, although her hair was never blue. Bart's original design, which appeared in the first shorts, had spikier hair, and the spikes were of different lengths. The number was later limited to nine spikes, all of the same size. At the time Groening was primarily drawing in black and "not thinking that [Bart] would eventually be drawn in color" gave him spikes that appear to be an extension of his head. Lisa's physical features are generally not used in other characters; for example, in the later seasons, no character other than Maggie shares her hairline. While designing Lisa, Groening "couldn't be bothered to even think about girls' hair styles". When designing Lisa and Maggie, he "just gave them this kind of spiky starfish hair style, not thinking that they would eventually be drawn in color". Groening storyboarded and scripted every short (now known as "The Simpsons shorts"), which were then animated by a team including David Silverman and Wes Archer, both of whom would later become directors on the series.

The Simpsons shorts first appeared in "The Tracey Ullman Show" on April 19, 1987. Another family member, Grampa Simpson, was introduced in the later shorts. Years later, during the early seasons of "The Simpsons", when it came time to give Grampa a first name, Groening says he refused to name him after his own grandfather, Abraham Groening, leaving it to other writers to choose a name. By coincidence, they chose Abraham, unaware that it was the name of Groening's grandfather.

Although "The Tracey Ullman Show" was not a big hit, the popularity of the shorts led to a half-hour spin-off in 1989. A team of production companies adapted "The Simpsons" into a half-hour series for the Fox Broadcasting Company. The team included what is now the Klasky Csupo animation house. James L. Brooks negotiated a provision in the contract with the Fox network that prevented Fox from interfering with the show's content. Groening said his goal in creating the show was to offer the audience an alternative to what he called "the mainstream trash" that they were watching. The half-hour series premiered on December 17, 1989 with "Simpsons Roasting on an Open Fire", a Christmas special. "Some Enchanted Evening" was the first full-length episode produced, but it did not broadcast until May 1990, as the last episode of the first season, because of animation problems.

The series quickly became a worldwide phenomenon, to the surprise of many. Groening said: "Nobody thought "The Simpsons" was going to be a big hit. It sneaked up on everybody." "The Simpsons" was co-developed by Groening, Brooks, and Sam Simon, a writer-producer with whom Brooks had worked on previous projects. Groening and Simon, however, did not get along and were often in conflict over the show; Groening once described their relationship as "very contentious." Simon eventually left the show in 1993 over creative differences.

Like the main family members, several characters from the show have names that were inspired by people, locations or films. The name "Wiggum" for police chief Chief Wiggum is Groening's mother's maiden name. The names of a few other characters were taken from major street names in Groening's hometown of Portland, Oregon, including Flanders, Lovejoy, Powell, Quimby and Kearney. Despite common fan belief that Sideshow Bob Terwilliger was named after SW Terwilliger Boulevard in Portland, he was actually named after the character Dr. Terwilliker from the film "The 5,000 Fingers of Dr. T".

Although Groening has pitched a number of spin-offs from "The Simpsons", those attempts have been unsuccessful. In 1994, Groening and other "Simpsons" producers pitched a live-action spin-off about Krusty the Clown (with Dan Castellaneta playing the lead role), but were unsuccessful in getting it off the ground. Groening has also pitched "Young Homer" and a spin-off about the non-Simpsons citizens of Springfield.

In 1995, Groening got into a major disagreement with Brooks and other "Simpsons" producers over "A Star Is Burns", a crossover episode with "The Critic", an animated show also produced by Brooks and staffed with many former "Simpsons" crew members. Groening claimed that he feared viewers would "see it as nothing but a pathetic attempt to advertise "The Critic" at the expense of "The Simpsons"," and was concerned about the possible implication that he had created or produced "The Critic". He requested his name be taken off the episode.

Groening is credited with writing or co-writing the episodes "Some Enchanted Evening", "The Telltale Head", "Colonel Homer" and "22 Short Films About Springfield", as well as "The Simpsons Movie", released in 2007. He has had several cameo appearances in the show, with a speaking role in the episode "My Big Fat Geek Wedding". He currently serves at "The Simpsons" as an executive producer and creative consultant.

After spending a few years researching science fiction, Groening got together with "Simpsons" writer/producer David X. Cohen (known as David S. Cohen at the time) in 1997 and developed "Futurama", an animated series about life in the year 3000. By the time they pitched the series to Fox in April 1998, Groening and Cohen had composed many characters and storylines; Groening claimed they had gone "overboard" in their discussions. Groening described trying to get the show on the air as "by far the worst experience of [his] grown-up life." The show premiered on March 28, 1999. Groening's writing credits for the show are for the premiere episode, "Space Pilot 3000" (co-written with Cohen), "Rebirth" (story) and "In-A-Gadda-Da-Leela" (story).
After four years on the air, the show was canceled by Fox. In a situation similar to "Family Guy", however, strong DVD sales and very stable ratings on Adult Swim brought "Futurama" back to life. When Comedy Central began negotiating for the rights to air "Futurama" reruns, Fox suggested that there was a possibility of also creating new episodes. When Comedy Central committed to sixteen new episodes, it was decided that four straight-to-DVD films – "" (2007), "" (2008), "" (2008) and "" (2009) – would be produced.

Since no new "Futurama" projects were in production, the movie "Into the Wild Green Yonder" was designed to stand as the "Futurama" series finale. However, Groening had expressed a desire to continue the "Futurama" franchise in some form, including as a theatrical film. In an interview with CNN, Groening said that "we have a great relationship with Comedy Central and we would love to do more episodes for them, but I don't know... We're having discussions and there is some enthusiasm but I can't tell if it's just me". Comedy Central commissioned an additional 26 new episodes, and began airing them in 2010. The show continued in to 2013, before Comedy Central announced in April 2013 that they would not be renewing it beyond its seventh season. The final episode aired on September 4, 2013.

On January 15, 2016, it was announced that Groening was in talks with Netflix to develop a new animated series. On July 25, 2017 the series, "Disenchantment", was ordered by Netflix with a two-season order, totalling 20 episodes.

In 1994, Groening formed Bongo Comics (named after the character Bongo from "Life in Hell") with Steve Vance, Cindy Vance and Bill Morrison, which publishes comic books based on "The Simpsons" and "Futurama" (including "Futurama Simpsons Infinitely Secret Crossover Crisis", a crossover between the two), as well as a few original titles. According to Groening, the goal with Bongo is to "[try] to bring humor into the fairly grim comic book market." He also formed Zongo Comics in 1995, an imprint of Bongo that published comics for more mature readers, which included three issues of Mary Fleener's "Fleener" and seven issues of his close friend Gary Panter's "Jimbo" comics.

Groening is known for his eclectic taste in music. His favorite band is Frank Zappa and The Mothers of Invention and his favorite album is "Trout Mask Replica" by Captain Beefheart (which was produced by Zappa). He guest-edited Da Capo Press's "Best Music Writing 2003" and curated a US All Tomorrow's Parties music festival in 2003. He illustrated the cover of Frank Zappa's posthumous album "" (1996). In May 2010, he curated another edition of All Tomorrow's Parties in Minehead, England. He also plays the drums in the all-author rock and roll band The Rock Bottom Remainders (although he is listed as the cowbell player), whose other members include Dave Barry, Ridley Pearson, Scott Turow, Amy Tan, James McBride, Mitch Albom, Roy Blount Jr., Stephen King, Kathi Kamen Goldmark, Sam Barry and Greg Iles. In July 2013, Groening co-authored "Hard Listening" (2013) with the rest of the Rock Bottom Remainders (published by Coliloquy, LLC).
Groening and Deborah Caplan married in 1986 and had two sons together, Homer (who goes by Will) and Abe, both of whom Groening occasionally portrays as rabbits in "Life in Hell". The couple divorced in 1999 after thirteen years of marriage. In 2011, Groening married Argentine artist Agustina Picasso after a four-year relationship, and became stepfather to her daughter Camila Costantini. In May 2013, Picasso gave birth to Nathaniel Philip Picasso Groening, named after writer Nathanael West. She joked that "his godfather is SpongeBob's creator Stephen Hillenburg". Matt is the brother-in-law of "Hey Arnold!" and "Dinosaur Train" creator, Craig Bartlett, who is married to Groening's sister, Lisa. Arnold used to appear in "Simpsons Illustrated".

Groening identifies himself as agnostic and a liberal and has often made campaign contributions to Democratic Party candidates. His first cousin, Laurie Monnes Anderson, is a member of the Oregon State Senate representing eastern Multnomah County.

Due to the success of "The Simpsons", Matt Groening is estimated to be worth $500 million. In 2011, Groening bought an $11.7 million mansion in Santa Monica, California.

Groening has been nominated for 25 Emmy Awards and has won twelve, ten for "The Simpsons" and two for "Futurama" in the "Outstanding Animated Program (for programming one hour or less)" category. Groening received the 2002 National Cartoonist Society Reuben Award, and had been nominated for the same award in 2000. He received a British Comedy Award for "outstanding contribution to comedy" in 2004. In 2007, he was ranked fourth (and highest American by birth) in a list of the "top 100 living geniuses", published by British newspaper "The Daily Telegraph".

He received the 2,459th star on the Hollywood Walk of Fame on February 14, 2012.




</doc>
<doc id="18895" url="https://en.wikipedia.org/wiki?curid=18895" title="Metaphysics">
Metaphysics

Metaphysics is a branch of philosophy that explores the nature of being, existence, and reality. Metaphysics seeks to answer, in a "suitably abstract and fully general manner", the questions: 

Topics of metaphysical investigation include existence, objects and their properties, space and time, cause and effect, and possibility. 

Like mathematics, metaphysics is a non-empirical study which is conducted using analytical thought alone. Like foundational mathematics (which is sometimes considered a special case of metaphysics applied to the existence of number), it tries to give a coherent account of the structure of the world, capable of explaining our everyday and scientific perception of the world, and being free from contradictions. In mathematics, there are many different ways to define numbers; similarly in metaphysics there are many different ways to define objects, properties, concepts, and other entities which are claimed to make up the world. While metaphysics may, as a special case, study the entities postulated by fundamental science such as atoms and superstrings, its core topic is the set of categories such as object, property and causality which those scientific theories assume. For example: claiming that "electrons have charge" is a scientific theory; while exploring what it means for electrons to be (or at least, to be perceived as) "objects", charge to be a "property", and for both to exist in a topological entity called "space" is the task of metaphysics.

There are two broad stances about what is "the world" studied by metaphysics. The strong, classical view assumes that the objects studied by metaphysics exist independently of any observer, so that the subject is the most fundamental of all sciences. The weak, modern view assumes that the objects studied by metaphysics exist inside the mind of an observer, so the subject becomes a form of introspection and conceptual analysis. Some philosophers, notably Kant, discuss both of these "worlds" and what can be inferred about each one. Some philosophers, such as the logical positivists, and many scientists, reject the strong view of metaphysics as meaningless and unverifiable. Others reply that this criticism also applies to any type of knowledge, including hard science, which claims to describe anything other than the contents of human perception, and thus that the world of perception "is" the objective world in some sense. Metaphysics itself usually assumes that some stance has been taken on these questions and that it may proceed independently of the choice -- the question of which stance to take belongs instead to another branch of philosophy, epistemology.

Ontology is the philosophical study of the nature of being, becoming, existence or reality, as well as the basic categories of being and their relations. Traditionally listed as the core of metaphysics, ontology often deals with questions concerning what entities exist or may be said to exist and how such entities may be grouped, related within a hierarchy, and subdivided according to similarities and differences.

Identity is a fundamental metaphysical issue. Metaphysicians investigating identity are tasked with the question of what, exactly, it means for something to be identical to itself, or - more controversially - to something else. Issues of identity arise in the context of time: what does it mean for something to be itself across two moments in time? How do we account for this? Another question of identity arises when we ask what our criteria ought to be for determining identity? And how does the reality of identity interface with linguistic expressions?

The metaphysical positions one takes on identity have far-reaching implications on issues such as the mind-body problem, personal identity, ethics, and law.

The ancient Greeks took extreme positions on the nature of change. Parmenides denied change altogether, while Heraclitus argued that change was ubiquitous: "[Y]ou cannot step into the same river twice."

Identity, sometimes called Numerical Identity, is the relation that a "thing" bears to itself, and which no "thing" bears to anything other than itself (cf. sameness).

A modern philosopher who made a lasting impact on the philosophy of identity was Leibniz, whose "Law of the Indiscernibility of Identicals" is still in wide use today. It states that if some object x is identical to some object y, then any property that x has, y will have as well.

Put formally, it states 

However, it seems, too, that objects can change over time. If one were to look at a tree one day, and the tree later lost a leaf, it would seem that one could still be looking at that same tree. Two rival theories to account for the relationship between change and identity are Perdurantism, which treats the tree as a series of tree-stages, and Endurantism, which maintains that the organism—the same tree—is present at every stage in its history.

Objects appear to us in space and time, while abstract entities such as classes, properties, and relations do not. What then is meant by space and time such that it can serve this function as a ground for objects? Are space and time entities themselves, of some form, or must they exist prior to other entities? How exactly can they be defined? For example, if time is defined as a "rate of change" then must there always be something changing in order for time to exist? 

Classical philosophy recognized a number of causes, including teleological future causes. In special relativity and quantum field theory the notions of space, time and causality become tangled together, with temporal orders of causations becoming dependent on who is observing them. The laws of physics are symmetrical in time, so could equally well be used to describe time as running backwards. Why then do we perceive it as flowing in one direction, the arrow of time, and as containing causation flowing in the same direction?

Causality is linked by most philosophers to the concept of counterfactuals. To say that A caused B means that if A had not happened then B would not have happened.

Causality is usually required as a foundation for philosophy of science, if science aims to understand causes and effects and make predictions about them.

Metaphysicians investigate questions about the ways the world could have been. David Lewis, in "On the Plurality of Worlds," endorsed a view called Concrete Modal realism, according to which facts about how things could have been are made true by other concrete worlds, just as in ours, in which things are different. Other philosophers, such as Gottfried Leibniz, have dealt with the idea of possible worlds as well. The idea of necessity is that any necessary fact is true across all possible worlds. A possible fact is true in some possible world, even if not in the actual world. For example, it is possible that cats could have had two tails, or that any particular apple could have not existed. By contrast, certain propositions seem necessarily true, such as analytic propositions, e.g., "All bachelors are unmarried." The particular example of analytic truth being necessary is not universally held among philosophers. A less controversial view might be that self-identity is necessary, as it seems fundamentally incoherent to claim that for any x, it is not identical to itself; this is known as the "law of identity", a putative "first principle". Aristotle describes the "principle of non-contradiction", "It is impossible that the same quality should both belong and not belong to the same thing ... This is the most certain of all principles ... Wherefore they who demonstrate refer to this as an ultimate opinion. For it is by nature the source of all the other axioms."

What is "central" and "peripheral" to metaphysics has varied over time and schools; however contemporary analytic philosophy as taught in USA and UK universities generally regards the above as "central" and the following as "applications" or "peripheral" topics; or in some cases as distinct subjects which have grown out of and depend upon metaphysics:

Metaphysical cosmology is the branch of metaphysics that deals with the world as the totality of all phenomena in space and time. Historically, it formed a major part of the subject alongside Ontology, though its role is more peripheral in contemporary philosophy. It has had a broad scope, and in many cases was founded in religion. The ancient Greeks drew no distinction between this use and their model for the cosmos. However, in modern times it addresses questions about the Universe which are beyond the scope of the physical sciences. It is distinguished from religious cosmology in that it approaches these questions using philosophical methods (e.g. dialectics).

Cosmogony deals specifically with the origin of the universe. Modern metaphysical cosmology and cosmogony try to address questions such as:

Accounting for the existence of mind in a world otherwise composed of matter is a metaphysical problem which is so large and important as to have become a specialized subject of study in its own right, philosophy of mind.

Substance dualism is a classical theory in which mind and body are essentially different, with the mind having some of the attributes traditionally assigned to the soul, and which creates an immediate conceptual puzzle about how the two interact. Idealism postulates that material objects do not exist unless perceived and only as perceptions. Panpsychism and panexperientialism, are property dualist theories in which everything "has" or "is" a mind rather than everything exists "in" a mind. Neutral monism postulates that existence consists of a single substance that in itself is neither mental nor physical, but is capable of mental and physical aspects or attributesthus it implies a dual-aspect theory. For the last century, the dominant theories have been science-inspired including materialistic monism, Type identity theory, token identity theory, functionalism, reductive physicalism, nonreductive physicalism, eliminative materialism, anomalous monism, property dualism, epiphenomenalism and emergence.

Determinism is the philosophical proposition that every event, including human cognition, decision and action, is causally determined by an unbroken chain of prior occurrences. It holds that nothing happens that has not already been determined. The principal consequence of the deterministic claim is that it poses a challenge to the existence of free will.

The problem of free will is the problem of whether rational agents exercise control over their own actions and decisions. Addressing this problem requires understanding the relation between freedom and causation, and determining whether the laws of nature are causally deterministic. Some philosophers, known as Incompatibilists, view determinism and free will as mutually exclusive. If they believe in determinism, they will therefore believe free will to be an illusion, a position known as "Hard Determinism". Proponents range from Baruch Spinoza to Ted Honderich.

Others, labeled Compatibilists (or "Soft Determinists"), believe that the two ideas can be reconciled coherently. Adherents of this view include Thomas Hobbes and many modern philosophers such as John Martin Fischer.

Incompatibilists who accept free will but reject determinism are called Libertarians, a term not to be confused with the political sense. Robert Kane and Alvin Plantinga are modern defenders of this theory.

The earliest type of classification of social construction traces back to Plato in his dialogue Phaedrus where he claims that the biological classification system seems to "carve nature at the joints". In contrast, later philosophers such as Michel Foucault and Jorge Luis Borges have challenged the capacity of natural and social classification. In his essay The Analytical Language of John Wilkins, Borges makes us imagine a certain encyclopedia where the animals are divided into (a) those that belong to the emperor; (b) embalmed ones; (c) those that are trained;... and so forth, in order to bring forward the ambiguity of natural and social kinds. According to metaphysics author Alyssa Ney: "the reason all this is interesting is that there seems to be a metahysical difference between the Borgesian system and Platos". The difference is not obvious but one classification attempts to carve entities up according to objective distinction while the other does not. According to Quine this notion is closely related to the notion of similarity.

Main article: philosophy of mathematics
There are different ways to set up the notion of number in metaphysics theories. Platonist theories postulate number as a fundamental category itself. Others consider it to be a property of an entity called a "group" comprising other entities; or to be a relation held between several groups of entities, such as "the number four is the set of all sets of four things". Many of the debates around universals are applied to the study of number, and are of particular importance due to its status as a foundation for the philosophy of mathematics and for mathematics itself.

Although metaphysics as a philosophical enterprise is highly hypothetical, it also has practical application in most other branches of philosophy, science, and now also information technology. Such areas generally assume some basic ontology (such as a system of objects, properties, classes, and spacetime) as well as other metaphysical stances on topics such as causality and agency, then build their own particular theories upon these. 

In Science for example, some theories are based on the ontological assumption of objects with properties (such as electrons having charge) while others may reject objects completely (such as quantum field theories, where spread-out "electronness" becomes a property of spacetime rather than an object). Some Science theories, such as Newton's second law "F=ma" are based on a realist stance on causation and agency: the law says that an agent can cause motion, a, of a mass, m, by applying a force, F. Other, such as Albert Einstein's theory of relativity are deterministic and lack agents and causes.

"Social" branches of philosophy such as philosophy of morality, aesthetics and philosophy of religion (which in turn give rise to practical subjects such as ethics, politics, law, and art) all require metaphysical foundations, which may be considered as branches or applications of metaphysics. For example they may postulate the existence of basic entities such as value, beauty, and God respectively. Then they use these postulates to make their own arguments about consequences resulting from them. When philosophers in these subjects make their foundations they are doing applied metaphysics, and may draw upon its core topics and methods to guide them, including ontology and other core and peripheral topics. As in Science, the foundations chosen will in turn depend on the underlying ontology used, so philosophers in these subjects may have to dig right down to the ontological layer of metaphysics to find what is possible for their theories. For example, a contradiction obtained in a theory of God or Beauty might be due to an assumption that it is an object rather than some other kind of ontological entity.

Prior to the modern history of science, scientific questions were addressed as a part of natural philosophy. Originally, the term "science" (Latin "scientia") simply meant "knowledge". The scientific method, however, transformed natural philosophy into an empirical activity deriving from experiment, unlike the rest of philosophy. By the end of the 18th century, it had begun to be called "science" to distinguish it from philosophy. Thereafter, metaphysics denoted philosophical enquiry of a non-empirical character into the nature of existence.

Metaphysics continues asking "why" where science leaves off. For example, any theory of fundamental physics is based on some set of axioms, which may postulate the existence of entities such as atoms, particles, forces, charges, mass, or fields. Stating such postulates is considered to be the "end" of a science theory. Metaphysics takes these postulates and explores what they mean as human concepts. For example, do all theories of physics require the existence of space and time, objects, and properties? Or can they be expressed using only objects, or only properties? Do the objects have to retain their identity over time or do they change? If they change, then are they still the same object? Can theories be reformulated by converting properties or predicates (such as "red") into entities (such as redness or redness fields). Is the distinction between objects and properties fundamental to the physical world or to our perception of it?

Much recent work has been devoted to analyzing the role of metaphysics in scientific theorizing. Alexandre Koyré led this movement, declaring in his book "Metaphysics and Measurement", "It is not by following experiment, but by outstripping experiment, that the scientific mind makes progress." That metaphysical propositions can influence scientific theorizing is John Watkins' most lasting contribution to philosophy. Since 1957 "he showed the ways in which some untestable and hence, according to Popperian ideas, non-empirical propositions can nevertheless be influential in the development of properly testable and hence scientific theories. These profound results in applied elementary logic...represented an important corrective to positivist teachings about the meaninglessness of metaphysics and of normative claims". Imre Lakatos maintained that all scientific theories have a metaphysical "hard core" essential for the generation of hypotheses and theoretical assumptions. Thus, according to Lakatos, "scientific changes are connected with vast cataclysmic metaphysical revolutions."

An example from biology of Lakatos' thesis: David Hull has argued that changes in the ontological status of the species concept have been central in the development of biological thought from Aristotle through Cuvier, Lamarck, and Darwin. Darwin's ignorance of metaphysics made it more difficult for him to respond to his critics because he could not readily grasp the ways in which their underlying metaphysical views differed from his own.

In physics, new metaphysical ideas have arisen in connection with quantum mechanics, where subatomic particles arguably do not have the same sort of individuality as the particulars with which philosophy has traditionally been concerned. Also, adherence to a deterministic metaphysics in the face of the challenge posed by the quantum-mechanical uncertainty principle led physicists such as Albert Einstein to propose alternative theories that retained determinism. A. N. Whitehead is famous for creating a process philosophy metaphysics inspired by electromagnetism and special relativity.

In chemistry, Gilbert Newton Lewis addressed the nature of motion, arguing that an electron should not be said to move when it has none of the properties of motion.

Katherine Hawley notes that the metaphysics even of a widely accepted scientific theory may be challenged if it can be argued that the metaphysical presuppositions of the theory make no contribution to its predictive success.

A number of individuals have suggested that much or all of metaphysics should be rejected. In the eighteenth century, David Hume took an extreme position, arguing that all genuine knowledge involves either mathematics or matters of fact and that metaphysics, which goes beyond these, is worthless. He concludes his "Enquiry Concerning Human Understanding" with the statement:

If we take in our hand any volume; of divinity or school metaphysics, for instance; let us ask, "Does it contain any abstract reasoning concerning quantity or number?" No. "Does it contain any experimental reasoning concerning matter of fact and existence?" No. Commit it then to the flames: for it can contain nothing but sophistry and illusion.

Thirty-three years after Hume's "Enquiry" appeared, Immanuel Kant published his "Critique of Pure Reason". Although he followed Hume in rejecting much of previous metaphysics, he argued that there was still room for some "synthetic a priori" knowledge, concerned with matters of fact yet obtainable independent of experience. These included fundamental structures of space, time, and causality. He also argued for the freedom of the will and the existence of "things in themselves", the ultimate (but unknowable) objects of experience.

Wittgenstein introduced the concept that metaphysics could be influenced by theories of aesthetics, via logic, vis. a world composed of "atomical facts".

In the 1930s, A. J. Ayer and Rudolf Carnap endorsed Hume's position; Carnap quoted the passage above. They argued that metaphysical statements are neither true nor false but meaningless since, according to their verifiability theory of meaning, a statement is meaningful only if there can be empirical evidence for or against it. Thus, while Ayer rejected the monism of Spinoza, he avoided a commitment to pluralism, the contrary position, by holding both views to be without meaning. Carnap took a similar line with the controversy over the reality of the external world. While the logical positivism movement is now considered dead, (with a major proponent AJ Ayer admitting in a TV interview that "it was a lot of fun ... but it was false") it has continued to influence philosophy development.

Arguing against such rejections, the Scholastic philosopher Edward Feser has observed that Hume's critique of metaphysics, and specifically Hume's fork, is "notoriously self-refuting". Feser argues that Hume's fork itself is not a conceptual truth and is not empirically testable.

Some living philosophers, such as Amie Thomasson, have argued that many metaphysical questions can be dissolved just by looking at the way we use words; others, such as Ted Sider, have argued that metaphysical questions are substantive, and that we can make progress toward answering them by comparing theories according to a range of theoretical virtues inspired by the sciences, such as simplicity and explanatory power.

The word "metaphysics" derives from the Greek words μετά ("metá", "beyond", "upon" or "after") and φυσικά ("physiká", "physics"). It was first used as the title for several of Aristotle's works, because they were usually anthologized after the works on physics in complete editions. The prefix "meta-" ("after") indicates that these works come "after" the chapters on physics. However, Aristotle himself did not call the subject of these books metaphysics: he referred to it as "first philosophy." The editor of Aristotle's works, Andronicus of Rhodes, is thought to have placed the books on first philosophy right after another work, "Physics", and called them ("tà metà tà physikà biblía") or "the books [that come] after the [books on] physics". This was misread by Latin scholiasts, who thought it meant "the science of what is beyond the physical".

However, once the name was given, the commentators sought to find intrinsic reasons for its appropriateness. For instance, it was understood to mean "the science of the world beyond nature" (φύσις - "phýsis" in Greek), that is, the science of the immaterial. Again, it was understood to refer to the chronological or pedagogical order among our philosophical studies, so that the "metaphysical sciences" would mean "those that we study after having mastered the sciences that deal with the physical world" (St. Thomas Aquinas, "Expositio in librum Boethii De hebdomadibus", V, 1).

A person who does, or is doing, metaphysics is called a "metaphysician".

Common parlance also uses the word "metaphysics" for a different referent from that of the present article, namely for beliefs in arbitrary non-physical or magical entities. For example "Metaphysical healing" to refer to healing by means of remedies that are magical rather than scientific. This usage stemmed from the various historical schools of speculative metaphysics which operated by postulating all manner of physical, mental and spiritual entities as bases for particular metaphysical systems. Metaphysics as a subject does not preclude beliefs in such magical entities but neither does it promote them. Rather, it is the subject which provides the vocabulary and logic with which such beliefs might be analyzed and studied, for example to search for inconsistencies both within themselves and with other accepted systems such as Science.

Cognitive archeology such as analysis of cave paintings and other pre-historic art and customs suggests that a form of perennial philosophy or Shamanism metaphysics may stretch back to the birth of behavioral modernity, all around the world. Similar beliefs are found in present-day "stone age" cultures such as Australian aboriginals. Perennial philosophy postulates the existence of a spirit or concept world alongside the day-to-day world, and interactions between these worlds during dreaming and ritual, or on special days or at special places. It has been argued that perennial philosophy formed the basis for Platonism, with Plato articulating, rather than creating, much older widespread beliefs. 

Bronze Age cultures such as ancient Mesopotamia and ancient Egypt (along with similarly structured but chronologically later cultures such as Mayans and Aztecs) developed belief systems based on mythology, anthropomorphic gods, mind-body dualism, and a spirit world, to explain causes and cosmology. These cultures appear to have been interested in astronomy and may have associated or identified the stars with some of these entities. In ancient Egypt, the ontological distinction between order (maat) and chaos (Isfet) seems to have been important. 

The first named Greek philosopher, according to Aristotle, is Thales of Miletus, early 6th century BCE. He made use of purely physical explanations to explain the phenomena of the world rather than the mythological and divine explanations of tradition. He is thought to have posited water as the single underlying principle (or "Arche in later Aristotelian terminology) of the material world". His fellow, but younger Miletians, Anaximander and Anaximenes, also posited monistic underlying principles, namely apeiron (the indefinite or boundless) and air respectively.

Another school was the Eleatics, in southern Italy. The group was founded in the early fifth century BCE by Parmenides, and included Zeno of Elea and Melissus of Samos. Methodologically, the Eleatics were broadly rationalist, and took logical standards of clarity and necessity to be the criteria of truth. Parmenides' chief doctrine was that reality is a single unchanging and universal Being. Zeno used "reductio ad absurdum", to demonstrate the illusory nature of change and time in his paradoxes.

Heraclitus of Ephesus, in contrast, made change central, teaching that "all things flow". His philosophy, expressed in brief aphorisms, is quite cryptic. For instance, he also taught the unity of opposites.

Democritus and his teacher Leucippus, are known for formulating an atomic theory for the cosmos. They are considered forerunners of the scientific method.

Metaphysics in Chinese philosophy can be traced back to the earliest Chinese philosophical concepts from the Zhou Dynasty such as Tian (Heaven) and Yin and Yang. The fourth century BCE saw a turn towards cosmogony with the rise of Taoism (in the Daodejing and Zhuangzi) and sees the natural world as dynamic and constantly changing processes which spontaneously arise from a single immanent metaphysical source or principle (Tao). Another philosophical school which arose around this time was the School of Naturalists which saw the ultimate metaphysical principle as the Taiji, the "supreme polarity" composed of the forces of Ying and Yang which were always in a state of change seeking balance. Another concern of Chinese metaphysics, especially Taoism, is the relationship and nature of Being and non-Being (you 有 and wu 無). The Taoists held that the ultimate, the Tao, was also non-being or no-presence. Other important concepts were those of spontaneous generation or natural vitality (Ziran) and "correlative resonance" (Ganying).

After the fall of the Han Dynasty (220 CE), China saw the rise of the Neo-Taoist Xuanxue school. This school was very influential in developing the concepts of later Chinese metaphysics. Buddhist philosophy entered China (c 1st century) and was influenced by the native Chinese metaphysical concepts to develop new theories. The native Tiantai and Huayen schools of philosophy maintained and reinterpreted the Indian theories of shunyata (emptiness, kong 空) and Buddha-nature (Fo xing 佛性) into the theory of interpenetration of phenomena. Neo-Confucians like Zhang Zai under the influence of other schools developed the concepts of "principle" (li) and vital energy (qi).

Socrates is known for his dialectic or questioning approach to philosophy rather than a positive metaphysical doctrine.

His pupil, Plato is famous for his theory of forms (which he places in the mouth of Socrates in his dialogues). Platonic realism (also considered a form of idealism) is considered to be a solution to the problem of universals; i.e., what particular objects have in common is that they share a specific Form which is universal to all others of their respective kind.

The theory has a number of other aspects:

Platonism developed into Neoplatonism, a philosophy with a monotheistic and mystical flavour that survived well into the early Christian era.

Plato's pupil Aristotle wrote widely on almost every subject, including metaphysics. His solution to the problem of universals contrasts with Plato's. Whereas Platonic Forms are existentially apparent in the visible world, Aristotelian essences dwell in particulars.

Potentiality and Actuality are principles of a dichotomy which Aristotle used throughout his philosophical works to analyze motion, causality and other issues.

The Aristotelian theory of change and causality stretches to four causes: the material, formal, efficient and final. The efficient cause corresponds to what is now known as a cause "simpliciter". Final causes are explicitly teleological, a concept now regarded as controversial in science. The Matter/Form dichotomy was to become highly influential in later philosophy as the substance/essence distinction.

The opening arguments in Aristotle's "Metaphysics", Book I, revolve around the senses, knowledge, experience, theory, and wisdom. The first main focus in the Metaphysics is attempting to determine how intellect "advances from sensation through memory, experience, and art, to theoretical knowledge". Aristotle claims that eyesight provides us with the capability to recognize and remember experiences, while sound allows us to learn.

"More on Indian philosophy: Hindu philosophy"

"Sāṃkhya" is an ancient system of Indian philosophy based on a dualism involving the ultimate principles of consciousness and matter. It is described as the rationalist school of Indian philosophy. It is most related to the Yoga school of Hinduism, and its method was most influential on the development of Early Buddhism.

The Sāmkhya is an enumerationist philosophy whose epistemology accepts three of six pramanas (proofs) as the only reliable means of gaining knowledge. These include "pratyakṣa" (perception), "anumāṇa" (inference) and "śabda" ("āptavacana", word/testimony of reliable sources).

Samkhya is strongly dualist. Sāmkhya philosophy regards the universe as consisting of two realities; puruṣa (consciousness) and prakṛti (matter). Jiva (a living being) is that state in which puruṣa is bonded to prakṛti in some form. This fusion, state the Samkhya scholars, led to the emergence of "buddhi" ("spiritual awareness") and "ahaṅkāra" (ego consciousness). The universe is described by this school as one created by purusa-prakṛti entities infused with various permutations and combinations of variously enumerated elements, senses, feelings, activity and mind. During the state of imbalance, one of more constituents overwhelm the others, creating a form of bondage, particularly of the mind. The end of this imbalance, bondage is called liberation, or moksha, by the Samkhya school.

The existence of God or supreme being is not directly asserted, nor considered relevant by the Samkhya philosophers. Sāṃkhya denies the final cause of Ishvara (God). While the Samkhya school considers the Vedas as a reliable source of knowledge, it is an atheistic philosophy according to Paul Deussen and other scholars. A key difference between Samkhya and Yoga schools, state scholars, is that Yoga school accepts a "personal, yet essentially inactive, deity" or "personal god".

Samkhya is known for its theory of guṇas (qualities, innate tendencies). Guṇa, it states, are of three types: "sattva" being good, compassionate, illuminating, positive, and constructive; "rajas" is one of activity, chaotic, passion, impulsive, potentially good or bad; and "tamas" being the quality of darkness, ignorance, destructive, lethargic, negative. Everything, all life forms and human beings, state Samkhya scholars, have these three guṇas, but in different proportions. The interplay of these guṇas defines the character of someone or something, of nature and determines the progress of life. The Samkhya theory of guṇas was widely discussed, developed and refined by various schools of Indian philosophies, including Buddhism. Samkhya's philosophical treatises also influenced the development of various theories of Hindu ethics.

Realization of the nature of Self-identity is the principal object of the Vedanta system of Indian metaphysics. In the Upanishads, self-consciousness is not the first-person indexical self-awareness or the self-awareness which is self-reference without identification, and also not the self-consciousness which as a kind of desire is satisfied by another self-consciousness. It is Self-realisation; the realisation of the Self consisting of consciousness that leads all else.

The word "Self-consciousness" in the Upanishads means the knowledge about the existence and nature of Brahman. It means the consciousness of our own real being, the primary reality. Self-consciousness means Self-knowledge, the knowledge of Prajna i.e. of Prana which is Brahman. According to the Upanishads the Atman or Paramatman is phenomenally unknowable; it is the object of realisation. The Atman is unknowable in its essential nature; it is unknowable in its essential nature because it is the eternal subject who knows about everything including itself. The Atman is the knower and also the known.

Metaphysicians regard the Self either to be distinct from the Absolute or entirely identical with the Absolute. They have given form to three schools of thought – a) the "Dualistic school", b) the "Quasi-dualistic school" and c) the "Monistic school", as the result of their varying mystical experiences. Prakrti and Atman, when treated as two separate and distinct aspects form the basis of the Dualism of the Shvetashvatara Upanishad. Quasi-dualism is reflected in the Vaishnavite-monotheism of Ramanuja and the absolute Monism, in the teachings of Adi Shankara.

Self-consciousness is the Fourth state of consciousness or "Turiya", the first three being "Vaisvanara", "Taijasa" and "Prajna". These are the four states of individual consciousness.

There are three distinct stages leading to Self-realisation. The First stage is in mystically apprehending the glory of the Self within us as though we were distinct from it. The Second stage is in identifying the "I-within" with the Self, that we are in essential nature entirely identical with the pure Self. The Third stage is in realising that the Atman is Brahman, that there is no difference between the Self and the Absolute. The Fourth stage is in realising "I am the Absolute" - "Aham Brahman Asmi". The Fifth stage is in realising that Brahman is the "All" that exists, as also that which does not exist.

In Buddhist philosophy there are various metaphysical traditions that have proposed different questions about the nature of reality based on the teachings of the Buddha in the early Buddhist texts. The Buddha of the early texts does not focus on metaphysical questions but on ethical and spiritual training and in some cases, he dismisses certain metaphysical questions as unhelpful and indeterminate Avyakta, which he recommends should be set aside. The development of systematic metaphysics arose after the Buddha's death with the rise of the Abhidharma traditions. The Buddhist Abhidharma schools developed their analysis of reality based on the concept of "dharmas" which are the ultimate physical and mental events that make up experience and their relations to each other. Noa Ronkin has called their approach "phenomenological".

Later philosophical traditions include the Madhyamika school of Nagarjuna, which further developed the theory of the emptiness (shunyata) of all phenomena or dharmas which rejects any kind of substance. This has been interpreted as a form of anti-foundationalism and anti-realism which sees reality has having no ultimate essence or ground. The Yogacara school meanwhile promoted a theory called "awareness only" (vijnapti-matra) which has been interpreted as a form of Idealism or Phenomenology and denies the split between awareness itself and the objects of awareness.

Islamic metaphysics was highly active during Europe's 'Dark Ages', beginning with the arrival and translation of Aristotle into Arabic.

Between about 1100 and 1500, philosophy as a discipline took place as part of the Catholic church's teaching system, known as scholasticism. Scholastic philosophy took place within an established
framework blending Christian theology with Aristotelian teachings. Although fundamental orthodoxies were not commonly challenged, there were nonetheless deep metaphysical disagreements, particularly over the problem of universals, which engaged Duns Scotus and Pierre Abelard. William of Ockham is remembered for his principle of ontological parsimony.

In the early modern period (17th and 18th centuries), the system-building "scope" of philosophy is often linked to the rationalist "method" of philosophy, that is the technique of deducing the nature of the world by pure reason. The scholastic concepts of substance and accident were employed.

British empiricism marked something of a reaction to rationalist and system-building metaphysics, or "speculative" metaphysics as it was pejoratively termed. The sceptic David Hume famously declared that most metaphysics should be consigned to the flames (see below). Hume was notorious among his contemporaries as one of the first philosophers to openly doubt religion, but is better known now for his critique of causality. John Stuart Mill, Thomas Reid and John Locke were less sceptical, embracing a more cautious style of metaphysics based on realism, common sense and science. Other philosophers, notably George Berkeley were led from empiricism to idealistic metaphysics.

Christian Wolff had theoretical philosophy divided into an ontology or "philosophia prima" as a general metaphysics, which arises as a preliminary to the distinction of the three "special metaphysics" on the soul, world and God: rational psychology, rational cosmology and rational theology. The three disciplines are called empirical and rational because they are independent of revelation. This scheme, which is the counterpart of religious tripartition in creature, creation, and Creator, is best known to philosophical students by Kant's treatment of it in the "Critique of Pure Reason". In the "Preface" of the 2nd edition of Kant's book, Wolff is defined "the greatest of all dogmatic philosophers."

Immanuel Kant attempted a grand synthesis and revision of the trends already mentioned: scholastic philosophy, systematic metaphysics, and skeptical empiricism, not to forget the burgeoning science of his day. As did the systems builders, he had an overarching framework in which all questions were to be addressed. Like Hume, who famously woke him from his 'dogmatic slumbers', he was suspicious of metaphysical speculation, and also places much emphasis on the limitations of the human mind.
Kant described his shift in metaphysics away from making claims about an objective noumenal world, towards exploring the subjective phenomenal world, as a Copernican Revolution, by analogy to (though opposite in direction to) Copernicus' shift from man (the subject) to the sun (an object) at the center of the universe.

Kant saw rationalist philosophers as aiming for a kind of metaphysical knowledge he defined as the "synthetic apriori"—that is knowledge that does not come from the senses (it is a priori) but is nonetheless about reality (synthetic). Inasmuch as it is about reality, it differs from abstract mathematical propositions (which he terms analytical apriori), and being apriori it is distinct from empirical, scientific knowledge (which he terms synthetic aposteriori). The only synthetic apriori knowledge we can have is of how our minds organise the data of the senses; that organising framework is space and time, which for Kant have no mind-independent existence, but nonetheless operate uniformly in all humans. Apriori knowledge of space and time is all that remains of metaphysics as traditionally conceived. There "is" a reality beyond sensory data or phenomena, which he calls the realm of noumena; however, we cannot know it as it is in itself, but only as it appears to us. He allows himself to speculate that the origins of phenomenal God, morality, and free will "might" exist in the noumenal realm, but these possibilities have to be set against its basic unknowability for humans. Although he saw himself as having disposed of metaphysics, in a sense, he has generally been regarded in retrospect as having a metaphysics of his own, and as beginning the modern analytical conception of the subject.

Nineteenth century philosophy was overwhelmingly influenced by Kant and his successors. Schopenhauer, Schelling, Fichte and Hegel all purveyed their own panoramic versions of German Idealism, Kant's own caution about metaphysical speculation, and refutation of idealism, having fallen by the wayside. The idealistic impulse continued into the early twentieth century with British idealists such as F. H. Bradley and J. M. E. McTaggart. Followers of Karl Marx took Hegel's dialectic view of history and re-fashioned it as materialism.

During the period when idealism was dominant in philosophy, science had been making great advances. The arrival of a new generation of scientifically minded philosophers led to a sharp decline in the popularity of idealism during the 1920s.

Analytical philosophy was spearheaded by Bertrand Russell and G. E. Moore. Russell and William James tried to compromise between idealism and materialism with the theory of neutral monism.

The early to mid twentieth century philosophy saw a trend to reject metaphysical questions as meaningless. The driving force behind this tendency was the philosophy of logical positivism as espoused by the Vienna Circle, which argued that the meaning of a statement was its prediction of observable results of an experiment, and thus that there is no need to postulate the existence of any objects other than these perceptual observations.

At around the same time, the American pragmatists were steering a middle course between materialism and idealism.
System-building metaphysics, with a fresh inspiration from science, was revived by A. N. Whitehead and Charles Hartshorne.

The forces that shaped analytical philosophy—the break with idealism, and the influence of science—were much less significant outside the English speaking world, although there was a shared turn toward language. Continental philosophy continued in a trajectory from post Kantianism.

The phenomenology of Husserl and others was intended as a collaborative project for the investigation of the features and structure of consciousness common to all humans, in line with Kant's basing his synthetic apriori on the uniform operation of consciousness. It was officially neutral with regards to ontology, but was nonetheless to spawn a number of metaphysical systems. Brentano's concept of intentionality would become widely influential, including on analytical philosophy.

Heidegger, author of "Being and Time", saw himself as re-focusing on Being-qua-being, introducing the novel concept of "Dasein" in the process. Classing himself an existentialist, Sartre wrote an extensive study of "Being and Nothingness".

The speculative realism movement marks a return to full blooded realism.

There are two fundamental aspects of everyday experience: change and persistence. Until recently, the Western philosophical tradition has arguably championed substance and persistence, with some notable exceptions, however. According to process thinkers, novelty, flux and accident do matter, and sometimes they constitute the ultimate reality.

In a broad sense, process metaphysics is as old as Western philosophy, with figures such as Heraclitus, Plotinus, Duns Scotus, Leibniz, David Hume, Georg Wilhelm Friedrich Hegel, Friedrich Wilhelm Joseph von Schelling, Gustav Theodor Fechner, Friedrich Adolf Trendelenburg, Charles Renouvier, Karl Marx, Ernst Mach, Friedrich Wilhelm Nietzsche, Émile Boutroux, Henri Bergson, Samuel Alexander and Nicolas Berdyaev. It seemingly remains an open question whether major "Continental" figures such as the late Martin Heidegger, Maurice Merleau-Ponty, Gilles Deleuze, Michel Foucault, or Jacques Derrida should be included.

In a strict sense, process metaphysics may be limited to the works of a few founding fathers: G. W. F. Hegel, Charles Sanders Peirce, William James, Henri Bergson, A. N. Whitehead, and John Dewey. From a European perspective, there was a very significant and early Whiteheadian influence on the works of outstanding scholars such as Émile Meyerson (1859–1933), Louis Couturat (1868–1914), Jean Wahl (1888–1974), Robin George Collingwood (1889–1943), Philippe Devaux (1902–1979), Hans Jonas (1903–1993), Dorothy M. Emmett (1904–2000), Maurice Merleau Ponty (1908–1961), Enzo Paci (1911–1976), Charlie Dunbar Broad (1887–1971), Wolfe Mays (1912–), Ilya Prigogine (1917–2003), Jules Vuillemin (1920–2001), Jean Ladrière (1921–), Gilles Deleuze (1925–1995), Wolfhart Pannenberg (1928–), and Reiner Wiehl (1929–2010).

While early analytic philosophy tended to reject metaphysical theorizing, under the influence of logical positivism, it was revived in the second half of the twentieth century. Philosophers such as David K. Lewis and David Armstrong developed elaborate theories on a range of topics such as universals, causation, possibility and necessity and abstract objects. However, the focus of analytical philosophy generally is away from the construction of all-encompassing systems and toward close analysis of individual ideas.

Among the developments that led to the revival of metaphysical theorizing were Quine's attack on the analytic–synthetic distinction, which was generally taken to undermine Carnap's distinction between existence questions internal to a framework and those external to it.

The philosophy of fiction, the problem of empty names, and the debate over existence's status as a property have all come of relative obscurity into the limelight, while perennial issues such as free will, possible worlds, and the philosophy of time have had new life breathed into them.

The analytic view is of metaphysics as studying phenomenal human concepts rather than making claims about the noumenal world, so its style often blurs into philosophy of language and introspective psychology. Compared to system-building, it can seem very dry, stylistically similar to computer programming, mathematics or even accountancy (as a common stated goal is to "account for" entities in the world). Despite, or perhaps because of, this scientific dryness, it is generally regarded as having made "progress" where other schools have not. For example, concepts from analytical metaphysics are now routinely employed and cited as useful guides in computational ontologies for databases and to frame computer natural language processing and knowledge representation software.






</doc>
<doc id="18896" url="https://en.wikipedia.org/wiki?curid=18896" title="Human spaceflight">
Human spaceflight

Human spaceflight (also referred to as crewed spaceflight or manned spaceflight) is space travel with a crew or passengers aboard the spacecraft. Spacecraft carrying people may be operated directly, by human crew, or it may be either remotely operated from ground stations on Earth or be autonomous, able to carry out a specific mission with no human involvement.

The first human spaceflight was launched by the Soviet Union on 12 April 1961 as a part of the Vostok program, with cosmonaut Yuri Gagarin aboard. Humans have been continuously present in space for on the International Space Station. All early human spaceflight was crewed, where at least some of the passengers acted to carry out tasks of piloting or operating the spacecraft. After 2015, several human-capable spacecraft are being explicitly designed with the ability to operate autonomously.

Since the retirement of the US Space Shuttle in 2011, only Russia and China have maintained human spaceflight capability with the Soyuz program and Shenzhou program. Currently, all expeditions to the International Space Station use Soyuz vehicles, which remain attached to the station to allow quick return if needed. The United States is developing commercial crew transportation to facilitate domestic access to ISS and low Earth orbit, as well as the Orion vehicle for beyond-low Earth orbit applications.

While spaceflight has typically been a government-directed activity, commercial spaceflight has gradually been taking on a greater role. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight, and a number of non-governmental companies have been working to develop a space tourism industry. NASA has also played a role to stimulate private spaceflight through programs such as Commercial Orbital Transportation Services (COTS) and Commercial Crew Development (CCDev). With its 2011 budget proposals released in 2010, the Obama administration moved towards a model where commercial companies would supply NASA with transportation services of both people and cargo transport to low Earth orbit. The vehicles used for these services could then serve both NASA and potential commercial customers. Commercial resupply of ISS began two years after the retirement of the Shuttle, and commercial crew launches could begin by 2018.

Human spaceflight capability was first developed during the Cold War between the United States and the Soviet Union (USSR), which developed the first intercontinental ballistic missile rockets to deliver nuclear weapons. These rockets were large enough to be adapted to carry the first artificial satellites into low Earth orbit. After the first satellites were launched in 1957 and 1958, the US worked on Project Mercury to launch men singly into orbit, while the USSR secretly pursued the Vostok program to accomplish the same thing. The USSR launched the first human in space, Yuri Gagarin, into a single orbit in Vostok 1 on a Vostok 3KA rocket, on 12 April 1961. The US launched its first astronaut, Alan Shepard, on a suborbital flight aboard "Freedom 7" on a Mercury-Redstone rocket, on 5 May 1961. Unlike Gagarin, Shepard manually controlled his spacecraft's attitude, and landed inside it. The first American in orbit was John Glenn aboard "Friendship 7", launched 20 February 1962 on a Mercury-Atlas rocket. The USSR launched five more cosmonauts in Vostok capsules, including the first woman in space, Valentina Tereshkova aboard Vostok 6 on 16 June 1963. The US launched a total of two astronauts in suborbital flight and four into orbit through 1963.

US President John F. Kennedy raised the stakes of the Space Race by setting the goal of landing a man on the Moon and returning him safely by the end of the 1960s. The US started the three-man Apollo program in 1961 to accomplish this, launched by the Saturn family of launch vehicles, and the interim two-man Project Gemini in 1962, which flew 10 missions launched by Titan II rockets in 1965 and 1966. Gemini's objective was to support Apollo by developing American orbital spaceflight experience and techniques to be used in the Moon mission.

Meanwhile, the USSR remained silent about their intentions to send humans to the Moon, and proceeded to stretch the limits of their single-pilot Vostok capsule into a two- or three-person Voskhod capsule to compete with Gemini. They were able to launch two orbital flights in 1964 and 1965 and achieved the first spacewalk, made by Alexei Leonov on Voskhod 2 on 8 March 1965. But Voskhod did not have Gemini's capability to maneuver in orbit, and the program was terminated. The US Gemini flights did not accomplish the first spacewalk, but overcame the early Soviet lead by performing several spacewalks and solving the problem of astronaut fatigue caused by overcoming the lack of gravity, demonstrating up to two weeks endurance in a human spaceflight, and the first space rendezvous and dockings of spacecraft.

The US succeeded in developing the Saturn V rocket necessary to send the Apollo spacecraft to the Moon, and sent Frank Borman, James Lovell, and William Anders into 10 orbits around the Moon in Apollo 8 in December 1968. In July 1969, Apollo 11 accomplished Kennedy's goal by landing Neil Armstrong and Buzz Aldrin on the Moon 21 July and returning them safely on 24 July along with Command Module pilot Michael Collins. A total of six Apollo missions landed 12 men to walk on the Moon through 1972, half of which drove electric powered vehicles on the surface. The crew of Apollo 13, Lovell, Jack Swigert, and Fred Haise, survived a catastrophic in-flight spacecraft failure and returned to Earth safely without landing on the Moon.
Meanwhile, the USSR secretly pursued human lunar orbiting and landing programs. They successfully developed the three-person Soyuz spacecraft for use in the lunar programs, but failed to develop the N1 rocket necessary for a human landing, and discontinued the lunar programs in 1974. On losing the Moon race, they concentrated on the development of space stations, using the Soyuz as a ferry to take cosmonauts to and from the stations. They started with a series of Salyut sortie stations from 1971 to 1986.

After the Apollo program, the US launched the Skylab sortie space station in 1973, manning it for 171 days with three crews aboard Apollo spacecraft. President Richard Nixon and Soviet Premier Leonid Brezhnev negotiated an easing of relations known as détente, an easing of Cold War tensions. As part of this, they negotiated the Apollo-Soyuz Test Project, in which an Apollo spacecraft carrying a special docking adapter module rendezvoused and docked with Soyuz 19 in 1975. The American and Russian crews shook hands in space, but the purpose of the flight was purely diplomatic and symbolic.
Nixon appointed his Vice President Spiro Agnew to head a Space Task Group in 1969 to recommend follow-on human spaceflight programs after Apollo. The group proposed an ambitious Space Transportation System based on a reusable Space Shuttle which consisted of a winged, internally fueled orbiter stage burning liquid hydrogen, launched by a similar, but larger kerosene-fueled booster stage, each equipped with airbreathing jet engines for powered return to a runway at the Kennedy Space Center launch site. Other components of the system included a permanent modular space station, reusable space tug and nuclear interplanetary ferry, leading to a human expedition to Mars as early as 1986, or as late as 2000, depending on the level of funding allocated. However, Nixon knew the American political climate would not support Congressional funding for such an ambition, and killed proposals for all but the Shuttle, possibly to be followed by the space station. Plans for the Shuttle were scaled back to reduce development risk, cost, and time, replacing the piloted flyback booster with two reusable solid rocket boosters, and the smaller orbiter would use an expendable external propellant tank to feed its hydrogen-fueled main engines. The orbiter would have to make unpowered landings.
The two nations continued to compete rather than cooperate in space, as the US turned to developing the Space Shuttle and planning the space station, dubbed "Freedom". 
The USSR launched three Almaz military sortie stations from 1973 to 1977, disguised as Salyuts. They followed Salyut with the development of "Mir", the first modular, semi-permanent space station, the construction of which took place from 1986 to 1996. "Mir" orbited at an altitude of , at a 51.6° inclination. It was occupied for 4,592 days, and made a controlled reentry in 2001.
The Space Shuttle started flying in 1981, but the US Congress failed to approve sufficient funds to make "Freedom" a reality. A fleet of four shuttles was built: "Columbia", "Challenger", "Discovery", and "Atlantis". A fifth shuttle, "Endeavour", was built to replace "Challenger", which was destroyed in an accident during launch that killed 7 astronauts on 28 January 1986. Twenty-two Shuttle flights carried a European Space Agency sortie space station called Spacelab in the payload bay from 1983 to 1998.

The USSR copied the reusable Space Shuttle orbiter, which it called "Buran". It was designed to be launched into orbit by the expendable Energia rocket, and capable of robotic orbital flight and landing. Unlike the US Shuttle, "Buran" had no main rocket engines, but like the Shuttle used its orbital maneuvering engines to perform its final orbital insertion. A single unmanned orbital test flight was successfully made in November 1988. A second test flight was planned by 1993, but the program was cancelled due to lack of funding and the dissolution of the Soviet Union in 1991. Two more orbiters were never completed, and the first one was destroyed in a hangar roof collapse in May 2002.

The dissolution of the Soviet Union in 1991 brought an end to the Cold War and opened the door to true cooperation between the US and Russia. The Soviet Soyuz and Mir programs were taken over by the Russian Federal Space Agency, now known as the Roscosmos State Corporation. The Shuttle-Mir Program included American Space Shuttles visiting the "Mir" space station, Russian cosmonauts flying on the Shuttle, and an American astronaut flying aboard a Soyuz spacecraft for long-duration expeditions aboard "Mir".

In 1993, President Bill Clinton secured Russia's cooperation in converting the planned Space Station "Freedom" into the International Space Station (ISS). Construction of the station began in 1998. The station orbits at an altitude of and an inclination of 51.65°.

The Space Shuttle was retired in 2011 after 135 orbital flights, several of which helped assemble, supply, and crew the ISS. "Columbia" was destroyed in another accident during reentry, which killed 7 astronauts on 1 February 2003.

After Russia's launch of Sputnik 1 in 1957, Chairman Mao Zedong intended to place a Chinese satellite in orbit by 1959 to celebrate the 10th anniversary of the founding of the People's Republic of China (PRC), However, China did not successfully launch its first satellite until 24 April 1970. Mao and Premier Zhou Enlai decided on 14 July 1967, that the PRC should not be left behind, and started China's own human spaceflight program. The first attempt, the Shuguang spacecraft copied from the US Gemini, was cancelled on 13 May 1972.
China later designed the Shenzhou spacecraft resembling the Russian Soyuz, and became the third nation to achieve independent human spaceflight capability by launching Yang Liwei on a 21-hour flight aboard Shenzhou 5 on 15 October 2003. China launched the Tiangong-1 space station on 29 September 2011, and two sortie missions to it: Shenzhou 9 16–29 June 2012, with China's first female astronaut Liu Yang; and Shenzhou 10, 13–26 June 2013. The station was retired on 21 March 2016 and remains in a , 42.77° inclination orbit.

The European Space Agency began development in 1987 of the Hermes spaceplane, to be launched on the Ariane 5 expendable launch vehicle. The project was cancelled in 1992, when it became clear that neither cost nor performance goals could be achieved. No Hermes shuttles were ever built.

Japan began development in the 1980s of the HOPE-X experimental spaceplane, to be launched on its H-IIA expendable launch vehicle. A string of failures in 1998 led to funding reduction, and the project's cancellation in 2003.

Under the Bush administration, the Constellation Program included plans for retiring the Shuttle program and replacing it with the capability for spaceflight beyond low Earth orbit. In the 2011 United States federal budget, the Obama administration cancelled Constellation for being over budget and behind schedule while not innovating and investing in critical new technologies. For beyond low Earth orbit human spaceflight NASA is developing the Orion spacecraft to be launched by the Space Launch System. Under the Commercial Crew Development plan, NASA will rely on transportation services provided by the private sector to reach low Earth orbit, such as SpaceX's Falcon 9/Dragon V2, Sierra Nevada Corporation's Dream Chaser, or Boeing's CST-100. The period between the retirement of the shuttle in 2011 and the initial operational capability of new systems in 2017, similar to the gap between the end of Apollo in 1975 and the first space shuttle flight in 1981, is referred to by a presidential Blue Ribbon Committee as the U.S. human spaceflight gap.

Since the early 2000s, a variety of private spaceflight ventures have been undertaken. Several of the companies, including Blue Origin, SpaceX, Virgin Galactic, and Sierra Nevada have explicit plans to advance human spaceflight. , all four of those companies have development programs underway to fly commercial passengers.

A commercial suborbital spacecraft aimed at the space tourism market is being developed by Virgin Galactic called SpaceshipTwo, and could reach space around 2018.
Blue Origin has begun a multi-year test program of their New Shepard vehicle and carried out six successful uncrewed test flights in 2015–2016. Blue Origin plan to fly "test passengers" in Q2 2017, and initiate commercial flights in 2018.

SpaceX and Boeing are both developing passenger-capable orbital space capsules as of 2015, planning to fly NASA astronauts to the International Space Station by 2018. SpaceX will be carrying passengers on Dragon 2 launched on a Falcon 9 launch vehicle. Boeing will be doing it with their CST-100 launched on a United Launch Alliance Atlas V launch vehicle.
Development funding for these orbital-capable technologies has been provided by a mix of government and private funds, with SpaceX providing a greater portion of total development funding for this human-carrying capability from private investment.
There have been no public announcements of commercial offerings for orbital flights from either company, although both companies are planning some flights with their own private, not NASA, astronauts on board.

Yuri Gagarin became the first human to orbit the Earth on April 12, 1961.

John Glenn became the first American to orbit the Earth on February 20, 1962.

Alan Shepard became the first American to reach space on Mercury-Redstone 3 on May 5, 1961.

Valentina Tereshkova became the first woman to orbit the Earth on June 16, 1963.

Joseph A. Walker became the first human to pilot a spaceplane, the X-15 Flight 90, into space on July 19, 1963.

Alexey Leonov became the first human to leave a spacecraft in orbit on March 18, 1965.

Frank Borman, Jim Lovell, and William Anders became the first humans to travel beyond low Earth orbit (LEO) Dec 21–27, 1968, when the Apollo 8 mission took them to 10 orbits around the Moon and back.

Neil Armstrong and Buzz Aldrin became the first humans to land on the Moon on July 20, 1969.

Svetlana Savitskaya became the first woman to walk in space on July 25, 1984.

Sally Ride became the first American woman in space in 1983. Eileen Collins was the first female shuttle pilot, and with shuttle mission STS-93 in 1999 she became the first woman to command a U.S. spacecraft.

The longest single human spaceflight is that of Valeri Polyakov, who left Earth on 8 January 1994, and did not return until 22 March 1995 (a total of 437 days 17 h 58 min 16 s). Sergei Krikalyov has spent the most time of anyone in space, 803 days, 9 hours, and 39 minutes altogether. The longest period of continuous human presence in space is on the International Space Station, exceeding the previous record of almost 10 years (or 3,634 days) held by Mir, spanning the launch of Soyuz TM-8 on 5 September 1989 to the landing of Soyuz TM-29 on 28 August 1999.

Yang Liwei became the first human to orbit the Earth as part of the Chinese manned space program on October 15, 2003.

For many years, only the USSR (later Russia) and the United States had their own astronauts. Citizens of other nations flew in space, beginning with the flight of Vladimir Remek, a Czech, on a Soviet spacecraft on 2 March 1978, in the Interkosmos programme. , citizens from 38 nations (including space tourists) have flown in space aboard Soviet, American, Russian, and Chinese spacecraft.

Human spaceflight programs have been conducted by the former Soviet Union and current Russian Federation, the United States, the People's Republic of China and by private spaceflight company Scaled Composites.
Space vehicles are spacecraft used for transportation between the Earth's surface and outer space, or between locations in outer space. The following space vehicles and spaceports are currently used for launching human spaceflights:

The following space stations are currently maintained in Earth orbit for human occupation:

Numerous private companies attempted human spaceflight programs in an effort to win the $10 million Ansari X Prize. The first private human spaceflight took place on 21 June 2004, when SpaceShipOne conducted a suborbital flight. SpaceShipOne captured the prize on 4 October 2004, when it accomplished two consecutive flights within one week. SpaceShipTwo, launching from the carrier aircraft White Knight Two, is planned to conduct regular suborbital space tourism.

Most of the time, the only humans in space are those aboard the ISS, whose crew of six spends up to six months at a time in low Earth orbit.

NASA and ESA use the term "human spaceflight" to refer to their programs of launching people into space. These endeavors have also been referred to as "manned space missions," though because of gender specificity this is no longer official parlance according to NASA style guides.

NASA is developing a plan to land humans on Mars by the 2030s. The first step in this mission begins sometime during 2020, when NASA plans to send an uncrewed craft into deep space to retrieve an asteroid. The asteroid will be pushed into the moon’s orbit, and studied by astronauts aboard Orion, NASA’s first human spacecraft in a generation. Orion’s crew will return to Earth with samples of the asteroid and their collected data. In addition to broadening America’s space capabilities, this mission will test newly developed technology, such as solar electric propulsion, which uses solar arrays for energy and requires ten times less propellant than the conventional chemical counterpart used for powering space shuttles to orbit.

Several other countries and space agencies have announced and begun human spaceflight programs by their own technology, Japan (JAXA), Iran (ISA) and Malaysia (MNSA).

The Indian Space Research Organisation (ISRO) has begun work on pre-project activities of a human space flight mission program. The objective is to carry a crew of two to Low Earth Orbit (LEO) and return them safely to a predefined destination on Earth. The program is proposed to be implemented in defined phases. Currently, the pre-project activities are progressing with a focus on the development of critical technologies for subsystems such as the Crew Module (CM), Environmental Control and Life Support System (ECLSS), Crew Escape System, etc. The department has initiated pre-project activities to study technical and managerial issues related to crewed missions. The program envisages the development of a fully autonomous orbital vehicle carrying 2 or 3 crew members to about 300 km low Earth orbit and their safe return.

A number of spacecraft have been proposed over the decades that might facilitate spaceliner passenger travel. Somewhat analogous to travel by airliner after the middle of the 20th century, these vehicles are proposed to transport a large number of passengers to destinations in space, or to destinations on Earth which travel through space. To date, none of these concepts have been built, although a few vehicles that carry fewer than 10 persons are currently in the flight testing phase of their development process.

One large spaceliner concept currently in early development is the SpaceX BFR which, in addition to replacing the Falcon 9 and Falcon Heavy launch vehicles in the legacy Earth-orbit market after 2020, has been proposed by SpaceX for long-distance commercial travel on Earth. This is to transport people on point-to-point suborbital flights between two points on Earth in under one hour, also known as "Earth-to-Earth," and carrying 100+ passengers.

Small spaceplane or small capsule suborbital spacecraft have been under development for the past decade or so and, , at least one of each type are under development. Both Virgin Galactic and Blue Origin are in active development, with the SpaceShipTwo spaceplane and the New Shepard capsule, respectively. Both would carry approximately a half-dozen passengers up to space for a brief time of zero gravity before returning to the same location from where the trip began. XCOR Aerospace had been developing the Lynx single-passenger spaceplane since the 2000s but development was halted in 2017.

There are two main sources of hazard in space flight: those due to the "environment" of space which make it hostile to the human body, and the potential for "mechanical" malfunctions of the equipment required to accomplish space flight.

Planners of human spaceflight missions face a number of safety concerns.

The immediate needs for breathable air and drinkable water are addressed by the life support system of the spacecraft.

Medical consequences such as possible blindness and bone loss have been associated with human space flight.

On 31 December 2012, a NASA-supported study reported that spaceflight may harm the brain of astronauts and accelerate the onset of Alzheimer's disease.

In October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, including a human mission to Mars.

On 2 November 2017, scientists reported that significant changes in the position and structure of the brain have been found in astronauts who have taken trips in space, based on MRI studies. Astronauts who took longer space trips were associated with greater brain changes.

Medical data from astronauts in low Earth orbits for long periods, dating back to the 1970s, show several adverse effects of a microgravity environment: loss of bone density, decreased muscle strength and endurance, postural instability, and reductions in aerobic capacity. Over time these deconditioning effects can impair astronauts’ performance or increase their risk of injury.

In a weightless environment, astronauts put almost no weight on the back muscles or leg muscles used for standing up, which causes them to weaken and get smaller. Astronauts can lose up to twenty per cent of their muscle mass on spaceflights lasting five to eleven days. The consequent loss of strength could be a serious problem in case of a landing emergency. Upon return to Earth from long-duration flights, astronauts are considerably weakened, and are not allowed to drive a car for twenty-one days.

Astronauts experiencing weightlessness will often lose their orientation, get motion sickness, and lose their sense of direction as their bodies try to get used to a weightless environment. When they get back to Earth, or any other mass with gravity, they have to readjust to the gravity and may have problems standing up, focusing their gaze, walking and turning. Importantly, those body motor disturbances after changing from different gravities only get worse the longer the exposure to little gravity. These changes will affect operational activities including approach and landing, docking, remote manipulation, and emergencies that may happen while landing. This can be a major roadblock to mission success.

In addition, after long space flight missions, male astronauts may experience severe eyesight problems. Such eyesight problems may be a major concern for future deep space flight missions, including a crewed mission to the planet Mars.

Without proper shielding, the crews of missions beyond low Earth orbit (LEO) might be at risk from high-energy protons emitted by solar flares. Lawrence Townsend of the University of Tennessee and others have studied the most powerful solar flare ever recorded. That flare was seen by the British astronomer Richard Carrington in September 1859. Radiation doses astronauts would receive from a Carrington-type flare could cause acute radiation sickness and possibly even death.

Another type of radiation, galactic cosmic rays, presents further challenges to human spaceflight beyond low Earth orbit.

There is also some scientific concern that extended spaceflight might slow down the body’s ability to protect itself against diseases. Some of the problems are a weakened immune system and the activation of dormant viruses in the body. Radiation can cause both short and long term consequences to the bone marrow stem cells which create the blood and immune systems. Because the interior of a spacecraft is so small, a weakened immune system and more active viruses in the body can lead to a fast spread of infection.

During long missions, astronauts are isolated and confined into small spaces. Depression, cabin fever and other psychological problems may impact the crew's safety and mission success.

Astronauts may not be able to quickly return to Earth or receive medical supplies, equipment or personnel if a medical emergency occurs. The astronauts may have to rely for long periods on their limited existing resources and medical advice from the ground.

Space flight requires much higher velocities than ground or air transportation, which in turn requires the use of high energy density propellants for launch, and the dissipation of large amounts of energy, usually as heat, for safe reentry through the Earth's atmosphere.

Since rockets carry the potential for fire or explosive destruction, space capsules generally employ some sort of launch escape system, consisting either of a tower-mounted solid fuel rocket to quickly carry the capsule away from the launch vehicle (employed on Mercury, Apollo, and Soyuz), or else ejection seats (employed on Vostok and Gemini) to carry astronauts out of the capsule and away for individual parachute landing. The escape tower is discarded at some point before the launch is complete, at a point where an abort can be performed using the spacecraft's engines.

Such a system is not always practical for multiple crew member vehicles (particularly spaceplanes), depending on location of egress hatch(es). When the single-hatch Vostok capsule was modified to become the 2 or 3-person Voskhod, the single-cosmonaut ejection seat could not be used, and no escape tower system was added. The two Voskhod flights in 1964 and 1965 avoided launch mishaps. The Space Shuttle carried ejection seats and escape hatches for its pilot and copilot in early flights, but these could not be used for passengers who sat below the flight deck on later flights, and so were discontinued.

The only in-flight launch abort of a crewed flight occurred on Soyuz 18a on 5 April 1975. The abort occurred after the launch escape system had been jettisoned, when the launch vehicle's spent second stage failed to separate before the third stage ignited. The vehicle strayed off course, and the crew separated the spacecraft and fired its engines to pull it away from the errant rocket. Both cosmonauts landed safely.

In the only use of a launch escape system on a crewed flight, the planned Soyuz T-10a launch on 26 September 1983 was aborted by a launch vehicle fire 90 seconds before liftoff. Both cosmonauts aboard landed safely.

The only crew fatality during launch occurred on 28 January 1986, when the Space Shuttle "Challenger" broke apart 73 seconds after liftoff, due to failure of a solid rocket booster seal which caused separation of the booster and failure of the external fuel tank, resulting in explosion of the fuel. All seven crew members were killed.

The single pilot of Soyuz 1, Vladimir Komarov was killed when his capsule's parachutes failed during an emergency landing on 24 April 1967, causing the capsule to crash.

The crew of seven aboard the Space Shuttle "Columbia" were killed on reentry after completing a successful mission in space on 1 February 2003. A wing leading edge reinforced carbon-carbon heat shield had been damaged by a piece of frozen external tank foam insulation which broke off and struck the wing during launch. Hot reentry gasses entered and destroyed the wing structure, leading to breakup of the orbiter vehicle.

There are two basic choices for an artificial atmosphere: either an Earth-like mixture of oxygen in an inert gas such as nitrogen or helium, or pure oxygen, which can be used at lower than standard atmospheric pressure. A nitrogen-oxygen mixture is used in the International Space Station and Soyuz spacecraft, while low-pressure pure oxygen is commonly used in space suits for extravehicular activity.

Use of a gas mixture carries risk of decompression sickness (commonly known as "the bends") when transitioning to or from the pure oxygen space suit environment. There have also been instances of injury and fatalities caused by suffocation in the presence of too much nitrogen and not enough oxygen.

A pure oxygen atmosphere carries risk of fire. The original design of the Apollo spacecraft used pure oxygen at greater than atmospheric pressure prior to launch. An electrical fire started in the cabin of Apollo 1 during a ground test at Cape Kennedy Air Force Station Launch Complex 34 on 27 January 1967, and spread rapidly. The high pressure (increased even higher by the fire) prevented removal of the plug door hatch cover in time to rescue the crew. All three, Gus Grissom, Ed White, and Roger Chaffee, were killed. This led NASA to use a nitrogen/oxygen atmosphere before launch, and low pressure pure oxygen only in space.

The March 1966 Gemini 8 mission was aborted in orbit when an attitude control system thruster stuck in the on position, sending the craft into a dangerous spin which threatened the lives of Neil Armstrong and David Scott. Armstrong had to shut the control system off and use the reentry control system to stop the spin. The craft made an emergency reentry and the astronauts landed safely. The most probable cause was determined to be an electrical short due to a static electricity discharge, which caused the thruster to remain powered even when switched off. The control system was modified to put each thruster on its own isolated circuit.

The third lunar landing expedition Apollo 13 in April 1970, was aborted and the lives of the crew, James Lovell, Jack Swigert and Fred Haise, were threatened by failure of a cryogenic liquid oxygen tank en route to the Moon. The tank burst when electrical power was applied to internal stirring fans in the tank, causing the immediate loss of all of its contents, and also damaging the second tank, causing the loss of its remaining oxygen in a span of 130 minutes. This in turn caused loss of electrical power provided by fuel cells to the command spacecraft. The crew managed to return to Earth safely by using the lunar landing craft as a "life boat". The tank failure was determined to be caused by two mistakes. The tank's drain fitting had been damaged when it was dropped during factory testing. This necessitated use of its internal heaters to boil out the oxygen after a pre-launch test, which in turn damaged the fan wiring's electrical insulation, because the thermostats on the heaters did not meet the required voltage rating due to a vendor miscommunication.

, 22 crew members have died in accidents aboard spacecraft. Over 100 others have died in accidents during activity directly related to spaceflight or testing.





</doc>
<doc id="18899" url="https://en.wikipedia.org/wiki?curid=18899" title="Mendelevium">
Mendelevium

Mendelevium is a synthetic element with chemical symbol Md (formerly Mv) and atomic number 101. A metallic radioactive transuranic element in the actinide series, it is the first element that currently cannot be produced in macroscopic quantities through neutron bombardment of lighter elements. It is the third-to-last actinide and the ninth transuranic element. It can only be produced in particle accelerators by bombarding lighter elements with charged particles. A total of sixteen mendelevium isotopes are known, the most stable being Md with a half-life of 51 days; nevertheless, the shorter-lived Md (half-life 1.17 hours) is most commonly used in chemistry because it can be produced on a larger scale.

Mendelevium was discovered by bombarding einsteinium with alpha particles in 1955, the same method still used to produce it today. It was named after Dmitri Mendeleev, father of the periodic table of the chemical elements. Using available microgram quantities of the isotope einsteinium-253, over a million mendelevium atoms may be produced each hour. The chemistry of mendelevium is typical for the late actinides, with a preponderance of the +3 oxidation state but also an accessible +2 oxidation state. Owing to the small amounts of produced mendelevium and all of its isotopes having relatively short half-lives, there are currently no uses for it outside basic scientific research.

Mendelevium was the ninth transuranic element to be synthesized. It was first synthesized by Albert Ghiorso, Glenn T. Seaborg, Gregory Robert Choppin, Bernard G. Harvey, and team leader Stanley G. Thompson in early 1955 at the University of California, Berkeley. The team produced Md (half-life of 77 minutes) when they bombarded an Es target consisting of only a billion (10) einsteinium atoms with alpha particles (helium nuclei) in the Berkeley Radiation Laboratory's 60-inch cyclotron, thus increasing the target's atomic number by two. Md thus became the first isotope of any element to be synthesized one atom at a time. In total, seventeen mendelevium atoms were produced. This discovery was part of a program, begun in 1952, that irradiated plutonium with neutrons to transmute it into heavier actinides. This method was necessary as the previous method used to synthesize transuranic elements, neutron capture, could not work because of a lack of known beta decaying isotopes of fermium that would produce isotopes of the next element, mendelevium, and also due to the very short half-life to spontaneous fission of Fm that thus constituted a hard limit to the success of the neutron capture process.

To predict if the production of mendelevium would be possible, the team made use of a rough calculation. The number of atoms that would be produced would be approximately equal to the product of the number of atoms of target material, the target's cross section, the ion beam intensity, and the time of bombardment; this last factor was related to the half-life of the product when bombarding for a time on the order of its half-life. This gave one atom per experiment. Thus under optimum conditions, the preparation of only one atom of element 101 per experiment could be expected. This calculation demonstrated that it was feasible to go ahead with the experiment. The target material, einsteinium-253, could be produced readily from irradiating plutonium: one year of irradiation would give a billion atoms, and its three-week half-life meant that the element 101 experiments could be conducted in one week after the produced einsteinium was separated and purified to make the target. However, it was necessary to upgrade the cyclotron to obtain the needed intensity of 10 alpha particles per second; Seaborg applied for the necessary funds.
While Seaborg applied for funding, Harvey worked on the einsteinium target, while Thomson and Choppin focused on methods for chemical isolation. Choppin suggested using α-hydroxyisobutyric acid to separate the mendelevium atoms from those of the lighter actinides. The actual synthesis was done by a recoil technique, introduced by Albert Ghiorso. In this technique, the einsteinium was placed on the opposite side of the target from the beam, so that the recoiling mendelevium atoms would get enough momentum to leave the target and be caught on a catcher foil made of gold. This recoil target was made by an electroplating technique, developed by Alfred Chetham-Strode. This technique gave a very high yield, which was absolutely necessary when working with such a rare and valuable product as the einsteinium target material. The recoil target consisted of 10 atoms of Es which were deposited electrolytically on a thin gold foil. It was bombarded by 41 MeV alpha particles in the Berkeley cyclotron with a very high beam density of 6×10 particles per second over an area of 0.05 cm. The target was cooled by water or liquid helium, and the foil could be replaced.

Initial experiments were carried out in September 1954. No alpha decay was seen from mendelevium atoms; thus, Ghiorso suggested that the mendelevium had all decayed by electron capture to fermium and that the experiment should be repeated to search instead for spontaneous fission events. The repetition of the experiment happened in February 1955.
On the day of discovery, 19 February, alpha irradiation of the einsteinium target occurred in three three-hour sessions. The cyclotron was in the University of California campus, while the Radiation Laboratory was on the next hill. To deal with this situation, a complex procedure was used: Ghiorso took the catcher foils (there were three targets and three foils) from the cyclotron to Harvey, who would use aqua regia to dissolve it and pass it through an anion-exchange resin column to separate out the transuranium elements from the gold and other products. The resultant drops entered a test tube, which Choppin and Ghiorso took in a car to get to the Radiation Laboratory as soon as possible. There Thompson and Choppin used a cation-exchange resin column and the α-hydroxyisobutyric acid. The solution drops were collected on platinum disks and dried under heat lamps. The three disks were expected to contain respectively the fermium, no new elements, and the mendelevium. Finally, they were placed in their own counters, which were connected to recorders such that spontaneous fission events would be recorded as huge deflections in a graph showing the number and time of the decays. There thus was no direct detection, but by observation of spontaneous fission events arising from its electron-capture daughter Fm. The first one was identified with a "hooray" followed by a "double hooray" and a "triple hooray". The fourth one eventually officially proved the chemical identification of the 101st element, mendelevium. In total, five decays were reported up till 4 a.m. Seaborg was notified and the team left to sleep. Additional analysis and further experimentation showed the produced mendelevium isotope to have mass 256 and to decay by electron capture to fermium-256 with a half-life of 1.5 h.

Being the first of the second hundred of the chemical elements, it was decided that the element would be named "mendelevium" after the Russian chemist Dmitri Mendeleev, father of the periodic table. Because this discovery came during the Cold War, Seaborg had to request permission of the government of the United States to propose that the element be named for a Russian, but it was granted. The name "mendelevium" was accepted by the International Union of Pure and Applied Chemistry (IUPAC) in 1955 with symbol "Mv", which was changed to "Md" in the next IUPAC General Assembly (Paris, 1957).

In the periodic table, mendelevium is located to the right of the actinide fermium, to the left of the actinide nobelium, and below the lanthanide thulium. Mendelevium metal has not yet been prepared in bulk quantities, and bulk preparation is currently impossible. Nevertheless, a number of predictions and some preliminary experimental results have been done regarding its properties.

The lanthanides and actinides, in the metallic state, can exist as either divalent (such as europium and ytterbium) or trivalent (most other lanthanides) metals. The former have fds configurations, whereas the latter have fs configurations. In 1975, Johansson and Rosengren examined the measured and predicted values for the cohesive energies (enthalpies of crystallization) of the metallic lanthanides and actinides, both as divalent and trivalent metals. The conclusion was that the increased binding energy of the [Rn]5f6d7s configuration over the [Rn]5f7s configuration for mendelevium was not enough to compensate for the energy needed to promote one 5f electron to 6d, as is true also for the very late actinides: thus einsteinium, fermium, mendelevium, and nobelium were expected to be divalent metals. The increasing predominance of the divalent state well before the actinide series concludes is attributed to the relativistic stabilization of the 5f electrons, which increases with increasing atomic number. Thermochromatographic studies with trace quantities of mendelevium by Zvara and Hübener from 1976 to 1982 confirmed this prediction. In 1990, Haire and Gibson estimated mendelevium metal to have an enthalpy of sublimation between 134 and 142 kJ/mol. Divalent mendelevium metal should have a metallic radius of around . Like the other divalent late actinides (except the once again trivalent lawrencium), metallic mendelevium should assume a face-centered cubic crystal structure. Mendelevium's melting point has been estimated at 827 °C, the same value as that predicted for the neighboring element nobelium. Its density is predicted to be around .

The chemistry of mendelevium is mostly known only in solution, in which it can take on the +3 or +2 oxidation states. The +1 state has also been reported, but has not yet been confirmed.

Before mendelevium's discovery, Seaborg and Katz predicted that it should be predominantly trivalent in aqueous solution and hence should behave similarly to other tripositive lanthanides and actinides. After the synthesis of mendelevium in 1955, these predictions were confirmed, first in the observation at its discovery that it eluted just after fermium in the trivalent actinide elution sequence from a cation-exchange column of resin, and later the 1967 observation that mendelevium could form insoluble hydroxides and fluorides that coprecipitated with trivalent lanthanide salts. Cation-exchange and solvent extraction studies led to the conclusion that mendelevium was a trivalent actinide with an ionic radius somewhat smaller than that of the previous actinide, fermium. Mendelevium can form coordination complexes with 1,2-cyclohexanedinitrilotetraacetic acid (DCTA).

In reducing conditions, mendelevium(III) can be easily reduced to mendelevium(II), which is stable in aqueous solution. The standard reduction potential of the "E"°(Md→Md) couple was variously estimated in 1967 as −0.10 V or −0.20 V: later 2013 experiments established the value as . In comparison, "E"°(Md→Md) should be around −1.74 V, and "E"°(Md→Md) should be around −2.5 V. Mendelevium(II)'s elution behavior has been compared with that of strontium(II) and europium(II).

In 1973, mendelevium(I) was reported to have been produced by Russian scientists, who obtained it by reducing higher oxidation states of mendelevium with samarium(II). It was found to be stable in neutral water–ethanol solution and be homologous to caesium(I). However, later experiments found no evidence for mendelevium(I) and found that mendelevium behaved like divalent elements when reduced, not like the monovalent alkali metals. Nevertheless, the Russian team conducted further studies on the thermodynamics of cocrystallizing mendelevium with alkali metal chlorides, and concluded that mendelevium(I) had formed and could form mixed crystals with divalent elements, thus cocrystallizing with them. The status of the +1 oxidation state is still tentative.

Although "E"°(Md→Md) was predicted in 1975 to be +5.4 V, suggesting that mendelevium(III) could be oxidized to mendelevium(IV), 1967 experiments with the strong oxidizing agent sodium bismuthate were unable to oxidize mendelevium(III) to mendelevium(IV).

A mendelevium atom has 101 electrons, of which at least three (and perhaps four) can act as valence electrons. They are expected to be arranged in the configuration [Rn]5f7s (ground state term symbol F), although experimental verification of this electron configuration had not yet been made as of 2006. In forming compounds, three valence electrons may be lost, leaving behind a [Rn]5f core: this conforms to the trend set by the other actinides with their [Rn] 5f electron configurations in the tripositive state. The first ionization potential of mendelevium was measured to be at most (6.58 ± 0.07) eV in 1974, based on the assumption that the 7s electrons would ionize before the 5f ones; this value has since not yet been refined further due to mendelevium's scarcity and high radioactivity. The ionic radius of hexacoordinate Md had been preliminarily estimated in 1978 to be around 91.2 pm; 1988 calculations based on the logarithmic trend between distribution coefficients and ionic radius produced a value of 89.6 pm, as well as an enthalpy of hydration of . Md should have an ionic radius of 115 pm and hydration enthalpy −1413 kJ/mol; Md should have ionic radius 117 pm.

Sixteen isotopes of mendelevium are known, with mass numbers from 245 to 260; all are radioactive. Additionally, five nuclear isomers are known: Md, Md, Md, Md, and Md. Of these, the longest-lived isotope is Md with a half-life of 51.5 days, and the longest-lived isomer is Md with a half-life of 58.0 minutes. Nevertheless, the slightly shorter-lived Md (half-life 1.17 hours) is more often used in chemical experimentation because it can be produced in larger quantities from alpha particle irradiation of einsteinium. After Md, the next most stable mendelevium isotopes are Md with a half-life of 31.8 days, Md with a half-life of 5.52 hours, Md with a half-life of 1.60 hours, and Md with a half-life of 1.17 hours. All of the remaining mendelevium isotopes have half-lives that are less than an hour, and the majority of these have half-lives that are less than 5 minutes.

The half-lives of mendelevium isotopes mostly increase smoothly from Md onwards, reaching a maximum at Md. Experiments and predictions suggest that the half-lives will then decrease, apart from Md with a half-life of 31.8 days, as spontaneous fission becomes the dominant decay mode due to the mutual repulsion of the protons posing a limit to the island of relative stability of long-lived nuclei in the actinide series.

Mendelevium-256, the chemically most important isotope of mendelevium, decays through electron capture 90.7% of the time and alpha decay 9.9% of the time. It is most easily detected through the spontaneous fission of its electron-capture daughter fermium-256, but in the presence of other nuclides that undergo spontaneous fission, alpha decays at the characteristic energies for mendelevium-256 (7.205 and 7.139 MeV) can provide more useful identification.

The lightest mendelevium isotopes (Md to Md) are mostly produced through bombardment of bismuth targets with heavy argon ions, while slightly heavier ones (Md to Md) are produced by bombarding plutonium and americium targets with lighter ions of carbon and nitrogen. The most important and most stable isotopes are in the range from Md to Md and are produced through bombardment of einsteinium isotopes with alpha particles: einsteinium-253, -254, and -255 can all be used. Md is produced as a daughter of No, and Md can be produced in a transfer reaction between einsteinium-254 and oxygen-18. Typically, the most commonly used isotope Md is produced by bombarding either einsteinium-253 or -254 with alpha particles: einsteinium-254 is preferred when available because it has a longer half-life and therefore can be used as a target for longer. Using available microgram quantities of einsteinium, femtogram quantities of mendelevium-256 may be produced.

The recoil momentum of the produced mendelevium-256 atoms is used to bring them physically far away from the einsteinium target from which they are produced, bringing them onto a thin foil of metal (usually beryllium, aluminium, platinum, or gold) just behind the target in a vacuum. This eliminates the need for immediate chemical separation, which is both costly and prevents reusing of the expensive einsteinium target. The mendelevium atoms are then trapped in a gas atmosphere (frequently helium), and a gas jet from a small opening in the reaction chamber carries the mendelevium along. Using a long capillary tube, and including potassium chloride aerosols in the helium gas, the mendelevium atoms can be transported over tens of meters to be chemically analyzed and have their quantity determined. The mendelevium can then be separated from the foil material and other fission products by applying acid to the foil and then coprecipitating the mendelevium with lanthanum fluoride, then using a cation-exchange resin column with a 10% ethanol solution saturated with hydrochloric acid, acting as an eluant. However, if the foil is made of gold and thin enough, it is enough to simply dissolve the gold in aqua regia before separating the trivalent actinides from the gold using anion-exchange chromatography, the eluant being 6 M hydrochloric acid.

Mendelevium can finally be separated from the other trivalent actinides using selective elution from a cation-exchange resin column, the eluant being ammonia α-HIB. Using the gas-jet method often renders the first two steps unnecessary. The above procedure is the most commonly used one for the separation of transeinsteinium elements.

Another possible way to separate the trivalent actinides is via solvent extraction chromatography using bis-(2-ethylhexyl) phosphoric acid (abbreviated as HDEHP) as the stationary organic phase and nitric acid as the mobile aqueous phase. The actinide elution sequence is reversed from that of the cation-exchange resin column, so that the heavier actinides elute later. The mendelevium separated by this method has the advantage of being free of organic complexing agent compared to the resin column; the disadvantage is that mendelevium then elutes very late in the elution sequence, after fermium.

Another method to isolate mendelevium exploits the distinct elution properties of Md from those of Es and Fm. The initial steps are the same as above, and employs HDEHP for extraction chromatography, but coprecipitates the mendelevium with terbium fluoride instead of lanthanum fluoride. Then, 50 mg of chromium is added to the mendelevium to reduce it to the +2 state in 0.1 M hydrochloric acid with zinc or mercury. The solvent extraction then proceeds, and while the trivalent and tetravalent lanthanides and actinides remain on the column, mendelevium(II) does not and stays in the hydrochloric acid. It is then reoxidized to the +3 state using hydrogen peroxide and then isolated by selective elution with 2 M hydrochloric acid (to remove impurities, including chromium) and finally 6 M hydrochloric acid (to remove the mendelevium). It is also possible to use a column of cationite and zinc amalgam, using 1 M hydrochloric acid as an eluant, reducing Md(III) to Md(II) where it behaves like the alkaline earth metals. Thermochromatographic chemical isolation could be achieved using the volatile mendelevium hexafluoroacetylacetonate: the analogous fermium compound is also known and is also volatile.

Although few people come in contact with mendelevium, the International Commission on Radiological Protection has set annual exposure limits for the most stable isotope. For mendelevium-258, the ingestion limit was set at 9×10 becquerels (1 Bq is equivalent to one decay per second), and the inhalation limit at 6000 Bq.




</doc>
<doc id="18900" url="https://en.wikipedia.org/wiki?curid=18900" title="Modus ponens">
Modus ponens

In propositional logic, modus ponens (; MP; also modus ponendo ponens (Latin for "mode that affirms by affirming") or implication elimination) is a rule of inference. It can be summarized as ""P implies Q" and "P" are both asserted to be true, so therefore "Q" must be true."

"Modus ponens" is closely related to another valid form of argument, "modus tollens". Both have apparently similar but invalid forms such as affirming the consequent, denying the antecedent, and evidence of absence. Constructive dilemma is the disjunctive version of "modus ponens". Hypothetical syllogism is closely related to "modus ponens" and sometimes thought of as "double "modus ponens"."

The history of "modus ponens" goes back to antiquity. The first to explicitly describe the argument form "modus ponens" was Theophrastus.

The "modus ponens" rule may be written in sequent notation:

where ⊢ is a metalogical symbol meaning that "Q" is a syntactic consequence of "P" → "Q" and "P" in some logical system;

or as the statement of a truth-functional tautology or theorem of propositional logic:

where "P", and "Q" are propositions expressed in some formal system.

The argument form has two premises (hypothesis). The first premise is the "if–then" or conditional claim, namely that "P" implies "Q". The second premise is that "P", the antecedent of the conditional claim, is true. From these two premises it can be logically concluded that "Q", the consequent of the conditional claim, must be true as well. In artificial intelligence, "modus ponens" is often called forward chaining.

An example of an argument that fits the form "modus ponens":

This argument is valid, but this has no bearing on whether any of the statements in the argument are true; for "modus ponens" to be a sound argument, the premises must be true for any true instances of the conclusion. An argument can be valid but nonetheless unsound if one or more premises are false; if an argument is valid "and" all the premises are true, then the argument is sound. For example, John might be going to work on Wednesday. In this case, the reasoning for John's going to work (because it is Wednesday) is unsound. The argument is not only sound on Tuesdays (when John goes to work), but valid on every day of the week. A propositional argument using "modus ponens" is said to be deductive.

In single-conclusion sequent calculi, "modus ponens" is the Cut rule. The cut-elimination theorem for a calculus says that every proof involving Cut can be transformed (generally, by a constructive method) into a proof without Cut, and hence that Cut is admissible.

The Curry–Howard correspondence between proofs and programs relates "modus ponens" to function application: if "f" is a function of type "P" → "Q" and "x" is of type "P", then "f x" is of type "Q".

The validity of "modus ponens" in classical two-valued logic can be clearly demonstrated by use of a truth table.
In instances of "modus ponens" we assume as premises that "p" → "q" is true and "p" is true. Only one line of the truth table—the first—satisfies these two conditions ("p" and "p" → "q"). On this line, "q" is also true. Therefore, whenever "p" → "q" is true and "p" is true, "q" must also be true.

While "modus ponens" is one of the most commonly used argument forms in logic it must not be mistaken for a logical law; rather, it is one of the accepted mechanisms for the construction of deductive proofs that includes the "rule of definition" and the "rule of substitution". "Modus ponens" allows one to eliminate a conditional statement from a logical proof or argument (the antecedents) and thereby not carry these antecedents forward in an ever-lengthening string of symbols; for this reason modus ponens is sometimes called the rule of detachment or the law of detachment. Enderton, for example, observes that "modus ponens can produce shorter formulas from longer ones", and Russell observes that "the process of the inference cannot be reduced to symbols. Its sole record is the occurrence of ⊦q [the consequent] . . . an inference is the dropping of a true premise; it is the dissolution of an implication".

A justification for the "trust in inference is the belief that if the two former assertions [the antecedents] are not in error, the final assertion [the consequent] is not in error". In other words: if one statement or proposition implies a second one, and the first statement or proposition is true, then the second one is also true. If "P" implies "Q" and "P" is true, then "Q" is true.

"Modus ponens" represents an instance of the Law of total probability which for a binary variable is expressed as:

formula_3,

where e.g. formula_4 denotes the probability of formula_5 and the conditional probability formula_6 generalizes the logical implication formula_7. Assume that formula_8 is equivalent to formula_5 being TRUE, and that formula_10 is equivalent to formula_5 being FALSE. It is then easy to see that formula_8 when formula_13 and formula_14. Hence, the law of total probability represents a generalization of "modus ponens" .

"Modus ponens" represents an instance of the binomial deduction operator in subjective logic expressed as:

formula_15,

where formula_16 denotes the subjective opinion about formula_17 as expressed by source formula_18, and the conditional opinion formula_19 generalizes the logical implication formula_7. The deduced marginal opinion about formula_5 is denoted by formula_22. The case where formula_16 is an absolute TRUE opinion about formula_17 is equivalent to source formula_18 saying that formula_17 is TRUE, and the case where formula_16 is an absolute FALSE opinion about formula_17 is equivalent to source formula_18 saying that formula_17 is FALSE. The deduction operator formula_31 of subjective logic produces an absolute TRUE deduced opinion formula_22 when the conditional opinion formula_19 is absolute TRUE and the antecedent opinion formula_16 is absolute TRUE. Hence, subjective logic deduction represents a generalization of both "modus ponens" and the Law of total probability .

The fallacy of affirming the consequent is a common misinterpretation of the modus ponens.





</doc>
<doc id="18901" url="https://en.wikipedia.org/wiki?curid=18901" title="Modus tollens">
Modus tollens

In propositional logic, modus tollens (; MT; also modus tollendo tollens (Latin for "mode that denies by denying") or denying the consequent) is a valid argument form and a rule of inference. It is an application of the general truth that if a statement is true, then so is its contra-positive.

The inference rule "modus tollens" validates the inference from formula_1 implies formula_2 and the contradictory of formula_2 to the contradictory of formula_1.

The "modus tollens" rule can be stated formally as:

where formula_6 stands for the statement "P implies Q". formula_7 stands for "it is not the case that Q" (or in brief "not Q"). Then, whenever "formula_6" and "formula_9" each appear by themselves as a line of a proof, then "formula_10" can validly be placed on a subsequent line. The history of the inference rule "modus tollens" goes back to antiquity.

"Modus tollens" is closely related to "modus ponens". There are two similar, but invalid, forms of argument: affirming the consequent and denying the antecedent. See also contraposition and proof by contrapositive.

The first to explicitly describe the argument form "modus tollens" was Theophrastus.

The "modus tollens" rule may be written in sequent notation:

where formula_12 is a metalogical symbol meaning that formula_10 is a syntactic consequence of formula_6 and formula_9 in some logical system;

or as the statement of a functional tautology or theorem of propositional logic:

where formula_1 and formula_2 are propositions expressed in some formal system;

or including assumptions:
though since the rule does not change the set of assumptions, this is not strictly necessary.

More complex rewritings involving "modus tollens" are often seen, for instance in set theory:

Also in first-order predicate logic:

Strictly speaking these are not instances of "modus tollens", but they may be derived from "modus tollens" using a few extra steps.

Requirements:

Consider an example:

Supposing that the premises are both true (the dog will bark if it detects an intruder, and does indeed not bark), it follows that no intruder has been detected. This is a valid argument since it is not possible for the conclusion to be false if the premises are true. (It is conceivable that there may have been an intruder that the dog did not detect, but that does not invalidate the argument; the first premise is "if the watch-dog detects an intruder." The thing of importance is that the dog detects or does not detect an intruder, not whether there is one.)

Another example:

Another example: 

Every use of "modus tollens" can be converted to a use of "modus ponens" and one use of transposition to the premise which is a material implication. For example:

Likewise, every use of "modus ponens" can be converted to a use of "modus tollens" and transposition.

The validity of "modus tollens" can be clearly demonstrated through a truth table.

In instances of "modus tollens" we assume as premises that p → q is true and q is false. There is only one line of the truth table—the fourth line—which satisfies these two conditions. In this line, p is false. Therefore, in every instance in which p → q is true and q is false, p must also be false.

"Modus tollens" represents an instance of the Law of total probability combined with Bayes' theorem expressed as:

formula_26,

where the conditionals formula_27 and formula_28 are obtained with (the extended form of) Bayes' theorem expressed as:

formula_29 and formula_30.

In the equations above formula_31 denotes the probability of formula_2, and formula_33 denotes the base rate (aka. prior probability) of formula_1. The conditional probability formula_35 generalizes the logical statement formula_6, i.e. in addition to assigning TRUE or FALSE we can also assign any probability to the statement. Assume that formula_37 is equivalent to formula_2 being TRUE, and that formula_39 is equivalent to formula_2 being FALSE. It is then easy to see that formula_41 when formula_42 and formula_39. This is because formula_44 so that formula_45 in the last equation. Therefore, the product terms in the first equation always have a zero factor so that formula_41 which is equivalent to formula_1 being FALSE. Hence, the law of total probability combined with Bayes' theorem represents a generalization of "modus tollens" .

"Modus tollens" represents an instance of the abduction operator in subjective logic expressed as:

formula_48,

where formula_49 denotes the subjective opinion about formula_2, and formula_51 denotes a pair of binomial conditional opinions, as expressed by source formula_52. The parameter formula_53 denotes the base rate (aka. the prior probability) of formula_1. The abduced marginal opinion on formula_1 is denoted formula_56. The conditional opinion formula_57 generalizes the logical statement formula_6, i.e. in addition to assigning TRUE or FALSE the source formula_52 can assign any subjective opinion to the statement. The case where formula_49 is an absolute TRUE opinion is equivalent to source formula_52 saying that formula_2 is TRUE, and the case where formula_49 is an absolute FALSE opinion is equivalent to source formula_52 saying that formula_2 is FALSE. The abduction operator formula_66 of subjective logic produces an absolute FALSE abduced opinion formula_67 when the conditional opinion formula_57 is absolute TRUE and the consequent opinion formula_49 is absolute FALSE. Hence, subjective logic abduction represents a generalization of both "modus tollens" and of the Law of total probability combined with Bayes' theorem .





</doc>
<doc id="18902" url="https://en.wikipedia.org/wiki?curid=18902" title="Mathematician">
Mathematician

A mathematician is someone who uses an extensive knowledge of mathematics in his or her work, typically to solve mathematical problems.

Mathematics is concerned with numbers, data, quantity, structure, space, models, and change.

One of the earliest known mathematicians was Thales of Miletus (c. 624–c.546 BC); he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem.

The number of known mathematicians grew when Pythagoras of Samos (c. 582–c. 507 BC) established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was "All is number". It was the Pythagoreans who coined the term "mathematics", and with whom the study of mathematics for its own sake begins.

The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350 - 415). She succeeded her father as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria punished her, presuming she was involved, by stripping her naked and scraping off her skin with clamshells (some say roofing tiles).

Science and mathematics in the Islamic world during the Middle Ages followed various models and modes of funding varied based primarily on scholars. It was extensive patronage and strong intellectual policies implemented by specific rulers that allowed scientific knowledge to develop in many areas. Funding for translation of scientific texts in other languages was ongoing throughout the reign of certain caliphs, and it turned out that certain scholars became experts in the works they translated and in turn received further support for continuing to develop certain sciences. As these sciences received wider attention from the elite, more scholars were invited and funded to study particular sciences. An example of a translator and mathematician who benefited from this type of support was al-Khawarizmi. A notable feature of many scholars working under Muslim rule in medieval times is that they were often polymaths. Examples include the work on optics, maths and astronomy of Ibn al-Haytham.

The Renaissance brought an increased emphasis on mathematics and science to Europe. During this period of transition from a mainly feudal and ecclesiastical culture to a predominantly secular one, many notable mathematicians had other occupations: Luca Pacioli (founder of accounting); Niccolò Fontana Tartaglia (notable engineer and bookkeeper); Gerolamo Cardano (earliest founder of probability and binomial expansion); Robert Recorde (physician) and François Viète (lawyer).

As time passed, many mathematicians gravitated towards universities. An emphasis on free thinking and experimentation had begun in Britain's oldest universities beginning in the seventeenth century at Oxford with the scientists Robert Hooke and Robert Boyle, and at Cambridge where Isaac Newton was Lucasian Professor of Mathematics & Physics. Moving into the 19th century, the objective of universities all across Europe evolved from teaching the “regurgitation of knowledge” to “encourag[ing] productive thinking.” In 1810, Humboldt convinced the King of Prussia to build a university in Berlin based on Friedrich Schleiermacher’s liberal ideas; the goal was to demonstrate the process of the discovery of knowledge and to teach students to “take account of fundamental laws of science in all their thinking.” Thus, seminars and laboratories started to evolve.

British universities of this period adopted some approaches familiar to the Italian and German universities, but as they already enjoyed substantial freedoms and autonomy the changes there had begun with the Age of Enlightenment, the same influences that inspired Humboldt. The Universities of Oxford and Cambridge emphasized the importance of research, arguably more authentically implementing Humboldt’s idea of a university than even German universities, which were subject to state authority. Overall, science (including mathematics) became the focus of universities in the 19th and 20th centuries. Students could conduct research in seminars or laboratories and began to produce doctoral theses with more scientific content. According to Humboldt, the mission of the University of Berlin was to pursue scientific knowledge. The German university system fostered professional, bureaucratically regulated scientific research performed in well-equipped laboratories, instead of the kind of research done by private and individual scholars in Great Britain and France. In fact, Rüegg asserts that the German system is responsible for the development of the modern research university because it focused on the idea of “freedom of scientific research, teaching and study.”

Mathematicians usually cover a breadth of topics within mathematics in their undergraduate education, and then proceed to specialize in topics of their own choice at the graduate level. In some universities, a qualifying exam serves to test both the breadth and depth of a student's understanding of mathematics; the students, who pass, are permitted to work on a doctoral dissertation.

Mathematicians involved with solving problems with applications in real life are called applied mathematicians. Applied mathematicians are mathematical scientists who, with their specialized knowledge and professional methodology, approach many of the imposing problems presented in related scientific fields. With professional focus on a wide variety of problems, theoretical systems, and localized constructs, applied mathematicians work regularly in the study and formulation of mathematical models. Mathematicians and applied mathematicians are considered to be two of the STEM (science, technology, engineering, and mathematics) careers.

The discipline of applied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry; thus, "applied mathematics" is a mathematical science with specialized knowledge. The term "applied mathematics" also describes the professional specialty in which mathematicians work on problems, often concrete but sometimes abstract. As professionals focused on problem solving, "applied mathematicians" look into the "formulation, study, and use of mathematical models" in science, engineering, business, and other areas of mathematical practice.

Pure mathematics is mathematics that studies entirely abstract concepts. From the eighteenth century onwards, this was a recognized category of mathematical activity, sometimes characterized as "speculative mathematics", and at variance with the trend towards meeting the needs of navigation, astronomy, physics, economics, engineering, and other applications.

Another insightful view put forth is that "pure mathematics is not necessarily applied mathematics": it is possible to study abstract entities with respect to their intrinsic nature, and not be concerned with how they manifest in the real world. Even though the pure and applied viewpoints are distinct philosophical positions, in practice there is much overlap in the activity of pure and applied mathematicians.

To develop accurate models for describing the real world, many applied mathematicians draw on tools and techniques that are often considered to be "pure" mathematics. On the other hand, many pure mathematicians draw on natural and social phenomena as inspiration for their abstract research.

Many professional mathematicians also engage in the teaching of mathematics. Duties may include:

Many careers in mathematics outside of universities involve consulting. For instance, actuaries assemble and analyze data to estimate the probability and likely cost of the occurrence of an event such as death, sickness, injury, disability, or loss of property. Actuaries also address financial questions, including those involving the level of pension contributions required to produce a certain retirement income and the way in which a company should invest resources to maximize its return on investments in light of potential risk. Using their broad knowledge, actuaries help design and price insurance policies, pension plans, and other financial strategies in a manner which will help ensure that the plans are maintained on a sound financial basis.

As another example, mathematical finance will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. Mathematical consistency is required, not compatibility with economic theory. Thus, for example, while a financial economist might study the structural reasons why a company may have a certain share price, a financial mathematician may take the share price as a given, and attempt to use stochastic calculus to obtain the corresponding value of derivatives of the stock ("see: Valuation of options; Financial modeling").

According to the Dictionary of Occupational Titles occupations in mathematics include the following.


The following are quotations about mathematicians, or by mathematicians.

There is no Nobel Prize in mathematics, though sometimes mathematicians have won the Nobel Prize in a different field, such as economics. Prominent prizes in mathematics include the Abel Prize, the Chern Medal, the Fields Medal, the Gauss Prize, the Nemmers Prize, the Balzan Prize, the Crafoord Prize, the Shaw Prize, the Steele Prize, the Wolf Prize, the Schock Prize, and the Nevanlinna Prize.

The American Mathematical Society, Association for Women in Mathematics, and other mathematical societies offer several prizes aimed at increasing the representation of women and minorities in the future of mathematics.

Several well known mathematicians have written autobiographies in part to explain to a general audience what it is about mathematics that has made them want to devote their lives to its study. These provide some of the best glimpses into what it means to be a mathematician. The following list contains some works that are not autobiographies, but rather essays on mathematics and mathematicians with strong autobiographical elements.




</doc>
<doc id="18906" url="https://en.wikipedia.org/wiki?curid=18906" title="Microfluidics">
Microfluidics

Microfluidics deals with the behaviour, precise control and manipulation of fluids that are geometrically constrained to a small, typically sub-millimeter, scale at which capillary penetration governs mass transport. It is a multidisciplinary field at the intersection of engineering, physics, chemistry, biochemistry, nanotechnology, and biotechnology, with practical applications in the design of systems in which low volumes of fluids are processed to achieve multiplexing, automation, and high-throughput screening. Microfluidics emerged in the beginning of the 1980s and is used in the development of inkjet printheads, DNA chips, lab-on-a-chip technology, micro-propulsion, and micro-thermal technologies.

Typically, micro means one of the following features:

Typically in microfluidic systems fluids are transported, mixed, separated or otherwise processed. The various applications of such systems rely on passive fluid control using capillary forces. In some applications, external actuation means are additionally used for a directed transport of the media. Examples are rotary drives applying centrifugal forces for the fluid transport on the passive chips. Active microfluidics refers to the defined manipulation of the working fluid by active (micro) components such as micropumps or microvalves. Micropumps supply fluids in a continuous manner or are used for dosing. Microvalves determine the flow direction or the mode of movement of pumped liquids. Often processes which are normally carried out in a lab are miniaturised on a single chip in order to enhance efficiency and mobility as well as reducing sample and reagent volumes.

The behaviour of fluids at the microscale can differ from "macrofluidic" behaviour in that factors such as surface tension, energy dissipation, and fluidic resistance start to dominate the system. Microfluidics studies how these behaviours change, and how they can be worked around, or exploited for new uses.

At small scales (channel size of around 100 nanometers to 500 micrometers) some interesting and sometimes unintuitive properties appear. In particular, the Reynolds number (which compares the effect of the momentum of a fluid to the effect of viscosity) can become very low. A key consequence is co-flowing fluids do not necessarily mix in the traditional sense, as flow becomes laminar rather than turbulent; molecular transport between them must often be through diffusion.

High specificity of chemical and physical properties (concentration, pH, temperature, shear force, etc.) can also be ensured resulting in more uniform reaction conditions and higher grade products in single and multi-step reactions.

Microfluidic structures include micropneumatic systems, i.e. microsystems for the handling of off-chip fluids (liquid pumps, gas valves, etc.), and microfluidic structures for the on-chip handling of nanoliter (nl) and picoliter (pl) volumes. To date, the most successful commercial application of microfluidics is the inkjet printhead. Additionally, advances in microfluidic manufacturing allow the devices to be produced in low-cost plastics and part quality may be verified automatically.

Advances in microfluidics technology are revolutionizing molecular biology procedures for enzymatic analysis (e.g., glucose and lactate assays), DNA analysis (e.g., polymerase chain reaction and high-throughput sequencing), and proteomics. The basic idea of microfluidic biochips is to integrate assay operations such as detection, as well as sample pre-treatment and sample preparation on one chip.

An emerging application area for biochips is clinical pathology, especially the immediate point-of-care diagnosis of diseases. In addition, microfluidics-based devices, capable of continuous sampling and real-time testing of air/water samples for biochemical toxins and other dangerous pathogens, can serve as an always-on "bio-smoke alarm" for early warning.

Microfluidic technology has led to the creation of powerful tools for biologists to control the complete cellular environment, leading to new questions and discoveries. Many diverse advantages of this technology for microbiology are listed below:

Some of these areas are further elaborated in the sections below.

In open microfluidics, at least one boundary of the system is removed, exposing the fluid to air or another interface (i.e. liquid). Advantages of open microfluidics include accessibility to the flowing liquid for intervention, larger liquid-gas surface area, and minimized bubble formation. Another advantage of open microfluidics is the ability to integrate open systems with surface-tension driven fluid flow, which eliminates the need for external pumping methods such as peristaltic or syringe pumps. Open microfluidic devices are also easy and inexpensive to fabricate by milling, thermoforming, and hot embossing. In addition, open microfluidics eliminates the need to glue or bond a cover for devices which could be detrimental for capillary flows. Examples of open microfluidics include open-channel microfluidics, rail-based microfluidics, paper-based, and thread-based microfluidics. Disadvantages to open systems include susceptibility to evaporation, contamination, and limited flow rate.

These technologies are based on the manipulation of continuous liquid flow through microfabricated channels. Actuation of liquid flow is implemented either by external pressure sources, external mechanical pumps, integrated mechanical micropumps, or by combinations of capillary forces and electrokinetic mechanisms. Continuous-flow microfluidic operation is the mainstream approach because it is easy to implement and less sensitive to protein fouling problems. Continuous-flow devices are adequate for many well-defined and simple biochemical applications, and for certain tasks such as chemical separation, but they are less suitable for tasks requiring a high degree of flexibility or fluid manipulations. These closed-channel systems are inherently difficult to integrate and scale because the parameters that govern flow field vary along the flow path making the fluid flow at any one location dependent on the properties of the entire system. Permanently etched microstructures also lead to limited reconfigurability and poor fault tolerance capability. Computer-aided design automation approaches for continuous-flow microfluidics have been proposed in recent years to alleviate the design effort and to solve the scalability problems. 

Process monitoring capabilities in continuous-flow systems can be achieved with highly sensitive microfluidic flow sensors based on MEMS technology which offers resolutions down to the nanoliter range.

Droplet-based microfluidics is a subcategory of microfluidics in contrast with continuous microfluidics; droplet-based microfluidics manipulates discrete volumes of fluids in immiscible phases with low Reynolds number and laminar flow regimes. Interest in droplet-based microfluidics systems has been growing substantially in past decades. Microdroplets allow for handling miniature volumes (μl to fl) of fluids conveniently, provide better mixing, encapsulation, sorting, and sensing, and suit high throughput experiments. Exploiting the benefits of droplet-based microfluidics efficiently requires a deep understanding of droplet generation to perform various logical operations such as droplet motion, droplet sorting, droplet merging, and droplet breakup.

Alternatives to the above closed-channel continuous-flow systems include novel open structures, where discrete, independently controllable droplets
are manipulated on a substrate using electrowetting. Following the analogy of digital microelectronics, this approach is referred to as digital microfluidics. Le Pesant et al. pioneered the use of electrocapillary forces to move droplets on a digital track. The "fluid transistor" pioneered by Cytonix also played a role. The technology was subsequently commercialised by Duke University. By using discrete unit-volume droplets, a microfluidic function can be reduced to a set of repeated basic operations, i.e., moving one unit of fluid over one unit of distance. This "digitisation" method facilitates the use of a hierarchical and cell-based approach for microfluidic biochip design. Therefore, digital microfluidics offers a flexible and scalable system architecture as well as high fault-tolerance capability. Moreover, because each droplet can be controlled independently, these systems also have dynamic reconfigurability, whereby groups of unit cells in a microfluidic array can be reconfigured to change their functionality during the concurrent execution of a set of bioassays. Although droplets are manipulated in confined microfluidic channels, since the control on droplets is not independent, it should not be confused as "digital microfluidics". One common actuation method for digital microfluidics is electrowetting-on-dielectric (EWOD). Many lab-on-a-chip applications have been demonstrated within the digital microfluidics paradigm using electrowetting. However, recently other techniques for droplet manipulation have also been demonstrated using surface acoustic waves, optoelectrowetting, mechanical actuation, etc.

Paper-based microfluidic devices fill a growing niche for portable, cheap, and user-friendly medical diagnostic systems. 
Paper based microfluidics rely on the phenomenon of capillary penetration in porous media. In order to tune fluid penetration in porous substrates such as paper, in two and three dimensions, the pore structure, wettability and geometry of the microfluidic devices can be controlled while the viscosity and evaporation rate of the liquid play a further significant role. Many such devices feature hydrophobic barriers on hydrophilic paper that passively transport aqueous solutions to outlets where biological reactions take place. Current applications include portable glucose detection and environmental testing, with hopes of reaching areas that lack advanced medical diagnostic tools.

Early biochips were based on the idea of a DNA microarray, e.g., the GeneChip DNAarray from Affymetrix, which is a piece of glass, plastic or silicon substrate, on which pieces of DNA (probes) are affixed in a microscopic array. Similar to a DNA microarray, a protein array is a miniature array where a multitude of different capture agents, most frequently monoclonal antibodies, are deposited on a chip surface; they are used to determine the presence and/or amount of proteins in biological samples, e.g., blood. A drawback of DNA and protein arrays is that they are neither reconfigurable nor scalable after manufacture. Digital microfluidics has been described as a means for carrying out Digital PCR.

In addition to microarrays, biochips have been designed for two-dimensional electrophoresis, transcriptome analysis, and PCR amplification. Other applications include various electrophoresis and liquid chromatography applications for proteins and DNA, cell separation, in particular, blood cell separation, protein analysis, cell manipulation and analysis including cell viability analysis and microorganism capturing.

By combining microfluidics with landscape ecology and nanofluidics, a nano/micro fabricated fluidic landscape can be constructed by building local patches of bacterial habitat and connecting them by dispersal corridors. The resulting landscapes can be used as physical implementations of an adaptive landscape, by generating a spatial mosaic of patches of opportunity distributed in space and time. The patchy nature of these fluidic landscapes allows for the study of adapting bacterial cells in a metapopulation system. The evolutionary ecology of these bacterial systems in these synthetic ecosystems allows for using biophysics to address questions in evolutionary biology.

The ability to create precise and carefully controlled chemoattractant gradients makes microfluidics the ideal tool to study motility, chemotaxis and the ability to evolve / develop resistance to antibiotics in small populations of microorganisms and in a short period of time. These microorganisms including bacteria and the broad range of organisms that form the marine microbial loop, responsible for regulating much of the oceans' biogeochemistry.

Microfluidics has also greatly aided the study of durotaxis by facilitating the creation of durotactic (stiffness) gradients.

By rectifying the motion of individual swimming bacteria, microfluidic structures can be used to extract mechanical motion from a population of motile bacterial cells. This way, bacteria-powered rotors can be built.

The merger of microfluidics and optics is typical known as optofluidics. Examples of optofluidic devices are tunable microlens arrays and optofluidic microscopes.

Microfluidic flow enables fast sample throughput, automated imaging of large sample populations, as well as 3D capabilities. or superresolution.

Acoustic droplet ejection uses a pulse of ultrasound to move low volumes of fluids (typically nanoliters or picoliters) without any physical contact. This technology focuses acoustic energy into a fluid sample in order to eject droplets as small as a millionth of a millionth of a litre (picoliter = 10 litre). ADE technology is a very gentle process, and it can be used to transfer proteins, high molecular weight DNA and live cells without damage or loss of viability. This feature makes the technology suitable for a wide variety of applications including proteomics and cell-based assays.

Microfluidic fuel cells can use laminar flow to separate the fuel and its oxidant to control the interaction of the two fluids without a physical barrier as would be required in conventional fuel cells.

In order to understand the prospects for life to exist elsewhere in the universe, astrobiologists are interested in measuring the chemical composition of extraplanetary bodies. Because of their small size and wide-ranging functionality, microfluidic devices are uniquely suited for these remote sample analyses. From an extraterrestrial sample, the organic content can be assessed using microchip capillary electrophoresis and selective fluorescent dyes. These devices are capable of detecting amino acids, peptides, fatty acids, and simple aldehydes, ketones, and thiols. These analyses coupled together could allow powerful detection of the key components of life, and hopefully inform our search for functioning extraterrestrial life.






</doc>
<doc id="18908" url="https://en.wikipedia.org/wiki?curid=18908" title="Mersenne prime">
Mersenne prime

In mathematics, a Mersenne prime is a prime number that is one less than a power of two. That is, it is a prime number of the form for some integer . They are named after Marin Mersenne, a French Minim friar, who studied them in the early 17th century.

The exponents which give Mersenne primes are 2, 3, 5, 7, 13, 17, 19, 31, ... and the resulting Mersenne primes are 3, 7, 31, 127, 8191, 131071, 524287, 2147483647, ... .

If is a composite number then so is . ( is divisible by both and .) This definition is therefore equivalent to a definition as a prime number of the form for some prime .

More generally, numbers of the form without the primality requirement are called Mersenne numbers. Mersenne numbers are sometimes defined to have the additional requirement that be prime, equivalently that they be pernicious Mersenne numbers, namely those numbers whose binary representation contains a prime number of ones and no zeros. The smallest composite pernicious Mersenne number is .

Mersenne primes are also noteworthy due to their connection to perfect numbers.

A new Mersenne prime was found in December 2017. , 50 are now known. The largest known prime number is a Mersenne prime. Since 1997, all newly found Mersenne primes have been discovered by the Great Internet Mersenne Prime Search (GIMPS), a distributed computing project on the Internet.

Many fundamental questions about Mersenne primes remain unresolved. It is not even known whether the set of Mersenne primes is finite or infinite. The Lenstra–Pomerance–Wagstaff conjecture asserts that there are infinitely many Mersenne primes and predicts their order of growth. It is also not known whether infinitely many Mersenne numbers with prime exponents are composite, although this would follow from widely believed conjectures about prime numbers, for example, the infinitude of Sophie Germain primes congruent to 3 (mod 4). For these primes , (which is also prime) will divide , e.g., , , , , , , , and . Since for these primes , is congruent to 7 mod 8, so 2 is a quadratic residue mod , and the multiplicative order of 2 mod must divide formula_1 = . Since is a prime, it must be or 1. However, it cannot be 1 since formula_2 and 1 has no prime factors, so it must be . Hence, divides formula_3 and = cannot be prime.

The first four Mersenne primes are

A basic theorem about Mersenne numbers states that if is prime, then the exponent must also be prime. This follows from the identity

This rules out primality for Mersenne numbers with composite exponent, such as .

Though the above examples might suggest that is prime for all primes , this is not the case, and the smallest counterexample is the Mersenne number

The evidence at hand suggests that a randomly selected Mersenne number is much more likely to be prime than an arbitrary randomly selected odd integer of similar size. Nonetheless, prime appear to grow increasingly sparse as increases. For example, eight of the first 11 primes give rise to a Mersenne prime (the correct terms on Mersenne's original list), while is prime for only 43 of the first two million prime numbers (up to 32,452,843).

The lack of any simple test to determine whether a given Mersenne number is prime makes the search for Mersenne primes a difficult task, since Mersenne numbers grow very rapidly. The Lucas–Lehmer primality test (LLT) is an efficient primality test that greatly aids this task, making it much easier to test the primality of Mersenne numbers than that of most other numbers of the same size. The search for the largest known prime has somewhat of a cult following. Consequently, a lot of computer power has been expended searching for new Mersenne primes, much of which is now done using distributed computing.

Mersenne primes are used in pseudorandom number generators such as the Mersenne twister, Park–Miller random number generator, Generalized Shift Register and Fibonacci RNG.

Mersenne primes are also noteworthy due to their connection with perfect numbers. In the 4th century BC, Euclid proved that if is prime, then ) is a perfect number. This number, also expressible as , is the th triangular number and the th hexagonal number. In the 18th century, Leonhard Euler proved that, conversely, all even perfect numbers have this form. This is known as the Euclid–Euler theorem. It is unknown whether there are any odd perfect numbers.

Mersenne primes take their name from the 17th-century French scholar Marin Mersenne, who compiled what was supposed to be a list of Mersenne primes with exponents up to 257. The exponents listed by Mersenne were as follows:

His list replicated the known primes of his time with exponents up to 19. His next entry, 31, was correct, but the list then became largely incorrect, as Mersenne mistakenly included and (which are composite) and omitted , , and (which are prime). Mersenne gave little indication how he came up with his list.

Édouard Lucas proved in 1876 that is indeed prime, as Mersenne claimed. This was the largest known prime number for 75 years, and the largest ever found by hand. was determined to be prime in 1883 by Ivan Mikheevich Pervushin, though Mersenne claimed it was composite, and for this reason it is sometimes called Pervushin's number. This was the second-largest known prime number, and it remained so until 1911. Lucas had shown another error in Mersenne's list in 1876. Without finding a factor, Lucas demonstrated that is actually composite. No factor was found until a famous talk by Frank Nelson Cole in 1903. Without speaking a word, he went to a blackboard and raised 2 to the 67th power, then subtracted one. On the other side of the board, he multiplied and got the same number, then returned to his seat (to applause) without speaking. He later said that the result had taken him "three years of Sundays" to find. A correct list of all Mersenne primes in this number range was completed and rigorously verified only about three centuries after Mersenne published his list.

Fast algorithms for finding Mersenne primes are available, and the seven largest known prime numbers are Mersenne primes.

The first four Mersenne primes , , and were known in antiquity. The fifth, , was discovered anonymously before 1461; the next two ( and ) were found by Pietro Cataldi in 1588. After nearly two centuries, was verified to be prime by Leonhard Euler in 1772. The next (in historical, not numerical order) was , found by Édouard Lucas in 1876, then by Ivan Mikheevich Pervushin in 1883. Two more ( and ) were found early in the 20th century, by R. E. Powers in 1911 and 1914, respectively.

The best method presently known for testing the primality of Mersenne numbers is the Lucas–Lehmer primality test. Specifically, it can be shown that for prime , is prime if and only if divides , where and for .

During the era of manual calculation, all the exponents up to and including 257 were tested with the Lucas–Lehmer test and found to be composite. A notable contribution was made by retired Yale physics professor Horace Scudder Uhler, who did the calculations for exponents 157, 167, 193, 199, 227, and 229. Unfortunately for those investigators, the interval they were testing contains the largest known gap between Mersenne primes, in relative terms: the next Mersenne prime exponent, 521, would turn out to be more than four times larger than the previous record of 127.
The search for Mersenne primes was revolutionized by the introduction of the electronic digital computer. Alan Turing searched for them on the Manchester Mark 1 in 1949, but the first successful identification of a Mersenne prime, , by this means was achieved at 10:00 pm on January 30, 1952 using the U.S. National Bureau of Standards Western Automatic Computer (SWAC) at the Institute for Numerical Analysis at the University of California, Los Angeles, under the direction of Lehmer, with a computer search program written and run by Prof. R. M. Robinson. It was the first Mersenne prime to be identified in thirty-eight years; the next one, , was found by the computer a little less than two hours later. Three more — , ,  — were found by the same program in the next several months. is the first Mersenne prime that is titanic, is the first gigantic, and was the first megaprime to be discovered, being a prime with at least 1,000,000 digits. All three were the first known prime of any kind of that size. The number of digits in the decimal representation of equals , where denotes the floor function (or equivalently ).

In September 2008, mathematicians at UCLA participating in GIMPS won part of a $100,000 prize from the Electronic Frontier Foundation for their discovery of a very nearly 13-million-digit Mersenne prime. The prize, finally confirmed in October 2009, is for the first known prime with at least 10 million digits. The prime was found on a Dell OptiPlex 745 on August 23, 2008. This was the eighth Mersenne prime discovered at UCLA.

On April 12, 2009, a GIMPS server log reported that a 47th Mersenne prime had possibly been found. The find was verified on June 12, 2009. The prime is . Although it is chronologically the 47th Mersenne prime to be discovered, it is smaller than the largest known at the time, which was the 45th to be discovered.

On January 25, 2013, Curtis Cooper, a mathematician at the University of Central Missouri, discovered a 48th Mersenne prime, (a number with 17,425,170 digits), as a result of a search executed by a GIMPS server network.

On January 19, 2016, Cooper published his discovery of a 49th Mersenne prime, (a number with 22,338,618 digits), as a result of a search executed by a GIMPS server network. This was the fourth Mersenne prime discovered by Cooper and his team in the past ten years.

On January 3, 2018, it was announced that Jonathan Pace, a 51-year-old electrical engineer living in Germantown, Tennessee, had found a 50th Mersenne prime, (a number with 23,249,425 digits), as a result of a search executed by a GIMPS server network.


The table below lists all known Mersenne primes (sequence () and () in OEIS):

All Mersenne numbers below the 50th Mersenne prime () have been tested at least once but some have not been double-checked. Primes are not always discovered in increasing order. For example, the 29th Mersenne prime was discovered "after" the 30th and the 31st. Similarly, was followed by two smaller Mersenne primes, first 2 weeks later and then 8 months later. was the first discovered prime number with more than 10 million decimal digits.

The largest known Mersenne prime is also the largest known prime number.

In modern times, the largest known prime has almost always been a Mersenne prime.

Since they are prime numbers, Mersenne primes are divisible only by 1 and by themselves. However, not all Mersenne numbers are Mersenne primes, and the composite Mersenne numbers may be factored nontrivially. Mersenne numbers are very good test cases for the special number field sieve algorithm, so often the largest number factorized with this algorithm has been a Mersenne number. , 2 − 1 is the record-holder, having been factored with a variant of the special number field sieve that allows the factorization of several numbers at once. See integer factorization records for links to more information. The special number field sieve can factorize numbers with more than one large factor. If a number has only one very large factor then other algorithms can factorize larger numbers by first finding small factors and then making a primality test on the cofactor. , the largest factorization with probable prime factors allowed is , where is a 2,201,714-digit probable prime. It was discovered by Oliver Kruse. , the Mersenne number "M" is the smallest composite Mersenne number with no known factors; it has no prime factors below 2.

The table below shows factorizations for the first 20 composite Mersenne numbers .

The primitive part of Mersenne number is , the th cyclotomic polynomial at 2, they are

Besides, if we notice those prime factors, and delete "old prime factors", for example, 3 divides the 2nd, 6th, 18th, 54th, 162nd, ... terms of this sequence, we only allow the 2nd term divided by 3, if we do, they are

The numbers for which is prime are

The numbers for which has an only primitive prime factor are

In computer science, unsigned -bit integers can be used to express numbers up to . Signed -bit integers can express values between and , using the two's complement representation.

In the mathematical problem Tower of Hanoi, solving a puzzle with an -disc tower requires steps, assuming no mistakes are made. The number of rice grains on the whole chessboard in the wheat and chessboard problem is .

The asteroid with minor planet number 8191 is named 8191 Mersenne after Marin Mersenne, because 8191 is a Mersenne prime (3 Juno, 7 Iris, 31 Euphrosyne and 127 Johanna having been discovered and named during the 19th century).

A Mersenne–Fermat number is defined as , with prime, natural number, and can be written as , when , it is a Mersenne number, and when , it is a Fermat number, the only known Mersenne–Fermat prime with are

In fact, , where is the cyclotomic polynomial.

The simplest generalized Mersenne primes are prime numbers of the form , where is a low-degree polynomial with small integer coefficients. An example is , in this case, , and ; another example is , in this case, , and .

It is also natural to try to generalize primes of the form to primes of the form (for and ). However (see also theorems above), is always divisible by , so unless the latter is a unit, the former is not a prime. There are two ways to deal with that:

In the ring of integers (on real numbers), if is a unit, then is either 2 or 0. But are the usual Mersenne primes, and the formula does not lead to anything interesting (since it is always −1 for all ). Thus, we can regard a ring of "integers" on complex numbers instead of real numbers, like Gaussian integers and Eisenstein integers.

If we regard the ring of Gaussian integers, we get the case and , and can ask (WLOG) for what the number is a "Gaussian prime" which will then be called a Gaussian Mersenne prime.

This sequence is in many ways similar to the list of exponents of ordinary Mersenne primes.

The norms (i.e. squares of absolute values) of these Gaussian primes are rational primes:

We can also regard the ring of Eisenstein integers, we get the case and , and can ask for what the number is an "Eisenstein prime" which will then be called a Eisenstein Mersenne prime.

The norms (i.e. squares of absolute values) of these Eisenstein primes are rational primes:

The other way to deal with the fact that is always divisible by , it is to simply take out this factor and ask which values of make
be prime. (The integer can be either positive or negative.) If for example we take , we get values of:
These primes are called repunit primes. Another example is when we take , we get values of:
It is a conjecture that for every integer which is not a perfect power, there are infinitely many values of such that is prime. (When is a perfect power, it can be shown that there is at most one value such that is prime)

Least such that is prime are (starting with , if no such exists)

For negative bases , they are (starting with , if no such exists)

Least base such that is prime are

For negative bases , they are

Another generalized Mersenne number is
with , any coprime integers, and . (Since is always divisible by , the division is necessary for there to be any chance of finding prime numbers. In fact, this number is the same as the Lucas number , since and are the roots of the quadratic equation , and this number equals 1 when ) We can ask which makes this number prime. It can be shown that such must be primes themselves or equal to 4, and can be 4 if and only if and is prime. (Since . Thus, in this case the pair must be and must be prime. That is, must be in .) It is a conjecture that for any pair such that for every natural number , and are not both perfect th powers, and is not a perfect fourth power. there are infinitely many values of such that is prime. (When and are both perfect th powers for an or when is a perfect fourth power, it can be shown that there are at most two values with this property, since if so, then can be factored algebraically) However, this has not been proved for any single value of .

Note: if and is even, then the numbers are not included in the corresponding OEIS sequence.

A conjecture related to the generalized Mersenne primes: (the conjecture predicts where is the next generalized Mersenne prime, if the conjecture is true, then there are infinitely many primes for all such pairs)

For any integers and which satisfy the conditions:

has prime numbers of the form

for prime , the prime numbers will be distributed near the best fit line

where

and there are about

prime numbers of this form less than .


We also have the following three properties:


If this conjecture is true, then for all such pairs, let be the th prime of the form , the graph of versus is almost linear. (See )

When , it is , a difference of two consecutive perfect th powers, and if is prime, then must be , because it is divisible by .

Least such that is prime are

Least such that is prime are




</doc>
<doc id="18909" url="https://en.wikipedia.org/wiki?curid=18909" title="Magnesium">
Magnesium

Magnesium is a chemical element with symbol Mg and atomic number 12. It is a shiny gray solid which bears a close physical resemblance to the other five elements in the second column (group 2, or alkaline earth metals) of the periodic table: all group 2 elements have the same electron configuration in the outer electron shell and a similar crystal structure.

Magnesium is the ninth most abundant element in the universe. It is produced in large, aging stars from the sequential addition of three helium nuclei to a carbon nucleus. When such stars explode as supernovas, much of the magnesium is expelled into the interstellar medium where it may recycle into new star systems. Magnesium is the eighth most abundant element in the Earth's crust and the fourth most common element in the Earth (after iron, oxygen and silicon), making up 13% of the planet's mass and a large fraction of the planet's mantle. It is the third most abundant element dissolved in seawater, after sodium and chlorine.

Magnesium occurs naturally only in combination with other elements, where it invariably has a +2 oxidation state. The free element (metal) can be produced artificially, and is highly reactive (though in the atmosphere, it is soon coated in a thin layer of oxide that partly inhibits reactivity — see passivation). The free metal burns with a characteristic brilliant-white light. The metal is now obtained mainly by electrolysis of magnesium salts obtained from brine, and is used primarily as a component in aluminium-magnesium alloys, sometimes called "magnalium" or "magnelium". Magnesium is less dense than aluminium, and the alloy is prized for its combination of lightness and strength.

Magnesium is the eleventh most abundant element by mass in the human body and is essential to all cells and some 300 enzymes. Magnesium ions interact with polyphosphate compounds such as ATP, DNA, and RNA. Hundreds of enzymes require magnesium ions to function. Magnesium compounds are used medicinally as common laxatives, antacids (e.g., milk of magnesia), and to stabilize abnormal nerve excitation or blood vessel spasm in such conditions as eclampsia.

Elemental magnesium is a gray-white lightweight metal, two-thirds the density of aluminium. It tarnishes slightly when exposed to air, although, unlike the other alkaline earth metals, an oxygen-free environment is unnecessary for storage because magnesium is protected by a thin layer of oxide that is fairly impermeable and difficult to remove. Magnesium has the lowest melting () and the lowest boiling point of all the alkaline earth metals.

Magnesium reacts with water at room temperature, though it reacts much more slowly than calcium, a similar group 2 metal. When submerged in water, hydrogen bubbles form slowly on the surface of the metal—though, if powdered, it reacts much more rapidly. The reaction occurs faster with higher temperatures (see safety precautions). Magnesium's reversible reaction with water can be harnessed to store energy and run a magnesium-based engine.

Magnesium also reacts exothermically with most acids such as hydrochloric acid (HCl), producing the metal chloride and hydrogen gas, similar to the HCl reaction with aluminium, zinc, and many other metals.

Magnesium is highly flammable, especially when powdered or shaved into thin strips, though it is difficult to ignite in mass or bulk. Flame temperatures of magnesium and magnesium alloys can reach , although flame height above the burning metal is usually less than . Once ignited, such fires are difficult to extinguish, because combustion continues in nitrogen (forming magnesium nitride), carbon dioxide (forming magnesium oxide and carbon), and water (forming magnesium oxide and hydrogen). This property was used in incendiary weapons during the firebombing of cities in World War II, where the only practical civil defense was to smother a burning flare under dry sand to exclude atmosphere from the combustion.

Magnesium may also be used as an igniter for thermite, a mixture of aluminium and iron oxide powder that ignites only at a very high temperature.

When burning in air, magnesium produces a brilliant-white light that includes strong ultraviolet wavelengths. Magnesium powder (flash powder) was used for subject illumination in the early days of photography. Later, magnesium filament was used in electrically ignited single-use photography flashbulbs. Magnesium powder is used in fireworks and marine flares where a brilliant white light is required. It was also used for various theatrical effects, such as lightning, pistol flashes, and supernatural appearances.

Magnesium is the eighth-most-abundant element in the Earth's crust by mass and tied in seventh place with iron in molarity. It is found in large deposits of magnesite, dolomite, and other minerals, and in mineral waters, where magnesium ion is soluble.

Although magnesium is found in more than 60 minerals, only dolomite, magnesite, brucite, carnallite, talc, and olivine are of commercial importance.

The cation is the second-most-abundant cation in seawater (about ⅛ the mass of sodium ions in a given sample), which makes seawater and sea salt attractive commercial sources for Mg. To extract the magnesium, calcium hydroxide is added to seawater to form magnesium hydroxide precipitate.

Magnesium hydroxide (brucite) is insoluble in water and can be filtered out and reacted with hydrochloric acid to produced concentrated magnesium chloride.
From magnesium chloride, electrolysis produces magnesium.

As of 2013, magnesium alloy consumption was less than one million tons per year, compared with 50 million tons of aluminum alloys. Its use has been historically limited by its tendency to corrode, creep at high temperatures, and combust.

The presence of iron, nickel, copper, and cobalt strongly activates corrosion. Greater than a very small percentage, these metals precipitate as intermetallic compounds, and the precipitate locales function as active cathodic sites that reduce water, causing the loss of magnesium. Controlling the quantity of these metals improves corrosion resistance. Sufficient manganese overcomes the corrosive effects of iron. This requires precise control over composition, increasing costs. Adding a cathodic poison captures atomic hydrogen within the structure of a metal. This prevents the formation of free hydrogen gas, an essential factor of corrosive chemical processes. The addition of about one in three hundred parts arsenic reduces its corrosion rate in a salt solution by a factor of nearly ten.

Research showed that magnesium's tendency to creep at high-temperatures is eliminated by the addition of scandium and gadolinium. Flammability is greatly reduced by a small amount of calcium in the alloy.

Magnesium forms a variety of compounds important to industry and biology, including magnesium carbonate, magnesium chloride, magnesium citrate, magnesium hydroxide (milk of magnesia), magnesium oxide, magnesium sulfate, and magnesium sulfate heptahydrate (Epsom salts).

Magnesium has three stable isotopes: , and . All are present in significant amounts (see table of isotopes above). About 79% of Mg is . The isotope is radioactive and in the 1950s to 1970s was produced by several nuclear power plants for use in scientific experiments. This isotope has a relatively short half-life (21 hours) and its use was limited by shipping times.

The nuclide has found application in isotopic geology, similar to that of aluminium. is a radiogenic daughter product of , which has a half-life of 717,000 years. Excessive quantities of stable have been observed in the Ca-Al-rich inclusions of some carbonaceous chondrite meteorites. This anomalous abundance is attributed to the decay of its parent in the inclusions, and researchers conclude that such meteorites were formed in the solar nebula before the had decayed. These are among the oldest objects in the solar system and contain preserved information about its early history.

It is conventional to plot / against an Al/Mg ratio. In an isochron dating plot, the Al/Mg ratio plotted is/. The slope of the isochron has no age significance, but indicates the initial / ratio in the sample at the time when the systems were separated from a common reservoir.

China is the dominant supplier of magnesium, with approximately 80% of the world market share. China is almost completely reliant on the silicothermic Pidgeon process (the reduction of the oxide at high temperatures with silicon, often provided by a ferrosilicon alloy in which the iron is but a spectator in the reactions) to obtain the metal. The process can also be carried out with carbon at approx 2300 °C:

In the United States, magnesium is obtained principally with the Dow process, by electrolysis of fused magnesium chloride from brine and sea water. A saline solution containing ions is first treated with lime (calcium oxide) and the precipitated magnesium hydroxide is collected:

The hydroxide is then converted to a partial hydrate of magnesium chloride by treating the hydroxide with hydrochloric acid and heating of the product:

The salt is then electrolyzed in the molten state. At the cathode, the ion is reduced by two electrons to magnesium metal:

At the anode, each pair of ions is oxidized to chlorine gas, releasing two electrons to complete the circuit:

A new process, solid oxide membrane technology, involves the electrolytic reduction of MgO. At the cathode, ion is reduced by two electrons to magnesium metal. The electrolyte is yttria-stabilized zirconia (YSZ). The anode is a liquid metal. At the YSZ/liquid metal anode is oxidized. A layer of graphite borders the liquid metal anode, and at this interface carbon and oxygen react to form carbon monoxide. When silver is used as the liquid metal anode, there is no reductant carbon or hydrogen needed, and only oxygen gas is evolved at the anode. It has been reported that this method provides a 40% reduction in cost per pound over the electrolytic reduction method. This method is more environmentally sound than others because there is much less carbon dioxide emitted.

The United States has traditionally been the major world supplier of this metal, supplying 45% of world production even as recently as 1995. Today, the US market share is at 7%, with a single domestic producer left, US Magnesium, a Renco Group company in Utah born from now-defunct Magcorp.

The name magnesium originates from the Greek word for a district in Thessaly called Magnesia. It is related to magnetite and manganese, which also originated from this area, and required differentiation as separate substances. See manganese for this history.

In 1618, a farmer at Epsom in England attempted to give his cows water from a well there. The cows refused to drink because of the water's bitter taste, but the farmer noticed that the water seemed to heal scratches and rashes. The substance became known as Epsom salts and its fame spread. It was eventually recognized as hydrated magnesium sulfate, ·7.

The metal itself was first isolated by Sir Humphry Davy in England in 1808. He used electrolysis on a mixture of magnesia and mercuric oxide. Antoine Bussy prepared it in coherent form in 1831. Davy's first suggestion for a name was magnium, but the name magnesium is now used.

Magnesium is the third-most-commonly-used structural metal, following iron and aluminium.

The main applications of magnesium are, in order: aluminium alloys, die-casting (alloyed with zinc), removing sulfur in the production of iron and steel, and the production of titanium in the Kroll process.

Magnesium is used in super-strong, lightweight materials and alloys. For example, when infused with silicon carbide nanoparticles, it has extremely high specific strength.

Historically, magnesium was one of the main aerospace construction metals and was used for German military aircraft as early as World War I and extensively for German aircraft in World War II.

The Germans coined the name "Elektron" for magnesium alloy, a term which is still used today. In the commercial aerospace industry, magnesium was generally restricted to engine-related components, due fire and corrosion hazards. Currently, magnesium alloy use in aerospace is increasing, driven by the importance of fuel economy. Development and testing of new magnesium alloys continues, notably Elektron 21, which (in test) has proved suitable for aerospace engine, internal, and airframe components. The European Community runs three R&D magnesium projects in the Aerospace priority of Six Framework Program.

In the form of thin ribbons, magnesium is used to purify solvents; for example, preparing super-dry ethanol.


Both AJ62A and AE44 are recent developments in high-temperature low-creep magnesium alloys. The general strategy for such alloys is to form intermetallic precipitates at the grain boundaries, for example by adding mischmetal or calcium. New alloy development and lower costs that make magnesium competitive with aluminium will increase the number of automotive applications.

Because of low weight and good mechanical and electrical properties, magnesium is widely used for manufacturing of mobile phones, laptop and tablet computers, cameras, and other electronic components.

Magnesium, being readily available and relatively nontoxic, has a variety of uses:

Magnesium metal and its alloys can be explosive hazards; they are highly flammable in their pure form when molten or in powder or ribbon form. Burning or molten magnesium reacts violently with water. When working with powdered magnesium, safety glasses with eye protection and UV filters (such as welders use) are employed because burning magnesium produces ultraviolet light that can permanently damage the retina of a human eye.

Magnesium is capable of reducing water and releasing highly flammable hydrogen gas:

Therefore, water cannot extinguish magnesium fires. The hydrogen gas produced intensifies the fire. Dry sand is an effective smothering agent, but only on relatively level and flat surfaces.

Magnesium reacts with carbon dioxide exothermically to form magnesium oxide and carbon:

Hence, carbon dioxide fuels rather than extinguishes magnesium fires.

Burning magnesium can be quenched by using a Class D dry chemical fire extinguisher, or by covering the fire with sand or magnesium foundry flux to remove its air source.

Magnesium compounds, primarily magnesium oxide (MgO), are used as a refractory material in furnace linings for producing iron, steel, nonferrous metals, glass, and cement. Magnesium oxide and other magnesium compounds are also used in the agricultural, chemical, and construction industries. Magnesium oxide from calcination is used as an electrical insulator in fire-resistant cables.

Magnesium reacted with an alkyl halide gives a Grignard reagent, which is a very useful tool for preparing alcohols.

Magnesium salts are included in various foods, fertilizers (magnesium is a component of chlorophyll), and microbe culture media.

Magnesium sulfite is used in the manufacture of paper (sulfite process).

Magnesium phosphate is used to fireproof wood used in construction.

Magnesium hexafluorosilicate is used for moth-proofing textiles.

The important interaction between phosphate and magnesium ions makes magnesium essential to the basic nucleic acid chemistry of all cells of all known living organisms. More than 300 enzymes require magnesium ions for their catalytic action, including all enzymes using or synthesizing ATP and those that use other nucleotides to synthesize DNA and RNA. The ATP molecule is normally found in a chelate with a magnesium ion.

 Spices, nuts, cereals, cocoa and vegetables are rich sources of magnesium. Green leafy vegetables such as spinach are also rich in magnesium.

In the UK, the recommended daily values for magnesium are 300 mg for men and 270 mg for women. In the U.S. the Recommended Dietary Allowances (RDAs) are 400 mg for men ages 19–30 and 420 mg for older; for women 310 mg for ages 19–30 and 320 mg for older.

Numerous pharmaceutical preparations of magnesium and dietary supplements are available. In two human trials magnesium oxide, one of the most common forms in magnesium dietary supplements because of its high magnesium content per weight, was less bioavailable than magnesium citrate, chloride, lactate or aspartate.

An adult has 22–26 grams of magnesium, with 60% in the skeleton, 39% intracellular (20% in skeletal muscle), and 1% extracellular. Serum levels are typically 0.7–1.0 mmol/L or 1.8–2.4 mEq/L. Serum magnesium levels may be normal even when intracellular magnesium is deficient. The mechanisms for maintaining the magnesium level in the serum are varying gastrointestinal absorption and renal excretion. Intracellular magnesium is correlated with intracellular potassium. Increased magnesium lowers calcium and can either prevent hypercalcemia or cause hypocalcemia depending on the initial level. Both low and high protein intake conditions inhibit magnesium absorption, as does the amount of phosphate, phytate, and fat in the gut. Unabsorbed dietary magnesium is excreted in feces; absorbed magnesium is excreted in urine and sweat.

Magnesium status may be assessed by measuring serum and erythrocyte magnesium concentrations coupled with urinary and fecal magnesium content, but intravenous magnesium loading tests are more accurate and practical. A retention of 20% or more of the injected amount indicates deficiency. No biomarker has been established for magnesium.

Magnesium concentrations in plasma or serum may be monitored for efficacy and safety in those receiving the drug therapeutically, to confirm the diagnosis in potential poisoning victims, or to assist in the forensic investigation in a case of fatal overdose. The newborn children of mothers who received parenteral magnesium sulfate during labor may exhibit toxicity with normal serum magnesium levels.

Low plasma magnesium (hypomagnesemia) is common: it is found in 2.5–15% of the general population. The primary cause of deficiency is low dietary intake: less than 10% of people in the United States meet the recommended dietary allowance. Other causes are increased renal or gastrointestinal loss, an increased intracellular shift, and proton-pump inhibitor antacid therapy. Most are asymptomatic, but symptoms referable to neuromuscular, cardiovascular, and metabolic dysfunction may occur. Alcoholism is often associated with magnesium deficiency. Chronically low serum magnesium levels are associated with metabolic syndrome, diabetes mellitus type 2, fasciculation, and hypertension.


Sorted by type of magnesium salt, other therapeutic applications include:

Overdose from dietary sources alone is unlikely because excess magnesium in the blood is promptly filtered by the kidneys, and overdose is more likely in the presence of impaired renal function. In spite of this, megadose therapy has caused death in a young child, and severe hypermagnesemia in a woman and a young girl who had healthy kidneys.
The most common symptoms of overdose are nausea, vomiting, and diarrhea; other symptoms include hypotension, confusion, slowed heart and respiratory rate, deficiencies of other minerals, coma, cardiac arrhythmia, and death from cardiac arrest.

Plants require magnesium to synthesize chlorophyll, essential for photosynthesis. Magnesium in the center of the porphyrin ring in chlorophyll functions in a manner similar to the iron in the center of the porphyrin ring in heme. Magnesium deficiency in plants causes late-season yellowing between leaf veins, especially in older leaves, and can be corrected by either applying epsom salts (which is rapidly leached), or crushed dolomitic limestone, to the soil.



</doc>
<doc id="18910" url="https://en.wikipedia.org/wiki?curid=18910" title="Markup language">
Markup language

In computer text processing, a markup language is a system for annotating a document in a way that is syntactically distinguishable from the text. The idea and terminology evolved from the "marking up" of paper manuscripts, i.e., the revision instructions by editors, traditionally written with a blue pencil on authors' manuscripts. In digital media, this "blue pencil instruction text" was replaced by tags, that is, instructions are expressed directly by tags or "instruction text encapsulated by tags." However the whole idea of a mark up language is to avoid the formatting work for the text, as the tags in the mark up language serve the purpose to format the appropriate text (like a header or beginning of a next para...etc.). Every tag used in a Markup language has a property to format the text we write.

Examples include typesetting instructions such as those found in troff, TeX and LaTeX, or structural markers such as XML tags. Markup instructs the software that displays the text to carry out appropriate actions, but is omitted from the version of the text that users see.

Some markup languages, such as the widely used HTML, have pre-defined presentation semantics—meaning that their specification prescribes how to present the structured data. Others, such as XML, do not have them and are general purpose.

HyperText Markup Language (HTML), one of the document formats of the World Wide Web, is an instance of Standard Generalized Markup Language or SGML, and follows many of the markup conventions used in the publishing industry in the communication of printed work between authors, editors, and printers.

The term "markup" is derived from the traditional publishing practice of ""marking up"" a manuscript, which involves adding handwritten annotations in the form of conventional symbolic printer's instructions in the margins and text of a paper manuscript or printed. It is computer jargon used in coding proof. For centuries, this task was done primarily by skilled typographers known as "markup men" or "copy markers" who marked up text to indicate what typeface, style, and size should be applied to each part, and then passed the manuscript to others for typesetting by hand. Markup was also commonly applied by editors, proofreaders, publishers, and graphic designers, and indeed by document authors.

There are three main general categories of electronic markup:




There is considerable blurring of the lines between the types of markup. In modern word-processing systems, presentational markup is often saved in descriptive-markup-oriented systems such as XML, and then processed procedurally by implementations. The programming constructs in procedural-markup systems such as TeX may be used to create higher-level markup systems that are more descriptive, such as LaTeX.

In recent years, a number of small and largely unstandardized markup languages have been developed to allow authors to create formatted text via web browsers, for use in wikis and web forums. These are sometimes called lightweight markup languages. Markdown or the markup language used by Wikipedia are examples of such wiki markup.

The first well-known public presentation of markup languages in computer text processing was made by William W. Tunnicliffe at a conference in 1967, although he preferred to call it "generic coding." It can be seen as a response to the emergence of programs such as RUNOFF that each used their own control notations, often specific to the target typesetting device. In the 1970s, Tunnicliffe led the development of a standard called GenCode for the publishing industry and later was the first chair of the International Organization for Standardization committee that created SGML, the first standard descriptive markup language. Book designer Stanley Rice published speculation along similar lines in 1970. Brian Reid, in his 1980 dissertation at Carnegie Mellon University, developed the theory and a working implementation of descriptive markup in actual use.

However, IBM researcher Charles Goldfarb is more commonly seen today as the "father" of markup languages. Goldfarb hit upon the basic idea while working on a primitive document management system intended for law firms in 1969, and helped invent IBM GML later that same year. GML was first publicly disclosed in 1973.

In 1975, Goldfarb moved from Cambridge, Massachusetts to Silicon Valley and became a product planner at the IBM Almaden Research Center. There, he convinced IBM's executives to deploy GML commercially in 1978 as part of IBM's Document Composition Facility product, and it was widely used in business within a few years.

SGML, which was based on both GML and GenCode, was developed by Goldfarb in 1974. Goldfarb eventually became chair of the SGML committee. SGML was first released by ISO as the ISO 8879 standard in October 1986.

Some early examples of computer markup languages available outside the publishing industry can be found in typesetting tools on Unix systems such as troff and nroff. In these systems, formatting commands were inserted into the document text so that typesetting software could format the text according to the editor's specifications. It was a trial and error iterative process to get a document printed correctly. Availability of WYSIWYG ("what you see is what you get") publishing software supplanted much use of these languages among casual users, though serious publishing work still uses markup to specify the non-visual structure of texts, and WYSIWYG editors now usually save documents in a markup-language-based format.

Another major publishing standard is TeX, created and refined by Donald Knuth in the 1970s and '80s. TeX concentrated on detailed layout of text and font descriptions to typeset mathematical books. This required Knuth to spend considerable time investigating the art of typesetting. TeX is mainly used in academia, where it is a "de facto" standard in many scientific disciplines. A TeX macro package known as LaTeX provides a descriptive markup system on top of TeX, and is widely used.

The first language to make a clean distinction between structure and presentation was Scribe, developed by Brian Reid and described in his doctoral thesis in 1980. Scribe was revolutionary in a number of ways, not least that it introduced the idea of styles separated from the marked up document, and of a grammar controlling the usage of descriptive elements. Scribe influenced the development of Generalized Markup Language (later SGML) and is a direct ancestor to HTML and LaTeX .

In the early 1980s, the idea that markup should be focused on the structural aspects of a document and leave the visual presentation of that structure to the interpreter led to the creation of SGML. The language was developed by a committee chaired by Goldfarb. It incorporated ideas from many different sources, including Tunnicliffe's project, GenCode. Sharon Adler, Anders Berglund, and James A. Marke were also key members of the SGML committee.

SGML specified a syntax for including the markup in documents, as well as one for separately describing "what" tags were allowed, and "where" (the Document Type Definition (DTD) or schema). This allowed authors to create and use any markup they wished, selecting tags that made the most sense to them and were named in their own natural languages. Thus, SGML is properly a meta-language, and many particular markup languages are derived from it. From the late '80s on, most substantial new markup languages have been based on SGML system, including for example TEI and DocBook. SGML was promulgated as an International Standard by International Organization for Standardization, ISO 8879, in 1986.

SGML found wide acceptance and use in fields with very large-scale documentation requirements. However, many found it cumbersome and difficult to learn—a side effect of its design attempting to do too much and be too flexible. For example, SGML made end tags (or start-tags, or even both) optional in certain contexts, because its developers thought markup would be done manually by overworked support staff who would appreciate saving keystrokes.

In 1989, computer scientist Sir Tim Berners-Lee wrote a memo proposing an Internet-based hypertext system, then specified HTML and wrote the browser and server software in the last part of 1990. The first publicly available description of HTML was a document called "HTML Tags", first mentioned on the Internet by Berners-Lee in late 1991. It describes 18 elements comprising the initial, relatively simple design of HTML. Except for the hyperlink tag, these were strongly influenced by SGMLguid, an in-house SGML-based documentation format at CERN. Eleven of these elements still exist in HTML 4.

Berners-Lee considered HTML an SGML application. The Internet Engineering Task Force (IETF) formally defined it as such with the mid-1993 publication of the first proposal for an HTML specification: "Hypertext Markup Language (HTML)" Internet-Draft by Berners-Lee and Dan Connolly, which included an SGML Document Type Definition to define the grammar. Many of the HTML text elements are found in the 1988 ISO technical report TR 9537 "Techniques for using SGML", which in turn covers the features of early text formatting languages such as that used by the RUNOFF command developed in the early 1960s for the CTSS (Compatible Time-Sharing System) operating system. These formatting commands were derived from those used by typesetters to manually format documents. Steven DeRose argues that HTML's use of descriptive markup (and influence of SGML in particular) was a major factor in the success of the Web, because of the flexibility and extensibility that it enabled. HTML became the main markup language for creating web pages and other information that can be displayed in a web browser, and is quite likely the most used markup language in the world today.

XML (Extensible Markup Language) is a meta markup language that is now widely used. XML was developed by the World Wide Web Consortium, in a committee created and chaired by Jon Bosak. The main purpose of XML was to simplify SGML by focusing on a particular problem—documents on the Internet. XML remains a meta-language like SGML, allowing users to create any tags needed (hence "extensible") and then describing those tags and their permitted uses.

XML adoption was helped because every XML document can be written in such a way that it is also an SGML document, and existing SGML users and software could switch to XML fairly easily. However, XML eliminated many of the more complex and human-oriented features of SGML to simplify implementation environments such as documents and publications. However, it appeared to strike a happy medium between simplicity and flexibility, and was rapidly adopted for many other uses. XML is now widely used for communicating data between applications.

Since January 2000, all W3C Recommendations for HTML have been based on XML rather than SGML, using the abbreviation XHTML (Extensible HyperText Markup Language). The language specification requires that XHTML Web documents must be "well-formed" XML documents. This allows for more rigorous and robust documents while using tags familiar from HTML.

One of the most noticeable differences between HTML and XHTML is the rule that "all tags must be closed": empty HTML tags such as codice_2 must either be "closed" with a regular end-tag, or replaced by a special form: (the space before the 'codice_3' on the end tag is optional, but frequently used because it enables some pre-XML Web browsers, and SGML parsers, to accept the tag). Another is that all attribute values in tags must be quoted. Finally, all tag and attribute names within the XHTML namespace must be lowercase to be valid. HTML, on the other hand, was case-insensitive.

Many XML-based applications now exist, including the Resource Description Framework as RDF/XML, XForms, DocBook, SOAP, and the Web Ontology Language (OWL). For a partial list of these, see List of XML markup languages.

A common feature of many markup languages is that they intermix the text of a document with markup instructions in the same data stream or file. This is not necessary; it is possible to isolate markup from text content, using pointers, offsets, IDs, or other methods to co-ordinate the two. Such "standoff markup" is typical for the internal representations that programs use to work with marked-up documents. However, embedded or "inline" markup is much more common elsewhere. Here, for example, is a small section of text marked up in HTML:

The codes enclosed in angle-brackets codice_4 are markup instructions (known as tags), while the text between these instructions is the actual text of the document. The codes codice_5, codice_6, and codice_7 are examples of "semantic" markup, in that they describe the intended purpose or meaning of the text they include. Specifically, codice_5 means "this is a first-level heading", codice_6 means "this is a paragraph", and codice_7 means "this is an emphasized word or phrase". A program interpreting such structural markup may apply its own rules or styles for presenting the various pieces of text, using different typefaces, boldness, font size, indentation, colour, or other styles, as desired.
A tag such as "h1" (header level 1) might be presented in a large bold sans-serif typeface, for example, or in a monospaced (typewriter-style) document it might be underscored – or it might not change the presentation at all.

In contrast, the codice_11 tag in HTML is an example of "presentational" markup; it is generally used to specify a particular characteristic of the text (in this case, the use of an italic typeface) without specifying the reason for that appearance.

The Text Encoding Initiative (TEI) has published extensive guidelines for how to encode texts of interest in the humanities and social sciences, developed through years of international cooperative work. These guidelines are used by projects encoding historical documents, the works of particular scholars, periods, or genres, and so on.

While the idea of markup language originated with text documents, there is increasing use of markup languages in the presentation of other types of information, including playlists, vector graphics, web services, content syndication, and user interfaces. Most of these are XML applications, because XML is a well-defined and extensible language.

The use of XML has also led to the possibility of combining multiple markup languages into a single profile, like XHTML+SMIL and XHTML+MathML+SVG.

Because markup languages, and more generally data description languages (not necessarily textual markup), are not programming languages (they are data without instructions), they are more easily manipulated than programming languages—for example, web pages are presented as HTML documents, not C code, and thus can be embedded within other web pages, displayed when only partially received, and so forth. This leads to the web design principle of the rule of least power, which advocates using the "least" (computationally) powerful language that satisfies a task to facilitate such manipulation and reuse.



</doc>
<doc id="18916" url="https://en.wikipedia.org/wiki?curid=18916" title="Meaning">
Meaning

Meaning my refer to:





</doc>
<doc id="18917" url="https://en.wikipedia.org/wiki?curid=18917" title="Meta-ethics">
Meta-ethics

Meta-ethics is the branch of ethics that seeks to understand the nature of ethical properties, statements, attitudes, and judgments. Meta-ethics is one of the three branches of ethics generally studied by philosophers, the others being normative ethics and applied ethics.

While normative ethics addresses such questions as "What should I do?", thus endorsing some ethical evaluations and rejecting others, meta-ethics addresses questions such as "What "is" goodness?" and "How can we tell what is good from what is bad?", seeking to understand the nature of ethical properties and evaluations.

Some theorists argue that a metaphysical account of morality is necessary for the proper evaluation of actual moral theories and for making practical moral decisions; others reason from opposite premises and suggest that studying moral judgments about proper actions can guide us to a true account of the nature of morality.

According to Richard Garner and Bernard Rosen, there are three kinds of meta-ethical problems, or three general questions:
A question of the first type might be, "What do the words 'good', 'bad', 'right' and 'wrong' mean?" (see value theory). The second category includes questions of whether moral judgments are universal or relative, of one kind or many kinds, etc. Questions of the third kind ask, for example, how we can know if something is right or wrong, if at all. Garner and Rosen say that answers to the three basic questions "are not unrelated, and sometimes an answer to one will strongly suggest, or perhaps even entail, an answer to another."

A meta-ethical theory, unlike a normative ethical theory, does not attempt to evaluate specific choices as being better, worse, good, bad, or evil; although it may have profound implications as to the validity and meaning of normative ethical claims. An answer to any of the three example questions above would not itself be a normative ethical statement.

These theories mainly put forward a position on the first of the three questions above, "What is the meaning of moral terms or judgments?" They may however imply or even entail answers to the other two questions as well.


Yet another way of categorizing meta-ethical theories is to distinguish between centralist and non-centralist theories. The debate between centralism and non-centralism revolves around the relationship between the so-called "thin" and "thick" concepts of morality. Thin moral concepts are those such as good, bad, right, and wrong; thick moral concepts are those such as courageous, inequitable, just, or dishonest. While both sides agree that the thin concepts are more general and the thick more specific, centralists hold that the thin concepts are antecedent to the thick ones and that the latter are therefore dependent on the former. That is, centralists argue that one must understand words like "right" and "ought" before understanding words like "just" and "unkind." Non-centralism rejects this view, holding that thin and thick concepts are on par with one another and even that the thick concepts are a sufficient starting point for understanding the thin ones.

Non-centralism has been of particular importance to ethical naturalists in the late 20th and early 21st centuries as part of their argument that normativity is a non-excisable aspect of language and that there is no way of analyzing thick moral concepts into a purely descriptive element attached to a thin moral evaluation, thus undermining any fundamental division between facts and norms. Allan Gibbard, R. M. Hare, and Simon Blackburn have argued in favor of the fact/norm distinction, meanwhile, with Gibbard going so far as to argue that, even if conventional English has only mixed normative terms (that is, terms that are neither purely descriptive nor purely normative), we could develop a nominally English metalanguage that still allowed us to maintain the division between factual descriptions and normative evaluations.

These theories attempt to answer the second of the above questions: "What is the nature of moral judgments?"


These are theories that attempt to answer questions like, "How may moral judgments be supported or defended?" or "Why should I be moral?"

If one presupposes a cognitivist interpretation of moral sentences, morality is justified by the moralist's knowledge of moral facts, and the theories to justify moral judgements are epistemological theories.





</doc>
<doc id="18921" url="https://en.wikipedia.org/wiki?curid=18921" title="Montesquieu (disambiguation)">
Montesquieu (disambiguation)

Montesquieu (1689–1755) was a French lawyer, man of letters, and political philosopher.

Montesquieu may also refer to:


</doc>
<doc id="18925" url="https://en.wikipedia.org/wiki?curid=18925" title="Mormons">
Mormons

Mormons are a religious and cultural group related to Mormonism, the principal branch of the Latter Day Saint movement of Restorationist Christianity, initiated by Joseph Smith in upstate New York during the 1820s. After Smith's death in 1844, the Mormons followed Brigham Young to what would become the Utah Territory. Today, most Mormons are understood to be members of The Church of Jesus Christ of Latter-day Saints (LDS Church). Some Mormons are also either independent or non-practicing. The center of Mormon cultural influence is in Utah, and North America has more Mormons than any other continent, though the majority of Mormons live outside the United States.

Mormons have developed a strong sense of commonality that stems from their doctrine and history. During the 19th century, Mormon converts tended to gather to a central geographic location, and between 1852 and 1890 a minority of Mormons openly practiced plural marriage, a form of religious polygamy. Mormons dedicate large amounts of time and resources to serving in their church, and many young Mormons choose to serve a full-time proselytizing mission. Mormons have a health code which eschews alcoholic beverages, tobacco, coffee, tea, and other addictive substances. They tend to be very family-oriented and have strong connections across generations and with extended family, reflective of their belief that families can be sealed together beyond death. Mormons also have a strict law of chastity, requiring abstention from sexual relations outside heterosexual marriage and fidelity within marriage.

Mormons self-identify as Christian, although some non-Mormons consider Mormons non-Christian and some of their beliefs differ from mainstream Christianity. Mormons believe in the Bible, as well as other books of scripture, such as the Book of Mormon. They have a unique view of cosmology and believe that all people are spirit-children of God. Mormons believe that returning to God requires following the example of Jesus Christ, and accepting his atonement through ordinances such as baptism. They believe that Christ's church was restored through Joseph Smith and is guided by living prophets and apostles. Central to Mormon faith is the belief that God speaks to his children and answers their prayers.

Due to their high birth and conversion rates, the Mormon population has grown significantly in recent decades. The number of members in 1971 was 3,090,953 and as of 2018, there are 16,118,169 members worldwide.

The word "Mormons" most often refers to members of The Church of Jesus Christ of Latter-day Saints (LDS Church) because of their belief in the Book of Mormon, though members often refer to themselves as "Latter-day Saints" or sometimes just "Saints". The term "Mormons" has been embraced by most adherents of Mormonism, most notably Mormon fundamentalists, while other Latter Day Saint denominations, such as the Community of Christ, have rejected it. Both LDS Church members (or "Latter-day Saints") and members of fundamentalist groups commonly use the word "Mormon" in reference to themselves. The LDS Church, however, disagrees with this self-characterization, and encourages the use of the word "Mormon" only in reference to LDS Church members. Church leaders also encourage members to use the church's full name to emphasize its focus on Jesus Christ.

The word "Mormon" is often associated with polygamy (or plural marriage), which was a distinguishing practice of many early Mormons; however, it was renounced by the LDS Church in 1890
and discontinued over the next 15 years.
Today, polygamy is practiced within Mormonism only by people that have broken with the LDS Church.

The history of the Mormons has shaped them into a people with a strong sense of unity and commonality. From the start, Mormons have tried to establish what they call "Zion", a utopian society of the righteous.
Mormon history can be divided into three broad time periods: (1) the early history during the lifetime of Joseph Smith, (2) a "pioneer era" under the leadership of Brigham Young and his successors, and (3) a modern era beginning around the turn of the 20th century. In the first period, Smith had tried literally to build a city called Zion, in which converts could gather. During the pioneer era, Zion became a "landscape of villages" in Utah. In modern times, Zion is still an ideal, though Mormons gather together in their individual congregations rather than a central geographic location.

Mormons trace their origins to the visions that Joseph Smith reported having in the early 1820s while living in upstate New York. In 1823, Smith said an angel directed him to a buried book written on golden plates containing the religious history of an ancient people. Smith published what he said was a translation of these plates in March 1830 as the Book of Mormon, named after Mormon, the ancient prophet–historian who compiled the book. On April 6, 1830, Smith founded the Church of Christ. The early church grew westward as Smith sent missionaries to proselytize. In 1831, the church moved to Kirtland, Ohio where missionaries had made a large number of converts and Smith began establishing an outpost in Jackson County, Missouri, where he planned to eventually build the city of Zion (or the New Jerusalem). In 1833, Missouri settlers, alarmed by the rapid influx of Mormons, expelled them from Jackson County into the nearby Clay County, where local residents were more welcoming.
After Smith led a mission, known as Zion's Camp, to recover the land, he began building Kirtland Temple in Lake County, Ohio, where the church flourished. When the Missouri Mormons were later asked to leave Clay County in 1836, they secured land in what would become Caldwell County.

The Kirtland era ended in 1838, after the failure of a church-sponsored anti-bank caused widespread defections, and Smith regrouped with the remaining church in Far West, Missouri. During the fall of 1838, tensions escalated into the Mormon War with the old Missouri settlers. On October 27, the governor of Missouri ordered that the Mormons "must be treated as enemies" and be exterminated or driven from the state. Between November and April, some eight thousand displaced Mormons migrated east into Illinois.

In 1839, the Mormons purchased the small town of Commerce, converted swampland on the banks of the Mississippi River, and renamed the area Nauvoo, Illinois and began construction of the Nauvoo Temple. The city became the church's new headquarters and gathering place, and it grew rapidly, fueled in part by converts immigrating from Europe. Meanwhile, Smith introduced temple ceremonies meant to seal families together for eternity, as well as the doctrines of eternal progression or exaltation, and plural marriage.
Smith created a service organization for women called the Relief Society, as well as an organization called the Council of Fifty, representing a future theodemocratic "Kingdom of God" on the earth.
Smith also published the story of his First Vision, in which the Father and the Son appeared to him while he was about 14 years old.
This vision would come to be regarded by some Mormons as the most important event in human history after the birth, ministry, and resurrection of Jesus Christ.

In 1844, local prejudices and political tensions, fueled by Mormon peculiarity and internal dissent, escalated into conflicts between Mormons and "anti-Mormons". On June 27, 1844, Smith and his brother Hyrum were killed by a mob in Carthage, Illinois. Because Hyrum was Smith's logical successor, their deaths caused a succession crisis, and Brigham Young assumed leadership over the majority of Latter Day Saints. Young had been a close associate of Smith's and was senior apostle of the Quorum of the Twelve. Smaller groups of Latter Day Saints followed other leaders to form other denominations of the Latter Day Saint movement.

For two years after Smith's death, conflicts escalated between Mormons and other Illinois residents. To prevent war, Brigham Young led the Mormon pioneers (constituting most of the Latter Day Saints) to a temporary winter quarters in Nebraska and then, eventually (beginning in 1847), to what became the Utah Territory. Having failed to build Zion within the confines of American society, the Mormons began to construct a society in isolation, based on their beliefs and values. The cooperative ethic that Mormons had developed over the last decade and a half became important as settlers branched out and colonized a large desert region now known as the Mormon Corridor. Colonizing efforts were seen as religious duties, and the new villages were governed by the Mormon bishops (local lay religious leaders). The Mormons viewed land as commonwealth, devising and maintaining a co-operative system of irrigation that allowed them to build a farming community in the desert.

From 1849 to 1852, the Mormons greatly expanded their missionary efforts, establishing several missions in Europe, Latin America, and the South Pacific. Converts were expected to "gather" to Zion, and during Young's presidency (1847–77) over seventy thousand Mormon converts immigrated to America. Many of the converts came from England and Scandinavia, and were quickly assimilated into the Mormon community. Many of these immigrants crossed the Great Plains in wagons drawn by oxen, while some later groups pulled their possessions in small handcarts. During the 1860s, newcomers began using the new railroad that was under construction.

In 1852, church leaders publicized the previously secret practice of plural marriage, a form of polygamy. Over the next 50 years, many Mormons (between 20 and 30 percent of Mormon families) entered into plural marriages as a religious duty, with the number of plural marriages reaching a peak around 1860, and then declining through the rest of the century. Besides the doctrinal reasons for plural marriage, the practice made some economic sense, as many of the plural wives were single women who arrived in Utah without brothers or fathers to offer them societal support.

By 1857, tensions had again escalated between Mormons and other Americans, largely as a result of accusations involving polygamy and the theocratic rule of the Utah Territory by Brigham Young. In 1857, U.S. President James Buchanan sent an army to Utah, which Mormons interpreted as open aggression against them. Fearing a repeat of Missouri and Illinois, the Mormons prepared to defend themselves, determined to torch their own homes in the case that they were invaded. The relatively peaceful Utah War ensued from 1857 to 1858, in which the most notable instance of violence was the Mountain Meadows massacre, when leaders of a local Mormon militia ordered the killing of a civilian emigrant party that was traveling through Utah during the escalating tensions. In 1858, Young agreed to step down from his position as governor and was replaced by a non-Mormon, Alfred Cumming. Nevertheless, the LDS Church still wielded significant political power in the Utah Territory.

At Young's death in 1877, he was followed by other LDS Church presidents, who resisted efforts by the United States Congress to outlaw Mormon polygamous marriages. In 1878, the U.S. Supreme Court ruled in "Reynolds v. United States" that religious duty was not a suitable defense for practicing polygamy, and many Mormon polygamists went into hiding; later, Congress began seizing church assets. In September 1890, church president Wilford Woodruff issued a Manifesto that officially suspended the practice of polygamy. Although this Manifesto did not dissolve existing plural marriages, relations with the United States markedly improved after 1890, such that Utah was admitted as a U.S. state in 1896. After the Manifesto, some Mormons continued to enter into polygamous marriages, but these eventually stopped in 1904 when church president Joseph F. Smith disavowed polygamy before Congress and issued a "Second Manifesto" calling for all plural marriages in the church to cease. Eventually, the church adopted a policy of excommunicating members found practicing polygamy, and today seeks actively to distance itself from "fundamentalist" groups that continue the practice.

During the early 20th century, Mormons began to reintegrate into the American mainstream. In 1929, the Mormon Tabernacle Choir began broadcasting a weekly performance on national radio, becoming an asset for public relations. Mormons emphasized patriotism and industry, rising in socioeconomic status from the bottom among American religious denominations to middle-class.
In the 1920s and 1930s, Mormons began migrating out of Utah, a trend hurried by the Great Depression, as Mormons looked for work wherever they could find it. As Mormons spread out, church leaders created programs that would help preserve the tight-knit community feel of Mormon culture. In addition to weekly worship services, Mormons began participating in numerous programs such as Boy Scouting, a Young Women organization, church-sponsored dances, ward basketball, camping trips, plays, and religious education programs for youth and college students. During the Great Depression, the church started a welfare program to meet the needs of poor members, which has since grown to include a humanitarian branch that provides relief to disaster victims.

During the later half of the 20th century, there was a retrenchment movement in Mormonism in which Mormons became more conservative, attempting to regain their status as a "peculiar people".
Though the 1960s and 1970s brought changes such as Women's Liberation and the civil rights movement, Mormon leaders were alarmed by the erosion of traditional values, the sexual revolution, the widespread use of recreational drugs, moral relativism, and other forces they saw as damaging to the family.
Partly to counter this, Mormons put an even greater emphasis on family life, religious education, and missionary work, becoming more conservative in the process. As a result, Mormons today are probably less integrated with mainstream society than they were in the early 1960s.

Although black people have been members of Mormon congregations since Joseph Smith's time, before 1978, black membership was small. From 1852 to 1978, the LDS Church enforced a policy that restricted men of black African descent from being ordained to the church's lay priesthood. The church was sharply criticized for its policy during the civil rights movement, but the policy remained in force until a 1978 reversal that was prompted in part by questions about mixed-race converts in Brazil. In general, Mormons greeted the change with joy and relief. Since 1978, black membership has grown, and in 1997 there were approximately 500,000 black members of the church (about 5 percent of the total membership), mostly in Africa, Brazil and the Caribbean. Black membership has continued to grow substantially, especially in West Africa, where two temples have been built. Many black Mormons are members of the Genesis Group, an organization of black members that predates the priesthood ban, and is endorsed by the church.

The LDS Church grew rapidly after World War II and became a worldwide organization as missionaries were sent across the globe. The church doubled in size every 15 to 20 years, and by 1996, there were more Mormons outside the United States than inside. In 2012, there were an estimated 14.8 million Mormons, with roughly 57 percent living outside the United States. It is estimated that approximately 4.5 million Mormons – roughly 30% of the total membership – regularly attend services. A majority of U.S. Mormons are white and non-Hispanic (84 percent). Most Mormons are distributed in North and South America, the South Pacific, and Western Europe. The global distribution of Mormons resembles a contact diffusion model, radiating out from the organization's headquarters in Utah. The church enforces general doctrinal uniformity, and congregations on all continents teach the same doctrines, and international Mormons tend to absorb a good deal of Mormon culture, possibly because of the church's top-down hierarchy and a missionary presence. However, international Mormons often bring pieces of their own heritage into the church, adapting church practices to local cultures.

Chile, Uruguay, and several areas in the South Pacific have a higher percentage of Mormons than the United States (which is at about 2 percent). South Pacific countries and dependencies that are more than 10 percent Mormon include American Samoa, the Cook Islands, Kiribati, Niue, Samoa, and Tonga.

Isolation in Utah had allowed Mormons to create a culture of their own. As the faith spread around the world, many of its more distinctive practices followed. Mormon converts are urged to undergo lifestyle changes, repent of sins, and adopt sometimes foreign standards of conduct. Practices common to Mormons include studying scriptures, praying daily, fasting regularly, attending Sunday worship services, participating in church programs and activities on weekdays, and refraining from work on Sundays when possible. The most important part of the church services is considered to be the Lord's Supper (commonly called sacrament), in which church members renew covenants made at baptism. Mormons also emphasize standards they believe were taught by Jesus Christ, including personal honesty, integrity, obedience to law, chastity outside marriage and fidelity within marriage.

In 2010, around 13–14 percent of Mormons lived in Utah, the center of cultural influence for Mormonism. Utah Mormons (as well as Mormons living in the Intermountain West) are on average more culturally and/or politically conservative than those living in some cosmopolitan centers elsewhere in the U.S. Utahns self-identifying as Mormon also attend church somewhat more on average than Mormons living in other states. (Nonetheless, whether they live in Utah or elsewhere in the U.S., Mormons tend to be more culturally and/or politically conservative than members of other U.S. religious groups.) Utah Mormons often place a greater emphasis on pioneer heritage than international Mormons who generally are not descendants of the Mormon pioneers.

Mormons have a strong sense of communality that stems from their doctrine and history. LDS Church members have a responsibility to dedicate their time and talents to helping the poor and building the church. The church is divided by locality into congregations called "wards", with several wards or branches to create a "stake". The vast majority of church leadership positions are lay positions, and church leaders may work 10 to 15 hours a week in unpaid church service. Observant Mormons also contribute 10 percent of their income to the church as tithing, and are often involved in humanitarian efforts. Many LDS young men, women and elderly couples choose to serve a proselytizing mission, during which they dedicate all of their time to the church, without pay.

Mormons adhere to the Word of Wisdom, a health law or code that is interpreted as prohibiting the consumption of tobacco, alcohol, coffee and tea, while encouraging the use of herbs, grains, fruits, and a moderate consumption of meat. The Word of Wisdom is also understood to forbid other harmful and addictive substances and practices, such as the use of illegal drugs and abuse of prescription drugs. Mormons are encouraged to keep a year's supplies that include a food supply and a financial reserve. Mormons also oppose behaviors such as viewing pornography and gambling.

The concept of a united family that lives and progresses forever is at the core of Latter-day Saint doctrine, and Mormons place a high importance on family life. Many Mormons hold weekly Family Home Evenings, in which an evening is set aside for family bonding, study, prayer and other activities they consider to be wholesome. Latter-day Saint fathers who hold the priesthood typically name and bless their children shortly after birth to formally give the child a name. Mormon parents hope and pray that their children will gain testimonies of the "gospel" so they can grow up and marry in temples.

Mormons have a strict law of chastity, requiring abstention from sexual relations outside opposite-sex marriage and strict fidelity within marriage. All sexual activity (heterosexual and homosexual) outside marriage is considered a serious sin, with marriage recognized as only between a man and a woman. Same-sex marriages are not performed or supported by the LDS Church. Church members are encouraged to marry and have children, and Latter-day Saint families tend to be larger than average. Mormons are opposed to abortion, except in some exceptional circumstances, such as when pregnancy is the result of incest or rape, or when the life or health of the mother is in serious jeopardy. Many practicing adult Mormons wear religious undergarments that remind them of covenants and encourage them to dress modestly. Latter-day Saints are counseled not to partake of any form of media that is obscene or pornographic in any way, including media that depicts graphic representations of sex or violence. Tattoos and body piercings are also discouraged, with the exception of a single pair of earrings for LDS women.

LGBT Mormons, or Mormons who self-identify as gay, lesbian, or bisexual, remain in good standing in the church if they abstain from homosexual relations and obey the law of chastity. While there are no official numbers, LDS Family Services estimates that there are on average four or five members per LDS ward who experience same-sex attraction. Gary Watts, former president of Family Fellowship, estimates that only 10 percent of homosexuals stay in the church. Many of these individuals have come forward through different support groups or websites discussing their homosexual attractions and concurrent church membership.

Note that the categories below are not necessary mutually exclusive.

Members of the LDS Church, also known as Latter-day Saints, constitute over 95 percent of Mormons. The beliefs and practices of LDS Mormons are generally guided by the teachings of LDS Church leaders. However, several smaller groups substantially differ from "mainstream" Mormonism in various ways.

LDS Church members who do not actively participate in worship services or church callings are often called "less-active" or "inactive" (akin to the qualifying expressions "non-observant" or "non-practicing" used in relation to members of other religious groups). The LDS Church does not release statistics on church activity, but it is likely that about 40 percent of Mormons in the United States and 30 percent worldwide regularly attend worship services. Reasons for inactivity can include lifestyle issues and problems with social integration. Activity rates tend to vary with age, and disengagement occurs most frequently between age 16 and 25. A majority of less active members return to church activity later in life. Former Latter-day Saints who seek to disassociate themselves from the religion are often referred to as ex-Mormons.

Members of sects that broke with the LDS Church over the issue of polygamy have become known as fundamentalist Mormons; these groups differ from mainstream Mormonism primarily in their belief in and practice of plural marriage. There are thought to be between 20,000 and 60,000 members of fundamentalist sects, (0.1–0.4 percent of Mormons), with roughly half of them practicing polygamy. There are a number of fundamentalist sects, the largest two being the Fundamentalist Church of Jesus Christ of Latter-Day Saints (FLDS Church) and the Apostolic United Brethren (AUB). In addition to plural marriage, some of these groups also practice a form of Christian communalism known as the law of consecration or the United Order. The LDS Church seeks to distance itself from all such polygamous groups, excommunicating their members if discovered practicing or teaching it, and today a majority of Mormon fundamentalists have never been members of the LDS Church.

Liberal Mormons, also known as Progressive Mormons, take an interpretive approach to LDS teachings and scripture. They look to the scriptures for spiritual guidance, but may not necessarily believe the teachings to be literally or uniquely true. For liberal Mormons, revelation is a process through which God gradually brings fallible human beings to greater understanding. Liberal Mormons place doing good and loving fellow human beings above the importance of believing correctly. In a separate context, members of small progressive breakaway groups have also adopted the label.

Cultural Mormons are individuals who may not believe in certain doctrines or practices of the institutional LDS Church yet identify as Mormon. Usually this is a result of having been raised in the LDS faith, or as having converted and spent a large portion of one's life as an active member of the LDS Church. Cultural Mormons may or may not be actively involved with the LDS church. In some cases they may not be members of the LDS Church.

Mormons have a scriptural canon consisting of the Bible (both Old and New Testaments), the Book of Mormon, and a collection of revelations and writings by Joseph Smith known as the Doctrine and Covenants and Pearl of Great Price. Mormons, however, have a relatively open definition of scripture. As a general rule, anything spoken or written by a prophet, while under inspiration, is considered to be the word of God. Thus, the Bible, written by prophets and apostles, is the word of God, so far as it is translated correctly. The Book of Mormon is also believed to have been written by ancient prophets, and is viewed as a companion to the Bible. By this definition, the teachings of Smith's successors are also accepted as scripture, though they are always measured against, and draw heavily from the scriptural canon.

Mormons believe in "a friendly universe", governed by a God whose aim it is to bring his children to immortality and eternal life. Mormons have a unique perspective on the nature of God, the origin of man, and the purpose of life. For instance, Mormons believe in a pre-mortal existence where people were literal spirit children of God, and that God presented a plan of salvation that would allow his children to progress and become more like him. The plan involved the spirits receiving bodies on earth and going through trials in order to learn, progress, and receive a "fulness of joy". The most important part of the plan involved Jesus, the eldest of God's children, coming to earth as the literal Son of God, to conquer sin and death so that God's other children could return. According to Mormons, every person who lives on earth will be resurrected, and nearly all of them will be received into various kingdoms of glory. To be accepted into the highest kingdom, a person must fully accept Christ through faith, repentance, and through ordinances such as baptism and the laying on of hands.

According to Mormons, a deviation from the original principles of Christianity, known as the Great Apostasy, began not long after the ascension of Jesus Christ. It was marked with the corruption of Christian doctrine by Greek and other philosophies, with followers dividing into different ideological groups. Mormons claim the martyrdom of the Apostles led to a loss of Priesthood authority to administer the church and its ordinances.
Mormons believe that God restored the early Christian church through Joseph Smith. In particular, Mormons believe that angels such as Peter, James, John, John the Baptist, Moses, and Elijah appeared to Smith and others and bestowed various priesthood authorities on them. Mormons believe that their church is the "only true and living church" because of the divine authority restored through Smith. Mormons self-identify as being Christian, while many Christians, particularly evangelical Protestants, disagree with this view. Mormons view other religions as having portions of the truth, doing good works, and having genuine value.

Though the LDS Church has a top-down hierarchical structure with a president–prophet dictating revelations for the whole church, there is a bottom-up aspect as well. Ordinary Mormons have access to the same inspiration that is thought to guide their prophets, and are encouraged to seek their own personal revelations. Mormons see Joseph Smith's First Vision as proof that the heavens are open, and that God answers prayers. They place considerable emphasis on "asking God" to find out if something is true. Most Mormons do not claim to have had heavenly visions like Smith's in response to prayers, but feel that God talks to them in their hearts and minds through the Holy Ghost. Though Mormons have some beliefs that are considered strange in a modernized world, they continue to hold onto their beliefs because they feel God has spoken to them.



</doc>
<doc id="18926" url="https://en.wikipedia.org/wiki?curid=18926" title="Manitoba">
Manitoba

Manitoba () is a province at the longitudinal centre of Canada. It is often considered one of the three prairie provinces (with Alberta and Saskatchewan) and is Canada's fifth-most populous province with its estimated 1.3 million people. Manitoba covers with a widely varied landscape, stretching from the northern oceanic coastline to the southern border with the United States. The province is bordered by the provinces of Ontario to the east and Saskatchewan to the west, the territories of Nunavut to the north, and Northwest Territories to the northwest, and the US states of North Dakota and Minnesota to the south.

Aboriginal peoples have inhabited what is now Manitoba for thousands of years. In the late 17th century, fur traders arrived in the area when it was part of Rupert's Land and owned by the Hudson's Bay Company. In 1869, negotiations for the creation of the province of Manitoba led to an armed uprising of the Métis people against the Government of Canada, a conflict known as the Red River Rebellion. The rebellion's resolution led to the Parliament of Canada passing the Manitoba Act in 1870 that created the province.

Manitoba's capital and largest city, Winnipeg, is the eighth-largest census metropolitan area in Canada. Other census agglomerations in the province are Brandon, Steinbach, Portage la Prairie, and Thompson.

The name "Manitoba" is believed to be derived from the Cree, Ojibwe or Assiniboine languages. The name derives from Cree "manitou-wapow" or Ojibwa "manidoobaa", both meaning "straits of Manitou, the Great Spirit", a place referring to what are now called The Narrows in the centre of Lake Manitoba. It may also be from the Assiniboine for "Lake of the Prairie".

The lake was known to French explorers as "Lac des Prairies." Thomas Spence chose the name to refer to a new republic he proposed for the area south of the lake. Métis leader Louis Riel also chose the name, and it was accepted in Ottawa under the Manitoba Act of 1870.

Manitoba is bordered by the provinces of Ontario to the east and Saskatchewan to the west, the territories of Nunavut to the north, and the US states of North Dakota and Minnesota to the south. The province possibly meets the Northwest Territories at the four corners quadripoint to the extreme northwest, though surveys have not been completed and laws are unclear about the exact location of the Nunavut–NWT boundary. Manitoba adjoins Hudson Bay to the northeast, and is the only prairie province to have a saltwater coastline. The Port of Churchill is Canada's only Arctic deep-water port. Lake Winnipeg is the tenth-largest freshwater lake in the world. Hudson Bay is the world's second-largest bay by area. Manitoba is at the heart of the giant Hudson Bay watershed, once known as Rupert's Land. It was a vital area of the Hudson's Bay Company, with many rivers and lakes that provided excellent opportunities for the lucrative fur trade.

The province has a saltwater coastline bordering Hudson Bay and more than 110,000 lakes, covering approximately 15.6 percent or of its surface area. Manitoba's major lakes are Lake Manitoba, Lake Winnipegosis, and Lake Winnipeg, the tenth-largest freshwater lake in the world. Some traditional Native lands and boreal forest on Lake Winnipeg's east side are a proposed UNESCO World Heritage Site.

Manitoba is at the centre of the Hudson Bay drainage basin, with a high volume of the water draining into Lake Winnipeg and then north down the Nelson River into Hudson Bay. This basin's rivers reach far west to the mountains, far south into the United States, and east into Ontario. Major watercourses include the Red, Assiniboine, Nelson, Winnipeg, Hayes, Whiteshell and Churchill rivers. Most of Manitoba's inhabited south has developed in the prehistoric bed of Glacial Lake Agassiz. This region, particularly the Red River Valley, is flat and fertile; receding glaciers left hilly and rocky areas throughout the province.

Baldy Mountain is the province's highest point at above sea level, and the Hudson Bay coast is the lowest at sea level. Riding Mountain, the Pembina Hills, Sandilands Provincial Forest, and the Canadian Shield are also upland regions. Much of the province's sparsely inhabited north and east lie on the irregular granite Canadian Shield, including Whiteshell, Atikaki, and Nopiming Provincial Parks.

Extensive agriculture is found only in the province's southern areas, although there is grain farming in the Carrot Valley Region (near The Pas). The most common agricultural activity is cattle husbandry (34.6%), followed by assorted grains (19.0%) and oilseed (7.9%). Around 12 percent of Canada's farmland is in Manitoba.

Manitoba has an extreme continental climate. Temperatures and precipitation generally decrease from south to north and increase from east to west. Manitoba is far from the moderating influences of mountain ranges or large bodies of water. Because of the generally flat landscape, it is exposed to cold Arctic high-pressure air masses from the northwest during January and February. In the summer, air masses sometimes come out of the Southern United States, as warm humid air is drawn northward from the Gulf of Mexico. Temperatures exceed numerous times each summer, and the combination of heat and humidity can bring the humidex value to the mid-40s. Carman, Manitoba recorded the second-highest humidex ever in Canada in 2007, with 53.0. According to Environment Canada, Manitoba ranked first for clearest skies year round, and ranked second for clearest skies in the summer and for the sunniest province in the winter and spring.
Southern Manitoba (including the city of Winnipeg), falls into the humid continental climate zone (Köppen Dfb). This area is cold and windy in the winter and has frequent blizzards because of the open landscape. Summers are warm with a moderate length. This region is the most humid area in the prairie provinces, with moderate precipitation. Southwestern Manitoba, though under the same climate classification as the rest of Southern Manitoba, is closer to the semi-arid interior of Palliser's Triangle. The area is drier and more prone to droughts than other parts of southern Manitoba. This area is cold and windy in the winter and has frequent blizzards due to the openness of the prairie landscape. Summers are generally warm to hot, with low to moderate humidity.

Southern parts of the province just north of Tornado Alley, experience tornadoes, with 16 confirmed touchdowns in 2016. In 2007, on 22 and 23 June, numerous tornadoes touched down, the largest an F5 tornado that devastated parts of Elie (the strongest recorded tornado in Canada).

The province's northern sections (including the city of Thompson) fall in the subarctic climate zone (Köppen climate classification "Dfc"). This region features long and extremely cold winters and brief, warm summers with little precipitation. Overnight temperatures as low as occur on several days each winter.

Manitoba natural communities may be grouped within five ecozones: boreal plains, prairie, taiga shield, boreal shield and Hudson plains. Three of these—taiga shield, boreal shield and Hudson plain—contain part of the Boreal forest of Canada which covers the province's eastern, southeastern, and northern reaches.

Forests make up about , or 48 percent, of the province's land area. The forests consist of pines (Jack Pine, Red Pine, Eastern White Pine), spruces (White Spruce, Black Spruce), Balsam Fir, Tamarack (larch), poplars (Trembling Aspen, Balsam Poplar), birches (White Birch, Swamp Birch) and small pockets of Eastern White Cedar.

Two sections of the province are not dominated by forest. The province's northeast corner bordering Hudson Bay is above the treeline and is considered tundra. The tallgrass prairie once dominated the south central and southeastern parts including the Red River Valley. Mixed grass prairie is found in the southwestern region. Agriculture has replaced much of the natural prairie but prairie still can be found in parks and protected areas; some are notable for the presence of the endangered western prairie fringed orchid.

Manitoba is especially noted for its northern polar bear population; Churchill is commonly referred to as the "Polar Bear Capital". Other large animals, including moose, white-tailed deer, black bears, cougars, lynx, and wolves, are common throughout the province, especially in the provincial and national parks. There is a large population of red sided garter snakes near Narcisse; the dens there are home to the world's largest concentration of snakes.

Manitoba's bird diversity is enhanced by its position on two major migration routes, with 392 confirmed identified species; 287 of these nesting within the province. These include the great grey owl, the province's official bird, and the endangered peregrine falcon.

Manitoba's lakes host 18 species of game fish, particularly species of trout, pike, and goldeye, as well as many smaller fish.

Modern-day Manitoba was inhabited by the First Nations people shortly after the last ice age glaciers retreated in the southwest about 10,000 years ago; the first exposed land was the Turtle Mountain area. The Ojibwe, Cree, Dene, Sioux, Mandan, and Assiniboine peoples founded settlements, and other tribes entered the area to trade. In Northern Manitoba, quartz was mined to make arrowheads. The first farming in Manitoba was along the Red River, where corn and other seed crops were planted before contact with Europeans.
In 1611, Henry Hudson was one of the first Europeans to sail into what is now known as Hudson Bay, where he was abandoned by his crew. The first European to reach present-day central and southern Manitoba was Sir Thomas Button, who travelled upstream along the Nelson River to Lake Winnipeg in 1612 in an unsuccessful attempt to find and rescue Hudson. When the British ship "Nonsuch" sailed into Hudson Bay in 1668–1669, she became the first trading vessel to reach the area; that voyage led to the formation of the Hudson's Bay Company, to which the British government gave absolute control of the entire Hudson Bay watershed. This watershed was named Rupert's Land, after Prince Rupert, who helped to subsidize the Hudson's Bay Company. York Factory was founded in 1684 after the original fort of the Hudson's Bay Company, Fort Nelson (built in 1682), was destroyed by rival French traders.

Pierre Gaultier de Varennes, sieur de La Vérendrye, visited the Red River Valley in the 1730s to help open the area for French exploration and trade. As French explorers entered the area, a Montreal-based company, the North West Company, began trading with the local Indigenous people. Both the North West Company and the Hudson's Bay Company built fur-trading forts; the two companies competed in southern Manitoba, occasionally resulting in violence, until they merged in 1821 (the Hudson's Bay Company Archives in Winnipeg preserve the history of this era).

Great Britain secured the territory in 1763 after their victory over France in the North American theatre of the Seven Years' War, better known as the French and Indian War in North America; lasting from 1754 to 1763. The founding of the first agricultural community and settlements in 1812 by Lord Selkirk, north of the area which is now downtown Winnipeg, led to conflict between British colonists and the Métis. Twenty colonists, including the governor, and one Métis were killed in the Battle of Seven Oaks in 1816. Thomas Spence attempted to be President of the Republic of Manitobah in 1867, that he and his council named.

Rupert's Land was ceded to Canada by the Hudson's Bay Company in 1869 and incorporated into the Northwest Territories; a lack of attention to Métis concerns caused Métis leader Louis Riel to establish a local provisional government as part of the Red River Rebellion. In response, Prime Minister John A. Macdonald introduced the Manitoba Act in the House of Commons of Canada, the bill was given Royal Assent and Manitoba was brought into Canada as a province in 1870. Louis Riel was pursued by British army officer Garnet Wolseley because of the rebellion, and Riel fled into exile. The Canadian government blocked the Métis' attempts to obtain land promised to them as part of Manitoba's entry into confederation. Facing racism from the new flood of white settlers from Ontario, large numbers of Métis moved to what would become Saskatchewan and Alberta.

Numbered Treaties were signed in the late 19th century with the chiefs of various First Nations that lived in the area. These treaties made specific promises of land for every family. As a result, a reserve system was established under the jurisdiction of the Federal Government. The prescribed amount of land promised to the native peoples was not always given; this led aboriginal groups to assert rights to the land through aboriginal land claims, many of which are still ongoing.

The original province of Manitoba was a square one-eighteenth of its current size, and was known colloquially as the "postage stamp province". Its borders were expanded in 1881, taking land from the Northwest Territories and the District of Keewatin, but Ontario claimed a large portion of the land; the disputed portion was awarded to Ontario in 1889. Manitoba grew to its current size in 1912, absorbing land from the Northwest Territories to reach 60°N, uniform with the northern reach of its western neighbours Saskatchewan, Alberta and British Columbia.

The Manitoba Schools Question showed the deep divergence of cultural values in the territory. The Catholic Franco-Manitobans had been guaranteed a state-supported separate school system in the original constitution of Manitoba, but a grassroots political movement among English Protestants from 1888 to 1890 demanded the end of French schools. In 1890, the Manitoba legislature passed a law removing funding for French Catholic schools. The French Catholic minority asked the federal government for support; however, the Orange Order and other anti-Catholic forces mobilized nationwide to oppose them.

The federal Conservatives proposed remedial legislation to override Manitoba, but they were blocked by the Liberals, led by Wilfrid Laurier, who opposed the remedial legislation because of his belief in provincial rights. Once elected Prime Minister in 1896, Laurier implemented a compromise stating Catholics in Manitoba could have their own religious instruction for 30 minutes at the end of the day if there were enough students to warrant it, implemented on a school-by-school basis.

By 1911, Winnipeg was the third largest city in Canada, and remained so until overtaken by Vancouver in the 1920s. A boomtown, it grew quickly around the start of the 20th century, with outside investors and immigrants contributing to its success. The drop in growth in the second half of the decade was a result of the opening of the Panama Canal in 1914, which reduced reliance on transcontinental railways for trade, as well as a decrease in immigration due to the outbreak of the First World War. Over 18,000 Manitoba residents enlisted in the first year of the war; by the end of the war, 14 Manitobans had received the Victoria Cross.

After the First World War ended, severe discontent among farmers (over wheat prices) and union members (over wage rates) resulted in an upsurge of radicalism, coupled with a polarization over the rise of Bolshevism in Russia. The most dramatic result was the Winnipeg general strike of 1919. It began on 15 May and collapsed on 25 June 1919; as the workers gradually returned to their jobs, the Central Strike Committee decided to end the movement.

Government efforts to violently crush the strike, including a Royal Northwest Mounted Police charge into a crowd of protesters that resulted in multiple casualties and one death, had led to the arrest of the movement's leaders. In the aftermath, eight leaders went on trial, and most were convicted on charges of seditious conspiracy, illegal combinations, and seditious libel; four were aliens who were deported under the Canadian Immigration Act.

The Great Depression (1929–c. 1939) hit especially hard in Western Canada, including Manitoba. The collapse of the world market combined with a steep drop in agricultural production due to drought led to economic diversification, moving away from a reliance on wheat production. The Manitoba Co-operative Commonwealth Federation, forerunner to the New Democratic Party of Manitoba (NDP), was founded in 1932.

Canada entered the Second World War in 1939. Winnipeg was one of the major commands for the British Commonwealth Air Training Plan to train fighter pilots, and there were air training schools throughout Manitoba. Several Manitoba-based regiments were deployed overseas, including Princess Patricia's Canadian Light Infantry. In an effort to raise money for the war effort, the Victory Loan campaign organized "If Day" in 1942. The event featured a simulated Nazi invasion and occupation of Manitoba, and eventually raised over C$65 million.
Winnipeg was inundated during the 1950 Red River Flood and had to be partially evacuated. In that year, the Red River reached its highest level since 1861 and flooded most of the Red River Valley. The damage caused by the flood led then-Premier Duff Roblin to advocate for the construction of the Red River Floodway; it was completed in 1968 after six years of excavation. Permanent dikes were erected in eight towns south of Winnipeg, and clay dikes and diversion dams were built in the Winnipeg area. In 1997, the "Flood of the Century" caused over in damages in Manitoba, but the floodway prevented Winnipeg from flooding.

In 1990, Prime Minister Brian Mulroney attempted to pass the Meech Lake Accord, a series of constitutional amendments to persuade Quebec to endorse the Canada Act 1982. Unanimous support in the legislature was needed to bypass public consultation. Manitoba politician Elijah Harper, a Cree, opposed because he did not believe First Nations had been adequately involved in the Accord's process, and thus the Accord failed.

In 2013, Manitoba was the second province to make accessibility legislation law, protecting the rights of persons with disabilities.

At the 2011 census, Manitoba had a population of 1,208,268, more than half of which is in the Winnipeg Capital Region; Winnipeg is Canada's eighth-largest Census Metropolitan Area, with a population of 730,018 (2011 Census). Although initial colonization of the province revolved mostly around homesteading, the last century has seen a shift towards urbanization; Manitoba is the only Canadian province with over fifty-five percent of its population located in a single city.

According to the 2006 Canadian census, the largest ethnic group in Manitoba is English (22.9%), followed by German (19.1%), Scottish (18.5%), Ukrainian (14.7%), Irish (13.4%), North American Indian (10.6%), Polish (7.3%), Métis (6.4%), French (5.6%), Dutch (4.9%), and Russian (4.0%). Almost one-fifth of respondents also identified their ethnicity as "Canadian". There is a significant indigenous community: aboriginals (including Métis) are Manitoba's fastest-growing ethnic group, representing 13.6 percent of Manitoba's population as of 2001 (some reserves refused to allow census-takers to enumerate their populations or were otherwise incompletely counted). There is a significant Franco-Manitoban minority (148,370) and a growing aboriginal population (192,865, including the Métis). Gimli, Manitoba is home to the largest Icelandic community outside of Iceland.

Most Manitobans belong to a Christian denomination: on the 2001 census, 758,760 Manitobans (68.7%) reported being Christian, followed by 13,040 (1.2%) Jewish, 5,745 (0.5%) Buddhist, 5,485 (0.5%) Sikh, 5,095 (0.5%) Muslim, 3,840 (0.3%) Hindu, 3,415 (0.3%) Aboriginal spirituality and 995 (0.1%) pagan. 201,825 Manitobans (18.3%) reported no religious affiliation. The largest Christian denominations by number of adherents were the Roman Catholic Church with 292,970 (27%); the United Church of Canada with 176,820 (16%); and the Anglican Church of Canada with 85,890 (8%).

Manitoba has a moderately strong economy based largely on natural resources. Its Gross Domestic Product was C$50.834 billion in 2008. The province's economy grew 2.4 percent in 2008, the third consecutive year of growth; in 2009, it neither increased nor decreased. The average individual income in Manitoba in 2006 was C$25,100 (compared to a national average of C$26,500), ranking fifth-highest among the provinces. As of October 2009, Manitoba's unemployment rate was 5.8 percent.

Manitoba's economy relies heavily on agriculture, tourism, energy, oil, mining, and forestry. Agriculture is vital and is found mostly in the southern half of the province, although grain farming occurs as far north as The Pas. Around 12 percent of Canadian farmland is in Manitoba. The most common type of farm found in rural areas is cattle farming (34.6%), followed by assorted grains (19.0%) and oilseed (7.9%).

Manitoba is the nation's largest producer of sunflower seed and dry beans, and one of the leading sources of potatoes. Portage la Prairie is a major potato processing centre, and is home to the McCain Foods and Simplot plants, which provide French fries for McDonald's, Wendy's, and other commercial chains. Can-Oat Milling, one of the largest oat mills in the world, also has a plant in the municipality.

Manitoba's largest employers are government and government-funded institutions, including crown corporations and services like hospitals and universities. Major private-sector employers are The Great-West Life Assurance Company, Cargill Ltd., and James Richardson and Sons Ltd. Manitoba also has large manufacturing and tourism sectors. Churchill's Arctic wildlife is a major tourist attraction; the town is a world capital for polar bear and beluga whale watchers. Manitoba is the only province with an Arctic deep-water seaport, at Churchill.

In January 2018, the Canadian Federation of Independent Business claimed that Manitoba was the most improved province for tackling red tape.

Manitoba's early economy depended on mobility and living off the land. Aboriginal Nations (Cree, Ojibwa, Dene, Sioux and Assiniboine) followed herds of bison and congregated to trade among themselves at key meeting places throughout the province. After the arrival of the first European traders in the 17th century, the economy centred on the trade of beaver pelts and other furs. Diversification of the economy came when Lord Selkirk brought the first agricultural settlers in 1811, though the triumph of the Hudson's Bay Company (HBC) over its competitors ensured the primacy of the fur trade over widespread agricultural colonization.

HBC control of Rupert's Land ended in 1868; when Manitoba became a province in 1870, all land became the property of the federal government, with homesteads granted to settlers for farming. Transcontinental railways were constructed to simplify trade. Manitoba's economy depended mainly on farming, which persisted until drought and the Great Depression led to further diversification.

CFB Winnipeg is a Canadian Forces Base at the Winnipeg International Airport. The base is home to flight operations support divisions and several training schools, as well as the 1 Canadian Air Division and Canadian NORAD Region Headquarters. 17 Wing of the Canadian Forces is based at CFB Winnipeg; the Wing has three squadrons and six schools. It supports 113 units from Thunder Bay to the Saskatchewan/Alberta border, and from the 49th parallel north to the high Arctic. 17 Wing acts as a deployed operating base for CF-18 Hornet fighter–bombers assigned to the Canadian NORAD Region.

The two 17 Wing squadrons based in the city are: the 402 ("City of Winnipeg" Squadron), which flies the Canadian designed and produced de Havilland Canada CT-142 Dash 8 navigation trainer in support of the 1 Canadian Forces Flight Training School's Air Combat Systems Officer and Airborne Electronic Sensor Operator training programs (which trains all Canadian Air Combat Systems Officer); and the 435 ("Chinthe" Transport and Rescue Squadron), which flies the Lockheed C-130 Hercules tanker/transport in airlift search and rescue roles, and is the only Air Force squadron equipped and trained to conduct air-to-air refuelling of fighter aircraft.

Canadian Forces Base Shilo (CFB Shilo) is an Operations and Training base of the Canadian Forces located east of Brandon. During the 1990s, Canadian Forces Base Shilo was designated as an Area Support Unit, acting as a local base of operations for Southwest Manitoba in times of military and civil emergency. CFB Shilo is the home of the 1st Regiment, Royal Canadian Horse Artillery, both battalions of the 1 Canadian Mechanized Brigade Group, and the Royal Canadian Artillery. The Second Battalion of Princess Patricia's Canadian Light Infantry (2 PPCLI), which was originally stationed in Winnipeg (first at Fort Osborne, then in Kapyong Barracks), has operated out of CFB Shilo since 2004. CFB Shilo hosts a training unit, 3rd Canadian Division Training Centre. It serves as a base for support units of 3rd Canadian Division, also including 3 CDSG Signals Squadron, Shared Services Unit (West), 11 CF Health Services Centre, 1 Dental Unit, 1 Military Police Regiment, and an Integrated Personnel Support Centre. The base currently houses 1,700 soldiers.

After the control of Rupert's Land was passed from Great Britain to the Government of Canada in 1869, Manitoba attained full-fledged rights and responsibilities of self-government as the first Canadian province carved out of the Northwest Territories. The Legislative Assembly of Manitoba was established on 14 July 1870. Political parties first emerged between 1878 and 1883, with a two-party system (Liberals and Conservatives). The United Farmers of Manitoba appeared in 1922, and later merged with the Liberals in 1932. Other parties, including the Co-operative Commonwealth Federation (CCF), appeared during the Great Depression; in the 1950s, Manitoban politics became a three-party system, and the Liberals gradually declined in power. The CCF became the New Democratic Party of Manitoba (NDP), which came to power in 1969. Since then, the Progressive Conservatives and the NDP have been the dominant parties.

Like all Canadian provinces, Manitoba is governed by a unicameral legislative assembly. The executive branch is formed by the governing party; the party leader is the premier of Manitoba, the head of the executive branch. The head of state, Queen Elizabeth II, is represented by the Lieutenant Governor of Manitoba, who is appointed by the Governor General of Canada on advice of the Prime Minister. The head of state is primarily a ceremonial role, although the Lieutenant Governor has the official responsibility of ensuring that Manitoba has a duly constituted government.

The Legislative Assembly consists of the 57 Members elected to represent the people of Manitoba. The premier of Manitoba is Brian Pallister of the PC Party. The PCs were elected with a majority government of 40 seats. The NDP holds 14 seats, and the Liberal Party have three seats but does not have official party status in the Manitoba Legislature. The last provincial general election was held on 19 April 2016. The province is represented in federal politics by 14 Members of Parliament and six Senators.

Manitoba's judiciary consists of the Court of Appeal, the Court of Queen's Bench, and the Provincial Court. The Provincial Court is primarily for criminal law; 95 percent of criminal cases in Manitoba are heard here. The Court of Queen's Bench is the highest trial court in the province. It has four jurisdictions: family law (child and family services cases), civil law, criminal law (for indictable offences), and appeals. The Court of Appeal hears appeals from both benches; its decisions can only be appealed to the Supreme Court of Canada.

English and French are the official languages of the legislature and courts of Manitoba, according to §23 of the Manitoba Act, 1870 (part of the Constitution of Canada). In April 1890, the Manitoba legislature attempted to abolish the official status of French, and ceased to publish bilingual legislation. However, in 1985 the Supreme Court of Canada ruled in the Reference re Manitoba Language Rights that §23 still applied, and that legislation published only in English was invalid (unilingual legislation was declared valid for a temporary period to allow time for translation).

Although French is an official language for the purposes of the legislature, legislation, and the courts, the Manitoba Act does not require it to be an official language for the purpose of the executive branch (except when performing legislative or judicial functions). Hence, Manitoba's government is not completely bilingual. The Manitoba French Language Services Policy of 1999 is intended to provide a comparable level of provincial government services in both official languages. According to the 2006 Census, 82.8 percent of Manitoba's population spoke only English, 3.2 percent spoke only French, 15.1 percent spoke both, and 0.9 percent spoke neither.

In 2010, the provincial government of Manitoba passed the Aboriginal Languages Recognition Act, which gives official recognition to seven indigenous languages: Cree, Dakota, Dene, Inuktitut, Michif, Ojibway and Oji-Cree.

Transportation and warehousing contribute approximately to Manitoba's GDP. Total employment in the industry is estimated at 34,500, or around 5 percent of Manitoba's population. Trucks haul 95 percent of land freight in Manitoba, and trucking companies account for 80 percent of Manitoba's merchandise trade to the United States. Five of Canada's twenty-five largest employers in for-hire trucking are headquartered in Manitoba. of Manitoba's GDP comes directly or indirectly from trucking.

Greyhound Canada and Grey Goose Bus Lines offer domestic bus service from the Winnipeg Bus Terminal. The terminal was relocated from downtown Winnipeg to the airport in 2009, and is a Greyhound hub. Municipalities also operate localized transit bus systems.

Manitoba has two Class I railways: Canadian National Railway (CN) and Canadian Pacific Railway (CPR). Winnipeg is centrally located on the main lines of both carriers, and both maintain large inter-modal terminals in the city. CN and CPR operate a combined of track in Manitoba. Via Rail offers transcontinental and Northern Manitoba passenger service from Winnipeg's Union Station. Numerous small regional and short-line railways also run trains within Manitoba: the Hudson Bay Railway, the Southern Manitoba Railway, Burlington Northern Santa Fe Manitoba, Greater Winnipeg Water District Railway, and Central Manitoba Railway. Together, these smaller lines operate approximately of track in the province.
Winnipeg James Armstrong Richardson International Airport, Manitoba's largest airport, is one of only a few 24-hour unrestricted airports in Canada and is part of the National Airports System. A new, larger terminal opened in October 2011. The airport handles approximately of cargo annually, making it the third largest cargo airport in the country.

Eleven regional passenger airlines and nine smaller and charter carriers operate out of the airport, as well as eleven air cargo carriers and seven freight forwarders. Winnipeg is a major sorting facility for both FedEx and Purolator, and receives daily trans-border service from UPS. Air Canada Cargo and Cargojet Airways use the airport as a major hub for national traffic.

The Port of Churchill, owned by OmniTRAX, is the only Arctic deep-water port in Canada. It is nautically closer to ports in Northern Europe and Russia than any other port in Canada. It has four deep-sea berths for the loading and unloading of grain, general cargo and tanker vessels. The port is served by the Hudson Bay Railway (also owned by OmniTRAX). Grain represented 90 percent of the port's traffic in the 2004 shipping season. In that year, over of agricultural products were shipped through the port.

The first school in Manitoba was founded in 1818 by Roman Catholic missionaries in present-day Winnipeg; the first Protestant school was established in 1820. A provincial board of education was established in 1871; it was responsible for public schools and curriculum, and represented both Catholics and Protestants. The Manitoba Schools Question led to funding for French Catholic schools largely being withdrawn in favour of the English Protestant majority. Legislation making education compulsory for children between seven and fourteen was first enacted in 1916, and the leaving age was raised to sixteen in 1962.

Public schools in Manitoba fall under the regulation of one of thirty-seven school divisions within the provincial education system (except for the Manitoba Band Operated Schools, which are administered by the federal government). Public schools follow a provincially mandated curriculum in either French or English. There are sixty-five funded independent schools in Manitoba, including three boarding schools. These schools must follow the Manitoban curriculum and meet other provincial requirements. There are forty-four non-funded independent schools, which are not required to meet those standards.

There are five universities in Manitoba, regulated by the Ministry of Advanced Education and Literacy. Four of these universities are in Winnipeg: the University of Manitoba, the largest and most comprehensive; the University of Winnipeg, a liberal arts school primarily focused on undergrad studies located downtown; Université de Saint-Boniface, the province's only French-language university; and the Canadian Mennonite University, a religious-based institution. The Université de Saint-Boniface, established in 1818 and now affiliated with the University of Manitoba, is the oldest university in Western Canada. Brandon University, formed in 1899 and located in Brandon, is the province's only university not in Winnipeg.

Manitoba has thirty-eight public libraries; of these, twelve have French-language collections and eight have significant collections in other languages. Twenty-one of these are part of the Winnipeg Public Library system. The first lending library in Manitoba was founded in 1848.

Manitoba's culture has been influenced by traditional (Aboriginal and Métis) and modern Canadian artistic values, as well as by the cultures of its immigrant populations and American neighbours. The Minister of Culture, Heritage, Tourism and Sport is responsible for promoting and, to some extent, financing Manitoban culture. Manitoba is the birthplace of the Red River Jig, a combination of aboriginal pow-wows and European reels popular among early settlers. Manitoba's traditional music has strong roots in Métis and Aboriginal culture, in particular the old-time fiddling of the Métis. Manitoba's cultural scene also incorporates classical European traditions. The Winnipeg-based Royal Winnipeg Ballet (RWB), is Canada's oldest ballet and North America's longest continuously operating ballet company; it was granted its royal title in 1953 under Queen Elizabeth II. The Winnipeg Symphony Orchestra (WSO) performs classical music and new compositions at the Centennial Concert Hall. Manitoba Opera, founded in 1969, also performs out of the Centennial Concert Hall.
Le Cercle Molière (founded 1925) is the oldest French-language theatre in Canada, and Royal Manitoba Theatre Centre (founded 1958) is Canada's oldest English-language regional theatre. Manitoba Theatre for Young People was the first English-language theatre to win the Canadian Institute of the Arts for Young Audiences Award, and offers plays for children and teenagers as well as a theatre school. The Winnipeg Art Gallery (WAG), Manitoba's largest art gallery and the sixth largest in the country, hosts an art school for children; the WAG's permanent collection comprises over twenty thousand works, with a particular emphasis on Manitoban and Canadian art.

The 1960s pop group The Guess Who was formed in Manitoba, and later became the first Canadian band to have a No. 1 hit in the United States; Guess Who guitarist Randy Bachman later created Bachman–Turner Overdrive (BTO) with fellow Winnipeg-based musician Fred Turner. Fellow rocker Neil Young, lived for a time in Manitoba, played with Stephen Stills in Buffalo Springfield, and again in supergroup Crosby, Stills, Nash & Young. Soft-rock band Crash Test Dummies formed in the late 1980s in Winnipeg and were the 1992 Juno Awards Group of the Year.

Several prominent Canadian films were produced in Manitoba, such as "The Stone Angel", based on the Margaret Laurence book of the same title, "The Saddest Music in the World", "Foodland", "For Angela", and "My Winnipeg". Major films shot in Manitoba include "The Assassination of Jesse James by the Coward Robert Ford" and "Capote", both of which received Academy Award nominations. "Falcon Beach", an internationally broadcast television drama, was filmed at Winnipeg Beach, Manitoba.

Manitoba has a strong literary tradition. Manitoban writer Bertram Brooker won the first-ever Governor General's Award for Fiction in 1936. Cartoonist Lynn Johnston, author of the comic strip "For Better or For Worse", was nominated for a Pulitzer Prize and inducted into the Canadian Cartoonist Hall of Fame. Margaret Laurence's "The Stone Angel" and "A Jest of God" were set in Manawaka, a fictional town representing Neepawa; the latter title won the Governor General's Award in 1966. Carol Shields won both the Governor General's Award and the Pulitzer Prize for "The Stone Diaries". Gabrielle Roy, a Franco-Manitoban writer, won the Governor General's Award three times. A quote from her writings is featured on the Canadian $20 bill.

Festivals take place throughout the province, with the largest centred in Winnipeg. The inaugural Winnipeg Folk Festival was held in 1974 as a one-time celebration to mark Winnipeg's 100th anniversary. Today, the five-day festival is one of the largest folk festivals in North America with over 70 acts from around the world and an annual attendance that exceeds 80,000. The Winnipeg Folk Festival's home – Birds Hill Provincial Park – is located 34 kilometres outside of Winnipeg and for the five days of the festival, it becomes Manitoba's third largest "city." The Festival du Voyageur is an annual ten-day event held in Winnipeg's French Quarter, and is Western Canada's largest winter festival. It celebrates Canada's fur-trading past and French-Canadian heritage and culture. Folklorama, a multicultural festival run by the Folk Arts Council, receives around 400,000 pavilion visits each year, of which about thirty percent are from non-Winnipeg residents. The Winnipeg Fringe Theatre Festival is an annual alternative theatre festival, the second-largest festival of its kind in North America (after the Edmonton International Fringe Festival).

Manitoban museums document different aspects of the province's heritage. The Manitoba Museum is the largest museum in Manitoba and focuses on Manitoban history from prehistory to the 1920s. The full-size replica of the Nonsuch is the museum's showcase piece. The Manitoba Children's Museum at The Forks presents exhibits for children. There are two museums dedicated to the native flora and fauna of Manitoba: the Living Prairie Museum, a tall grass prairie preserve featuring 160 species of grasses and wildflowers, and FortWhyte Alive, a park encompassing prairie, lake, forest and wetland habitats, home to a large herd of bison. The Canadian Fossil Discovery Centre houses the largest collection of marine reptile fossils in Canada. Other museums feature the history of aviation, marine transport, and railways in the area. The Canadian Museum for Human Rights is the first Canadian national museum outside of the National Capital Region.

Winnipeg has two daily newspapers: the "Winnipeg Free Press", a broadsheet with the highest circulation numbers in Manitoba, as well as the "Winnipeg Sun", a smaller tabloid-style paper. There are several ethnic weekly newspapers, including the weekly French-language "La Liberté", and regional and national magazines based in the city. Brandon has two newspapers: the daily "Brandon Sun" and the weekly "Wheat City Journal". Many small towns have local newspapers.

There are five English-language television stations and one French-language station based in Winnipeg. The Global Television Network (owned by Canwest) is headquartered in the city. Winnipeg is home to twenty-one AM and FM radio stations, two of which are French-language stations. Brandon's five local radio stations are provided by Astral Media and Westman Communications Group. In addition to the Brandon and Winnipeg stations, radio service is provided in rural areas and smaller towns by Golden West Broadcasting, Corus Entertainment, and local broadcasters. CBC Radio broadcasts local and national programming throughout the province. Native Communications is devoted to Aboriginal programming and broadcasts to many of the isolated native communities as well as to larger cities.
Manitoba has four professional sports teams: the Winnipeg Blue Bombers (Canadian Football League), the Winnipeg Jets (National Hockey League), the Manitoba Moose (American Hockey League), and the Winnipeg Goldeyes (American Association). The province was previously home to another team called the Winnipeg Jets, which played in the World Hockey Association and National Hockey League from 1972 until 1996, when financial troubles prompted a sale and move of the team, renamed the Phoenix Coyotes. A second incarnation of the Winnipeg Jets returned, after True North Sports & Entertainment bought the Atlanta Thrashers and moved the team to Winnipeg in time for the 2011 hockey season. Manitoba has one major junior-level hockey team, the Western Hockey League's Brandon Wheat Kings, and one junior football team, the Winnipeg Rifles of the Canadian Junior Football League.

The province is represented in university athletics by the University of Manitoba Bisons, the University of Winnipeg Wesmen, and the Brandon University Bobcats. All three teams compete in the Canada West Universities Athletic Association (the regional division of Canadian Interuniversity Sport).

Curling is an important winter sport in the province with Manitoba producing more men's national champions than any other province, while additionally in the top 3 women's national champions, as well as multiple world champions in the sport. The province also hosts the world's largest curling tournament in the MCA Bonspiel. The province is regular host to Grand Slam events which feature as the largest cash events in the sport such as the annual Manitoba Lotteries Women's Curling Classic as well as other rotating events.

Though not as prominent as hockey and curling, long track speed skating also features as a notable and top winter sport in Manitoba. The province has produced some of the world's best female speed skaters including Susan Auch and the country's top Olympic medal earners Cindy Klassen and Clara Hughes.




</doc>
<doc id="18929" url="https://en.wikipedia.org/wiki?curid=18929" title="Mount Logan">
Mount Logan

Mount Logan is the highest mountain in Canada and the second-highest peak in North America, after Denali. The mountain was named after Sir William Edmond Logan, a Canadian geologist and founder of the Geological Survey of Canada (GSC). Mount Logan is located within Kluane National Park Reserve in Southwestern Yukon, less than north of the Yukon–Alaska border. Mount Logan is the source of the Hubbard and Logan Glaciers. Logan is believed to have the largest base circumference of any non-volcanic mountain on Earth (a large number of shield volcanoes are much larger in size and mass), including a massif with eleven peaks over .

Due to active tectonic uplifting, Mount Logan is still rising in height. Before 1992, the exact elevation of Mount Logan was unknown and measurements ranged from . In May 1992, a GSC expedition climbed Mount Logan and fixed the current height of using GPS.

Temperatures are extremely low on and near Mount Logan. On the 5,000 m high plateau, air temperature hovers around in the winter and reaches near freezing in summer with the median temperature for the year around . Minimal snow melt leads to a significant ice cap, reaching almost in certain spots.

The Mount Logan massif is considered to contain all the surrounding peaks with less than of prominence, as listed below:

In 1922, a geologist approached the Alpine Club of Canada with the suggestion that the club send a team to the mountain to reach the summit for the first time. An international team of Canadian, British and American climbers was assembled and initially they had planned their attempt in 1924 but funding and preparation delays postponed the trip until 1925. The international team of climbers began their journey in early May, crossing the mainland from the Pacific coast by train. They then walked the remaining to within of the Logan Glacier where they established base camp. In the early evening of June 23, 1925, Albert H. MacCarthy (leader), H.F. Lambart, Allen Carpé, W.W. Foster, Norman H. Read and Andy Taylor stood on top for the first time. It had taken them 65 days to approach the mountain from the nearest town, McCarthy, summit and return, with all climbers intact.


Following the death of former Prime Minister Pierre Trudeau, Prime Minister Jean Chrétien, a close friend of Trudeau's, considered renaming the mountain Mount Trudeau;
however, opposition from Yukoners, mountaineers, geologists, Trudeau's political critics, and many other Canadians forced the plan to be dropped. A mountain in British Columbia's Premier Range was named Mount Pierre Elliott Trudeau instead.

During the last few days of May 2005, three climbers from the North Shore Search and Rescue team of North Vancouver became stranded on the mountain. A joint operation by Canadian and American forces rescued the three climbers and took them to Anchorage, Alaska for treatment of frostbite.




</doc>
<doc id="18930" url="https://en.wikipedia.org/wiki?curid=18930" title="Subject (philosophy)">
Subject (philosophy)

A subject is a being who has a unique consciousness and/or unique personal experiences, or an entity that has a relationship with another entity that exists outside itself (called an "object"). A "subject" is an observer and an "object" is a thing observed. This concept is especially important in Continental philosophy, where 'the Subject' is a central term in debates over human autonomy and the nature of the self.

The sharp distinction between subject and object corresponds to the distinction, in the philosophy of René Descartes, between thought and extension. Descartes believed that thought (subjectivity) was the essence of the mind, and that extension (the occupation of space) was the essence of matter.

In the modern continental tradition, debates over the nature of the Subject play a role comparable to debates over personhood within the distinct Anglo-American tradition of analytical philosophy.

In critical theory and psychology, subjectivity is also the actions or discourses that produce individuals or 'I'—the 'I' is the "subject".

"Subject" as a key-term in thinking about human consciousness began its career with the German Idealists, in response to David Hume's radical skepticism. The idealists' starting point was Hume's conclusion that there is nothing to the self over and above a big, fleeting bundle of perceptions. The next step was to ask how this undifferentiated bundle comes to be experienced as a unity – as a single "subject". Hume had offered the following proposal:

Kant, Hegel and their successors sought to flesh out the process by which the subject is constituted out of the flow of sense impressions. Hegel, for example, stated in his Preface to the "Phenomenology of Spirit" that a subject is constituted by "the process of reflectively mediating itself with itself."

Hegel begins his definition of the subject at a standpoint derived from Aristotelian physics: "the unmoved which is also "self-moving"" (Preface, para. 22). That is, what is not moved by an outside force, but which propels itself, has a "prima facie" case for subjectivity. Hegel's next step, however, is to identify this power to move, this unrest that is the subject, as "pure negativity". Subjective self-motion, for Hegel, comes not from any pure or simple kernel of authentic individuality, but rather, it is

The Hegelian subject's "modus operandi" is therefore cutting, splitting and introducing distinctions by injecting negation into the flow of sense-perceptions. Subjectivity is thus a kind of structural effect – what happens when Nature is diffused, refracted around a field of negativity and the "unity of the subject" for Hegel, is in fact a second-order effect, a "negation of negation". The subject experiences itself as a unity only by purposively negating the very diversity it itself had produced. The Hegelian subject may therefore be characterized either as "self-restoring sameness" or else as "reflection in otherness within itself" (ibid.) In short, a subject in the Hegelian sense is subjected to subjection.

The thinking of Karl Marx and Sigmund Freud provided a point of departure for questioning the notion of a unitary, autonomous Subject, which for many thinkers in the Continental tradition is seen as the foundation of the liberal theory of the social contract. These thinkers opened up the way for the deconstruction of the subject as a core-concept of metaphysics.

Sigmund Freud's explorations of the unconscious mind added up to a wholesale indictment of Enlightenment notions of subjectivity.

Among the most radical re-thinkers of human self-consciousness was Martin Heidegger, whose concept of "Dasein" or "Being-there" displaces traditional notions of the personal subject altogether. With Heidegger, phenomenology tries to go beyond the classical dichotomy between subject and object, because they are linked by an inseparable and original relationship, in the sense that there can be no world without a subject, nor the subject without world.

Jacques Lacan, inspired by Heidegger and Ferdinand de Saussure, built on Freud's psychoanalytic model of the subject, in which the "split subject" is constituted by a double bind: alienated from jouissance when he or she leaves the Real, enters into the Imaginary (during the mirror stage), and separates from the Other when he or she comes into the realm of language, difference, and demand in the Symbolic or the Name of the Father.

Thinkers such as structural Marxist Louis Althusser and poststructuralist Michel Foucault theorize the subject as a social construction, the so-called poststructuralist subject. According to Althusser, the "subject" is an ideological construction (more exactly, constructed by the "Ideological State Apparatuses"). One's subjectivity exists, "always already" and is discovered through the process of interpellation. Ideology inaugurates one into being a subject, and every ideology is intended to maintain and glorify its idealized subject, as well as the metaphysical category of the subject itself (see antihumanism).

According to Foucault, it is the "effect" of power and "disciplines" (see "Discipline and Punish": construction of the subject (subjectivation, ) as student, soldier, "criminal", etc.). Foucault believed it was possible to transform oneself; he used the word ethopoiein from the word "ethos" to describe the process.

In contemporary analytic philosophy, the issue of subject—and more specifically the "point of view" of the subject, or "subjectivity"—has received attention as one of the major intractable problems in philosophy of mind (a related issue being the mind-body problem). In the essay "What is it like to be a bat?", Thomas Nagel famously argued that explaining subjective experience—the "what it is like" to be something—is currently beyond the reach of scientific inquiry, because scientific understanding by definition requires an objective perspective, which, according to Nagel, is diametrically opposed to the subjective first-person point of view. Furthermore, one cannot have a definition of objectivity without being connected to subjectivity in the first place since they are mutual and interlocked.

In Nagel's book "The View From Nowhere", he asks: "What kind of fact is it that I am Thomas Nagel?". Subjects have a perspective but each subject has a unique perspective and this seems to be a fact in Nagel's view from nowhere (i.e. the birds-eye view of the objective description in the universe). The Indian view of "Brahman" suggests that the ultimate and fundamental subject is existence itself, through which each of us as it were "looks out" as an aspect of a frozen and timeless everything, experienced subjectively due to our separated sensory and memory apparati. These additional features of subjective experience are often referred to as "qualia" (see Frank Cameron Jackson and Mary's room).




</doc>
<doc id="18932" url="https://en.wikipedia.org/wiki?curid=18932" title="Media bias">
Media bias

Media bias is the bias or perceived bias of journalists and news producers within the mass media in the selection of events and stories that are reported and how they are covered. The term "media bias" implies a pervasive or widespread bias contravening the standards of journalism, rather than the perspective of an individual journalist or article. The direction and degree of media bias in various countries is widely disputed.

Practical limitations to media neutrality include the inability of journalists to report all available stories and facts, and the requirement that selected facts be linked into a coherent narrative. Government influence, including overt and covert censorship, biases the media in some countries, for example North Korea and Myanmar. Market forces that result in a biased presentation include the ownership of the news source, concentration of media ownership, the selection of staff, the preferences of an intended audience, and pressure from advertisers.

There are a number of national and international watchdog groups that report on bias in the media.

The most commonly discussed forms of bias occur when the (allegedly partisan) media support or attack a particular political party, candidate, or ideology.

D'Alessio and Allen list three forms of media bias as the most widely studied:
Other common forms of political and non-political media bias include:


Other forms of bias include reporting that favors or attacks a particular race, religion, gender, age, sexual orientation, ethnic group, or even person.

Media bias in the United States occurs when the media in the United States systematically emphasizes one particular point of view in a manner that contravenes the standards of professional journalism. Claims of media bias in the United States include claims of liberal bias, conservative bias, mainstream bias, and corporate bias and activist/cause bias. To combat this, a variety of watchdog groups that attempt to find the facts behind both biased reporting and unfounded claims of bias have been founded. These include: 

Research about media bias is now a subject of systematic scholarship in a variety of disciplines.

Media bias is studied at schools of journalism, university departments (including Media studies, Cultural studies and Peace studies) and by independent watchdog groups from various parts of the political spectrum. In the United States, many of these studies focus on issues of a conservative/liberal balance in the media. Other focuses include international differences in reporting, as well as bias in reporting of particular issues such as economic class or environmental interests.

Martin Harrison's "TV News: Whose Bias?" (1985) criticized the methodology of the Glasgow Media Group, arguing that the GMG identified bias selectively, via their own preconceptions about what phrases qualify as biased descriptions. For example, the GMG sees the word "idle" to describe striking workers as pejorative, despite the word being used by strikers themselves.

Herman and Chomsky (1988) proposed a propaganda model hypothesizing systematic biases of U.S. media from structural economic causes. They hypothesize media ownership by corporations, funding from advertising, the use of official sources, efforts to discredit independent media ("flak"), and "anti-communist" ideology as the filters that bias news in favor of U.S. corporate interests.

Many of the positions in the preceding study are supported by a 2002 study by Jim A. Kuypers: "Press Bias and Politics: How the Media Frame Controversial Issues". In this study of 116 mainstream US papers (including "The New York Times", "the Washington Post", "Los Angeles Times", and the "San Francisco Chronicle"), Kuypers found that the mainstream print press in America operate within a narrow range of liberal beliefs. Those who expressed points of view further to the left were generally ignored, whereas those who expressed moderate or conservative points of view were often actively denigrated or labeled as holding a minority point of view. In short, if a political leader, regardless of party, spoke within the press-supported range of acceptable discourse, he or she would receive positive press coverage. If a politician, again regardless of party, were to speak outside of this range, he or she would receive negative press or be ignored. Kuypers also found that the liberal points of view expressed in editorial and opinion pages were found in hard news coverage of the same issues. Although focusing primarily on the issues of race and homosexuality, Kuypers found that the press injected opinion into its news coverage of other issues such as welfare reform, environmental protection, and gun control; in all cases favoring a liberal point of view.

Studies reporting perceptions of bias in the media are not limited to studies of print media. A joint study by the Joan Shorenstein Center on Press, Politics and Public Policy at Harvard University and the Project for Excellence in Journalism found that people see media bias in television news media such as CNN. Although both CNN and Fox were perceived in the study as not being centrist, CNN was perceived as being more liberal than Fox. Moreover, the study's findings concerning CNN's perceived bias are echoed in other studies. There is also a growing economics literature on mass media bias, both on the theoretical and the empirical side. On the theoretical side the focus is on understanding to what extent the political positioning of mass media outlets is mainly driven by demand or supply factors. This literature is surveyed by Andrea Prat of Columbia University and David Stromberg of Stockholm University.

According to Dan Sutter of the University of Oklahoma, a systematic liberal bias in the U.S. media could depend on the fact that owners and/or journalists typically lean to the left.

Along the same lines, David Baron of Stanford GSB presents a game-theoretic model of mass media behaviour in which, given that the pool of journalists systematically leans towards the left or the right, mass media outlets maximise their profits by providing content that is biased in the same direction. They can do so, because it is cheaper to hire journalists who write stories that are consistent with their political position. A concurrent theory would be that supply and demand would cause media to attain a neutral balance because consumers would of course gravitate towards the media they agreed with. This argument fails in considering the imbalance in self-reported political allegiances by journalists themselves, that distort any market analogy as regards offer: (..) "Indeed, in 1982, 85 percent of Columbia Graduate School of Journalism students identified themselves as liberal, versus 11 percent conservative"" (Lichter, Rothman, and Lichter 1986: 48), quoted in Sutter, 2001.

This same argument would have news outlets in equal numbers increasing profits of a more balanced media far more than the slight increase in costs to hire unbiased journalists, notwithstanding the extreme rarity of self-reported conservative journalists (Sutton, 2001).

As mentioned above, Tim Groseclose of UCLA and Jeff Milyo of the University of Missouri at Columbia use think tank quotes, in order to estimate the relative position of mass media outlets in the political spectrum. The idea is to trace out which think tanks are quoted by various mass media outlets within news stories, and to match these think tanks with the political position of members of the U.S. Congress who quote them in a non-negative way. Using this procedure, Groseclose and Milyo obtain the stark result that all sampled news providers -except Fox News' Special Report and the Washington Times- are located to the left of the average Congress member, i.e. there are signs of a liberal bias in the US news media. However, the news media also show a remarkable degree of centrism, just because all outlets but one are located –from an ideological point of view- between the average Democrat and average Republican in Congress.

The methods Groseclose and Milyo used to calculate this bias have been criticized by Mark Liberman, a professor of Linguistics at the University of Pennsylvania. Liberman concludes by saying he thinks "that many if not most of the complaints directed against G&M are motivated in part by ideological disagreement – just as much of the praise for their work is motivated by ideological agreement. It would be nice if there were a less politically fraught body of data on which such modeling exercises could be explored."

Sendhil Mullainathan and Andrei Shleifer of Harvard University construct a behavioural model, which is built around the assumption that readers and viewers hold beliefs that they would like to see confirmed by news providers. When news customers share common beliefs, profit-maximizing media outlets find it optimal to select and/or frame stories in order to pander to those beliefs. On the other hand, when beliefs are heterogeneous, news providers differentiate their offer and segment the market, by providing news stories that are slanted towards the two extreme positions in the spectrum of beliefs.

Matthew Gentzkow and Jesse Shapiro of Chicago GSB present another demand-driven theory of mass media bias. If readers and viewers have a priori views on the current state of affairs and are uncertain about the quality of the information about it being provided by media outlets, then the latter have an incentive to slant stories towards their customers' prior beliefs, in order to build and keep a reputation for high-quality journalism. The reason for this is that rational agents would tend to believe that pieces of information that go against their prior beliefs in fact originate from low-quality news providers.

Given that different groups in society have different beliefs, priorities, and interests, to which group would the media tailor its bias? David Stromberg constructs a demand-driven model where media bias arises because different audiences have different effects on media profits. Advertisers pay more for affluent audiences and media may tailor content to attract this audience, perhaps producing a right-wing bias. On the other hand, urban audiences are more profitable to newspapers because of lower delivery costs. Newspapers may for this reason tailor their content to attract the profitable predominantly liberal urban audiences. Finally, because of the increasing returns to scale in news production, small groups such as minorities are less profitable. This biases media content against the interest of minorities.

Jimmy Chan of Shanghai University and Wing Suen of the University of Hong Kong develop a model where media bias arises because the media cannot tell "the whole truth" but are restricted to simple messages, such as political endorsements. In this setting, media bias arises because biased media are more informative; people with a certain political bias prefer media with a similar bias because they can more trust their advice on what actions to take.

The economics empirical literature on mass media bias mainly focuses on the United States.

Steve Ansolabehere, Rebecca Lessem and Jim Snyder of the Massachusetts Institute of Technology analyze the political orientation of endorsements by U.S. newspapers. They find an upward trend in the average propensity to endorse a candidate, and in particular an incumbent one. There are also some changes in the average ideological slant of endorsements: while in the 1940s and in the 1950s there was a clear advantage to Republican candidates, this advantage continuously eroded in subsequent decades, to the extent that in the 1990s the authors find a slight Democratic lead in the average endorsement choice.

John Lott and Kevin Hassett of the American Enterprise Institute study the coverage of economic news by looking at a panel of 389 U.S. newspapers from 1991 to 2004, and from 1985 to 2004 for a subsample comprising the top 10 newspapers and the Associated Press. For each release of official data about a set of economic indicators, the authors analyze how newspapers decide to report on them, as reflected by the tone of the related headlines. The idea is to check whether newspapers display some kind of partisan bias, by giving more positive or negative coverage to the same economic figure, as a function of the political affiliation of the incumbent president. Controlling for the economic data being released, the authors find that there are between 9.6 and 14.7 percent fewer positive stories when the incumbent president is a Republican.

Riccardo Puglisi of the Massachusetts Institute of Technology looks at the editorial choices of the "New York Times" from 1946 to 1997. He finds that the "Times" displays Democratic partisanship, with some watchdog aspects. This is the case, because during presidential campaigns the "Times" systematically gives more coverage to Democratic topics of civil rights, health care, labor and social welfare, but only when the incumbent president is a Republican. These topics are classified as Democratic ones, because Gallup polls show that on average U.S. citizens think that Democratic candidates would be better at handling problems related to them. According to Puglisi, in the post-1960 period the "Times" displays a more symmetric type of watchdog behaviour, just because during presidential campaigns it also gives more coverage to the typically Republican issue of Defense when the incumbent president is a Democrat, and less so when the incumbent is a Republican.

Alan Gerber and Dean Karlan of Yale University use an experimental approach to examine not whether the media are biased, but whether the media influence political decisions and attitudes. They conduct a randomized control trial just prior to the November 2005 gubernatorial election in Virginia and randomly assign individuals in Northern Virginia to (a) a treatment group that receives a free subscription to the Washington Post, (b) a treatment group that receives a free subscription to the Washington Times, or (c) a control group. They find that those who are assigned to the Washington Post treatment group are eight percentage points more likely to vote for the Democrat in the elections. The report also found that "exposure to either newspaper was weakly linked to a movement away from the Bush administration and Republicans."

Another unaffiliated group, Media Study Group, established seven categories of poor journalistic practice: for example, the journalist stating personal opinion in a report, asserting incorrect facts, applying unequal space or treatment to two sides of a controversial issue; then analyzed The Age Newspaper (Melbourne Australia) for the frequency of infraction of this code of practice. The resultant instances were then analyzed statistically with respect to the frequency they supported one or other side of the two-sided controversial issue under consideration. The goal of this group was to establish a quantitative methodology for the study of bias.

A self-described "progressive" media watchdog group, Fairness and Accuracy in Reporting (FAIR), in consultation with the Survey and Evaluation Research Laboratory at Virginia Commonwealth University, sponsored a 1998 survey in which 141 Washington bureau chiefs and Washington-based journalists were asked a range of questions about how they did their work and about how they viewed the quality of media coverage in the broad area of politics and economic policy. "They were asked for their opinions and views about a range of recent policy issues and debates. Finally, they were asked for demographic and identifying information, including their political orientation". They then compared to the same or similar questions posed with "the public" based on Gallup, and Pew Trust polls. Their study concluded that a majority of journalists, although relatively liberal on social policies, were significantly to the right of the public on economic, labor, health care and foreign policy issues.

This study continues: "we learn much more about the political orientation of news content by looking at sourcing patterns rather than journalists' personal views. As this survey shows, it is government officials and business representatives to whom journalists "nearly always" turn when covering economic policy. Labor representatives and consumer advocates were at the bottom of the list. This is consistent with earlier research on sources. For example, analysts from the non-partisan Brookings Institution and from conservative think tanks such as the Heritage Foundation and the American Enterprise Institute are those most quoted in mainstream news accounts.

In direct contrast to the FAIR survey, in 2014, media communication researcher Jim A. Kuypers published a 40-year longitudinal, aggregate study of the political beliefs and actions of American journalists. In every single category (for instance, social, economic, unions, health care, and foreign policy) he found that nationwide, print and broadcast journalists and editors as a group were "considerably" to the political left of the majority of Americans, and that these political beliefs found their way into news stories. Kuypers concluded, "Do the political proclivities of journalists influence their interpretation of the news? I answer that with a resounding, yes. As part of my evidence, I consider testimony from journalists themselves. ... [A] solid majority of journalists do allow their political ideology to influence their reporting."

Jonathan M. Ladd, who has conducted intensive studies of media trust and media bias, concluded that the primary cause of belief in media bias is media telling their audience that particular media are biased. People who are told that a medium is biased tend to believe that it is biased, and this belief is unrelated to whether that medium is actually biased or not. The only other factor with as strong an influence on belief that media is biased is extensive coverage of celebrities. A majority of people see such media as biased, while at the same time preferring media with extensive coverage of celebrities.

A major problem in studies is experimenter's bias. Research into studies of media bias in the United States shows that liberal experimenters tend to get results that say the media has a conservative bias, while conservatives experimenters tend to get results that say the media has a liberal bias, and those who do not identify themselves as either liberal or conservative get results indicating little bias, or mixed bias.

The study "A Measure of Media Bias" by political scientist Timothy J. Groseclose of UCLA and economist Jeffrey D. Milyo of the University of Missouri-Columbia, purports to rank news organizations in terms of identifying with liberal or conservative values relative to each other. They used the Americans for Democratic Action (ADA) scores as a quantitative proxy for political leanings of the referential organizations. Thus their definition of "liberal" includes the RAND Corporation, a nonprofit research organization with strong ties to the Defense Department. Their work claims to detect a bias towards liberalism in the American media.

A technique used to avoid bias is the "point/counterpoint" or "round table", an adversarial format in which representatives of opposing views comment on an issue. This approach theoretically allows diverse views to appear in the media. However, the person organizing the report still has the responsibility to choose people who really represent the breadth of opinion, to ask them non-prejudicial questions, and to edit or arbitrate their comments fairly. When done carelessly, a point/counterpoint can be as unfair as a simple biased report, by suggesting that the "losing" side lost on its merits.

Using this format can also lead to accusations that the reporter has created a misleading appearance that viewpoints have equal validity (sometimes called "false balance"). This may happen when a taboo exists around one of the viewpoints, or when one of the representatives habitually makes claims that are easily shown to be inaccurate.

One such allegation of misleading balance came from Mark Halperin, political director of ABC News. He stated in an internal e-mail message that reporters should not "artificially hold George W. Bush and John Kerry 'equally' accountable" to the public interest, and that complaints from Bush supporters were an attempt to "get away with ... renewed efforts to win the election by destroying Senator Kerry." When the conservative web site the Drudge Report published this message, many Bush supporters viewed it as "smoking gun" evidence that Halperin was using ABC to propagandize against Bush to Kerry's benefit, by interfering with reporters' attempts to avoid bias. An academic content analysis of election news later found that coverage at ABC, CBS, and NBC was more favorable toward Kerry than Bush, while coverage at Fox News Channel was more favorable toward Bush.

Scott Norvell, the London bureau chief for Fox News, stated in a May 20, 2005 interview with the "Wall Street Journal" that:
"Even we at Fox News manage to get some lefties on the air occasionally, and often let them finish their sentences before we club them to death and feed the scraps to Karl Rove and Bill O'Reilly. And those who hate us can take solace in the fact that they aren't subsidizing Bill's bombast; we payers of the BBC license fee don't enjoy that peace of mind.<br>
Fox News is, after all, a private channel and our presenters are quite open about where they stand on particular stories. That's our appeal. People watch us because they know what they are getting. The Beeb's (British Broadcasting Corporation) (BBC) institutionalized leftism would be easier to tolerate if the corporation was a little more honest about it".

Another technique used to avoid bias is disclosure of affiliations that may be considered a possible conflict of interest. This is especially apparent when a news organization is reporting a story with some relevancy to the news organization itself or to its ownership individuals or conglomerate. Often this disclosure is mandated by the laws or regulations pertaining to stocks and securities. Commentators on news stories involving stocks are often required to disclose any ownership interest in those corporations or in its competitors.

In rare cases, a news organization may dismiss or reassign staff members who appear biased. This approach was used in the Killian documents affair and after Peter Arnett's interview with the Iraqi press. This approach is presumed to have been employed in the case of Dan Rather over a story that he ran on "60 Minutes" in the month prior to the 2004 election that attempted to impugn the military record of George W. Bush by relying on allegedly fake documents that were provided by Bill Burkett, a retired Lieutenant Colonel in the Texas Army National Guard.

Finally, some countries have laws enforcing balance in state-owned media. Since 1991, the CBC and Radio Canada, its French language counterpart, are governed by the Broadcasting Act. This act states, among other things:
the programming provided by the Canadian broadcasting system should
(i) be varied and comprehensive, providing a balance of information, enlightenment and entertainment for men, women and children of all ages, interests and tastes,
(iv) provide a reasonable opportunity for the public to be exposed to the expression of differing views on matters of public concern

Besides these manual approaches, several (semi-)automated approaches have been developed by social scientists and computer scientists. These approaches identify differences in news coverage, which potentially resulted from media bias, by analyzing the text and meta data, such as author and publishing date. For instance, NewsCube is a news aggregator that extracts key phrases that describe a topic differently. Other approaches make use of text- and meta-data, e.g., matrix-based news aggregation spans a matrix over two dimensions, such as "publisher countries" (in which articles have been published) and "mentioned countries" (on which country an article reports). As a result, each cell contains only articles that have been published in one country and that report on another country. Particularly in international news topics, matrix-based news aggregation helps to reveal differences in media coverage between the involved countries.

Political bias has been a feature of the mass media since its birth with the invention of the printing press. The expense of early printing equipment restricted media production to a limited number of people. Historians have found that publishers often served the interests of powerful social groups.

John Milton's pamphlet "Areopagitica, a Speech for the Liberty of Unlicensed Printing", published in 1644, was one of the first publications advocating freedom of the press.

In the 19th century, journalists began to recognize the concept of unbiased reporting as an integral part of journalistic ethics. This coincided with the rise of journalism as a powerful social force. Even today, though, the most conscientiously objective journalists cannot avoid accusations of bias.

Like newspapers, the broadcast media (radio and television) have been used as a mechanism for propaganda from their earliest days, a tendency made more pronounced by the initial ownership of broadcast spectrum by national governments. Although a process of media deregulation has placed the majority of the western broadcast media in private hands, there still exists a strong government presence, or even monopoly, in the broadcast media of many countries across the globe. At the same time, the concentration of media ownership in private hands, and frequently amongst a comparatively small number of individuals, has also led to accusations of media bias.

There are many examples of accusations of bias being used as a political tool, sometimes resulting in government censorship.


Not all accusations of bias are political. Science writer Martin Gardner has accused the entertainment media of anti-science bias. He claims that television programs such as "The X-Files" promote superstition. In contrast, the Competitive Enterprise Institute, which is funded by businesses, accuses the media of being biased in favor of science and against business interests, and of credulously reporting science that shows that greenhouse gasses cause global warming.

Mass media, despite its ability to project worldwide, is limited in its cross-ethnic compatibility by one simple attribute – language. Ethnicity, being largely developed by a divergence in geography, language, culture, genes and similarly, point of view, has the potential to be countered by a common source of information. Therefore, language, in the absence of translation, comprises a barrier to a worldwide community of debate and opinion, although it is also true that media within any given society may be split along class, political or regional lines.
Furthermore, if the language is translated, the translator has room to shift a bias by choosing weighed words for translation.

Language may also be seen as a political factor in mass media, particularly in instances where a society is characterized by a large number of languages spoken by its populace. The choice of language of mass media may represent a bias towards the group most likely to speak that language, and can limit the public participation by those who do not speak the language. On the other hand, there have also been attempts to use a common-language mass media to reach out to a large, geographically dispersed population, such as in the use of Arabic language by news channel Al Jazeera.

Many media theorists concerned with language and media bias point towards the media of the United States, a large country where English is spoken by the majority of the population. Some theorists argue that the common language is not homogenizing; and that there still remain strong differences expressed within the mass media. This viewpoint asserts that moderate views are bolstered by drawing influences from the extremes of the political spectrum. In the United States, the national news therefore contributes to a sense of cohesion within the society, proceeding from a similarly informed population. According to this model, most views within society are freely expressed, and the mass media are accountable to the people and tends to reflect the spectrum of opinion.

Language may also introduce a more subtle form of bias. The selection of metaphors and analogies, or the inclusion of personal information in one situation but not another can introduce bias, such as a gender bias. Use of a word with positive or negative connotations rather than a more neutral synonym can form a biased picture in the audience's mind. For example, it makes a difference whether the media calls a group "terrorists" or "freedom fighters" or "insurgents". A 2005 memo to the staff of the CBC states:

In a widely criticized episode, initial online BBC reports of the 7 July 2005 London bombings identified the perpetrators as terrorists, in contradiction to the BBC's internal policy. But by the next day, journalist Tom Gross noted that the online articles had been edited, replacing "terrorists" by "bombers". In another case, March 28, 2007, the BBC paid almost $400,000 in legal fees in a London court to keep an internal memo dealing with alleged anti-Israeli bias from becoming public. The BBC has both been accused of having a pro-Palestinian bias, with many examples cited, including a documentary falsely accusing Israel of developing a nuclear weapon during the second Palestinian intifada in 2000, as well as of having a pro-Israel bias, which it has partially admitted to in a case in 2013.

Many news organizations reflect, or are perceived to reflect in some way, the viewpoint of the geographic, ethnic, and national population that they primarily serve. Media within countries are sometimes seen as being sycophantic or unquestioning about the country's government.

Western media are often criticized in the rest of the world (including eastern Europe, Asia, Africa, and the Middle East) as being pro-Western with regard to a variety of political, cultural and economic issues. Al Jazeera is frequently criticized both in the West and in the Arab world.

The Israeli–Palestinian conflict and wider Arab–Israeli issues are a particularly controversial area, and nearly all coverage of any kind generates accusation of bias from one or both sides. This topic is covered in a separate article.

It has been observed that the world's principal suppliers of news, the news agencies, and the main buyers of news are Anglophone corporations and this gives an Anglophone bias to the selection and depiction of events. Anglophone definitions of what constitutes news are paramount; the news provided originates in Anglophone capitals and responds first to their own rich domestic markets.

Despite the plethora of news services, most news printed and broadcast throughout the world each day comes from only a few major agencies, the three largest of which are the Associated Press, Reuters and Agence France-Presse. Although these agencies are 'global' in the sense of their activities, they each retain significant associations with particular nations, namely the United States (AP), the United Kingdom (Reuters) and France (AFP). Chambers and Tinckell suggest that the so-called global media are agents of Anglophone values which privilege norms of 'competitive individualism, "laissez-faire" capitalism, parliamentary democracy and consumerism.' They see the presentation of the English language as international as a further feature of Anglophone dominance.

The media are often accused of bias favoring a particular religion or of bias against a particular religion. In some countries, only reporting approved by a state religion is permitted. In other countries, derogatory statements about any belief system are considered hate crimes and are illegal.

According to the Encyclopedia of Social Work (19th edition), the news media play an influential role in the general public's perception of cults. As reported in several studies, the media have depicted cults as problematic, controversial, and threatening from the beginning, tending to favor sensationalistic stories over balanced public debates. It furthers the analysis that media reports on cults rely heavily on police officials and cult "experts" who portray cult activity as dangerous and destructive, and when divergent views are presented, they are often overshadowed by horrific stories of ritualistic torture, sexual abuse, mind control, and other such practices. Furthermore, unfounded allegations, when proved untrue, receive little or no media attention.

In 2012, "Huffington Post", columnist Jacques Berlinerblau argued that secularism has often been misinterpreted in the media as another word for atheism, stating that: "Secularism must be the most misunderstood and mangled ism in the American political lexicon. Commentators on the right and the left routinely equate it with Stalinism, Nazism and Socialism, among other dreaded isms. In the United States, of late, another false equation has emerged. That would be the groundless association of secularism with atheism. The religious right has profitably promulgated this misconception at least since the 1970s."

According to Stuart A. Wright, there are six factors that contribute to media bias against minority religions: first, the knowledge and familiarity of journalists with the subject matter; second, the degree of cultural accommodation of the targeted religious group; third, limited economic resources available to journalists; fourth, time constraints; fifth, sources of information used by journalists; and finally, the frond-end/back-end disproportionality of reporting. According to Yale Law professor Stephen Carter, "it has long been the American habit to be more suspicious of—and more repressive toward—religions that stand outside the mainline Protestant-Roman Catholic-Jewish troika that dominates America's spiritual life." As for front-end/back-end disproportionality, Wright says: "news stories on unpopular or marginal religions frequently are predicated on unsubstantiated allegations or government actions based on faulty or weak evidence occurring at the front-end of an event. As the charges weighed in against material evidence, these cases often disintegrate. Yet rarely is there equal space and attention in the mass media given to the resolution or outcome of the incident. If the accused are innocent, often the public is not made aware."

The apparent bias of media is not always specifically political in nature. The news media tend to appeal to a specific audience, which means that stories that affect a large number of people on a global scale often receive less coverage in some markets than local stories, such as a public school shooting, a celebrity wedding, a plane crash, a "missing white woman", or similarly glamorous or shocking stories. For example, the deaths of millions of people in an ethnic conflict in Africa might be afforded scant mention in American media, while the shooting of five people in a high school is analyzed in depth. Bias is also known to exist in sports broadcasting; in the United States, broadcasters tend to favor teams on the East Coast, teams in major markets, older and more established teams and leagues, teams based in their respective country (in international sport) and teams that include high-profile celebrity athletes. The reason for these types of bias is a function of what the public wants to watch and/or what producers and publishers believe the public wants to watch.

Bias has also been claimed in instances referred to as conflict of interest, whereby the owners of media outlets have vested interests in other commercial enterprises or political parties. In such cases in the United States, the media outlet is required to disclose the conflict of interest.

However, the decisions of the editorial department of a newspaper and the corporate parent frequently are not connected, as the editorial staff retains freedom to decide what is covered as well as what is not. Biases, real or implied, frequently arise when it comes to deciding what stories will be covered and who will be called for those stories.

Accusations that a source is biased, if accepted, may cause media consumers to distrust certain kinds of statements, and place added confidence on others.

How People View The Media:
Two-thirds (67%) said agreed with the statement: "In dealing with political and social issues, news organizations tend to favor one side." That was up 14 points from 53 percent who gave that answer in 1985.
Those who believed the media "deal fairly with all sides" fell from 34 percent to 27 percent.
"In one of the most telling complaints, a majority (54%) of Americans believe the news media gets in the way of society solving its problems," Pew reported.
Republicans "are more likely to say news organizations favor one side than are Democrats or independents (77 percent vs. 58 percent and 69 percent, respectively)."
The percentage who felt "news organizations get the facts straight" fell from 55 percent to 37 percent.




</doc>
<doc id="18934" url="https://en.wikipedia.org/wiki?curid=18934" title="Muhammad">
Muhammad

Muhammad (; ; c. 570 CE – 8 June 632 CE) was the founder of Islam. According to Islamic doctrine, he was a prophet and God's messenger, sent to present and confirm the monotheistic teachings preached previously by Adam, Abraham, Moses, Jesus, and other prophets. He is viewed as the final prophet of God in all the main branches of Islam, though some modern denominations diverge from this belief. Muhammad united Arabia into a single Muslim polity, with the Quran as well as his teachings and practices forming the basis of Islamic religious belief.

Born in approximately 570CE (Year of the Elephant) in the Arabian city of Mecca, Muhammad was orphaned at an early age; he was raised under the care of his paternal uncle Abu Talib and Abu Talib's wife Fatimah bint Asad. Periodically, he would seclude himself in a mountain cave named Hira for several nights of prayer; later, at age 40, he reported being visited by Gabriel in the cave, where he stated he received his first revelation from God. Three years later, in 610, Muhammad started preaching these revelations publicly, proclaiming that "God is One", that complete "submission" ("islām") to God is the right course of action ("dīn"), and that he was a prophet and messenger of God, similar to the other prophets in Islam.

Muhammad gained few early followers, and experienced hostility from Meccan polytheists. To escape ongoing persecution, he sent some followers to Abyssinia in 615, before he and his followers migrated from Mecca to Medina (then known as Yathrib) later in 622. This event, the "Hijra", marks the beginning of the Islamic calendar, also known as the Hijri Calendar. In Medina, Muhammad united the tribes under the Constitution of Medina. In December 629, after eight years of intermittent wars with Meccan tribes, Muhammad gathered an army of 10,000 Muslim converts and marched on the city of Mecca. The conquest went largely uncontested and Muhammad seized the city with little bloodshed. In 632, a few months after returning from the Farewell Pilgrimage, he fell ill and died. By his death, most of the Arabian Peninsula had converted to Islam.

The revelations (each known as "Ayah", lit. "Sign [of God]"), which Muhammad reported receiving until his death, form the verses of the Quran, regarded by Muslims as the verbatim "Word of God" and around which the religion is based. Besides the Quran, Muhammad's teachings and practices ("sunnah"), found in the Hadith and "sira" (biography) literature, are also upheld and used as sources of Islamic law (see Sharia).

The name "Muhammad" () means "praiseworthy" and appears four times in the Quran. The Quran addresses Muhammad in the second person by various appellations; prophet, messenger, servant of God ("'abd"), announcer ("bashir"), witness ("shahid"), bearer of good tidings ("mubashshir"), warner ("nathir"), reminder ("mudhakkir"), one who calls [unto God] ("dā'ī"), light personified ("noor"), and the light-giving lamp ("siraj munir"). Muhammad is sometimes addressed by designations deriving from his state at the time of the address: thus he is referred to as the enwrapped ("Al-Muzzammil") in Quran and the shrouded ("al-muddaththir") in Quran . In Sura Al-Ahzab God singles out Muhammad as the "Seal of the prophets", or the last of the prophets. The Quran also refers to Muhammad as "Aḥmad" "more praiseworthy" (, Sura As-Saff ).

The name Abū al-Qāsim Muḥammad ibn ʿAbd Allāh ibn ʿAbd al-Muṭṭalib ibn Hāshim, begins with the "kunya" Abū, which corresponds to the English, "father of".

The Quran is the central religious text of Islam. Muslims believe it represents the words of God revealed by the archangel Gabriel to Muhammad. The Quran, however, provides minimal assistance for Muhammad's chronological biography; most Quranic verses do not provide significant historical context.

Important sources regarding Muhammad's life may be found in the historic works by writers of the 2nd and 3rd centuries of the Muslim era (AH – 8th and 9th century CE). These include traditional Muslim biographies of Muhammad, which provide additional information about Muhammad's life.

The earliest surviving written "sira" (biographies of Muhammad and quotes attributed to him) is Ibn Ishaq's "Life of God's Messenger" written c. 767 CE (150 AH). Although the work was lost, this sira was used at great length by Ibn Hisham and to a lesser extent by Al-Tabari. However, Ibn Hisham admits in the preface to his biography of Muhammad that he omitted matters from Ibn Ishaq's biography that "would distress certain people". Another early history source is the history of Muhammad's campaigns by al-Waqidi (death 207 of Muslim era), and the work of his secretary Ibn Sa'd al-Baghdadi (death 230 of Muslim era).

Many scholars accept these early biographies as authentic, though their accuracy is unascertainable. Recent studies have led scholars to distinguish between traditions touching legal matters and purely historical events. In the legal group, traditions could have been subject to invention while historic events, aside from exceptional cases, may have been only subject to "tendential shaping".

Other important sources include the hadith collections, accounts of the verbal and physical teachings and traditions of Muhammad. Hadiths were compiled several generations after his death by followers including Muhammad al-Bukhari, Muslim ibn al-Hajjaj, Muhammad ibn Isa at-Tirmidhi, Abd ar-Rahman al-Nasai, Abu Dawood, Ibn Majah, Malik ibn Anas, al-Daraqutni.

Some Western academics cautiously view the hadith collections as accurate historical sources. Scholars such as Madelung do not reject the narrations which have been compiled in later periods, but judge them in the context of history and on the basis of their compatibility with the events and figures. Muslim scholars on the other hand typically place a greater emphasis on the hadith literature instead of the biographical literature, since hadiths maintain a verifiable chain of transmission (isnad); the lack of such a chain for the biographical literature makes it less verifiable in their eyes.

The Arabian Peninsula was largely arid and volcanic, making agriculture difficult except near oases or springs. The landscape was dotted with towns and cities; two of the most prominent being Mecca and Medina. Medina was a large flourishing agricultural settlement, while Mecca was an important financial center for many surrounding tribes. Communal life was essential for survival in the desert conditions, supporting indigenous tribes against the harsh environment and lifestyle. Tribal affiliation, whether based on kinship or alliances, was an important source of social cohesion. Indigenous Arabs were either nomadic or sedentary, the former constantly travelling from one place to another seeking water and pasture for their flocks, while the latter settled and focused on trade and agriculture. Nomadic survival also depended on raiding caravans or oases; nomads did not view this as a crime.

In pre-Islamic Arabia, gods or goddesses were viewed as protectors of individual tribes, their spirits being associated with sacred trees, stones, springs and wells. As well as being the site of an annual pilgrimage, the Kaaba shrine in Mecca housed 360 idols of tribal patron deities. Three goddesses were associated with Allah as his daughters: Allāt, Manāt and al-'Uzzá. Monotheistic communities existed in Arabia, including Christians and Jews. Hanifs – native pre-Islamic Arabs who "professed a rigid monotheism" – are also sometimes listed alongside Jews and Christians in pre-Islamic Arabia, although their historicity is disputed among scholars. According to Muslim tradition, Muhammad himself was a Hanif and one of the descendants of Ishmael, son of Abraham.

The second half of the sixth century was a period of political disorder in Arabia and communication routes were no longer secure. Religious divisions were an important cause of the crisis. Judaism became the dominant religion in Yemen while Christianity took root in the Persian Gulf area. In line with broader trends of the ancient world, the region witnessed a decline in the practice of polytheistic cults and a growing interest in a more spiritual form of religion. While many were reluctant to convert to a foreign faith, those faiths provided intellectual and spiritual reference points.

During the early years of Muhammad's life, the Quraysh tribe he belonged to became a dominant force in western Arabia. They formed the cult association of "hums", which tied members of many tribes in western Arabia to the Kaaba and reinforced the prestige of the Meccan sanctuary. To counter the effects of anarchy, Quraysh upheld the institution of sacred months during which all violence was forbidden, and it was possible to participate in pilgrimages and fairs without danger. Thus, although the association of "hums" was primarily religious, it also had important economic consequences for the city.

Abū al-Qāsim Muḥammad ibn ʿAbd Allāh ibn ʿAbd al-Muṭṭalib ibn Hāshim, was born about the year 570 and his birthday is believed to be in the month of Rabi' al-awwal. He belonged to the Banu Hashim clan, part of the Quraysh tribe, and was one of Mecca's prominent families, although it appears less prosperous during Muhammad's early lifetime. Tradition places the year of Muhammad's birth as corresponding with the Year of the Elephant, which is named after the failed destruction of Mecca that year by the Abraha, Yemen's king, who supplemented his army with elephants.
Alternatively some 20th century scholars have suggested different years, such as 568 or 569.

Muhammad's father, Abdullah, died almost six months before he was born. According to Islamic tradition, soon after birth he was sent to live with a Bedouin family in the desert, as desert life was considered healthier for infants; some western scholars reject this tradition's historicity. Muhammad stayed with his foster-mother, Halimah bint Abi Dhuayb, and her husband until he was two years old. At the age of six, Muhammad lost his biological mother Amina to illness and became an orphan. For the next two years, until he was eight years old, Muhammad was under the guardianship of his paternal grandfather Abdul-Muttalib, of the Banu Hashim clan until his death. He then came under the care of his uncle Abu Talib, the new leader of the Banu Hashim. According to Islamic historian William Montgomery Watt there was a general disregard by guardians in taking care of weaker members of the tribes in Mecca during the 6th century, "Muhammad's guardians saw that he did not starve to death, but it was hard for them to do more for him, especially as the fortunes of the clan of Hashim seem to have been declining at that time."

In his teens, Muhammad accompanied his uncle on Syrian trading journeys to gain experience in commercial trade. Islamic tradition states that when Muhammad was either nine or twelve while accompanying the Meccans' caravan to Syria, he met a Christian monk or hermit named Bahira who is said to have foreseen Muhammad's career as a prophet of God.

Little is known of Muhammad during his later youth, available information is fragmented, making it difficult to separate history from legend. It is known that he became a merchant and "was involved in trade between the Indian Ocean and the Mediterranean Sea." Due to his upright character he acquired the nickname "al-Amin" (Arabic: الامين), meaning "faithful, trustworthy" and "al-Sadiq" meaning "truthful" and was sought out as an impartial arbitrator. His reputation attracted a proposal in 595 from Khadijah, a 40-year-old widow. Muhammad consented to the marriage, which by all accounts was a happy one.

Several years later, according to a narration collected by historian Ibn Ishaq, Muhammad was involved with a well-known story about setting the Black Stone in place in the wall of the Kaaba in 605 CE. The Black Stone, a sacred object, was removed during renovations to the Kaaba. The Meccan leaders could not agree which clan should return the Black Stone to its place. They decided to ask the next man who comes through the gate to make that decision; that man was the 35-year-old Muhammad. This event happened five years before the first revelation by Gabriel to him. He asked for a cloth and laid the Black Stone in its center. The clan leaders held the corners of the cloth and together carried the Black Stone to the right spot, then Muhammad laid the stone, satisfying the honour of all.

Muhammad began to pray alone in a cave named Hira on Mount Jabal al-Nour, near Mecca for several weeks every year. Islamic tradition holds that during one of his visits to that cave, in the year 610 the angel Gabriel appeared to him and commanded Muhammad to recite verses that would be included in the Quran. Consensus exists that the first Quranic words revealed were the beginning of Surah .
Muhammad was deeply distressed upon receiving his first revelations. After returning home, Muhammad was consoled and reassured by Khadijah and her Christian cousin, Waraka ibn Nawfal. He also feared that others would dismiss his claims as being possessed. Shi'a tradition states Muhammad was not surprised or frightened at Gabriel's appearance; rather he welcomed the angel, as if he was expected. The initial revelation was followed by a three-year pause (a period known as "fatra") during which Muhammad felt depressed and further gave himself to prayers and spiritual practices. When the revelations resumed he was reassured and commanded to begin preaching: "Thy Guardian-Lord hath not forsaken thee, nor is He displeased."

Sahih Bukhari narrates Muhammad describing his revelations as "sometimes it is (revealed) like the ringing of a bell". Aisha reported, "I saw the Prophet being inspired Divinely on a very cold day and noticed the sweat dropping from his forehead (as the Inspiration was over)". According to Welch these descriptions may be considered genuine, since they are unlikely to have been forged by later Muslims. Muhammad was confident that he could distinguish his own thoughts from these messages. According to the Quran, one of the main roles of Muhammad is to warn the unbelievers of their eschatological punishment (Quran , Quran ). Occasionally the Quran did not explicitly refer to Judgment day but provided examples from the history of extinct communities and warns Muhammad's contemporaries of similar calamities (Quran ). Muhammad did not only warn those who rejected God's revelation, but also dispensed good news for those who abandoned evil, listening to the divine words and serving God. Muhammad's mission also involves preaching monotheism: The Quran commands Muhammad to proclaim and praise the name of his Lord and instructs him not to worship idols or associate other deities with God. 

The key themes of the early Quranic verses included the responsibility of man towards his creator; the resurrection of the dead, God's final judgment followed by vivid descriptions of the tortures in Hell and pleasures in Paradise, and the signs of God in all aspects of life. Religious duties required of the believers at this time were few: belief in God, asking for forgiveness of sins, offering frequent prayers, assisting others particularly those in need, rejecting cheating and the love of wealth (considered to be significant in the commercial life of Mecca), being chaste and not committing female infanticide.

According to Muslim tradition, Muhammad's wife Khadija was the first to believe he was a prophet. She was followed by Muhammad's ten-year-old cousin Ali ibn Abi Talib, close friend Abu Bakr, and adopted son Zaid. Around 613, Muhammad began to preach to the public (Quran ). Most Meccans ignored and mocked him, though a few became his followers. There were three main groups of early converts to Islam: younger brothers and sons of great merchants; people who had fallen out of the first rank in their tribe or failed to attain it; and the weak, mostly unprotected foreigners.

According to Ibn Saad, opposition in Mecca started when Muhammad delivered verses that condemned idol worship and the polytheism practiced by the Meccan forefathers. However, the Quranic exegesis maintains that it began as Muhammad started public preaching. As his followers increased, Muhammad became a threat to the local tribes and rulers of the city, whose wealth rested upon the Ka'aba, the focal point of Meccan religious life that Muhammad threatened to overthrow. Muhammad's denunciation of the Meccan traditional religion was especially offensive to his own tribe, the Quraysh, as they were the guardians of the Ka'aba. Powerful merchants attempted to convince Muhammad to abandon his preaching; he was offered admission to the inner circle of merchants, as well as an advantageous marriage. He refused both of these offers.

Tradition records at great length the persecution and ill-treatment towards Muhammad and his followers. Sumayyah bint Khayyat, a slave of a prominent Meccan leader Abu Jahl, is famous as the first martyr of Islam; killed with a spear by her master when she refused to give up her faith. Bilal, another Muslim slave, was tortured by Umayyah ibn Khalaf who placed a heavy rock on his chest to force his conversion.

In 615, some of Muhammad's followers emigrated to the Ethiopian Kingdom of Aksum and founded a small colony under the protection of the Christian Ethiopian emperor Aṣḥama ibn Abjar. Ibn Sa'ad mentions two separate migrations. According to him, most of the Muslims returned to Mecca prior to Hijra, while a second group rejoined them in Medina. Ibn Hisham and Tabari, however, only talk about one migration to Ethiopia. These accounts agree that Meccan persecution played a major role in Muḥammad's decision to suggest that a number of his followers seek refuge among the Christians in Abyssinia. According to the famous letter of ʿUrwa preserved in al-Tabari, the majority of Muslims returned to their native town as Islam gained strength and high ranking Meccans, such as Umar and Hamzah converted.

However, there is a completely different story on the reason why the Muslims returned from Ethiopia to Mecca. According to this account—initially mentioned by Al-Waqidi then rehashed by Ibn Sa'ad and Tabari, but not by Ibn Hisham and not by Ibn Ishaq—Muhammad, desperately hoping for an accommodation with his tribe, pronounced a verse acknowledging the existence of three Meccan goddesses considered to be the daughters of Allah. Muhammad retracted the verses the next day at the behest of Gabriel, claiming that the verses were whispered by the devil himself. Instead, a ridicule of these gods was offered. This episode, known as "The Story of the Cranes," is also known as "Satanic Verses". According to the story, this led to a general reconciliation between Muḥammad and the Meccans, and the Abyssinia Muslims began to return home. When they arrived Gabriel had informed Muḥammad the two verses were not part of the revelation, but had been inserted by Satan. Notable scholars at the time argued against the historic authenticity of these verses and the story itself on various grounds. Al-Waqidi was severely criticized by Islamic scholars such as Malik ibn Anas, al-Shafi'i, Ahmad ibn Hanbal, Al-Nasa'i, al-Bukhari, Abu Dawood, Al-Nawawi and others as a liar and forger. Later, the incident received some acceptance among certain groups, though strong objections to it continued onwards past the tenth century. The objections continued until rejection of these verses and the story itself eventually became the only acceptable orthodox Muslim position.

In 617, the leaders of Makhzum and Banu Abd-Shams, two important Quraysh clans, declared a public boycott against Banu Hashim, their commercial rival, to pressure it into withdrawing its protection of Muhammad. The boycott lasted three years but eventually collapsed as it failed in its objective. During this time, Muhammad was only able to preach during the holy pilgrimage months in which all hostilities between Arabs was suspended.

Islamic tradition states that in 620, Muhammad experienced the "Isra and Mi'raj", a miraculous night-long journey said to have occurred with the angel Gabriel. At the journey's beginning, the "Isra", he is said to have traveled from Mecca on a winged steed to "the farthest mosque." Later, during the "Mi'raj", Muhammad is said to have toured heaven and hell, and spoke with earlier prophets, such as Abraham, Moses, and Jesus. Ibn Ishaq, author of the first biography of Muhammad, presents the event as a spiritual experience; later historians, such as Al-Tabari and Ibn Kathir, present it as a physical journey.

Some western scholars hold that the Isra and Mi'raj journey traveled through the heavens from the sacred enclosure at Mecca to the celestial "al-Baytu l-Maʿmur" (heavenly prototype of the Kaaba); later traditions indicate Muhammad's journey as having been from Mecca to Jerusalem.

Muhammad's wife Khadijah and uncle Abu Talib both died in 619, the year thus being known as the "Year of Sorrow". With the death of Abu Talib, leadership of the Banu Hashim clan passed to Abu Lahab, a tenacious enemy of Muhammad. Soon afterward, Abu Lahab withdrew the clan's protection over Muhammad. This placed Muhammad in danger; the withdrawal of clan protection implied that blood revenge for his killing would not be exacted. Muhammad then visited Ta'if, another important city in Arabia, and tried to find a protector, but his effort failed and further brought him into physical danger. Muhammad was forced to return to Mecca. A Meccan man named Mut'im ibn Adi (and the protection of the tribe of Banu Nawfal) made it possible for him to safely re-enter his native city.
Many people visited Mecca on business or as pilgrims to the Kaaba. Muhammad took this opportunity to look for a new home for himself and his followers. After several unsuccessful negotiations, he found hope with some men from Yathrib (later called Medina). The Arab population of Yathrib were familiar with monotheism and were prepared for the appearance of a prophet because a Jewish community existed there. They also hoped, by the means of Muhammad and the new faith, to gain supremacy over Mecca; the Yathrib were jealous of its importance as the place of pilgrimage. Converts to Islam came from nearly all Arab tribes in Medina; by June of the subsequent year, seventy-five Muslims came to Mecca for pilgrimage and to meet Muhammad. Meeting him secretly by night, the group made what is known as the ""Second Pledge of al-'Aqaba"", or, in Orientalists' view, the ""Pledge of War"". Following the pledges at Aqabah, Muhammad encouraged his followers to emigrate to Yathrib. As with the migration to Abyssinia, the Quraysh attempted to stop the emigration. However, almost all Muslims managed to leave.
The Hijra is the migration of Muhammad and his followers from Mecca to Medina in 622 CE. In June 622, warned of a plot to assassinate him, Muhammad secretly slipped out of Mecca and moved his followers to Medina, north of Mecca.

A delegation, consisting of the representatives of the twelve important clans of Medina, invited Muhammad to serve as chief arbitrator for the entire community; due to his status as a neutral outsider. There was fighting in Yathrib: primarily the dispute involved its Arab and Jewish inhabitants, and was estimated to have lasted for around a hundred years before 620. The recurring slaughters and disagreements over the resulting claims, especially after the Battle of Bu'ath in which all clans were involved, made it obvious to them that the tribal concept of blood-feud and an eye for an eye were no longer workable unless there was one man with authority to adjudicate in disputed cases. The delegation from Medina pledged themselves and their fellow-citizens to accept Muhammad into their community and physically protect him as one of themselves.

Muhammad instructed his followers to emigrate to Medina, until nearly all his followers left Mecca. Being alarmed at the departure, according to tradition, the Meccans plotted to assassinate Muhammad. With the help of Ali, Muhammad fooled the Meccans watching him, and secretly slipped away from the town with Abu Bakr. By 622, Muhammad emigrated to Medina, a large agricultural oasis. Those who migrated from Mecca along with Muhammad became known as "muhajirun" (emigrants).

Among the first things Muhammad did to ease the longstanding grievances among the tribes of Medina was to draft a document known as the Constitution of Medina, "establishing a kind of alliance or federation" among the eight Medinan tribes and Muslim emigrants from Mecca; this specified rights and duties of all citizens, and the relationship of the different communities in Medina (including the Muslim community to other communities, specifically the Jews and other "Peoples of the Book"). The community defined in the Constitution of Medina, "Ummah", had a religious outlook, also shaped by practical considerations and substantially preserved the legal forms of the old Arab tribes.

The first group of converts to Islam in Medina were the clans without great leaders; these clans had been subjugated by hostile leaders from outside. This was followed by the general acceptance of Islam by the pagan population of Medina, with some exceptions. According to Ibn Ishaq, this was influenced by the conversion of Sa'd ibn Mu'adh (a prominent Medinan leader) to Islam. Medinans who converted to Islam and helped the Muslim emigrants find shelter became known as the "ansar" (supporters). Then Muhammad instituted brotherhood between the emigrants and the supporters and he chose Ali as his own brother.

Following the emigration, the people of Mecca seized property of Muslim emigrants to Medina. War would later break out between the people of Mecca and the Muslims. Muhammad delivered Quranic verses permitting Muslims to fight the Meccans (see sura Al-Hajj, Quran ). According to the traditional account, on 11 February 624, while praying in the Masjid al-Qiblatayn in Medina, Muhammad received revelations from God that he should be facing Mecca rather than Jerusalem during prayer. Muhammad adjusted to the new direction, and his companions praying with him followed his lead, beginning the tradition of facing Mecca during prayer.

In March 624, Muhammad led some three hundred warriors in a raid on a Meccan merchant caravan. The Muslims set an ambush for the caravan at Badr. Aware of the plan, the Meccan caravan eluded the Muslims. A Meccan force was sent to protect the caravan and went on to confront the Muslims upon receiving word that the caravan was safe. The Battle of Badr commenced. Though outnumbered more than three to one, the Muslims won the battle, killing at least forty-five Meccans with fourteen Muslims dead. They also succeeded in killing many Meccan leaders, including Abu Jahl. Seventy prisoners had been acquired, many of whom were ransomed. Muhammad and his followers saw the victory as confirmation of their faith and Muhammad ascribed the victory as assisted from an invisible host of angels. The Quranic verses of this period, unlike the Meccan verses, dealt with practical problems of government and issues like the distribution of spoils.

The victory strengthened Muhammad's position in Medina and dispelled earlier doubts among his followers. As a result, the opposition to him became less vocal. Pagans who had not yet converted were very bitter about the advance of Islam. Two pagans, Asma bint Marwan of the Aws Manat tribe and Abu 'Afak of the 'Amr b. 'Awf tribe, had composed verses taunting and insulting the Muslims. They were killed by people belonging to their own or related clans, and Muhammad did not disapprove of the killings. This report, however, is considered by some to be a fabrication. Most members of those tribes converted to Islam, and little pagan opposition remained.

Muhammad expelled from Medina the Banu Qaynuqa, one of three main Jewish tribes, but some historians contend that the expulsion happened after Muhammad's death. According to al-Waqidi, after Abd-Allah ibn Ubaiy spoke for them, Muhammad refrained from executing them and commanded that they be exiled from Medina. Following the Battle of Badr, Muhammad also made mutual-aid alliances with a number of Bedouin tribes to protect his community from attacks from the northern part of Hejaz.

The Meccans were eager to avenge their defeat. To maintain economic prosperity, the Meccans needed to restore their prestige, which had been reduced at Badr. In the ensuing months, the Meccans sent ambush parties to Medina while Muhammad led expeditions against tribes allied with Mecca and sent raiders onto a Meccan caravan. Abu Sufyan gathered an army of 3000 men and set out for an attack on Medina.

A scout alerted Muhammad of the Meccan army's presence and numbers a day later. The next morning, at the Muslim conference of war, a dispute arose over how best to repel the Meccans. Muhammad and many senior figures suggested it would be safer to fight within Medina and take advantage of the heavily fortified strongholds. Younger Muslims argued that the Meccans were destroying crops, and huddling in the strongholds would destroy Muslim prestige. Muhammad eventually conceded to the younger Muslims and readied the Muslim force for battle. Muhammad led his force outside to the mountain of Uhud (the location of the Meccan camp) and fought the Battle of Uhud on 23 March 625. Although the Muslim army had the advantage in early encounters, lack of discipline on the part of strategically placed archers led to a Muslim defeat; 75 Muslims were killed including Hamza, Muhammad's uncle who became one of the best known martyrs in the Muslim tradition. The Meccans did not pursue the Muslims, instead, they marched back to Mecca declaring victory. The announcement is probably because Muhammad was wounded and thought dead. When they discovered that Muhammad lived, the Meccans did not return due to false information about new forces coming to his aid. The attack had failed to achieve their aim of completely destroying the Muslims. The Muslims buried the dead and returned to Medina that evening. Questions accumulated about the reasons for the loss; Muhammad delivered Quranic verses indicating that the defeat was twofold: partly a punishment for disobedience, partly a test for steadfastness.

Abu Sufyan directed his effort towards another attack on Medina. He gained support from the nomadic tribes to the north and east of Medina; using propaganda about Muhammad's weakness, promises of booty, memories of Quraysh prestige and through bribery. Muhammad's new policy was to prevent alliances against him. Whenever alliances against Medina were formed, he sent out expeditions to break them up. Muhammad heard of men massing with hostile intentions against Medina, and reacted in a severe manner. One example is the assassination of Ka'b ibn al-Ashraf, a chieftain of the Jewish tribe of Banu Nadir. Al-Ashraf went to Mecca and wrote poems that roused the Meccans' grief, anger and desire for revenge after the Battle of Badr. Around a year later, Muhammad expelled the Banu Nadir from Medina forcing their emigration to Syria; he allowed them to take some possessions, as he was unable to subdue the Banu Nadir in their strongholds. The rest of their property was claimed by Muhammad in the name of God as it was not gained with bloodshed. Muhammad surprised various Arab tribes, individually, with overwhelming force, causing his enemies to unite to annihilate him. Muhammad's attempts to prevent a confederation against him were unsuccessful, though he was able to increase his own forces and stopped many potential tribes from joining his enemies.

With the help of the exiled Banu Nadir, the Quraysh military leader Abu Sufyan mustered a force of 10,000 men. Muhammad prepared a force of about 3,000 men and adopted a form of defense unknown in Arabia at that time; the Muslims dug a trench wherever Medina lay open to cavalry attack. The idea is credited to a Persian convert to Islam, Salman the Persian. The siege of Medina began on 31 March 627 and lasted two weeks. Abu Sufyan's troops were unprepared for the fortifications, and after an ineffectual siege, the coalition decided to return home. The Quran discusses this battle in sura Al-Ahzab, in verses .
During the battle, the Jewish tribe of Banu Qurayza, located to the south of Medina, entered into negotiations with Meccan forces to revolt against Muhammad. Although the Meccan forces were swayed by suggestions that Muhammad was sure to be overwhelmed, they desired reassurance in case the confederacy was unable to destroy him. No agreement was reached after prolonged negotiations, partly due to sabotage attempts by Muhammad's scouts. After the coalition's retreat, the Muslims accused the Banu Qurayza of treachery and besieged them in their forts for 25 days. The Banu Qurayza eventually surrendered; according to Ibn Ishaq, all the men apart from a few converts to Islam were beheaded, while the women and children were enslaved. Walid N. Arafat and Barakat Ahmad have disputed the accuracy of Ibn Ishaq's narrative. Arafat believes that Ibn Ishaq's Jewish sources, speaking over 100 years after the event, conflated this account with memories of earlier massacres in Jewish history; he notes that Ibn Ishaq was considered an unreliable historian by his contemporary Malik ibn Anas, and a transmitter of "odd tales" by the later Ibn Hajar. Ahmad argues that only some of the tribe were killed, while some of the fighters were merely enslaved. Watt finds Arafat's arguments "not entirely convincing", while Meir J. Kister has contradicted the arguments of Arafat and Ahmad.

In the siege of Medina, the Meccans exerted the available strength to destroy the Muslim community. The failure resulted in a significant loss of prestige; their trade with Syria vanished. Following the Battle of the Trench, Muhammad made two expeditions to the north, both ended without any fighting. While returning from one of these journeys (or some years earlier according to other early accounts), an accusation of adultery was made against Aisha, Muhammad's wife. Aisha was exonerated from accusations when Muhammad announced he had received a revelation confirming Aisha's innocence and directing that charges of adultery be supported by four eyewitnesses (sura 24, An-Nur).

Although Muhammad had delivered Quranic verses commanding the Hajj, the Muslims had not performed it due to Quraysh enmity. In the month of Shawwal 628, Muhammad ordered his followers to obtain sacrificial animals and to prepare for a pilgrimage ("umrah") to Mecca, saying that God had promised him the fulfillment of this goal in a vision when he was shaving his head after completion of the Hajj. Upon hearing of the approaching 1,400 Muslims, the Quraysh dispatched 200 cavalry to halt them. Muhammad evaded them by taking a more difficult route, enabling his followers to reach al-Hudaybiyya just outside Mecca. According to Watt, although Muhammad's decision to make the pilgrimage was based on his dream, he was also demonstrating to the pagan Meccans that Islam did not threaten the prestige of the sanctuaries, that Islam was an Arabian religion. 

Negotiations commenced with emissaries traveling to and from Mecca. While these continued, rumors spread that one of the Muslim negotiators, Uthman bin al-Affan, had been killed by the Quraysh. Muhammad called upon the pilgrims to make a pledge not to flee (or to stick with Muhammad, whatever decision he made) if the situation descended into war with Mecca. This pledge became known as the "Pledge of Acceptance" or the "Pledge under the Tree". News of Uthman's safety allowed for negotiations to continue, and a treaty scheduled to last ten years was eventually signed between the Muslims and Quraysh. The main points of the treaty included: cessation of hostilities, the deferral of Muhammad's pilgrimage to the following year, and agreement to send back any Meccan who emigrated to Medina without permission from their protector.

Many Muslims were not satisfied with the treaty. However, the Quranic sura "Al-Fath" (The Victory) (Quran ) assured them that the expedition must be considered a victorious one. It was later that Muhammad's followers realized the benefit behind the treaty. These benefits included the requirement of the Meccans to identify Muhammad as an equal, cessation of military activity allowing Medina to gain strength, and the admiration of Meccans who were impressed by the pilgrimage rituals.

After signing the truce, Muhammad assembled an expedition against the Jewish oasis of Khaybar, known as the Battle of Khaybar. This was possibly due to housing the Banu Nadir who were inciting hostilities against Muhammad, or to regain prestige from what appeared as the inconclusive result of the truce of Hudaybiyya. According to Muslim tradition, Muhammad also sent letters to many rulers, asking them to convert to Islam (the exact date is given variously in the sources). He sent messengers (with letters) to Heraclius of the Byzantine Empire (the eastern Roman Empire), Khosrau of Persia, the chief of Yemen and to some others. In the years following the truce of Hudaybiyya, Muhammad directed his forces against the Arabs on Transjordanian Byzantine soil in the Battle of Mu'tah.

The truce of Hudaybiyyah was enforced for two years. The tribe of Banu Khuza'a had good relations with Muhammad, whereas their enemies, the Banu Bakr, had allied with the Meccans. A clan of the Bakr made a night raid against the Khuza'a, killing a few of them. The Meccans helped the Banu Bakr with weapons and, according to some sources, a few Meccans also took part in the fighting. After this event, Muhammad sent a message to Mecca with three conditions, asking them to accept one of them. These were: either the Meccans would pay blood money for the slain among the Khuza'ah tribe, they disavow themselves of the Banu Bakr, or they should declare the truce of Hudaybiyyah null.

The Meccans replied that they accepted the last condition. Soon they realized their mistake and sent Abu Sufyan to renew the Hudaybiyyah treaty, a request that was declined by Muhammad.

Muhammad began to prepare for a campaign. In 630, Muhammad marched on Mecca with 10,000 Muslim converts. With minimal casualties, Muhammad seized control of Mecca. He declared an amnesty for past offences, except for ten men and women who were "guilty of murder or other offences or had sparked off the war and disrupted the peace". Some of these were later pardoned. Most Meccans converted to Islam and Muhammad proceeded to destroy all the statues of Arabian gods in and around the Kaaba. According to reports collected by Ibn Ishaq and al-Azraqi, Muhammad personally spared paintings or frescos of Mary and Jesus, but other traditions suggest that all pictures were erased. The Quran discusses the conquest of Mecca.

Following the conquest of Mecca, Muhammad was alarmed by a military threat from the confederate tribes of Hawazin who were raising an army double the size of Muhammad's. The Banu Hawazin were old enemies of the Meccans. They were joined by the Banu Thaqif (inhabiting the city of Ta'if) who adopted an anti-Meccan policy due to the decline of the prestige of Meccans. Muhammad defeated the Hawazin and Thaqif tribes in the Battle of Hunayn.

In the same year, Muhammad organized an attack against northern Arabia because of their previous defeat at the Battle of Mu'tah and reports of hostility adopted against Muslims. With great difficulty he assembled 30,000 men; half of whom on the second day returned with Abd-Allah ibn Ubayy, untroubled by the damning verses which Muhammad hurled at them. Although Muhammad did not engage with hostile forces at Tabuk, he received the submission of some local chiefs of the region.

He also ordered the destruction of any remaining pagan idols in Eastern Arabia. The last city to hold out against the Muslims in Western Arabia was Taif. Muhammad refused to accept the city's surrender until they agreed to convert to Islam and allowed men to destroy the statue of their goddess Al-Lat.

A year after the Battle of Tabuk, the Banu Thaqif sent emissaries to surrender to Muhammad and adopt Islam. Many bedouins submitted to Muhammad to safeguard against his attacks and to benefit from the spoils of war. However, the bedouins were alien to the system of Islam and wanted to maintain independence: namely their code of virtue and ancestral traditions. Muhammad required a military and political agreement according to which they "acknowledge the suzerainty of Medina, to refrain from attack on the Muslims and their allies, and to pay the Zakat, the Muslim religious levy."

In 632, at the end of the tenth year after migration to Medina, Muhammad completed his first true Islamic pilgrimage, setting precedence for the annual Great Pilgrimage, known as "Hajj". On the 9th of Dhu al-Hijjah Muhammad delivered his Farewell Sermon, at Mount Arafat east of Mecca. In this sermon, Muhammad advised his followers not to follow certain pre-Islamic customs. For instance, he said a white has no superiority over a black, nor a black has any superiority over a white except by piety and good action. He abolished old blood feuds and disputes based on the former tribal system and asked for old pledges to be returned as implications of the creation of the new Islamic community. Commenting on the vulnerability of women in his society, Muhammad asked his male followers to "be good to women, for they are powerless captives ("awan") in your households. You took them in God's trust, and legitimated your sexual relations with the Word of God, so come to your senses people, and hear my words ..." He told them that they were entitled to discipline their wives but should do so with kindness. He addressed the issue of inheritance by forbidding false claims of paternity or of a client relationship to the deceased and forbade his followers to leave their wealth to a testamentary heir. He also upheld the sacredness of four lunar months in each year. According to Sunni tafsir, the following Quranic verse was delivered during this event: "Today I have perfected your religion, and completed my favours for you and chosen Islam as a religion for you" (Quran ). According to Shia tafsir, it refers to the appointment of Ali ibn Abi Talib at the pond of Khumm as Muhammad's successor, this occurring a few days later when Muslims were returning from Mecca to Medina.

A few months after the farewell pilgrimage, Muhammad fell ill and suffered for several days with fever, head pain, and weakness. He died on Monday, 8 June 632, in Medina, at the age of 62 or 63, in the house of his wife Aisha. With his head resting on Aisha's lap, he asked her to dispose of his last worldly goods (seven coins), then spoke his final words: 

According to "Encyclopaedia of Islam", Muhammad's death may be presumed to have been caused by Medinan fever exacerbated by physical and mental fatigue.

Academics Reşit Haylamaz and Fatih Harpci say that "Ar-Rafiq Al-A'la" is referring to God. He was buried where he died in Aisha's house. During the reign of the Umayyad caliph al-Walid I, al-Masjid an-Nabawi (the Mosque of the Prophet) was expanded to include the site of Muhammad's tomb. The Green Dome above the tomb was built by the Mamluk sultan Al Mansur Qalawun in the 13th century, although the green color was added in the 16th century, under the reign of Ottoman sultan Suleiman the Magnificent. Among tombs adjacent to that of Muhammad are those of his companions (Sahabah), the first two Muslim caliphs Abu Bakr and Umar, and an empty one that Muslims believe awaits Jesus.
When bin Saud took Medina in 1805, Muhammad's tomb was stripped of its gold and jewel ornaments. Adherents to Wahhabism, bin Saud's followers destroyed nearly every tomb dome in Medina in order to prevent their veneration, and the one of Muhammad is said to have narrowly escaped. Similar events took place in 1925 when the Saudi militias retook—and this time managed to keep—the city. In the Wahhabi interpretation of Islam, burial is to take place in unmarked graves. Although frowned upon by the Saudis, many pilgrims continue to practice a ziyarat—a ritual visit—to the tomb.

Muhammad united several of the tribes of Arabia into a single Arab Muslim religious polity in the last years of his life. With Muhammad's death, disagreement broke out over who his successor would be. Umar ibn al-Khattab, a prominent companion of Muhammad, nominated Abu Bakr, Muhammad's friend and collaborator. With additional support Abu Bakr was confirmed as the first caliph. This choice was disputed by some of Muhammad's companions, who held that Ali ibn Abi Talib, his cousin and son-in-law, had been designated the successor by Muhammad at Ghadir Khumm. Abu Bakr immediately moved to strike against the Byzantine (or Eastern Roman Empire) forces because of the previous defeat, although he first had to put down a rebellion by Arab tribes in an event that Muslim historians later referred to as the Ridda wars, or "Wars of Apostasy".

The pre-Islamic Middle East was dominated by the Byzantine and Sassanian empires. The Roman–Persian Wars between the two had devastated the region, making the empires unpopular amongst local tribes. Furthermore, in the lands that would be conquered by Muslims many Christians (Nestorians, Monophysites, Jacobites and Copts) were disaffected from the Eastern Orthodox Church which deemed them heretics. Within a decade Muslims conquered Mesopotamia, Byzantine Syria, Byzantine Egypt, large parts of Persia, and established the Rashidun Caliphate.

According to William Montgomery Watt, religion for Muhammad was not a private and individual matter but "the total response of his personality to the total situation in which he found himself. He was responding [not only]... to the religious and intellectual aspects of the situation but also to the economic, social, and political pressures to which contemporary Mecca was subject." Bernard Lewis says there are two important political traditions in Islam – Muhammad as a statesman in Medina, and Muhammad as a rebel in Mecca. In his view, Islam is a great change, akin to a revolution, when introduced to new societies.

Historians generally agree that Islamic social changes in areas such as social security, family structure, slavery and the rights of women and children improved on the "status quo" of Arab society. For example, according to Lewis, Islam "from the first denounced aristocratic privilege, rejected hierarchy, and adopted a formula of the career open to the talents". Muhammad's message transformed society and moral orders of life in the Arabian Peninsula; society focused on the changes to perceived identity, world view, and the hierarchy of values.
Economic reforms addressed the plight of the poor, which was becoming an issue in pre-Islamic Mecca. The Quran requires payment of an alms tax (zakat) for the benefit of the poor; as Muhammad's power grew he demanded that tribes who wished to ally with him implement the zakat in particular.

The description given in Muhammad al-Bukhari's book Sahih al-Bukhari, in Chapter 61, Hadith 57 & Hadith 60, is depicted by two of his companions as:

The description given in Muhammad ibn Isa at-Tirmidhi's book Shama'il al-Mustafa, attributed to Ali ibn Abi Talib and Hind ibn Abi Hala is as follows:

The "seal of prophecy" between Muhammad's shoulders is generally described as having been a type of raised mole the size of a pigeon's egg. Another description of Muhammad was provided by Umm Ma'bad, a woman he met on his journey to Medina:

Descriptions like these were often reproduced in calligraphic panels ("hilya" or, in Turkish, "hilye"), which in the 17th century developed into an art form of their own in the Ottoman Empire.

Muhammad's life is traditionally defined into two periods: pre-hijra (emigration) in Mecca (from 570 to 622), and post-hijra in Medina (from 622 until 632). Muhammad is said to have had thirteen wives in total (although two have ambiguous accounts, Rayhana bint Zayd and Maria al-Qibtiyya, as wife or concubine.) Eleven of the thirteen marriages occurred after the migration to Medina.

At the age of 25, Muhammad married the wealthy Khadijah bint Khuwaylid who was 40 years old. The marriage lasted for 25 years and was a happy one. Muhammad did not enter into marriage with another woman during this marriage. After Khadijah's death, Khawla bint Hakim suggested to Muhammad that he should marry Sawda bint Zama, a Muslim widow, or Aisha, daughter of Um Ruman and Abu Bakr of Mecca. Muhammad is said to have asked for arrangements to marry both. Muhammad's marriages after the death of Khadijah were contracted mostly for political or humanitarian reasons. The women were either widows of Muslims killed in battle and had been left without a protector, or belonged to important families or clans whom it was necessary to honor and strengthen alliances with.

According to traditional sources Aisha was six or seven years old when betrothed to Muhammad, with the marriage not being consummated until she had reached puberty at the age of nine or ten years old. She was therefore a virgin at marriage. Modern Muslim authors who calculate Aisha's age based on other sources of information, such that available about her sister Asma about whom more is known, estimate that she was over thirteen and perhaps in her late teens at the time of her marriage.

After migration to Medina, Muhammad, who was then in his fifties, married several more women.

Muhammad performed household chores such as preparing food, sewing clothes, and repairing shoes. He is also said to have had accustomed his wives to dialogue; he listened to their advice, and the wives debated and even argued with him.

Khadijah is said to have had four daughters with Muhammad (Ruqayyah bint Muhammad, Umm Kulthum bint Muhammad, Zainab bint Muhammad, Fatimah Zahra) and two sons (Abd-Allah ibn Muhammad and Qasim ibn Muhammad, who both died in childhood). All but one of his daughters, Fatimah, died before him. Some Shi'a scholars contend that Fatimah was Muhammad's only daughter. Maria al-Qibtiyya bore him a son named Ibrahim ibn Muhammad, but the child died when he was two years old.

Nine of Muhammad's wives survived him. Aisha, who became known as Muhammad's favourite wife in Sunni tradition, survived him by decades and was instrumental in helping assemble the scattered sayings of Muhammad that form the Hadith literature for the Sunni branch of Islam.

Muhammad's descendants through Fatimah are known as "sharifs", "syeds" or "sayyids". These are honorific titles in Arabic, "sharif" meaning 'noble' and "sayed" or "sayyid" meaning 'lord' or 'sir'. As Muhammad's only descendants, they are respected by both Sunni and Shi'a, though the Shi'a place much more emphasis and value on their distinction.

Zayd ibn Haritha was a slave that Muhammad bought, freed, and then adopted as his son. He also had a wetnurse. According to a BBC summary, "the Prophet Muhammad did not try to abolish slavery, and bought, sold, captured, and owned slaves himself. But he insisted that slave owners treat their slaves well and stressed the virtue of freeing slaves. Muhammad treated slaves as human beings and clearly held some in the highest esteem".

Following the attestation to the oneness of God, the belief in Muhammad's prophethood is the main aspect of the Islamic faith. Every Muslim proclaims in "Shahadah": "I testify that there is no god but God, and I testify that Muhammad is a Messenger of God." The Shahadah is the basic creed or tenet of Islam. Islamic belief is that ideally the Shahadah is the first words a newborn will hear; children are taught it immediately and it will be recited upon death. Muslims repeat the shahadah in the call to prayer ("adhan") and the prayer itself. Non-Muslims wishing to convert to Islam are required to recite the creed.

In Islamic belief, Muhammad is regarded as the last prophet sent by God. states that "...it (the Quran) is a confirmation of (revelations) that went before it, and a fuller explanation of the Book – wherein there is no doubt – from The Lord of the Worlds.". Similarly states "...And before this was the book of Moses, as a guide and a mercy. And this Book confirms (it)...", while commands the believers of Islam to "Say: we believe in God and that which is revealed unto us, and that which was revealed unto Abraham and Ishmael and Isaac and Jacob and the tribes, and that which Moses and Jesus received, and which the prophets received from their Lord. We make no distinction between any of them, and unto Him we have surrendered."

Muslim tradition credits Muhammad with several miracles or supernatural events. For example, many Muslim commentators and some Western scholars have interpreted the Surah as referring to Muhammad splitting the Moon in view of the Quraysh when they began persecuting his followers. Western historian of Islam Denis Gril believes the Quran does not overtly describe Muhammad performing miracles, and the supreme miracle of Muhammad is identified with the Quran itself.

According to Islamic tradition, Muhammad was attacked by the people of Ta'if and was badly injured. The tradition also describes an angel appearing to him and offering retribution against the assailants. It is said that Muhammad rejected the offer and prayed for the guidance of the people of Ta'if.

The Sunnah represents actions and sayings of Muhammad (preserved in reports known as Hadith), and covers a broad array of activities and beliefs ranging from religious rituals, personal hygiene, burial of the dead to the mystical questions involving the love between humans and God. The Sunnah is considered a model of emulation for pious Muslims and has to a great degree influenced the Muslim culture. The greeting that Muhammad taught Muslims to offer each other, "may peace be upon you" (Arabic: "as-salamu 'alaykum") is used by Muslims throughout the world. Many details of major Islamic rituals such as daily prayers, the fasting and the annual pilgrimage are only found in the Sunnah and not the Quran.

The Sunnah contributed much to the development of Islamic law, particularly from the end of the first Islamic century. Muslim mystics, known as sufis, who were seeking for the inner meaning of the Quran and the inner nature of Muhammad, viewed the prophet of Islam not only as a prophet but also as a perfect human-being. All Sufi orders trace their chain of spiritual descent back to Muhammad.

Muslims have traditionally expressed love and veneration for Muhammad. Stories of Muhammad's life, his intercession and of his miracles (particularly "Splitting of the moon") have permeated popular Muslim thought and poetry. Among Arabic odes to Muhammad, Qasidat al-Burda ("Poem of the Mantle") by the Egyptian Sufi al-Busiri (1211–1294) is particularly well known, and widely held to possess a healing, spiritual power. The Quran refers to Muhammad as "a mercy ("rahmat") to the worlds" (Quran ). The association of rain with mercy in Oriental countries has led to imagining Muhammad as a rain cloud dispensing blessings and stretching over lands, reviving the dead hearts, just as rain revives the seemingly dead earth (see, for example, the Sindhi poem of Shah ʿAbd al-Latif). Muhammad's birthday is celebrated as a major feast throughout the Islamic world, excluding Wahhabi-dominated Saudi Arabia where these public celebrations are discouraged. When Muslims say or write the name of Muhammad, they usually follow it with "may God honor him and grant him peace" (Arabic: "ṣallā llahu ʿalayhi wa-sallam"). In casual writing, this is sometimes abbreviated as PBUH or SAW; in printed matter, a small calligraphic rendition is commonly used ().

In line with the hadith's prohibition against creating images of sentient living beings, which is particularly strictly observed with respect to God and Muhammad, Islamic religious art is focused on the word. Muslims generally avoid depictions of Muhammad, and mosques are decorated with calligraphy and Quranic inscriptions or geometrical designs, not images or sculptures. Today, the interdiction against images of Muhammad – designed to prevent worship of Muhammad, rather than God – is much more strictly observed in Sunni Islam (85%–90% of Muslims) and Ahmadiyya Islam (1%) than among Shias (10%–15%). While both Sunnis and Shias have created images of Muhammad in the past, Islamic depictions of Muhammad are rare. They have mostly been limited to the private and elite medium of the miniature, and since about 1500 most depictions show Muhammad with his face veiled, or symbolically represent him as a flame.

The earliest extant depictions come from 13th century Anatolian Seljuk and Ilkhanid Persian miniatures, typically in literary genres describing the life and deeds of Muhammad. During the Ilkhanid period, when Persia's Mongol rulers converted to Islam, competing Sunni and Shi'a groups used visual imagery, including images of Muhammad, to promote their particular interpretation of Islam's key events. Influenced by the Buddhist tradition of representational religious art predating the Mongol elite's conversion, this innovation was unprecedented in the Islamic world, and accompanied by a "broader shift in Islamic artistic culture away from abstraction toward representation" in "mosques, on tapestries, silks, ceramics, and in glass and metalwork" besides books. In the Persian lands, this tradition of realistic depictions lasted through the Timurid dynasty until the Safavids took power in the early 16th century. The Safavaids, who made Shi'i Islam the state religion, initiated a departure from the traditional Ilkhanid and Timurid artistic style by covering Muhammad's face with a veil to obscure his features and at the same time represent his luminous essence. Concomitantly, some of the unveiled images from earlier periods were defaced. Later images were produced in Ottoman Turkey and elsewhere, but mosques were never decorated with images of Muhammad. Illustrated accounts of the night journey ("mi'raj") were particularly popular from the Ilkhanid period through the Safavid era. During the 19th century, Iran saw a boom of printed and illustrated "mi'raj" books, with Muhammad's face veiled, aimed in particular at illiterates and children in the manner of graphic novels. Reproduced through lithography, these were essentially "printed manuscripts". Today, millions of historical reproductions and modern images are available in some Muslim-majority countries, especially Turkey and Iran, on posters, postcards, and even in coffee-table books, but are unknown in most other parts of the Islamic world, and when encountered by Muslims from other countries, they can cause considerable consternation and offense.

The earliest documented Christian knowledge of Muhammad stems from Byzantine sources. They indicate that both Jews and Christians saw Muhammad as a false prophet. Another Greek source for Muhammad is Theophanes the Confessor, a 9th-century writer. The earliest Syriac source is the 7th-century writer John bar Penkaye.

According to Hossein Nasr, the earliest European literature often refers to Muhammad unfavorably. A few learned circles of Middle Ages Europeprimarily Latin-literate scholarshad access to fairly extensive biographical material about Muhammad. They interpreted the biography through a Christian religious filter; one that viewed Muhammad as a person who seduced the Saracens into his submission under religious guise. Popular European literature of the time portrayed Muhammad as though he were worshipped by Muslims, similar to an idol or a heathen god.

In later ages, Muhammad came to be seen as a schismatic: Brunetto Latini's 13th century "Li livres dou tresor" represents him as a former monk and cardinal, and Dante's "Divine Comedy" (Inferno, Canto 28), written in the early 1300s, puts Muhammad and his son-in-law, Ali, in Hell "among the sowers of discord and the schismatics, being lacerated by devils again and again."

After the Reformation, Muhammad was often portrayed in a similar way. Guillaume Postel was among the first to present a more positive view of Muhammad when he argued that Muhammad should be esteemed by Christians as a valid prophet. Gottfried Leibniz praised Muhammad because "he did not deviate from the natural religion". Henri de Boulainvilliers, in his "Vie de Mahomed" which was published posthumously in 1730, described Muhammad as a gifted political leader and a just lawmaker. He presents him as a divinely inspired messenger whom God employed to confound the bickering Oriental Christians, to liberate the Orient from the despotic rule of the Romans and Persians, and to spread the knowledge of the unity of God from India to Spain. Voltaire had a somewhat mixed opinion on Muhammad: in his play "Le fanatisme, ou Mahomet le Prophète" he vilifies Muhammad as a symbol of fanaticism, and in a published essay in 1748 he calls him "a sublime and hearty charlatan", but in his historical survey "Essai sur les mœurs", he presents him as legislator and a conqueror and calls him an "enthusiast." Jean-Jacques Rousseau, in his Social Contract (1762), "brushing aside hostile legends of Muhammad as a trickster and impostor, presents him as a sage legislator who wisely fused religious and political powers." Emmanuel Pastoret published in 1787 his "Zoroaster, Confucius and Muhammad", in which he presents the lives of these three "great men", "the greatest legislators of the universe", and compares their careers as religious reformers and lawgivers. He rejects the common view that Muhammad is an impostor and argues that the Quran proffers "the most sublime truths of cult and morals"; it defines the unity of God with an "admirable concision." Pastoret writes that the common accusations of his immorality are unfounded: on the contrary, his law enjoins sobriety, generosity, and compassion on his followers: the "legislator of Arabia" was "a great man." Napoleon Bonaparte admired Muhammad and Islam, and described him as a model lawmaker and a great man. Thomas Carlyle in his book "Heroes and Hero Worship and the Heroic in History" (1840) describes Muhammad as "[a] silent great soul; [...] one of those who cannot "but" be in earnest". Carlyle's interpretation has been widely cited by Muslim scholars as a demonstration that Western scholarship validates Muhammad's status as a great man in history.

Ian Almond says that German Romantic writers generally held positive views of Muhammad: "Goethe’s “extraordinary” poet-prophet, Herder’s nation builder (...) Schlegel’s admiration for Islam as an aesthetic product, enviably authentic, radiantly holistic, played such a central role in his view of Mohammed as an exemplary world-fashioner that he even used it as a scale of judgement for the classical (the dithyramb, we are told, has to radiate pure beauty if it is to resemble “a Koran of poetry”.)" After quoting Heinrich Heine, who said in a letter to some friend that "I must admit that you, great prophet of Mecca, are the greatest poet and that your Quran... will not easily escape my memory", John Tolan goes on to show how Jews in Europe in particular held more nuanced views about Muhammad and Islam, being an ethnoreligious minority feeling discriminated, they specifically lauded Al-Andalus, and thus, "writing about Islam was for Jews a way of indulging in a fantasy world, far from the persecution and pogroms of nineteenth-century Europe, where Jews could live in harmony with their non-Jewish neighbors."

Recent writers such as William Montgomery Watt and Richard Bell dismiss the idea that Muhammad deliberately deceived his followers, arguing that Muhammad "was absolutely sincere and acted in complete good faith" and Muhammad's readiness to endure hardship for his cause, with what seemed to be no rational basis for hope, shows his sincerity. Watt, however, says that sincerity does not directly imply correctness: In contemporary terms, Muhammad might have mistaken his subconscious for divine revelation. Watt and Bernard Lewis argue that viewing Muhammad as a self-seeking impostor makes it impossible to understand Islam's development. Alford T. Welch holds that Muhammad was able to be so influential and successful because of his firm belief in his vocation.

Bahá'ís venerate Muhammad as one of a number of prophets or "Manifestations of God". He is thought to be the final manifestation, or seal of the Adamic cycle, but consider his teachings to have been superseded by those of Bahá'u'lláh, the founder of the Bahai faith, and the first of Manifestation of the current cycle.

As early as the 7th century Muhammad was attacked by non-Muslim Arab contemporaries for preaching monotheism. In modern times, criticism has also dealt with Muhammad's sincerity in claiming to be a prophet, his morality, warfare, and his marriages.





</doc>
<doc id="18935" url="https://en.wikipedia.org/wiki?curid=18935" title="Morse code">
Morse code

Morse code is a method of transmitting text information as a series of on-off tones, lights, or clicks that can be directly understood by a skilled listener or observer without special equipment. It is named for Samuel F. B. Morse, an inventor of the telegraph. The International Morse Code encodes the ISO basic Latin alphabet, some extra Latin letters, the Arabic numerals and a small set of punctuation and procedural signals (prosigns) as standardized sequences of short and long signals called "dots" and "dashes", or "dits" and "dahs", as in amateur radio practice. Because many non-English natural languages use more than the 26 Roman letters, extensions to the Morse alphabet exist for those languages.

Each Morse code symbol represents either a text character (letter or numeral) or a prosign and is represented by a unique sequence of dots and dashes. The dot duration is the basic unit of time measurement in code transmission. The duration of a dash is three times the duration of a dot. Each dot or dash is followed by a short silence, equal to the dot duration. The letters of a word are separated by a space equal to three dots (one dash), and the words are separated by a space equal to seven dots. To increase the speed of the communication, the code was designed so that the length of each character in Morse is approximately inverse to its frequency of occurrence in English. Thus the most common letter in English, the letter "E", has the shortest code, a single dot.
In an emergency, Morse code can be sent by improvised methods that can be easily "keyed" on and off, making it one of the simplest and most versatile methods of telecommunication. The most common distress signal is SOS – three dots, three dashes, and three dots – internationally recognized by treaty.
Beginning in 1836, the American artist Samuel F. B. Morse, the American physicist Joseph Henry, and Alfred Vail developed an electrical telegraph system. This system sent pulses of electric current along wires which controlled an electromagnet that was located at the receiving end of the telegraph system. A code was needed to transmit natural language using only these pulses, and the silence between them. Around 1837, Morse, therefore, developed an early forerunner to the modern International Morse code. Around the same time, Carl Friedrich Gauss and Wilhelm Eduard Weber (1833) as well as Carl August von Steinheil (1837) had already used codes with varying word lengths for their telegraphs. Since around 1800, European experimenters had been making progress with earlier battery-powered signaling systems in emitting oxygen and hydrogen bubbles through liquid, flipping magnetic semaphore flags, tripping alarms across long distances over wire, and other techniques. The numerous ingenious experimental encoding designs they devised and demonstrated were telegraphic precursors to practical applications.

In 1837, William Cooke and Charles Wheatstone in England began using an electrical telegraph that also used electromagnets in its receivers. However, in contrast with any system of making sounds of clicks, their system used pointing needles that rotated above alphabetical charts to indicate the letters that were being sent. In 1841, Cooke and Wheatstone built a telegraph that printed the letters from a wheel of typefaces struck by a hammer. This machine was based on their 1840 telegraph and worked well; however, they failed to find customers for this system and only two examples were ever built.

On the other hand, the three Americans' system for telegraphy, which was first used in about 1844, was designed to make indentations on a paper tape when electric currents were received. Morse's original telegraph receiver used a mechanical clockwork to move a paper tape. When an electrical current was received, an electromagnet engaged an armature that pushed a stylus onto the moving paper tape, making an indentation on the tape. When the current was interrupted, a spring retracted the stylus, and that portion of the moving tape remained unmarked.

The Morse code was developed so that operators could translate the indentations marked on the paper tape into text messages. In his earliest code, Morse had planned to transmit only numerals, and to use a codebook to look up each word according to the number which had been sent. However, the code was soon expanded by Alfred Vail in 1840 to include letters and special characters, so it could be used more generally. Vail estimated the frequency of use of letters in the English language by counting the movable type he found in the type-cases of a local newspaper in Morristown. The shorter marks were called "dots", and the longer ones "dashes", and the letters most commonly used were assigned the shorter sequences of dots and dashes. This code was used since 1844 and became known as "Morse landline code" or "American Morse code".

In the original Morse telegraphs, the receiver's armature made a clicking noise as it moved in and out of position to mark the paper tape. The telegraph operators soon learned that they could translate the clicks directly into dots and dashes, and write these down by hand, thus making the paper tape unnecessary. When Morse code was adapted to radio communication, the dots and dashes were sent as short and long tone pulses. It was later found that people become more proficient at receiving Morse code when it is taught as a language that is heard, instead of one read from a page.

To reflect the sounds of Morse code receivers, the operators began to vocalize a dot as "dit", and a dash as "dah". Dots which are not the final element of a character became vocalized as "di". For example, the letter "c" was then vocalized as "dah-di-dah-dit". Morse code was sometimes facetiously known as "iddy-umpty", and a dash as "umpty", leading to the word "umpteen".

The Morse code, as it is used internationally today, was derived from a much refined proposal which became known as "Hamburg alphabet" by Friedrich Clemens Gerke in 1848. It was adopted by the Deutsch-Österreichischer Telegraphenverein (German-Austrian Telegraph Society) in 1851. This finally led to the International Morse code in 1865.

In the 1890s, Morse code began to be used extensively for early radio communication, before it was possible to transmit voice. In the late 19th and early 20th centuries, most high-speed international communication used Morse code on telegraph lines, undersea cables and radio circuits. In aviation, Morse code in radio systems started to be used on a regular basis in the 1920s. Although previous transmitters were bulky and the spark gap system of transmission was difficult to use, there had been some earlier attempts. In 1910, the US Navy experimented with sending Morse from an airplane. That same year, a radio on the airship "America" had been instrumental in coordinating the rescue of its crew. Zeppelin airships equipped with radio were used for bombing and naval scouting during World War I, and ground-based radio direction finders were used for airship navigation. Allied airships and military aircraft also made some use of radiotelegraphy. However, there was little aeronautical radio in general use during World War I, and in the 1920s, there was no radio system used by such important flights as that of Charles Lindbergh from New York to Paris in 1927. Once he and the "Spirit of St. Louis" were off the ground, Lindbergh was truly alone and incommunicado. On the other hand, when the first airplane flight was made from California to Australia in 1928 on the "Southern Cross", one of its four crewmen was its radio operator who communicated with ground stations via radio telegraph.

Beginning in the 1930s, both civilian and military pilots were required to be able to use Morse code, both for use with early communications systems and for identification of navigational beacons which transmitted continuous two- or three-letter identifiers in Morse code. Aeronautical charts show the identifier of each navigational aid next to its location on the map.

Radiotelegraphy using Morse code was vital during World War II, especially in carrying messages between the warships and the naval bases of the belligerents. Long-range ship-to-ship communication was by radio telegraphy, using encrypted messages because the voice radio systems on ships then were quite limited in both their range and their security. Radiotelegraphy was also extensively used by warplanes, especially by long-range patrol planes that were sent out by those navies to scout for enemy warships, cargo ships, and troop ships.

In addition, rapidly moving armies in the field could not have fought effectively without radiotelegraphy because they moved more rapidly than telegraph and telephone lines could be erected. This was seen especially in the blitzkrieg offensives of the Nazi German Wehrmacht in Poland, Belgium, France (in 1940), the Soviet Union, and in North Africa; by the British Army in North Africa, Italy, and the Netherlands; and by the U.S. Army in France and Belgium (in 1944), and in southern Germany in 1945.
Morse code was used as an international standard for maritime distress until 1999 when it was replaced by the Global Maritime Distress Safety System. When the French Navy ceased using Morse code on January 31, 1997, the final message transmitted was "Calling all. This is our last cry before our eternal silence." In the United States the final commercial Morse code transmission was on July 12, 1999, signing off with Samuel Morse's original 1844 message, "What hath God wrought", and the prosign "SK".

As of 2015, the United States Air Force still trains ten people a year in Morse. The United States Coast Guard has ceased all use of Morse code on the radio, and no longer monitors any radio frequencies for Morse code transmissions, including the international medium frequency (MF) distress frequency of 500 kHz. However, the Federal Communications Commission still grants commercial radiotelegraph operator licenses to applicants who pass its code and written tests. Licensees have reactivated the old California coastal Morse station KPH and regularly transmit from the site under either this Call sign or as KSM. Similarly, a few US Museum ship stations are operated by Morse enthusiasts.

Morse code speed is measured in words per minute (wpm) or characters per minute (cpm). Characters have differing lengths because they contain differing numbers of dots and dashes. Consequently, words also have different lengths in terms of dot duration, even when they contain the same number of characters. For this reason, a standard word is helpful to measure operator transmission speed. "PARIS" and "CODEX" are two such standard words. Operators skilled in Morse code can often understand ("copy") code in their heads at rates in excess of 40 wpm.

In addition to knowing, understanding, and being able to copy the standard written alpha-numeric and punctuation characters or symbols at high speeds, skilled high speed operators must also be fully knowledgeable of all of the special unwritten Morse code symbols for the standard Prosigns for Morse code and the meanings of these special procedural signals in standard Morse code communications protocol.

International contests in code copying are still occasionally held. In July 1939 at a contest in Asheville, North Carolina in the United States Ted R. McElroy set a still-standing record for Morse copying, 75.2 wpm. William Pierpont N0HFF also notes that some operators may have passed 100 wpm. By this time, they are "hearing" phrases and sentences rather than words. The fastest speed ever sent by a straight key was achieved in 1942 by Harry Turner W9YZE (d. 1992) who reached 35 wpm in a demonstration at a U.S. Army base. To accurately compare code copying speed records of different eras it is useful to keep in mind that different standard words (50 dot durations versus 60 dot durations) and different interword gaps (5 dot durations versus 7 dot durations) may have been used when determining such speed records. For example, speeds run with the CODEX standard word and the PARIS standard may differ by up to 20%.

Today among amateur operators there are several organizations that recognize high-speed code ability, one group consisting of those who can copy Morse at 60 wpm. Also, Certificates of Code Proficiency are issued by several amateur radio societies, including the American Radio Relay League. Their basic award starts at 10 wpm with endorsements as high as 40 wpm, and are available to anyone who can copy the transmitted text. Members of the Boy Scouts of America may put a Morse interpreter's strip on their uniforms if they meet the standards for translating code at 5 wpm.

Morse code has been in use for more than 160 years—longer than any other electrical coding system. What is called Morse code today is actually somewhat different from what was originally developed by Vail and Morse. The Modern International Morse code, or "continental code", was created by Friedrich Clemens Gerke in 1848 and initially used for telegraphy between Hamburg and Cuxhaven in Germany. Gerke changed nearly half of the alphabet and all of the numerals, providing the foundation for the modern form of the code. After some minor changes, International Morse Code was standardized at the International Telegraphy Congress in 1865 in Paris and was later made the standard by the International Telecommunication Union (ITU). Morse's original code specification, largely limited to use in the United States and Canada, became known as American Morse code or "railroad code". American Morse code is now seldom used except in historical re-enactments.

In aviation, pilots use radio navigation aids. To ensure that the stations the pilots are using are serviceable, the stations transmit a set of identification letters (usually a two-to-five-letter version of the station name) in Morse code. Station identification letters are shown on air navigation charts. For example, the VOR-DME based at Vilo Acuña Airport in Cayo Largo del Sur, Cuba is coded as "UCL", and UCL in Morse code is transmitted on its radio frequency. In some countries, during periods of maintenance, the facility may radiate a T-E-S-T code () or the code may be removed which tells pilots and navigators that the station is unreliable. In Canada, the identification is removed entirely to signify the navigation aid is not to be used. In the aviation service, Morse is typically sent at a very slow speed of about 5 words per minute. In the U.S., pilots do not actually have to know Morse to identify the transmitter because the dot/dash sequence is written out next to the transmitter's symbol on aeronautical charts. Some modern navigation receivers automatically translate the code into displayed letters.

International Morse code today is most popular among amateur radio operators, in the mode commonly referred to as "continuous wave" or "CW". (This name was chosen to distinguish it from the damped wave emissions from spark transmitters, not because the transmission is continuous.) Other keying methods are available in radio telegraphy, such as frequency shift keying.

The original amateur radio operators used Morse code exclusively since voice-capable radio transmitters did not become commonly available until around 1920. Until 2003, the International Telecommunication Union mandated Morse code proficiency as part of the amateur radio licensing procedure worldwide. However, the World Radiocommunication Conference of 2003 made the Morse code requirement for amateur radio licensing optional. Many countries subsequently removed the Morse requirement from their licence requirements.

Until 1991, a demonstration of the ability to send and receive Morse code at a minimum of five words per minute (wpm) was required to receive an amateur radio license for use in the United States from the Federal Communications Commission. Demonstration of this ability was still required for the privilege to use the HF bands. Until 2000, proficiency at the 20 wpm level was required to receive the highest level of amateur license (Amateur Extra Class); effective April 15, 2000, the FCC reduced the Extra Class requirement to five wpm. Finally, effective on February 23, 2007, the FCC eliminated the Morse code proficiency requirements from all amateur radio licenses.

While voice and data transmissions are limited to specific amateur radio bands under U.S. rules, Morse code is permitted on all amateur bands—LF, MF, HF, VHF, and UHF. In some countries, certain portions of the amateur radio bands are reserved for transmission of Morse code signals only.

The relatively limited speed at which Morse code can be sent led to the development of an extensive number of abbreviations to speed communication. These include prosigns, Q codes, and a set of Morse code abbreviations for typical message components. For example, CQ is broadcast to be interpreted as "seek you" (I'd like to converse with anyone who can hear my signal). OM (old man), YL (young lady) and XYL ("ex-YL" – wife) are common abbreviations. YL or OM is used by an operator when referring to the other operator, XYL or OM is used by an operator when referring to his or her spouse. QTH is "location" ("My QTH" is "My location"). The use of abbreviations for common terms permits conversation even when the operators speak different languages.

Although the traditional telegraph key (straight key) is still used by some amateurs, the use of mechanical semi-automatic keyers (known as "bugs") and of fully automatic electronic keyers is prevalent today. Software is also frequently employed to produce and decode Morse code radio signals.

Many amateur radio repeaters identify with Morse, even though they are used for voice communications.

Through May 2013, the First, Second, and Third Class (commercial) Radiotelegraph Licenses using code tests based upon the CODEX standard word were still being issued in the United States by the Federal Communications Commission. The First Class license required 20 WPM code group and 25 WPM text code proficiency, the others 16 WPM code group test (five letter blocks sent as simulation of receiving encrypted text) and 20 WPM code text (plain language) test. It was also necessary to pass written tests on operating practice and electronics theory. A unique additional demand for the First Class was a requirement of a year of experience for operators of shipboard and coast stations using Morse. This allowed the holder to be chief operator on board a passenger ship. However, since 1999 the use of satellite and very high-frequency maritime communications systems (GMDSS) has made them obsolete. (By that point meeting experience requirement for the First was very difficult.) Currently, only one class of license, the Radiotelegraph Operator License, is issued. This is granted either when the tests are passed or as the Second and First are renewed and become this lifetime license. For new applicants, it requires passing a written examination on electronic theory and radiotelegraphy practices, as well as 16 WPM codegroup and 20 WPM text tests. However, the code exams are currently waived for holders of Amateur Extra Class licenses who obtained their operating privileges under the old 20 WPM test requirement.

Radio navigation aids such as VORs and NDBs for aeronautical use broadcast identifying information in the form of Morse Code, though many VOR stations now also provide voice identification. Warships, including those of the U.S. Navy, have long used signal lamps to exchange messages in Morse code. Modern use continues, in part, as a way to communicate while maintaining radio silence.

ATIS (Automatic Transmitter Identification System) uses Morse code to identify uplink sources of analog satellite transmissions.

An important application is signalling for help through SOS, "". This can be sent many ways: keying a radio on and off, flashing a mirror, toggling a flashlight, and similar methods. SOS is not three separate characters, rather, it is a prosign SOS, and is keyed without gaps between characters.

Some Nokia mobile phones offer an option to alert the user of an incoming text message with the Morse tone "" (representing SMS or Short Message Service). In addition, applications are now available for mobile phones that enable short messages to be input in Morse Code.

Morse code has been employed as an assistive technology, helping people with a variety of disabilities to communicate. Morse can be sent by persons with severe motion disabilities, as long as they have some minimal motor control. An original solution to the problem that caretakers have to learn to decode has been an electronic typewriter with the codes written on the keys. Codes were sung by users; see the voice typewriter employing morse or votem, Newell and Nabarro, 1968.

Morse code can also be translated by computer and used in a speaking communication aid. In some cases, this means alternately blowing into and sucking on a plastic tube ("sip-and-puff" interface). An important advantage of Morse code over row column scanning is that once learned, it does not require looking at a display. Also, it appears faster than scanning.

People with severe motion disabilities in addition to sensory disabilities (e.g. people who are also deaf or blind) can receive Morse through a skin buzzer. .

In one case reported in the radio amateur magazine "QST", an old shipboard radio operator who had a stroke and lost the ability to speak or write could communicate with his physician (a radio amateur) by blinking his eyes in Morse. Two examples of communication in intensive care units were also published in "QST", Another example occurred in 1966 when prisoner of war Jeremiah Denton, brought on television by his North Vietnamese captors, Morse-blinked the word "TORTURE". In these two cases, interpreters were available to understand those series of eye-blinks.

International Morse code is composed of five elements:


Morse code can be transmitted in a number of ways: originally as electrical pulses along a telegraph wire, but also as an audio tone, a radio signal with short and long tones, or as a mechanical, audible, or visual signal (e.g. a flashing light) using devices like an Aldis lamp or a heliograph, a common flashlight, or even a car horn. Some mine rescues have used pulling on a rope - a short pull for a dot and a long pull for a dash. 

Morse code is transmitted using just two states (on and off). Historians have called it the first digital code. Morse code may be represented as a binary code, and that is what telegraph operators do when transmitting messages. Working from the above ITU definition and further defining a bit as a dot time, a Morse code sequence may be made from a combination of the following five-bit strings:


Note that the marks and gaps alternate: dots and dashes are always separated by one of the gaps, and that the gaps are always separated by a dot or a dash.

Morse messages are generally transmitted by a hand-operated device such as a telegraph key, so there are variations introduced by the skill of the sender and receiver — more experienced operators can send and receive at faster speeds. In addition, individual operators differ slightly, for example, using slightly longer or shorter dashes or gaps, perhaps only for particular characters. This is called their "fist", and experienced operators can recognize specific individuals by it alone. A good operator who sends clearly and is easy to copy is said to have a "good fist". A "poor fist" is a characteristic of sloppy or hard to copy Morse code.

The very long time constants of 19th and early 20th century submarine communications cables required a different form of Morse signalling. Instead of keying a voltage on and off for varying times, the dits and dahs were represented by two polarities of voltage impressed on the cable, for a uniform time.

Below is an illustration of timing conventions. The phrase "MORSE CODE", in Morse code format, would normally be written something like this, where – represents dahs and · represents dits:

Next is the exact conventional timing for this phrase, with = representing "signal on", and . representing "signal off", each for the time length of exactly one dit:

Morse code is often spoken or written with "dah" for dashes, "dit" for dots located at the end of a character, and "di" for dots located at the beginning or internally within the character. Thus, the following Morse code sequence:
is orally:

"Dah-dah dah-dah-dah di-dah-dit di-di-dit dit, Dah-di-dah-dit dah-dah-dah dah-di-dit dit".

There is little point in learning to read Morse as above; rather, the of all of the letters and symbols need to be learned, for both sending and receiving.

All Morse code elements depend on the dot length. A dash is the length of 3 dots, and spacings are specified in number of dot lengths. An unambiguous method of specifying the transmission speed is to specify the dot duration as, for example, 50 milliseconds.

Specifying the dot duration is, however, not the common practice. Usually, speeds are stated in words per minute. That introduces ambiguity because words have different numbers of characters, and characters have different dot lengths. It is not immediately clear how a specific word rate determines the dot duration in milliseconds.

Some method to standardize the transformation of a word rate to a dot duration is useful. A simple way to do this is to choose a dot duration that would send a typical word the desired number of times in one minute. If, for example, the operator wanted a character speed of 13 words per minute, the operator would choose a dot rate that would send the typical word 13 times in exactly one minute.

The typical word thus determines the dot length. It is common to assume that a word is 5 characters long. There are two common typical words: "PARIS" and "CODEX". PARIS mimics a word rate that is typical of natural language words and reflects the benefits of Morse code's shorter code durations for common characters such as "e" and "t". CODEX offers a word rate that is typical of 5-letter code groups (sequences of random letters). Using the word PARIS as a standard, the number of dot units is 50 and a simple calculation shows that the dot length at 20 words per minute is 60 milliseconds. Using the word CODEX with 60 dot units, the dot length at 20 words per minute is 50 milliseconds.

Because Morse code is usually sent by hand, it is unlikely that an operator could be that precise with the dot length, and the individual characteristics and preferences of the operators usually override the standards.

For commercial radiotelegraph licenses in the United States, the Federal Communications Commission specifies tests for Morse code proficiency in words per minute and in code groups per minute. The Commission specifies that a word is 5 characters long. The Commission specifies Morse code test elements at 16 code groups per minute, 20 words per minute, 20 code groups per minute, and 25 words per minute. The word per minute rate would be close to the PARIS standard, and the code groups per minute would be close to the CODEX standard.

While the Federal Communications Commission no longer requires Morse code for amateur radio licenses, the old requirements were similar to the requirements for commercial radiotelegraph licenses.

A difference between amateur radio licenses and commercial radiotelegraph licenses is that commercial operators must be able to receive code groups of random characters along with plain language text. For each class of license, the code group speed requirement is slower than the plain language text requirement. For example, for the Radiotelegraph Operator License, the examinee must pass a 20 word per minute plain text test and a 16 word per minute code group test.

Based upon a 50 dot duration standard word such as PARIS, the time for one dot duration or one unit can be computed by the formula:

Where: "T" is the unit time, or dot duration in milliseconds, and "W" is the speed in wpm.

High-speed telegraphy contests are held; according to the "Guinness Book of Records" in June 2005 at the International Amateur Radio Union's 6th World Championship in High Speed Telegraphy in Primorsko, Bulgaria, Andrei Bindasov of Belarus transmitted 230 morse code marks of mixed text in one minute.

Sometimes, especially while teaching Morse code, the timing rules above are changed so two different speeds are used: a character speed and a text speed. The character speed is how fast each individual letter is sent. The text speed is how fast the entire message is sent. For example, individual characters may be sent at a 13 words-per-minute rate, but the intercharacter and interword gaps may be lengthened so the word rate is only 5 words per minute.

Using different character and text speeds is, in fact, a common practice, and is used in the Farnsworth method of learning Morse code.

Some methods of teaching Morse code use a dichotomic search table.
Morse Code cannot be treated as a classical radioteletype (RTTY) signal when it comes to calculating a link margin or a link budget for the simple reason of it possessing variable length dots and dashes as well as variant timing between letters and words. For the purposes of Information Theory and Channel Coding comparisons, the word "PARIS" is used to determine Morse Code's properties because it has an even number of dots and dashes.

Morse Code, when transmitted essentially, creates an AM signal (even in on/off keying mode), assumptions about signal can be made with respect to similarly timed RTTY signalling. Because Morse code transmissions employ an on-off keyed radio signal, it requires less complex transmission equipment than other forms of radio communication.

Morse code also requires less signal bandwidth than voice communication, typically , compared to the roughly 2400 Hz used by single-sideband voice, although at a lower data rate.

Morse code is usually heard at the receiver as a medium-pitched on/off audio tone (600–1000 Hz), so transmissions are easier to copy than voice through the noise on congested frequencies, and it can be used in very high noise / low signal environments. The transmitted power is concentrated into a limited bandwidth so narrow receiver filters can be used to suppress interference from adjacent frequencies. The audio tone is usually created by use of a beat frequency oscillator.

The narrow signal bandwidth also takes advantage of the natural aural selectivity of the human brain, further enhancing weak signal readability. This efficiency makes CW extremely useful for DX (distance) transmissions, as well as for low-power transmissions (commonly called "QRP operation", from the Q-code for "reduce power").

The ARRL has a readability standard for robot encoders called ARRL Farnsworth Spacing that is supposed to have higher readability for both robot and human decoders. Some programs like WinMorse have implemented the standard.

People learning Morse code using the Farnsworth method are taught to send and receive letters and other symbols at their full target speed, that is with normal relative timing of the dots, dashes, and spaces within each symbol for that speed. The Farnsworth method is named for Donald R. "Russ" Farnsworth, also known by his call sign, W6TTB. However, initially exaggerated spaces between symbols and words are used, to give "thinking time" to make the sound "shape" of the letters and symbols easier to learn. The spacing can then be reduced with practice and familiarity.

Another popular teaching method is the Koch method, named after German psychologist Ludwig Koch, which uses the full target speed from the outset but begins with just two characters. Once strings containing those two characters can be copied with 90% accuracy, an additional character is added, and so on until the full character set is mastered.

In North America, many thousands of individuals have increased their code recognition speed (after initial memorization of the characters) by listening to the regularly scheduled code practice transmissions broadcast by W1AW, the American Radio Relay League's headquarters station.

Visual mnemonic charts have been devised over the ages. Baden-Powell included one in the Girl Guides handbook in 1918.

In the United Kingdom, many people learned the Morse code by means of a series of words or phrases that have the same rhythm as a Morse character. For instance, "Q" in Morse is dah-dah-di-dah, which can be memorized by the phrase "God save the Queen", and the Morse for "F" is di-di-dah-dit, which can be memorized as "Did she like it."

A well-known Morse code rhythm from the Second World War period derives from Beethoven's Fifth Symphony, the opening phrase of which was regularly played at the beginning of BBC broadcasts. The timing of the notes corresponds to the Morse for "V", di-di-di-dah, understood as "V for Victory" (as well as the Roman numeral for the number five).

Prosigns for Morse code are special (usually) unwritten procedural signals or symbols that are used to indicate changes in communications protocol status or white space text formatting actions.

The symbols !, $ and & are not defined inside the ITU recommendation on Morse code, but conventions for them exist. The @ symbol was formally added in 2004.


There is no standard representation for the exclamation mark (!), although the KW digraph () was proposed in the 1980s by the Heathkit Company (a vendor of assembly kits for amateur radio equipment).

While Morse code translation software prefers the Heathkit version, on-air use is not yet universal as some amateur radio operators in North America and the Caribbean continue to prefer the older MN digraph () carried over from American landline telegraphy code.




For Chinese, Chinese telegraph code is used to map Chinese characters to four-digit codes and send these digits out using standard Morse code. Korean Morse code uses the SKATS mapping, originally developed to allow Korean to be typed on western typewriters. SKATS maps hangul characters to arbitrary letters of the Latin script and has no relationship to pronunciation in Korean. For Russian and Bulgarian, Russian Morse code is used to map the Cyrillic characters to four-element codes. Many of the characters are encoded the same way (A, O, E, I, T, M, N, R, K, etc.). Bulgarian alphabet contains 30 characters, which exactly match all possible combinations of 1, 2, 3, and 4 dots and dashes. Russian requires 1 extra character, "Ы" which is encoded with 5 elements.

During early World War I (1914–1916), Germany briefly experimented with 'dotty' and 'dashy' Morse, in essence adding a dot or a dash at the end of each Morse symbol. Each one was quickly broken by Allied SIGINT, and standard Morse was restored by Spring 1916. Only a small percentage of Western Front (North Atlantic and Mediterranean Sea) traffic was in 'dotty' or 'dashy' Morse during the entire war. In popular culture, this is mostly remembered in the book "The Codebreakers" by Kahn and in the national archives of the UK and Australia (whose SIGINT operators copied most of this Morse variant). Kahn's cited sources come from the popular press and wireless magazines of the time.

Other forms of 'Fractional Morse' or 'Fractionated Morse' have emerged.

Decoding software for Morse code ranges from software-defined wide-band radio receivers coupled to the Reverse Beacon Network, which decodes signals and detects CQ messages on ham bands, to smartphone applications.




</doc>
<doc id="18937" url="https://en.wikipedia.org/wiki?curid=18937" title="Mapping">
Mapping

Mapping is the creation of maps, a graphic symbolic representation of the significant features of a part of the surface of the Earth.



</doc>
<doc id="18939" url="https://en.wikipedia.org/wiki?curid=18939" title="Emergency contraception">
Emergency contraception

Emergency contraception (EC), or emergency postcoital contraception, are birth control measures that may be used after sexual intercourse to prevent pregnancy.

Forms of EC include:

Important information to be aware of (repeated further below in this article). 

In France, Sweden, and Britain—where Yuzpe-regimen EC had been available by prescription for more than a decade and progestin-only EC has been available without a prescription for 8, 6, and 2 years respectively—the abortion rate was stable or higher during that time period. Another study concluded that distribution of free, advance supplies of Emergency Contraceptives to large numbers of women in Scotland did not reduce abortion rates. A randomized controlled trial of 2000 women in China compared women with advance access to EC to women without access, and noted that the pregnancy rate was the same between the two groups. The study observed that "...providing EC in advance increases use, but there is no direct evidence that it reduces unintended pregnancy" and concluded that EC may not lower abortion rates.

In September 2006, emergency contraception expert Anna Glasier wrote a "BMJ" editorial entitled "Emergency Contraception. Is it worth all the fuss?" that said in closing: "So is emergency contraception worth the fuss? If you are a woman who has had unprotected sex then of course it is, because emergency contraception will prevent pregnancy in some women some of the time—and if you don’t want to get pregnant anything is better than nothing."

Emergency contraceptive pills (ECPs) (sometimes referred to as emergency hormonal contraception, EHC) may contain higher doses of the same hormones (estrogens, progestins, or both) found in regular combined oral contraceptive pills. Taken after unprotected sexual intercourse or contraceptive failure, such higher doses may prevent pregnancy from occurring.

Four types of emergency contraceptive pills are available: combined estrogen and progestin pills, progestin-only (levonorgestrel, LNG) pills, and antiprogestin (ulipristal acetate or mifepristone) pills. Progestin-only and antiprogestin pills are available as dedicated (specifically packaged for use as) emergency contraceptive pills. Combined estrogen and progestin pills are no longer available as dedicated emergency contraceptive pills, but certain regular combined oral contraceptive pills may be used as emergency contraceptive pills.

Progestin-only emergency contraceptive pills contain levonorgestrel, either as a single tablet or as a split dose of two tablets taken 12 hours apart, effective up to 72 hours after intercourse. Progestin-only ECPs are sold under many different brand names. Progestin-only ECPs are available over-the-counter (OTC) in several countries (e.g. Bangladesh, Bulgaria, Canada, Cyprus, Czech Republic, Denmark, Estonia, India, Malta, Netherlands, Norway, Portugal, Romania, Slovakia, South Africa, Sweden, United States), from a pharmacist without a prescription, and available with a prescription in some other countries.

The antiprogestin ulipristal acetate is available as a micronized emergency contraceptive tablet, effective up to 120 hours after intercourse. Ulipristal acetate ECPs developed by HRA Pharma are available by prescription in over 50 countries under the brand names "ellaOne", "ella" (marketed by Watson Pharmaceuticals in the United States), "Duprisal 30", "Ulipristal 30", and "UPRIS".

The antiprogestin mifepristone (also known as RU-486) is available in five countries as a low-dose or mid-dose emergency contraceptive tablet, effective up to 120 hours after intercourse. Low-dose mifepristone ECPs are available by prescription in Armenia, Russia, Ukraine, and Vietnam and from a pharmacist without a prescription in China. Mid-dose mifepristone ECPs are available by prescription in China and Vietnam.

Combined estrogen (ethinylestradiol) and progestin (levonorgestrel or norgestrel) pills used to be available as dedicated emergency contraceptive pills under several brand names: "Schering PC4", "Tetragynon", "Neoprimavlar", and "Preven" (in the United States) but were withdrawn after more effective dedicated progestin-only (levonorgestrel) emergency contraceptive pills with fewer side effects became available. If other more effective dedicated emergency contraceptive pills (levonorgestrel, ulipristal acetate, or mifepristone) are not available, specific combinations of regular combined oral contraceptive pills can be taken in split doses 12 hours apart (the Yuzpe regimen), effective up to 72 hours after intercourse. The U.S. Food and Drug Administration (FDA) approved this off-label use of certain brands of regular combined oral contraceptive pills in 1997. As of 2014, there are 26 brands of regular combined oral contraceptive pills containing levonorgestrel or norgestrel available in the United States that can be used in the emergency contraceptive Yuzpe regimen.

Ulipristal acetate, and mid-dose mifepristone is more effective than levonorgestrel, which is more effective than the Yupse method.

The effectiveness of emergency contraception is expressed as a percentage reduction in pregnancy rate for a single use of EC. Using an example of "75% effective", the effectiveness calculation thus: ... these numbers do not translate into a pregnancy rate of 25 percent. Rather, they mean that if 1,000 women have unprotected intercourse in the middle two weeks of their menstrual cycles, approximately 80 will become pregnant. Use of emergency contraceptive pills would reduce this number by 75 percent, to 20 women.

The progestin-only regimen (using levonorgestrel) has an 89% effectiveness. , the labeling on the U.S. brand Plan B explained this effectiveness rate by stating, "Seven out of every eight women who would have gotten pregnant will not become pregnant."

In 1999, a meta-analysis of eight studies of the combined (Yuzpe) regimen concluded that the best point estimate of effectiveness was 74%. A 2003 analysis of two of the largest combined (Yuzpe) regimen studies, using a different calculation method, found effectiveness estimates of 47% and 53%.

For both the progestin-only and Yuzpe regimens, the effectiveness of emergency contraception is highest when taken within 12 hours of intercourse and declines over time.
The World Health Organization (WHO) suggested that reasonable effectiveness may continue for up to 120 hours (5 days) after intercourse.

For 10 mg of mifepristone taken up to 120 hours (5 days) after intercourse, the combined estimate from three trials was an effectiveness of 83%. A review found that a moderate dose of mifepristone is better than LNG or Yuzpe.

HRA Pharma changed its packaging information for Norlevo (which has dosage and chemical makeup identical to many other EHCs) in November 2013 warning that the drug loses effectiveness in women who weigh more than 165 pounds and is completely ineffective for women who weigh over 176 pounds.

The most common side effect reported by users of emergency contraceptive pills was nausea (50.5% of 979 Yuzpe regimen users and 23.1% of 977 levonorgestrel-only users in the 1998 WHO trial; 14.3% of 2,720 levonorgestrel-only users in the 2002 WHO trial); vomiting is much less common and unusual with levonorgestrel-only ECPs (18.8% of 979 Yuzpe regimen users and 5.6% of levonorgestrel-only users in the 1998 WHO trial; 1.4% of 2,720 levonorgestrel-only users in the 2002 WHO trial). Anti-emetics are not routinely recommended with levonorgestrel-only ECPs. If a woman vomits within 2 hours of taking a levonorgestrel-only ECP, she should take a further dose as soon as possible.

Other common side effects (each reported by less than 20% of levonorgestrel-only users in both the 1998 and 2002 WHO trials) were abdominal pain, fatigue, headache, dizziness, and breast tenderness. Side effects usually do not occur for more than a few days after treatment, and they generally resolve within 24 hours.

Temporary disruption of the menstrual cycle is also commonly experienced. If taken before ovulation, the high doses of progestogen in levonorgestrel treatments may induce progestogen withdrawal bleeding a few days after the pills are taken. One study found that about half of women who used levonorgestrel ECPs experienced bleeding within 7 days of taking the pills. If levonorgestrel is taken after ovulation, it may increase the length of the luteal phase, thus delaying menstruation by a few days. Mifepristone, if taken before ovulation, may delay ovulation by 3–4 days (delayed ovulation may result in a delayed menstruation). These disruptions only occur in the cycle in which ECPs were taken; subsequent cycle length is not significantly affected. If a woman's menstrual period is delayed by two weeks or more, it is advised that she take a pregnancy test. (Earlier testing may not give accurate results.)

Existing pregnancy is not a contraindication in terms of safety, as there is no known harm to the woman, the course of her pregnancy, or the fetus if progestin-only or combined emergency contraception pills are accidentally used, but EC is not indicated for a woman with a known or suspected pregnancy because it is not effective in women who are already pregnant.

The World Health Organization (WHO) lists no medical condition for which the risks of emergency contraceptive pills outweigh the benefits. The American Academy of Pediatrics (AAP) and experts on emergency contraception have concluded that progestin-only ECPs may be preferable to combined ECPs containing estrogen in women with a history of blood clots, stroke, or migraine.

There are no medical conditions in which progestin-only ECPs are contraindicated. Current venous thromboembolism, current or past history of breast cancer, inflammatory bowel disease, and acute intermittent porphyria are conditions where the advantages of using emergency contraceptive pills generally outweigh the theoretical or proven risks.

ECPs, like all other contraceptives, reduce the absolute risk of ectopic pregnancy by preventing pregnancies and there is no increase in the relative risk of ectopic pregnancy in women who become pregnant after using progestin-only ECPs.

The herbal preparation of St John's wort and some enzyme-inducing drugs (e.g. anticonvulsants or rifampicin) may reduce the effectiveness of ECP, and a larger dose may be required.

An alternative to emergency contraceptive pills is the copper-T intrauterine device (IUD) which can be used up to 5 days after unprotected intercourse to prevent pregnancy. Insertion of an IUD is more effective than use of Emergency Contraceptive Pills - pregnancy rates when used as emergency contraception are the same as with normal IUD use. IUDs may be left in place following the subsequent menstruation to provide ongoing contraception (3–10 years depending upon type).

One brand of levonorgestrel pills was marketed as an ongoing method of postcoital contraception. However, there are serious drawbacks to such use of postcoital high-dose progestin-only oral contraceptive pills, especially if they are not used according to their package directions, but are instead used according to the package directions of emergency contraceptive pills:

ECPs are generally recommended for backup or "emergency" use, rather than as the primary means of contraception. They are intended for use when other means of contraception have failed—for example, if a woman has forgotten to take a birth control pill or when a condom is torn during sex.

The current (December 2012) American Academy of Pediatrics (AAP) Policy Statement on Emergency Contraception says: "Despite multiple studies showing no increased risk behavior and evidence that hormonal emergency contraception will not disrupt an established pregnancy, public and medical discourse reflects that personal values of physicians and pharmacists continue to affect emergency-contraception access, particularly for adolescents."

The latest (December 2013) review by emergency contraception experts Trussell and Raymond says: "Published evidence would seem to demonstrate convincingly that making ECPs more widely available does not increase risk-taking or adversely affect regular contraceptive use". . . ."However, reanalysis of one of the randomized trials suggests that easier access to ECPs may have increased the frequency of coital acts with the potential to lead to pregnancy." and notes that four randomized controlled trials have found that advanced provision of emergency contraceptive pills did not increase rates of sexually transmitted infections or sexual risk taking. Trussell and Raymond noted that after one of the four studies had been reanalysed later, the data did show higher sexual risk-taking, specifically substituting in some cases emergency contraceptives for contraceptives such as condoms that are more effective.

In France, Sweden, and Britain—where Yuzpe-regimen EC had been available by prescription for more than a decade and progestin-only EC has been available without a prescription for 8, 6, and 2 years respectively—the abortion rate was stable or higher during that time period. Another study concluded that distribution of free, advance supplies of EC to large numbers of women in Scotland did not reduce abortion rates. A randomized controlled trial of 2000 women in China compared women with advance access to EC to women without access, and noted that the pregnancy rate was the same between the two groups. The study observed that "...providing EC in advance increases use, but there is no direct evidence that it reduces unintended pregnancy" and concluded that EC may not lower abortion rates.

In September 2006, emergency contraception expert Anna Glasier wrote a "BMJ" editorial entitled "Emergency Contraception. Is it worth all the fuss?" that said in closing: "So is emergency contraception worth the fuss? If you are a woman who has had unprotected sex then of course it is, because emergency contraception will prevent pregnancy in some women some of the time—and if you don’t want to get pregnant anything is better than nothing. If you are the "CMAJ’"s editor or FDA commissioner then yes, because scientific freedom is worth the fight. If you are looking for an intervention that will reduce abortion rates, emergency contraception may not be the solution, and perhaps you should concentrate most on encouraging people to use contraception before or during sex, not after it."

Before EC was used in the general population or defined as "emergency contraception," it was used, beginning in the 1960s and 70s, specifically for women who had been sexually assaulted. Pregnancy rates among rape victims of child-bearing age are around 5%; in the U.S., about half of rape victims who become pregnant have abortions. Although EC is commonly used as an option for victims of sexual assault, some researchers believe such use is a public health measure that is not sufficiently widespread.

The primary mechanism of action of progestogen-only emergency contraceptive pills is to prevent fertilization by inhibition of ovulation. The best available evidence is that they do not have any post-fertilization effects such as the prevention of implantation. The U.S. FDA-approved labels and European EMA-approved labels (except for HRA Pharma's "NorLevo") levonorgestrel emergency contraceptive pills (based on labels for regular oral contraceptive pills) say they may cause endometrial changes that discourage implantation. Daily use of regular oral contraceptive pills can alter the endometrium (although this has not been proven to interfere with implantation), but the isolated use of a levonorgestrel emergency contraceptive pill does not have time to alter the endometrium. In March 2011, the International Federation of Gynecology and Obstetrics (FIGO) issued a statement that: "review of the evidence suggests that LNG [levonorgestreol] ECPs cannot prevent implantation of a fertilized egg. Language on implantation should not be included in LNG ECP product labeling." In June 2012, a "New York Times" editorial called on the FDA to remove from the label the unsupported suggestion that levonorgestrel emergency contraceptive pills inhibit implantation. In November 2013, the European Medicines Agency (EMA) approved a change to the label for HRA Pharma's "NorLevo" saying it cannot prevent implantation of a fertilized egg.

Progestogen-only emergency contraceptive does not appear to affect the function of the Fallopian tubes or increase the rate of ectopic pregnancies.

The primary mechanism of action of progesterone receptor modulator emergency contraceptive pills like low-dose and mid-dose mifepristone and ulipristal acetate is to prevent fertilization by inhibition or delay of ovulation. One clinical study found that post-ovulatory administration of ulipristal acetate altered the endometrium, but whether the changes would inhibit implantation is unknown. The European EMA-approved labels for ulipristal acetate emergency contraceptive pills do not mention an effect on implantation, but the U.S. FDA-approved label says: "alterations to the endometrium that may affect implantation may also contribute to efficacy."

The primary mechanism of action of copper-releasing intrauterine devices (IUDs) as emergency contraceptives is to prevent fertilization because of copper toxicity to sperm and ova. The very high effectiveness of copper-releasing IUDs as emergency contraceptives implies that they must also prevent some pregnancies by post-fertilization effects such as prevention of implantation.

In 1966, gynecologist John McLean Morris and biologist Gertrude Van Wagenen at the Yale School of Medicine reported the successful use of oral high-dose estrogen pills as post-coital contraceptives in women and rhesus macaque monkeys, respectively. A few different drugs were studied, with a focus on high-dose estrogens, and it was originally hoped that postcoital contraception would prove viable as an ongoing contraceptive method.

The first widely used methods were five-day treatments with high-dose estrogens, using diethylstilbestrol (DES) in the US and ethinylestradiol in the Netherlands by Dr. Haspels.

In the early 1970s, the Yuzpe regimen was developed by A. Albert Yuzpe in 1974; progestin-only postcoital contraception was investigated (1975); and the copper IUD was first studied for use as emergency contraception (1975). Danazol was tested in the early 1980s in the hopes that it would have fewer side effects than Yuzpe, but was found to be ineffective.

The Yuzpe regimen became the standard course of treatment for postcoital contraception in many countries in the 1980s. The first prescription-only combined estrogen-progestin dedicated product, Schering PC4 (ethinylestradiol and norgestrel), was approved in the UK in January 1984 and first marketed in October 1984. Schering introduced a second prescription-only combined product, Tetragynon (ethinylestradiol and levonorgestrel) in Germany in 1985. By 1997, Schering AG dedicated prescription-only combined products had been approved in only 9 countries: the UK (Schering PC4), New Zealand (Schering PC4), South Africa (E-Gen-C), Germany (Tetragynon), Switzerland (Tetragynon), Denmark (Tetragynon), Norway (Tetragynon), Sweden (Tetragynon) and Finland (Neoprimavlar); and had been withdrawn from marketing in New Zealand in 1997 to prevent it being sold over-the-counter. Regular combined oral contraceptive pills (which were less expensive and more widely available) were more commonly used for the Yuzpe regimen even in countries where dedicated products were available.

Over time, interest in progestin-only treatments increased. The Special Program on Human Reproduction (HRP), an international organization whose members include the World Bank and World Health Organization, "played a pioneering role in emergency contraception" by "confirming the effectiveness of levonorgestrel." After the WHO conducted a large trial comparing Yuzpe and levonorgestrel in 1998, combined estrogen-progestin products were gradually withdrawn from some markets ("Preven" in the United States discontinued May 2004, "Schering PC4" in the UK discontinued October 2001, and "Tetragynon" in France) in favor of progestin-only EC, although prescription-only dedicated Yuzpe regimen products are still available in some countries.

In 2002, China became the first country in which mifepristone was registered for use as EC.

Early studies of emergency contraceptives did not attempt to calculate a failure rate; they simply reported the number of women who became pregnant after using an emergency contraceptive. Since 1980, clinical trials of emergency contraception have first calculated probable pregnancies in the study group if no treatment were given. The effectiveness is calculated by dividing observed pregnancies by the estimated number of pregnancies without treatment.

Placebo-controlled trials that could give a precise measure of the pregnancy rate without treatment would be unethical, so the effectiveness percentage is based on estimated pregnancy rates. These are currently estimated using variants of the calendar method.
Women with irregular cycles for any reason (including recent hormone use such as oral contraceptives and breastfeeding) must be excluded from such calculations. Even for women included in the calculation, the limitations of calendar methods of fertility determination have long been recognized. In their February 2014 emergency review article, Trussell and Raymond note:
Calculation of effectiveness, and particularly the denominator of the fraction, involves many assumptions that are difficult to validate...The risk of pregnancy for women requesting ECPs appears to be lower than assumed in the estimates of ECP efficacy, which are consequently likely to be overestimates. Yet, precise estimates of efficacy may not be highly relevant to many women who have had unprotected intercourse, since ECPs are often the only available treatment.

In 1999, hormonal assay was suggested as a more accurate method of estimating fertility for EC studies.







</doc>
<doc id="18940" url="https://en.wikipedia.org/wiki?curid=18940" title="Meat">
Meat

Meat is animal flesh that is eaten as food. Humans have hunted and killed animals for meat since prehistoric times. The advent of civilization allowed the domestication of animals such as chickens, sheep, rabbits, pigs and cattle. This eventually led to their use in meat production on an industrial scale with the aid of slaughterhouses.

Meat is mainly composed of water, protein, and fat. It is edible raw, but is normally eaten after it has been cooked and seasoned or processed in a variety of ways. Unprocessed meat will spoil or rot within hours or days as a result of infection with and decomposition by bacteria and fungi.

Meat is important in economy and culture, even though its mass production and consumption has been determined to pose risks for human health and the environment. Many religions have rules about which meat may or may not be eaten, and vegetarian people abstain from eating meat because of concerns about the ethics of eating meat or about the effects of meat production or consumption.

The word "meat" comes from the Old English word "mete", which referred to food in general. The term is related to "mad" in Danish, "mat" in Swedish and Norwegian, and "matur" in Icelandic and Faroese, which also mean 'food'. The word "mete" also exists in Old Frisian (and to a lesser extent, modern West Frisian) to denote important food, differentiating it from "swiets" (sweets) and "dierfied" (animal feed).

Most often, "meat" refers to skeletal muscle and associated fat and other tissues, but it may also describe other edible tissues such as offal. "Meat" is sometimes also used in a more restrictive sense to mean the flesh of mammalian species (pigs, cattle, lambs, etc.) raised and prepared for human consumption, to the exclusion of fish, other seafood, insects, poultry, or other animals.

Paleontological evidence suggests that meat constituted a substantial proportion of the diet of even the earliest humans. Early hunter-gatherers depended on the organized hunting of large animals such as bison and deer.

The domestication of animals, of which we have evidence dating back to the end of the last glacial period (c. 10,000 BCE), allowed the systematic production of meat and the breeding of animals with a view to improving meat production. The animals which are now the principal sources of meat were domesticated in conjunction with the development of early civilizations:


Other animals are or have been raised or hunted for their flesh. The type of meat consumed varies much between different cultures, changes over time, depending on factors such as tradition and the availability of the animals. The amount and kind of meat consumed also varies by income, both between countries and within a given country.

Modern agriculture employs a number of techniques, such as progeny testing, to speed artificial selection by breeding animals to rapidly acquire the qualities desired by meat producers. For instance, in the wake of well-publicised health concerns associated with saturated fats in the 1980s, the fat content of United Kingdom beef, pork and lamb fell from 20–26 percent to 4–8 percent within a few decades, due to both selective breeding for leanness and changed methods of butchery. Methods of genetic engineering aimed at improving the meat production qualities of animals are now also becoming available.
Even though it is a very old industry, meat production continues to be shaped strongly by the evolving demands of customers. The trend towards selling meat in pre-packaged cuts has increased the demand for larger breeds of cattle, which are better suited to producing such cuts. Even more animals not previously exploited for their meat are now being farmed, especially the more agile and mobile species, whose muscles tend to be developed better than those of cattle, sheep or pigs. Examples are the various antelope species, the zebra, water buffalo and camel, as well as non-mammals, such as the crocodile, emu and ostrich. Another important trend in contemporary meat production is organic farming which, while providing no organoleptic benefit to meat so produced, meets an increasing demand for organic meat.

For most of human history, meat was a largely unquestioned part of the human diet. Only in the 20th century did it begin to become a topic of discourse and contention in society, politics and wider culture.

The founders of Western philosophy disagreed about the ethics of eating meat. Plato's "Republic" has Socrates describe the ideal state as vegetarian. Pythagoras believed that humans and animals were equal and therefore disapproved of meat consumption, as did Plutarch, whereas Zeno and Epicurus were vegetarian but allowed meat-eating in their philosophy. Conversely, Aristotle's "Politics" assert that animals, as inferior beings, exist to serve humans, including as food. Augustine drew on Aristotle to argue that the universe's natural hierarchy allows humans to eat animals, and animals to eat plants. Enlightenment philosophers were likewise divided. Descartes wrote that animals are merely animated machines, and Kant considered them inferior beings for lack of discernment; means rather than ends. But Voltaire and Rousseau disagreed. The latter argued that meat-eating is a social rather than a natural act, because children aren't interested in meat.

Later philosophers examined the changing practices of eating meat in the modern age as part of a process of detachment from animals as living beings. Norbert Elias, for instance, noted that in medieval times cooked animals were brought to the table whole, but that since the Renaissance only the edible parts are served, which are no longer recognizably part of an animal. Modern eaters, according to , demand an "ellipsis" between meat and dead animals; for instance, calves' eyes are no longer considered a delicacy as in the Middle Ages, but provoke disgust. Even in the English language, distinctions emerged between animals and their meat, such as between cattle and beef, pigs and pork. Fernand Braudel wrote that since the European diet of the 15th and 16th century was particularly heavy in meat, European colonialism helped export meat-eating across the globe, as colonized peoples took up the culinary habits of their colonizers, which they associated with wealth and power.

Unlike most other food, meat is not perceived as gender-neutral, and is particularly associated with men and masculinity. Sociological research, ranging from African tribal societies to contemporary barbecues, indicates that men are much more likely to participate in preparing meat than other food. This has been attributed to the influence of traditional male gender roles, in view of the "male familiarity with killing animals and even humans" (Goody) or the "violent" nature of roasting as opposed to boiling (Lévi-Strauss). By and large, at least in modern societies, men also tend to consume more meat than women, and men often prefer red meat whereas women tend to prefer chicken and fish.

Meat consumption varies worldwide, depending on cultural or religious preferences, as well as economic conditions. Vegetarians choose not to eat meat because of ethical, economic, environmental, religious or health concerns that are associated with meat production and consumption.

According to the analysis of the FAO the overall consumption for white meat between 1990 and 2009 has dramatically increased. For example, poultry meat has increased by 76.6% per kilo per capita and pig meat by 19.7%. However, on the contrary, bovine meat has decreased from /capita in 1990 to /capita in 2009.

Agricultural science has identified several factors bearing on the growth and development of meat in animals.

Several economically important traits in meat animals are heritable to some degree (see the adjacent table) and can thus be selected for by animal breeding. In cattle, certain growth features are controlled by recessive genes which have not so far been controlled, complicating breeding. One such trait is dwarfism; another is the doppelender or "double muscling" condition, which causes muscle hypertrophy and thereby increases the animal's commercial value. Genetic analysis continues to reveal the genetic mechanisms that control numerous aspects of the endocrine system and, through it, meat growth and quality.

Genetic engineering techniques can shorten breeding programs significantly because they allow for the identification and isolation of genes coding for desired traits, and for the reincorporation of these genes into the animal genome. To enable such manipulation, research is ongoing () to map the entire genome of sheep, cattle and pigs. Some research has already seen commercial application. For instance, a recombinant bacterium has been developed which improves the digestion of grass in the rumen of cattle, and some specific features of muscle fibres have been genetically altered.

Experimental reproductive cloning of commercially important meat animals such as sheep, pig or cattle has been successful. The multiple asexual reproduction of animals bearing desirable traits can thus be anticipated, although this is not yet practical on a commercial scale.

Heat regulation in livestock is of great economic significance, because mammals attempt to maintain a constant optimal body temperature. Low temperatures tend to prolong animal development and high temperatures tend to retard it. Depending on their size, body shape and insulation through tissue and fur, some animals have a relatively narrow zone of temperature tolerance and others (e.g. cattle) a broad one. Static magnetic fields, for reasons still unknown, also retard animal development.

The quality and quantity of usable meat depends on the animal's "plane of nutrition", i.e., whether it is over- or underfed. Scientists disagree, however, about how exactly the plane of nutrition influences carcase composition.

The composition of the diet, especially the amount of protein provided, is also an important factor regulating animal growth. Ruminants, which may digest cellulose, are better adapted to poor-quality diets, but their ruminal microorganisms degrade high-quality protein if supplied in excess. Because producing high-quality protein animal feed is expensive (see also "Environmental impact" below), several techniques are employed or experimented with to ensure maximum utilization of protein. These include the treatment of feed with formalin to protect amino acids during their passage through the rumen, the recycling of manure by feeding it back to cattle mixed with feed concentrates, or the partial conversion of petroleum hydrocarbons to protein through microbial action.

In plant feed, environmental factors influence the availability of crucial nutrients or micronutrients, a lack or excess of which can cause a great many ailments. In Australia, for instance, where the soil contains limited phosphate, cattle are being fed additional phosphate to increase the efficiency of beef production. Also in Australia, cattle and sheep in certain areas were often found losing their appetite and dying in the midst of rich pasture; this was at length found to be a result of cobalt deficiency in the soil. Plant toxins are also a risk to grazing animals; for instance, sodium fluoroacetate, found in some African and Australian plants, kills by disrupting the cellular metabolism. Certain man-made pollutants such as methylmercury and some pesticide residues present a particular hazard due to their tendency to bioaccumulate in meat, potentially poisoning consumers.

Meat producers may seek to improve the fertility of female animals through the administration of gonadotrophic or ovulation-inducing hormones. In pig production, sow infertility is a common problem — possibly due to excessive fatness. No methods currently exist to augment the fertility of male animals. Artificial insemination is now routinely used to produce animals of the best possible genetic quality, and the efficiency of this method is improved through the administration of hormones that synchronize the ovulation cycles within groups of females.

Growth hormones, particularly anabolic agents such as steroids, are used in some countries to accelerate muscle growth in animals. This practice has given rise to the beef hormone controversy, an international trade dispute. It may also decrease the tenderness of meat, although research on this is inconclusive, and have other effects on the composition of the muscle flesh. Where castration is used to improve control over male animals, its side effects are also counteracted by the administration of hormones.

Sedatives may be administered to animals to counteract stress factors and increase weight gain. The feeding of antibiotics to certain animals has been shown to improve growth rates also. This practice is particularly prevalent in the USA, but has been banned in the EU, partly because it causes antimicrobial resistance in pathogenic microorganisms.

Numerous aspects of the biochemical composition of meat vary in complex ways depending on the species, breed, sex, age, plane of nutrition, training and exercise of the animal, as well as on the anatomical location of the musculature involved. Even between animals of the same litter and sex there are considerable differences in such parameters as the percentage of intramuscular fat.

Adult mammalian muscle flesh consists of roughly 75 percent water, 19 percent protein, 2.5 percent intramuscular fat, 1.2 percent carbohydrates and 2.3 percent other soluble non-protein substances. These include nitrogenous compounds, such as amino acids, and inorganic substances such as minerals.

Muscle proteins are either soluble in water (sarcoplasmic proteins, about 11.5 percent of total muscle mass) or in concentrated salt solutions (myofibrillar proteins, about 5.5 percent of mass). There are several hundred sarcoplasmic proteins. Most of them – the glycolytic enzymes – are involved in the glycolytic pathway, i.e., the conversion of stored energy into muscle power. The two most abundant myofibrillar proteins, myosin and actin, are responsible for the muscle's overall structure. The remaining protein mass consists of connective tissue (collagen and elastin) as well as organelle tissue.

Fat in meat can be either adipose tissue, used by the animal to store energy and consisting of "true fats" (esters of glycerol with fatty acids), or intramuscular fat, which contains considerable quantities of phospholipids and of unsaponifiable constituents such as cholesterol.

Meat can be broadly classified as "red" or "white" depending on the concentration of myoglobin in muscle fibre. When myoglobin is exposed to oxygen, reddish oxymyoglobin develops, making myoglobin-rich meat appear red. The redness of meat depends on species, animal age, and fibre type: Red meat contains more narrow muscle fibres that tend to operate over long periods without rest, while white meat contains more broad fibres that tend to work in short fast bursts.

Generally, the meat of adult mammals such as cows, sheep, and horses is considered red, while chicken and turkey breast meat is considered white.

All muscle tissue is very high in protein, containing all of the essential amino acids, and in most cases is a good source of zinc, vitamin B, selenium, phosphorus, niacin, vitamin B, choline, riboflavin and iron. Several forms of meat are also high in vitamin K. Muscle tissue is very low in carbohydrates and does not contain dietary fiber. While taste quality may vary between meats, the proteins, vitamins, and minerals available from meats are generally consistent.

The fat content of meat can vary widely depending on the species and breed of animal, the way in which the animal was raised, including what it was fed, the anatomical part of the body, and the methods of butchering and cooking. Wild animals such as deer are typically leaner than farm animals, leading those concerned about fat content to choose game such as venison. Decades of breeding meat animals for fatness is being reversed by consumer demand for meat with less fat. The fatty deposits that exist with the muscle fibers in meats soften meat when it is cooked and improve the flavor through chemical changes initiated through heat that allow the protein and fat molecules to interact. The fat, when cooked with meat, also makes the meat seem juicier. However, the nutritional contribution of the fat is mainly calories as opposed to protein. As fat content rises, the meat's contribution to nutrition declines. In addition, there is cholesterol associated with fat surrounding the meat. The cholesterol is a lipid associated with the kind of saturated fat found in meat. The increase in meat consumption after 1960 is associated with, though not definitively the cause of, significant imbalances of fat and cholesterol in the human diet.

The table in this section compares the nutritional content of several types of meat. While each kind of meat has about the same content of protein and carbohydrates, there is a very wide range of fat content.

Meat is produced by killing an animal and cutting flesh out of it. These procedures are called slaughter and butchery, respectively. There is ongoing research into producing meat "in vitro", that is, outside of animals.

Upon reaching a predetermined age or weight, livestock are usually transported "en masse" to the slaughterhouse. Depending on its length and circumstances, this may exert stress and injuries on the animals, and some may die "en route". Unnecessary stress in transport may adversely affect the quality of the meat. In particular, the muscles of stressed animals are low in water and glycogen, and their pH fails to attain acidic values, all of which results in poor meat quality. Consequently, and also due to campaigning by animal welfare groups, laws and industry practices in several countries tend to become more restrictive with respect to the duration and other circumstances of livestock transports.

Animals are usually slaughtered by being first stunned and then exsanguinated (bled out). Death results from the one or the other procedure, depending on the methods employed. Stunning can be effected through asphyxiating the animals with carbon dioxide, shooting them with a gun or a captive bolt pistol, or shocking them with electric current. In most forms of ritual slaughter, stunning is not allowed.

Draining as much blood as possible from the carcass is necessary because blood causes the meat to have an unappealing appearance and is a breeding ground for microorganisms. The exsanguination is accomplished by severing the carotid artery and the jugular vein in cattle and sheep, and the anterior vena cava in pigs.

After exsanguination, the carcass is dressed; that is, the head, feet, hide (except hogs and some veal), excess fat, viscera and offal are removed, leaving only bones and edible muscle. Cattle and pig carcases, but not those
of sheep, are then split in half along the mid ventral axis, and the carcase is cut into wholesale pieces. The dressing and cutting sequence, long a province of manual labor, is progressively being fully automated.

Under hygienic conditions and without other treatment, meat can be stored at above its freezing point (–1.5 °C) for about six weeks without spoilage, during which time it undergoes an aging process that increases its tenderness and flavor.

During the first day after death, glycolysis continues until the accumulation of lactic acid causes the pH to reach about 5.5. The remaining glycogen, about 18 g per kg, is believed to increase the water-holding capacity and tenderness of the flesh when cooked. "Rigor mortis" sets in a few hours after death as ATP is used up, causing actin and myosin to combine into rigid actomyosin and lowering the meat's water-holding capacity, causing it to lose water ("weep"). In muscles that enter "rigor" in a contracted position, actin and myosin filaments overlap and cross-bond, resulting in meat that is tough on cooking – hence again the need to prevent pre-slaughter stress in the animal.

Over time, the muscle proteins denature in varying degree, with the exception of the collagen and elastin of connective tissue, and "rigor mortis" resolves. Because of these changes, the meat is tender and pliable when cooked just after death or after the resolution of "rigor", but tough when cooked during "rigor." As the muscle pigment myoglobin denatures, its iron oxidates, which may cause a brown discoloration near the surface of the meat. Ongoing proteolysis also contributes to conditioning. Hypoxanthine, a breakdown product of ATP, contributes to the meat's flavor and odor, as do other products of the decomposition of muscle fat and protein.

When meat is industrially processed in preparation of consumption, it may be enriched with additives to protect or modify its flavor or color, to improve its tenderness, juiciness or cohesiveness, or to aid with its preservation. Meat additives include the following:

With the rise of complex supply chains, including cold chains, in developed economies, the distance between the farmer or fisherman and customer has grown, increasing the possibility for intentional and unintentional misidentification of meat at various points in the supply chain.

In 2013, reports emerged across Europe that products labelled as containing beef actually contained horse meat. In February 2013 a study was published showing that about one-third of raw fish are misidentified across the United States.

Various forms of imitation meat have been created for people who wish not to eat meat but still want to taste its flavor and texture. Meat imitates are typically some form of processed soybean (tofu, tempeh), but they can also be based on wheat gluten, pea protein isolate, or even fungi (quorn).

Various environmental effects are associated with meat production. Among these are greenhouse gas emissions, fossil energy use, water use, water quality changes, and effects on grazed ecosystems.

The livestock sector may be the largest source of water pollution (due to animal wastes, fertilizers, pesticides), and it contributes to emergence of antibiotic resistance. It accounts for over 8% of global human water use. It is by far the biggest cause of land use, as it accounts for nearly 40% of the global land surface. It is a significant driver of biodiversity loss, as it causes deforestation, ocean dead zones, land degradation, pollution, and overfishing.

The occurrence, nature and significance of environmental effects varies among livestock production systems. Grazing of livestock can be beneficial for some wildlife species, but not for others. Targeted grazing of livestock is used as a food-producing alternative to herbicide use in some vegetation management.

Meat production is responsible for 14.5% and possibly up to 51% of the world's anthropogenic greenhouse gas emissions. However, greenhouse gas emission depends on the economy and country: animal products (meat, fish, and dairy) account for 22%, 65%, and 70% of emissions in the diets of lower-middle–, upper-middle–, and high-income nations, respectively. Some nations show very different impacts to counterparts within the same group, with Brazil and Australia having emissions over 200% higher than the average of their respective income groups and driven by meat consumption.

According to a report produced by United Nations Environment Programme's (UNEP) international panel for sustainable resource management, a worldwide transition in the direction of a meat and dairy free diet is indispensable if adverse global climate change were to be prevented.

Meat consumption is considered one of the primary contributors of the sixth mass extinction. A 2017 study by the World Wildlife Fund found that 60% of global biodiversity loss is attributable to meat-based diets, in particular from the vast scale of feed crop cultivation needed to rear tens of billions of farm animals for human consumption puts an enormous strain on natural resources resulting in a wide-scale loss of lands and species. Currently, livestock make up 60% of all mammals on earth, followed by humans (36%) and wild mammals (4%). In November 2017, 15,364 world scientists signed a Warning to Humanity calling for, among other things, drastically diminishing our per capita consumption of meat and "dietary shifts towards mostly plant-based foods". 

A July 2018 study in "Science" says that meat consumption is set to rise as the human population increases along with affluence, which will increase greenhouse gas emissions and further reduce biodiversity.

Meat-producing livestock can provide environmental benefits through waste reduction, e.g. conversion of human-inedible residues of food crops. Manure from meat-producing livestock is used as fertilizer; it may be composted before application to food crops. Substitution of animal manures for synthetic fertilizers in crop production can be environmentally significant, as between 43 and 88 MJ of fossil fuel energy are used per kg of nitrogen in manufacture of synthetic nitrogenous fertilizers.

The spoilage of meat occurs, if untreated, in a matter of hours or days and results in the meat becoming unappetizing, poisonous or infectious. Spoilage is caused by the practically unavoidable infection and subsequent decomposition of meat by bacteria and fungi, which are borne by the animal itself, by the people handling the meat, and by their implements. Meat can be kept edible for a much longer time – though not indefinitely – if proper hygiene is observed during production and processing, and if appropriate food safety, food preservation and food storage procedures are applied. Without the application of preservatives and stabilizers, the fats in meat may also begin to rapidly decompose after cooking or processing, leading to an objectionable taste known as warmed over flavor.

Fresh meat can be cooked for immediate consumption, or be processed, that is, treated for longer-term preservation and later consumption, possibly after further preparation. Fresh meat cuts or processed cuts may produce iridescence, commonly thought to be due to spoilage but actually caused structural coloration and diffraction of the light. A common additive to processed meats, both for preservation and because it prevents discoloring, is sodium nitrite, which, however, is also a source of health concerns, because it may form carcinogenic nitrosamines when heated.

Meat is prepared in many ways, as steaks, in stews, fondue, or as dried meat like beef jerky. It may be ground then formed into patties (as hamburgers or croquettes), loaves, or sausages, or used in loose form (as in "sloppy joe" or Bolognese sauce).

Some meat is cured by smoking, which is the process of flavoring, cooking, or preserving food by exposing it to the smoke from burning or smoldering plant materials, most often wood. In Europe, alder is the traditional smoking wood, but oak is more often used now, and beech to a lesser extent. In North America, hickory, mesquite, oak, pecan, alder, maple, and fruit-tree woods are commonly used for smoking. Meat can also be cured by pickling, preserving in salt or brine (see salted meat and other curing methods). Other kinds of meat are marinated and barbecued, or simply boiled, roasted, or fried.

Meat is generally eaten cooked, but many recipes call for raw beef, veal or fish (tartare). Steak tartare is a meat dish made from finely chopped or minced raw beef or horse meat. Meat is often spiced or seasoned, particularly with meat products such as sausages. Meat dishes are usually described by their source (animal and part of body) and method of preparation (e.g., a beef rib).

Meat is a typical base for making sandwiches. Popular varieties of sandwich meat include ham, pork, salami and other sausages, and beef, such as steak, roast beef, corned beef, pepperoni, and pastrami. Meat can also be molded or pressed (common for products that include offal, such as haggis and scrapple) and canned.

A study of 400,000 subjects conducted by the European Prospective Investigation into Cancer and Nutrition and published in 2013 showed "a moderate positive association between processed meat consumption and mortality, in particular due to cardiovascular diseases, but also to cancer."

A 1999 metastudy combined data from five studies from western countries. The metastudy reported mortality ratios, where lower numbers indicated fewer deaths, for fish eaters to be 0.82, vegetarians to be 0.84, occasional meat eaters to be 0.84. Regular meat eaters and vegans shared the highest mortality ratio of 1.00.

In response to changing prices as well as health concerns about saturated fat and cholesterol, consumers have altered their consumption of various meats. A USDA report points out that consumption of beef in the United States between 1970–1974 and 1990–1994 dropped by 21%, while consumption of chicken increased by 90%. During the same period of time, the price of chicken dropped by 14% relative to the price of beef. In 1995 and 1996, beef consumption increased due to higher supplies and lower prices.

The "2015–2020 Dietary Guidelines for Americans" asked men and teenage boys to increase their consumption of vegetables or other underconsumed foods because they eat too much protein.

Various toxic compounds can contaminate meat, including heavy metals, mycotoxins, pesticide residues, and polyaromatic hydrocarbons. Often, these compounds are not very dangerous themselves but can be metabolized in the body to form harmful by-products, so any actual toxicological effects may depend on the individual genome, diet, and history of the consumer.

Meat and meat products may contain substances such as dioxins, polychlorinated biphenyl (PCBs), and cooked meat may contain carcinogens, that are toxic to the consumer, although any chemical's toxicity is dependent on the dose and timing of exposure. Toxins may be introduced to meat as part of animal feed, as veterinary drug residues, or during processing and cooking.

Carcinogenesis is the main long-term toxic response of consuming meat and meat byproducts.

Health concerns have been raised about the consumption of meat increasing the risk of cancer. In particular, red meat and processed meat were found to be associated with higher risk of cancers of the lung, esophagus, liver, and colon, among others — although also a reduced risk for some minor type of cancers.

The International Agency for Research on Cancer (IARC) is the specialized cancer agency of the World Health Organization (WHO). IARC classified processed meat (e.g., bacon, ham, hot dogs, sausages) as, ""carcinogenic to humans" (Group 1), based on "sufficient evidence" in humans that the consumption of processed meat causes colorectal cancer." IARC also classified red meat as ""probably carcinogenic to humans" (Group 2A), based on "limited evidence" that the consumption of red meat causes cancer in humans and "strong" mechanistic evidence supporting a carcinogenic effect."

Another study found an increase risk of pancreatic cancer for red meat and pork. That study noted that, "findings suggest that intakes of red meat and processed meat are positively associated with pancreatic cancer risk and thus are potential target factors for disease prevention. [...] Future analyses of meat and pancreatic cancer risk should focus on meat preparation methods and related carcinogens."

That study also suggests that fat and saturated fat are not likely contributors to pancreatic cancer. Animal fat, particularly from ruminants, tends to have a higher percentage of saturated fat vs. monounsaturated and polyunsaturated fat when compared to vegetable fats, with the exception of some tropical plant fats; consumption of which has been correlated with various health problems. The saturated fat found in meat has been associated with significantly raised risks of colon cancer, although evidence suggests that risks of prostate cancer are unrelated to animal fat consumption.

Other research does not support significant links between meat consumption and various cancers. Key et al. found that "There were no significant differences between vegetarians and nonvegetarians in mortality from cerebrovascular disease, stomach cancer, colorectal cancer, lung cancer, breast cancer, prostate cancer or all other causes combined." Truswell reviewed numerous studies, concluding that the relationship of colorectal cancer with meat consumption appeared weaker than the "probable" status it had been given by the World Cancer Research Foundation in 1997. A study by Chao et al. (2005) found an apparent association of colorectal cancer with red meat consumption after adjustment for age and energy intake. However, after further adjustment for body mass index, cigarette smoking and other covariates, no association with red meat consumption was found. Alex' ander conducted a meta-analysis which found no association of colorectal cancer with consumption of animal fat or protein. Based on European data (EPIC-Oxford study), Key et al. found that incidence of colorectal cancer was somewhat lower among meat eaters than among vegetarians. However, they concluded that 'the study is not large enough to exclude small or moderate differences for specific causes of death, and more research on this topic is required'. A study within the European Prospective Investigation into Cancer and Nutrition (EPIC) found that association between esophageal cancer risk and total and processed meat intake was not statistically significant.

The correlation of consumption to increased risk of heart disease is controversial. Some studies fail to find a link between red meat consumption and heart disease (although the same study found statistically significant correlation between the consumption of processed meat and coronary heart disease), while another study, a survey, conducted in 1960, of 25,153 California Seventh-Day Adventists, found that the risk of heart disease is three times greater for 45- to 64-year-old men who eat meat daily, versus those who did not eat meat.

A major Harvard University study in 2010 involving over one million people who ate meat found that only processed meat had an adverse risk in relation to coronary heart disease. The study suggests that eating 50 g (less than 2oz) of processed meat per day increases risk of coronary heart disease by 42%, and diabetes by 19%. Equivalent levels of fat, including saturated fats, in unprocessed meat (even when eating twice as much per day) did not show any deleterious effects, leading the researchers to suggest that "differences in salt and preservatives, rather than fats, might explain the higher risk of heart disease and diabetes seen with processed meats, but not with unprocessed red meats."

The EPIC-PANACEA study, published in 2010 in the "American Journal of Clinical Nutrition" closely tracked 373,803 people over a period of 8 years across 10 countries. It concluded that meat consumption is positively associated with weight gain in men and women. The National Cattlemen's Beef Association countered by stating that meat consumption may not be associated with fat gain. In response, the authors of the original study controlled for just abdominal fat across a sample of 91,214 people and found that even when controlling for calories and lifestyle factors, meat consumption is linked with obesity. Additional studies and reviews have confirmed the finding that greater meat consumption is positively linked with greater weight gain even when controlling for calories, and lifestyle factors.

A 2011 study by the Translational Genomics Research Institute showed that nearly half (47%) of the meat and poultry in U.S. grocery stores were contaminated with "S. aureus", with more than half (52%) of those bacteria resistant to antibiotics. A 2018 investigation by the Bureau of Investigative Journalism and "The Guardian" found that around 15 percent of the US population suffers from foodborne illnesses every year. The investigation also highlighted unsanitary conditions in US-based meat plants, which included meat products covered in excrement and abscesses "filled with pus".

Meat can transmit certain diseases, but complete cooking and avoiding recontamination reduces this possibility.

Several studies published since 1990 indicate that cooking muscle meat creates heterocyclic amines (HCAs), which are thought to increase cancer risk in humans. Researchers at the National Cancer Institute published results of a study which found that human subjects who ate beef rare or medium-rare had less than one third the risk of stomach cancer than those who ate beef medium-well or well-done. While eating muscle meat raw may be the only way to avoid HCAs fully, the National Cancer Institute states that cooking meat below creates "negligible amounts" of HCAs. Also, microwaving meat before cooking may reduce HCAs by 90%.

Nitrosamines, present in processed and cooked foods, have been noted as being carcinogenic, being linked to colon cancer. Also, toxic compounds called PAHs, or polycyclic aromatic hydrocarbons, present in processed, smoked and cooked foods, are known to be carcinogenic.

Meat is part of the human diet in most cultures, where it often has symbolic meaning and important social functions. Many people, however, choose not to eat meat (this is referred to as vegetarianism) or any food made from animals (veganism). The reasons for not eating all or some meat may include ethical objections to killing animals for food, health concerns, environmental concerns or religious dietary laws.

Ethical issues regarding the consumption of meat include objecting to the act of killing animals or to the agricultural practices used in meat production. Reasons for objecting to killing animals for consumption may include animal rights, environmental ethics, or an aversion to inflicting pain or harm on other sentient creatures. Some people, while not vegetarians, refuse to eat the flesh of certain animals (such as cows, pigs, cats, dogs, horses, or rabbits) due to cultural or religious traditions.

Some people eat only the flesh of animals that they believe have not been mistreated, and abstain from the flesh of animals raised in factory farms or else abstain from particular products, such as foie gras and veal. Some people also abstain from milk and its derivatives for ethical reasons, because the production of veal is a byproduct of the dairy industry. The ethical issues with intensive agriculture have to do with the concentration of animals, animal waste, and the potential for dead animals in a small space.

Some techniques of intensive agriculture may be cruel to animals: foie gras is a food product made from the liver of ducks or geese that have been force fed corn to fatten the organ; veal is criticised because the veal calves may be highly restricted in movement, have unsuitable flooring, spend their entire lives indoors, experience prolonged deprivation (sensory, social, and exploratory), and be more susceptible to high amounts of stress and disease.

The religion of Jainism has always opposed eating meat, and there are also schools of Buddhism and Hinduism that condemn the eating of meat. Jewish dietary rules ("Kashrut") allow certain ("kosher") meat and forbid other ("treif"). The rules include prohibitions on the consumption of unclean animals (such as pork, shellfish including mollusca and crustacea, and most insects), and mixtures of meat and milk. Similar rules apply in Islamic dietary laws: The Quran explicitly forbids meat from animals that die naturally, blood, the meat of swine (porcine animals, pigs), and animals dedicated to other than Allah (either undedicated or dedicated to idols) which are haram as opposed to halal. Sikhism forbids meat of slowly slaughtered animals ("kutha") and prescribes killing animals with a single strike ("jhatka"), but some Sikh groups oppose eating any meat.

Research in applied psychology has investigated practices of meat eating in relation to morality, emotions, cognition, and personality characteristics. Psychological research suggests meat eating is correlated with masculinity, support for social hierarchy, and reduced openness to experience. Research into the consumer psychology of meat is relevant both to meat industry marketing and to advocates of reduced meat consumption.



</doc>
<doc id="18942" url="https://en.wikipedia.org/wiki?curid=18942" title="Monty Python">
Monty Python

Monty Python (also collectively known as The Pythons) were a British surreal comedy group who created their sketch comedy show "Monty Python's Flying Circus", which first aired on the BBC in 1969. Forty-five episodes were made over four series. The Python phenomenon developed from the television series into something larger in scope and impact, including touring stage shows, films, numerous albums, several books, and musicals. The Pythons' influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been referred to as "not only one of the more enduring icons of 1970s British popular culture, but also an important moment in the evolution of television comedy."

Broadcast by the BBC between 1969 and 1974, "Monty Python's Flying Circus" was conceived, written, and performed by its members Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. Loosely structured as a sketch show, but with an innovative stream-of-consciousness approach, aided by Gilliam's animation, it pushed the boundaries of what was acceptable in style and content. A self-contained comedy team responsible for both writing and performing their work, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. Following their television work, they began making films, which include "Holy Grail" (1975), "Life of Brian" (1979) and "The Meaning of Life" (1983). Their influence on British comedy has been apparent for years, while in North America, it has coloured the work of cult performers from the early editions of "Saturday Night Live" through to more recent absurdist trends in television comedy. "Pythonesque" has entered the English lexicon as a result.

In a 2005 poll of over 300 comics, comedy writers, producers and directors throughout the English-speaking world to find "The Comedian's Comedian", three of the six Pythons members were voted to be among the top 50 greatest comedians ever: Cleese at No. 2, Idle at No. 21, and Palin at No. 30.

Jones and Palin met at Oxford University, where they performed together with the Oxford Revue. Chapman and Cleese met at Cambridge University. Idle was also at Cambridge, but started a year after Chapman and Cleese. Cleese met Gilliam in New York City while on tour with the Cambridge University Footlights revue "Cambridge Circus" (originally entitled "A Clump of Plinths"). Chapman, Cleese, and Idle were members of the Footlights, which at that time also included the future "Goodies" (Tim Brooke-Taylor, Bill Oddie, and Graeme Garden), and Jonathan Lynn (co-writer of "Yes Minister" and "Yes, Prime Minister"). During Idle's presidency of the club, feminist writer Germaine Greer and broadcaster Clive James were members. Recordings of Footlights' revues (called "Smokers") at Pembroke College include sketches and performances by Cleese and Idle, which, along with tapes of Idle's performances in some of the drama society's theatrical productions, are kept in the archives of the Pembroke Players.

The six Python members appeared in or wrote these shows before "Flying Circus":
"The Frost Report" is credited as first uniting the British Pythons and providing an environment in which they could develop their particular styles.

Following the success of "Do Not Adjust Your Set", a tea-time children's programme, ITV offered Gilliam, Idle, Jones, and Palin their own late-night adult comedy series together. At the same time, Chapman and Cleese were offered a show by the BBC, which had been impressed by their work on "The Frost Report" and "At Last the 1948 Show". Cleese was reluctant to do a two-man show for various reasons, including Chapman's supposedly difficult and erratic personality. Cleese had fond memories of working with Palin on "How to Irritate People" and invited him to join the team. With no studio available at ITV until summer 1970 for the late-night show, Palin agreed to join Cleese and Chapman, and suggested the involvement of his writing partner Jones and colleague Idle—who in turn wanted Gilliam to provide animations for the projected series. Much has been made of the fact that the Monty Python troupe is the result of Cleese's desire to work with Palin and the chance circumstances that brought the other four members into the fold.

By contrast, according to John Cleese's autobiography, the origins of "Monty Python" lay in the admiration that writing partners Cleese and Chapman had for the new type of comedy being done on "Do Not Adjust Your Set"; as a result, a meeting was initiated by Cleese between Chapman, Idle, Jones, Palin, and himself at which it was agreed to pool their writing and performing efforts and jointly seek production sponsorship.

The Pythons had a definite idea about what they wanted to do with the series. They were admirers of the work of Peter Cook, Alan Bennett, Jonathan Miller, and Dudley Moore on "Beyond the Fringe", and had worked on "Frost", which was similar in style. They enjoyed Cook and Moore's sketch show "Not Only... But Also". One problem the Pythons perceived with these programmes was that though the body of the sketch would be strong, the writers would often struggle to then find a punchline funny enough to end on, and this would detract from the overall sketch quality. They decided that they would simply not bother to "cap" their sketches in the traditional manner, and early episodes of the "Flying Circus" series make great play of this abandonment of the punchline (one scene has Cleese turn to Idle, as the sketch descends into chaos, and remark that "This is the silliest sketch I've ever been in"—they all resolve not to carry on and simply walk off the set). However, as they began assembling material for the show, the Pythons watched one of their collective heroes, Spike Milligan, recording his groundbreaking series "Q5" (1969). Not only was the programme more irreverent and anarchic than any previous television comedy, but Milligan also would often "give up" on sketches halfway through and wander off set (often muttering "Did I write this?"). It was clear that their new series would now seem less original, and Jones in particular became determined the Pythons should innovate.

After much debate, Jones remembered an animation Gilliam had created for "Do Not Adjust Your Set" called "Beware of the Elephants", which had intrigued him with its stream-of-consciousness style. Jones felt it would be a good concept to apply to the series: allowing sketches to blend into one another. Palin had been equally fascinated by another of Gilliam's efforts, entitled "Christmas Cards", and agreed that it represented "a way of doing things differently". Since Cleese, Chapman, and Idle were less concerned with the overall flow of the programme, Jones, Palin, and Gilliam became largely responsible for the presentation style of the "Flying Circus" series, in which disparate sketches are linked to give each episode the appearance of a single stream-of-consciousness (often using a Gilliam animation to move from the closing image of one sketch to the opening scene of another).

Writing started at 9 am and finished at 5 pm. Typically, Cleese and Chapman worked as one pair isolated from the others, as did Jones and Palin, while Idle wrote alone. After a few days, they would join together with Gilliam, critique their scripts, and exchange ideas. Their approach to writing was democratic. If the majority found an idea humorous, it was included in the show. The casting of roles for the sketches was a similarly unselfish process, since each member viewed himself primarily as a "writer", rather than an actor eager for screen time. When the themes for sketches were chosen, Gilliam had a free hand in bridging them with animations, using a camera, scissors, and airbrush.

While the show was a collaborative process, different factions within Python were responsible for elements of the team's humour. In general, the work of the Oxford-educated members (Jones and Palin) was more visual, and more fanciful conceptually (e.g., the arrival of the Spanish Inquisition in a suburban front room), while the Cambridge graduates' sketches tended to be more verbal and more aggressive (for example, Cleese and Chapman's many "confrontation" sketches, where one character intimidates or hurls abuse, or Idle's characters with bizarre verbal quirks, such as "The Man Who Speaks In Anagrams"). Cleese confirmed that "most of the sketches with heavy abuse were Graham's and mine, anything that started with a slow pan across countryside and impressive music was Mike and Terry's, and anything that got utterly involved with words and disappeared up any personal orifice was Eric's". Gilliam's animations, meanwhile, ranged from the whimsical to the savage (the cartoon format allowing him to create some astonishingly violent scenes without fear of censorship).

Several names for the show were considered before "Monty Python's Flying Circus" was settled upon. Some were "Owl Stretching Time", "The Toad Elevating Moment", "A Horse, a Spoon and a Bucket", "Vaseline Review", and "Bun, Wackett, Buzzard, Stubble and Boot". "Flying Circus" stuck when the BBC explained it had printed that name in its schedules and was not prepared to amend it. Many variations on the name in front of this title then came and went (popular legend holds that the BBC considered "Monty Python's Flying Circus" to be a ridiculous name, at which point the group threatened to change their name every week until the BBC relented). "Gwen Dibley's Flying Circus" was named after a woman Palin had read about in the newspaper, thinking it would be amusing if she were to discover she had her own TV show. "Baron Von Took's Flying Circus" was considered as an affectionate tribute to Barry Took, the man who had brought them together. "Arthur Megapode's Flying Circus" was suggested, then discarded. The name "Baron Von Took's Flying Circus" had the form of "Baron Manfred von Richthofen's Flying Circus" of WWI fame, and the new group was forming in a time when the Royal Guardsmen's 1966 song "Snoopy vs. the Red Baron" had peaked. The term 'flying circus' was also another name for the popular entertainment of the 1920s known as barnstorming, where multiple performers collaborated with their stunts to perform a combined set of acts.

Differing, somewhat confusing accounts are given of the origins of the Python name, although the members agree that its only "significance" was that they thought it sounded funny. In the 1998 documentary "Live at Aspen" during the US Comedy Arts Festival, where the troupe was awarded the AFI Star Award by the American Film Institute, the group implied that "Monty" was selected (Eric Idle's idea) as a gently mocking tribute to Field Marshal Lord Montgomery, a legendary British general of World War II; requiring a "slippery-sounding" surname, they settled on "Python". On other occasions, Idle has claimed that the name "Monty" was that of a popular and rotund fellow who drank in his local pub; people would often walk in and ask the barman, "Has Monty been in yet?", forcing the name to become stuck in his mind. The name Monty Python was later described by the BBC as being "envisaged by the team as the perfect name for a sleazy entertainment agent".

"Flying Circus" popularised innovative formal techniques, such as the cold open, in which an episode began without the traditional opening titles or announcements. An example of this is the "It's" man: Palin, outfitted in Robinson Crusoe garb, making a tortuous journey across various terrains, before finally approaching the camera to state, "It's ...", only to be then cut off by the title sequence and theme music.

On several occasions, the cold open lasted until mid-show, after which the regular opening titles ran. Occasionally, the Pythons tricked viewers by rolling the closing credits halfway through the show, usually continuing the joke by fading to the familiar globe logo used for BBC continuity, over which Cleese would parody the clipped tones of a BBC announcer. On one occasion, the credits ran directly after the opening titles.

Because of their dislike of finishing with punchlines, they experimented with ending the sketches by cutting abruptly to another scene or animation, walking offstage, addressing the camera (breaking the fourth wall), or introducing a totally unrelated event or character. A classic example of this approach was the use of Chapman's "anti-silliness" character of "the Colonel", who walked into several sketches and ordered them to be stopped because things were becoming "far too silly".
Another favourite way of ending sketches was to drop a cartoonish "16-ton weight" prop on one of the characters when the sketch seemed to be losing momentum, or a knight in full armour (played by Terry Gilliam) would wander on-set and hit characters over the head with a rubber chicken, before cutting to the next scene. Yet another way of changing scenes was when John Cleese, usually outfitted in a dinner suit, would come in as a radio commentator and, in a rather pompous manner, make the formal and determined announcement "And now for something completely different.", which later became the title of the first Monty Python film.

The Python theme music is "The Liberty Bell", a march by John Philip Sousa, which was chosen, among other reasons, because the recording was in the public domain.

The use of Gilliam's surreal, collage stop motion animations was another innovative intertextual element of the Python style. Many of the images Gilliam used were lifted from famous works of art, and from Victorian illustrations and engravings. The giant foot which crushes the show's title at the end of the opening credits is in fact the foot of Cupid, cut from a reproduction of the Renaissance masterpiece "Venus, Cupid, Folly and Time" by Bronzino. This foot, and Gilliam's style in general, are visual trademarks of the programme.

The Pythons used the British tradition of cross-dressing comedy by donning frocks and makeup and playing female roles themselves while speaking in falsetto. Jones specialised in playing the working-class housewife, with Palin and Idle in being generally more posh. The other members played female roles more sparsely. Generally speaking, female roles were played by women only when the scene specifically required that the character be sexually attractive (although sometimes they used Idle for this). The troupe later turned to Carol Cleveland, who co-starred in numerous episodes after 1970. In some episodes and later in "Monty Python's Life of Brian", they took the idea one step further by playing women who impersonated men (in the stoning scene).

Many sketches are well-known and widely quoted. "Dead Parrot sketch", "The Lumberjack Song", "Spam" (which led to the coining of the term email spam), "Nudge Nudge", "The Spanish Inquisition", "Upper Class Twit of the Year", "Cheese Shop", and "The Ministry of Silly Walks" are just a few examples.

The Canadian Broadcasting Corporation (CBC) added "Monty Python's Flying Circus" to its national September 1970 fall line-up. They aired the 13 episodes of series 1, which had first run on the BBC the previous fall (October 1969 to January 1970), as well as the first six episodes of series 2 only a few weeks after they first appeared on the BBC (September to November 1970). The CBC dropped the show when it returned to regular programming after the Christmas 1970 break, choosing to not place the remaining seven episodes of series 2 on the January 1971 CBC schedule. Within a week, the CBC received hundreds of calls complaining of the cancellation, and more than 100 people staged a demonstration at the CBC's Montreal studios. The show eventually returned, becoming a fixture on the network during the first half of the 1970s.

Time-Life Films had the rights to distribute all BBC-TV programmes in the United States; however, they decided that British comedy simply would not work in America, so it would not be worth the investment to convert the Python episodes from the European PAL standard to the American NTSC standard.

Sketches from "Monty Python's Flying Circus" were introduced to American audiences in August 1972, with the release of the Python film "And Now for Something Completely Different", featuring sketches from series 1 and 2 of the television show. This 1972 release met limited box office success.

In the summer of 1974, Ron Devillier, the programme director for nonprofit PBS television station KERA in Dallas, Texas, started airing episodes of "Monty Python's Flying Circus". Ratings shot through the roof, providing an encouraging sign to the other 100 PBS stations that had signed up to begin airing the show in October 1974—exactly five years after their BBC debut. There was also cross-promotion from FM Radio stations across the country, whose airing of tracks from the Python LPs had already introduced American audiences to this bizarre brand of comedy. The popularity on PBS resulted in the 1974 re-release of the 1972 "...Completely Different" film, with much greater box office success.

The ability to show "Monty Python's Flying Circus" under the American NTSC standard had been made possible by the commercial actions of American television producer Greg Garrison. Garrison produced the NBC series "The Dean Martin Comedy World", which ran during the summer of 1974. The concept was to show clips from comedy shows produced in other countries, including tape of the Python sketches "Bicycle Repairman" and "The Dull Life of a Stockbroker". Payment for use of these two sketches was enough to allow Time-Life Films to convert the entire Python library to NTSC standard, allowing for the sale to the PBS network stations which then brought the entire show to US audiences.

In 1975, ABC broadcast two 90-minute "Monty Python" specials, each with three shows, but cut out a total of 24 minutes from each, in part to make time for commercials, and in part to avoid upsetting their audience. As the judge observed in "Gilliam v. American Broadcasting Companies, Inc.", where Monty Python sued for damages caused by broadcast of the mutilated version, "According to the network, appellants should have anticipated that most of the excised material contained scatological references inappropriate for American television and that these scenes would be replaced with commercials, which presumably are more palatable to the American public." Monty Python won the case.

With the popularity of Python throughout the rest of the 1970s and through most of the 1980s, PBS stations looked at other British comedies, leading to UK shows such as "Are You Being Served?" gaining a US audience, and leading, over time, to many PBS stations having a "British Comedy Night" which airs many popular UK comedies.

Having considered the possibility at the end of the second season, Cleese left the "Flying Circus" at the end of the third. He later explained that he felt he no longer had anything fresh to offer the show, and claimed that only two Cleese- and Chapman-penned sketches in the third series ("Dennis Moore" and the "Cheese Shop") were truly original, and that the others were bits and pieces from previous work cobbled together in slightly different contexts. He was also finding Chapman, who was at that point in the full throes of alcoholism, difficult to work with. According to an interview with Idle, "It was on an Air Canada flight on the way to Toronto, when John (Cleese) turned to all of us and said 'I want out.' Why? I don't know. He gets bored more easily than the rest of us. He's a difficult man, not easy to be friendly with. He's so funny because he never wanted to be liked. That gives him a certain fascinating, arrogant freedom."

The rest of the group carried on for one more "half" season before calling a halt to the programme in 1974. The name "Monty Python's Flying Circus" appears in the opening animation for season four, but in the end credits, the show is listed as simply "Monty Python". Although Cleese left the show, he was credited as a writer for three of the six episodes, largely concentrated in the "Michael Ellis" episode, which had begun life as one of the many drafts of the "Holy Grail" motion picture. When a new direction for "Grail" was decided upon, the subplot of Arthur and his knights wandering around a strange department store in modern times was lifted out and recycled as the aforementioned TV episode.

While the first three seasons contained 13 episodes each, the fourth ended after just six. Extremely keen to keep the now massively popular show going, the BBC had offered the troupe a full 13 episodes, but the truncated troupe (now under the unspoken 'leadership' of Terry Jones) had come to a common agreement while writing the fourth series that there was only enough material, and more importantly only enough enthusiasm, to shoot the six that were made.

The Pythons' first feature film was directed by Ian MacNaughton, reprising his role from the television series. It consisted of sketches from the first two seasons of the "Flying Circus", reshot on a low budget (and often slightly edited) for cinema release. Material selected for the film includes: "Dead Parrot", "The Lumberjack Song", "Upper Class Twit of the Year", "Hell's Grannies", "Self-Defence Class", "How Not to Be Seen", and "Nudge Nudge". Financed by "Playboy"s UK executive Victor Lownes, it was intended as a way of breaking Monty Python into America, and although it was ultimately unsuccessful in this, the film did good business in the UK, this being in the era before home video would make the original material much more accessible. The group did not consider the film a success.

In 1974, between production on the third and fourth seasons, the group decided to embark on their first "proper" feature film, containing entirely new material. "Monty Python and the Holy Grail" was based on Arthurian legend and was directed by Jones and Gilliam. Again, the latter also contributed linking animations (and put together the opening credits). Along with the rest of the Pythons, Jones and Gilliam performed several roles in the film, but Chapman took the lead as King Arthur. Cleese returned to the group for the film, feeling that they were once again breaking new ground. "Holy Grail" was filmed on location, in picturesque rural areas of Scotland, with a budget of only £229,000; the money was raised in part with investments from rock groups such as Pink Floyd, Jethro Tull, and Led Zeppelin—and UK music industry entrepreneur Tony Stratton-Smith (founder and owner of the Charisma Records label, for which the Pythons recorded their comedy albums).

The backers of the film wanted to cut the famous Black Knight scene (in which the Black Knight loses his limbs in a duel), but it was eventually kept in the movie.

Following the success of "Holy Grail", reporters asked for the title of the next Python film, despite the fact that the team had not even begun to consider a third one. Eventually, Idle flippantly replied "Jesus Christ – Lust for Glory", which became the group's stock answer once they realised that it shut reporters up. However, they soon began to seriously consider a film lampooning the New Testament era in the same way "Holy Grail" had lampooned Arthurian legend. Despite them all sharing a distrust of organised religion, they agreed not to mock Jesus or his teachings directly. They also mentioned that they could not think of anything legitimate to make fun of about him. Instead, they decided to write a satire on credulity and hypocrisy among the followers of someone who had been mistaken for the "Messiah", but who had no desire to be followed as such. Chapman was cast in the lead role of Brian.

The focus therefore shifted to a separate individual born at the same time, in a neighbouring stable. When Jesus appears in the film (first, as a baby in the stable, and then later on the Mount, speaking the Beatitudes), he is played straight (by actor Kenneth Colley) and portrayed with respect. The comedy begins when members of the crowd mishear his statements of peace, love, and tolerance ("I think he said, 'Blessed are the cheesemakers).

Directing duties were handled solely by Jones, having amicably agreed with Gilliam that Jones' approach to film-making was better suited for Python's general performing style. "Holy Grail's" production had often been stilted by their differences behind the camera. Gilliam again contributed two animated sequences (one being the opening credits) and took charge of set design. The film was shot on location in Tunisia, the finances being provided this time by former Beatle George Harrison, who together with Denis O'Brien formed the production company Hand-Made Films for the movie. Harrison had a cameo role as the "owner of the Mount".

Despite its subject matter attracting controversy, particularly upon its initial release, it has (together with its predecessor) been ranked among the greatest comedy films. A Channel 4 poll in 2005 ranked "Holy Grail" in sixth place, with "Life of Brian" at the top.

Filmed at the Hollywood Bowl in Los Angeles during preparations for "The Meaning of Life", this was a concert film (directed by Terry Hughes) in which the Pythons performed sketches from the television series in front of an audience. The released film also incorporated footage from the German television specials (the inclusion of which gives Ian MacNaughton his first on-screen credit for Python since the end of "Flying Circus") and live performances of several songs from the troupe's then-current "Monty Python's Contractual Obligation Album".

The Pythons' final film returned to something structurally closer to the style of "Flying Circus". A series of sketches loosely follows the ages of man from birth to death. Directed again by Jones solo, "The Meaning of Life" is embellished with some of the group's most bizarre and disturbing moments, as well as various elaborate musical numbers. The film is by far their darkest work, containing a great deal of black humour, garnished by some spectacular violence (including an operation to remove a liver from a living patient without anaesthetic and the morbidly obese Mr. Creosote exploding over several restaurant patrons). At the time of its release, the Pythons confessed their aim was to offend "absolutely everyone".

Besides the opening credits and the fish sequence, Gilliam, by now an established live-action director, no longer wanted to produce any linking cartoons, offering instead to direct one sketch, "The Crimson Permanent Assurance". Under his helm, though, the segment grew so ambitious and tangential that it was cut from the movie and used as a supporting feature in its own right. (Television screenings also use it as a prologue.) This was the last project on which all six Pythons collaborated, except for the 1989 compilation "Parrot Sketch Not Included," where they are all seen sitting in a closet for four seconds. This was the last time Chapman appeared on screen with the Pythons.

Members of Python contributed their services to charitable endeavours and causes—sometimes as an ensemble, at other times as individuals. The cause that has been the most frequent and consistent beneficiary has been the human rights work of Amnesty International. Between 1976 and 1981, the troupe or its members appeared in four major fund-raisers for Amnesty—known collectively as the "Secret Policeman's Ball" shows—which were turned into multiple films, TV shows, videos, record albums, and books. These benefit shows and their many spin-offs raised considerable sums of money for Amnesty, raised public and media awareness of the human rights cause, and influenced many other members of the entertainment community (especially rock musicians) to become involved in political and social issues. Among the many musicians who have publicly attributed their activism—and the organisation of their own benefit events—to the inspiration of the work in this field of Monty Python are U2, Bob Geldof, Pete Townshend, and Sting. The shows are credited by Amnesty with helping the organisation develop public awareness in the US, where one of the spin-off films was a major success.

Cleese and Jones had an involvement (as performer, writer or director) in all four Amnesty benefit shows, Palin in three, Chapman in two, and Gilliam in one. Idle did not participate in the Amnesty shows. Notwithstanding Idle's lack of participation, the other five members (together with "Associate Pythons" Carol Cleveland and Neil Innes) all appeared together in the first "Secret Policeman's Ball" benefit—the 1976 "A Poke in the Eye"—where they performed several Python sketches. In this first show, they were collectively billed as "Monty Python". (Peter Cook deputised for the errant Idle in a courtroom sketch.) In the next three shows, the participating Python members performed many Python sketches, but were billed under their individual names rather than under the collective Python banner. After a six-year break, Amnesty resumed producing "Secret Policeman's Ball" benefit shows in 1987 (sometimes with, and sometimes without, variants of the iconic title) and by 2006 had presented a total of 12 such shows. The shows since 1987 have featured newer generations of British comedic performers, including many who have attributed their participation in the show to their desire to emulate the Python's pioneering work for Amnesty. (Cleese and Palin made a brief cameo appearance in the 1989 Amnesty show; apart from that, the Pythons have not appeared in shows after the first four.)

Each member has pursued various film, television, and stage projects since the break-up of the group, but often continued to work with one another. Many of these collaborations were very successful, most notably "A Fish Called Wanda" (1988), written by Cleese, in which he starred along with Palin. The pair also appeared in "Time Bandits" (1981), a film directed by Gilliam, who wrote it together with Palin. Gilliam directed "Jabberwocky" (1977), and also directed and co-wrote "Brazil" (1985), which featured Palin, and "The Adventures of Baron Munchausen" (1988), which featured Idle. "Yellowbeard" (1983) was co-written by Chapman and featured Chapman, Idle, and Cleese, as well as many other English comedians including Peter Cook, Spike Milligan, and Marty Feldman.

Palin and Jones wrote the comedic TV series "Ripping Yarns" (1976–79), starring Palin. Jones also appeared in the pilot episode and Cleese appeared in a nonspeaking part in the episode "Golden Gordon". Jones' film "Erik the Viking" also has Cleese playing a small part.

In 1996, Terry Jones wrote and directed an adaptation of Kenneth Grahame's novel "The Wind in the Willows". It featured four members of Monty Python: Jones as Mr. Toad, Idle as Ratty, Cleese as Mr. Toad's lawyer, and Palin as the Sun. Gilliam was considered for the voice of the river.

In terms of numbers of productions, Cleese has the most prolific solo career, having appeared in dozens of films, several TV shows or series (including "Cheers", "3rd Rock from the Sun", Q's assistant in the James Bond movies, and "Will & Grace"), many direct-to-video productions, some video games, and a number of commercials. His BBC sitcom "Fawlty Towers" (written by and starring Cleese together with his then-wife Connie Booth) is the only comedy series to rank higher than the "Flying Circus" on the BFI TV 100's list, topping the whole poll.

Idle enjoyed critical success with "Rutland Weekend Television" in the mid-1970s, out of which came the Beatles parody the Rutles (responsible for the cult mockumentary "All You Need Is Cash"), and as an actor in "Nuns on the Run" (1990) with Robbie Coltrane. In 1976, Idle directed music videos for George Harrison songs "This Song" and "Crackerbox Palace", the latter of which also featured cameo appearances from Neil Innes and John Cleese. Idle has had success with Python songs: "Always Look on the Bright Side of Life" went to no. 3 in the UK singles chart in 1991. The song had been revived by Simon Mayo on BBC Radio 1, and was consequently released as a single that year. The theatrical phenomenon of the Python musical "Spamalot" has made Idle the most financially successful of the troupe after Python. Written by Idle (and featuring a pre-recorded cameo of Cleese as the voice of God), it has proved to be an enormous hit on Broadway, London's West End, and Las Vegas. This was followed by "Not the Messiah", which repurposes "The Life of Brian" as an oratorio. For the work's 2007 premiere at the Luminato festival in Toronto (which commissioned the work), Idle himself sang the "baritone-ish" part.

Since "The Meaning of Life", their last project as a team, the Pythons have often been the subject of reunion rumours. The final reunion of all six members occurred during the "Parrot Sketch Not Included – 20 Years of Monty Python" special. The death of Chapman in 1989 (on the eve of their 20th anniversary) put an end to the speculation of any further reunions. Several occasions since 1989 have occurred when the surviving five members have gathered together for appearances—albeit not formal reunions. In 1996, Jones, Idle, Cleese, and Palin were featured in a film adaptation of "The Wind in the Willows", which was later renamed "Mr. Toad's Wild Ride".

In 1998 during the US Comedy Arts Festival, where the troupe was awarded the AFI Star Award by the American Film Institute, the five remaining members, along with what was purported to be Chapman's ashes, were reunited on stage for the first time in 18 years. The occasion was in the form of an interview called "Monty Python Live at Aspen", (hosted by Robert Klein, with an appearance by Eddie Izzard) in which the team looked back at some of their work and performed a few new sketches.

On 9 October 1999, to commemorate 30 years since the first "Flying Circus" television broadcast, BBC2 devoted an evening to Python programmes, including a documentary charting the history of the team, interspersed with new sketches by the Monty Python team filmed especially for the event. The program appears, with a few omissions, on the DVD "The Life of Python". Idle's involvement in the special is limited, yet the final sketch marks the only time since 1989 that all surviving members of the troupe appear in one sketch, albeit not in the same room.

The surviving Pythons had agreed in principle to perform a live tour of America in 1999. Several shows were to be linked with Q&A meetings in various cities. Although all had said yes, Palin later changed his mind, much to the annoyance of Idle, who had begun work organising the tour. This led to Idle refusing to take part in the new material shot for the BBC anniversary evening.

In 2002, four of the surviving members, bar Cleese, performed "The Lumberjack Song" and "Sit on My Face" for George Harrison's memorial concert. The reunion also included regular supporting contributors Neil Innes and Carol Cleveland, with a special appearance from Tom Hanks.

In an interview to publicise the DVD release of "The Meaning of Life", Cleese said a further reunion was unlikely. "It is absolutely impossible to get even a majority of us together in a room, and I'm not joking," Cleese said. He said that the problem was one of busyness rather than one of bad feelings. A sketch appears on the same DVD spoofing the impossibility of a full reunion, bringing the members "together" in a deliberately unconvincing fashion with modern bluescreen/greenscreen techniques.

Idle has responded to queries about a Python reunion by adapting a line used by George Harrison in response to queries about a possible Beatles reunion. When asked in November 1989 about such a possibility, Harrison responded: "As far as I'm concerned, there won't be a Beatles reunion as long as John Lennon remains dead." Idle's version of this was that he expected to see a proper Python reunion, "just as soon as Graham Chapman comes back from the dead", but added, "we're talking to his agent about terms."

"The Pythons Autobiography by the Pythons" (2003), compiled from interviews with the surviving members, reveals that a series of disputes in 1998, over a possible sequel to "Holy Grail" that had been conceived by Idle, may have resulted in the group's permanent split. Cleese's feeling was that "The Meaning of Life" had been personally difficult and ultimately mediocre, and did not wish to be involved in another Python project for a variety of reasons (not least amongst them was the absence of Chapman, whose straight man-like central roles in the "Grail" and "Brian" films had been considered to be an essential anchoring performance). Apparently, Idle was angry with Cleese for refusing to do the film, which most of the remaining Pythons thought reasonably promising (the basic plot would have taken on a self-referential tone, featuring them in their main 'knight' guises from "Holy Grail", mulling over the possibilities of reforming their posse). The book also reveals that a secondary option around this point was the possibility of revitalising the Python brand with a new stage tour, perhaps with the promise of new material. This idea had also met with Cleese's refusal, this time with the backing of other members.

March 2005 had a full, if nonperforming, reunion of the surviving cast members at the premiere of Idle's musical "Spamalot", based on "Monty Python and the Holy Grail". It opened in Chicago and has since played in New York on Broadway, London, and numerous other major cities across the world. In 2004, it was nominated for 14 Tony Awards and won three: Best Musical, Best Direction of a Musical for Mike Nichols, and Best Performance by a Featured Actress in a Musical for Sara Ramirez, who played the Lady of the Lake, a character specially added for the musical. Cleese played the voice of God, played in the film by Chapman.

Owing in part to the success of "Spamalot", PBS announced on 13 July 2005 that it would begin to re-air the entire run of "Monty Python's Flying Circus" and new one-hour specials focusing on each member of the group, called "Monty Python's Personal Best". Each episode was written and produced by the individual being honoured, with the five remaining Pythons collaborating on Chapman's programme, the only one of the editions to take on a serious tone with its new material.

In 2009, to commemorate the 40th anniversary of the first episode of "Monty Python's Flying Circus", a six-part documentary entitled "" was released, featuring interviews with the surviving members of the team, as well as archive interviews with Graham Chapman and numerous excerpts from the television series and films.

Also in commemoration of the 40th anniversary, Idle, Palin, Jones, and Gilliam appeared in a production of "Not the Messiah" at the Royal Albert Hall. The European premiere was held on 23 October 2009. An official 40th anniversary Monty Python reunion event took place in New York City on 15 October 2009, where the team received a Special Award from the British Academy of Film and Television Arts.

In June 2011, it was announced that "A Liar's Autobiography", an animated 3D movie based on the memoir of Graham Chapman, was in the making. The book "A Liar's Autobiography" was published in 1980 and details Chapman's journey through medical school, alcoholism, acknowledgement of his gay identity, and the tolls of surreal comedy. Asked what was true in a deliberately fanciful account by Chapman of his life, Terry Jones joked: "Nothing ... it's all a downright, absolute, blackguardly lie."

The film uses Chapman's own voice – from a reading of his autobiography shortly before he died of cancer – and entertainment channel Epix announced that the film will be released in early 2012 in both 2D and 3D formats. Produced and directed by London-based Bill Jones, Ben Timlett, and Jeff Simpson, the new film has 15 animation companies working on chapters that will range from three to 12 minutes in length, each in a different style.

John Cleese recorded dialogue which was matched with Chapman's voice. Michael Palin voiced Chapman's father and Terry Jones voiced his mother. Terry Gilliam voiced Graham's psychiatrist. They all play various other roles. Among the original Python group, only Eric Idle was not involved.

On 26 January 2012, Terry Jones announced that the five surviving Pythons would reunite in a sci-fi comedy film called "Absolutely Anything". The film would combine computer-generated imagery and live action. It would be directed by Jones based on a script by Jones and Gavin Scott. The plot revolves around a teacher who discovers aliens (voiced by the Pythons) have given him magical powers to do "absolutely anything". Eric Idle responded via Twitter that he would not, in fact, be participating, although he was later added to the cast.

In 2013, the Pythons lost a legal case to Mark Forstater, the film producer of "Monty Python and the Holy Grail", over royalties for the derivative work "Spamalot". They owed a combined £800,000 in legal fees and back royalties to Forstater. They proposed a reunion show to pay their legal bill.

On 19 November 2013, a new reunion was reported, following months of "secret talks". The original plan was for a live, one-off stage show at the O2 Arena in London on 1 July 2014, with "some of Monty Python's greatest hits, with modern, topical, Pythonesque twists" according to a press release. The tickets for this show went on sale in November 2013 and sold out in just 43 seconds. Nine additional shows were added, all of them at the O2, the last on 20 July. They have said that their reunion was inspired by "South Park" creators Trey Parker and Matt Stone, who are massive Monty Python fans.

Michael Palin stated that the final reunion show on 20 July would be the last time that the troupe would perform together. The event was first shown live from the UK nationwide and was titled "Monty Python Live (Mostly)" and was later reshown at select theatres in recorded form in August.

Graham Chapman was originally a medical student, joining the Footlights at Cambridge. He completed his medical training and was legally entitled to practice as a doctor. Chapman is best remembered for the lead roles in "Holy Grail", as King Arthur, and "Life of Brian", as Brian Cohen. He died of spinal and throat cancer on 4 October 1989. At Chapman's memorial service, Cleese delivered an irreverent eulogy that included all the euphemisms for being dead from the "Dead Parrot" sketch, which they had written. Chapman's comedic fictional memoir, "A Liar's Autobiography", was adapted into an animated 3D movie in 2012.

John Cleese is the oldest Python. He met his future Python writing partner, Chapman, in Cambridge. Outside of Python, he is best known for setting up the Video Arts group and for the sitcom "Fawlty Towers" (co-written with Connie Booth, whom Cleese met during work on Python and to whom he was married for a decade). Cleese has also co-authored several books on psychology and wrote the screenplay for the award-winning "A Fish Called Wanda", in which he starred with Michael Palin.

Terry Gilliam, an American by birth, is the only member of the troupe of non-British origin. He started off as an animator and strip cartoonist for Harvey Kurtzman's "Help!" magazine, one issue of which featured Cleese. Moving from the US to England, he animated features for "Do Not Adjust Your Set" and was then asked by its makers to join them on their next project: "Monty Python's Flying Circus". He co-directed "Monty Python and the Holy Grail" and directed short segments of other Python films (for instance "The Crimson Permanent Assurance", the short film that appears before "The Meaning of Life").

When Monty Python was first formed, two writing partnerships were already in place: Cleese and Chapman, Jones and Palin. That left two in their own corners: Gilliam, operating solo due to the nature of his work, and Eric Idle. Regular themes in Idle's contributions were elaborate wordplay and musical numbers. After "Flying Circus", he hosted "Saturday Night Live" four times in the first five seasons. Idle's initially successful solo career faltered in the 1990s with the failures of his 1993 film "Splitting Heirs" (written, produced by, and starring him) and 1998's "" (in which he starred), which was awarded five Razzies, including 'Worst Picture of the Year'. He revived his career by returning to the source of his worldwide fame, adapting Monty Python material for other media. He also wrote the Broadway musical "Spamalot", based on the "Holy Grail" movie. He also wrote "Not the Messiah", an oratorio derived from the "Life of Brian".

Terry Jones has been described by other members of the team as the "heart" of the operation. Jones had a lead role in maintaining the group's unity and creative independence. Python biographer George Perry has commented that should "[you] speak to him on subjects as diverse as fossil fuels, or Rupert Bear, or mercenaries in the Middle Ages or Modern China ... in a moment you will find yourself hopelessly out of your depth, floored by his knowledge." Many others agree that Jones is characterised by his irrepressible, good-natured enthusiasm. However, Jones' passion often led to prolonged arguments with other group members—in particular Cleese—with Jones often unwilling to back down. Since his major contributions were largely behind the scenes (direction, writing), and he often deferred to the other members of the group as an actor, Jones' importance to Python was often under-rated. However, he does have the legacy of delivering possibly the most famous line in all of Python, as Brian's mother Mandy in "Life of Brian", "He's not the Messiah, he's a very naughty boy!", a line voted the funniest in film history on two occasions.

Michael Palin attended Oxford, where he met his Python writing partner Jones. The two also wrote the series "Ripping Yarns" together. Palin and Jones originally wrote face-to-face, but soon found it was more productive to write apart and then come together to review what the other had written. Therefore, Jones and Palin's sketches tended to be more focused than that of the others, taking one bizarre situation, sticking to it, and building on it. After "Flying Circus", Palin hosted "Saturday Night Live" four times in the first 10 seasons. His comedy output began to decrease in amount following the increasing success of his travel documentaries for the BBC. Palin released a book of diaries from the Python years entitled "Michael Palin Diaries 1969–1979", published in 2007.

Several people have been accorded unofficial "associate Python" status over the years. Occasionally such people have been referred to as the 'seventh Python', in a style reminiscent of George Martin (or other associates of the Beatles) being dubbed "the Fifth Beatle". The two collaborators with the most meaningful and plentiful contributions have been Neil Innes and Carol Cleveland. Both were present and presented as Associate Pythons at the official Monty Python 25th-anniversary celebrations held in Los Angeles in July 1994.

Neil Innes is the only non-Python besides Douglas Adams to be credited with writing material for "Flying Circus". He appeared in sketches and the Python films, as well as performing some of his songs in "Monty Python Live at the Hollywood Bowl". He was also a regular stand-in for absent team members on the rare occasions when they recreated sketches. For example, he took the place of Cleese at the Concert for George. Gilliam once noted that if anyone qualified for the title of the seventh Python, it would certainly be Innes. He was one of the creative talents in the off-beat Bonzo Dog Band. He would later portray Ron Nasty of the Rutles and write all of the Rutles' compositions for "All You Need Is Cash" (1978). By 2005, a falling out had occurred between Idle and Innes over additional Rutles projects, the results being Innes' critically acclaimed Rutles "reunion" album "The Rutles: Archaeology" and Idle's straight-to-DVD "The Rutles 2: Can't Buy Me Lunch", each undertaken without the other's participation. According to an interview with Idle in the "Chicago Tribune" in May 2005, his attitude is that Innes and he go back "too far. And no further." Innes has remained silent on the dispute.

Carol Cleveland was the most important female performer in the Monty Python ensemble, commonly referred to as "the female Python". She was originally hired by producer/director John Howard Davies for just the first five episodes of the "Flying Circus". The Pythons then pushed to make Cleveland a permanent recurring performer after producer/director Ian MacNaughton brought in several other actresses who were not as good as she was. Cleveland went on to appear in about two-thirds of the episodes, as well as in all of the Python films, and in most of their stage shows, as well.

Cleese's first wife, Connie Booth, appeared as various characters in all four series of "Flying Circus". Her most significant role was the "best girl" of the eponymous Lumberjack in "The Lumberjack Song", though this role was sometimes played by Carol Cleveland. Booth appeared in a total of six sketches and also played one-off characters in Python feature films "And Now for Something Completely Different" and "Monty Python and the Holy Grail".

Douglas Adams was "discovered" by Chapman when a version of "Footlights Revue" (a 1974 BBC2 television show featuring some of Adams' early work) was performed live in London's West End. In Cleese's absence from the final TV series, the two formed a brief writing partnership, with Adams earning a writing credit in one episode for a sketch called "Patient Abuse". In the sketch, a man who had been stabbed by a nurse arrives at his doctor's office bleeding profusely from the stomach, when the doctor makes him fill in numerous senseless forms before he can administer treatment. He also had two cameo appearances in this season. Firstly, in the episode "The Light Entertainment War", Adams shows up in a surgeon's mask (as Dr. Emile Koning, according to the on-screen captions), pulling on gloves, while Palin narrates a sketch that introduces one person after another, and never actually gets started. Secondly, at the beginning of "Mr. Neutron", Adams is dressed in a "pepperpot" outfit and loads a missile onto a cart being driven by Terry Jones, who is calling out for scrap metal ("Any old iron ..."). Adams and Chapman also subsequently attempted a few non-Python projects, including "Out of the Trees." He also contributed to a sketch on the soundtrack album for "Monty Python and the Holy Grail".

Other than Carol Cleveland, the only other non-Python to make a significant number of appearances in the "Flying Circus" was Ian Davidson. He appeared in the first two series of the show, and played over 10 roles. While Davidson is primarily known as a scriptwriter, it is not known if he had any contribution toward the writing of the sketches, as he is only credited as a performer. In total, Davidson is credited as appearing in eight episodes of the show, which is more than any other male actor who was not a Python. Despite this, Davidson did not appear in any Python-related media subsequent to series 2, though footage of him was shown on the documentary "Python Night – 30 Years of Monty Python".

Stand-up comedian Eddie Izzard, a devoted fan of the group, has occasionally stood in for absent members. When the BBC held a "Python Night" in 1999 to celebrate 30 years of the first broadcast of "Flying Circus", the Pythons recorded some new material with Izzard standing in for Idle, who had declined to partake in person (he taped a solo contribution from the US). Izzard hosted "The Life of Python" (1999), a history of the group that was part of Python Night and appeared with them at a festival/tribute in Aspen, Colorado, in 1998 (released on DVD as "Live at Aspen"). Izzard has said that Monty Python was a significant influence on his style of comedy and Cleese has referred to him as "the lost Python".

Series director of "Flying Cirus", Ian MacNaughton, is also regularly associated with the group and made a few on-screen appearances in the show and in the film "And Now for Something Completely Different". Apart from Neil Innes, others to contribute musically included Fred Tomlinson and the Fred Tomlinson Singers. They made appearances in songs such as "The Lumberjack Song" as a backup choir. In addition, various other contributors and performers for the Pythons included John Howard Davies, John Hughman, Lyn Ashley, Bob Raymond, John Young, Rita Davies, Stanley Mason, Maureen Flanagan, and David Ballantyne.

Monty Python in films

Monty Python live

Monty Python reunions

By the time of Monty Python's 25th anniversary, in 1994, the point was already being made that "the five surviving members had with the passing years begun to occupy an institutional position in the edifice of British social culture that they had once had so much fun trying to demolish". A similar point is made in a 2006 book on the relationship between Monty and philosophy: "It is remarkable, after all, not only that the utterly bizarre "Monty Python's Flying Circus" was sponsored by the BBC in the first place, but that Monty Python itself grew into an institution of enormous cultural influence."

Monty Python has been named as being influential to the comedy stylings of a great many additional people including: Sacha Baron Cohen, David Cross, Noel Fielding, Seth MacFarlane, Seth Meyers, Trey Parker, Matt Stone, and Vic and Bob.


Amongst the more visible cultural influences of Monty Python is the inclusion of terms either directly from, or derived from, Monty Python, into the lexicon of the English language.


The Japanese anime series, "Girls und Panzer", featured the special episode, "Survival War!", which referenced the 'Spam' sketch.

Beyond a dictionary definition, Python terms have entered the lexicon in other ways.


On St George's Day, 23 April 2007, the cast and creators of "Spamalot" gathered in Trafalgar Square under the tutelage of the two Terrys (Jones and Gilliam) to set a new record for the world's largest coconut orchestra. They led 5,567 people "clip-clopping" in time to the Python classic, "Always Look on the Bright Side of Life", for the "Guinness World Records" attempt.


Five Monty Python productions were released as theatrical films:









</doc>
<doc id="18943" url="https://en.wikipedia.org/wiki?curid=18943" title="Married... with Children">
Married... with Children

Married... with Children is an American television sitcom that aired on Fox, created by Michael G. Moye and Ron Leavitt. Originally broadcast from April 5, 1987 to June 9, 1997, it is the longest-lasting live-action sitcom on Fox and the first to be broadcast in the network's primetime programming slot.

The show follows the lives of Al Bundy, a once glorious high school football player turned hard-luck women's shoe salesman; his obnoxious wife, Peggy; their attractive, promiscuous, and clueless daughter, Kelly; and their girl-crazy, wisecracking son, Bud. Their neighbors are the upwardly mobile Steve Rhoades and his feminist wife Marcy, who later gets remarried to Jefferson D'Arcy, a white-collar criminal who becomes her "trophy husband" and Al's sidekick. Most storylines involve Al's schemes being foiled by his own cartoonish dim wit and bad luck.

The series comprises 259 episodes and 11 seasons. Its theme song is "Love and Marriage" by Sammy Cahn and Jimmy Van Heusen, performed by Frank Sinatra from the 1955 television production "Our Town".

The first two seasons of the series were videotaped at ABC Television Center in Hollywood. From season three to season eight, the show was taped at Sunset Gower Studios in Hollywood, and the remaining three seasons were taped at Sony Pictures Studios in Culver City. The series was produced by Embassy Communications during its first season and half of its second season and the remaining seasons by ELP Communications under the studio Columbia Pictures Television.

In 2008, the show placed number 94 on "Entertainment Weekly" "New TV Classics" list.


It is never explained how Peggy was able to go to high school in Chicago when she lived in Wisconsin. While the audience is aware that Al's father died years ago, his mother's whereabouts are never mentioned or acknowledged. She is only heard once during the season one episode "Nightmare On Al's Street". Peggy calls her briefly on the phone, because she promised to invite her for summer the next time Al was "right about anything".



On April 22, 2012, Fox reaired the series premiere in commemoration of its 25th anniversary.

During its 11-season run on the Fox network, "Married ... with Children" aired 258 episodes. A 259th episode, "I'll See You in Court" from season 3, never aired on Fox (see below), but premiered on FX and has since been included on DVD and in syndication packages. The episode counts in the chart below include it. Three specials also aired following the series' cancellation, including a cast reunion.

Despite the show's enduring popularity and fanbase, "Married ... with Children" was never a huge ratings success. Part of the reason was the fact that Fox, being a new startup network, did not have the affiliate base of the Big Three television networks, thus preventing the series from reaching the entire country. In an interview for a special commemorating the series' 20-year anniversary in 2007, Katey Sagal stated that part of the problem the series faced was that many areas of the country were able to get Fox only through low-quality UHF channels well into the early 1990s, while some areas of the country did not receive the new network at all, a problem not largely rectified until the 1994 United States broadcast TV realignment which brought the NFC football rights to the network, and beyond, as the network maintained a national cable channel to carry the network in very small markets over cable as late as 2006.

Another problem lay in the fact that many of the newly developed series on Fox were unsuccessful, which kept the network from building a popular lineup to draw in a larger audience. In its original airing debut, "Married ... with Children" was part of a Sunday lineup that competed with the popular "Murder, She Wrote" and Sunday-night movie on CBS. Fellow freshman series included "Duet", cancelled in 1989, along with "It's Garry Shandling's Show" and "The Tracey Ullman Show", both of which were canceled in 1990. The success of "The Simpsons", which debuted on "The Tracey Ullman Show" in 1987, helped draw some viewers over to Fox, allowing "Married ... with Children" to sneak into the top 50 of television shows for seasons 3 through 9 doing its best overall rating at number 8 for its third and tenth season. Although these ratings were somewhat small in comparison with the other three networks, they were good enough for Fox to keep renewing the show.

Ratings data for some seasons courtesy of TVTango.com.

In 1989, Terry Rakolta, from Bloomfield Hills, Michigan, led a boycott of the show after viewing the episode "Her Cups Runneth Over". Offended by the images of an old man wearing a woman's garter and stockings, the scene where Steve touches the pasties of a mannequin dressed in S&M gear, a homosexual man wearing a tiara on his head (and Al's line "...and they wonder why we call them 'queens'"), and a half-nude woman who takes off her bra in front of Al (and is shown with her arms covering her bare chest in the next shot), Rakolta began a letter-writing campaign to advertisers, demanding they boycott the show.

After advertisers began dropping their support for the show and while Rakolta made several appearances on television talk shows demanding the show's cancellation, Fox executives refused to air the episode titled "I'll See You in Court". This episode would become known as the "Lost Episode" and was aired on FX on June 18, 2002, with some parts cut. The episode was packaged with the rest of the third season in the January 2005 DVD release (and in the first volume of the "Married ... With Children Most Outrageous Episode" DVD set) with the parts cut from syndication restored.

Ironically, viewers' curiosity over the boycott and over the show itself led to a drastic ratings boost in an example of the Streisand Effect, which Rakolta has since acknowledged. She has been referenced twice on the show: "Rock and Roll Girl", when a newscaster mentioned the city Bloomfield Hills, and "No Pot to Pease In", when a television show was made about the Bundy family and then was cancelled because (according to Marcy) "some woman in Michigan didn't like it".

The conservative Parents Television Council named "Married... with Children" the worst show of both the 1995–96 and 1996–97 television seasons in its first two years in operation. In 1996, the organization called the show the "crudest comedy on prime time television...peppered with lewd punch lines about sex, masturbation, the gay lifestyle, and the lead character's fondness for pornographic magazines and strip clubs."

A book discussing the creation of "Married. . . with Children," the controversy around it and the aftermath of that controversy, as well as telling the entire behind-the-scenes story of the show, was released in 2017 by Bear Manor Media. The book is written by Denise Noe who interviewed David Garrison, E.E. Bell, dog trainer Steven Ritt, and others who worked on the show for the book.

Sony Pictures Home Entertainment has released all eleven seasons of "Married... with Children" on DVD in Regions 1, 2, & 4. On December 12, 2010, Sony released a complete series set on DVD in Region 1.

In December 2007, the Big Bundy Box—a special collection box with all seasons plus new interviews with Sagal and David Faustino—was released. This boxset was released in Australia (Region 4) on November 23, 2009.

The Sony DVD box sets from season 3 onward do not feature the original "Love and Marriage" theme song in the opening sequence. This was done because Sony was unable to obtain the licensing rights to the song for later sets. Despite this, the end credits on the DVDs for season 3 still include a credit for "Love and Marriage."

On August 27, 2013, it was announced that Mill Creek Entertainment had acquired the home media rights to various television series from the Sony Pictures library including "Married... with Children" with the original theme song "Love and Marriage" sung by Frank Sinatra. They have subsequently re-released the 11 seasons on DVD. A complete series DVD set was re-released on July 7, 2015.


"Married... with Children" was adapted into a comic book series by NOW Comics in 1990.


Two series (10 in all) of 8" action figures were produced by Classic TV Toys in 2005 and 2006.

An Armenian remake was made in 2016, called "The Azizyans". The Azizyans is an Armenian sitcom television series developed by Robert Martirosyan and Van Grigoryan. The series premiered on Armenia TV on October 31, 2016. However, the series were not available to the public until Armenia TV started airing the sitcom from October 10, 2017. The series takes place in Yerevan, Armenia. The Azizyans sitcom is starred by Hayk Marutyan. He embodies the character of Garnik Azizyan – a clothes store seller, who is the only one working in the family. Mrs.Ruzan Azizyan is lazy enough to perform the duties of a housewife. The problems of the father of the family don’t bother his 3 children – his daughter, who is internet-addicted and is active in all social networks; his unemployed eldest son, who is a complete loser, and his youngest son, who is a schoolboy. The roles in this sitcom, created for family watching, are played by Ani Lupe, Satenik Hazaryan, Ishkhan Gharibyan, Suren Arustamyan and other popular Armenian actors. The project is directed by Arman Marutyan. In the second season of the sitcom, the Azizyan family continues to survive thanks to the meager salary of Garnik. The wife of Garnik - Ruzan, remains in the status of a housewife, without even thinking about finding a job. The elder son of Garnik and Ruzan - Azat, continues to look for a new job, a young man appears in the life of Marie, who is trying to win the girl's heart. Their younger son Levon, continues to live his own life and does not understand what he has in common with this family. And their neighbors Irina and Alik continue to be friends with the family, which Azizyans do not quite approve. The only bright spot in the life of the family is their house, which Garnik inherited from his grandfather. 

An Argentine remake was made in 2005, called "Casados con Hijos". The series was also shown by local channels in Uruguay, Paraguay, and Peru. Only two seasons were made (2005 and 2006), but it is still aired Monday through Friday at 2pm and Saturday at 11.30 pm by Telefe.

The character names are: José "Pepe" Argento (based on Al, played by Guillermo Francella), Mónica "Moni" Argento (based on Peggy, played by Florencia Peña), Paola Argento (based on Kelly, played by Luisana Lopilato), Alfio "Coqui" Argento (based on Bud, played by Darío Lopilato), Dardo and María Elena Fuseneco (based on Marcy and Jefferson D'Arcy, played by Marcelo de Bellis and Érica Rivas).

In Brazil Rede Bandeirantes made a remake in 1999 with the name "A Guerra dos Pintos" (The War of The Pintos). 52 episodes were recorded but only 22 aired before cancelation.

In Bulgaria a remake is airing from March 26, 2012 with the name "Женени с деца в България" (Zheneni s detsa v Bulgaria) (Married with children in Bulgaria).

In Croatia a remake called "Bračne vode" was broadcast from September 2008 until November 2009 on Nova TV channel. The characters based on the Bundys were called Zvonimir, Sunčica, Kristina and Boris Bandić while the ones based on Marcy and Steve were called Marica and Ivan Kumarica.

In Germany, the 1992 remake "Hilfe, meine Familie spinnt", broadcast in the prime time, reached double the audience than the original (broadcast in the early fringe time). This, however, was not enough to maintain the series, so it was cancelled after one season. The remake used the exact translated scripts of the original series (which already substituted localised humour and in-jokes for incomprehensible references to American TV shows not shown in Germany, as well as some totally different jokes) and just renamed place and person names according to the new setting.

" was aired from March to December 1993 for 26 episodes.

In 2006, Hungarian TV network TV2 purchased the license rights including scripts and hired the original producers from Sony Pictures for a remake show placed in Hungarian environment. It was entitled " (in English: "Married with children in Budapest", loan translation: "A gruesomely decent family in Budapest"). The main story began with the new family called the Bándis inherit an outskirt house from their American relatives the Bundys. They filmed a whole season of 26 episodes, all of them being remade versions of the plots of the original first seasons. It was the highest budget sitcom ever made in Hungary. First it was aired on Tuesday nights, but was beaten by a new season of "ER", then placed to Wednesday nights. The remake lost its viewers, but stayed on the air due to the contract between Sony and TV2.


The Original "Married... With Children" ran on TV-6 Russia in the late 1990s and early 2000s (before closing channel) in prime-time basis, broadcasting the episodes from seasons 1–10. The show later aired on DTV and Domashniy TV. However, for unknown reasons, most episodes from season 11 were not shown. A Russian adaptation, titled "Happy Together" (Schastlivy Vmeste; "Happy Together"), is now airing on TNT channel across the country.

The character names are: Gena Bukin (based on Al, played by Viktor Loginov), Dasha Bukina (based on Peggy, played by Natalya Bochkareva), Sveta Bukina (based on Kelly, played by Darya Sagalova), Roma Bukin (based on Bud, played by Alexander Yakin), Elena and Anatoliy Poleno (based on Marcy and Jefferson D'Arcy, played by Yulia Zaharova and Pavel Savinkov), Evgeniy Stepanov (based on Steve Rhoades, played by Aleksey Sekirin), Sema Bukin (based on Seven, played by Ilya Butkovskiy), Baron Bukin (based on Buck and Lucky, played by Bayra).


ITV had been screening the original "Married... With Children" since 1988. In 1996, the UK production company Central Television and Columbia Pictures Television (Columbia TriStar Central Productions) produced UK version called Married for Life, which lasted for one series with seven episodes.

"Top of the Heap" was a sitcom starring Matt LeBlanc. The show was about Vinnie Verducci (played by LeBlanc) and his father Charlie (played by Joseph Bologna) always trying get rich quick schemes. The Verduccis were introduced in an earlier episode where Vinnie dated Kelly Bundy, and Charlie was introduced as an old friend of Al Bundy's. The end of the pilot episode shows Al breaking into their apartment and stealing their TV to replace the one he lost betting on Vinnie in a boxing match. However, the show didn't last long and was ultimately cancelled. It had its own spin-off/sequel called "Vinnie & Bobby" a year later, which was also cancelled.

Also, an attempt was made to make a spin-off out of David Garrison's Steve Rhoades character which took place on Bud's Trumaine University called "Radio Free Trumaine" where Garrison played the Dean. "Enemies" was another spin-off, but played to be a spoof on the TV series, "Friends".

On September 11, 2014, it was announced that a spin-off was in the works, centered on the character of Bud Bundy.

Distributed by Columbia Pictures Television Distribution (now Sony Pictures Television Distribution), "Married... with Children" debuted in off-network syndication in the fall of 1991. The series later began airing on cable on FX from September 1998 until 2007. In June 2002, FX became the first television network to air the controversial, previously banned episode "I'll See You in Court", albeit in an edited format. The fully uncensored version of "I'll See You in Court" can only be seen on the DVD release "Married... with Children: The Most Outrageous Episodes Volume 1" and the Mill Creek Entertainment complete series collection. The version found on the Third Season DVD set under Sony is the edited-for-TV version. In 2008, the Spike network reportedly paid US$12 million for broadcast rights to every episode including the unedited version of the infamous episode, "I'll See You in Court".

The series started airing on Spike TV on September 29, 2008 with a weeklong marathon. TBS also began airing the show shortly after, acquiring the show in fall 2008 to run in the early morning hours, it runs for two to three hours (on rare occasions four or five) on TBS during the early morning hours (depending on the length of overnight programming). TV Land picked up the rights to broadcast the show from its MTV Networks sister Spike in August 2009. Comedy Central began airing the show on February 8, 2010; Comedy Central acquired rights to air the series from TV Land, who in turn, had earlier acquired the rights to the series from Spike, though Comedy Central dropped the rights to the series in April 2010. Spike picked up the rights to series again, and began airing the series for the second time on July 10, 2010, airing on weekend mornings only. All three cable channels are owned by Viacom. The comedy began airing on Nick at Nite on July 6, 2011. MTV2 added the series on March 21, 2012 and VH1 Classic began airing the series on April 9, 2012. The series has aired on a total of seven MTV Networks owned cable networks since 2008. It aired on Antenna TV, Ion Television and currently airs on TBS and LOGO TV

"Married...with Children" has also been a ratings success in other countries around the world.
The opening footage comprises views of Chicago, opening with a shot of Buckingham Fountain in Grant Park. The aerial downtown shot was taken from the Lake Shore Drive section north of the Loop. The expressway entrance shot was taken from the 1983 movie "National Lampoon's Vacation" featuring the Griswolds' green family truckster with a northeastward view of the Dan Ryan/Stevenson junction southwest of the Loop. Both the downtown view and the highway entrance shot were omitted from Season 4 onwards, but the remaining fountain shot included an "In Stereo Where Available" note. Non-English versions might differ, e.g. the dubbed German version always includes the expressway shot.




</doc>
<doc id="18947" url="https://en.wikipedia.org/wiki?curid=18947" title="Metre">
Metre

The metre (British spelling and BIPM spelling) or meter (American spelling) (from the French unit "mètre", from the Greek noun μέτρον, "measure") is the base unit of length in some metric systems, including the International System of Units (SI). The SI unit symbol is m. The metre is defined as the length of the path travelled by light in a vacuum in second.

The metre was originally defined in 1793 as one ten-millionth of the distance from the equator to the North Pole. In 1799, it was redefined in terms of a prototype metre bar (the actual bar used was changed in 1889). In 1960, the metre was redefined in terms of a certain number of wavelengths of a certain emission line of krypton-86. In 1983, the current definition was adopted.

The imperial inch is defined as 0.0254 metres (2.54 centimetres or 25.4 millimetres). One metre is about inches longer than a yard, i.e. about inches.

"Metre" is the standard spelling of the metric unit for length in nearly all English-speaking nations except the United States and the Philippines, which use "meter." Other Germanic languages, such as German, Dutch, and the Scandinavian languages likewise spell the word "meter."

Measuring devices (such as ammeter, speedometer) are spelled "-meter" in all variants of English. The suffix "-meter" has the same Greek origin as the unit of length.

The etymological roots of "metre" can be traced to the Greek verb ' (metreo) (to measure, count or compare) and noun ' (metron) (a measure), which were used for physical measurement, for poetic metre and by extension for moderation or avoiding extremism (as in "be measured in your response"). This range of uses is also found in Latin ("metior", "mensura"), French ("mètre", "mesure"), English and other languages. The motto "ΜΕΤΡΩ ΧΡΩ" (metro chro) in the seal of the International Bureau of Weights and Measures (BIPM), which was a saying of the Greek statesman and philosopher Pittacus of Mytilene and may be translated as "Use measure!", thus calls for both measurement and moderation.

In 1668 the English cleric and philosopher John Wilkins proposed in an essay a decimal-based unit of length, the "universal measure" or "standard" based on a pendulum with a two-second period. The use of the seconds pendulum to define length had been suggested to the Royal Society in 1660 by Christopher Wren. Christiaan Huygens had observed that length to be 38 Rijnland inches or 39.26 English inches; that is, 997mm. No official action was taken regarding these suggestions.

In 1670 Gabriel Mouton, Bishop of Lyon, also suggested a universal length standard with decimal multiples and divisions, to be based on a one-minute angle of the Earth's meridian arc or (as the Earth's circumference was not easy to measure) on a pendulum with a two-second period. In 1675, the Italian scientist Tito Livio Burattini, in his work ', used the phrase ' ("universal measure"), derived from the Greek ("métron katholikón"), to denote the standard unit of length derived from a pendulum. As a result of the French Revolution, the French Academy of Sciences charged a commission with determining a single scale for all measures. On 7 October 1790 that commission advised the adoption of a decimal system, and on 19 March 1791 advised the adoption of the term "mètre" ("measure"), a basic unit of length, which they defined as equal to one ten-millionth of the distance between the North Pole and the Equator. In 1793, the French National Convention adopted the proposal; this use of "metre" in English began at least as early as 1797.

In 1791, the French Academy of Sciences selected the meridional definition over the pendular definition because the force of gravity varies slightly over the surface of the Earth, which affects the period of a pendulum.

To establish a universally accepted foundation for the definition of the metre, more accurate measurements of this meridian were needed. The French Academy of Sciences commissioned an expedition led by Jean Baptiste Joseph Delambre and Pierre Méchain, lasting from 1792 to 1799, which attempted to accurately measure the distance between a belfry in Dunkerque and Montjuïc castle in Barcelona to estimate the length of the meridian arc through Dunkerque. This portion of the meridian, assumed to be the same length as the Paris meridian, was to serve as the basis for the length of the half meridian connecting the North Pole with the Equator. The problem with this approach is that the exact shape of the Earth is not a simple mathematical shape, such as a sphere or oblate spheroid, at the level of precision required for defining a standard of length. The irregular and particular shape of the Earth smoothed to sea level is represented by a mathematical model called a geoid, which literally means "Earth-shaped". Despite these issues, in 1793 France adopted this definition of the metre as its official unit of length based on provisional results from this expedition. However, it was later determined that the first prototype metre bar was short by about 200 micrometres because of miscalculation of the flattening of the Earth, making the prototype about 0.02% shorter than the original proposed definition of the metre. Regardless, this length became the French standard and was progressively adopted by other countries in Europe.

The expedition was fictionalised in Denis Guedj, "Le mètre du Monde". Ken Alder wrote factually about the expedition in "The Measure of All Things: the seven year odyssey and hidden error that transformed the world".

In 1867 at the second general conference of the International Association of Geodesy held in Berlin, the question of an international standard unit of length was discussed in order to combine the measurements made in different countries to determine the size and shape of the Earth. The conference recommended the adoption of the metre and the creation of an international metre commission, according to the proposal of Johann Jacob Baeyer, Adolphe Hirsch and Carlos Ibáñez e Ibáñez de Ibero.

In the 1870s and in light of modern precision, a series of international conferences was held to devise new metric standards. The Metre Convention ("Convention du Mètre") of 1875 mandated the establishment of a permanent International Bureau of Weights and Measures (BIPM: ') to be located in Sèvres, France. This new organisation was to construct and preserve a prototype metre bar, distribute national metric prototypes, and maintain comparisons between them and non-metric measurement standards. The organisation created such a bar in 1889 at the first General Conference on Weights and Measures (CGPM: '), establishing the "International Prototype Metre" as the distance between two lines on a standard bar composed of an alloy of 90% platinum and 10% iridium, measured at the melting point of ice.

The original international prototype of the metre is still kept at the BIPM under the conditions specified in 1889.

In 1893, the standard metre was first measured with an interferometer by Albert A. Michelson, the inventor of the device and an advocate of using some particular wavelength of light as a standard of length. By 1925, interferometry was in regular use at the BIPM. However, the International Prototype Metre remained the standard until 1960, when the eleventh CGPM defined the metre in the new International System of Units (SI) as equal to 1 650 763.73 wavelengths of the orange-red emission line in the electromagnetic spectrum of the krypton-86 atom in a vacuum.

To further reduce uncertainty, the 17th CGPM in 1983 replaced the definition of the metre with its current definition, thus fixing the length of the metre in terms of the second and the speed of light:

This definition fixed the speed of light in vacuum at exactly metres per second (≈). An intended by-product of the 17th CGPM's definition was that it enabled scientists to compare lasers accurately using frequency, resulting in wavelengths with one-fifth the uncertainty involved in the direct comparison of wavelengths, because interferometer errors were eliminated. To further facilitate reproducibility from lab to lab, the 17th CGPM also made the iodine-stabilised helium–neon laser "a recommended radiation" for realising the metre. For the purpose of delineating the metre, the BIPM currently considers the HeNe laser wavelength, , to be with an estimated relative standard uncertainty ("U") of . This uncertainty is currently one limiting factor in laboratory realisations of the metre, and it is several orders of magnitude poorer than that of the second, based upon the caesium fountain atomic clock (). Consequently, a realisation of the metre is usually delineated (not defined) today in labs as wavelengths of helium-neon laser light in a vacuum, the error stated being only that of frequency determination. This bracket notation expressing the error is explained in the article on measurement uncertainty.

Practical realisation of the metre is subject to uncertainties in characterising the medium, to various uncertainties of interferometry, and to uncertainties in measuring the frequency of the source. A commonly used medium is air, and the National Institute of Standards and Technology (NIST) has set up an online calculator to convert wavelengths in vacuum to wavelengths in air. As described by NIST, in air, the uncertainties in characterising the medium are dominated by errors in measuring temperature and pressure. Errors in the theoretical formulas used are secondary. By implementing a refractive index correction such as this, an approximate realisation of the metre can be implemented in air, for example, using the formulation of the metre as wavelengths of helium–neon laser light in vacuum, and converting the wavelengths in a vacuum to wavelengths in air. Air is only one possible medium to use in a realisation of the metre, and any partial vacuum can be used, or some inert atmosphere like helium gas, provided the appropriate corrections for refractive index are implemented.

The metre is "defined" as the path length travelled by light in a given time and practical laboratory length measurements in metres are determined by counting the number of wavelengths of laser light of one of the standard types that fit into the length, and converting the selected unit of wavelength to metres. Three major factors limit the accuracy attainable with laser interferometers for a length measurement:
Of these, the last is peculiar to the interferometer itself. The conversion of a length in wavelengths to a length in metres is based upon the relation

which converts the unit of wavelength to metres using "c", the speed of light in vacuum in m/s. Here "n" is the refractive index of the medium in which the measurement is made, and "f" is the measured frequency of the source. Although conversion from wavelengths to metres introduces an additional error in the overall length due to measurement error in determining the refractive index and the frequency, the measurement of frequency is one of the most accurate measurements available.


SI prefixes are often employed to denote decimal multiples and submultiples of the metre, as shown in the table below. As indicated in the table, some are commonly used, while others are not. Long distances are usually expressed in km, astronomical units (149.6 Gm), light-years (10 Pm), or parsecs (31 Pm), rather than in Mm, Gm, Tm, Pm, Em, Zm or Ym; "30 cm", "30 m", and "300 m" are more common than "3 dm", "3 dam", and "3 hm", respectively.

The terms "micron" and (occasionally) "millimicron" are often used instead of "micrometre" (μm) and "nanometre" (nm), but this practice is officially discouraged.

Within this table, "inch" and "yard" mean "international inch" and "international yard" respectively, though approximate conversions in the left column hold for both international and survey units.

One metre is exactly equivalent to inches and to yards.
A simple mnemonic aid exists to assist with conversion, as three "3"s:

The ancient Egyptian cubit was about 0.5m (surviving rods are 523–529mm). Scottish and English definitions of the ell (two cubits) were 941mm (0.941m) and 1143mm (1.143m) respectively. The ancient Parisian "toise" (fathom) was slightly shorter than 2m and was standardised at exactly 2m in the mesures usuelles system, such that 1m was exactly toise. The Russian verst was 1.0668km. The Swedish mil was 10.688km, but was changed to 10km when Sweden converted to metric units.




</doc>
<doc id="18948" url="https://en.wikipedia.org/wiki?curid=18948" title="Mole">
Mole

Mole may refer to:













</doc>
<doc id="18955" url="https://en.wikipedia.org/wiki?curid=18955" title="Mentha">
Mentha

Mentha (also known as mint, from Greek , Linear B "mi-ta") is a genus of plants in the family Lamiaceae (mint family). It is estimated that 13 to 18 species exist, and the exact distinction between species is still unclear. Hybridization between some of the species occurs naturally. Many other hybrids, as well as numerous cultivars, are known.

The genus has a subcosmopolitan distribution across Europe, Africa, Asia, Australia, and North America.

Mints are aromatic, almost exclusively perennial, rarely annual herbs. They have wide-spreading underground and overground stolons and erect, square, branched stems. The leaves are arranged in opposite pairs, from oblong to lanceolate, often downy, and with a serrated margin. Leaf colors range from dark green and gray-green to purple, blue, and sometimes pale yellow. The flowers are white to purple and produced in false whorls called verticillasters. The corolla is two-lipped with four subequal lobes, the upper lobe usually the largest. The fruit is a nutlet, containing one to four seeds.

While the species that make up the genus "Mentha" are widely distributed and can be found in many environments, most grow best in wet environments and moist soils. Mints will grow 10–120 cm tall and can spread over an indeterminate area. Due to their tendency to spread unchecked, some mints are considered invasive.

The list below includes all of the taxa recognized as species in recent works on "Mentha". No author has recognized all of them. As with all biological classifications of plants, this list can go out of date at a moment's notice. Common names are also given for species that have them. Synonyms, along with cultivars and varieties, are given in articles on the species.

"Mentha" is a member of the tribe Mentheae in the subfamily Nepetoideae. The tribe contains about 65 genera, and relationships within it remain obscure. Authors have disagreed on the circumscription of "Mentha". Some authors have excluded "M. cervina" from the genus. "M. cunninghamii" has also been excluded by some authors, even in some recent treatments of the genus. In 2004, a molecular phylogenetic study indicated both of these species should be included in "Mentha".

The mint genus has a large grouping of recognized hybrids. Synonyms, along with cultivars and varieties where available, are included within the specific species.

All mints thrive near pools of water, lakes, rivers, and cool moist spots in partial shade. In general, mints tolerate a wide range of conditions, and can also be grown in full sun. Mint grows all year round.

They are fast-growing, extending their reach along surfaces through a network of runners. Due to their speedy growth, one plant of each desired mint, along with a little care, will provide more than enough mint for home use. Some mint species are more invasive than others. Even with the less invasive mints, care should be taken when mixing any mint with any other plants, lest the mint take over. To control mints in an open environment, they should be planted in deep, bottomless containers sunk in the ground, or planted above ground in tubs and barrels.

Some mints can be propagated by seed, but growth from seed can be an unreliable method for raising mint for two reasons: mint seeds are highly variable — one might not end up with what one supposed was planted — and some mint varieties are sterile. It is more effective to take and plant cuttings from the runners of healthy mints.

The most common and popular mints for commercial cultivation are peppermint ("Mentha × piperita"), native spearmint ("Mentha spicata"), Scotch spearmint ("Mentha x gracilis"), and cornmint ("Mentha arvensis"); also (more recently) apple mint ("Mentha suaveolens").

Mints are supposed to make good companion plants, repelling pesty insects and attracting beneficial ones. They are susceptible to whitefly and aphids.

Harvesting of mint leaves can be done at any time. Fresh leaves should be used immediately or stored up to a few days in plastic bags in a refrigerator. Optionally, leaves can be frozen in ice cube trays. Dried mint leaves should be stored in an airtight container placed in a cool, dark, dry area.

The leaf, fresh or dried, is the culinary source of mint. Fresh mint is usually preferred over dried mint when storage of the mint is not a problem. The leaves have a warm, fresh, aromatic, sweet flavor with a cool aftertaste, and are used in teas, beverages, jellies, syrups, candies, and ice creams. In Middle Eastern cuisine, mint is used on lamb dishes, while in British cuisine and American cuisine, mint sauce and mint jelly are used, respectively.

Mint is a necessary ingredient in Touareg tea, a popular tea in northern African and Arab countries. Alcoholic drinks sometimes feature mint for flavor or garnish, such as the mint julep and the mojito. "Crème de menthe" is a mint-flavored liqueur used in drinks such as the grasshopper.

Mint essential oil and menthol are extensively used as flavorings in breath fresheners, drinks, antiseptic mouth rinses, toothpaste, chewing gum, desserts, and candies, such as mint (candy) and mint chocolate. The substances that give the mints their characteristic aromas and flavors are menthol (the main aroma of peppermint and Japanese peppermint) and pulegone (in pennyroyal and Corsican mint). The compound primarily responsible for the aroma and flavor of spearmint is "L"-carvone.

Mints are used as food plants by the larvae of some Lepidoptera species, including buff ermine moths.

Mint was originally used as a medicinal herb to treat stomach ache and chest pains. There are several uses in traditional medicine and preliminary research for possible use in treating irritable bowel syndrome.

Menthol from mint essential oil (40–90%) is an ingredient of many cosmetics and some perfumes. Menthol and mint essential oil are also used in aromatherapy which may have clinical use to alleviate post-surgery nausea.

Although it is used in many consumer products, mint may cause allergic reactions in some people, inducing symptoms such as abdominal cramps, diarrhea, headaches, heartburn, tingling or numbing around the mouth, anaphylaxis or contact dermatitis.

Mint oil is also used as an environmentally friendly insecticide for its ability to kill some common pests such as wasps, hornets, ants, and cockroaches.

Known in Greek mythology as the herb of hospitality, one of mint's first known uses in Europe was as a room deodorizer. The herb was strewn across floors to cover the smell of the hard-packed soil. Stepping on the mint helped to spread its scent through the room. Today, it is more commonly used for aromatherapy through the use of essential oils.

Mint descends from the Latin word "mentha", which is rooted in the Greek word "minthe", personified in Greek mythology as Minthe, a nymph who was transformed into a mint plant. The word itself probably derives from a now extinct pre-Greek language (see Pre-Greek substrate).

Mint leaves, without a qualifier like 'peppermint' or 'apple mint', generally refers to spearmint leaves.

In Spain and Central and South America, mint is known as "menta". In Lusophone countries, especially in Portugal, mint species are popularly known as "". In many Indo-Aryan languages, it is called "pudīna", (), Hindi: पुदीना 

The taxonomic family Lamiaceae is known as the mint family. It includes many other aromatic herbs, including most of the more common cooking herbs, such as basil, rosemary, sage, oregano, and catnip.

As an English colloquial term, any small mint-flavored confectionery item can be called a mint.

In common usage, other plants with fragrant leaves may be called "mint", although they are not in the mint family.

†Mentha pliocenica fossil seeds have been excavated in Pliocene deposits of Dvorets on the right bank of the Dnieper river between the cities of Rechitsa and Loev, in south-eastern Belarus. The fossil seeds are similar to the seeds of "Mentha aquatica" and "Mentha arvensis".



</doc>
<doc id="18956" url="https://en.wikipedia.org/wiki?curid=18956" title="Marjoram">
Marjoram

Marjoram (; Origanum majorana) is a somewhat cold-sensitive perennial herb or undershrub with sweet pine and citrus flavors. In some Middle Eastern countries, marjoram is synonymous with oregano, and there the names sweet marjoram and knotted marjoram are used to distinguish it from other plants of the genus "Origanum". It is also called pot marjoram, although this name is also used for other cultivated species of "Origanum".

The name marjoram (Old French "majorane", Medieval Latin "majorana") does not directly derive from the Latin word "maior" (major). Marjoram is indigenous to Cyprus and southern Turkey, and was known to the Greeks and Romans as a symbol of happiness.

Leaves are smooth, simple, petiolated, ovate to oblong-ovate, long, wide, with obtuse apex, entire margin, symmetrical but tapering base, and reticulate venation. The texture is extremely smooth due to the presence of numerous hairs.

Considered a tender perennial (USDA Zones 7–9), marjoram can sometimes prove hardy even in zone 5.

Marjoram is cultivated for its aromatic leaves, either green or dry, for culinary purposes; the tops are cut as the plants begin to flower and are dried slowly in the shade. It is often used in herb combinations such as "herbes de Provence" and "za'atar". The flowering leaves and tops of marjoram are steam-distilled to produce an essential oil that is yellowish in color (darkening to brown as it ages). It has many chemical components, some of which are borneol, camphor, and pinene.
Oregano ("Origanum vulgare"), sometimes listed with marjoram as "O. majorana") is also called wild marjoram. It is a perennial common in southern Europe and north to Sweden in dry copses and on hedge-banks, with many stout stems high, bearing short-stalked, somewhat ovate leaves and clusters of purple flowers. It has a stronger flavor than marjoram.

Pot marjoram or Cretan oregano ("O. onites") has similar uses to marjoram.

Hardy marjoram or French marjoram, a cross of marjoram with oregano, is much more resistant to cold, but is slightly less sweet. "O. pulchellum" is known as showy marjoram or showy oregano.
Marjoram is used for seasoning soups, stews, dressings, and sauces.



</doc>
<doc id="18957" url="https://en.wikipedia.org/wiki?curid=18957" title="Medicine">
Medicine

Medicine is the science and practice of the diagnosis, treatment, and prevention of disease. Medicine encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Contemporary medicine applies biomedical sciences, biomedical research, genetics, and medical technology to diagnose, treat, and prevent injury and disease, typically through pharmaceuticals or surgery, but also through therapies as diverse as psychotherapy, external splints and traction, medical devices, biologics, and ionizing radiation, amongst others.

Medicine has existed for thousands of years, during most of which it was an art (an area of skill and knowledge) frequently having connections to the religious and philosophical beliefs of local culture. For example, a medicine man would apply herbs and say prayers for healing, or an ancient philosopher and physician would apply bloodletting according to the theories of humorism. In recent centuries, since the advent of modern science, most medicine has become a combination of art and science (both basic and applied, under the umbrella of medical science). While stitching technique for sutures is an art learned through practice, the knowledge of what happens at the cellular and molecular level in the tissues being stitched arises through science.

Prescientific forms of medicine are now known as traditional medicine and folk medicine. They remain commonly used with or instead of scientific medicine and are thus called alternative medicine. For example, evidence on the effectiveness of acupuncture is "variable and inconsistent" for any condition, but is generally safe when done by an appropriately trained practitioner. In contrast, treatments outside the bounds of safety and efficacy are termed quackery.

Medicine (, ) is the science and practice of the diagnosis, treatment, and prevention of disease. The word "medicine" is derived from Latin "medicus", meaning "a physician".

Medical availability and clinical practice varies across the world due to regional differences in culture and technology. Modern scientific medicine is highly developed in the Western world, while in developing countries such as parts of Africa or Asia, the population may rely more heavily on traditional medicine with limited evidence and efficacy and no required formal training for practitioners. Even in the developed world however, evidence-based medicine is not universally used in clinical practice; for example, a 2007 survey of literature reviews found that about 49% of the interventions lacked sufficient evidence to support either benefit or harm.

In modern clinical practice, physicians personally assess patients in order to diagnose, treat, and prevent disease using clinical judgment. The doctor-patient relationship typically begins an interaction with an examination of the patient's medical history and medical record, followed by a medical interview and a physical examination. Basic diagnostic medical devices (e.g. stethoscope, tongue depressor) are typically used. After examination for signs and interviewing for symptoms, the doctor may order medical tests (e.g. blood tests), take a biopsy, or prescribe pharmaceutical drugs or other therapies. Differential diagnosis methods help to rule out conditions based on the information provided. During the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust. The medical encounter is then documented in the medical record, which is a legal document in many jurisdictions. Follow-ups may be shorter but follow the same general procedure, and specialists follow a similar process. The diagnosis and treatment may take only a few minutes or a few weeks depending upon the complexity of the issue.

The components of the medical interview and encounter are:

The physical examination is the examination of the patient for medical signs of disease, which are objective and observable, in contrast to symptoms which are volunteered by the patient and not necessarily objectively observable. The healthcare provider uses the senses of sight, hearing, touch, and sometimes smell (e.g., in infection, uremia, diabetic ketoacidosis). Four actions are the basis of physical examination: inspection, palpation (feel), percussion (tap to determine resonance characteristics), and auscultation (listen), generally in that order although auscultation occurs prior to percussion and palpation for abdominal assessments.

The clinical examination involves the study of:

It is to likely focus on areas of interest highlighted in the medical history and may not include everything listed above.

The treatment plan may include ordering additional medical laboratory tests and medical imaging studies, starting therapy, referral to a specialist, or watchful observation. Follow-up may be advised. Depending upon the health insurance plan and the managed care system, various forms of "utilization review", such as prior authorization of tests, may place barriers on accessing expensive services.

The medical decision-making (MDM) process involves analysis and synthesis of all the above data to come up with a list of possible diagnoses (the differential diagnoses), along with an idea of what needs to be done to obtain a definitive diagnosis that would explain the patient's problem.

On subsequent visits, the process may be repeated in an abbreviated manner to obtain any new history, symptoms, physical findings, and lab or imaging results or specialist consultations.

Contemporary medicine is in general conducted within health care systems. Legal, credentialing and financing frameworks are established by individual governments, augmented on occasion by international organizations, such as churches. The characteristics of any given health care system have significant impact on the way medical care is provided.

From ancient times, Christian emphasis on practical charity gave rise to the development of systematic nursing and hospitals and the Catholic Church today remains the largest non-government provider of medical services in the world. Advanced industrial countries (with the exception of the United States) and many developing countries provide medical services through a system of universal health care that aims to guarantee care for all through a single-payer health care system, or compulsory private or co-operative health insurance. This is intended to ensure that the entire population has access to medical care on the basis of need rather than ability to pay. Delivery may be via private medical practices or by state-owned hospitals and clinics, or by charities, most commonly by a combination of all three.

Most tribal societies provide no guarantee of healthcare for the population as a whole. In such societies, healthcare is available to those that can afford to pay for it or have self-insured it (either directly or as part of an employment contract) or who may be covered by care financed by the government or tribe directly.

Transparency of information is another factor defining a delivery system. Access to information on conditions, treatments, quality, and pricing greatly affects the choice by patients/consumers and, therefore, the incentives of medical professionals. While the US healthcare system has come under fire for lack of openness, new legislation may encourage greater openness. There is a perceived tension between the need for transparency on the one hand and such issues as patient confidentiality and the possible exploitation of information for commercial gain on the other.

Provision of medical care is classified into primary, secondary, and tertiary care categories.
Primary care medical services are provided by physicians, physician assistants, nurse practitioners, or other health professionals who have first contact with a patient seeking medical treatment or care. These occur in physician offices, clinics, nursing homes, schools, home visits, and other places close to patients. About 90% of medical visits can be treated by the primary care provider. These include treatment of acute and chronic illnesses, preventive care and health education for all ages and both sexes.

Secondary care medical services are provided by medical specialists in their offices or clinics or at local community hospitals for a patient referred by a primary care provider who first diagnosed or treated the patient. Referrals are made for those patients who required the expertise or procedures performed by specialists. These include both ambulatory care and inpatient services, Emergency departments, intensive care medicine, surgery services, physical therapy, labor and delivery, endoscopy units, diagnostic laboratory and medical imaging services, hospice centers, etc. Some primary care providers may also take care of hospitalized patients and deliver babies in a secondary care setting.

Tertiary care medical services are provided by specialist hospitals or regional centers equipped with diagnostic and treatment facilities not generally available at local hospitals. These include trauma centers, burn treatment centers, advanced neonatology unit services, organ transplants, high-risk pregnancy, radiation oncology, etc.

Modern medical care also depends on information – still delivered in many health care settings on paper records, but increasingly nowadays by electronic means.

In low-income countries, modern healthcare is often too expensive for the average person. International healthcare policy researchers have advocated that "user fees" be removed in these areas to ensure access, although even after removal, significant costs and barriers remain.

Separation of prescribing and dispensing is a practice in medicine and pharmacy in which the physician who provides a medical prescription is independent from the pharmacist who provides the prescription drug. In the Western world there are centuries of tradition for separating pharmacists from physicians. In Asian countries it is traditional for physicians to also provide drugs.

Working together as an interdisciplinary team, many highly trained health professionals besides medical practitioners are involved in the delivery of modern health care. Examples include: nurses, emergency medical technicians and paramedics, laboratory scientists, pharmacists, podiatrists, physiotherapists, respiratory therapists, speech therapists, occupational therapists, radiographers, dietitians, and bioengineers, surgeons, surgeon's assistant, surgical technologist.

The scope and sciences underpinning human medicine overlap many other fields. Dentistry, while considered by some a separate discipline from medicine, is a medical field.

A patient admitted to the hospital is usually under the care of a specific team based on their main presenting problem, e.g., the cardiology team, who then may interact with other specialties, e.g., surgical, radiology, to help diagnose or treat the main problem or any subsequent complications/developments.

Physicians have many specializations and subspecializations into certain branches of medicine, which are listed below. There are variations from country to country regarding which specialties certain subspecialties are in.

The main branches of medicine are:



In the broadest meaning of "medicine", there are many different specialties. In the UK, most specialities have their own body or college, which have its own entrance examination. These are collectively known as the Royal Colleges, although not all currently use the term "Royal". The development of a speciality is often driven by new technology (such as the development of effective anaesthetics) or ways of working (such as emergency departments); the new specialty leads to the formation of a unifying body of doctors and the prestige of administering their own examination.

Within medical circles, specialities usually fit into one of two broad categories: "Medicine" and "Surgery." "Medicine" refers to the practice of non-operative medicine, and most of its subspecialties require preliminary training in Internal Medicine. In the UK, this was traditionally evidenced by passing the examination for the Membership of the Royal College of Physicians (MRCP) or the equivalent college in Scotland or Ireland. "Surgery" refers to the practice of operative medicine, and most subspecialties in this area require preliminary training in General Surgery, which in the UK leads to membership of the Royal College of Surgeons of England (MRCS). At present, some specialties of medicine do not fit easily into either of these categories, such as radiology, pathology, or anesthesia. Most of these have branched from one or other of the two camps above; for example anaesthesia developed first as a faculty of the Royal College of Surgeons (for which MRCS/FRCS would have been required) before becoming the Royal College of Anaesthetists and membership of the college is attained by sitting for the examination of the Fellowship of the Royal College of Anesthetists (FRCA).

Surgery is an ancient medical specialty that uses operative manual and instrumental techniques on a patient to investigate or treat a pathological condition such as disease or injury, to help improve bodily function or appearance or to repair unwanted ruptured areas (for example, a perforated ear drum). Surgeons must also manage pre-operative, post-operative, and potential surgical candidates on the hospital wards. Surgery has many sub-specialties, including "general surgery, ophthalmic surgery, cardiovascular surgery, colorectal surgery, neurosurgery, oral and maxillofacial surgery, oncologic surgery, orthopedic surgery, otolaryngology, plastic surgery, podiatric surgery, transplant surgery, trauma surgery, urology, vascular surgery, and pediatric surgery." In some centers, anesthesiology is part of the division of surgery (for historical and logistical reasons), although it is not a surgical discipline. Other medical specialties may employ surgical procedures, such as ophthalmology and dermatology, but are not considered surgical sub-specialties per se.

Surgical training in the U.S. requires a minimum of five years of residency after medical school. Sub-specialties of surgery often require seven or more years. In addition, fellowships can last an additional one to three years. Because post-residency fellowships can be competitive, many trainees devote two additional years to research. Thus in some cases surgical training will not finish until more than a decade after medical school. Furthermore, surgical training can be very difficult and time-consuming.

Internal medicine is the medical specialty dealing with the prevention, diagnosis, and treatment of adult diseases. According to some sources, an emphasis on internal structures is implied. In North America, specialists in internal medicine are commonly called "internists." Elsewhere, especially in Commonwealth nations, such specialists are often called physicians. These terms, "internist" or "physician" (in the narrow sense, common outside North America), generally exclude practitioners of gynecology and obstetrics, pathology, psychiatry, and especially surgery and its subspecialities.

Because their patients are often seriously ill or require complex investigations, internists do much of their work in hospitals. Formerly, many internists were not subspecialized; such "general physicians" would see any complex nonsurgical problem; this style of practice has become much less common. In modern urban practice, most internists are subspecialists: that is, they generally limit their medical practice to problems of one organ system or to one particular area of medical knowledge. For example, gastroenterologists and nephrologists specialize respectively in diseases of the gut and the kidneys.

In the Commonwealth of Nations and some other countries, specialist pediatricians and geriatricians are also described as "specialist physicians" (or internists) who have subspecialized by age of patient rather than by organ system. Elsewhere, especially in North America, general pediatrics is often a form of primary care.

There are many subspecialities (or subdisciplines) of internal medicine:

Training in internal medicine (as opposed to surgical training), varies considerably across the world: see the articles on "medical education" and "physician" for more details. In North America, it requires at least three years of residency training after medical school, which can then be followed by a one- to three-year fellowship in the subspecialties listed above. In general, resident work hours in medicine are less than those in surgery, averaging about 60 hours per week in the US. This difference does not apply in the UK where all doctors are now required by law to work less than 48 hours per week on average.


The followings are some major medical specialties that do not directly fit into any of the above-mentioned groups:


Some interdisciplinary sub-specialties of medicine include:

Medical education and training varies around the world. It typically involves entry level education at a university medical school, followed by a period of supervised practice or internship, or residency. This can be followed by postgraduate vocational training. A variety of teaching methods have been employed in medical education, still itself a focus of active research. In Canada and the United States of America, a Doctor of Medicine degree, often abbreviated M.D., or a Doctor of Osteopathic Medicine degree, often abbreviated as D.O. and unique to the United States, must be completed in and delivered from a recognized university.

Since knowledge, techniques, and medical technology continue to evolve at a rapid rate, many regulatory authorities require continuing medical education. Medical practitioners upgrade their knowledge in various ways, including medical journals, seminars, conferences, and online programs.
In most countries, it is a legal requirement for a medical doctor to be licensed or registered. In general, this entails a medical degree from a university and accreditation by a medical board or an equivalent national organization, which may ask the applicant to pass exams. This restricts the considerable legal authority of the medical profession to physicians that are trained and qualified by national standards. It is also intended as an assurance to patients and as a safeguard against charlatans that practice inadequate medicine for personal gain. While the laws generally require medical doctors to be trained in "evidence based", Western, or Hippocratic Medicine, they are not intended to discourage different paradigms of health.

In the European Union, the profession of doctor of medicine is regulated. A profession is said to be regulated when access and exercise is subject to the possession of a specific professional qualification.
The regulated professions database contains a list of regulated professions for doctor of medicine in the EU member states, EEA countries and Switzerland. This list is covered by the Directive 2005/36/EC.

Doctors who are negligent or intentionally harmful in their care of patients can face charges of medical malpractice and be subject to civil, criminal, or professional sanctions.

Medical ethics is a system of moral principles that apply values and judgments to the practice of medicine. As a scholarly discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology. Six of the values that commonly apply to medical ethics discussions are:

Values such as these do not give answers as to how to handle a particular situation, but provide a useful framework for understanding conflicts. When moral values are in conflict, the result may be an ethical dilemma or crisis. Sometimes, no good solution to a dilemma in medical ethics exists, and occasionally, the values of the medical community (i.e., the hospital and its staff) conflict with the values of the individual patient, family, or larger non-medical community. Conflicts can also arise between health care providers, or among family members. For example, some argue that the principles of autonomy and beneficence clash when patients refuse blood transfusions, considering them life-saving; and truth-telling was not emphasized to a large extent before the HIV era.

Prehistoric medicine incorporated plants (herbalism), animal parts, and minerals. In many cases these materials were used ritually as magical substances by priests, shamans, or medicine men. Well-known spiritual systems include animism (the notion of inanimate objects having spirits), spiritualism (an appeal to gods or communion with ancestor spirits); shamanism (the vesting of an individual with mystic powers); and divination (magically obtaining the truth). The field of medical anthropology examines the ways in which culture and society are organized around or impacted by issues of health, health care and related issues.

Early records on medicine have been discovered from ancient Egyptian medicine, Babylonian Medicine, Ayurvedic medicine (in the Indian subcontinent), classical Chinese medicine (predecessor to the modern traditional Chinese medicine), and ancient Greek medicine and Roman medicine.

In Egypt, Imhotep (3rd millennium BCE) is the first physician in history known by name. The oldest Egyptian medical text is the "Kahun Gynaecological Papyrus" from around 2000 BCE, which describes gynaecological diseases. The "Edwin Smith Papyrus" dating back to 1600 BCE is an early work on surgery, while the "Ebers Papyrus" dating back to 1500 BCE is akin to a textbook on medicine.

In China, archaeological evidence of medicine in Chinese dates back to the Bronze Age Shang Dynasty, based on seeds for herbalism and tools presumed to have been used for surgery. The "Huangdi Neijing", the progenitor of Chinese medicine, is a medical text written beginning in the 2nd century BCE and compiled in the 3rd century.

In India, the surgeon Sushruta described numerous surgical operations, including the earliest forms of plastic surgery. Earliest records of dedicated hospitals come from Mihintale in Sri Lanka where evidence of dedicated medicinal treatment facilities for patients are found.

In Greece, the Greek physician Hippocrates, the "father of modern medicine", laid the foundation for a rational approach to medicine. Hippocrates introduced the Hippocratic Oath for physicians, which is still relevant and in use today, and was the first to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, "exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence". The Greek physician Galen was also one of the greatest surgeons of the ancient world and performed many audacious operations, including brain and eye surgeries. After the fall of the Western Roman Empire and the onset of the Early Middle Ages, the Greek tradition of medicine went into decline in Western Europe, although it continued uninterrupted in the Eastern Roman (Byzantine) Empire.

Most of our knowledge of ancient Hebrew medicine during the 1st millennium BC comes from the Torah, i.e. the Five Books of Moses, which contain various health related laws and rituals. The Hebrew contribution to the development of modern medicine started in the Byzantine Era, with the physician Asaph the Jew.

The concept of hospital as institution to offer medical care and possibility of a cure for the patients due to the ideals of Christian charity, rather than just merely a place to die, appeared in the Byzantine Empire.

Although the concept of uroscopy was known to Galen, he did not see the importance of using it to localize the disease. It was under the Byzantines with physicians such of Theophilus Protospatharius that they realized the potential in uroscopy to determine disease in a time when no microscope or stethoscope existed. That practice eventually spread to the rest of Europe.

After 750 CE, the Muslim world had the works of Hippocrates, Galen and Sushruta translated into Arabic, and Islamic physicians engaged in some significant medical research. Notable Islamic medical pioneers include the Persian polymath, Avicenna, who, along with Imhotep and Hippocrates, has also been called the "father of medicine". He wrote "The Canon of Medicine", considered one of the most famous books in the history of medicine. Others include Abulcasis, Avenzoar, Ibn al-Nafis, and Averroes. Rhazes was one of the first to question the Greek theory of humorism, which nevertheless remained influential in both medieval Western and medieval Islamic medicine. "Al-Risalah al-Dhahabiah" by Ali al-Ridha, the eighth Imam of Shia Muslims, is revered as the most precious Islamic literature in the Science of Medicine. The Persian Bimaristan hospitals were an early example of public hospitals.

In Europe, Charlemagne decreed that a hospital should be attached to each cathedral and monastery and the historian Geoffrey Blainey likened the activities of the Catholic Church in health care during the Middle Ages to an early version of a welfare state: "It conducted hospitals for the old and orphanages for the young; hospices for the sick of all ages; places for the lepers; and hostels or inns where pilgrims could buy a cheap bed and meal". It supplied food to the population during famine and distributed food to the poor. This welfare system the church funded through collecting taxes on a large scale and possessing large farmlands and estates. The Benedictine order was noted for setting up hospitals and infirmaries in their monasteries, growing medical herbs and becoming the chief medical care givers of their districts, as at the great Abbey of Cluny. The Church also established a network of cathedral schools and universities where medicine was studied. The Schola Medica Salernitana in Salerno, looking to the learning of Greek and Arab physicians, grew to be the finest medical school in Medieval Europe.

However, the fourteenth and fifteenth century Black Death devastated both the Middle East and Europe, and it has even been argued that Western Europe was generally more effective in recovering from the pandemic than the Middle East. In the early modern period, important early figures in medicine and anatomy emerged in Europe, including Gabriele Falloppio and William Harvey.

The major shift in medical thinking was the gradual rejection, especially during the Black Death in the 14th and 15th centuries, of what may be called the 'traditional authority' approach to science and medicine. This was the notion that because some prominent person in the past said something must be so, then that was the way it was, and anything one observed to the contrary was an anomaly (which was paralleled by a similar shift in European society in general – see Copernicus's rejection of Ptolemy's theories on astronomy). Physicians like Vesalius improved upon or disproved some of the theories from the past. The main tomes used both by medicine students and expert physicians were Materia Medica and Pharmacopoeia.

Andreas Vesalius was the author of "De humani corporis fabrica", an important book on human anatomy. Bacteria and microorganisms were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field microbiology. Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the "Manuscript of Paris" in 1546, and later published in the theological work for which he paid with his life in 1553. Later this was described by Renaldus Columbus and Andrea Cesalpino. Herman Boerhaave is sometimes referred to as a "father of physiology" due to his exemplary teaching in Leiden and textbook 'Institutiones medicae' (1708). Pierre Fauchard has been called "the father of modern dentistry".

Veterinary medicine was, for the first time, truly separated from human medicine in 1761, when the French veterinarian Claude Bourgelat founded the world's first veterinary school in Lyon, France. Before this, medical doctors treated both humans and other animals.

Modern scientific biomedical research (where results are testable and reproducible) began to replace early Western traditions based on herbalism, the Greek "four humours" and other such pre-modern notions. The modern era really began with Edward Jenner's discovery of the smallpox vaccine at the end of the 18th century (inspired by the method of inoculation earlier practiced in Asia), Robert Koch's discoveries around 1880 of the transmission of disease by bacteria, and then the discovery of antibiotics around 1900.

The post-18th century modernity period brought more groundbreaking researchers from Europe. From Germany and Austria, doctors Rudolf Virchow, Wilhelm Conrad Röntgen, Karl Landsteiner and Otto Loewi made notable contributions. In the United Kingdom, Alexander Fleming, Joseph Lister, Francis Crick and Florence Nightingale are considered important. Spanish doctor Santiago Ramón y Cajal is considered the father of modern neuroscience.

From New Zealand and Australia came Maurice Wilkins, Howard Florey, and Frank Macfarlane Burnet.

Others that did significant work include William Williams Keen, William Coley, James D. Watson (United States); Salvador Luria (Italy); Alexandre Yersin (Switzerland); Kitasato Shibasaburō (Japan); Jean-Martin Charcot, Claude Bernard, Paul Broca (France); Adolfo Lutz (Brazil); Nikolai Korotkov, Sir William Osler, and Harvey Cushing (Russia).
As science and technology developed, medicine became more reliant upon medications. Throughout history and in Europe right until the late 18th century, not only animal and plant products were used as medicine, but also human body parts and fluids. Pharmacology developed in part from herbalism and some drugs are still derived from plants (atropine, ephedrine, warfarin, aspirin, digoxin, "vinca" alkaloids, taxol, hyoscine, etc.). Vaccines were discovered by Edward Jenner and Louis Pasteur.

The first antibiotic was arsphenamine (Salvarsan) discovered by Paul Ehrlich in 1908 after he observed that bacteria took up toxic dyes that human cells did not. The first major class of antibiotics was the sulfa drugs, derived by German chemists originally from azo dyes.

Pharmacology has become increasingly sophisticated; modern biotechnology allows drugs targeted towards specific physiological processes to be developed, sometimes designed for compatibility with the body to reduce side-effects. Genomics and knowledge of human genetics and human evolution is having increasingly significant influence on medicine, as the causative genes of most monogenic genetic disorders have now been identified, and the development of techniques in molecular biology, evolution, and genetics are influencing medical technology, practice and decision-making.

Evidence-based medicine is a contemporary movement to establish the most effective algorithms of practice (ways of doing things) through the use of systematic reviews and meta-analysis. The movement is facilitated by modern global information science, which allows as much of the available evidence as possible to be collected and analyzed according to standard protocols that are then disseminated to healthcare providers. The Cochrane Collaboration leads this movement. A 2001 review of 160 Cochrane systematic reviews revealed that, according to two readers, 21.3% of the reviews concluded insufficient evidence, 20% concluded evidence of no effect, and 22.5% concluded positive effect.

Traditional medicine (also known as indigenous or folk medicine) comprises knowledge systems that developed over generations within various societies before the introduction of modern medicine. The World Health Organization (WHO) defines traditional medicine as "the sum total of the knowledge, skills, and practices based on the theories, beliefs, and experiences indigenous to different cultures, whether explicable or not, used in the maintenance of health as well as in the prevention, diagnosis, improvement or treatment of physical and mental illness."

In some Asian and African countries, up to 80% of the population relies on traditional medicine for their primary health care needs. When adopted outside of its traditional culture, traditional medicine is often called alternative medicine. Practices known as traditional medicines include Ayurveda, Siddha medicine, Unani, ancient Iranian medicine, Irani, Islamic medicine, traditional Chinese medicine, traditional Korean medicine, acupuncture, Muti, Ifá, and traditional African medicine.

The WHO notes however that "inappropriate use of traditional medicines or practices can have negative or dangerous effects" and that "further research is needed to ascertain the efficacy and safety" of several of the practices and medicinal plants used by traditional medicine systems. The line between alternative medicine and quackery is a contentious subject.

Traditional medicine may include formalized aspects of folk medicine, that is to say longstanding remedies passed on and practised by lay people. Folk medicine consists of the healing practices and ideas of body physiology and health preservation known to some in a culture, transmitted informally as general knowledge, and practiced or applied by anyone in the culture having prior experience. Folk medicine may also be referred to as traditional medicine, alternative medicine, indigenous medicine, or natural medicine. These terms are often considered interchangeable, even though some authors may prefer one or the other because of certain overtones they may be willing to highlight. In fact, out of these terms perhaps only "indigenous medicine" and "traditional medicine" have the same meaning as "folk medicine", while the others should be understood rather in a modern or modernized context.


</doc>
<doc id="18959" url="https://en.wikipedia.org/wiki?curid=18959" title="2001 Mars Odyssey">
2001 Mars Odyssey

2001 Mars Odyssey is a robotic spacecraft orbiting the planet Mars. The project was developed by NASA, and contracted out to Lockheed Martin, with an expected cost for the entire mission of US$297 million. Its mission is to use spectrometers and a thermal imager to detect evidence of past or present water and ice, as well as study the planet's geology and radiation environment. It is hoped that the data "Odyssey" obtains will help answer the question of whether life existed on Mars and create a risk-assessment of the radiation that future astronauts on Mars might experience. It also acts as a relay for communications between the Mars Exploration Rovers, Mars Science Laboratory, and previously the "Phoenix" lander to Earth. The mission was named as a tribute to Arthur C. Clarke, evoking the name of "".

"Odyssey" was launched April 7, 2001, on a Delta II rocket from Cape Canaveral Air Force Station, and reached Mars orbit on October 24, 2001, at 02:30 UTC (October 23, 19:30 PDT, 22:30 EDT).

By December 15, 2010, it broke the record for longest serving spacecraft at Mars, with 3,340 days of operation. It is currently in a polar orbit around Mars with a semi-major axis of about 3,800 km or 2,400 miles. It has enough propellant to function until 2025.

On May 28, 2002 (sol ), NASA reported that "Odyssey"s GRS instrument had detected large amounts of hydrogen, a sign that there must be ice lying within a meter of the planet's surface, and proceeded to map the distribution of water below the shallow surface. The orbiter also discovered vast deposits of bulk water ice near the surface of equatorial regions.

"Odyssey" has also served as the primary means of communications for NASA's Mars surface explorers in the past decade, up to the "Curiosity" rover. By December 15, 2010, it broke the record for longest serving spacecraft at Mars, with 3,340 days of operation, claiming the title from NASA's "Mars Global Surveyor". It currently holds the record for the longest-surviving continually active spacecraft in orbit around a planet other than Earth, ahead of the Pioneer Venus Orbiter (served 14 years) and the Mars Express (serving over 14 years), at .

"Mars Odyssey" was originally a component of the "Mars Surveyor 2001" program, and was named the Mars Surveyor 2001 Orbiter. It was intended to have a companion spacecraft known as Mars Surveyor 2001 Lander, but the lander mission was canceled in May 2000 following the failures of Mars Climate Orbiter and Mars Polar Lander in late 1999. Subsequently, the name "2001 Mars Odyssey" was selected for the orbiter as a specific tribute to the vision of space exploration shown in works by Arthur C. Clarke, including "". The music from Mythodea by Greek composer Vangelis was used as the theme music for the mission.
In August 2000, NASA solicited candidate names for the mission. Out of 200 names submitted, the committee chose Astrobiological Reconnaissance and Elemental Surveyor, abbreviated ARES (a tribute to Ares, the Greek god of war). Faced with criticism that this name was not very compelling, and too aggressive, the naming committee reconvened. The candidate name "2001 Mars Odyssey" had earlier been rejected because of copyright and trademark concerns. However, NASA e-mailed Arthur C. Clarke in Sri Lanka, who responded that he would be delighted to have the mission named after his books, and he had no objections. On September 20, NASA associate administrator Ed Weiler wrote to the associate administrator for public affairs recommending a name change from ARES to "2001 Mars Odyssey". Peggy Wilhide then approved the name change.

The three primary instruments "Odyssey" uses are the:


"Mars Odyssey" launched from Cape Canaveral on April 7, 2001, and arrived at Mars about 200 days later on October 24. The spacecraft's main engine fired in order to decelerate, which allowed it to be captured into orbit around Mars. "Odyssey" then spent about three months aerobraking, using friction from the upper reaches of the Martian atmosphere to gradually slow down and reduce and circularize its orbit. By using the atmosphere of Mars to slow the spacecraft in its orbit rather than firing its engine or thrusters, "Odyssey" was able to save more than 200 kilograms (440 lb) of propellant. This reduction in spacecraft weight allowed the mission to be launched on a Delta II 7925 launch vehicle, rather than a larger, more expensive launcher. Aerobraking ended in January 2002, and Odyssey began its science mapping mission on February 19, 2002. "Odyssey"s original, nominal mission lasted until August 2004, but repeated mission extensions have kept the mission active.

About 85% of images and other data from NASA's twin Mars Exploration Rovers, "Spirit" and "Opportunity", have reached Earth via communications relay by "Odyssey". "Odyssey" continues to receive transmissions from the surviving rover, "Opportunity", every day. The orbiter helped analyze potential landing sites for the rovers and performed the same task for NASA's Phoenix mission, which landed on Mars in May 2008. "Odyssey" aided NASA's "Mars Reconnaissance Orbiter", which reached Mars in March 2006, by monitoring atmospheric conditions during months when the newly arrived orbiter used aerobraking to alter its orbit into the desired shape.

"Odyssey" is in a Sun-synchronous orbit, which provides consistent lighting for its photographs. On September 30, 2008 (sol ) the spacecraft altered its orbit to gain better sensitivity for its infrared mapping of Martian minerals. The new orbit eliminated the use of the gamma ray detector, due to the potential for overheating the instrument at the new orbit.

The payload's MARIE radiation experiment stopped taking measurements after a large solar event bombarded the "Odyssey" spacecraft on October 28, 2003. Engineers believe the most likely cause is that a computer chip was damaged by a solar particle smashing into the MARIE computer board.

One of the orbiter's three flywheels failed in June 2012. However, "Odyssey"s design included a fourth flywheel, a spare carried against exactly this eventuality. The spare was spun up and successfully brought into service. Since July 2012, "Odyssey" has been back in full, nominal operation mode following three weeks of 'safe' mode on remote maintenance.

On February 11, 2014, mission control accelerated "Odyssey"s drift toward a morning-daylight orbit to "enable observation of changing ground temperatures after sunrise and after sunset in thousands of places on Mars". The orbital change occurred gradually until November 2015. Those observations could yield insight about the composition of the ground and about temperature-driven processes, such as warm seasonal flows observed on some slopes, and geysers fed by spring thawing of carbon dioxide (CO ice near Mars' poles.

On October 19, 2014, NASA reported that the "Mars Odyssey" Orbiter, as well as the "Mars Reconnaissance Orbiter" and "MAVEN", were healthy after the Comet Siding Spring flyby.

In 2010, a spokesman for NASA's Jet Propulsion Laboratory stated that "Odyssey" could continue operating until at least 2016. This estimate has since been extended until 2025.

By 2008, "Mars Odyssey" had mapped the basic distribution of water below the shallow surface. The ground truth for its measurements came on July 31, 2008, when NASA announced that the Phoenix lander confirmed the presence of water on Mars, as predicted in 2002 based on data from the "Odyssey" orbiter. The science team is trying to determine whether the water ice ever thaws enough to be available for microscopic life, and if carbon-containing chemicals and other raw materials for life are present.

The orbiter also discovered vast deposits of bulk water ice near the surface of equatorial regions. Evidence for equatorial hydration is both morphological and compositional and is seen at both the Medusae Fossae formation and the Tharsis Montes.

"Mars Odyssey"s THEMIS instrument was used to help select a landing site for the Mars Science Laboratory (MSL). Several days before MSL's landing in August 2012, "Odyssey"s orbit was altered to ensure that it would be able to capture signals from the rover during its first few minutes on the Martian surface. "Odyssey" also acts as a relay for UHF radio signals from the (MSL) rover "Curiosity". Because "Odyssey" is in a Sun-synchronous orbit, it consistently passes over "Curiosity"s location at the same two times every day, allowing for convenient scheduling of contact with Earth.



</doc>
<doc id="18960" url="https://en.wikipedia.org/wiki?curid=18960" title="Macedonian phalanx">
Macedonian phalanx

The Macedonian phalanx is an infantry formation developed by Philip II and used by his son Alexander the Great to conquer the Achaemenid Empire and other armies. Phalanxes remained dominant on battlefields throughout the Hellenistic period, although wars had developed into more protracted operations generally involving sieges and naval combat as much as pitched battles, until they were ultimately displaced by the Roman legions.

Philip II spent much of his youth as a hostage at Thebes, where he studied under the renowned general Epaminondas, whose reforms were the basis for the phalanx. Phalangites were professional soldiers, and were among the first troops ever to be drilled, thereby allowing them to execute complex maneuvers well beyond the reach of most other armies. They fought packed in a close rectangular formation, usually eight men deep, with a leader at the head of each column and a secondary leader in the middle, so that the back rows could move off to the sides if more frontage was needed.

Each phalangite carried as his primary weapon a sarissa, a double-pointed pike over 6 m (18 ft) in length. Before a battle the sarissa were carried in two pieces and then slid together when they were being used. At close range such large weapons were of little use, but an intact phalanx could easily keep its enemies at a distance; the weapons of the first five rows of men all projected beyond the front of the formation, so that there were more spearpoints than available targets at any given time. Men in rows behind the initial five angled their spears at a 45 degree angle in an attempt to ward off arrows or other projectiles. The secondary weapon was a shortsword called a kopis, which had a heavy curved section at the end.

The phalanx consisted of a line-up of several battalion blocks called "syntagmata", each of its 16 files ("lochoi") numbering 16 men, for a total of 256 in each unit. Each "syntagma" was commanded by a "syntagmatarch", who - together with his subordinate officers - would form the first row of each block.

Neither Philip nor Alexander actually used the phalanx as their arm of choice, but instead used it to hold the enemy in place while their heavy cavalry broke through their ranks. The Macedonian cavalry fought in wedge formation and was stationed on the far right; after these broke through the enemy lines they were followed by the hypaspists, elite infantrymen who served as the king's bodyguard, and then the phalanx proper. The left flank was generally covered by allied cavalry supplied by the Thessalians, which fought in rhomboid formation and served mainly in a defensive role. Other forces—skirmishers, range troops, reserves of allied hoplites, archers, and artillery—were also employed.


</doc>
<doc id="18964" url="https://en.wikipedia.org/wiki?curid=18964" title="Madagascar">
Madagascar

Madagascar (; ), officially the Republic of Madagascar ( ; ), and previously known as the Malagasy Republic, is an island country in the Indian Ocean, off the coast of East Africa. The nation comprises the island of Madagascar (the fourth-largest island in the world), and numerous smaller peripheral islands. Following the prehistoric breakup of the supercontinent Gondwana, Madagascar split from the Indian peninsula around 88 million years ago, allowing native plants and animals to evolve in relative isolation. Consequently, Madagascar is a biodiversity hotspot; over 90% of its wildlife is found nowhere else on Earth. The island's diverse ecosystems and unique wildlife are threatened by the encroachment of the rapidly growing human population and other environmental threats.

The first archaeological evidence for human foraging on Madagascar dates to 2000 BC. Human settlement of Madagascar occurred between 350 BC and AD 550 by Austronesian peoples, arriving on outrigger canoes from Borneo. These were joined around AD 1000 by Bantu migrants crossing the Mozambique Channel from East Africa. Other groups continued to settle on Madagascar over time, each one making lasting contributions to Malagasy cultural life. The Malagasy ethnic group is often divided into 18 or more subgroups of which the largest are the Merina of the central highlands.

Until the late 18th century, the island of Madagascar was ruled by a fragmented assortment of shifting sociopolitical alliances. Beginning in the early 19th century, most of the island was united and ruled as the Kingdom of Madagascar by a series of Merina nobles. The monarchy ended in 1897 when the island was absorbed into the French colonial empire, from which the island gained independence in 1960. The autonomous state of Madagascar has since undergone four major constitutional periods, termed republics. Since 1992, the nation has officially been governed as a constitutional democracy from its capital at Antananarivo. However, in a popular uprising in 2009, president Marc Ravalomanana was made to resign and presidential power was transferred in March 2009 to Andry Rajoelina. Constitutional governance was restored in January 2014, when Hery Rajaonarimampianina was named president following a 2013 election deemed fair and transparent by the international community. Madagascar is a member of the United Nations, the African Union (AU), the Southern African Development Community (SADC), and the Organisation Internationale de la Francophonie.

Madagascar belongs to the group of least developed countries, according to the United Nations. Malagasy and French are both official languages of the state. The majority of the population adheres to traditional beliefs, Christianity, or an amalgamation of both. Ecotourism and agriculture, paired with greater investments in education, health, and private enterprise, are key elements of Madagascar's development strategy. Under Ravalomanana, these investments produced substantial economic growth, but the benefits were not evenly spread throughout the population, producing tensions over the increasing cost of living and declining living standards among the poor and some segments of the middle class. , the economy has been weakened by the 2009–2013 political crisis, and quality of life remains low for the majority of the Malagasy population.

In the Malagasy language, the island of Madagascar is called "Madagasikara" and its people are referred to as "Malagasy". The island's appellation "Madagascar" is not of local origin, but rather was popularized in the Middle Ages by Europeans. The name "Madageiscar" was first recorded in the memoirs of 13th-century Venetian explorer Marco Polo as a corrupted transliteration of the name Mogadishu, the Somali port with which Polo had confused the island.

On St. Laurence's Day in 1500, Portuguese explorer Diogo Dias landed on the island and named it "São Lourenço". Polo's name was preferred and popularized on Renaissance maps. No single Malagasy-language name predating "Madagasikara" appears to have been used by the local population to refer to the island, although some communities had their own name for part or all of the land they inhabited.

At , Madagascar is the world's 47th largest country and the fourth-largest island. The country lies mostly between latitudes 12°S and 26°S, and longitudes 43°E and 51°E. Neighboring islands include the French territory of Réunion and the country of Mauritius to the east, as well as the state of Comoros and the French territory of Mayotte to the north west. The nearest mainland state is Mozambique, located to the west.

The prehistoric breakup of the supercontinent Gondwana separated the Madagascar–Antarctica–India landmass from the Africa–South America landmass around 135 million years ago. Madagascar later split from India about 88 million years ago, allowing plants and animals on the island to evolve in relative isolation. Along the length of the eastern coast runs a narrow and steep escarpment containing much of the island's remaining tropical lowland forest.

To the west of this ridge lies a plateau in the center of the island ranging in altitude from above sea level. These central highlands, traditionally the homeland of the Merina people and the location of their historic capital at Antananarivo, are the most densely populated part of the island and are characterized by terraced, rice-growing valleys lying between grassy hills and patches of the subhumid forests that formerly covered the highland region. To the west of the highlands, the increasingly arid terrain gradually slopes down to the Mozambique Channel and mangrove swamps along the coast.

Madagascar's highest peaks rise from three prominent highland massifs: Maromokotro in the Tsaratanana Massif is the island's highest point, followed by Boby Peak in the Andringitra Massif, and Tsiafajavona in the Ankaratra Massif. To the east, the "Canal des Pangalanes" is a chain of man-made and natural lakes connected by canals built by the French just inland from the east coast and running parallel to it for some .

The western and southern sides, which lie in the rain shadow of the central highlands, are home to dry deciduous forests, spiny forests, and deserts and xeric shrublands. Due to their lower population densities, Madagascar's dry deciduous forests have been better preserved than the eastern rain forests or the original woodlands of the central plateau. The western coast features many protected harbors, but silting is a major problem caused by sediment from the high levels of inland erosion carried by rivers crossing the broad western plains.

The combination of southeastern trade winds and northwestern monsoons produces a hot rainy season (November–April) with frequently destructive cyclones, and a relatively cooler dry season (May–October). Rain clouds originating over the Indian Ocean discharge much of their moisture over the island's eastern coast; the heavy precipitation supports the area's rainforest ecosystem. The central highlands are both drier and cooler while the west is drier still, and a semi-arid climate prevails in the southwest and southern interior of the island.

Tropical cyclones annually cause damage to infrastructure and local economies as well as loss of life. In 2004 Cyclone Gafilo became the strongest cyclone ever recorded to hit Madagascar. The storm killed 172 people, left 214,260 homeless and caused more than US$250 million in damage.

As a result of the island's long isolation from neighboring continents, Madagascar is home to an abundance of plants and animals found nowhere else on Earth. Approximately 90% of all plant and animal species found in Madagascar are endemic, including the lemurs (a type of strepsirrhine primate), the carnivorous fossa and many birds. This distinctive ecology has led some ecologists to refer to Madagascar as the "eighth continent", and the island has been classified by Conservation International as a biodiversity hotspot.

More than 80 percent of Madagascar's 14,883 plant species are found nowhere else in the world, including five plant families. The family "Didiereaceae", composed of four genera and 11 species, is limited to the spiny forests of southwestern Madagascar. Four-fifths of the world's "Pachypodium" species are endemic to the island. Three-fourths of Madagascar's 860 orchid species are found here alone, as are six of the world's nine baobab species. The island is home to around 170 palm species, three times as many as on all of mainland Africa; 165 of them are endemic. Many native plant species are used as herbal remedies for a variety of afflictions. The drugs vinblastine and vincristine are "vinca" alkaloids, used to treat Hodgkin's disease, leukemia, and other cancers, were derived from the Madagascar periwinkle. The traveler's palm, known locally as "ravinala" and endemic to the eastern rain forests, is highly iconic of Madagascar and is featured in the national emblem as well as the Air Madagascar logo.

Like its flora, Madagascar's fauna is diverse and exhibits a high rate of endemism. Lemurs have been characterized as "Madagascar's flagship mammal species" by Conservation International. In the absence of monkeys and other competitors, these primates have adapted to a wide range of habitats and diversified into numerous species. , there were officially 103 species and subspecies of lemur, 39 of which were described by zoologists between 2000 and 2008. They are almost all classified as rare, vulnerable, or endangered. At least 17 species of lemur have become extinct since humans arrived on Madagascar, all of which were larger than the surviving lemur species.

A number of other mammals, including the cat-like fossa, are endemic to Madagascar. Over 300 species of birds have been recorded on the island, of which over 60 percent (including four families and 42 genera) are endemic. The few families and genera of reptile that have reached Madagascar have diversified into more than 260 species, with over 90 percent of these being endemic (including one endemic family). The island is home to two-thirds of the world's chameleon species, including the smallest known, and researchers have proposed that Madagascar may be the origin of all chameleons.

Endemic fish of Madagascar include two families, 15 genera and over 100 species, primarily inhabiting the island's freshwater lakes and rivers. Although invertebrates remain poorly studied on Madagascar, researchers have found high rates of endemism among the known species. All 651 species of terrestrial snail are endemic, as are a majority of the island's butterflies, scarab beetles, lacewings, spiders and dragonflies.

Madagascar's varied fauna and flora are endangered by human activity. Since the arrival of humans around 2,350 years ago, Madagascar has lost more than 90 percent of its original forest. This forest loss is largely fueled by "tavy" ("fat"), a traditional slash-and-burn agricultural practice imported to Madagascar by the earliest settlers. Malagasy farmers embrace and perpetuate the practice not only for its practical benefits as an agricultural technique, but for its cultural associations with prosperity, health and venerated ancestral custom ("fomba malagasy"). As human population density rose on the island, deforestation accelerated beginning around 1400 years ago. By the 16th century, the central highlands had been largely cleared of their original forests. More recent contributors to the loss of forest cover include the growth in cattle herd size since their introduction around 1000 years ago, a continued reliance on charcoal as a fuel for cooking, and the increased prominence of coffee as a cash crop over the past century. According to a conservative estimate, about 40 percent of the island's original forest cover was lost from the 1950s to 2000, with a thinning of remaining forest areas by 80 percent. In addition to traditional agricultural practice, wildlife conservation is challenged by the illicit harvesting of protected forests, as well as the state-sanctioned harvesting of precious woods within national parks. Although banned by then-President Marc Ravalomanana from 2000 to 2009, the collection of small quantities of precious timber from national parks was re-authorized in January 2009 and dramatically intensified under the administration of Andry Rajoelina as a key source of state revenues to offset cuts in donor support following Ravalomanana's ousting.

It is anticipated that all the island's rainforests, excluding those in protected areas and the steepest eastern mountain slopes, will have been deforested by 2025. Invasive species have likewise been introduced by human populations. Following the 2014 discovery in Madagascar of the Asian common toad, a relative of a toad species that has severely harmed wildlife in Australia since the 1930s, researchers warned the toad could "wreak havoc on the country's unique fauna." Habitat destruction and hunting have threatened many of Madagascar's endemic species or driven them to extinction. The island's elephant birds, a family of endemic giant ratites, became extinct in the 17th century or earlier, most probably due to human hunting of adult birds and poaching of their large eggs for food. Numerous giant lemur species vanished with the arrival of human settlers to the island, while others became extinct over the course of the centuries as a growing human population put greater pressures on lemur habitats and, among some populations, increased the rate of lemur hunting for food. A July 2012 assessment found that the exploitation of natural resources since 2009 has had dire consequences for the island's wildlife: 90 percent of lemur species were found to be threatened with extinction, the highest proportion of any mammalian group. Of these, 23 species were classified as critically endangered. By contrast, a previous study in 2008 had found only 38 percent of lemur species were at risk of extinction.

In 2003 Ravalomanana announced the Durban Vision, an initiative to more than triple the island's protected natural areas to over or 10 percent of Madagascar's land surface. , areas protected by the state included five Strict Nature Reserves ("Réserves Naturelles Intégrales"), 21 Wildlife Reserves ("Réserves Spéciales") and 21 National Parks ("Parcs Nationaux"). In 2007 six of the national parks were declared a joint World Heritage Site under the name Rainforests of the Atsinanana. These parks are Marojejy, Masoala, Ranomafana, Zahamena, Andohahela and Andringitra. Local timber merchants are harvesting scarce species of rosewood trees from protected rainforests within Marojejy National Park and exporting the wood to China for the production of luxury furniture and musical instruments. To raise public awareness of Madagascar's environmental challenges, the Wildlife Conservation Society opened an exhibit entitled ""Madagascar!"" in June 2008 at the Bronx Zoo in New York.

The settlement of Madagascar is a subject of ongoing research and debate. Archaeological finds such as cut marks on bones found in the northwest and stone tools in the northeast indicate that Madagascar was visited by foragers around 2000 BC. Traditionally, archaeologists have estimated that the earliest settlers arrived in successive waves throughout the period between 350 BC and 550 AD, while others are cautious about dates earlier than 250 AD. In either case, these dates make Madagascar one of the last major landmasses on Earth to be settled by humans.

Early settlers arrived in outrigger canoes from southern Borneo. Upon arrival, early settlers practiced slash-and-burn agriculture to clear the coastal rainforests for cultivation. The first settlers encountered Madagascar's abundance of megafauna, including giant lemurs, elephant birds, giant fossa and the Malagasy hippopotamus, which have since become extinct due to hunting and habitat destruction. By 600 AD groups of these early settlers had begun clearing the forests of the central highlands. Arab traders first reached the island between the seventh and ninth centuries. A wave of Bantu-speaking migrants from southeastern Africa arrived around 1000 AD. South Indian Tamil Merchants arrived around 11th Century. They introduced the zebu, a type of long-horned humped cattle, which they kept in large herds and irrigated paddy fields were developed in the central highland Betsileo Kingdom, and were extended with terraced paddies throughout the neighboring Kingdom of Imerina a century later. The rising intensity of land cultivation and the ever-increasing demand for zebu pasturage had largely transformed the central highlands from a forest ecosystem to grassland by the 17th century. The oral histories of the Merina people, who may have arrived in the central highlands between 600 and 1000 years ago, describe encountering an established population they called the Vazimba. Probably the descendants of an earlier and less technologically advanced Austronesian settlement wave, the Vazimba were assimilated or expelled from the highlands by the Merina kings Andriamanelo, Ralambo and Andrianjaka in the 16th and early 17th centuries. Today, the spirits of the Vazimba are revered as "tompontany" (ancestral masters of the land) by many traditional Malagasy communities.

Madagascar was an important transoceanic trading hub connecting ports of the Indian Ocean in the early centuries following human settlement. The written history of Madagascar began with the Arabs, who established trading posts along the northwest coast by at least the 10th century and introduced Islam, the Arabic script (used to transcribe the Malagasy language in a form of writing known as "sorabe"), Arab astrology, and other cultural elements. European contact began in 1500, when the Portuguese sea captain Diogo Dias sighted the island. The French established trading posts along the east coast in the late 17th century.

From about 1774 to 1824, Madagascar gained prominence among pirates and European traders, particularly those involved in the trans-Atlantic slave trade. The small island of Nosy Boroha off the northeastern coast of Madagascar has been proposed by some historians as the site of the legendary pirate utopia of Libertalia. Many European sailors were shipwrecked on the coasts of the island, among them Robert Drury, whose journal is one of the few written depictions of life in southern Madagascar during the 18th century. The wealth generated by maritime trade spurred the rise of organized kingdoms on the island, some of which had grown quite powerful by the 17th century. Among these were the Betsimisaraka alliance of the eastern coast and the Sakalava chiefdoms of Menabe and Boina on the west coast. The Kingdom of Imerina, located in the central highlands with its capital at the royal palace of Antananarivo, emerged at around the same time under the leadership of King Andriamanelo.

Upon its emergence in the early 17th century, the highland kingdom of Imerina was initially a minor power relative to the larger coastal kingdoms and grew even weaker in the early 18th century when King Andriamasinavalona divided it among his four sons. Following almost a century of warring and famine, Imerina was reunited in 1793 by King Andrianampoinimerina (1787–1810). From his initial capital Ambohimanga, and later from the Rova of Antananarivo, this Merina king rapidly expanded his rule over neighboring principalities. His ambition to bring the entire island under his control was largely achieved by his son and successor, King Radama I (1810–28), who was recognized by the British government as King of Madagascar. Radama concluded a treaty in 1817 with the British governor of Mauritius to abolish the lucrative slave trade in return for British military and financial assistance. Artisan missionary envoys from the London Missionary Society began arriving in 1818 and included such key figures as James Cameron, David Jones and David Griffiths, who established schools, transcribed the Malagasy language using the Roman alphabet, translated the Bible, and introduced a variety of new technologies to the island.

Radama's successor, Queen Ranavalona I (1828–61), responded to increasing political and cultural encroachment on the part of Britain and France by issuing a royal edict prohibiting the practice of Christianity in Madagascar and pressuring most foreigners to leave the territory. She made heavy use of the traditional practice of "fanompoana" (forced labor as tax payment) to complete public works projects and develop a standing army of between 20,000 and 30,000 Merina soldiers, whom she deployed to pacify outlying regions of the island and further expand the Kingdom of Merina to encompass most of Madagascar. Residents of Madagascar could accuse one another of various crimes, including theft, Christianity and especially witchcraft, for which the ordeal of "tangena" was routinely obligatory. Between 1828 and 1861, the "tangena" ordeal caused about 3,000 deaths annually. In 1838, it was estimated that as many as 100,000 people in Imerina died as a result of the tangena ordeal, constituting roughly 20 percent of the population. The combination of regular warfare, disease, difficult forced labor and harsh measures of justice resulted in a high mortality rate among soldiers and civilians alike during her 33-year reign. Among those who continued to reside in Imerina were Jean Laborde, an entrepreneur who developed munitions and other industries on behalf of the monarchy, and Joseph-François Lambert, a French adventurer and slave trader, with whom then-Prince Radama II signed a controversial trade agreement termed the Lambert Charter. Succeeding his mother, Radama II (1861–63) attempted to relax the queen's stringent policies, but was overthrown two years later by Prime Minister Rainivoninahitriniony (1852–1865) and an alliance of "Andriana" (noble) and "Hova" (commoner) courtiers, who sought to end the absolute power of the monarch.

Following the coup, the courtiers offered Radama's queen Rasoherina (1863–68) the opportunity to rule, if she would accept a power sharing arrangement with the Prime Minister—a new social contract that would be sealed by a political marriage between them. Queen Rasoherina accepted, first wedding Rainivoninahitriniony, then later deposing him and wedding his brother, Prime Minister Rainilaiarivony (1864–95), who would go on to marry Queen Ranavalona II (1868–83) and Queen Ranavalona III (1883–97) in succession. Over the course of Rainilaiarivony's 31-year tenure as prime minister, numerous policies were adopted to modernize and consolidate the power of the central government. Schools were constructed throughout the island and attendance was made mandatory. Army organization was improved, and British consultants were employed to train and professionalize soldiers. Polygamy was outlawed and Christianity, declared the official religion of the court in 1869, was adopted alongside traditional beliefs among a growing portion of the populace. Legal codes were reformed on the basis of British common law and three European-style courts were established in the capital city. In his joint role as Commander-in-Chief, Rainilaiarivony also successfully ensured the defense of Madagascar against several French colonial incursions.

Primarily on the basis that the Lambert Charter had not been respected, France invaded Madagascar in 1883 in what became known as the first Franco-Hova War. At the end of the war, Madagascar ceded the northern port town of Antsiranana (Diego Suarez) to France and paid 560,000 francs to Lambert's heirs. In 1890, the British accepted the full formal imposition of a French protectorate on the island, but French authority was not acknowledged by the government of Madagascar. To force capitulation, the French bombarded and occupied the harbor of Toamasina on the east coast, and Mahajanga on the west coast, in December 1894 and January 1895 respectively.

A French military flying column then marched toward Antananarivo, losing many men to malaria and other diseases. Reinforcements came from Algeria and Sub-Saharan Africa. Upon reaching the city in September 1895, the column bombarded the royal palace with heavy artillery, causing heavy casualties and leading Queen Ranavalona III to surrender. France annexed Madagascar in 1896 and declared the island a colony the following year, dissolving the Merina monarchy and sending the royal family into exile on Réunion Island and to Algeria. A two-year resistance movement organized in response to the French capture of the royal palace was effectively put down at the end of 1897.

Under colonial rule, plantations were established for the production of a variety of export crops. Slavery was abolished in 1896 and approximately 500,000 slaves were freed; many remained in their former masters' homes as servants or as sharecroppers; in many parts of the island strong discriminatory views against slave descendants are still held today. Wide paved boulevards and gathering places were constructed in the capital city of Antananarivo and the Rova palace compound was turned into a museum. Additional schools were built, particularly in rural and coastal areas where the schools of the Merina had not reached. Education became mandatory between the ages of 6 to 13 and focused primarily on French language and practical skills.

The Merina royal tradition of taxes paid in the form of labor was continued under the French and used to construct a railway and roads linking key coastal cities to Antananarivo. Malagasy troops fought for France in World War I. In the 1930s, Nazi political thinkers developed the Madagascar Plan that had identified the island as a potential site for the deportation of Europe's Jews. During the Second World War, the island was the site of the Battle of Madagascar between the Vichy government and the British.

The occupation of France during the Second World War tarnished the prestige of the colonial administration in Madagascar and galvanized the growing independence movement, leading to the Malagasy Uprising of 1947. This movement led the French to establish reformed institutions in 1956 under the "Loi Cadre" (Overseas Reform Act), and Madagascar moved peacefully towards independence. The Malagasy Republic was proclaimed on 14 October 1958, as an autonomous state within the French Community. A period of provisional government ended with the adoption of a constitution in 1959 and full independence on 26 June 1960.

Since regaining independence, Madagascar has transitioned through four republics with corresponding revisions to its constitution. The First Republic (1960–72), under the leadership of French-appointed President Philibert Tsiranana, was characterized by a continuation of strong economic and political ties to France. Many high-level technical positions were filled by French expatriates, and French teachers, textbooks and curricula continued to be used in schools around the country. Popular resentment over Tsiranana's tolerance for this "neo-colonial" arrangement inspired a series of farmer and student protests that overturned his administration in 1972.

Gabriel Ramanantsoa, a major general in the army, was appointed interim president and prime minister that same year, but low public approval forced him to step down in 1975. Colonel Richard Ratsimandrava, appointed to succeed him, was assassinated six days into his tenure. General Gilles Andriamahazo ruled after Ratsimandrava for four months before being replaced by another military appointee: Vice Admiral Didier Ratsiraka, who ushered in the socialist-Marxist Second Republic that ran under his tenure from 1975 to 1993.

This period saw a political alignment with the Eastern Bloc countries and a shift toward economic insularity. These policies, coupled with economic pressures stemming from the 1973 oil crisis, resulted in the rapid collapse of Madagascar's economy and a sharp decline in living standards, and the country had become completely bankrupt by 1979. The Ratsiraka administration accepted the conditions of transparency, anti-corruption measures and free market policies imposed by the International Monetary Fund, World Bank and various bilateral donors in exchange for their bailout of the nation's broken economy.

Ratsiraka's dwindling popularity in the late 1980s reached a critical point in 1991 when presidential guards opened fire on unarmed protesters during a rally. Within two months, a transitional government had been established under the leadership of Albert Zafy (1993–96), who went on to win the 1992 presidential elections and inaugurate the Third Republic (1992–2010). The new Madagascar constitution established a multi-party democracy and a separation of powers that placed significant control in the hands of the National Assembly. The new constitution also emphasized human rights, social and political freedoms, and free trade. Zafy's term, however, was marred by economic decline, allegations of corruption, and his introduction of legislation to give himself greater powers. He was consequently impeached in 1996, and an interim president, Norbert Ratsirahonana, was appointed for the three months prior to the next presidential election. Ratsiraka was then voted back into power on a platform of decentralization and economic reforms for a second term which lasted from 1996 to 2001.

The contested 2001 presidential elections in which then-mayor of Antananarivo, Marc Ravalomanana, eventually emerged victorious, caused a seven-month standoff in 2002 between supporters of Ravalomanana and Ratsiraka. The negative economic impact of the political crisis was gradually overcome by Ravalomanana's progressive economic and political policies, which encouraged investments in education and ecotourism, facilitated foreign direct investment, and cultivated trading partnerships both regionally and internationally. National GDP grew at an average rate of 7 percent per year under his administration. In the later half of his second term, Ravalomanana was criticised by domestic and international observers who accused him of increasing authoritarianism and corruption.

Opposition leader and then-mayor of Antananarivo, Andry Rajoelina, led a movement in early 2009 in which Ravalomanana was pushed from power in an unconstitutional process widely condemned as a "coup d'état". In March 2009, Rajoelina was declared by the Supreme Court as the President of the High Transitional Authority, an interim governing body responsible for moving the country toward presidential elections. In 2010, a new constitution was adopted by referendum, establishing a Fourth Republic, which sustained the democratic, multi-party structure established in the previous constitution. Hery Rajaonarimampianina was declared the winner of the 2013 presidential election, which the international community deemed fair and transparent.

Madagascar is a semi-presidential representative democratic multi-party republic, wherein the popularly elected president is the head of state and selects a prime minister, who recommends candidates to the president to form his cabinet of ministers. According to the constitution, executive power is exercised by the government while legislative power is vested in the ministerial cabinet, the Senate and the National Assembly, although in reality these two latter bodies have very little power or legislative role. The constitution establishes independent executive, legislative and judicial branches and mandates a popularly elected president limited to three five-year terms.

The public directly elects the president and the 127 members of the National Assembly to five-year terms. All 33 members of the Senate serve six-year terms, with 22 senators elected by local officials and 11 appointed by the president. The last National Assembly election was held on 20 December 2013 and the last Senate election was held on 30 December 2015.

At the local level, the island's 22 provinces are administered by a governor and provincial council. Provinces are further subdivided into regions and communes. The judiciary is modeled on the French system, with a High Constitutional Court, High Court of Justice, Supreme Court, Court of Appeals, criminal tribunals, and tribunals of first instance. The courts, which adhere to civil law, lack the capacity to quickly and transparently try the cases in the judicial system, often forcing defendants to pass lengthy pretrial detentions in unsanitary and overcrowded prisons.

Antananarivo is the administrative capital and largest city of Madagascar. It is located in the highlands region, near the geographic center of the island. King Andrianjaka founded Antananarivo as the capital of his Imerina Kingdom around 1610 or 1625 upon the site of a captured Vazimba capital on the hilltop of Analamanga. As Merina dominance expanded over neighboring Malagasy peoples in the early 19th century to establish the Kingdom of Madagascar, Antananarivo became the center of administration for virtually the entire island. In 1896 the French colonizers of Madagascar adopted the Merina capital as their center of colonial administration. The city remained the capital of Madagascar after regaining independence in 1960. In 2017, the capital's population was estimated at 1,391,433 inhabitants. The next largest cities are Antsirabe (500,000), Toamasina (450,000) and Mahajanga (400,000).

Since Madagascar gained independence from France in 1960, the island's political transitions have been marked by numerous popular protests, several disputed elections, an impeachment, two military coups and one assassination. The island's recurrent political crises are often prolonged, with detrimental effects on the local economy, international relations and Malagasy living standards. The eight-month standoff between incumbent Ratsiraka and challenger Marc Ravalomanana following the 2001 presidential elections cost Madagascar millions of dollars in lost tourism and trade revenue as well as damage to infrastructure, such as bombed bridges and buildings damaged by arson. A series of protests led by Andry Rajoelina against Ravalomanana in early 2009 became violent, with more than 170 people killed. Modern politics in Madagascar are colored by the history of Merina subjugation of coastal communities under their rule in the 19th century. The consequent tension between the highland and coastal populations has periodically flared up into isolated events of violence.

Madagascar has historically been perceived as being on the margin of mainstream African affairs despite being a founding member of the Organisation of African Unity, which was established in 1963 and dissolved in 2002 to be replaced by the African Union. Madagascar was not permitted to attend the first African Union summit because of a dispute over the results of the 2001 presidential election, but rejoined the African Union in July 2003 after a 14-month hiatus. Madagascar was again suspended by the African Union in March 2009 following the unconstitutional transfer of executive power to Rajoelina. Madagascar is a member of the International Criminal Court with a Bilateral Immunity Agreement of protection for the United States military. Eleven countries have established embassies in Madagascar, including France, the United Kingdom, the United States, China and India.

Human rights in Madagascar are protected under the constitution and the state is a signatory to numerous international agreements including the Universal Declaration of Human Rights and the Convention on the Rights of the Child. Religious, ethnic and sexual minorities are protected under the law. Freedom of association and assembly are also guaranteed under the law, although in practice the denial of permits for public assembly has occasionally been used to impede political demonstrations. Torture by security forces is rare and state repression is low relative to other countries with comparably few legal safeguards, although arbitrary arrests and the corruption of military and police officers remain problems. Ravalomanana's 2004 creation of BIANCO, an anti-corruption bureau, resulted in reduced corruption among Antananarivo's lower-level bureaucrats in particular, although high-level officials have not been prosecuted by the bureau.

The rise of centralized kingdoms among the Sakalava, Merina and other ethnic groups produced the island's first standing armies by the 16th century, initially equipped with spears but later with muskets, cannons and other firearms. By the early 19th century, the Merina sovereigns of the Kingdom of Madagascar had brought much of the island under their control by mobilizing an army of trained and armed soldiers numbering as high as 30,000. French attacks on coastal towns in the later part of the century prompted then-Prime Minister Rainilaiarivony to solicit British assistance to provide training to the Merina monarchy's army. Despite the training and leadership provided by British military advisers, the Malagasy army was unable to withstand French weaponry and was forced to surrender following an attack on the royal palace at Antananarivo. Madagascar was declared a colony of France in 1897.

The political independence and sovereignty of the Malagasy armed forces, which comprises an army, navy and air force, was restored with independence from France in 1960. Since this time the Malagasy military has never engaged in armed conflict with another state or within its own borders, but has occasionally intervened to restore order during periods of political unrest. Under the socialist Second Republic, Admiral Didier Ratsiraka instated mandatory national armed or civil service for all young citizens regardless of gender, a policy that remained in effect from 1976 to 1991. The armed forces are under the direction of the Minister of the Interior and have remained largely neutral during times of political crisis, as during the protracted standoff between incumbent Ratsiraka and challenger Marc Ravalomanana in the disputed 2001 presidential elections, when the military refused to intervene in favor of either candidate. This tradition was broken in 2009, when a segment of the army defected to the side of Andry Rajoelina, then-mayor of Antananarivo, in support of his attempt to force President Ravalomanana from power.

The Minister of the Interior is responsible for the national police force, paramilitary force ("gendarmerie") and the secret police. The police and gendarmerie are stationed and administered at the local level. However, in 2009 fewer than a third of all communes had access to the services of these security forces, with most lacking local-level headquarters for either corps. Traditional community tribunals, called "dina", are presided over by elders and other respected figures and remain a key means by which justice is served in rural areas where state presence is weak. Historically, security has been relatively high across the island. Violent crime rates are low, and criminal activities are predominantly crimes of opportunity such as pickpocketing and petty theft, although child prostitution, human trafficking and the production and sale of marijuana and other illegal drugs are increasing. Budget cuts since 2009 have severely impacted the national police force, producing a steep increase in criminal activity in recent years.

Madagascar is subdivided into 22 regions ("faritra"). The regions are further subdivided into 119 districts, 1,579 communes, and 17,485 "fokontany".

Madagascar became a Member State of the United Nations on 20 September 1960, shortly after gaining its independence on 26 June 1960. As of January 2017, 34 police officers from Madagascar are deployed in Haiti as part of the United Nations Stabilisation Mission in Haiti. Starting in 2015, under the direction of and with assistance from the UN, the World Food Programme started the Madagascar Country Programme with the two main goals of long-term development/ reconstruction efforts and addressing the food insecurity issues in the southern regions of Madagascar. These goals plan to be accomplished by providing meals for specific schools in rural and urban priority areas and by developing national school feeding policies to increase consistency of nourishment throughout the country. Small and local farmers have also been assisted in increasing both the quantity and quality of their production, as well as improving their crop yield in unfavorable weather conditions.

During Madagascar's First Republic, France heavily influenced Madagascar's economic planning and policy and served as its key trading partner. Key products were cultivated and distributed nationally through producers' and consumers' cooperatives. Government initiatives such as a rural development program and state farms were established to boost production of commodities such as rice, coffee, cattle, silk and palm oil. Popular dissatisfaction over these policies was a key factor in launching the socialist-Marxist Second Republic, in which the formerly private bank and insurance industries were nationalized; state monopolies were established for such industries as textiles, cotton and power; and import–export trade and shipping were brought under state control. Madagascar's economy quickly deteriorated as exports fell, industrial production dropped by 75 percent, inflation spiked and government debt increased; the rural population was soon reduced to living at subsistence levels. Over 50 percent of the nation's export revenue was spent on debt servicing.

The IMF forced Madagascar's government to accept structural adjustment policies and liberalization of the economy when the state became bankrupt in 1982 and state-controlled industries were gradually privatized over the course of the 1980s. The political crisis of 1991 led to the suspension of IMF and World Bank assistance. Conditions for the resumption of aid were not met under Zafy, who tried unsuccessfully to attract other forms of revenue for the State before aid was once again resumed under the interim government established upon Zafy's impeachment. The IMF agreed to write off half Madagascar's debt in 2004 under the Ravalomanana administration. Having met a set of stringent economic, governance and human rights criteria, Madagascar became the first country to benefit from the Millennium Challenge Account in 2005.

Madagascar's GDP in 2015 was estimated at 9.98 billion USD, with a per capita GDP of $411.82. Approximately 69 percent of the population lives below the national poverty line threshold of one dollar per day. Over the last five years, the average growth rate has been 2.6% but is expected to have reached 4.1% in 2016, due to public works programs and a growth of the service sector. The agriculture sector constituted 29 percent of Malagasy GDP in 2011, while manufacturing formed 15 percent of GDP. Madagascar's other sources of growth are tourism, agriculture and the extractive industries. Tourism focuses on the niche eco-tourism market, capitalizing on Madagascar's unique biodiversity, unspoiled natural habitats, national parks and lemur species. An estimated 365,000 tourists visited Madagascar in 2008, but the sector declined during the political crisis with 180,000 tourists visiting in 2010. However, the sector has been growing steadily for a few years; In 2016, 293,000 tourists landed in the African island with an increase of 20% compared to 2015; For 2017 the country has the goal of reaching 366,000 visitors, while for 2018 government estimates are expected to reach 500,000 annual tourists. 

The island is still a very poor country in 2018; structural brakes remain in the development of the economy: corruption and the shackles of the public administration, lack of legal certainty, backwardness of land legislation. The economy of the African country, however, has been growing since 2011, with rates exceeding 4% per year; the economic indicators seem almost all growing, the income per capita of the island is often referred to as around 1600 dollars a year to 2017 , one of the lowest in the world, however, growing since 2012; The gross domestic product is also growing; unemployment was also cut, which in 2016 was equal to 2.1% at 2017 c 'was a strong work force of 13.400.000 inhabitants. The main economic resources of Madagascar are tourism, textile export, production and export agricultural and mining.

Madagascar's natural resources include a variety of unprocessed agricultural and mineral resources. Agriculture (including raffia), fishing and forestry are mainstays of the economy. Madagascar is the world's principal supplier of vanilla, cloves and ylang-ylang. Other key agricultural resources include coffee, lychees and shrimp. Key mineral resources include various types of precious and semi-precious stones, and Madagascar currently provides half of the world's supply of sapphires, which were discovered near Ilakaka in the late 1990s.

Madagascar has one of the world's largest reserves of ilmenite (titanium ore), as well as important reserves of chromite, coal, iron, cobalt, copper and nickel. Several major projects are underway in the mining, oil and gas sectors that are anticipated to give a significant boost to the Malagasy economy. These include such projects as ilmenite and zircon mining from heavy mineral sands near Tôlanaro by Rio Tinto, extraction of nickel near Moramanga and its processing near Toamasina by Sherritt International, and the development of the giant onshore heavy oil deposits at Tsimiroro and Bemolanga by Madagascar Oil.

Exports formed 28 percent of GDP in 2009. Most of the country's export revenue is derived from the textiles industry, fish and shellfish, vanilla, cloves and other foodstuffs. France is Madagascar's main trading partner, although the United States, Japan and Germany also have strong economic ties to the country. The Madagascar-U.S. Business Council was formed in May 2003, as a collaboration between USAID and Malagasy artisan producers to support the export of local handicrafts to foreign markets. Imports of such items as foodstuffs, fuel, capital goods, vehicles, consumer goods and electronics consume an estimated 52 percent of GDP. The main sources of Madagascar's imports include China, France, Iran, Mauritius and Hong Kong.

In 2010, Madagascar had approximately of paved roads, of railways and of navigable waterways. The majority of roads in Madagascar are unpaved, with many becoming impassable in the rainy season. Largely paved national routes connect the six largest regional towns to Antananarivo, with minor paved and unpaved routes providing access to other population centers in each district.

There are several rail lines. Antananarivo is connected to Toamasina, Ambatondrazaka and Antsirabe by rail, and another rail line connects Fianarantsoa to Manakara. The most important seaport in Madagascar is located on the east coast at Toamasina. Ports at Mahajanga and Antsiranana are significantly less used due to their remoteness. The island's newest port at Ehoala, constructed in 2008 and privately managed by Rio Tinto, will come under state control upon completion of the company's mining project near Tôlanaro around 2038. Air Madagascar services the island's many small regional airports, which offer the only practical means of access to many of the more remote regions during rainy season road washouts.

Running water and electricity are supplied at the national level by a government service provider, Jirama, which is unable to service the entire population. , only 6.8 percent of Madagascar's "fokontany" had access to water provided by Jirama, while 9.5 percent had access to its electricity services. 56% of Madagascar's power is provided by hydroelectric power plants with the remaining 44% provided by diesel engine generators. Mobile telephone and internet access are widespread in urban areas but remain limited in rural parts of the island. Approximately 30 percent of the districts are able to access the nations' several private telecommunications networks via mobile telephones or land lines.

Radio broadcasts remain the principal means by which the Malagasy population access international, national and local news. Only state radio broadcasts are transmitted across the entire island. Hundreds of public and private stations with local or regional range provide alternatives to state broadcasting. In addition to the state television channel, a variety of privately owned television stations broadcast local and international programming throughout Madagascar. Several media outlets are owned by political partisans or politicians themselves, including the media groups MBS (owned by Ravalomanana) and Viva (owned by Rajoelina), contributing to political polarization in reporting.

The media has historically come under varying degrees of pressure over time to censor their criticism of the government. Reporters are occasionally threatened or harassed and media outlets are periodically forced to close. Accusations of media censorship have increased since 2009 due to the alleged intensification of restrictions on political criticism. Access to the internet has grown dramatically over the past decade, with an estimated 352,000 residents of Madagascar accessing the internet from home or in one of the nation's many internet cafés in December 2011.

Medical centers, dispensaries and hospitals are found throughout the island, although they are concentrated in urban areas and particularly in Antananarivo. Access to medical care remains beyond the reach of many Malagasy, especially in the rural areas, and many recourse to traditional healers. In addition to the high expense of medical care relative to the average Malagasy income, the prevalence of trained medical professionals remains extremely low. In 2010 Madagascar had an average of three hospital beds per 10,000 people and a total of 3,150 doctors, 5,661 nurses, 385 community health workers, 175 pharmacists and 57 dentists for a population of 22 million. 14.6 percent of government spending in 2008 was directed toward the health sector. Approximately 70 percent of spending on health was contributed by the government, while 30 percent originated with international donors and other private sources. The government provides at least one basic health center per commune. Private health centers are concentrated within urban areas and particularly those of the central highlands.

Despite these barriers to access, health services have shown a trend toward improvement over the past twenty years. Child immunizations against such diseases as hepatitis B, diphtheria and measles increased an average of 60 percent in this period, indicating low but increasing availability of basic medical services and treatments. The Malagasy fertility rate in 2009 was 4.6 children per woman, declining from 6.3 in 1990. Teen pregnancy rates of 14.8 percent in 2011, much higher than the African average, are a contributing factor to rapid population growth. In 2010 the maternal mortality rate was 440 per 100,000 births, compared to 373.1 in 2008 and 484.4 in 1990, indicating a decline in perinatal care following the 2009 coup. The infant mortality rate in 2011 was 41 per 1,000 births, with an under-five mortality rate at 61 per 1,000 births. Schistosomiasis, malaria and sexually transmitted diseases are common in Madagascar, although infection rates of AIDS remain low relative to many countries in mainland Africa, at only 0.2 percent of the adult population. The malaria mortality rate is also among the lowest in Africa at 8.5 deaths per 100,000 people, in part due to the highest frequency use of insecticide treated nets in Africa. Adult life expectancy in 2009 was 63 years for men and 67 years for women.

In 2017, Madagascar had an outbreak of the bubonic plague (also known as the black death) that affected urban areas.

Prior to the 19th century, all education in Madagascar was informal and typically served to teach practical skills as well as social and cultural values, including respect for ancestors and elders. The first formal European-style school was established in 1818 at Toamasina by members of the London Missionary Society (LMS). The LMS was invited by King Radama I (1810–28) to expand its schools throughout Imerina to teach basic literacy and numeracy to aristocratic children. The schools were closed by Ranavalona I in 1835 but reopened and expanded in the decades after her death.

By the end of the 19th century Madagascar had the most developed and modern school system in pre-colonial Sub-Saharan Africa. Access to schooling was expanded in coastal areas during the colonial period, with French language and basic work skills becoming the focus of the curriculum. During the post-colonial First Republic, a continued reliance on French nationals as teachers, and French as the language of instruction, displeased those desiring a complete separation from the former colonial power.Consequently, under the socialist Second Republic, French instructors and other nationals were expelled, Malagasy was declared the language of instruction and a large cadre of young Malagasy were rapidly trained to teach at remote rural schools under the mandatory two-year national service policy.

This policy, known as "malgachization", coincided with a severe economic downturn and a dramatic decline in the quality of education. Those schooled during this period generally failed to master the French language or many other subjects and struggled to find employment, forcing many to take low-paying jobs in the informal or black market that mired them in deepening poverty. Excepting the brief presidency of Albert Zafy, from 1992 to 1996, Ratsiraka remained in power from 1975 to 2001 and failed to achieve significant improvements in education throughout his tenure.

Education was prioritized under the Ravalomanana administration (2002–09), and is currently free and compulsory from ages 6 to 13. The primary schooling cycle is five years, followed by four years at the lower secondary level and three years at the upper secondary level. During Ravalomanana's first term, thousands of new primary schools and additional classrooms were constructed, older buildings were renovated, and tens of thousands of new primary teachers were recruited and trained. Primary school fees were eliminated and kits containing basic school supplies were distributed to primary students.

Government school construction initiatives have ensured at least one primary school per "fokontany" and one lower secondary school within each commune. At least one upper secondary school is located in each of the larger urban centers. The three branches of the national public university are located at Antananarivo (founded in 1961), Mahajanga (1977) and Fianarantsoa (1988). These are complemented by public teacher-training colleges and several private universities and technical colleges.

As a result of increased educational access, enrollment rates more than doubled between 1996 and 2006. However, education quality is weak, producing high rates of grade repetition and dropout. Education policy in Ravalomanana's second term focused on quality issues, including an increase in minimum education standards for the recruitment of primary teachers from a middle school leaving certificate (BEPC) to a high school leaving certificate (BAC), and a reformed teacher training program to support the transition from traditional didactic instruction to student-centered teaching methods to boost student learning and participation in the classroom. Public expenditure on education was 13.4 percent of total government expenditure and 2.9 percent of GDP in 2008. Primary classrooms are crowded, with average pupil to teacher ratios of 47:1 in 2008.

In , the population of Madagascar was estimated at /1e6 round 0 million, up from 2.2 million in 1900. The annual population growth rate in Madagascar was approximately 2.9 percent in 2009.

Approximately 42.5 percent of the population is younger than 15 years of age, while 54.5 percent are between the ages of 15 and 64. Those aged 65 and older form three percent of the total population. Only two general censuses, in 1975 and 1993, have been carried out after independence. The most densely populated regions of the island are the eastern highlands and the eastern coast, contrasting most dramatically with the sparsely populated western plains.

The Malagasy ethnic group forms over 90 percent of Madagascar's population and is typically divided into eighteen ethnic subgroups. Recent DNA research revealed that the genetic makeup of the average Malagasy person constitutes an approximately equal blend of Southeast Asian and East African genes, although the genetics of some communities show a predominance of Southeast Asian or East African origins or some Arab, Indian or European ancestry.

Southeast Asian features – specifically from the southern part of Borneo – are most predominant among the Merina of the central highlands, who form the largest Malagasy ethnic subgroup at approximately 26 percent of the population, while certain communities among the coastal peoples (collectively called "côtiers") have relatively stronger East African features. The largest coastal ethnic subgroups are the Betsimisaraka (14.9 percent) and the Tsimihety and Sakalava (6 percent each).

Chinese, Indian and Comorian minorities are present in Madagascar, as well as a small European (primarily French) populace. Emigration in the late 20th century has reduced these minority populations, occasionally in abrupt waves, such as the exodus of Comorans in 1976, following anti-Comoran riots in Mahajanga. By comparison, there has been no significant emigration of Malagasy peoples. The number of Europeans has declined since independence, reduced from 68,430 in 1958 to 17,000 three decades later. There were an estimated 25,000 Comorans, 18,000 Indians, and 9,000 Chinese living in Madagascar in the mid-1980s.

The Malagasy language is of Malayo-Polynesian origin and is generally spoken throughout the island. The numerous dialects of Malagasy, which are generally mutually intelligible, can be clustered under one of two subgroups: eastern Malagasy, spoken along the eastern forests and highlands including the Merina dialect of Antananarivo and western Malagasy, spoken across the western coastal plains. French became the official language during the colonial period, when Madagascar came under the authority of France. In the first national Constitution of 1958, Malagasy and French were named the official languages of the Malagasy Republic. Madagascar is a francophone country, and French is mostly spoken as a second language among the educated population and used for international communication.

No official languages were recorded in the Constitution of 1992, although Malagasy was identified as the national language. Nonetheless, many sources still claimed that Malagasy and French were official languages, eventually leading a citizen to initiate a legal case against the state in April 2000, on the grounds that the publication of official documents only in the French language was unconstitutional. The High Constitutional Court observed in its decision that, in the absence of a language law, French still had the character of an official language.

In the Constitution of 2007, Malagasy remained the national language while official languages were reintroduced: Malagasy, French, and English. English was removed as an official language from the constitution approved by voters in the November 2010 referendum. The outcome of the referendum, and its consequences for official and national language policy, are not recognized by the political opposition, who cite lack of transparency and inclusiveness in the way the election was organized by the High Transitional Authority.

Language Policy

Over the years, Madagascar has had different language policies under different influences of authority. The indigenous language of Madagascar, Malagasy, was the predominant language on the island until the French colonization in 1897. Malagasy has developed throughout the decades from an oral language to a language that has a written system (Latin orthography), a change that was enforced by King Radama I, in 1823.
Following the French colonization, the language of instruction and media changed from Malagasy to almost exclusively French. Moreover, the first French governor-general, Gallieni, also encouraged the French officials to learn Malagasy as well. After the advent of the Malagasy independence, the Madagascans tried to reinstate Malagasy as a language of instruction especially in secondary schools. However, the language policy was inadequately planned and Malagasy was struggling to surpass French as the language of instruction.
Today, Madagascar has two official languages: Malagasy and French. Madagascar managed to maintain the indigenous language, Malagasy, in society and in schools despite the colonizing power. Malagasy and French are both the language of instruction in primary and secondary schools in Madagascar. The inclusion of the African language as a medium of instruction is usually uncommon in other colonized African countries.

According to the US Department of State in 2011, 41% of Madagascans practiced Christianity and 52% adhered to traditional religions, which tends to emphasize links between the living and the "razana" (ancestors). But according to the Pew Research Center in 2010, 85% of the population practiced Christianity, while just 4.5% of Madagascans practiced folk religions; among Christians, practitioners of Protestantism outnumbered adherents of Roman Catholicism. 

The veneration of ancestors has led to the widespread tradition of tomb building, as well as the highlands practice of the "famadihana", whereby a deceased family member's remains are exhumed and re-wrapped in fresh silk shrouds, before being replaced in the tomb. The famadihana is an occasion to celebrate the beloved ancestor's memory, reunite with family and community, and enjoy a festive atmosphere. Residents of surrounding villages are often invited to attend the party, where food and rum are typically served and a hiragasy troupe or other musical entertainment is commonly present. Consideration for ancestors is also demonstrated through adherence to "fady", taboos that are respected during and after the lifetime of the person who establishes them. It is widely believed that by showing respect for ancestors in these ways, they may intervene on behalf of the living. Conversely, misfortunes are often attributed to ancestors whose memory or wishes have been neglected. The sacrifice of zebu is a traditional method used to appease or honor the ancestors. In addition, the Malagasy traditionally believe in a creator god, called Zanahary or Andriamanitra.

In 1818, the London Missionary Society sent the first Christian missionaries to the island, where they built churches, translated the Bible into the Malagasy language and began to gain converts. Beginning in 1835, Queen Ranavalona I persecuted these converts as part of an attempt to halt European cultural and political influence on the island. In 1869, a successor, Queen Ranavalona II, converted the court to Christianity and encouraged Christian missionary activity, burning the "sampy" (royal idols) in a symbolic break with traditional beliefs. Today, many Christians integrate their religious beliefs with traditional ones related to honoring the ancestors. For instance, they may bless their dead at church before proceeding with traditional burial rites or invite a Christian minister to consecrate a "famadihana" reburial. The Malagasy Council of Churches comprises the four oldest and most prominent Christian denominations of Madagascar (Roman Catholic, Church of Jesus Christ in Madagascar, Lutheran, and Anglican) and has been an influential force in Malagasy politics.

Islam is also practiced on the island. Islam was first brought to Madagascar in the Middle Ages by Arab and Somali Muslim traders, who established several Islamic schools along the eastern coast. While the use of Arabic script and loan words and the adoption of Islamic astrology would spread across the island, the Islamic religion failed to take hold in all but a handful of southeastern coastal communities. Today, Muslims constitute 3–7 percent of the population of Madagascar and are largely concentrated in the northwestern provinces of Mahajanga and Antsiranana. The vast majority of Muslims are Sunni. Muslims are divided between those of Malagasy ethnicity, Indians, Pakistanis and Comorians. More recently, Hinduism was introduced to Madagascar through Gujarati people immigrating from the Saurashtra region of India in the late 19th century. Most Hindus in Madagascar speak Gujarati or Hindi at home.

Each of the many ethnic subgroups in Madagascar adhere to their own set of beliefs, practices and ways of life that have historically contributed to their unique identities. However, there are a number of core cultural features that are common throughout the island, creating a strongly unified Malagasy cultural identity. In addition to a common language and shared traditional religious beliefs around a creator god and veneration of the ancestors, the traditional Malagasy worldview is shaped by values that emphasize "fihavanana" (solidarity), "vintana" (destiny), "tody" (karma), and "hasina", a sacred life force that traditional communities believe imbues and thereby legitimates authority figures within the community or family. Other cultural elements commonly found throughout the island include the practice of male circumcision; strong kinship ties; a widespread belief in the power of magic, diviners, astrology and witch doctors; and a traditional division of social classes into nobles, commoners, and slaves.

Although social castes are no longer legally recognized, ancestral caste affiliation often continues to affect social status, economic opportunity and roles within the community. Malagasy people traditionally consult "Mpanandro" ("Makers of the Days") to identify the most auspicious days for important events such as weddings or "famadihana", according to a traditional astrological system introduced by Arabs. Similarly, the nobles of many Malagasy communities in the pre-colonial period would commonly employ advisers known as the "ombiasy" (from "olona-be-hasina", "man of much virtue") of the southeastern Antemoro ethnic group, who trace their ancestry back to early Arab settlers.

The diverse origins of Malagasy culture are evident in its tangible expressions. The most emblematic instrument of Madagascar, the "valiha", is a bamboo tube zither carried to Madagascar by early settlers from southern Borneo, and is very similar in form to those found in Indonesia and the Philippines today. Traditional houses in Madagascar are likewise similar to those of southern Borneo in terms of symbolism and construction, featuring a rectangular layout with a peaked roof and central support pillar. Reflecting a widespread veneration of the ancestors, tombs are culturally significant in many regions and tend to be built of more durable material, typically stone, and display more elaborate decoration than the houses of the living. The production and weaving of silk can be traced back to the island's earliest settlers, and Madagascar's national dress, the woven "lamba", has evolved into a varied and refined art.

The Southeast Asian cultural influence is also evident in Malagasy cuisine, in which rice is consumed at every meal, typically accompanied by one of a variety of flavorful vegetable or meat dishes. African influence is reflected in the sacred importance of zebu cattle and their embodiment of their owner's wealth, traditions originating on the African mainland. Cattle rustling, originally a rite of passage for young men in the plains areas of Madagascar where the largest herds of cattle are kept, has become a dangerous and sometimes deadly criminal enterprise as herdsmen in the southwest attempt to defend their cattle with traditional spears against increasingly armed professional rustlers.

A wide variety of oral and written literature has developed in Madagascar. One of the island's foremost artistic traditions is its oratory, as expressed in the forms of "hainteny" (poetry), "kabary" (public discourse) and "ohabolana" (proverbs). An epic poem exemplifying these traditions, the "Ibonia", has been handed down over the centuries in several different forms across the island, and offers insight into the diverse mythologies and beliefs of traditional Malagasy communities. This tradition was continued in the 20th century by such artists as Jean-Joseph Rabearivelo, who is considered Africa's first modern poet, and Elie Rajaonarison, an exemplar of the new wave of Malagasy poetry. Madagascar has also developed a rich musical heritage, embodied in dozens of regional musical genres such as the coastal "salegy" or highland "hiragasy" that enliven village gatherings, local dance floors and national airwaves. Additionally, Madagascar also has a growing culture of classical music fostered through youth academies, organizations and orchestras that promote youth involvement in classical music.

The plastic arts are also widespread throughout the island. In addition to the tradition of silk weaving and lamba production, the weaving of raffia and other local plant materials has been used to create a wide array of practical items such as floor mats, baskets, purses and hats. Wood carving is a highly developed art form, with distinct regional styles evident in the decoration of balcony railings and other architectural elements. Sculptors create a variety of furniture and household goods, "aloalo" funerary posts, and wooden sculptures, many of which are produced for the tourist market. The decorative and functional woodworking traditions of the Zafimaniry people of the central highlands was inscribed on UNESCO's list of Intangible Cultural Heritage in 2008.

Among the Antaimoro people, the production of paper embedded with flowers and other decorative natural materials is a long-established tradition that the community has begun to market to eco-tourists. Embroidery and drawn thread work are done by hand to produce clothing, as well as tablecloths and other home textiles for sale in local crafts markets. A small but growing number of fine art galleries in Antananarivo, and several other urban areas, offer paintings by local artists, and annual art events, such as the Hosotra open-air exhibition in the capital, contribute to the continuing development of fine arts in Madagascar.

A number of traditional pastimes have emerged in Madagascar. "Moraingy", a type of hand-to-hand combat, is a popular spectator sport in coastal regions. It is traditionally practiced by men, but women have recently begun to participate. The wrestling of zebu cattle, which is named savika or "tolon-omby", is also practiced in many regions. In addition to sports, a wide variety of games are played. Among the most emblematic is "fanorona", a board game widespread throughout the Highland regions. According to folk legend, the succession of King Andrianjaka after his father Ralambo was partially due to the obsession that Andrianjaka's older brother may have had with playing "fanorona" to the detriment of his other responsibilities.

Western recreational activities were introduced to Madagascar over the past two centuries. Rugby union is considered the national sport of Madagascar. Soccer is also popular. Madagascar has produced a world champion in pétanque, a French game similar to lawn bowling, which is widely played in urban areas and throughout the Highlands. School athletics programs typically include soccer, track and field, judo, boxing, women's basketball and women's tennis. Madagascar sent its first competitors to the Olympic Games in 1964 and has also competed in the African Games. Scouting is represented in Madagascar by its own local federation of three scouting clubs. Membership in 2011 was estimated at 14,905.

Because of its advanced sports facilities, Antananarivo gained the hosting rights for several of Africa's top international basketball events, including the 2011 FIBA Africa Championship, the 2009 FIBA Africa Championship for Women, the 2014 FIBA Africa Under-18 Championship, the 2013 FIBA Africa Under-16 Championship, and the 2015 FIBA Africa Under-16 Championship for Women.

Malagasy cuisine reflects the diverse influences of Southeast Asian, African, Indian, Chinese and European culinary traditions. The complexity of Malagasy meals can range from the simple, traditional preparations introduced by the earliest settlers, to the refined festival dishes prepared for the island's 19th-century monarchs. Throughout almost the entire island, the contemporary cuisine of Madagascar typically consists of a base of rice ("vary") served with an accompaniment ("laoka"). The many varieties of laoka may be vegetarian or include animal proteins, and typically feature a sauce flavored with such ingredients as ginger, onion, garlic, tomato, vanilla, coconut milk, salt, curry powder, green peppercorns or, less commonly, other spices or herbs. In parts of the arid south and west, pastoral families may replace rice with maize, cassava, or curds made from fermented zebu milk. A wide variety of sweet and savory fritters as well as other street foods are available across the island, as are diverse tropical and temperate-climate fruits. Locally produced beverages include fruit juices, coffee, herbal teas and teas, and alcoholic drinks such as rum, wine, and beer. Three Horses Beer is the most popular beer on the island and is considered emblematic of Madagascar. The island also produces some of the world's finest chocolate; Chocolaterie Robert, established in 1940, is the most famous chocolate company on the island.





</doc>
<doc id="18967" url="https://en.wikipedia.org/wiki?curid=18967" title="Flowering plant">
Flowering plant

The flowering plants, also known as angiosperms, Angiospermae or Magnoliophyta, are the most diverse group of land plants, with 416 families, approximately 13,164 known genera and c. 295,383 known species. Like gymnosperms, angiosperms are seed-producing plants. However, they are distinguished from gymnosperms by characteristics including flowers, endosperm within the seeds, and the production of fruits that contain the seeds. Etymologically, "angiosperm" means a plant that produces seeds within an enclosure; in other words, a fruiting plant. The term comes from the Greek words "angeion" ("case" or "casing") and "sperma" ("seed").

The ancestors of flowering plants diverged from gymnosperms in the Triassic Period, 245 to 202 million years ago (mya), and the first flowering plants are known from 160 mya. They diversified extensively during the Lower Cretaceous, became widespread by 120 mya, and replaced conifers as the dominant trees from 100 to 60 mya.

Angiosperms differ from other seed plants in several ways, described in the table below. These distinguishing characteristics taken together have made the angiosperms the most diverse and numerous land plants and the most commercially important group to humans.

The amount and complexity of tissue-formation in flowering plants exceeds that of gymnosperms. The vascular bundles of the stem are arranged such that the xylem and phloem form concentric rings.

In the dicotyledons, the bundles in the very young stem are arranged in an open ring, separating a central pith from an outer cortex. In each bundle, separating the xylem and phloem, is a layer of meristem or active formative tissue known as cambium. By the formation of a layer of cambium between the bundles (interfascicular cambium), a complete ring is formed, and a regular periodical increase in thickness results from the development of xylem on the inside and phloem on the outside. The soft phloem becomes crushed, but the hard wood persists and forms the bulk of the stem and branches of the woody perennial. Owing to differences in the character of the elements produced at the beginning and end of the season, the wood is marked out in transverse section into concentric rings, one for each season of growth, called annual rings.

Among the monocotyledons, the bundles are more numerous in the young stem and are scattered through the ground tissue. They contain no cambium and once formed the stem increases in diameter only in exceptional cases.

The characteristic feature of angiosperms is the flower. Flowers show remarkable variation in form and elaboration, and provide the most trustworthy external characteristics for establishing relationships among angiosperm species. The function of the flower is to ensure fertilization of the ovule and development of fruit containing seeds. The floral apparatus may arise terminally on a shoot or from the axil of a leaf (where the petiole attaches to the stem). Occasionally, as in violets, a flower arises singly in the axil of an ordinary foliage-leaf. More typically, the flower-bearing portion of the plant is sharply distinguished from the foliage-bearing or vegetative portion, and forms a more or less elaborate branch-system called an inflorescence.

There are two kinds of reproductive cells produced by flowers. Microspores, which will divide to become pollen grains, are the "male" cells and are borne in the stamens (or microsporophylls). The "female" cells called megaspores, which will divide to become the egg cell (megagametogenesis), are contained in the ovule and enclosed in the carpel (or megasporophyll).

The flower may consist only of these parts, as in willow, where each flower comprises only a few stamens or two carpels. Usually, other structures are present and serve to protect the sporophylls and to form an envelope attractive to pollinators. The individual members of these surrounding structures are known as sepals and petals (or tepals in flowers such as "Magnolia" where sepals and petals are not distinguishable from each other). The outer series (calyx of sepals) is usually green and leaf-like, and functions to protect the rest of the flower, especially the bud. The inner series (corolla of petals) is, in general, white or brightly colored, and is more delicate in structure. It functions to attract insect or bird pollinators. Attraction is effected by color, scent, and nectar, which may be secreted in some part of the flower. The characteristics that attract pollinators account for the popularity of flowers and flowering plants among humans.

While the majority of flowers are perfect or hermaphrodite (having both pollen and ovule producing parts in the same flower structure), flowering plants have developed numerous morphological and physiological mechanisms to reduce or prevent self-fertilization. Heteromorphic flowers have short carpels and long stamens, or vice versa, so animal pollinators cannot easily transfer pollen to the pistil (receptive part of the carpel). Homomorphic flowers may employ a biochemical (physiological) mechanism called self-incompatibility to discriminate between self and non-self pollen grains. In other species, the male and female parts are morphologically separated, developing on different flowers.

The botanical term "Angiosperm", from the Ancient Greek αγγείον, "angeíon" (bottle, vessel) and σπέρμα, (seed), was coined in the form Angiospermae by Paul Hermann in 1690, as the name of one of his primary divisions of the plant kingdom. This included flowering plants possessing seeds enclosed in capsules, distinguished from his Gymnospermae, or flowering plants with achenial or schizo-carpic fruits, the whole fruit or each of its pieces being here regarded as a seed and naked. The term and its antonym were maintained by Carl Linnaeus with the same sense, but with restricted application, in the names of the orders of his class Didynamia. Its use with any approach to its modern scope became possible only after 1827, when Robert Brown established the existence of truly naked ovules in the Cycadeae and Coniferae, and applied to them the name Gymnosperms. From that time onward, as long as these Gymnosperms were, as was usual, reckoned as dicotyledonous flowering plants, the term Angiosperm was used antithetically by botanical writers, with varying scope, as a group-name for other dicotyledonous plants.
In 1851, Hofmeister discovered the changes occurring in the embryo-sac of flowering plants, and determined the correct relationships of these to the Cryptogamia. This fixed the position of Gymnosperms as a class distinct from Dicotyledons, and the term Angiosperm then gradually came to be accepted as the suitable designation for the whole of the flowering plants other than Gymnosperms, including the classes of Dicotyledons and Monocotyledons. This is the sense in which the term is used today.

In most taxonomies, the flowering plants are treated as a coherent group. The most popular descriptive name has been Angiospermae (Angiosperms), with Anthophyta ("flowering plants") a second choice. These names are not linked to any rank. The Wettstein system and the Engler system use the name Angiospermae, at the assigned rank of subdivision. The Reveal system treated flowering plants as subdivision Magnoliophytina (Frohne & U. Jensen ex Reveal, Phytologia 79: 70 1996), but later split it to Magnoliopsida, Liliopsida, and Rosopsida. The Takhtajan system and Cronquist system treat this group at the rank of division, leading to the name Magnoliophyta (from the family name Magnoliaceae). The Dahlgren system and Thorne system (1992) treat this group at the rank of class, leading to the name Magnoliopsida. The APG system of 1998, and the later 2003 and 2009 revisions, treat the flowering plants as a clade called angiosperms without a formal botanical name. However, a formal classification was published alongside the 2009 revision in which the flowering plants form the Subclass Magnoliidae.

The internal classification of this group has undergone considerable revision. The Cronquist system, proposed by Arthur Cronquist in 1968 and published in its full form in 1981, is still widely used but is no longer believed to accurately reflect phylogeny. A consensus about how the flowering plants should be arranged has recently begun to emerge through the work of the Angiosperm Phylogeny Group (APG), which published an influential reclassification of the angiosperms in 1998. Updates incorporating more recent research were published as the APG II system in 2003, the APG III system in 2009, and the APG IV system in 2016.

Traditionally, the flowering plants are divided into two groups,


which in the Cronquist system are called Magnoliopsida (at the rank of class, formed from the family name Magnoliaceae) and Liliopsida (at the rank of class, formed from the family name Liliaceae). Other descriptive names allowed by Article 16 of the ICBN include Dicotyledones or Dicotyledoneae, and Monocotyledones or Monocotyledoneae, which have a long history of use. In English a member of either group may be called a dicotyledon (plural dicotyledons) and monocotyledon (plural monocotyledons), or abbreviated, as dicot (plural dicots) and monocot (plural monocots). These names derive from the observation that the dicots most often have two cotyledons, or embryonic leaves, within each seed. The monocots usually have only one, but the rule is not absolute either way. From a broad diagnostic point of view, the number of cotyledons is neither a particularly handy nor a reliable character.

Recent studies, as by the APG, show that the monocots form a monophyletic group (clade) but that the dicots do not (they are paraphyletic). Nevertheless, the majority of dicot species do form a monophyletic group, called the eudicots or tricolpates. Of the remaining dicot species, most belong to a third major clade known as the magnoliids, containing about 9,000 species. The rest include a paraphyletic grouping of early branching taxa known collectively as the basal angiosperms, plus the families Ceratophyllaceae and Chloranthaceae.

There are eight groups of living angiosperms:


The exact relationship between these eight groups is not yet clear, although there is agreement that the first three groups to diverge from the ancestral angiosperm were Amborellales, Nymphaeales, and Austrobaileyales. The term basal angiosperms refers to these three groups. Among the remaining five groups (core angiosperms), the relationship between the three broadest of these groups (magnoliids, monocots, and eudicots) remains unclear. Zeng and colleagues (Fig. 1) describe four competing schemes. Of these, eudicots and monocots are the largest and most diversified, with ~ 75% and 20% of angiosperm species, respectively. Some analyses make the magnoliids the first to diverge, others the monocots. "Ceratophyllum" seems to group with the eudicots rather than with the monocots. The 2016 Angiosperm Phylogeny Group revision (APG IV) retained the overall higher order relationship described in APG III.

Fossilized spores suggest that higher plants (embryophytes) have lived on land for at least 475 million years. Early land plants reproduced sexually with flagellated, swimming sperm, like the green algae from which they evolved. An adaptation to terrestrialization was the development of upright meiosporangia for dispersal by spores to new habitats. This feature is lacking in the descendants of their nearest algal relatives, the Charophycean green algae. A later terrestrial adaptation took place with retention of the delicate, avascular sexual stage, the gametophyte, within the tissues of the vascular sporophyte. This occurred by spore germination within sporangia rather than spore release, as in non-seed plants. A current example of how this might have happened can be seen in the precocious spore germination in "Selaginella", the spike-moss. The result for the ancestors of angiosperms was enclosing them in a case, the seed. The first seed bearing plants, like the ginkgo, and conifers (such as pines and firs), did not produce flowers. The pollen grains (male gametophytes) of "Ginkgo" and cycads produce a pair of flagellated, mobile sperm cells that "swim" down the developing pollen tube to the female and her eggs.
The apparently sudden appearance of nearly modern flowers in the fossil record initially posed such a problem for the theory of evolution that Charles Darwin called it an ""abominable mystery"". However, the fossil record has considerably grown since the time of Darwin, and recently discovered angiosperm fossils such as "Archaefructus", along with further discoveries of fossil gymnosperms, suggest how angiosperm characteristics may have been acquired in a series of steps. Several groups of extinct gymnosperms, in particular seed ferns, have been proposed as the ancestors of flowering plants, but there is no continuous fossil evidence showing exactly how flowers evolved. Some older fossils, such as the upper Triassic "Sanmiguelia", have been suggested. Based on current evidence, some propose that the ancestors of the angiosperms diverged from an unknown group of gymnosperms in the Triassic period (245–202 million years ago). Fossil angiosperm-like pollen from the Middle Triassic (247.2–242.0 Ma) suggests an older date for their origin. A close relationship between angiosperms and gnetophytes, proposed on the basis of morphological evidence, has more recently been disputed on the basis of molecular evidence that suggest gnetophytes are instead more closely related to other gymnosperms.

The evolution of seed plants and later angiosperms appears to be the result of two distinct rounds of whole genome duplication events. These occurred at and . Another possible whole genome duplication event at perhaps created the ancestral line that led to all modern flowering plants. That event was studied by sequencing the genome of an ancient flowering plant, "Amborella trichopoda", and directly addresses Darwin's ""abominable mystery"."
The earliest known macrofossil confidently identified as an angiosperm, "Archaefructus liaoningensis", is dated to about 125 million years BP (the Cretaceous period), whereas pollen considered to be of angiosperm origin takes the fossil record back to about 130 million years BP. However, one study has suggested that the early-middle Jurassic plant "Schmeissneria", traditionally considered a type of ginkgo, may be the earliest known angiosperm, or at least a close relative. In addition, circumstantial chemical evidence has been found for the existence of angiosperms as early as 250 million years ago. Oleanane, a secondary metabolite produced by many flowering plants, has been found in Permian deposits of that age together with fossils of gigantopterids. Gigantopterids are a group of extinct seed plants that share many morphological traits with flowering plants, although they are not known to have been flowering plants themselves.

In 2013 flowers encased in amber were found and dated 100 million years before present. The amber had frozen the act of sexual reproduction in the process of taking place. Microscopic images showed tubes growing out of pollen and penetrating the flower's stigma. The pollen was sticky, suggesting it was carried by insects.

Recent DNA analysis based on molecular systematics showed that "Amborella trichopoda", found on the Pacific island of New Caledonia, belongs to a sister group of the other flowering plants, and morphological studies suggest that it has features that may have been characteristic of the earliest flowering plants.

The orders Amborellales, Nymphaeales, and Austrobaileyales diverged as separate lineages from the remaining angiosperm clade at a very early stage in flowering plant evolution.

The great angiosperm radiation, when a great diversity of angiosperms appears in the fossil record, occurred in the mid-Cretaceous (approximately 100 million years ago). However, a study in 2007 estimated that the division of the five most recent (the genus "Ceratophyllum", the family Chloranthaceae, the eudicots, the magnoliids, and the monocots) of the eight main groups occurred around 140 million years ago.
By the late Cretaceous, angiosperms appear to have dominated environments formerly occupied by ferns and cycadophytes, but large canopy-forming trees replaced conifers as the dominant trees only close to the end of the Cretaceous 66 million years ago or even later, at the beginning of the Tertiary. The radiation of herbaceous angiosperms occurred much later. Yet, many fossil plants recognizable as belonging to modern families (including beech, oak, maple, and magnolia) had already appeared by the late Cretaceous.

It has been proposed that the swift rise of angiosperms to dominance was facilitated by a reduction in their genome size. During the early Cretaceous period, only angiosperms underwent rapid genome downsizing, while genome sizes of ferns and gymnosperms remained unchanged. Smaller genomes–and smaller nuclei–allow for faster rates of cell division and smaller cells. Thus, species with smaller genomes can pack more, smaller cells–in particular veins and stomata–into a given leaf volume. Genome downsizing therefore facilitated higher rates of leaf gas exchange (transpiration and photosynthesis) and faster rates of growth. This would have countered some of the negative physiological effects of genome duplications, facilitated increased uptake of carbon dioxide despite concurrent declines in atmospheric CO concentrations, and allowed the flowering plants to outcompete other land plants.
It is generally assumed that the function of flowers, from the start, was to involve mobile animals in their reproduction processes. That is, pollen can be scattered even if the flower is not brightly colored or oddly shaped in a way that attracts animals; however, by expending the energy required to create such traits, angiosperms can enlist the aid of animals and, thus, reproduce more efficiently.

Island genetics provides one proposed explanation for the sudden, fully developed appearance of flowering plants. Island genetics is believed to be a common source of speciation in general, especially when it comes to radical adaptations that seem to have required inferior transitional forms. Flowering plants may have evolved in an isolated setting like an island or island chain, where the plants bearing them were able to develop a highly specialized relationship with some specific animal (a wasp, for example). Such a relationship, with a hypothetical wasp carrying pollen from one plant to another much the way fig wasps do today, could result in the development of a high degree of specialization in both the plant(s) and their partners. Note that the wasp example is not incidental; bees, which, it is postulated, evolved specifically due to mutualistic plant relationships, are descended from wasps.

Animals are also involved in the distribution of seeds. Fruit, which is formed by the enlargement of flower parts, is frequently a seed-dispersal tool that attracts animals to eat or otherwise disturb it, incidentally scattering the seeds it contains (see frugivory). Although many such mutualistic relationships remain too fragile to survive competition and to spread widely, flowering proved to be an unusually effective means of reproduction, spreading (whatever its origin) to become the dominant form of land plant life.

Flower ontogeny uses a combination of genes normally responsible for forming new shoots. The most primitive flowers probably had a variable number of flower parts, often separate from (but in contact with) each other. The flowers tended to grow in a spiral pattern, to be bisexual (in plants, this means both male and female parts on the same flower), and to be dominated by the ovary (female part). As flowers evolved, some variations developed parts fused together, with a much more specific number and design, and with either specific sexes per flower or plant or at least "ovary-inferior".

Flower evolution continues to the present day; modern flowers have been so profoundly influenced by humans that some of them cannot be pollinated in nature. Many modern domesticated flower species were formerly simple weeds, which sprouted only when the ground was disturbed. Some of them tended to grow with human crops, perhaps already having symbiotic companion plant relationships with them, and the prettiest did not get plucked because of their beauty, developing a dependence upon and special adaptation to human affection.

A few paleontologists have also proposed that flowering plants, or angiosperms, might have evolved due to interactions with dinosaurs. One of the idea's strongest proponents is Robert T. Bakker. He proposes that herbivorous dinosaurs, with their eating habits, provided a selective pressure on plants, for which adaptations either succeeded in deterring or coping with predation by herbivores.

In August 2017, scientists presented a detailed description and 3D model image of what the first flower possibly looked like, and presented the hypothesis that it may have lived about 140 million years ago.

A Bayesian analysis of 52 angiosperm taxa suggested that the crown group of angiosperms evolved between and .

The number of species of flowering plants is estimated to be in the range of 250,000 to 400,000. This compares to around 12,000 species of moss or 11,000 species of pteridophytes, showing that the flowering plants are much more diverse. The number of families in APG (1998) was 462. In APG II (2003) it is not settled; at maximum it is 457, but within this number there are 55 optional segregates, so that the minimum number of families in this system is 402. In APG III (2009) there are 415 families.

The diversity of flowering plants is not evenly distributed. Nearly all species belong to the eudicot (75%), monocot (23%), and magnoliid (2%) clades. The remaining 5 clades contain a little over 250 species in total; i.e. less than 0.1% of flowering plant diversity, divided among 9 families. The 43 most-diverse of 443 families of flowering plants by species, in their APG circumscriptions, are
Of these, the Orchidaceae, Poaceae, Cyperaceae, Araceae, Bromeliaceae, Arecaceae, and Iridaceae are monocot families; Piperaceae, Lauraceae, and Annonaceae are magnoliid dicots; the rest of the families are eudicots.

Double fertilization refers to a process in which two sperm cells fertilize cells in the ovule. This process begins when a pollen grain adheres to the stigma of the pistil (female reproductive structure), germinates, and grows a long pollen tube. While this pollen tube is growing, a haploid generative cell travels down the tube behind the tube nucleus. The generative cell divides by mitosis to produce two haploid ("n") sperm cells. As the pollen tube grows, it makes its way from the stigma, down the style and into the ovary. Here the pollen tube reaches the micropyle of the ovule and digests its way into one of the synergids, releasing its contents (which include the sperm cells). The synergid that the cells were released into degenerates and one sperm makes its way to fertilize the egg cell, producing a diploid (2"n") zygote. The second sperm cell fuses with both central cell nuclei, producing a triploid (3"n") cell. As the zygote develops into an embryo, the triploid cell develops into the endosperm, which serves as the embryo's food supply. The ovary will now develop into a fruit and the ovule will develop into a seed.
As the development of embryo and endosperm proceeds within the embryo sac, the sac wall enlarges and combines with the nucellus (which is likewise enlarging) and the integument to form the "seed coat". The ovary wall develops to form the fruit or pericarp, whose form is closely associated with type of seed dispersal system.

Frequently, the influence of fertilization is felt beyond the ovary, and other parts of the flower take part in the formation of the fruit, e.g., the floral receptacle in the apple, strawberry, and others.

The character of the seed coat bears a definite relation to that of the fruit. They protect the embryo and aid in dissemination; they may also directly promote germination. Among plants with indehiscent fruits, in general, the fruit provides protection for the embryo and secures dissemination. In this case, the seed coat is only slightly developed. If the fruit is dehiscent and the seed is exposed, in general, the seed-coat is well developed, and must discharge the functions otherwise executed by the fruit.

Flowering plants generate gametes using a specialized cell division called meiosis. Meiosis takes place in the ovule (a structure within the ovary that is located within the pistil at the center of the flower) (see diagram labeled "Angiosperm lifecycle"). A diploid cell (megaspore mother cell) in the ovule undergoes meiosis (involving two successive cell divisions) to produce four cells (megaspores) with haploid nuclei. One of these four cells (megaspore) then undergoes three successive mitotic divisions to produce an immature embryo sac (megagametophyte) with eight haploid nuclei. Next, these nuclei are segregated into separate cells by cytokinesis to producing 3 antipodal cells, 2 synergid cells and an egg cell. Two polar nuclei are left in the central cell of the embryo sac.

Pollen is also produced by meiosis in the male anther (microsporangium). During meiosis, a diploid microspore mother cell undergoes two successive meiotic divisions to produce 4 haploid cells (microspores or male gametes). Each of these microspores, after further mitoses, becomes a pollen grain (microgametophyte) containing two haploid generative (sperm) cells and a tube nucleus. When a pollen grain makes contact with the female stigma, the pollen grain forms a pollen tube that grows down the style into the ovary. In the act of fertilization, a male sperm nucleus fuses with the female egg nucleus to form a diploid zygote that can then develop into an embryo within the newly forming seed. Upon germination of the seed, a new plant can grow and mature.

The adaptive function of meiosis is currently a matter of debate. A key event during meiosis in a diploid cell is the pairing of homologous chromosomes and homologous recombination (the exchange of genetic information) between homologous chromosomes. This process promotes the production of increased genetic diversity among progeny and the recombinational repair of damages in the DNA to be passed on to progeny. To explain the adaptive function of meiosis in flowering plants, some authors emphasize diversity and others emphasize DNA repair.

Apomixis (reproduction via asexually formed seeds) is found naturally in about 2.2% of angiosperm genera. One type of apomixis, gametophytic apomixis found in a dandelion species involves formation of an unreduced embryo sac due to incomplete meiosis (apomeiosis) and development of an embryo from the unreduced egg inside the embryo sac, without fertilization (parthenogenesis).

Agriculture is almost entirely dependent on angiosperms, which provide virtually all plant-based food, and also provide a significant amount of livestock feed. Of all the families of plants, the Poaceae, or grass family (providing grains), is by far the most important, providing the bulk of all feedstocks (rice, maize, wheat, barley, rye, oats, pearl millet, sugar cane, sorghum). The Fabaceae, or legume family, comes in second place. Also of high importance are the Solanaceae, or nightshade family (potatoes, tomatoes, and peppers, among others); the Cucurbitaceae, or gourd family (including pumpkins and melons); the Brassicaceae, or mustard plant family (including rapeseed and the innumerable varieties of the cabbage species "Brassica oleracea"); and the Apiaceae, or parsley family. Many of our fruits come from the Rutaceae, or rue family (including oranges, lemons, grapefruits, etc.), and the Rosaceae, or rose family (including apples, pears, cherries, apricots, plums, etc.).

In some parts of the world, certain single species assume paramount importance because of their variety of uses, for example the coconut ("Cocos nucifera") on Pacific atolls, and the olive ("Olea europaea") in the Mediterranean region.

Flowering plants also provide economic resources in the form of wood, paper, fiber (cotton, flax, and hemp, among others), medicines (digitalis, camphor), decorative and landscaping plants, and many other uses. The main area in which they are surpassed by other plants—namely, coniferous trees (Pinales), which are non-flowering (gymnosperms)—is timber and paper production.





</doc>
<doc id="18968" url="https://en.wikipedia.org/wiki?curid=18968" title="Malvales">
Malvales

The Malvales are an order of flowering plants. As circumscribed by APG II-system, the order includes about 6000 species within 9 families. The order is placed in the eurosids II, which are part of the eudicots.

The plants are mostly shrubs and trees; most of its families have a cosmopolitan distribution in the tropics and subtropics, with limited expansion into temperate regions. An interesting distribution occurs in Madagascar, where three endemic families of Malvales (Sphaerosepalaceae, Sarcolaenaceae and Diegodendraceae) occur.

Many species of Malvaceae "sensu lato" are known for their wood, with that of "Ochroma" (balsa) being known for its lightness, and that of "Tilia" (lime, linden, or basswood) as a popular wood for carving. Fruit of the cacao tree ("Theobroma cacao") are used as an ingredient for chocolate. Kola nuts (genus "Cola") are notable for their high content of caffeine, and in past were commonly used for preparing of various cola drinks. Other well-known members of Malvales in the APG II sense are daphnes, hibiscus, hollyhocks, okra, baobab trees, cotton, and kapok.

The morphology of Malvales is diverse, with few common characteristics. Among those most commonly encountered are palmate leaves, connate sepals, and a specific structure and chemical composition of the seeds. The cortex is often fibrous, built of soft phloem layers.

Early classifications such as that of Dahlgren placed the Malvales in the superorder Malviflorae (also called Malvanae). Family boundaries and circumscriptions of the "core" Malvales families, Malvaceae, Bombacaceae, Tiliaceae, and Sterculiaceae, have long been problematic. A close relationship among these families, and particularly Malvaceae and Bombacaceae, has generally been recognized, although until recently most classification systems have maintained them as separate families. With numerous molecular phylogenies showing Sterculiaceae, Bombacaceae, and Tiliaceae as traditionally defined are either paraphyletic or polyphyletic, a consensus has been emerging for a trend to expand Malvaceae to include these three families. This expanded circumscription of Malvaceae has been recognized in the most recent version of the Thorne system, by the Angiosperm Phylogeny Group, and in the most recent comprehensive treatment of vascular plant families and genera, the Kubitzki system.

The dominant family in the APG II-system is the extended Malvaceae (Malvaceae "sensu lato") with over 4000 species, followed by Thymelaeaceae with 750 species. This expanded circumscription of Malvaceae is taken to include the families Bombacaceae, Sterculiaceae and Tiliaceae. Under the older Cronquist system the order contained these four "core Malvales" families plus the Elaeocarpaceae and was placed among the Dilleniidae. Some of the currently included families were placed by Cronquist in the Violales.



</doc>
<doc id="18969" url="https://en.wikipedia.org/wiki?curid=18969" title="Myrtales">
Myrtales

The Myrtales are an order of flowering plants placed as a sister to the eurosids II clade as of the publishing of the "Eucalyptus grandis" genome in June 2014. The APG III system of classification for angiosperms still places it within the eurosids. The following families are included as of APGIII:


The Cronquist system gives essentially the same composition, except the Vochysiaceae are removed to the order Polygalales, and the Thymelaeaceae are included. The families Sonneratiaceae, Trapaceae, and Punicaceae are removed from the Lythraceae. In the classification system of Dahlgren the Myrtales were in the superorder Myrtiflorae (also called Myrtanae). The APGIII system agrees with the older Cronquist circumscriptions of treating Psiloxylaceae and Heteropyxidaceae within Myrtaceae, and Memecyclaceae within Melastomataceae.

Ellagitannins are reported in dicotyledoneous angiospermes, and notably in species in the order Myrtales.

Myrtales is dated to have begun 89–99 million years ago (mya) in Australasia. There is some contention as to that date however, which was obtained using nuclear DNA. When looking at chloroplast DNA, the myrtales ancestor is instead considered to have evolved in the mid-Cretaceous period (100mya) in Southeast Africa, rather than in Australasia. Although the APG system classifies myrtales as within the eurosids, the recently published genome of "Eucalyptus grandis" places the order myrtales as a sister to the eurosids rather than inside them. The discrepancy is thought to have arisen due to the difference between using a large number of taxa versus using a large number of genes for constructing a phylogeny.



</doc>
<doc id="18970" url="https://en.wikipedia.org/wiki?curid=18970" title="Malpighiales">
Malpighiales

The Malpighiales comprise one of the largest orders of flowering plants, containing about species, about 7.8% of the eudicots. The order is very diverse, containing plants as different as the willow, violet, poinsettia, and coca plant, and are hard to recognize except with molecular phylogenetic evidence. It is not part of any of the classification systems based only on plant morphology. Molecular clock calculations estimate the origin of stem group Malpighiales at around 100 million years ago (Mya) and the origin of crown group Malpighiales at about 90 Mya.

The Malpighiales are divided into 32 to 42 families, depending upon which clades in the order are given the taxonomic rank of family. In the APG III system, 35 families are recognized. Medusagynaceae, Quiinaceae, Peraceae, Malesherbiaceae, Turneraceae, Samydaceae, and Scyphostegiaceae are consolidated into other families. The largest family, by far, is the Euphorbiaceae, with about 6300 species in about 245 genera.

In a 2009 study of DNA sequences of 13 genes, 42 families were placed into 16 groups, ranging in size from one to 10 families. Almost nothing is known about the relationships among these 16 groups. Malpighiales and Lamiales are the two large orders whose phylogeny remains mostly unresolved.

Malpighiales is a member of a supraordinal group called the COM clade, which consists of the orders Celastrales, Oxalidales, and Malpighiales. Some describe it as containing a fourth order, Huales, separating the family Huaceae into its own order, separate from Oxalidales.

Some recent studies have placed Malpighiales as sister to Oxalidales "sensu lato" (including Huaceae), while others have found a different topology for the COM clade.

The COM clade is part of an unranked group known as Fabidae or eurosids I. The fabids, in turn, are part of a group that has long been recognized, namely, the rosids.

The great French botanist Charles Plumier named the genus "Malpighia" in honor of Marcello Malpighi's work on plants; "Malpighia" is the type genus for the Malpighiaceae, a family of tropical and subtropical flowering plants.

The family Malpighiaceae was the type family for one of the orders created by Jussieu in his 1789 work "Genera Plantarum". Friedrich von Berchtold and Jan Presl described such an order in 1820. Unlike modern taxonomists, these authors did not use the suffix "ales" in naming their orders. The name "Malpighiales" is attributed by some to Carl von Martius. In the 20th century, it was usually associated with John Hutchinson, who used it in all three editions of his book, "The Families of Flowering Plants". The name was not used by those who wrote later, in the 1970s, '80s, and '90s.

The taxon was largely presaged by Hans Hallier in 1912 in an article in the "Archiv. Néerl. Sci. Exact. Nat." titled "L'Origine et le système phylétique des angiospermes", in which his Passionales and Polygalinae were derived from Linaceae (in Guttales), with Passionales containing seven (of eight) families that also appear in the current Malpighiales, namely Passifloraceae, Salicaceae, Euphorbiaceae, Achariaceae, Flacourtiaceae, Malesherbiaceae, and Turneraceae, and Polygalinae containing four (of 10) families that also appear in the current Malpighiales, namely Malpighiaceae, Violaceae, Dichapetalaceae, and Trigoniaceae.

The first semblance of Malpighiales as now known came from a phylogeny of seed plants published in 1993 and based upon DNA sequences of the gene "rbcL". This study recovered a group of rosids unlike any group found in any previous system of plant classification. To make a clear break with classification systems being used at that time, the Angiosperm Phylogeny Group resurrected Hutchinson's name, though his concept of Malpighiales included much of what is now in Celastrales and Oxalidales.

Malpighiales is monophyletic and in molecular phylogenetic studies, it receives strong statistical support. Since the APG II system was published in 2003, minor changes to the circumscription of the order have been made. The family Peridiscaceae has been expanded from two genera to three, and then to four, and transferred to Saxifragales.

The genera "Cyrillopsis" (Ixonanthaceae), "Centroplacus" (Centroplacaceae), "Bhesa" (Centroplacaceae), "Aneulophus" (Erythroxylaceae), "Ploiarium" (Bonnetiaceae), "Trichostephanus" (Samydaceae), "Sapria" (Rafflesiaceae), "Rhizanthes" (Rafflesiaceae), and "Rafflesia" (Rafflesiaceae) had been either added or confirmed as members of Malpighiales by the end of 2009.

Some family delimitations have changed, as well, most notably, the segregation of Calophyllaceae from Clusiaceae "sensu lato" when it was shown that the latter is paraphyletic. Some differences of opinion on family delimitation exist, as well. For example, Samydaceae and Scyphostegiaceae may be recognized as families or included in a large version of Salicaceae.

The group is difficult to characterize phenotypically, although members often have dentate leaves, with the teeth having a single vein running into a congested and often deciduous apex (i.e., violoid, salicoid, or theoid). Also, zeylanol has recently been discovered in "Balanops" and "Dichapetalum" which are in the balanops clade (so-called Chrysobalanaceae s. l.). The so-called parietal suborder (the clusioid clade and Ochnaceae s. l. were also part of Parietales) corresponds with the traditional Violales as 8 (Achariaceae, Violaceae, Flacourtiaceae, Lacistemataceae, Scyphostegiaceae, Turneraceae, Malesherbiaceae, and Passifloraceae) of the order's 10 families along with Salicaceae, which have usually been assigned as a related order or suborder, are in this most derived malpighian suborder, so that eight of the 10 families of this suborder are Violales. The family Flacourtiaceae has proven to be polyphyletic as the cyanogenic members have been placed in Achariaceae and the ones with salicoid teeth were transferred to Salicaceae.

As of 2009, the phylogeny of Malpighiales is, at its deepest level, an unresolved polytomy of 16 clades. It has been estimated that complete resolution of the phylogeny will require at least 25000 base pairs of DNA sequence data per taxon. A similar situation exists with Lamiales and it has been analyzed in some detail. The phylogenetic tree shown below is from Wurdack and Davis (2009). The statistical support for each branch is 100% bootstrap percentage and 100% posterior probability, except where labeled, with bootstrap percentage followed by posterior probability.

In 2012, "Xi et al." managed to obtain a more resolved phylogenetic tree than previous studies through the use of data from a large number of genes. They included analyses of 82 plastid genes from 58 species (they ignored the problematic Rafflesiaceae), using partitions identified a posteriori by applying a Bayesian mixture model. Xi "et al." identified 12 additional clades and three major, basal clades.


</doc>
<doc id="18972" url="https://en.wikipedia.org/wiki?curid=18972" title="Miranda warning">
Miranda warning

In the United States, the Miranda" warning is a type of notification customarily given by police to criminal suspects in police custody (or in a custodial interrogation) advising them of their right to silence; that is, their right to refuse to answer questions or provide information to law enforcement or other officials. These rights are often referred to as Miranda" rights. The purpose of such notification is to preserve the admissibility of their statements made during custodial interrogation in later criminal proceedings.

A typical "Miranda" warning can read as follows:

You have the right to remain silent. Anything you say can and will be used against you in a court of law. You have the right to have an attorney. If you cannot afford one, one will be appointed to you by the court. With these rights in mind, are you still willing to talk with me about the charges against you?

The "Miranda" warning is part of a preventive criminal procedure rule that law enforcement are required to administer to protect an individual who is in custody and subject to direct questioning or its functional equivalent from a violation of his or her Fifth Amendment right against compelled self-incrimination. In "Miranda v. Arizona" (1966), the Supreme Court held that the admission of an elicited incriminating statement by a suspect not informed of these rights violates the Fifth Amendment and the Sixth Amendment right to counsel, through the incorporation of these rights into state law. Thus, if law enforcement officials decline to offer a "Miranda" warning to an individual in their custody, they may interrogate that person and act upon the knowledge gained, but may not use that person's statements as evidence against him or her in a criminal trial.

The concept of ""Miranda" rights" was enshrined in U.S. law following the 1966 "Miranda v. Arizona" Supreme Court decision, which found that the Fifth and Sixth Amendment rights of Ernesto Arturo Miranda had been violated during his arrest and trial for armed robbery, kidnapping, and rape of a mentally handicapped young woman (Miranda was subsequently retried and convicted, based primarily on his estranged ex-partner, who had been tracked down by the original arresting officer via Miranda's own parents, suddenly claiming that Miranda had confessed to her when she had visited him in jail; Miranda's lawyer later confessed that he 'goofed' the trial).

The circumstances triggering the Miranda safeguards, i.e. Miranda rights, are "custody" and "interrogation". Custody means formal arrest or the deprivation of freedom to an extent associated with formal arrest. Interrogation means explicit questioning or actions that are reasonably likely to elicit an incriminating response. The Supreme Court did not specify the exact wording to use when informing a suspect of his/her rights. However, the Court did create a set of guidelines that must be followed. The ruling states:

In "Berkemer v. McCarty" (1984), the Supreme Court decided that a person subjected to custodial interrogation is entitled to the benefit of the procedural safeguards enunciated in "Miranda", regardless of the nature or severity of the offense of which he is suspected or for which he was arrested.

As a result, American English developed the verb "Mirandize", meaning "read the "Miranda" rights to" a suspect (when the suspect is arrested).

Notably, the "Miranda" rights do not have to be read in any particular order, and they do not have to precisely match the language of the "Miranda" case as long as they are adequately and fully conveyed ("California v. Prysock", ).

In "Berghuis v. Thompkins" (2010), the Supreme Court held that unless a suspect expressly states that he or she is invoking this right, subsequent voluntary statements made to an officer can be used against them in court, and police can continue to interact with (or question) the alleged criminal.

Every U.S. jurisdiction has its own regulations regarding what, precisely, must be said to a person arrested or placed in a custodial situation. The typical warning states:

The courts have since ruled that the warning must be "meaningful", so it is usually required that the suspect be asked if he/she understands their rights. Sometimes, firm answers of "yes" are required. Some departments and jurisdictions require that an officer ask "do you understand?" after every sentence in the warning. An arrestee's silence is not a waiver, but on June 1, 2010, the Supreme Court ruled 5–4 that police are allowed to interrogate suspects who have invoked or waived their rights ambiguously, and any statement given during questioning prior to invocation or waiving is admissible as evidence. Evidence has in some cases been ruled inadmissible because of an arrestee's poor knowledge of English and the failure of arresting officers to provide the warning in the arrestee's language.

While the exact language above is not required by "Miranda", the police must advise the suspect that:
There is no precise language that must be used in advising a suspect of their Miranda rights. The point is that whatever language is used the substance of the rights outlined above must be communicated to the suspect. The suspect may be advised of their rights orally or in writing. Also, officers must make sure the suspect understands what the officer is saying, taking into account potential education levels. It may be necessary to "translate" to the suspect's level of understanding. Courts have ruled this admissible as long as the original waiver is said and the "translation" is recorded either on paper or on tape.

The Supreme Court has resisted efforts to require officers to more fully advise suspects of their rights. For example, the police are not required to advise the suspect that they can stop the interrogation at any time, that the decision to exercise the right cannot be used against the suspect, or that they have a right to talk to a lawyer before being asked any questions. Nor have the courts required to explain the rights. For example, the standard Miranda right to counsel states "You have a right to have an attorney present during the questioning". Police are not required to explain that this right is not merely a right to have a lawyer present while the suspect is being questioned. The right to counsel includes:

The circumstances triggering the "Miranda" safeguards, i.e. Miranda warnings, are "custody" and "interrogation". Custody means formal arrest or the deprivation of freedom to an extent associated with formal arrest. Interrogation means explicit questioning or actions that are reasonably likely to elicit an incriminating response. Suspects in "custody" who are about to be interrogated must be properly advised of their Miranda rights—namely, the Fifth Amendment right against compelled self incrimination (and, in furtherance of this right, the right to counsel while in custody). The Sixth Amendment right to counsel means that the suspect has the right to consult with an attorney before questioning begins and have an attorney present during the interrogation. The Fifth Amendment right against compelled self incrimination is the right to remain silent—the right to refuse to answer questions or to otherwise communicate information.

The duty to warn only arises when police officers conduct custodial interrogations. The Constitution does not require that a defendant be advised of the Miranda rights as part of the arrest procedure, or once an officer has probable cause to arrest, or if the defendant has become a suspect of the focus of an investigation. Custody and interrogation are the events that trigger the duty to warn.

Some jurisdictions provide the right of a juvenile to remain silent if their parent or guardian is not present. Some departments in New Jersey, Nevada, Oklahoma, and Alaska modify the "providing an attorney" clause as follows: 

Even though this sentence may be somewhat ambiguous to some laypersons, who can, and who "have" actually interpreted it as meaning that they will not get a lawyer until they confess and are arraigned in court, the U.S. Supreme Court has approved of it as an accurate description of the procedure in those states.

In states bordering Mexico, including Texas, New Mexico, Arizona, and California, suspects who are not United States citizens are given an additional warning:

Some states including Virginia require the following sentence, ensuring that the suspect knows that waiving Miranda rights is not a one-time absolute occurrence:

California, Texas, New York, Florida, Illinois, North Carolina, South Carolina, Virginia, Washington and Pennsylvania also add the following questions, presumably to comply with the Vienna Convention on Consular Relations:

An affirmative answer to both of the above questions waives the rights. If the suspect responds "no" to the first question, the officer is required to re-read the Miranda warning, while saying "no" to the second question invokes the right at that moment; in either case the interviewing officer or officers cannot question the suspect until the rights are waived.

Generally, when defendants invoke their Fifth Amendment right against self-incrimination and refuse to testify or submit to cross-examination at trial, the prosecutor cannot indirectly punish them for the exercise of a constitutional right by commenting on their silence and insinuating that it is an implicit admission of guilt. Since "Miranda" rights are simply a judicial gloss upon the Fifth Amendment which protects against coercive interrogations, the same rule also prevents prosecutors from commenting about the post-arrest silence of suspects who invoke their "Miranda" rights immediately after arrest. However, neither the Fifth Amendment nor "Miranda" extend to "pre-arrest" silence, which means that if a defendant takes the witness stand at trial (meaning he just waived his Fifth Amendment right to remain silent), the prosecutor can attack his credibility with his pre-arrest silence (based on his failure to immediately turn himself in and confess to the things he voluntarily testified about at trial).

Under the Uniform Code of Military Justice, Article 31 provides for the right against compelled self-incrimination. Interrogation subjects under Army jurisdiction must first be given Department of the Army Form 3881, which informs them of the charges and their rights, and the subjects must sign the form. The United States Navy and United States Marine Corps require that all arrested personnel be read the "rights of the accused" and must sign a form waiving those rights if they so desire; a verbal waiver is not sufficient.

It has been discussed whether a Miranda warning—if spoken or in writing—could be appropriately given to disabled persons. For example, "the right to remain silent" means little to a deaf individual and the word "constitutional" may not be understood by people with only an elementary education. In one case, a deaf murder suspect was kept at a therapy station until he was able to understand the meaning of the Miranda warning and other judicial proceedings.

The Miranda rule applies to the use of testimonial evidence in criminal proceedings that is the product of custodial police interrogation. The Miranda right to counsel and right to remain silent are derived from the self-incrimination clause of the Fifth Amendment. Therefore, for Miranda to apply, six requirements must be fulfilled:

Assuming that the six requirements are present and "Miranda" applies, the statement will be subject to suppression unless the prosecution can demonstrate:
and
The defendant may also be able to challenge the admissibility of the statement under provisions of state constitutions and state criminal procedure statutes.

It is important to note that immigrants who live in the United States illegally are also protected and should receive their Miranda warnings as well when being interrogated or placed under arrest. "Aliens receive constitutional protections when they have come within the territory of the United States and [have] developed substantial connections with this country".

The Fifth Amendment right to counsel, a component of the "Miranda" Rule, is different from the Sixth Amendment right to counsel. In the context of the law of confessions the Sixth Amendment right to counsel is defined by the Massiah Doctrine ("Massiah v. United States", 377 U.S. 201 (1964)).

Simply advising the suspect of their rights does not fully comply with the Miranda rule. The suspect must also voluntarily waive their Miranda rights before questioning can proceed. An express waiver is not necessary. However, most law enforcement agencies use written waiver forms. These include questions designed to establish that the suspect expressly waived their rights. Typical waiver questions are
and

The waiver must be "knowing and intelligent" and it must be "voluntary". These are separate requirements. To satisfy the first requirement the state must show that the suspect generally understood their rights (right to remain silent and right to counsel) and the consequences of forgoing those rights (that anything they said could be used against them in court). To show that the waiver was "voluntary" the state must show that the decision to waive the rights was not the product of police coercion. If police coercion is shown or evident, then the court proceeds to determine the voluntariness of the waiver under the totality of circumstances test focusing on the personal characteristics of the accused and the particulars of the coercive nature of the police conduct. The ultimate issue is whether the coercive police conduct was sufficient to overcome the will of a person under the totality of the circumstances. As noted previously, courts traditionally focused on two categories of factors in making this determination: (1) the personal characteristics of the suspect and (2) the circumstances attendant to the waiver. However, the Supreme Court significantly altered the voluntariness standard in the case of "Colorado v. Connelly". In "Connelly", the Court held that "Coercive police activity is a necessary predicate to a finding that a confession is not 'voluntary' within the meaning of the Due Process Clause of the Fourteenth Amendment." The Court has applied this same standard of voluntariness in determining whether a waiver of a suspect's Fifth Amendment Miranda rights was voluntary. Thus, a waiver of Miranda rights is voluntary unless the defendant can show that their decision to waive their rights and speak to the police was the product of police misconduct and coercion that overcame the defendant's free will. After "Connelly", the traditional totality of circumstances analysis is not even reached unless the defendant can first show such coercion by the police. Under "Connelly", a suspect's decisions need not be the product of rational deliberations. In addition to showing that the waiver was "voluntary", the prosecution must also show that the waiver was "knowing" and "intelligent". Essentially this means the prosecution must prove that the suspect had a basic understanding of their rights and an appreciation of the consequences of forgoing those rights. The focus of the analysis is directly on the personal characteristics of the suspect. If the suspect was under the influence of alcohol or other drugs, or suffered from an emotional or mental condition that substantially impaired their capacity to make rational decisions, the courts may well decide that the suspect's waiver was not knowing and intelligent.

A waiver must also be clear and unequivocal. An equivocal statement is ineffective as a waiver and the police may not proceed with the interrogation until the suspect's intentions are made clear. The requirement that a waiver be unequivocal must be distinguished from situations in which the suspect made an equivocal assertion of their Miranda rights after the interrogation began. Any post-waiver assertion of a suspect's Miranda rights must be clear and unequivocal. Any ambiguity or equivocation will be ineffective. If the suspect's assertion is ambiguous, the interrogating officers are permitted to ask questions to clarify the suspect's intentions, although they are not required to. In other words, if a suspect's assertion is ambiguous, the police may either attempt to clarify the suspect's intentions or they may simply ignore the ineffective assertion and continue with the interrogation. The timing of the assertion is significant. Requesting an attorney prior to arrest is of no consequence because "Miranda" applies only to custodial interrogations. The police may simply ignore the request and continue with the questioning; however, the suspect is also free to leave.

If the defendant asserts his right to remain silent all interrogation must immediately stop and the police may not resume the interrogation unless the police have "scrupulously honored" the defendant's assertion and obtain a valid waiver before resuming the interrogation. In determining whether the police "scrupulously honored" the assertion the courts apply a totality of the circumstances test. The most important factors are the length of time between termination of original interrogation and commencement of the second and a fresh set of Miranda warnings before resumption of interrogation.

The consequences of assertion of Sixth Amendment right to counsel are stricter. The police must immediately cease all interrogation and the police cannot reinitiate interrogation unless counsel is present (merely consulting with counsel is insufficient) or the defendant of his own volition contacts the police. If the defendant does reinitiate contact, a valid waiver must be obtained before interrogation may resume.

In "Berghuis v. Thompkins" (2010), the Supreme Court declared in a 5–4 decision that criminal defendants who have been read their "Miranda" rights (and who have indicated they understand them and have not already waived them), must explicitly state during or before an interrogation begins that they wish to be silent and not speak to police for that protection against self-incrimination to apply. If they speak to police about the incident before invoking the Miranda right to remain silent, or afterwards at any point during the interrogation or detention, the words they speak may be used against them if they have not stated they do not want to speak to police. Those who oppose the ruling contend that the requirement that the defendant must speak to indicate his intention to remain silent further erodes the ability of the defendant to stay completely silent about the case. This opposition must be put in context with the second option offered by the majority opinion, which allowed that the defendant had the option of remaining silent, saying: "Had he wanted to remain silent, he could have said nothing in response or unambiguously invoked his Miranda rights, ending the interrogation." Thus, having been "Mirandized", a suspect may avow explicitly the invocation of these rights, or, alternatively, simply remain silent. Absent the former, "anything [said] can and will be used against [the defendant] in a court of law".

Assuming that the six factors are present, the Miranda rule would apply unless the prosecution can establish that the statement falls within an exception to the Miranda rule. The three exceptions are:


Arguably only the last is a true exception—the first two can better be viewed as consistent with the "Miranda" factors. For example, questions that are routinely asked as part of the administrative process of arrest and custodial commitment are not considered "interrogation" under Miranda because they are not intended or likely to produce incriminating responses. Nonetheless, all three circumstances are treated as exceptions to the rule. The jail house informant exception applies to situations where the suspect does not know that he is speaking to a state-agent; either a police officer posing as a fellow inmate, a cellmate working as an agent for the state or a family member or friend who has agreed to cooperate with the state in obtaining incriminating information.

The "public safety" exception is a limited and case-specific exception, allowing certain unadvised statements (given without Miranda warnings) to be admissible into evidence at trial when they were elicited in circumstances where there was great danger to public safety; thus, the "Miranda" rule provides some elasticity.

The public safety exception derives from "New York v. Quarles" (1984), a case in which the Supreme Court considered the admissibility of a statement elicited by a police officer who apprehended a rape suspect who was thought to be carrying a firearm. The arrest took place during the middle of the night in a supermarket that was open to the public but apparently deserted except for the clerks at the checkout counter. When the officer arrested the suspect, he found an empty shoulder holster, handcuffed the suspect, and asked him where the gun was. The suspect nodded in the direction of the gun (which was near some empty cartons) and said, "The gun is over there". The Supreme Court found that such an unadvised statement was admissible in evidence because "[i]n a kaleidoscopic situation such as the one confronting these officers, where spontaneity rather than adherence to a police manual is necessarily the order of the day, the application of the exception we recognize today should not be made to depend on "post hoc" findings at a suppression hearing concerning the subjective motivation of the police officer". Thus, the jurisprudential rule of "Miranda" must yield in "a situation where concern for public safety must be paramount to adherence to the literal language of the prophylactic rules enunciated in Miranda".

Under this exception, to be admissible in the government's direct case at a trial, the questioning must not be "actually compelled by police conduct which overcame his will to resist", and must be focused and limited, involving a situation "in which police officers ask questions reasonably prompted by a concern for the public safety".

In 2010, the Federal Bureau of Investigation encouraged agents to use a broad interpretation of public safety-related questions in terrorism cases, stating that the "magnitude and complexity" of terrorist threats justified "a significantly more extensive public safety interrogation without Miranda warnings than would be permissible in an ordinary criminal case", continuing to list such examples as: "questions about possible impending or coordinated terrorist attacks; the location, nature and threat posed by weapons that might pose an imminent danger to the public; and the identities, locations, and activities or intentions of accomplices who may be plotting additional imminent attacks". A Department of Justice spokesman described this position as not altering the constitutional right, but as clarifying existing flexibility in the rule.

Prosecutors initially argued for this exception to be applied to the 16-hour interrogation of Dzhokhar Tsarnaev in connection with the 2013 Boston Marathon bombings. However, the exception was not considered by the court because the prosecutors later decided not to use any of that evidence in their case against Tsarnaev.

The New York Court of Appeals upheld the exception in a 2013 murder case, "People v Doll", where a man with blood on his clothes was detained and questioned.

The window of opportunity for the exception is small. Once the suspect is formally charged, the Sixth Amendment right to counsel would attach and surreptitious interrogation would be prohibited. The public safety exception applies where circumstances present a clear and present danger to the public's safety and the officers have reason to believe that the suspect has information that can end the emergency.

Assuming that a Miranda violation occurred—the six factors are present and no exception applies—the statement will be subject to suppression under the Miranda exclusionary rule. That is, if the defendant objects or files a motion to suppress, the exclusionary rule would prohibit the prosecution from offering the statement as proof of guilt. However, the statement can be used to impeach the defendant's testimony. Further, the fruit of the poisonous tree doctrine does not apply to Miranda violations. Therefore, the exclusionary rule exceptions, attenuation, independent source and inevitable discovery, do not come into play, and derivative evidence would be fully admissible. For example, suppose the police continue with a custodial interrogation after the suspect has asserted his right to silence. During his post-assertion statement the suspect tells the police the location of the gun he used in the murder. Using this information the police find the gun. Forensic testing identifies the gun as the murder weapon, and fingerprints lifted from the gun match the suspect's. The contents of the Miranda-defective statement could not be offered by the prosecution as substantive evidence, but the gun itself and all related forensic evidence could be used as evidence at trial.

Although the rules vary by jurisdiction, generally a person who wishes to contest the admissibility of evidence on the grounds that it was obtained in violation of his constitutional rights must comply with the following procedural requirements:


Failure to comply with a procedural requirement may result in summary dismissal of the motion. If the defendant meets the procedural requirement, the motion will normally be considered by the judge outside the presence of the jury. The judge hears evidence, determines the facts, makes conclusions of law and enters an order allowing or denying the motion.

In addition to Miranda, confession may be challenged under the Massiah Doctrine, the Voluntariness Standard, Provisions of Federal and State rules of criminal procedure and State Constitutional provisions.

The Massiah Doctrine (established by "Massiah v. United States") prohibits the admission of a confession obtained in violation of the defendant's Sixth Amendment right to counsel. Specifically, the Massiah rule applies to the use of testimonial evidence in criminal proceedings deliberately elicited by the police from a defendant after formal charges have been filed. The events that trigger the Sixth Amendment safeguards under "Massiah" are (1) the commencement of adversarial criminal proceedings and (2) deliberate elicitation of information from the defendant by governmental agents.

The Sixth Amendment guarantees a defendant a right to counsel in all criminal prosecutions. The purposes of the Sixth Amendment right to counsel are to protect a defendant's right to a fair trial and to assure that the adversarial system of justice functions properly by providing competent counsel as an advocate for the defendant in his contest against the "prosecutorial forces" of the state.

The Sixth Amendment right "attaches" once the government has committed itself to the prosecution of the case by the initiation of adversarial judicial proceedings "by way of formal charge, preliminary hearing, indictment, information or arraignment". Determining whether a particular event or proceeding constitutes the commencement of adversarial criminal proceedings requires both an examination of the rules of criminal procedure for the jurisdiction in which the crime is charged and the Supreme Courts cases dealing with the issue of when formal prosecution begins. Once adversarial criminal proceedings commence the right to counsel applies to all critical stages of the prosecution and investigation. A critical stage is "any stage of the prosecution, formal or informal, in court or out, where counsel's absence might derogate from the accused's right to a fair trial".

Government attempts to obtain incriminating statement related to the offense charged from the defendant by overt interrogation or surreptitious means is a critical stage and any information thus obtained is subject to suppression unless the government can show that an attorney was present or the defendant knowingly, voluntarily and intelligently waived his right to counsel.

Deliberate elicitation is defined as the intentional creation of circumstances by government agents that are likely to produce incriminating information from the defendant. Clearly express questioning (interrogation) would qualify but the concept also extends to surreptitious attempts to acquire information from the defendant through the use of undercover agents or paid informants.

The definition of "deliberate elicitation" is not the same as the definition of "interrogation" under the Miranda rule. Miranda interrogation includes express questioning and any actions or statements that an officer would reasonably foresee as likely to cause an incriminating response. Massiah applies to express questioning and any attempt to deliberately and intentionally obtain incriminating information from the defendant regarding the crime charged. The difference is purposeful creation of an environment likely to produce incriminating information (Massiah) and action likely to induce an incriminating response even if that was not the officer's purpose or intent (Miranda).

The Sixth Amendment right to counsel is offense-specific – the right only applies to post-commencement attempts to obtain information relating to the crime charged. The right does not extend to uncharged offenses if factually related to the charged crime.

As noted, information obtained in violation of the defendant's Sixth Amendment right to counsel is subject to suppression unless the government can establish that the defendant waived his right to counsel. The waiver must be knowing, intelligent and voluntary. A valid Miranda waiver operates as a waiver of Sixth Amendment right.


The voluntariness standard applies to all police interrogations regardless of the custodial status of the suspect and regardless of whether the suspect has been formally charged. The remedy for a violation of the standard is complete suppression of the statement and any evidence derived from the statement. The statement cannot be used as either substantive evidence of guilt or to impeach the defendant's testimony. The reason for the strictness is the common law's aversion to the use of coerced confessions because of their inherent unreliability. Further the rights to be free from coerced confession cannot be waived nor is it necessary that the victim of coercive police conduct assert his right. In considering the voluntariness standard one must consider the Supreme Court's decision in "Colorado v. Connelly". Although federal courts' application of the Connelly rule has been inconsistent and state courts have often failed to appreciate the consequences of the case, Connelly clearly marked a significant change in the application of the voluntariness standard. Before Connelly the test was whether the confession was voluntary considering the totality of the circumstances. "Voluntary" carried its everyday meaning: the confession had to be a product of the exercise of the defendant's free will rather than police coercion. After Connelly the totality of circumstances test is not even triggered unless the defendant can show coercive police conduct. Questions of free will and rational decision making are irrelevant to a due process claim unless police misconduct existed and a causal connection can be shown between the misconduct and the confession.

Every state constitution has articles and provision guaranteeing individual rights. In most cases the subject matter is similar to the federal bill of rights. Most state courts interpretation of their constitution is consistent with the interpretation federal court's of analogous provisions of the federal constitution. With regard to Miranda issues, state courts have exhibited significant resistance to incorporating into their state jurisprudence some of the limitations on the Miranda rule that have been created by the federal courts. As a consequence a defendant may be able to circumvent the federal limitation on the Miranda rule and successfully challenge the admissibility under state constitutional provisions. Practically every aspect of the Miranda rule has drawn state court criticism. However the primary point of contention involve the following limitations on the scope of the Miranda rule: (1) the Harris exception (2) the Burbine rule and (3) the Fare rule.

In addition to constitutionally based challenge, states permit a defendant to challenge the admissibility of a confession on the grounds that the confession was obtained in violation of a defendant's statutory rights. For example, North Carolina Criminal Procedure Act permits a defendant to move to suppress evidence obtained as a result of a "substantial" violation of the provision of the North Carolina Rules of Criminal Procedure.

Due to the prevalence of American television programs and motion pictures in which the police characters frequently read suspects their rights, it has become an expected element of arrest procedure—in the 2000 "Dickerson" decision, Chief Justice William Rehnquist wrote that Miranda warnings had "become embedded in routine police practice to the point where the warnings have become part of our national culture".

While arrests and interrogations can legally occur without the Miranda warning being given, this procedure would generally make the arrestee's pre-Miranda statements inadmissible at trial. (However, pursuant to the plurality opinion in "United States v. Patane", physical evidence obtained as a result of pre-Miranda statements may still be admitted. There was no majority opinion of the Court in that case.)

In some jurisdictions, a "detention" differs at law from an "arrest", and police are not required to give the Miranda warning until the person is arrested for a crime. In those situations, a person's statements made to police are generally admissible even though the person was not advised of their rights. Similarly, statements made while an arrest is in progress before the Miranda warning was given or completed are also generally admissible.

Because "Miranda" applies only to custodial interrogations, it does not protect detainees from standard booking questions such as name and address. Because it is a protective measure intended to safeguard the Fifth Amendment right against self-incrimination, it does not prevent the police from taking blood without a warrant from persons suspected of driving under the influence of alcohol. (Such evidence may be self-incriminatory, but are not considered statements of self-incrimination.)

If an inmate is in jail and invoked "Miranda" on one case, it is unclear whether this extends to any other cases that they may be charged with while in custody. For example: a subject is arrested, charged with cattle rustling, and is held in county jail awaiting trial. He invoked his Miranda rights on the cow case. While in custody, he is involved in a fight where a staff member loses his ability to walk. He speaks to the custodial staff regarding the fight without staff first invoking Miranda. It is unclear if this statement is admissible because of the original Miranda statement.

Many police departments give special training to interrogators with regard to the Miranda warning; specifically, how to influence a suspect's decision to waive the right. For instance, the officer may be required to specifically ask if the rights are understood and if the suspect wishes to talk. The officer is allowed, before asking the suspect a question, to speak at length about evidence collected, witness statements, etc. The officer will "then" ask if the suspect wishes to talk, and the suspect is then more likely to talk in an attempt to refute the evidence presented. Another tactic commonly taught is never to ask a question; the officer may simply sit the suspect down in an interrogation room, sit across from him and do paperwork, and wait for the suspect to begin talking. These tactics are intended to mitigate the restrictions placed on law officers against compelling a suspect to give evidence, and have stood up in court as valid lawful tactics. Nevertheless, such tactics are condemned by legal rights groups as deceptive.

In "Illinois v. Perkins", 496 U.S. 292 (1990), the United States Supreme Court held that undercover officers do not have to give suspects a "Miranda" warning prior to asking questions that may elicit incriminating responses. In this case, an undercover agent posed as an inmate and carried on a 35-minute conversation with another inmate that he suspected of committing a murder that was being investigated. During this conversation, the suspect implicated himself in the murder that the undercover agent was investigating.

The Supreme Court came to this conclusion despite the government's admission that a custodial interrogation had been conducted by a government agent.

Beginning in 2009, some detainees captured in Afghanistan have been read their Miranda rights by the FBI, according to Congressman Michael Rogers of Michigan, who claims to have witnessed this himself. According to the Justice Department, "There has been no policy change nor blanket instruction for FBI agents to Mirandize detainees overseas. While there have been specific cases in which FBI agents have Mirandized suspects overseas at both Bagram and in other situations, in order to preserve the quality of evidence obtained, there has been no overall policy change with respect to detainees."

Whether arising from their constitutions, common law, or statute, many nations recognize a defendant's right to silence. Those rights may be considerably more limited than those available to U.S. criminal defendants under the "Miranda" ruling.





</doc>
<doc id="18973" url="https://en.wikipedia.org/wiki?curid=18973" title="Moot">
Moot

Moot may refer to:




</doc>
<doc id="18974" url="https://en.wikipedia.org/wiki?curid=18974" title="Mississippian (geology)">
Mississippian (geology)

The Mississippian (also known as Lower Carboniferous or Early Carboniferous) is a subperiod in the geologic timescale or a subsystem of the geologic record. It is the earliest/lowermost of two subperiods of the Carboniferous period lasting from roughly 358.9 to 323.2 million years ago. As with most other geochronologic units, the rock beds that define the Mississippian are well identified, but the exact start and end dates are uncertain by a few million years. The Mississippian is so named because rocks with this age are exposed in the Mississippi River valley. 

The Mississippian was a period of marine transgression in the Northern Hemisphere: the sea level was so high that only the Fennoscandian Shield and the Laurentian Shield were dry land. The cratons were surrounded by extensive delta systems and lagoons, and carbonate sedimentation on the surrounding continental platforms, covered by shallow seas.

In North America, where the interval consists primarily of marine limestones, it is treated as a geologic period between the Devonian and the Pennsylvanian. During the Mississippian an important phase of orogeny occurred in the Appalachian Mountains. It is a major rock building period named for the exposures in the Mississippi Valley region. The USGS geologic time scale shows its relation to other periods.

In Europe, the Mississippian and Pennsylvanian are one more-or-less continuous sequence of lowland continental deposits and are grouped together as the Carboniferous system, and sometimes called the "Upper Carboniferous" and "Lower Carboniferous" instead.

In the official geologic timescale, the Mississippian is subdivided into three stages:

The lower two come from European stratigraphy, the top from Russian stratigraphy. Besides Europe and Russia, there are many local subdivisions that are used as alternatives for the international timescale. In the North American system, the Mississippian is subdivided into four stages:



</doc>
<doc id="18976" url="https://en.wikipedia.org/wiki?curid=18976" title="Meiosis">
Meiosis

Meiosis (; from Greek μείωσις, "meiosis", which means lessening) is a specialized type of cell division that reduces the chromosome number by half, creating four haploid cells, each genetically distinct from the parent cell that gave rise to them. This process occurs in all sexually reproducing single-celled and multicellular eukaryotes, including animals, plants, and fungi. Errors in meiosis resulting in aneuploidy are the leading known cause of miscarriage and the most frequent genetic cause of developmental disabilities.

In meiosis, DNA replication is followed by two rounds of cell division to produce four daughter cells, each with half the number of chromosomes as the original parent cell. The two meiotic divisions are known as Meiosis I and Meiosis II. Before meiosis begins, during S phase of the cell cycle, the DNA of each chromosome is replicated so that it consists of two identical sister chromatids, which remain held together through sister chromatid cohesion. This S-phase can be referred to as "premeiotic S-phase" or "meiotic S-phase". Immediately following DNA replication, meiotic cells enter a prolonged G2-like stage known as meiotic prophase. During this time, homologous chromosomes pair with each other and undergo genetic recombination, a programmed process in which DNA is cut and then repaired, which allows them to exchange some of their genetic information. A subset of recombination events results in crossovers, which create physical links known as chiasmata (singular: chiasma, for the Greek letter Chi (X)) between the homologous chromosomes. In most organisms, these links are essential to direct each pair of homologous chromosomes to segregate away from each other during Meiosis I, resulting in two haploid cells that have half the number of chromosomes as the parent cell. During Meiosis II, the cohesion between sister chromatids is released and they segregate from one another, as during mitosis. In some cases all four of the meiotic products form gametes such as sperm, spores, or pollen. In female animals, three of the four meiotic products are typically eliminated by extrusion into polar bodies, and only one cell develops to produce an ovum.

Because the number of chromosomes is halved during meiosis, gametes can fuse (i.e. fertilization) to form a diploid zygote that contains two copies of each chromosome, one from each parent. Thus, alternating cycles of meiosis and fertilization enable sexual reproduction, with successive generations maintaining the same number of chromosomes. For example, diploid human cells contain 23 pairs of chromosomes including 1 pair of sex chromosomes (46 total), half of maternal origin and half of paternal origin. Meiosis produces haploid gametes (ova or sperm) that contain one set of 23 chromosomes. When two gametes (an egg and a sperm) fuse, the resulting zygote is once again diploid, with the mother and father each contributing 23 chromosomes. This same pattern, but not the same number of chromosomes, occurs in all organisms that utilize meiosis.

Although the process of meiosis is related to the more general cell division process of mitosis, it differs in two important respects:
Meiosis begins with a diploid cell, which contains two copies of each chromosome, termed homologs. First, the cell undergoes DNA replication, so each homolog now consists of two identical sister chromatids. Then each set of homologs pair with each other and exchange DNA by homologous recombination leading to physical connections (crossovers) between the homologs. In the first meiotic division, the homologs are segregated to separate daughter cells by the spindle apparatus. The cells then proceed to a second division without an intervening round of DNA replication. The sister chromatids are segregated to separate daughter cells to produce a total of four haploid cells. Female animals employ a slight variation on this pattern and produce one large ovum and two small polar bodies. Because of recombination, an individual chromatid can consist of a new combination of maternal and paternal DNA, resulting in offspring that are genetically distinct from either parent. Furthermore, an individual gamete can include an assortment of maternal, paternal, and recombinant chromatids. This genetic diversity resulting from sexual reproduction contributes to the variation in traits upon which natural selection can act.

Meiosis uses many of the same mechanisms as mitosis, the type of cell division used by eukaryotes to divide one cell into two identical daughter cells. In some plants, fungi, and protists meiosis results in the formation of spores: haploid cells that can divide vegetatively without undergoing fertilization. Some eukaryotes, like bdelloid rotifers, do not have the ability to carry out meiosis and have acquired the ability to reproduce by parthenogenesis.

Meiosis does not occur in archaea or bacteria, which generally reproduce asexually via binary fission. However, a "sexual" process known as horizontal gene transfer involves the transfer of DNA from one bacterium or archaeon to another and recombination of these DNA molecules of different parental origin.

Meiosis was discovered and described for the first time in sea urchin eggs in 1876 by the German biologist Oscar Hertwig. It was described again in 1883, at the level of chromosomes, by the Belgian zoologist Edouard Van Beneden, in "Ascaris" roundworm eggs. The significance of meiosis for reproduction and inheritance, however, was described only in 1890 by German biologist August Weismann, who noted that two cell divisions were necessary to transform one diploid cell into four haploid cells if the number of chromosomes had to be maintained. In 1911 the American geneticist Thomas Hunt Morgan detected crossovers in meiosis in the fruit fly "Drosophila melanogaster", which helped to establish that genetic traits are transmitted on chromosomes.

The term "meiosis" (originally spelled "maiosis") is derived from the Greek word , meaning 'lessening'. It was introduced to biology by J.B. Farmer and J.E.S. Moore in 1905:
"We propose to apply the terms Maiosis or Maiotic phase to cover the whole series of nuclear changes included in the two divisions that were designated as Heterotype and Homotype by Flemming".

The term was linguistically corrected to "meiosis" by Koernicke (1905), and by Pantel and De Sinety (1906).

Meiosis is divided into meiosis I and meiosis II which are further divided into Karyokinesis I and Cytokinesis I and Karyokinesis II and Cytokinesis II respectively. The preparatory steps that lead up to meiosis are identical in pattern and name to interphase of the mitotic cell cycle. Interphase is divided into three phases:

Interphase is followed by meiosis I and then meiosis II. Meiosis I separates homologous chromosomes, each still made up of two sister chromatids, into two daughter cells, thus reducing the chromosome number by half. During meiosis II, sister chromatids decouple and the resultant daughter chromosomes are segregated into four daughter cells. For diploid organisms, the daughter cells resulting from meiosis are haploid and contain only one copy of each chromosome. In some species, cells enter a resting phase known as interkinesis between meiosis I and meiosis II.

Meiosis I and II are each divided into prophase, metaphase, anaphase, and telophase stages, similar in purpose to their analogous subphases in the mitotic cell cycle. Therefore, meiosis includes the stages of meiosis I (prophase I, metaphase I, anaphase I, telophase I) and meiosis II (prophase II, metaphase II, anaphase II, telophase II).

Meiosis generates gamete genetic diversity in two ways: (1) Law of Independent Assortment. The independent orientation of homologous chromosome pairs along the metaphase plate during metaphase I & orientation of sister chromatids in metaphase II, this is the subsequent separation of homologs and sister chromatids during anaphase I & II, it allows a random and independent distribution of chromosomes to each daughter cell (and ultimately to gametes); and (2) Crossing Over. The physical exchange of homologous chromosomal regions by homologous recombination during prophase I results in new combinations of DNA within chromosomes.

During meiosis, specific genes are more highly transcribed. In addition to strong meiotic stage-specific expression of mRNA, there are also pervasive translational controls (e.g. selective usage of preformed mRNA), regulating the ultimate meiotic stage-specific protein expression of genes during meiosis. Thus, both transcriptional and translational controls determine the broad restructuring of meiotic cells needed to carry out meiosis.

Meiosis I segregates homologous chromosomes, which are joined as tetrads (2n, 4c), producing two haploid cells (n chromosomes, 23 in humans) which each contain chromatid pairs (1n, 2c). Because the ploidy is reduced from diploid to haploid, meiosis I is referred to as a "reductional division". Meiosis II is an "equational division" analogous to mitosis, in which the sister chromatids are segregated, creating four haploid daughter cells (1n, 1c).
Prophase I is typically the longest phase of meiosis. During prophase I, homologous chromosomes pair and exchange DNA (homologous recombination). This often results in chromosomal crossover. This process is critical for pairing between homologous chromosomes and hence for accurate segregation of the chromosomes at the first meiosis division. The new combinations of DNA created during crossover are a significant source of genetic variation, and result in new combinations of alleles, which may be beneficial. The paired and replicated chromosomes are called bivalents or tetrads, which have two chromosomes and four chromatids, with one chromosome coming from each parent. The process of pairing the homologous chromosomes is called synapsis. At this stage, non-sister chromatids may cross-over at points called chiasmata (plural; singular chiasma). Prophase I has historically been divided into a series of substages which are named according to the appearance of chromosomes.

The first stage of prophase I is the "leptotene" stage, also known as "leptonema", from Greek words meaning "thin threads".In this stage of prophase I, individual chromosomes—each consisting of two sister chromatids—become "individualized" to form visible strands within the nucleus. The two sister chromatids closely associate and are visually indistinguishable from one another. During leptotene, lateral elements of the synaptonemal complex assemble. Leptotene is of very short duration and progressive condensation and coiling of chromosome fibers takes place.

The "zygotene" stage, also known as "zygonema", from Greek words meaning "paired threads", occurs as the chromosomes approximately line up with each other into homologous chromosome pairs. In some organisms, this is called the bouquet stage because of the way the telomeres cluster at one end of the nucleus. At this stage, the synapsis (pairing/coming together) of homologous chromosomes takes place, facilitated by assembly of central element of the synaptonemal complex. Pairing is brought about in a zipper-like fashion and may start at the centromere (procentric), at the chromosome ends (proterminal), or at any other portion (intermediate). Individuals of a pair are equal in length and in position of the centromere. Thus pairing is highly specific and exact. The paired chromosomes are called bivalent or tetrad chromosomes.

The "pachytene" stage ( ), also known as "pachynema", from Greek words meaning "thick threads". At this point a tetrad of the chromosomes has formed known as a bivalent. This is the stage when homologous recombination, including chromosomal crossover (crossing over), occurs. Nonsister chromatids of homologous chromosomes may exchange segments over regions of homology. Sex chromosomes, however, are not wholly identical, and only exchange information over a small region of homology. At the sites where exchange happens, chiasmata form. The exchange of information between the non-sister chromatids results in a recombination of information; each chromosome has the complete set of information it had before, and there are no gaps formed as a result of the process. Because the chromosomes cannot be distinguished in the synaptonemal complex, the actual act of crossing over is not perceivable through the microscope, and chiasmata are not visible until the next stage.

During the "diplotene" stage, also known as "diplonema", from Greek words meaning "two threads", the synaptonemal complex degrades and homologous chromosomes separate from one another a little. The chromosomes themselves uncoil a bit, allowing some transcription of DNA. However, the homologous chromosomes of each bivalent remain tightly bound at chiasmata, the regions where crossing-over occurred. The chiasmata remain on the chromosomes until they are severed at the transition to anaphase I.

In human fetal oogenesis, all developing oocytes develop to this stage and are arrested in prophase I before birth. This suspended state is referred to as the "dictyotene stage" or dictyate. It lasts until meiosis is resumed to prepare the oocyte for ovulation, which happens at puberty or even later.

Chromosomes condense further during the "diakinesis" stage, from Greek words meaning "moving through". This is the first point in meiosis where the four parts of the tetrads are actually visible. Sites of crossing over entangle together, effectively overlapping, making chiasmata clearly visible. Other than this observation, the rest of the stage closely resembles prometaphase of mitosis; the nucleoli disappear, the nuclear membrane disintegrates into vesicles, and the meiotic spindle begins to form.

During these stages, two centrosomes, containing a pair of centrioles in animal cells, migrate to the two poles of the cell. These centrosomes, which were duplicated during S-phase, function as microtubule organizing centers nucleating microtubules, which are essentially cellular ropes and poles. The microtubules invade the nuclear region after the nuclear envelope disintegrates, attaching to the chromosomes at the kinetochore. The kinetochore functions as a motor, pulling the chromosome along the attached microtubule toward the originating centrosome, like a train on a track. There are four kinetochores on each tetrad, but the pair of kinetochores on each sister chromatid fuses and functions as a unit during meiosis I.

Microtubules that attach to the kinetochores are known as "kinetochore microtubules". Other microtubules will interact with microtubules from the opposite centrosome: these are called "nonkinetochore microtubules" or "polar microtubules". A third type of microtubules, the aster microtubules, radiates from the centrosome into the cytoplasm or contacts components of the membrane skeleton.

Homologous pairs move together along the metaphase plate: As "kinetochore microtubules" from both centrosomes attach to their respective kinetochores, the paired homologous chromosomes align along an equatorial plane that bisects the spindle, due to continuous counterbalancing forces exerted on the bivalents by the microtubules emanating from the two kinetochores of homologous chromosomes. This attachment is referred to as a bipolar attachment. The physical basis of the independent assortment of chromosomes is the random orientation of each bivalent along the metaphase plate, with respect to the orientation of the other bivalents along the same equatorial line. The protein complex cohesin holds sister chromatids together from the time of their replication until anaphase. In mitosis, the force of kinetochore microtubules pulling in opposite directions creates tension. The cell senses this tension and does not progress with anaphase until all the chromosomes are properly bi-oriented. In meiosis, establishing tension requires at least one crossover per chromosome pair in addition to cohesin between sister chromatids.

Kinetochore microtubules shorten, pulling homologous chromosomes (which consist of a pair of sister chromatids) to opposite poles. Nonkinetochore microtubules lengthen, pushing the centrosomes farther apart. The cell elongates in preparation for division down the center. Unlike in mitosis, only the cohesin from the chromosome arms is degraded while the cohesin surrounding the centromere remains protected. This allows the sister chromatids to remain together while homologs are segregated.

The first meiotic division effectively ends when the chromosomes arrive at the poles. Each daughter cell now has half the number of chromosomes but each chromosome consists of a pair of chromatids. The microtubules that make up the spindle network disappear, and a new nuclear membrane surrounds each haploid set. The chromosomes uncoil back into chromatin. Cytokinesis, the pinching of the cell membrane in animal cells or the formation of the cell wall in plant cells, occurs, completing the creation of two daughter cells. Sister chromatids remain attached during telophase I.

Cells may enter a period of rest known as interkinesis or interphase II. No DNA replication occurs during this stage.

Meiosis II is the second meiotic division, and usually involves equational segregation, or separation of sister chromatids. Mechanically, the process is similar to mitosis, though its genetic results are fundamentally different. The end result is production of four haploid cells (n chromosomes, 23 in humans) from the two haploid cells (with n chromosomes, each consisting of two sister chromatids) produced in meiosis I. The four main steps of meiosis II are: prophase II, metaphase II, anaphase II, and telophase II.

In prophase II we see the disappearance of the nucleoli and the nuclear envelope again as well as the shortening and thickening of the chromatids. Centrosomes move to the polar regions and arrange spindle fibers for the second meiotic division.

In metaphase II, the centromeres contain two kinetochores that attach to spindle fibers from the centrosomes at opposite poles. The new equatorial metaphase plate is rotated by 90 degrees when compared to meiosis I, perpendicular to the previous plate.

This is followed by anaphase II, in which the remaining centromeric cohesin is cleaved allowing the sister chromatids to segregate. The sister chromatids by convention are now called sister chromosomes as they move toward opposing poles.

The process ends with telophase II, which is similar to telophase I, and is marked by decondensation and lengthening of the chromosomes and the disassembly of the spindle. Nuclear envelopes reform and cleavage or cell plate formation eventually produces a total of four daughter cells, each with a haploid set of chromosomes.

Meiosis is now complete and ends up with four new daughter cells.

Meiosis occurs in eukaryotic life cycles involving sexual reproduction, consisting of the constant cyclical process of meiosis and fertilization. This takes place alongside normal mitotic cell division. In multicellular organisms, there is an intermediary step between the diploid and haploid transition where the organism grows. At certain stages of the life cycle, germ cells produce gametes. Somatic cells make up the body of the organism and are not involved in gamete production.

Cycling meiosis and fertilization events produces a series of transitions back and forth between alternating haploid and diploid states. The organism phase of the life cycle can occur either during the diploid state ("diplontic" life cycle), during the haploid state ("haplontic" life cycle), or both ("haplodiplontic" life cycle, in which there are two distinct organism phases, one during the haploid state and the other during the diploid state). In this sense there are three types of life cycles that utilize sexual reproduction, differentiated by the location of the organism phase(s).

In the "diplontic life cycle" (with pre-gametic meiosis), of which humans are a part, the organism is diploid, grown from a diploid cell called the zygote. The organism's diploid germ-line stem cells undergo meiosis to create haploid gametes (the spermatozoa for males and ova for females), which fertilize to form the zygote. The diploid zygote undergoes repeated cellular division by mitosis to grow into the organism.

In the "haplontic life cycle" (with post-zygotic meiosis), the organism is haploid instead, spawned by the proliferation and differentiation of a single haploid cell called the gamete. Two organisms of opposing sex contribute their haploid gametes to form a diploid zygote. The zygote undergoes meiosis immediately, creating four haploid cells. These cells undergo mitosis to create the organism. Many fungi and many protozoa utilize the haplontic life cycle. 

Finally, in the "haplodiplontic life cycle" (with sporic or intermediate meiosis), the living organism alternates between haploid and diploid states. Consequently, this cycle is also known as the alternation of generations. The diploid organism's germ-line cells undergo meiosis to produce spores. The spores proliferate by mitosis, growing into a haploid organism. The haploid organism's gamete then combines with another haploid organism's gamete, creating the zygote. The zygote undergoes repeated mitosis and differentiation to become a diploid organism again. The haplodiplontic life cycle can be considered a fusion of the diplontic and haplontic life cycles.

Meiosis occurs in all animals and plants. The end result, the production of gametes with half the number of chromosomes as the parent cell, is the same, but the detailed process is different. In animals, meiosis produces gametes directly. In land plants and some algae, there is an alternation of generations such that meiosis in the diploid sporophyte generation produces haploid spores. These spores multiply by mitosis, developing into the haploid gametophyte generation, which then gives rise to gametes directly (i.e. without further meiosis). In both animals and plants, the final stage is for the gametes to fuse, restoring the original number of chromosomes.

In females, meiosis occurs in cells known as oocytes (singular: oocyte). Each primary oocyte divides twice in meiosis, unequally in each case. The first division produces a daughter cell, and a much smaller polar body which may or may not undergo a second division. In meiosis II, division of the daughter cell produces a second polar body, and a single haploid cell, which enlarges to become an ovum. Therefore, in females each primary oocyte that undergoes meiosis results in one mature ovum and one or two polar bodies.

Note that there are pauses during meiosis in females. Maturing oocytes are arrested in prophase I of meiosis I and lie dormant within a protective shell of somatic cells called the follicle. At the beginning of each menstrual cycle, FSH secretion from the anterior pituitary stimulates a few follicles to mature in a process known as folliculogenesis. During this process, the maturing oocytes resume meiosis and continue until metaphase II of meiosis II, where they are again arrested just before ovulation. If these oocytes are fertilized by sperm, they will resume and complete meiosis. During folliculogenesis in humans, usually one follicle becomes dominant while the others undergo atresia. The process of meiosis in females occurs during oogenesis, and differs from the typical meiosis in that it features a long period of meiotic arrest known as the dictyate stage and lacks the assistance of centrosomes.

In males, meiosis occurs during spermatogenesis in the seminiferous tubules of the testicles. Meiosis during spermatogenesis is specific to a type of cell called spermatocytes, which will later mature to become spermatozoa. Meiosis of primordial germ cells happens at the time of puberty, much later than in females. Tissues of the male testis suppress meiosis by degrading retinoic acid, a stimulator of meiosis. This is overcome at puberty when cells within seminiferous tubules called Sertoli cells start making their own retinoic acid. Sensitivity to retinoic acid is also adjusted by proteins called nanos and DAZL.

In female mammals, meiosis begins immediately after primordial germ cells migrate to the ovary in the embryo. It is retinoic acid, derived from the primitive kidney (mesonephros) that stimulates meiosis in ovarian oogonia. Tissues of the male testis suppress meiosis by degrading retinoic acid, a stimulator of meiosis. This is overcome at puberty when cells within seminiferous tubules called Sertoli cells start making their own retinoic acid.

The normal separation of chromosomes in meiosis I or sister chromatids in meiosis II is termed "disjunction". When the segregation is not normal, it is called "nondisjunction". This results in the production of gametes which have either too many or too few of a particular chromosome, and is a common mechanism for trisomy or monosomy. Nondisjunction can occur in the meiosis I or meiosis II, phases of cellular reproduction, or during mitosis.

Most monosomic and trisomic human embryos are not viable, but some aneuploidies can be tolerated, such as trisomy for the smallest chromosome, chromosome 21. Phenotypes of these aneuploidies range from severe developmental disorders to asymptomatic. Medical conditions include but are not limited to:

The probability of nondisjunction in human oocytes increases with increasing maternal age, presumably due to loss of cohesin over time.

Alongside with the variations of meiosis related to the moment when meiosis occur in life cycles, resulting in post-zygotic, pre-gametic and intermediate meiosis (see above), the number of nuclear divisions in meiosis is also variable. The majority of eukaryotes have a two-divisional meiosis (though sometimes achiasmatic), but a very rare form, one-divisional meiosis, occurs in some flagellates (parabasalids and oxymonads) from the gut of the wood-feeding cockroach "Cryptocercus".

In order to understand meiosis, a comparison to mitosis is helpful. The table below shows the differences between meiosis and mitosis.



</doc>
<doc id="18977" url="https://en.wikipedia.org/wiki?curid=18977" title="MINIX">
MINIX

MINIX (from "mini-Unix") is a POSIX-compliant (since version 2.0), Unix-like operating system based on a microkernel architecture.

Early versions of MINIX were created by Andrew S. Tanenbaum for educational purposes. Starting with MINIX 3, the primary aim of development shifted from education to the creation of a highly reliable and self-healing microkernel OS. MINIX is now developed as open-source software.

MINIX was first released in 1987, with its complete source code made available to universities for study in courses and research. It has been free and open-source software since it was re-licensed under the BSD license in April 2000.

Andrew S. Tanenbaum created MINIX at Vrije Universiteit in Amsterdam to exemplify the principles conveyed in his textbook, "" (1987).

An abridged 12,000 lines of the C source code of the kernel, memory manager, and file system of MINIX 1.0 are printed in the book. Prentice-Hall also released MINIX source code and binaries on floppy disk with a reference manual. MINIX 1 was system-call compatible with Seventh Edition Unix.

Tanenbaum originally developed MINIX for compatibility with the IBM PC and IBM PC/AT microcomputers available at the time.

MINIX 1.5, released in 1991, included support for MicroChannel IBM PS/2 systems and was also ported to the Motorola 68000 and SPARC architectures, supporting the Atari ST, Commodore Amiga, Apple Macintosh and Sun SPARCstation computer platforms. There were also unofficial ports to Intel 386 PC compatibles (in 32-bit protected mode), National Semiconductor NS32532, ARM and Inmos transputer processors. Meiko Scientific used an early version of MINIX as the basis for the MeikOS operating system for its transputer-based Computing Surface parallel computers. A version of MINIX running as a user process under SunOS and Solaris was also available, a simulator named SMX.

Demand for the 68k-based architectures waned, however, and MINIX 2.0, released in 1997, was only available for the x86 and Solaris-hosted SPARC architectures. It was the subject of the second edition of Tanenbaum's textbook, cowritten with Albert Woodhull and was distributed on a CD-ROM included with the book. MINIX 2.0 added POSIX.1 compliance, support for 386 and later processors in 32-bit mode and replaced the Amoeba network protocols included in MINIX 1.5 with a TCP/IP stack. Unofficial ports of MINIX 2.0.2 to the 68020-based ISICAD Prisma 700 workstation and the Hitachi SH3-based HP Jornada 680/690 personal digital assistant (PDA) were also developed.

Minix-vmd is a variant of MINIX 2 for Intel IA-32-compatible processors, created by two Vrije Universiteit researchers, which adds virtual memory and support for the X Window System.

MINIX 3 was publicly announced on 24 October 2005 by Andrew Tanenbaum during his keynote speech at the Association for Computing Machinery (ACM) Symposium on Operating Systems Principles (SOSP). Although it still serves as an example for the new edition of Tanenbaum and Woodhull's textbook, it is comprehensively redesigned to be "usable as a serious system on resource-limited and embedded computers and for applications requiring high reliability."

MINIX 3 currently supports IA-32 and ARM architecture systems. It is available in a Live CD format that allows it to be used on a computer without installing it on the hard drive, and in versions compatible with hardware emulating and virtualizing systems, including Bochs, QEMU, VMware Workstation/Fusion, VirtualBox, and Microsoft Virtual PC.

Version 3.1.5 was released 5 November 2009. It contains X11, emacs, vi, cc, gcc, perl, python, ash, bash, zsh, ftp, ssh, telnet, pine, and over 400 other common Unix utility programs. With the addition of X11, this version marks the transition away from a text-only system. It can also withstand driver crashes. In many cases it can automatically restart drivers without affecting running processes. In this way, MINIX is self-healing and can be used in applications demanding high reliability. MINIX 3 also has support for virtual memory management, making it suitable for desktop OS use. Desktop applications such as Firefox and OpenOffice.org are not yet available for MINIX 3 however.

As of version 3.2.0, the userland was mostly replaced by that of NetBSD and support from pkgsrc became possible, increasing the available software applications that MINIX can use. Clang replaced the prior compiler (with GCC optionally supported), and GDB, the GNU debugger, was ported.

MINIX 3.3.0, released in September 2014, brought ARM support. 

MINIX 3.4.0RC, Release Candidates became available in January 2016. however a stable release of MINIX 3.4.0 was not yet announced.

MINIX supports many programming languages, including C, C++, FORTRAN, Modula-2, Pascal, Perl, Python, and Tcl.

MINIX 3 still has an active development community with over 50 people attending MINIXCon 2016, a conference to discuss the history and future of MINIX.

All Intel chipsets post-2015 are running MINIX 3 internally as the software component of the Intel Management Engine.

Torvalds used and appreciated MINIX, but his design deviated from the MINIX architecture in significant ways, most notably by employing a monolithic kernel instead of a microkernel. This was disapproved of by Tanenbaum in the Tanenbaum–Torvalds debate. Tanenbaum explained again his rationale for using a microkernel in May 2006.

Early Linux kernel development was done on a MINIX host system, which led to Linux inheriting various features from MINIX, such as the MINIX file system.

In May 2004, Kenneth Brown of the Alexis de Tocqueville Institution made the accusation that major parts of the Linux kernel had been copied from the MINIX codebase, in a book named "Samizdat". These accusations were rebutted universally—most prominently by Andrew Tanenbaum, who strongly criticised Kenneth Brown and published a long rebuttal on his own personal Web site, also pointing out that Brown was funded by Microsoft.

At the time of MINIX's original development, its license was relatively liberal. Its licensing fee was very small ($69) relative to those of other operating systems. Tanenbaum wished for MINIX to be as accessible as possible to students, but his publisher was unwilling to offer material (such as the source code) that could be copied freely, so a restrictive license requiring a nominal fee (included in the price of Tanenbaum's book) was applied as a compromise. This prevented the use of MINIX as the basis for a freely distributed software system.

When free and open-source Unix-like operating systems such as Linux and 386BSD became available in the early 1990s, many volunteer software developers abandoned MINIX in favor of these. In April 2000, MINIX became free and open source software under a permissive free software license, but by this time other operating systems had surpassed its capabilities, and it remained primarily an operating system for students and hobbyists.




</doc>
<doc id="18982" url="https://en.wikipedia.org/wiki?curid=18982" title="Muscular dystrophy">
Muscular dystrophy

Muscular dystrophy (MD) is a group of muscle diseases that results in increasing weakening and breakdown of skeletal muscles over time. The disorders differ in which muscles are primarily affected, the degree of weakness, how fast they worsen, and when symptoms begin. Many people will eventually become unable to walk. Some types are also associated with problems in other organs.
The muscular dystrophy group contains thirty different genetic disorders which are usually classified into nine main categories or types. The most common type is Duchenne muscular dystrophy (DMD) which typically affects males beginning around the age of four. Other types include Becker muscular dystrophy, facioscapulohumeral muscular dystrophy, and myotonic dystrophy. They are due to mutations in genes that are involved in making muscle proteins. This can occur due to either inheriting the defect from one's parents or the mutation occurring during early development. Disorders may be X-linked recessive, autosomal recessive, or autosomal dominant. Diagnosis often involves blood tests and genetic testing.
There is no cure for muscular dystrophy. Physical therapy, braces, and corrective surgery may help with some symptoms. Assisted ventilation may be required in those with weakness of breathing muscles. Medications used include steroids to slow muscle degeneration, anticonvulsants to control seizures and some muscle activity, and immunosuppressants to delay damage to dying muscle cells. Outcomes depend on the specific type of disorder.
Duchenne muscular dystrophy, which represents about half of all cases of muscular dystrophy, affects about one in 5,000 males at birth. Muscular dystrophy was first described in the 1830s by Charles Bell. The word "dystrophy" is from the Greek "dys", meaning "difficult" and "troph" meaning "nourish". Gene therapy, as a treatment, is in the early stages of study in humans.

The signs and symptoms consistent with muscular dystrophy are:

These conditions are generally inherited, and the different muscular dystrophies follow various inheritance patterns. Muscular dystrophy can be inherited by individuals as an X-linked disorder, a recessive or dominant disorder. Furthermore, it can be a spontaneous mutation which means errors in the replication of DNA and spontaneous lesions. Spontaneous lesions are due to natural damage to DNA, where the most common are depurination and deamination.

Dystrophin protein is found in muscle fibre membrane; its helical nature allows it to act like a spring or shock absorber. Dystrophin links actin in the cytoskeleton and dystroglycans of the muscle cell plasma membrane, known as the sarcolemma (extracellular). In addition to mechanical stabilization, dystrophin also regulates calcium levels.

Recent studies on the interaction of proteins with missense mutations and its neighbors showed high degree of rigidity associated with central hub proteins involved in protein binding and flexible subnetworks having molecular functions involved with calcium.

The diagnosis of muscular dystrophy is based on the results of muscle biopsy, increased creatine phosphokinase (CpK3), electromyography, and genetic testing. A physical examination and the patient's medical history will help the doctor determine the type of muscular dystrophy. Specific muscle groups are affected by different types of muscular dystrophy.

Other tests that can be done are chest X-ray, echocardiogram, CT scan, and magnetic resonance image scan, which via a magnetic field can produce images whose detail helps diagnose muscular dystrophy. Quality of life can be measured using specific questionnaires.

Currently, there is no cure for muscular dystrophy. In terms of management, physical therapy, occupational therapy, orthotic intervention (e.g., ankle-foot orthosis), speech therapy, and respiratory therapy may be helpful. Low intensity corticosteroids such as prednisone, and deflazacort may help to maintain muscle tone. Orthoses (orthopedic appliances used for support) and corrective orthopedic surgery may be needed to improve the quality of life in some cases. The cardiac problems that occur with EDMD and myotonic muscular dystrophy may require a pacemaker. The myotonia (delayed relaxation of a muscle after a strong contraction) occurring in myotonic muscular dystrophy may be treated with medications such as quinine.

Occupational therapy assists the individual with MD to engage in activities of daily living (such as self-feeding and self-care activities) and leisure activities at the most independent level possible. This may be achieved with use of adaptive equipment or the use of energy-conservation techniques. Occupational therapy may implement changes to a person's environment, both at home or work, to increase the individual's function and accessibility; furthermore, it addresses psychosocial changes and cognitive decline which may accompany MD, and provides support and education about the disease to the family and individual.

Prognosis depends on the individual form of MD. In some cases, a person with a muscle disease will get progressively weaker to the extent that it shortens lifespan due to heart and breathing complications. However, some of the muscle diseases do not affect life expectancy at all, and ongoing research is attempting to find cures and treatments to slow muscle weakness.

In the 1860s, descriptions of boys who grew progressively weaker, lost the ability to walk, and died at an early age became more prominent in medical journals. In the following decade, French neurologist Guillaume Duchenne gave a comprehensive account of the most common and severe form of the disease, which now carries his name—Duchenne MD.

WHO International conducted trials on optimum steroid regimen for MD, in the UK in 2012. In terms of research within the United States, the primary federally funded organizations that focus on muscular dystrophy research, including gene therapy and regenerative medicine, are the National Institute of Neurological Disorders and Stroke, National Institute of Arthritis and Musculoskeletal and Skin Diseases, and National Institute of Child Health and Human Development.

In 1966, the Muscular Dystrophy Association began its annual "Jerry Lewis MDA Telethon", which has probably done more to raise awareness of muscular dystrophy than any other event or initiative. Disability rights advocates, however, have criticized the telethon for portraying victims of the disease as deserving pity rather than respect.

On December 18, 2001, the MD CARE Act was signed into law in the USA; it amends the Public Health Service Act to provide research for the various muscular dystrophies. This law also established the Muscular Dystrophy Coordinating Committee to help focus research efforts through a coherent research strategy.



</doc>
<doc id="18984" url="https://en.wikipedia.org/wiki?curid=18984" title="Mongols">
Mongols

The Mongols (, "Mongolchuud", ) are an East-Central Asian ethnic group native to Mongolia and China's Inner Mongolia Autonomous Region. They also live as minorities in other regions of China (e.g. Xinjiang), as well as in Russia. Mongolian people belonging to the Buryat and Kalmyk subgroups live predominantly in the Russian federal subjects of Buryatia and Kalmykia.

The Mongols are bound together by a common heritage and ethnic identity. Their indigenous dialects are collectively known as the Mongolian language. The ancestors of the modern-day Mongols are referred to as Proto-Mongols.

Broadly defined, the term includes the Mongols proper (also known as the Khalkha Mongols), Buryats, Oirats, the Kalmyk people and the Southern Mongols. The latter comprises the Abaga Mongols, Abaganar, Aohans, Baarins, Gorlos Mongols, Jalaids, Jaruud, Khishigten, Khuuchid, Muumyangan and Onnigud.

The designation "Mongol" briefly appeared in 8th century records of Tang China to describe a tribe of Shiwei. It resurfaced in the late 11th century during the Khitan-ruled Liao dynasty. After the fall of the Liao in 1125, the Khamag Mongols became a leading tribe on the Mongolian Plateau. However, their wars with the Jurchen-ruled Jin dynasty and the Tatar confederation had weakened them.

In the thirteenth century, the word Mongol grew into an umbrella term for a large group of Mongolic-speaking tribes united under the rule of Genghis Khan.

In various times Mongolic peoples have been equated with the Scythians, the Magog and the Tungusic peoples. Based on Chinese historical texts the ancestry of the Mongolic peoples can be traced back to the Donghu, a nomadic confederation occupying eastern Mongolia and Manchuria. The identity of the Xiongnu (Hünnü) is still debated today. Although some scholars maintain that they were proto-Mongols, they were more likely a multi-ethnic group of Mongolic and Turkic tribes. It has been suggested that the language of the Huns was related to the Hünnü.
The Donghu, however, can be much more easily labeled proto-Mongol since the Chinese histories trace only Mongolic tribes and kingdoms (Xianbei and Wuhuan peoples) from them, although some historical texts claim a mixed Xiongnu-Donghu ancestry for some tribes (e.g. the Khitan).

The Donghu are mentioned by Sima Qian as already existing in Inner Mongolia north of Yan in 699–632 BCE along with the Shanrong. Mentions in the "Yi Zhou Shu" ("Lost Book of Zhou") and the "Classic of Mountains and Seas" indicate the Donghu were also active during the Shang dynasty (1600–1046 BCE).

The Xianbei formed part of the Donghu confederation, but had earlier times of independence, as evidenced by a mention in the "Guoyu" ("晉語八" section), which states that during the reign of King Cheng of Zhou (reigned 1042–1021 BCE) they came to participate at a meeting of Zhou subject-lords at Qiyang (岐阳) (now Qishan County) but were only allowed to perform the fire ceremony under the supervision of Chu since they were not vassals by covenant (诸侯). The Xianbei chieftain was appointed joint guardian of the ritual torch along with Xiong Yi.

These early Xianbei came from the nearby Zhukaigou culture (2200–1500 BCE) in the Ordos Desert, where maternal DNA corresponds to the Mongol Daur people and the Tungusic Evenks. The Zhukaigou Xianbei (part of the Ordos culture of Inner Mongolia and northern Shaanxi) had trade relations with the Shang. In the late 2nd century, the Han dynasty scholar Fu Qian (服虔) wrote in his commentary "Jixie" (集解) that "Shanrong and Beidi are ancestors of the present-day Xianbei". Again in Inner Mongolia another closely connected core Mongolic Xianbei region was the Upper Xiajiadian culture (1000–600 BCE) where the Donghu confederation was centered.

After the Donghu were defeated by Xiongnu king Modu Chanyu, the Xianbei and Wuhuan survived as the main remnants of the confederation. Tadun Khan of the Wuhuan (died 207 AD) was the ancestor of the proto-Mongolic Kumo Xi. The Wuhuan are of the direct Donghu royal line and the "New Book of Tang" says that in 209 BCE, Modu Chanyu defeated the Wuhuan instead of using the word Donghu. The Xianbei, however, were of the lateral Donghu line and had a somewhat separate identity, although they shared the same language with the Wuhuan. In 49 CE the Xianbei ruler Bianhe (Bayan Khan?) raided and defeated the Xiongnu, killing 2000, after having received generous gifts from Emperor Guangwu of Han. The Xianbei reached their peak under Tanshihuai Khan (reigned 156–181) who expanded the vast, but short lived, Xianbei state (93–234).

Three prominent groups split from the Xianbei state as recorded by the Chinese histories: the Rouran (claimed by some to be the Pannonian Avars), the Khitan people and the Shiwei (a subtribe called the "Shiwei Menggu" is held to be the origin of the Genghisid Mongols). Besides these three Xianbei groups, there were others such as the Murong, Duan and Tuoba. Their culture was nomadic, their religion shamanism or Buddhism and their military strength formidable. There is still no direct evidence that the Rouran spoke Mongolic languages, although most scholars agree that they were Proto-Mongolic. The Khitan, however, had two scripts of their own and many Mongolic words are found in their half-deciphered writings.

Geographically, the Tuoba Xianbei ruled the southern part of Inner Mongolia and northern China, the Rouran (Yujiulü Shelun was the first to use the title khagan in 402) ruled eastern Mongolia, western Mongolia, the northern part of Inner Mongolia and northern Mongolia, the Khitan were concentrated in eastern part of Inner Mongolia north of Korea and the Shiwei were located to the north of the Khitan. These tribes and kingdoms were soon overshadowed by the rise of the Turkic Khaganate in 555, the Uyghur Khaganate in 745 and the Yenisei Kirghiz states in 840. The Tuoba were eventually absorbed into China. The Rouran fled west from the Göktürks and either disappeared into obscurity or, as some say, invaded Europe as the Avars under their Khan, Bayan I. Some Rouran under Tatar Khan migrated east, founding the Tatar confederation, who became part of the Shiwei. The Khitan, who were independent after their separation from the Kumo Xi (of Wuhuan origin) in 388, continued as a minor power in Manchuria until one of them, Ambagai (872–926), established the Liao dynasty (907–1125) as Emperor Taizu of Liao.

The destruction of Uyghur Khaganate by the Kirghiz resulted in the end of Turkic dominance in Mongolia. According to historians, Kirghiz were not interested in assimilating newly acquired lands; instead, they controlled local tribes through various manaps (tribal leader). The Khitans occupied the areas vacated by the Turkic Uyghurs bringing them under their control. The Yenisei Kirghiz state was centered on Khakassia and they were expelled from Mongolia by the Khitans in 924. Beginning in the 10th century, the Khitans, under the leadership of Abaoji, prevailed in a several military campaigns against the Tang Dynastys border guards, and the Xi, Shiwei and Jurchen nomadic groups.

The Khitan fled west after being defeated by the Jurchens (later known as Manchu) and founded the Qara Khitai (1125–1218) in eastern Kazakhstan. In 1218, Genghis Khan destroyed the Qara Khitai after which the Khitan passed into obscurity. With the expansion of the Mongol Empire, the Mongolic peoples settled over almost all Eurasia and carried on military campaigns from the Adriatic Sea to Indonesian Java island and from Japan to Palestine (Gaza). They simultaneously became Padishahs of Persia, Emperors of China, and Great Khans of Mongolia, and one became Sultan of Egypt (Al-Adil Kitbugha). The Mongolic peoples of the Golden Horde established themselves to govern Russia by 1240. By 1279, they conquered the Song dynasty and brought all of China under control of the Yuan dynasty.

With the breakup of the empire, the dispersed Mongolic peoples quickly adopted the mostly Turkic cultures surrounding them and were assimilated, forming parts of Azerbaijanis, Uzbeks, Karakalpaks, Tatars, Bashkirs, Turkmens, Uyghurs, Nogays, Kyrgyzs, Kazakhs, Caucasaus peoples, Iranian peoples and Moghuls; linguistic and cultural Persianization also began to be prominent in these territories. Some Mongols assimilated into the Yakuts after their migration to Northern Siberia and about 30% of Yakut words have Mongol origin. However, most of the Yuan Mongols returned to Mongolia in 1368, retaining their language and culture. There were 250,000 Mongols in Southern China and many Mongols were massacred by the rebel army. The survivors were trapped in southern china and eventually assimilated. The Dongxiangs, Bonans, Yugur and Monguor people were invaded by Chinese Ming dynasty.

After the fall of the Yuan dynasty in 1368, the Mongols continued to rule the Northern Yuan dynasty in Mongolia homeland. However, the Oirads began to challenge the Eastern Mongolic peoples under the Borjigin monarchs in the late 14th century and Mongolia was divided into two parts: Western Mongolia (Oirats) and Eastern Mongolia (Khalkha, Inner Mongols, Barga, Buryats). The earliest written references to the plough in Middle Mongolian language sources appear towards the end of the 14th c.

In 1434, Eastern Mongolian Taisun Khan's (1433–1452) prime minister Western Mongolian Togoon Taish reunited the Mongols after killing Eastern Mongolian another king Adai (Khorchin). Togoon died in 1439 and his son Esen Taish became prime minister.Esen carried out successful policy for Mongolian unification and independence. The Ming Empire attempted to invade Mongolia in the 14–16th centuries, however, the Ming Empire was defeated by the Oirat, Southern Mongol, Eastern Mongol and united Mongolian armies. Esen's 30,000 cavalries defeated 500,000 Chinese soldiers in 1449. Within eighteen months of his defeat of the titular Khan Taisun, in 1453, Esen himself took the title of Great Khan (1454–1455) of the Great Yuan.
The Khalkha emerged during the reign of Dayan Khan (1479–1543) as one of the six tumens of the Eastern Mongolic peoples. They quickly became the dominant Mongolic clan in Mongolia proper. He reunited the Mongols again. The Mongols voluntarily reunified during Eastern Mongolian Tümen Zasagt Khan rule (1558–1592) for the last time (the Mongol Empire united all Mongols before this).

Eastern Mongolia was divided into three parts in the 17th century: Outer Mongolia (Khalkha), Inner Mongolia (Inner Mongols) and the Buryat region in southern Siberia.

The last Mongol khagan was Ligdan in the early 17th century. He got into conflicts with the Manchus over the looting of Chinese cities, and managed to alienate most Mongol tribes. In 1618, Ligdan signed a treaty with the Ming dynasty to protect their northern border from the Manchus attack in exchange for thousands of taels of silver. By the 1620s, only the Chahars remained under his rule.

The Chahar army was defeated in 1625 and 1628 by the Inner Mongol and Manchu armies due to Ligdan's faulty tactics. The Qing forces secured their control over Inner Mongolia by 1635, and the army of the last khan Ligdan moved to battle against Tibetan Gelugpa sect (Yellow Hat sect) forces. The Gelugpa forces supported the Manchus, while Ligdan supported Kagyu sect (Red Hat sect) of Tibetan Buddhism. Ligden died in 1634 on his way to Tibet. By 1636, most Inner Mongolian nobles had submitted to the Qing dynasty founded by the Manchus. Inner Mongolian Tengis noyan revolted against the Qing in the 1640s and the Khalkha battled to protect Sunud.

Western Mongolian Oirats and Eastern Mongolian Khalkhas vied for domination in Mongolia since the 15th century and this conflict weakened Mongolian strength. In 1688, Western Mongolian Dzungar Khanate's king Galdan Boshugtu attacked Khalkha after murder of his younger brother by Tusheet Khan Chakhundorj (main or Central Khalkha leader) and the Khalkha-Oirat War began. Galdan threatened to kill Chakhundorj and Zanabazar (Javzandamba Khutagt I, spiritual head of Khalkha) but they escaped to Sunud (Inner Mongolia). Many Khalkha nobles and folks fled to Inner Mongolia because of the war. Few Khalkhas fled to the Buryat region and Russia threatened to exterminate them if they did not submit, but many of them submitted to Galdan Boshugtu.

In 1683 Galdan's armies reached Tashkent and the Syr Darya and crushed two armies of the Kazakhs. After that Galdan subjugated the Black Khirgizs and ravaged the Fergana Valley. From 1685 Galdan's forces aggressively pushed the Kazakhs. While his general Rabtan took Taraz, and his main force forced the Kazakhs to migrate westwards. In 1687, he besieged the City of Turkistan. Under the leadership of Abul Khair Khan, the Kazakhs won major victories over the Dzungars at the Bulanty River in 1726, and at the Battle of Anrakay in 1729.

The Khalkha eventually submitted to Qing rule in 1691 by Zanabazar's decision, thus bringing all of today's Mongolia under the rule of the Qing dynasty but Khalkha "de facto" remained under the rule of Galdan Boshugtu Khaan until 1696. The Mongol-Oirat's Code (a treaty of alliance) against foreign invasion between the Oirats and Khalkhas was signed in 1640, however, the Mongols could not unite against foreign invasions. Chakhundorj fought against Russian invasion of Outer Mongolia until 1688 and stopped Russian invasion of Khövsgöl Province. Zanabazar struggled to bring together the Oirats and Khalkhas before the war.

Galdan Boshugtu sent his army to "liberate" Inner Mongolia after defeating the Khalkha's army and called Inner Mongolian nobles to fight for Mongolian independence. Some Inner Mongolian nobles, Tibetans, Kumul Khanate and some Moghulistan's nobles supported his war against the Manchus, however, Inner Mongolian nobles did not battle against the Qing.

There were three khans in Khalkha and Zasagt Khan Shar (Western Khalkha leader) was Galdan's ally. Tsetsen Khan (Eastern Khalkha leader) did not engage in this conflict. While Galdan was fighting in Eastern Mongolia, his nephew Tseveenravdan seized the Dzungarian throne in 1689 and this event made Galdan impossible to fight against the Qing Empire. The Russian and Qing Empires supported his action because this coup weakened Western Mongolian strength. Galdan Boshugtu's army was defeated by the outnumbering Qing army in 1696 and he died in 1697. The Mongols who fled to the Buryat region and Inner Mongolia returned after the war. Some Khalkhas mixed with the Buryats.

The Buryats fought against Russian invasion since the 1620s and thousands of Buryats were massacred. The Buryat region was formally annexed to Russia by treaties in 1689 and 1727, when the territories on both the sides of Lake Baikal were separated from Mongolia. In 1689 the Treaty of Nerchinsk established the northern border of Manchuria north of the present line. The Russians retained Trans-Baikalia between Lake Baikal and the Argun River north of Mongolia. The Treaty of Kyakhta (1727), along with the Treaty of Nerchinsk, regulated the relations between Imperial Russia and the Qing Empire until the mid-nineteenth century. It established the northern border of Mongolia. Oka Buryats revolted in 1767 and Russia completely conquered the Buryat region in the late 18th century. Russia and Qing were rival empires until the early 20th century, however, both empires carried out united policy against Central Asians.

The Qing Empire conquered Upper Mongolia or the Oirat's Khoshut Khanate in the 1720s and 80,000 people were killed. By that period, Upper Mongolian population reached 200,000. The Dzungar Khanate conquered by the Qing dynasty in 1755–1758 because of their leaders and military commanders conflicts. Some scholars estimate that about 80% of the Dzungar population were destroyed by a combination of warfare and disease during the Qing conquest of the Dzungar Khanate in 1755–1758. Mark Levene, a historian whose recent research interests focus on genocide, has stated that the extermination of the Dzungars was "arguably the eighteenth century genocide par excellence." The Dzungar population reached 600,000 in 1755.

About 200,000–250,000 Oirats migrated from Western Mongolia to Volga River in 1607 and established the Kalmyk Khanate.The Torghuts were led by their Tayishi, Höö Örlög. Russia was concerned about their attack but the Kalmyks became Russian ally and a treaty to protect Southern Russian border was signed between the Kalmyk Khanate and Russia.In 1724 the Kalmyks came under control of Russia. By the early 18th century, there were approximately 300–350,000 Kalmyks and 15,000,000 Russians. The Tsardom of Russia gradually chipped away at the autonomy of the Kalmyk Khanate. These policies, for instance, encouraged the establishment of Russian and German settlements on pastures the Kalmyks used to roam and feed their livestock. In addition, the Tsarist government imposed a council on the Kalmyk Khan, thereby diluting his authority, while continuing to expect the Kalmyk Khan to provide cavalry units to fight on behalf of Russia. The Russian Orthodox church, by contrast, pressured Buddhist Kalmyks to adopt Orthodoxy.In January 1771, approximately 200,000 (170,000) Kalmyks began the migration from their pastures on the left bank of the Volga River to Dzungaria (Western Mongolia), through the territories of their Bashkir and Kazakh enemies. The last Kalmyk khan Ubashi led the migration to restore Mongolian independence. Ubashi Khan sent his 30,000 cavalries to the Russo-Turkish War in 1768–1769 to gain weapon before the migration.The Empress Catherine the Great ordered the Russian army, Bashkirs and Kazakhs to exterminate all migrants and the Empress abolished the Kalmyk Khanate. The Kyrgyzs attacked them near Balkhash Lake. About 100,000–150,000 Kalmyks who settled on the west bank of the Volga River could not cross the river because the river did not freeze in the winter of 1771 and Catherine the Great executed influential nobles of them. After seven months of travel, only one-third (66,073) of the original group reached Dzungaria (Balkhash Lake, western border of the Qing Empire). The Qing Empire transmigrated the Kalmyks to five different areas to prevent their revolt and influential leaders of the Kalmyks died soon (killed by the Manchus). Russia states that Buryatia voluntarily merged with Russia in 1659 due to Mongolian oppression and the Kalmyks voluntarily accepted Russian rule in 1609 but only Georgia voluntarily accepted Russian rule.

In the early 20th century, the late Qing government encouraged Han Chinese colonization of Mongolian lands under the name of "New Policies" or "New Administration" (xinzheng). As a result, some Mongol leaders (especially those of Outer Mongolia) decided to seek Mongolian independence. After the Xinhai Revolution, the Mongolian Revolution on 30 November 1911 in Outer Mongolia ended over 200-year rule of the Qing dynasty.

With the independence of Outer Mongolia, the Mongolian army controlled Khalkha and Khovd regions (modern day Uvs, Khovd, and Bayan-Ölgii provinces), but Northern Xinjiang (the Altai and Ili regions of the Qing Empire), Upper Mongolia, Barga and Inner Mongolia came under control of the newly formed Republic of China. On February 2, 1913 the Bogd Khanate of Mongolia sent Mongolian cavalries to "liberate" Inner Mongolia from China. Russia refused to sell weapons to the Bogd Khanate, and the Russian czar, Nicholas II, referred to it as "Mongolian imperialism". Additionally, the United Kingdom urged Russia to abolish Mongolian independence as it was concerned that "if Mongolians gain independence, then Central Asians will revolt". Khalkha and Inner Mongolian cavalries (about 3,500 Inner Mongols) defeated 70,000 Chinese soldiers and controlled almost all of Inner Mongolia; however, the Mongolian army retreated due to lack of weapons in 1914. 400 Mongol soldiers and 3,795 Chinese soldiers died in this war. The Khalkhas, Khovd Oirats, Buryats, Dzungarian Oirats, Upper Mongols, Barga Mongols, most Inner Mongolian and some Tuvan leaders sent statements to support Bogd Khan's call of Mongolian reunification. In reality however, most of them were too prudent or irresolute to attempt joining the Bogd Khan regime. Russia encouraged Mongolia to become an autonomous region of China in 1914. Mongolia lost Barga, Dzungaria, Tuva, Upper Mongolia and Inner Mongolia in the 1915 Treaty of Kyakhta.

In October 1919, the Republic of China occupied Mongolia after the suspicious deaths of Mongolian patriotic nobles. On 3 February 1921 the White Russian army—led by Baron Ungern and mainly consisting of Mongolian volunteer cavalries, and Buryat and Tatar cossacks—liberated the Mongolian capital. Baron Ungern's purpose was to find allies to defeat the Soviet Union. The Statement of Reunification of Mongolia was adopted by Mongolian revolutionist leaders in 1921. The Soviet, however, considered Mongolia to be Chinese territory in 1924 during secret meeting with the Republic of China. However, the Soviets officially recognized Mongolian independence in 1945 but carried out various policies (political, economic and cultural) against Mongolia until its fall in 1991 to prevent Pan-Mongolism and other irredentist movements.

On 10 April 1932 Mongolians revolted against the government's new policy and Soviets. The government and Soviet soldiers defeated the rebels in October.

The Buryats started to migrate to Mongolia in the 1900s due to Russian oppression. Joseph Stalin's regime stopped the migration in 1930 and started a campaign of ethnic cleansing against newcomers and Mongolians. During the Stalinist repressions in Mongolia almost all adult Buryat men and 22–33,000 Mongols (3–5% of the total population; common citizens, monks, Pan-Mongolists, nationalists, patriots, hundreds military officers, nobles, intellectuals and elite people) were shot dead under Soviet orders. Some authors also offer much higher estimates, up to 100,000 victims. Around the late 1930s the Mongolian People's Republic had an overall population of about 700,000 to 900,000 people.By 1939, Soviet said "We repressed too many people, the population of Mongolia is only hundred thousands". Proportion of victims in relation to the population of the country is much higher than the corresponding figures of the Great Purge in the Soviet Union.

The Manchukuo (1932–1945), puppet state of the Empire of Japan (1868–1947) invaded Barga and some part of Inner Mongolia with Japanese help. The Mongolian army advanced to the Great Wall of China during the Soviet–Japanese War of 1945 (Mongolian name: "Liberation War of 1945"). Japan forced Inner Mongolian and Barga people to fight against Mongolians but they surrendered to Mongolians and started to fight against their Japanese and Manchu allies. Marshal Khorloogiin Choibalsan called Inner Mongolians and Xinjiang Oirats to migrate to Mongolia during the war but the Soviet Army blocked Inner Mongolian migrants way. It was a part of Pan-Mongolian plan and few Oirats and Inner Mongols (Huuchids, Bargas, Tümeds, about 800 Uzemchins) arrived. Inner Mongolian leaders carried out active policy to merge Inner Mongolia with Mongolia since 1911. They founded the Inner Mongolian Army in 1929 but the Inner Mongolian Army disbanded after ending World War II. The Japanese Empire supported Pan-Mongolism since the 1910s but there have never been active relations between Mongolia and Imperial Japan due to Russian resistance. Inner Mongolian nominally independent Mengjiang state (1936–1945) was established with support of Japan in 1936 also some Buryat and Inner Mongol nobles founded Pan-Mongolist government with support of Japan in 1919.

The Inner Mongols established the short-lived Republic of Inner Mongolia in 1945.

Another part of Choibalsan's plan was to merge Inner Mongolia and Dzungaria with Mongolia. By 1945, Chinese communist leader Mao Zedong requested the Soviets to stop Pan-Mongolism because China lost its control over Inner Mongolia and without Inner Mongolian support the Communists were unable to defeat Japan and Kuomintang.

Mongolia and Soviet-supported Xinjiang Uyghurs and Kazakhs' movement in the 1930–1940s. By 1945, Soviet refused to support them after its alliance with the Communist Party of China and Mongolia interrupted its relations with the separatists under pressure. Xinjiang Oirat's militant groups operated together the Turkic peoples but the Oirats did not have the leading role due to their small population. Basmachis or Turkic and Tajik militants fought to liberate Central Asia (Soviet Central Asia) until 1942.

On February 2, 1913 the Treaty of friendship and alliance between the Government of Mongolia and Tibet was signed. Mongolian agents and Bogd Khan disrupted Soviet secret operations in Tibet to change its regime in the 1920s.

On 27 October 1961 UN recognized Mongolian independence after ending Western boycotts.

The Tsardom of Russia, Russian Empire, Soviet Union, capitalist and communist China performed many genocide actions against the Mongols (assimilate, reduce the population, extinguish the language, culture, tradition, history, religion and ethnic identity). Peter the Great said: "The headwaters of the Yenisei River must be Russian land". Russian Empire sent the Kalmyks and Buryats to war to reduce the populations (World War I and other wars).Soviet scientists attempted to convince the Kalmyks and Buryats that they're not the Mongols during the 20th century (demongolization policy). 35,000 Buryats were killed during and around one-third of Buryat population in Russia died in the 1900s–1950s. 10,000 Buryats of the Buryat-Mongol Autonomous Soviet Socialist Republic were massacred by Stalin's order in the 1930s. In 1919 the Buryats established a small theocratic Balagad state in Kizhinginsky District of Russia and the Buryat's state fell in 1926. In 1958, the name "Mongol" was removed from the name of the Buryat-Mongol Autonomous Soviet Socialist Republic.

On 22 January 1922 Mongolia proposed to migrate the Kalmyks during the Kalmykian Famine but bolshevik Russia refused.71–72,000 (93,000?; around half of the population) Kalmyks died during the Russian famine of 1921–22. The Kalmyks revolted against Soviet Union in 1926, 1930 and 1942–1943 (see Kalmykian Cavalry Corps). In 1913, Nicholas II, tsar of Russia, said: "We need to prevent from Volga Tatars. But the Kalmyks are more dangerous than them because they are the Mongols so send them to war to reduce the population". On 23 April 1923 Joseph Stalin, communist leader of Russia, said: "We are carrying out wrong policy on the Kalmyks who related to the Mongols.Our policy is too peaceful". In March 1927, Soviet deported 20,000 Kalmyks to Siberia, tundra and Karelia.The Kalmyks founded sovereign Republic of Oirat-Kalmyk on 22 March 1930. The Oirat's state had a small army and 200 Kalmyk soldiers defeated 1,700 Soviet soldiers in Durvud province of Kalmykia but the Oirat's state destroyed by the Soviet Army in 1930. Kalmykian nationalists and Pan-Mongolists attempted to migrate Kalmyks to Mongolia in the 1920s. Mongolia suggested to migrate the Soviet Union's Mongols to Mongolia in the 1920s but Russia refused the suggest.

Stalin deported all Kalmyks to Siberia in 1943 and around half of (97–98,000) Kalmyk people deported to Siberia died before being allowed to return home in 1957. The government of the Soviet Union forbade teaching Kalmyk language during the deportation.The Kalmyks' main purpose was to migrate to Mongolia and many Kalmyks joined the German Army.Marshal Khorloogiin Choibalsan attempted to migrate the deportees to Mongolia and he met with them in Siberia during his visit to Russia. Under the Law of the Russian Federation of April 26, 1991 "On Rehabilitation of Exiled Peoples" repressions against Kalmyks and other peoples were qualified as an act of genocide.
After the end of World War II, the Chinese Civil War resumed between the Chinese Nationalists (Kuomintang), led by Chiang Kai-shek, and the Chinese Communist Party, led by Mao Zedong. In December 1949, Chiang evacuated his government to Taiwan. Hundred thousands Inner Mongols were massacred during the Cultural Revolution in the 1960s and China forbade Mongol traditions, celebrations and the teaching of Mongolic languages during the revolution.In Inner Mongolia, some 790,000 people were persecuted. Approximately 1,000,000 Inner Mongols were killed during the 20th century. In 1960 Chinese newspaper wrote that "Han Chinese ethnic identity must be Chinese minorities ethnic identity". China-Mongolia relations were tense from the 1960s to the 1980s as a result of Sino-Soviet split, and there were several border conflicts during the period. Cross-border movement of Mongols was therefore hindered.

On 3 October 2002 the Ministry of Foreign Affairs announced that Taiwan recognizes Mongolia as an independent country, although no legislative actions were taken to address concerns over its constitutional claims to Mongolia. Offices established to support Taipei's claims over Outer Mongolia, such as the Mongolian and Tibetan Affairs Commission, lie dormant.

Agin-Buryat Okrug and Ust-Orda Buryat Okrugs merged with Irkutsk Oblast and Chita Oblast in 2008 despite Buryats' resistance. Small scale protests occurred in Inner Mongolia in 2011. The Inner Mongolian People's Party is a member of the Unrepresented Nations and Peoples Organization and its leaders are to establish sovereign state or merge Inner Mongolia with Mongolia.

Mongolian is the official national language of Mongolia, where it is spoken by nearly 2.8 million people (2010 estimate), and the official provincial language of China's Inner Mongolia Autonomous Region, where there are at least 4.1 million ethnic Mongols. Across the whole of China, the language is spoken by roughly half of the country's 5.8 million ethnic Mongols (2005 estimate) However, the exact number of Mongolian speakers in China is unknown, as there is no data available on the language proficiency of that country's citizens. The use of Mongolian in China, specifically in Inner Mongolia, has witnessed periods of decline and revival over the last few hundred years. The language experienced a decline during the late Qing period, a revival between 1947 and 1965, a second decline between 1966 and 1976, a second revival between 1977 and 1992, and a third decline between 1995 and 2012. However, in spite of the decline of the Mongolian language in some of Inner Mongolia's urban areas and educational spheres, the ethnic identity of the urbanized Chinese-speaking Mongols is most likely going to survive due to the presence of urban ethnic communities. The multilingual situation in Inner Mongolia does not appear to obstruct efforts by ethnic Mongols to preserve their language. Although an unknown number of Mongols in China, such as the Tumets, may have completely or partially lost the ability to speak their language, they are still registered as ethnic Mongols and continue to identify themselves as ethnic Mongols. The children of inter-ethnic Mongol-Chinese marriages also claim to be and are registered as ethnic Mongols.

The specific origin of the Mongolic languages and associated tribes is unclear. Linguists have traditionally proposed a link to the Tungusic and Turkic language families, included alongside Mongolic in the broader group of Altaic languages, though this remains controversial. Today the Mongolian peoples speak at least one of several Mongolic languages including Mongolian, Buryat, Oirat, Dongxiang, Tu, Bonan, Hazaragi, and Aimaq. Additionally, many Mongols speak either Russian or Mandarin Chinese as languages of inter-ethnic communication.

The original religion of the Mongolic peoples was Shamanism. The Xianbei came in contact with Confucianism and Daoism but eventually adopted Buddhism. However, the Xianbeis in Mongolia and Rourans followed a form of Shamanism. In the 5th century the Buddhist monk Dharmapriya was proclaimed State Teacher of the Rouran Khaganate and given 3000 families and some Rouran nobles became Buddhists. In 511 the Rouran Douluofubadoufa Khan sent Hong Xuan to the Tuoba court with a pearl-encrusted statue of the Buddha as a gift. The Tuoba Xianbei and Khitans were mostly Buddhists, although they still retained their original Shamanism. The Tuoba had a "sacrificial castle" to the west of their capital where ceremonies to spirits took place. Wooden statues of the spirits were erected on top of this sacrificial castle. One ritual involved seven princes with milk offerings who ascended the stairs with 20 female shamans and offered prayers, sprinkling the statues with the sacred milk. The Khitan had their holiest shrine on Mount Muye where portraits of their earliest ancestor Qishou Khagan, his wife Kedun and eight sons were kept in two temples. Mongolic peoples were also exposed to Zoroastrianism, Manicheism, Nestorianism, Eastern Orthodoxy and Islam from the west. The Mongolic peoples, in particular the Borjigin, had their holiest shrine on Mount Burkhan Khaldun where their ancestor Börte Chono (Blue Wolf) and Goo Maral (Beautiful Doe) had given birth to them. Genghis Khan usually fasted, prayed and meditated on this mountain before his campaigns. As a young man he had thanked the mountain for saving his life and prayed at the foot of the mountain sprinkling offerings and bowing nine times to the east with his belt around his neck and his hat held at his chest. Genghis Khan kept a close watch on the Mongolic supreme shaman Kokochu Teb who sometimes conflicted with his authority. Later the imperial cult of Genghis Khan (centered on the eight white gers and nine white banners in Ordos) grew into a highly organized indigenous religion with scriptures in the Mongolian script. Indigenous moral precepts of the Mongolic peoples were enshrined in oral wisdom sayings (now collected in several volumes), the anda (blood-brother) system and ancient texts such as the "Chinggis-un Bilig" (Wisdom of Genghis) and "Oyun Tulkhuur" (Key of Intelligence). These moral precepts were expressed in poetic form and mainly involved truthfulness, fidelity, help in hardship, unity, self-control, fortitude, veneration of nature, veneration of the state and veneration of parents.

In 1254 Möngke Khan organized a formal religious debate (in which William of Rubruck took part) between Christians, Muslims and Buddhists in Karakorum, a cosmopolitan city of many religions. The Mongolic Empire was known for its religious tolerance, but had a special leaning towards Buddhism and was sympathetic towards Christianity while still worshipping Tengri. The Mongolic leader Abaqa Khan sent a delegation of 13–16 to the Second Council of Lyon (1274), which created a great stir, particularly when their leader 'Zaganus' underwent a public baptism. Yahballaha III (1245–1317) and Rabban Bar Sauma (c. 1220–1294) were famous Mongolic Nestorian Christians. The Keraites in central Mongolia were Christian. The western Khanates, however, eventually adopted Islam (under Berke and Ghazan) and the Turkic languages (because of its commercial importance), although allegiance to the Great Khan and limited use of the Mongolic languages can be seen even in the 1330s. The Mongolic nobility during the Yuan dynasty studied Confucianism, built Confucian temples (including Beijing Confucius Temple) and translated Confucian works into Mongolic but mainly followed the Sakya school of Tibetan Buddhism under Phags-pa Lama. The general populace still practised Shamanism. Dongxiang and Bonan Mongols adopted Islam, as did Moghol-speaking peoples in Afghanistan. In the 1576 the Gelug school of Tibetan Buddhism became the state religion of the Mongolia. The Red Hat sect of Tibetan Buddhism coexisted with the Gelug Yellow Hat sect. Shamanism was absorbed into the state religion while being marginalized in its purer forms, later only surviving in far northern Mongolia. Monks were some of the leading intellectuals in Mongolia, responsible for much of the literature and art of the pre-modern period. Many Buddhist philosophical works lost in Tibet and elsewhere are preserved in older and purer form in Mongolian ancient texts (e.g. the Mongol Kanjur). Zanabazar (1635–1723), Zaya Pandita (1599–1662) and Danzanravjaa (1803–1856) are among the most famous Mongol holy men. The 4th Dalai Lama Yonten Gyatso (1589–1617), a Mongol himself, was the only non-Tibetan Dalai Lama.The name is a combination of the Mongolian word dalai meaning "ocean" and the Tibetan word (bla-ma) meaning "guru, teacher, mentor".[1] Many Buryats became Orthodox Christians due to the Russian expansion. During the socialist period religion was officially banned, although it was practiced in clandestine circles. Today, a sizable proportion of Mongolic peoples are atheist or agnostic. In the most recent census in Mongolia, almost forty percent of the population reported as being atheist, while the majority religion was Tibetan Buddhism, with 53%. Having survived suppression by the Communists, Buddhism among the Eastern, Northern, Southern and Western Mongols is today primarily of the Gelugpa (Yellow Hat sect) school of Tibetan Buddhism. There is a strong shamanistic influence in the Gelugpa sect among the Mongols.

They battled against the most powerful armies and warriors in Eurasia. The beating of the kettle and smoke signals were signs for the start of battle. One battle formation that they used consisted of five squadrons or units. The typical squadrons were divided by ranks. The first two ranks were in the front. These warriors had the heaviest armor and weapons. The back three ranks broke out between the front ranks and attacked first with their arrows. The forces simply kept their space from the enemy and killed them with arrow fire, during which time "archers did not aim at a specific target, but shot their arrows at a high path into a set 'killing zone' or target area." Mongolics also took hold of engineers from the defeated armies. They made engineers a permanent part of their army, so that their weapons and machinery were complex and efficient.

The traditional Mongol family was patriarchal, patrilineal and patrilocal. Wives were brought for each of the sons, while daughters were married off to other clans. Wife-taking clans stood in a relation of inferiority to wife-giving clans. Thus wife-giving clans were considered "elder" or "bigger" in relation to wife-taking clans, who were considered "younger" or "smaller". This distinction, symbolized in terms of "elder" and "younger" or "bigger" and "smaller", was carried into the clan and family as well, and all members of a lineage were terminologically distinguished by generation and age, with senior superior to junior.

In the traditional Mongolian family, each son received a part of the family herd as he married, with the elder son receiving more than the younger son. The youngest son would remain in the parental tent caring for his parents, and after their death he would inherit the parental tent in addition to his own part of the herd. This inheritance system was mandated by law codes such as the Yassa, created by Genghis Khan. Likewise, each son inherited a part of the family's camping lands and pastures, with the elder son receiving more than the younger son. The eldest son inherited the farthest camping lands and pastures, and each son in turn inherited camping lands and pastures closer to the family tent until the youngest son inherited the camping lands and pastures immediately surrounding the family tent. Family units would often remain near each other and in close cooperation, though extended families would inevitably break up after a few generations. It is probable that the Yasa simply put into written law the principles of customary law.
After the family, the next largest social units were the subclan and clan. These units were derived from groups claiming patrilineal descent from a common ancestor, ranked in order of seniority (the "conical clan"). By the Chingissid era this ranking was symbolically expressed at formal feasts, in which tribal chieftains were seated and received particular portions of the slaughtered animal according to their status. The lineage structure of Central Asia had three different modes. It was organized on the basis of genealogical distance, or the proximity of individuals to one another on a graph of kinship; generational distance, or the rank of generation in relation to a common ancestor, and birth order, the rank of brothers in relation to each another. The paternal descent lines were collaterally ranked according to the birth of their founders, and were thus considered senior and junior to each other. Of the various collateral patrilines, the senior in order of descent from the founding ancestor, the line of eldest sons, was the most noble. In the steppe, no one had his exact equal; everyone found his place in a system of collaterally ranked lines of descent from a common ancestor. It was according to this idiom of superiority and inferiority of lineages derived from birth order that legal claims to superior rank were couched.

The Mongol kinship is one of a particular patrilineal type classed as Omaha, in which relatives are grouped together under separate terms that crosscut generations, age, and even sexual difference. Thus, a man's father's sister's children, his sister's children, and his daughter's children are all called by another term. A further attribute is strict terminological differentiation of siblings according to seniority.

The division of Mongolian society into senior elite lineages and subordinate junior lineages was waning by the twentieth century. During the 1920s the Communist regime was established. The remnants of the Mongolian aristocracy fought alongside the Japanese and against Chinese, Soviets and Communist Mongols during World War II, but were defeated.

The anthropologist Herbert Harold Vreeland visited three Mongol communities in 1920 and published a highly detailed book with the results of his fieldwork, "Mongol community and kinship structure".

Today, the majority of Mongols live in the modern state of Mongolia, China (mainly Inner Mongolia and Xinjiang), Russia, Kyrgyzstan and Afghanistan.

The differentiation between tribes and peoples (ethnic groups) is handled differently depending on the country. The Tumed, Chahar, Ordos, Barga, Altai Uriankhai, Buryats, Dörböd (Dörvöd, Dörbed), Torguud, Dariganga, Üzemchin (or Üzümchin), Bayads, Khoton, Myangad (Mingad), Eljigin, Zakhchin, Darkhad, and Olots (or Öölds or Ölöts) are all considered as tribes of the Mongols.

The Eastern Mongols are mainly concentrated in Mongolia, including the Khalkha, Eljigin Khalkha, Darkhad, Sartuul Khalkha, and Dariganga (Khalkha).

The Buryats are mainly concentrated in their homeland, the Buryat Republic, a federal subject of Russia. They are the major northern subgroup of the Mongols. The Barga Mongols are mainly concentrated in Inner Mongolia, China, along with the Buryats and Hamnigan.

The Southern or Inner Mongols mainly are concentrated in Inner Mongolia, China. They comprise the Abaga Mongols, Abaganar, Aohan, Asud, Baarins, Chahar, Durved, Gorlos, Kharchin, Hishigten, Khorchin, Huuchid, Jalaid, Jaruud, Muumyangan, Naiman (Southern Mongols), Onnigud, Ordos, Sunud, Tümed, Urad, and Uzemchin.

The Western Mongols or Oirats are mainly concentrated in Western Mongolia:

Altai Uriankhai, Baatud, Bayad, Chantuu, Choros, Durvud, Khoshut, Khoid, Khoton, Myangad, Olots, Sart Kalmyks (mainly Olots), Torghut, Zakhchin.

In modern-day Mongolia, Mongols make up approximately 95% of the population, with the largest ethnic group being Khalkha Mongols, followed by Buryats, both belonging to the Eastern Mongolic peoples. They are followed by Oirats, who belong to the Western Mongolic peoples.

Mongolian ethnic groups:
Baarin, Baatud, Barga, Bayad, Buryat,
Selenge Chahar, Chantuu, Darkhad, Dariganga
Dörbet Oirat, Eljigin, Khalkha, Hamnigan, Kharchin, Khoid, Khorchin, Hotogoid, Khoton, Huuchid, Myangad, Olots, Sartuul,
Torgut, Tümed, Üzemchin, Zakhchin.

The 2010 census of the People's Republic of China counted more than 7 million people of various Mongolic groups. It should be noted that the 1992 census of China counted only 3.6 million ethnic Mongols. The 2010 census counted roughly 5.8 million ethnic Mongols, 621,500 Dongxiangs, 289,565 Mongours, 132,000 Daurs, 20,074 Baoans, and 14,370 Yugurs. Most of them live in the Inner Mongolia Autonomous Region, followed by Liaoning. Small numbers can also be found in provinces near those two.

There were 669,972 Mongols in Liaoning in 2011, making up 11.52% of Mongols in China. The closest Mongol area to the sea is the Dabao Mongol Ethnic Township () in Fengcheng, Liaoning. With 8,460 Mongols (37.4% of the township population) it is located from the North Korean border and from Korea Bay of the Yellow Sea. Another contender for closest Mongol area to the sea would be Erdaowanzi Mongol Ethnic Township () in Jianchang County, Liaoning. With 5,011 Mongols (20.7% of the township population) it is located around from the Bohai Sea.

Other peoples speaking Mongolic languages are the Daur, Sogwo Arig, Monguor people, Dongxiangs, Bonans, Sichuan Mongols and eastern part of the Yugur people. Those do not officially count as part of the Mongol ethnicity, but are recognized as ethnic groups of their own. The Mongols lost their contact with the Mongours, Bonan, Dongxiangs, Yunnan Mongols since the fall of the Yuan dynasty. Mongolian scientists and journalists met with the Dongxiangs and Yunnan Mongols in the 2000s.

Inner Mongolia:
Southern Mongols, Barga, Buryat, Dörbet Oirat, Khalkha, Dzungar people, Eznee Torgut.

Xinjiang province:
Altai Uriankhai, Chahar, Khoshut, Olots, Torghut, Zakhchin.

Qinghai province: Upper Mongols: Choros, Khalkha Mongols, Khoshut, Torghut.

Two Mongolic ethnic groups are present in Russia; the 2010 census found 461,410 Buryats and 183,400 Kalmyks.

Smaller numbers of Mongolic peoples exist in Western Europe and North America. Some of the more notable communities exist in South Korea, the United States, the Czech Republic and the United Kingdom.




</doc>
