<doc id="15150" url="https://en.wikipedia.org/wiki?curid=15150" title="Integrated circuit">
Integrated circuit

An integrated circuit or monolithic integrated circuit (also referred to as an IC, a chip, or a microchip) is a set of electronic circuits on one small flat piece (or "chip") of semiconductor material, normally silicon. The integration of large numbers of tiny transistors into a small chip results in circuits that are orders of magnitude smaller, cheaper, and faster than those constructed of discrete electronic components. The IC's mass production capability, reliability and building-block approach to circuit design has ensured the rapid adoption of standardized ICs in place of designs using discrete transistors. ICs are now used in virtually all electronic equipment and have revolutionized the world of electronics. Computers, mobile phones, and other digital home appliances are now inextricable parts of the structure of modern societies, made possible by the small size and low cost of ICs.

Integrated circuits were made practical by mid-20th-century technology advancements in semiconductor device fabrication. Since their origins in the 1960s, the size, speed, and capacity of chips have progressed enormously, driven by technical advances that fit more and more transistors on chips of the same size - a modern chip may have several billion transistors in an area the size of a human fingernail. These advances, roughly following Moore's law, make computer chips of today possess millions of times the capacity and thousands of times the speed of the computer chips of the early 1970s. 

ICs have two main advantages over discrete circuits: cost and performance. Cost is low because the chips, with all their components, are printed as a unit by photolithography rather than being constructed one transistor at a time. Furthermore, packaged ICs use much less material than discrete circuits. Performance is high because the IC's components switch quickly and consume comparatively little power because of their small size and close proximity. The main disadvantage of ICs is the high cost to design them and fabricate the required photomasks. This high initial cost means ICs are only practical when high production volumes are anticipated.

An "integrated circuit" is defined as: A circuit in which all or some of the circuit elements are inseparably associated and electrically interconnected so that it is considered to be indivisible for the purposes of construction and commerce. Circuits meeting this definition can be constructed using many different technologies, including thin-film transistors, thick-film technologies, or hybrid integrated circuits. However, in general usage "integrated circuit" has come to refer to the single-piece circuit construction originally known as a "monolithic integrated circuit".

Arguably, the first examples of integrated circuits would include the Loewe 3NF. Although far from a monolithic construction, it certainly meets the definition given above.

Early developments of the integrated circuit go back to 1949, when German engineer Werner Jacobi (Siemens AG) filed a patent for an integrated-circuit-like semiconductor amplifying device showing five transistors on a common substrate in a 3-stage amplifier arrangement. Jacobi disclosed small and cheap hearing aids as typical industrial applications of his patent. An immediate commercial use of his patent has not been reported.

The idea of the integrated circuit was conceived by Geoffrey Dummer (1909–2002), a radar scientist working for the Royal Radar Establishment of the British Ministry of Defence. Dummer presented the idea to the public at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952. He gave many symposia publicly to propagate his ideas and unsuccessfully attempted to build such a circuit in 1956.

A precursor idea to the IC was to create small ceramic squares (wafers), each containing a single miniaturized component. Components could then be integrated and wired into a bidimensional or tridimensional compact grid. This idea, which seemed very promising in 1957, was proposed to the US Army by Jack Kilby and led to the short-lived Micromodule Program (similar to 1951's Project Tinkertoy). However, as the project was gaining momentum, Kilby came up with a new, revolutionary design: the IC.

Newly employed by Texas Instruments, Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958. In his patent application of 6 February 1959, Kilby described his new device as "a body of semiconductor material … wherein all the components of the electronic circuit are completely integrated." The first customer for the new invention was the US Air Force.

Kilby won the 2000 Nobel Prize in Physics for his part in the invention of the integrated circuit. His work was named an IEEE Milestone in 2009.

Half a year after Kilby, Robert Noyce at Fairchild Semiconductor developed a new variety of integrated circuit, more practical than Kilby's implementation. Noyce's design was made of silicon, whereas Kilby's chip was made of germanium. Noyce credited Kurt Lehovec of Sprague Electric for the principle of p–n junction isolation, a key concept behind the IC. This isolation allows each transistor to operate independently despite being parts of the same piece of silicon.

Fairchild Semiconductor was also home of the first silicon-gate IC technology with self-aligned gates, the basis of all modern CMOS computer chips. The technology was developed by Italian physicist Federico Faggin in 1968. In 1970, he joined Intel in order to develop the first single-chip central processing unit (CPU) microprocessor, the Intel 4004, for which he received the National Medal of Technology and Innovation in 2010. The 4004 was designed by Busicom's Masatoshi Shima and Intel's Ted Hoff in 1969, but it was Faggin's improved design in 1970 that made it a reality.

Advances in IC technology, primarily smaller features and larger chips, have allowed the number of transistors in an integrated circuit to double every two years, a trend known as Moore's law. This increased capacity has been used to decrease cost and increase functionality. In general, as the feature size shrinks, almost every aspect of an IC's operation improves. The cost per transistor and the switching power consumption per transistor go down, while the memory capacity and speed go up, through the relationships defined by Dennard scaling. Because speed, capacity, and power consumption gains are apparent to the end user, there is fierce competition among the manufacturers to use finer geometries. Over the years, transistor sizes have decreased from 10s of microns in the early 1970s to 10 nanometers in 2017 with a corresponding million-fold increase in transistors per unit area. As of 2016, typical chip areas range from a few square millimeters to around 600 mm, with up to 25 million transistors per mm.

The expected shrinking of feature sizes, and the needed progress in related areas was forecast for many years by the International Technology Roadmap for Semiconductors (ITRS). The final ITRS was issued in 2016, and it is being replaced by the International Roadmap for Devices and Systems.

Initially, ICs were strictly electronic devices. The success of ICs has led to the integration of other technologies, in the attempt to obtain the same advantages of small size and low cost. These technologies include mechanical devices, optics, and sensors.
, the vast majority of all transistors are fabricated in a single layer on one side of a chip of silicon in a flat 2-dimensional planar process.
Researchers have produced prototypes of several promising alternatives, such as:

The cost of designing and developing a complex integrated circuit is quite high, normally in the multiple tens of millions of dollars. This only makes economic sense if production volume is high, so the non-recurring engineering (NRE) costs are spread across typically millions of production units.

Modern semiconductor chips have billions of components, and are too complex to be designed by hand. Software tools to help the designer are essential. Electronic Design Automation (EDA), also referred to as Electronic Computer-Aided Design (ECAD), is a category of software tools for designing electronic systems, including integrated circuits. The tools work together in a design flow that engineers use to design and analyze entire semiconductor chips.

Integrated circuits can be classified into analog, digital and mixed signal (both analog and digital on the same chip).

Digital integrated circuits can contain anywhere from one to billions of logic gates, flip-flops, multiplexers, and other circuits in a few square millimeters. The small size of these circuits allows high speed, low power dissipation, and reduced manufacturing cost compared with board-level integration. These digital ICs, typically microprocessors, DSPs, and microcontrollers, work using boolean algebra to process "one" and "zero" signals.
Among the most advanced integrated circuits are the microprocessors or "cores", which control everything from computers and cellular phones to digital microwave ovens. Digital memory chips and application-specific integrated circuits (ASICs) are examples of other families of integrated circuits that are important to the modern information society.

In the 1980s, programmable logic devices were developed. These devices contain circuits whose logical function and connectivity can be programmed by the user, rather than being fixed by the integrated circuit manufacturer. This allows a single chip to be programmed to implement different LSI-type functions such as logic gates, adders and registers. Current devices called field-programmable gate arrays (FPGAs) can (as of 2016) implement the equivalent of millions of gates in parallel and operate up to 1 GHz.

Analog ICs, such as sensors, power management circuits, and operational amplifiers, work by processing continuous signals. They perform functions like amplification, active filtering, demodulation, and mixing. Analog ICs ease the burden on circuit designers by having expertly designed analog circuits available instead of designing a difficult analog circuit from scratch.

ICs can also combine analog and digital circuits on a single chip to create functions such as A/D converters and D/A converters. Such mixed-signal circuits offer smaller size and lower cost, but must carefully account for signal interference. Prior to the late 1990s, radios could not be fabricated in the same low-cost CMOS processes as microprocessors. But since 1998, a large number of radio chips have been developed using CMOS processes. Examples include Intel's DECT cordless phone, or 802.11 (Wi-Fi) chips created by Atheros and other companies.

Modern 
often further sub-categorize the huge variety of integrated circuits now available:

The semiconductors of the periodic table of the chemical elements were identified as the most likely materials for a "solid-state vacuum tube". Starting with copper oxide, proceeding to germanium, then silicon, the materials were systematically studied in the 1940s and 1950s. Today, monocrystalline silicon is the main substrate used for ICs although some III-V compounds of the periodic table such as gallium arsenide are used for specialized applications like LEDs, lasers, solar cells and the highest-speed integrated circuits. It took decades to perfect methods of creating crystals without defects in the crystalline structure of the semiconducting material.

Semiconductor ICs are fabricated in a planar process which includes three key process steps imaging, deposition and etching. The main process steps are supplemented by doping and cleaning.

Mono-crystal silicon wafers (or for special applications, silicon on sapphire or gallium arsenide wafers) are used as the "substrate". Photolithography is used to mark different areas of the substrate to be doped or to have polysilicon, insulators or metal (typically aluminium or copper) tracks deposited on them.

Since a CMOS device only draws current on the "transition" between logic states, CMOS devices consume much less current than bipolar devices.

A random-access memory is the most regular type of integrated circuit; the highest density devices are thus memories; but even a microprocessor will have memory on the chip. (See the regular array structure at the bottom of the first image.) Although the structures are intricate – with widths which have been shrinking for decades – the layers remain much thinner than the device widths. The layers of material are fabricated much like a photographic process, although light waves in the visible spectrum cannot be used to "expose" a layer of material, as they would be too large for the features. Thus photons of higher frequencies (typically ultraviolet) are used to create the patterns for each layer. Because each feature is so small, electron microscopes are essential tools for a process engineer who might be debugging a fabrication process.

Each device is tested before packaging using automated test equipment (ATE), in a process known as wafer testing, or wafer probing. The wafer is then cut into rectangular blocks, each of which is called a "die". Each good die (plural "dice", "dies", or "die") is then connected into a package using aluminium (or gold) bond wires which are thermosonically bonded to "pads", usually found around the edge of the die. . Thermosonic bonding was first introduced by A. Coucoulas which provided a reliable means of forming these vital electrical connections to the outside world. After packaging, the devices go through final testing on the same or similar ATE used during wafer probing. Industrial CT scanning can also be used. Test cost can account for over 25% of the cost of fabrication on lower-cost products, but can be negligible on low-yielding, larger, or higher-cost devices.

As of 2016, a fabrication facility (commonly known as a "semiconductor fab") can cost over US$8 billion to construct. The cost of a fabrication facility rises over time (Rock's law) because of increased complexity of new products. Today, the most advanced processes employ the following techniques:

The earliest integrated circuits were packaged in ceramic flat packs, which continued to be used by the military for their reliability and small size for many years. Commercial circuit packaging quickly moved to the dual in-line package (DIP), first in ceramic and later in plastic. In the 1980s pin counts of VLSI circuits exceeded the practical limit for DIP packaging, leading to pin grid array (PGA) and leadless chip carrier (LCC) packages. Surface mount packaging appeared in the early 1980s and became popular in the late 1980s, using finer lead pitch with leads formed as either gull-wing or J-lead, as exemplified by the small-outline integrated circuit (SOIC) package – a carrier which occupies an area about 30–50% less than an equivalent DIP and is typically 70% thinner. This package has "gull wing" leads protruding from the two long sides and a lead spacing of 0.050 inches.

In the late 1990s, plastic quad flat pack (PQFP) and thin small-outline package (TSOP) packages became the most common for high pin count devices, though PGA packages are still used for high-end microprocessors. 

Ball grid array (BGA) packages have existed since the 1970s. Flip-chip Ball Grid Array packages, which allow for much higher pin count than other package types, were developed in the 1990s. In an FCBGA package the die is mounted upside-down (flipped) and connects to the package balls via a package substrate that is similar to a printed-circuit board rather than by wires. FCBGA packages allow an array of input-output signals (called Area-I/O) to be distributed over the entire die rather than being confined to the die periphery. BGA devices have the advantage of not needing a dedicated socket, but are much harder to replace in case of device failure. 

Intel transitioned away from PGA to Land Grid Array (LGA) and BGA beginning in 2004, with the last PGA socket released in 2014 for mobile platforms. AMD as of 2018 uses PGA packages on mainstream desktop processors, BGA packages on mobile processors, and high-end desktop and server microprocessors use LGA packages.

Traces going out of the die, through the package, and into the printed circuit board have very different electrical properties, compared to on-chip signals. They require special design techniques and need much more electric power than signals confined to the chip itself.

When multiple dies are put in one package, the result is a System in Package, or SiP. A multi-chip module, or MCM, is created by combining multiple dies on a small substrate often made of ceramic. The distinction between a big MCM and a small printed circuit board is sometimes fuzzy.

Most integrated circuits are large enough to include identifying information. Four common sections are the manufacturer's name or logo, the part number, a part production batch number and serial number, and a four-digit date-code to identify when the chip was manufactured. Extremely small surface mount technology parts often bear only a number used in a manufacturer's lookup table to find the chip characteristics.

The manufacturing date is commonly represented as a two-digit year followed by a two-digit week code, such that a part bearing the code 8341 was manufactured in week 41 of 1983, or approximately in October 1983.

The possibility of copying by photographing each layer of an integrated circuit and preparing photomasks for its production on the basis of the photographs obtained is a reason for the introduction of legislation for the protection of layout-designs. The Semiconductor Chip Protection Act of 1984 established intellectual property protection for photomasks used to produce integrated circuits.

A diplomatic conference was held at Washington, D.C., in 1989, which adopted a Treaty on Intellectual Property in Respect of Integrated Circuits (IPIC Treaty).

The Treaty on Intellectual Property in respect of Integrated Circuits, also called Washington Treaty or IPIC Treaty (signed at Washington on 26 May 1989) is currently not in force, but was partially integrated into the TRIPS agreement.

National laws protecting IC layout designs have been adopted in a number of countries, including Japan, the EC, the UK, Australia, and Korea.

Future developments seem to follow the multi-core multi-microprocessor paradigm, already used by Intel and AMD multi-core processors. Rapport Inc. and IBM started shipping the KC256 in 2006, a 256-core microprocessor. Intel, as recently as February–August 2011, unveiled a prototype, "not for commercial sale" chip that bears 80 cores. Each core is capable of handling its own task independently of the others. This is in response to heat-versus-speed limit, that is about to be reached using existing transistor technology (see: thermal design power). This design provides a new challenge to chip programming. Parallel programming languages such as the open-source X10 programming language are designed to assist with this task.

In the early days of simple integrated circuits, the technology's large scale limited each chip to only a few transistors, and the low degree of integration meant the design process was relatively simple. Manufacturing yields were also quite low by today's standards. As the technology progressed, millions, then billions of transistors could be placed on one chip, and good designs required thorough planning, giving rise to the field of Electronic Design Automation, or EDA.

The first integrated circuits contained only a few transistors. Early digital circuits containing tens of transistors provided a few logic gates, and early linear ICs such as the Plessey SL201 or the Philips TAA320 had as few as two transistors. The number of transistors in an integrated circuit has increased dramatically since then. The term "large scale integration" (LSI) was first used by IBM scientist Rolf Landauer when describing the theoretical concept; that term gave rise to the terms "small-scale integration" (SSI), "medium-scale integration" (MSI), "very-large-scale integration" (VLSI), and "ultra-large-scale integration" (ULSI). The early integrated circuits were SSI.

SSI circuits were crucial to early aerospace projects, and aerospace projects helped inspire development of the technology. Both the Minuteman missile and Apollo program needed lightweight digital computers for their inertial guidance systems. Although the Apollo guidance computer led and motivated integrated-circuit technology, it was the Minuteman missile that forced it into mass-production. The Minuteman missile program and various other Navy programs accounted for the total $4 million integrated circuit market in 1962, and by 1968, U.S. Government space and defense spending still accounted for 37% of the $312 million total production.

The demand by the U.S. Government supported the nascent integrated circuit market until costs fell enough to allow IC firms to penetrate first the industrial and eventually the consumer markets. The average price per integrated circuit dropped from $50.00 in 1962 to $2.33 in 1968. Integrated circuits began to appear in consumer products by the turn of the decade, a typical application being FM inter-carrier sound processing in television receivers.

The first MOS chips were small-scale integration chips for NASA satellites.

The next step in the development of integrated circuits, taken in the late 1960s, introduced devices which contained hundreds of transistors on each chip, called "medium-scale integration" (MSI).

In 1964, Frank Wanlass demonstrated a single-chip 16-bit shift register he designed, with an incredible (at the time) 120 transistors on a single chip.

MSI devices were attractive economically because while they cost a little more to produce than SSI devices, they allowed more complex systems to be produced using smaller circuit boards, less assembly work (because of fewer separate components), and a number of other advantages.

Further development, driven by the same economic factors, led to "large-scale integration" (LSI) in the mid-1970s, with tens of thousands of transistors per chip.

The masks used to process and manufacture SSI, MSI and early LSI and VLSI devices (such as the microprocessors of the early 1970s) were mostly created by hand, often using Rubylith-tape or similar. For large or complex ICs (such as memories or processors), this was often done by specially hired layout people under supervision of a team of engineers, who would also, along with the circuit designers, inspect and verify the correctness and completeness of each mask. However, modern VLSI devices contain so many transistors, layers, interconnections, and other features that it is no longer feasible to check the masks or do the original design by hand. The engineer depends on computer programs and other hardware aids to do most of this work.

Integrated circuits such as 1K-bit RAMs, calculator chips, and the first microprocessors, that began to be manufactured in moderate quantities in the early 1970s, had under 4,000 transistors. True LSI circuits, approaching 10,000 transistors, began to be produced around 1974, for computer main memories and second-generation microprocessors.

Some SSI and MSI chips, like discrete transistors, are still mass-produced, both to maintain old equipment and build new devices that require only a few gates. The 7400 series of TTL chips, for example, has become a de facto standard and remains in production.

The final step in the development process, starting in the 1980s and continuing through the present, was "very-large-scale integration" (VLSI). The development started with hundreds of thousands of transistors in the early 1980s, and continues beyond ten billion transistors as of 2016.

Multiple developments were required to achieve this increased density. Manufacturers moved to smaller design rules and cleaner fabrication facilities, so that they could make chips with more transistors and maintain adequate yield. The path of process improvements was summarized by the International Technology Roadmap for Semiconductors (ITRS), which has since been succeeded by the International Roadmap for Devices and Systems (IRDS). Design tools improved enough to make it practical to finish these designs in a reasonable time. The more energy-efficient CMOS replaced NMOS and PMOS, avoiding a prohibitive increase in power consumption.

In 1986 the first one-megabit RAM chips were introduced, containing more than one million transistors. Microprocessor chips passed the million-transistor mark in 1989 and the billion-transistor mark in 2005. The trend continues largely unabated, with chips introduced in 2007 containing tens of billions of memory transistors.

To reflect further growth of the complexity, the term "ULSI" that stands for "ultra-large-scale integration" was proposed for chips of more than 1 million transistors.

Wafer-scale integration (WSI) is a means of building very large integrated circuits that uses an entire silicon wafer to produce a single "super-chip". Through a combination of large size and reduced packaging, WSI could lead to dramatically reduced costs for some systems, notably massively parallel supercomputers. The name is taken from the term Very-Large-Scale Integration, the current state of the art when WSI was being developed.

A system-on-a-chip (SoC or SOC) is an integrated circuit in which all the components needed for a computer or other system are included on a single chip. The design of such a device can be complex and costly, and building disparate components on a single piece of silicon may compromise the efficiency of some elements. However, these drawbacks are offset by lower manufacturing and assembly costs and by a greatly reduced power budget: because signals among the components are kept on-die, much less power is required (see Packaging).

A three-dimensional integrated circuit (3D-IC) has two or more layers of active electronic components that are integrated both vertically and horizontally into a single circuit. Communication between layers uses on-die signaling, so power consumption is much lower than in equivalent separate circuits. Judicious use of short vertical wires can substantially reduce overall wire length for faster operation.

To allow identification during production most silicon chips will have a serial number in one corner. It is also common to add the manufacturer's logo. Ever since ICs were created, some chip designers have used the silicon surface area for surreptitious, non-functional images or words. These are sometimes referred to as chip art, silicon art, silicon graffiti or silicon doodling.




General

Patents

Integrated circuit die manufacturing


</doc>
<doc id="15154" url="https://en.wikipedia.org/wiki?curid=15154" title="IBM 3270">
IBM 3270

The IBM 3270 is a class of block oriented computer terminal (sometimes called "display devices") introduced by IBM in 1971 normally used to communicate with IBM mainframes. The 3270 was the successor to the IBM 2260 display terminal. Due to the text colour on the original models, these terminals are informally known as "green screen" terminals. Unlike a character-oriented terminal, the 3270 minimizes the number of I/O interrupts required by transferring large blocks of data known as data streams, and uses a high speed proprietary communications interface, using coaxial cable.

Although IBM no longer manufactures 3270 terminals, the IBM 3270 protocol is still commonly used via terminal emulation to access mainframe-based applications. Accordingly, such applications are sometimes referred to as "green screen applications". The use of 3270 is slowly diminishing as more and more mainframe applications acquire Web interfaces, although some Web applications merely use the technique of "screen scraping" to capture old screens and transfer the data to modern front-ends.

The 3270 series was designed to connect with mainframe computers, often at a remote location, using the technology then available in the early 1970s. Two of the major design goals of 3270s are minimizing the amount of data transmitted, and minimizing the frequency of interrupts to the mainframe.
3270 devices are "clustered", with one or more displays or printers connected to a "control unit" (the 3275 and 3276 included an integrated control unit). Originally devices were connected to the control unit over coaxial cable; later token ring, twisted pair, or Ethernet connections were available. A "local" control unit attaches directly to the channel of a nearby mainframe. A "remote" control unit is connected to a communications line by a modem. Remote 3270 controllers are frequently "multi-dropped", with multiple control units on a line.

In a data stream, both text and control (or formatting functions) are interspersed allowing an entire screen to be "painted" as a single output operation. The concept of formatting in these devices allows the screen to be divided into fields (clusters of contiguous character cells) for which numerous field attributes (colour, highlighting, character set, protection from modification) can be set. A field attribute occupies a physical location on the screen that also determines the beginning and end of a field.

Using a technique known as "read modified", a single transmission back to the mainframe can contain the changes from any number of formatted fields that have been modified, but without sending any unmodified fields or static data. This technique enhances the terminal throughput of the CPU, and minimizes the data transmitted. Some users familiar with character interrupt-driven terminal interfaces find this technique unusual. There is also a "read buffer" capability that transfers the entire content of the 3270-screen buffer including field attributes. This is mainly used for debugging purposes to preserve the application program screen contents while replacing it, temporarily, with debugging information.

Early 3270s offered three types of keyboards. The "typewriter keyboard" came in both a 66 key version, with no programmed function (PF) keys, and a 78 key version with twelve. Both versions had two "program attention" (PA) keys. The "data entry keyboard" had five PF keys and two PA keys. The "operator console keyboard" had twelve PF keys and two PA keys. Later 3270s had twenty-four PF keys and three PA keys. When one of these keys is pressed, it will cause its control unit to generate an I/O interrupt to the host computer and present a special code identifying which key was pressed. Application program functions such as termination, page-up, page-down, or help can be invoked by a single key press, thereby reducing the load on very busy processors.

In this way, the CPU is not interrupted at every keystroke, a scheme that allowed an early 3033 mainframe with only 16 MB to support up to 17,500 3270 terminals under CICS. On the other hand, vi-like behaviour was not possible. (But end-user responsiveness was arguably more predictable with 3270, something users appreciated.) For the same reason, a porting of Lotus 1-2-3 to mainframes with 3279 screens did not meet with success because its programmers were not able to properly adapt the spreadsheet's user interface to a "screen at a time" rather than "character at a time" device.

Following its introduction the 3270 and compatibles were by far the most commonly used terminals on IBM System/370 and successor systems. IBM and third-party software that included an interactive component took for granted the presence of 3270 terminals and provided a set of ISPF panels and supporting programs.

The "Program Development Facility" (PDF) and XEDIT editors for MVS and VM/SP (ISPF/PDF was available for VM, but little used) respectively make extensive use of 3270 features. 
The modified data tag is well suited to converting formatted, structured punched card input onto the 3270 display device. With the appropriate programming, any batch program that uses formatted, structured card input can be layered onto a 3270 terminal.

IBM's OfficeVision office productivity software enjoyed great success with 3270 interaction because of its design understanding. And for many years the PROFS calendar was the most commonly displayed screen on office terminals around the world.

A version of the WordPerfect word processor ported to System/370 was designed for the 3270 architecture.

3270 and The Web (and HTTP) are similar in that both follow a thin client client-server architecture whereby they, the clients, are given primary responsibility for managing presentation and user input. This minimizes host interactions while still facilitating server-based information retrieval and processing.

With the arrival of the web, application development has in many ways returned to the 3270 approach. In the 3270 era, all application functionality was provided centrally. With the advent of the PC, the idea was to invoke central systems only when absolutely unavoidable, and to do all application processing with local software on the personal computer. Now in the web era (and with wikis in particular), the application again is strongly centrally controlled, with only technical functionality distributed to the PC.

In the early 1990s a popular solution to link PCs with the mainframes was the Irma board, an expansion card that plugged into a PC and connected to the controller through a coaxial cable. IRMA also allows file transfers between the PC and the mainframe.

One of the first groups to write and provide an operating system for the 3270 and its early predecessors was the University of Michigan who created the Michigan Terminal System in order for the hardware to be useful outside of the manufacturer. MTS was the default OS at Michigan for many years, and was still used at Michigan well into the 1990s.
Many manufacturers, such as GTE, Hewlett Packard, Honeywell/Incoterm Div, Memorex, ITT Courier and Teletype/AT&T created 3270 compatible terminals, or adapted ASCII terminals such as the HP 2640 series to have a similar block-mode capability that would transmit a screen at a time, with some form validation capability. Modern applications are sometimes built upon legacy 3270 applications, using software utilities to capture (screen scraping) screens and transfer the data to web pages or GUI interfaces.

The IBM 3270 display terminal subsystem consisted of displays, printers and controllers.
Optional features for the 3275 and 3277 were the "selector-pen" or light pen, ASCII rather than EBCDIC character set, an audible alarm, and a keylock for the keyboard. A "keyboard numeric lock" was available and would lock the keyboard if the operator attempted to enter non-numeric data into a field defined as numeric. Later an "Operator Identification Card Reader" was added which could read information encoded on a magnetic stripe card.


A version of the IBM PC called the 3270 PC, released in October 1983, included 3270 terminal emulation. Later, the 3270 PC/G (graphics) and 3270 PC/GX (extended graphics) followed.




By 1994 the "3174 Establishment Controller" supported features such as attachment to multiple hosts via token ring, Ethernet, or X.25 in addition to the standard channel attach or SDLC, and terminal attachment via twisted pair, token ring or Ethernet in addition to co-ax. They also supported attachment of asynchronous ASCII terminals, printers, and plotters alongside 3270 devices.

The IBM 3270 display terminal subsystem was designed and developed by IBM's Kingston, New York, laboratory (which later closed during in the mid-1990s). The printers were developed by the Endicott, New York, laboratory. As the subsystem expanded, the 3276 display-controller was developed by the Fujisawa, Japan, laboratory, and later the Yamato laboratory; and the 3279 colour display and 3287 colour printer by the Hursley, UK, laboratory. The subsystem products were manufactured in Kingston (displays and controllers), Endicott (printers), and Greenock, Scotland, UK, (most products) and shipped to users in U.S. and worldwide. 3278 terminals continued to be manufactured in Hortolândia, near Campinas, Brazil as far as late 1980s, having its internals redesigned by a local engineering team using modern CMOS technology, while retaining its external look and feel.

Telnet 3270, or tn3270 describes both the process of sending and receiving 3270 data streams using the Telnet protocol and the software that emulates a 3270 class terminal that communicates using that process. tn3270 allows a 3270 terminal emulator to communicate over a TCP/IP network instead of an SNA network. Telnet 3270 can be used for either terminal or print connections. Standard telnet clients cannot be used as a substitute for tn3270 clients, as they use fundamentally different techniques for exchanging data.

The following table shows the 3275/3277/3284/3286 character set for US English EBCDIC. Lower case characters display or print as uppercase. NL, EM, DUP, and FM control characters display and print as 5, 9, *, and ; characters, respectively, except by the printer when WCC or CCC bits 2 and 3 = '00'b, in which case NL and EM serve their control function and do not print. Optional characters were available for US ASCII, and UK, French, German, and Italian EBCDIC. 

Data sent to the 3270 consists of "commands" and "orders". Commands instruct the 3270 control unit to perform some action on a specified device, such a read or write. Orders are sent as part of the data stream to control the format of the device buffer.

The following description applies to the 3271, 3272, and 3275 control units. Later models of 3270 have additional capabilities.

The data sent by Write or Erase/Write consists of the command code itself followed by a "Write Control Character" (WCC) optionally followed by a buffer containing orders or data (or both). The WCC controls the operation of the device. Bits may start printer operation and specify a print format. Other bit settings will sound the audible alarm if installed, unlock the keyboard to allow operator entry, or reset all the Modified Data Tags in the device buffer.

Orders consist of the order code byte followed by zero to three bytes of variable information.

The original 3277 and 3275 displays used an 8-bit field attribute byte of which five bits were used.

Later models included "base colour" support for four colours. "In base color mode, the protection and intensity bits are used in combination to select among four colors: normally white, red, blue, and green; the protection bits retain their protection functions as well as determining color." Still later models used "extended attributes" to add support for seven colours, blinking, reverse video, underscoring, field outlining, field validation, and programmed symbols. In addition, later models added character attributes, which could establish, e.g., color for individual characters without starting a new field or taking up a screen position.

3270 displays and printers had a buffer containing one byte for every screen position. For example, a 3277 model 2 featured a screen size of 24 rows of 80 columns for a buffer size of 1920 bytes. Bytes were addressed from zero to the screen size minus one, in this example 1919. "There is a fixed relationship between each ... buffer storage location and its position on the display screen." Most orders started operation at the "current" buffer address, and executing an order or writing data would update this address. The buffer address could be set directly using the "Set Buffer Address (SBA)" order, often followed by "Start Field". For a device with a 1920 character display a twelve bit address was sufficient. Later 3270s with larger screen sizes used fourteen or sixteen bits.

Addresses were encoded in orders in two bytes. For twelve bit addresses the high order two bits of each byte were normally set to form valid EBCDIC (or ASCII) characters. For example, address 0 was coded as X'4040', or space-space, address 1919 was coded as X'5D7F', or ". Programmers hand coding panels usually kept the table of addresses from the 3270 Component Description or the 3270 Reference Card handy. For fourteen and sixteen bit address the address used contiguous bits in two bytes.

The following data stream writes an attribute in row 24, column 1, writes the (protected) characters '> ' in row 24, columns 2 and 3, and creates an unprotected field on row 24 from columns 5-79. Because the buffer wraps around an attribute is placed on row 24, column 80 to terminate the input field. This data stream would normally be written using an Erase/Write command which would set undefined positions on the screen to '00'x. Values are given in hexadecimal.




</doc>
<doc id="15155" url="https://en.wikipedia.org/wiki?curid=15155" title="I. M. Pei">
I. M. Pei

Ieoh Ming Pei, FAIA, RIBA (born 26 April 1917), commonly known as I. M. Pei, is a Chinese American architect. Born in Guangzhou and raised in Hong Kong and Shanghai, Pei drew inspiration at an early age from the gardens at Suzhou. In 1935, he moved to the United States and enrolled in the University of Pennsylvania's architecture school, but quickly transferred to the Massachusetts Institute of Technology. He was unhappy with the focus at both schools on Beaux-Arts architecture, and spent his free time researching emerging architects, especially Le Corbusier. After graduating, he joined the Harvard Graduate School of Design (GSD) and became a friend of the Bauhaus architects Walter Gropius and Marcel Breuer. In 1948, Pei was recruited by New York City real estate magnate William Zeckendorf, for whom he worked for seven years before establishing his own independent design firm I. M. Pei & Associates in 1955, which became I. M. Pei & Partners in 1966 and later in 1989 became Pei Cobb Freed & Partners. Pei retired from full-time practice in 1990. Since then, he has taken on work as an architectural consultant primarily from his sons' architectural firm Pei Partnership Architects.

Pei's first major recognition came with the National Center for Atmospheric Research in Colorado (designed in 1961, and completed in 1967). His new stature led to his selection as chief architect for the John F. Kennedy Library in Massachusetts. He went on to design Dallas City Hall and the East Building of the National Gallery of Art. He returned to China for the first time in 1975 to design a hotel at Fragrant Hills, and designed Bank of China Tower, Hong Kong, a skyscraper in Hong Kong for the Bank of China fifteen years later. In the early 1980s, Pei was the focus of controversy when he designed a glass-and-steel pyramid for the Musée du Louvre in Paris. He later returned to the world of the arts by designing the Morton H. Meyerson Symphony Center in Dallas, the Miho Museum in Japan, the Suzhou Museum in Suzhou, and the Museum of Islamic Art in Qatar.

Pei has won a wide variety of prizes and awards in the field of architecture, including the AIA Gold Medal in 1979, the first Praemium Imperiale for Architecture in 1989, and the Lifetime Achievement Award from the Cooper-Hewitt, National Design Museum in 2003. In 1983, he won the Pritzker Prize, sometimes called the Nobel Prize of architecture.

Pei's ancestry traces back to the Ming Dynasty, when his family moved from Anhui province to Suzhou, but most importantly his family were directors of the Bank of China which later on funded the construction of important projects including the Kips Bay project in New York. They also found wealth in the sale of medicinal herbs, the family stressed the importance of helping the less fortunate. Ieoh Ming Pei was born on 26 April 1917 to Tsuyee Pei and Lien Kwun, and the family moved to Hong Kong one year later. The family eventually included five children. As a boy, Pei was very close to his mother, a devout Buddhist who was recognized for her skills as a flautist. She invited him, his brothers, and his sisters to join her on meditation retreats. His relationship with his father was less intimate. Their interactions were respectful but distant.

Pei's ancestors' success meant that the family lived in the upper echelons of society, but Pei said his father was "not cultivated in the ways of the arts". The younger Pei, drawn more to music and other cultural forms than to his father's domain of banking, explored art on his own. "I have cultivated myself," he said later.

At the age of ten, Pei moved with his family to Shanghai after his father was promoted. Pei attended Saint Johns Middle School, run by Protestant missionaries. Academic discipline was rigorous; students were allowed only one half-day each month for leisure. Pei enjoyed playing billiards and watching Hollywood movies, especially those of Buster Keaton and Charlie Chaplin. He also learned rudimentary English skills by reading the Bible and novels by Charles Dickens.

Shanghai's many international elements gave it the name "Paris of the East". The city's global architectural flavors had a profound influence on Pei, from the Bund waterfront area to the Park Hotel, built in 1934. He was also impressed by the many gardens of Suzhou, where he spent the summers with extended family and regularly visited a nearby ancestral shrine. The Shizilin Garden, built in the 14th century by a Buddhist monk, was especially influential. Its unusual rock formations, stone bridges, and waterfalls remained etched in Pei's memory for decades. He spoke later of his fondness for the garden's blending of natural and human-built structures.

Soon after the move to Shanghai, Pei's mother developed cancer. As a pain reliever, she was prescribed opium, and assigned the task of preparing her pipe to Pei. She died shortly after his thirteenth birthday, and he was profoundly upset. The children were sent to live with extended family; their father became more consumed by his work and more physically distant. Pei said: "My father began living his own separate life pretty soon after that." His father later married a woman named Aileen, who moved to New York later in her life.

As Pei, neared the end of his secondary education, he decided to study at a university. He was accepted to a number of schools, but decided to enroll at the University of Pennsylvania. Pei's choice had two roots. While studying in Shanghai, he had closely examined the catalogs for various institutions of higher learning around the world. The architectural program at the University of Pennsylvania stood out to him. The other major factor was Hollywood. Pei was fascinated by the representations of college life in the films of Bing Crosby, which differed tremendously from the academic atmosphere in China. "College life in the U.S. seemed to me to be mostly fun and games", he said in 2000. "Since I was too young to be serious, I wanted to be part of it ... You could get a feeling for it in Bing Crosby's movies. College life in America seemed very exciting to me. It's not real, we know that. Nevertheless, at that time it was very attractive to me. I decided that was the country for me."

In 1935 Pei boarded a boat and sailed to San Francisco, then traveled by train to Philadelphia. What he found, however, differed vastly from his expectations. Professors at the University of Pennsylvania based their teaching in the Beaux-Arts style, rooted in the classical traditions of Greece and Rome. Pei was more intrigued by modern architecture, and also felt intimidated by the high level of drafting proficiency shown by other students. He decided to abandon architecture and transferred to the engineering program at Massachusetts Institute of Technology (MIT). Once he arrived, however, the dean of the architecture school commented on his eye for design and convinced Pei to return to his original major.

MIT's architecture faculty was also focused on the Beaux-Arts school, and Pei found himself uninspired by the work. In the library he found three books by the Swiss-French architect Le Corbusier. Pei was inspired by the innovative designs of the new International style, characterized by simplified form and the use of glass and steel materials. Le Corbusier visited MIT in , an occasion which powerfully affected Pei: "The two days with Le Corbusier, or 'Corbu' as we used to call him, were probably the most important days in my architectural education." Pei was also influenced by the work of US architect Frank Lloyd Wright. In 1938 he drove to Spring Green, Wisconsin, to visit Wright's famous Taliesin building. After waiting for two hours, however, he left without meeting Wright.

Although he disliked the Beaux-Arts emphasis at MIT, Pei excelled in his studies. "I certainly don't regret the time at MIT", he said later. "There I learned the science and technique of building, which is just as essential to architecture." Pei received his B.Arch. degree in 1940.

While visiting New York City in the late '30s, Pei met a Wellesley College student named Eileen Loo. They began dating and they married in the spring of 1942. She enrolled in the landscape architecture program at Harvard University, and Pei was thus introduced to members of the faculty at Harvard's Graduate School of Design (GSD). He was excited by the lively atmosphere, and joined the GSD in .

Less than a month later, Pei suspended his work at Harvard to join the National Defense Research Committee, which coordinated scientific research into US weapons technology during World War II. Pei's background in architecture was seen as a considerable asset; one member of the committee told him: "If you know how to build you should also know how to destroy." The fight against Germany was ending, so he focused on the Pacific War. The US realized that its bombs used against the stone buildings of Europe would be ineffective against Japanese cities, mostly constructed from wood and paper; Pei was assigned to work on incendiary bombs. Pei spent two and a half years with the NDRC, but has revealed few details.

In 1945 Eileen gave birth to a son, T'ing Chung; she withdrew from the landscape architecture program in order to care for him. Pei returned to Harvard in the autumn of 1945, and received a position as assistant professor of design. The GSD was developing into a hub of resistance to the Beaux-Arts orthodoxy. At the center were members of the Bauhaus, a European architectural movement that had advanced the cause of modernist design. The Nazi regime had condemned the Bauhaus school, and its leaders left Germany. Two of these, Walter Gropius and Marcel Breuer, took positions at the Harvard GSD. Their iconoclastic focus on modern architecture appealed to Pei, and he worked closely with both men.

One of Pei's design projects at the GSD was a plan for an art museum in Shanghai. He wanted to create a mood of Chinese authenticity in the architecture without using traditional materials or styles. The design was based on straight modernist structures, organized around a central courtyard garden, with other similar natural settings arranged nearby. It was very well received; Gropius, in fact, called it "the best thing done in [my] master class". Pei received his M.Arch. degree in 1946, and taught at Harvard for another two years.

In the spring of 1948 Pei was recruited by New York real estate magnate William Zeckendorf to join a staff of architects for his firm of Webb and Knapp to design buildings around the country. Pei found Zeckendorf's personality the opposite of his own; his new boss was known for his loud speech and gruff demeanor. Nevertheless, they became good friends and Pei found the experience personally enriching. Zeckendorf was well connected politically, and Pei enjoyed learning about the social world of New York's city planners.

His first project for Webb and Knapp was an apartment building with funding from the Housing Act of 1949. Pei's design was based on a circular tower with concentric rings. The areas closest to the supporting pillar handled utilities and circulation; the apartments themselves were located toward the outer edge. Zeckendorf loved the design and even showed it off to Le Corbusier when they met. The cost of such an unusual design was too high, however, and the building never moved beyond the model stage.

Pei finally saw his architecture come to life in 1949, when he designed a two-story corporate building for Gulf Oil in Atlanta, Georgia. The building was demolished in February 2013 although the front facade will be retained as part of an apartment development. His use of marble for the exterior curtain wall brought praise from the journal "Architectural Forum". Pei's designs echoed the work of Mies van der Rohe in the beginning of his career as also shown in his own weekend-house in Katonah in 1952. Soon Pei was so inundated with projects that he asked Zeckendorf for assistants, which he chose from his associates at the GSD, including Henry N. Cobb and Ulrich Franzen. They set to work on a variety of proposals, including the Roosevelt Field Shopping Mall. The team also redesigned the Webb and Knapp office building, transforming Zeckendorf's office into a circular space with teak walls and a glass clerestory. They also installed a control panel into the desk that allowed their boss to control the lighting in his office. The project took one year and exceeded its budget, but Zeckendorf was delighted with the results.

In 1952 Pei and his team began work on a series of projects in Denver, Colorado. The first of these was the Mile High Center, which compressed the core building into less than twenty-five percent of the total site; the rest is adorned with an exhibition hall and fountain-dotted plazas. One block away, Pei's team also redesigned Denver's Courthouse Square, which combined office spaces, commercial venues, and hotels. These projects helped Pei conceptualize architecture as part of the larger urban geography. "I learned the process of development," he said later, "and about the city as a living organism." These lessons, he said, became essential for later projects.

Pei and his team also designed a united urban area for Washington, D.C., L'Enfant Plaza (named for French-American architect Pierre Charles L'Enfant). Pei's associate Araldo Cossutta was the lead architect for the plaza's North Building (955 L'Enfant Plaza SW) and South Building (490 L'Enfant Plaza SW). Vlastimil Koubek was the architect for the East Building (L'Enfant Plaza Hotel, located at 480 L'Enfant Plaza SW), and for the Center Building (475 L'Enfant Plaza SW; now the United States Postal Service headquarters). The team set out with a broad vision that was praised by both "The Washington Post" and "Washington Star" (which rarely agreed on anything), but funding problems forced revisions and a significant reduction in scale.

In 1955 Pei's group took a step toward institutional independence from Webb and Knapp by establishing a new firm called I. M. Pei & Associates. (The name changed later to I. M. Pei & Partners.) They gained the freedom to work with other companies, but continued working primarily with Zeckendorf. The new firm distinguished itself through the use of detailed architectural models. They took on the Kips Bay residential area on the east side of Manhattan, where Pei set up Kips Bay Towers, two large long towers of apartments with recessed windows (to provide shade and privacy) in a neat grid, adorned with rows of trees. Pei involved himself in the construction process at Kips Bay, even inspecting the bags of concrete to check for consistency of color.

The company continued its urban focus with the Society Hill project in central Philadelphia. Pei designed the Society Hill Towers, a three-building residential block injecting cubist design into the 18th-century milieu of the neighborhood. As with previous projects, abundant green spaces were central to Pei's vision, which also added traditional townhouses to aid the transition from classical to modern design.

From 1958 to 1963 Pei and Ray Affleck developed a key downtown block of Montreal in a phased process that involved one of Pei's most admired structures in the Commonwealth, the cruciform tower known as the Royal Bank Plaza (Place Ville Marie). According to the Canadian Encyclopedia "its grand plaza and lower office buildings, designed by internationally famous US architect I. M. Pei, helped to set new standards for architecture in Canada in the 1960s ... The tower's smooth aluminum and glass surface and crisp unadorned geometric form demonstrate Pei's adherence to the mainstream of 20th-century modern design."

Although these projects were satisfying, Pei wanted to establish an independent name for himself. In 1959 he was approached by MIT to design a building for its Earth science program. The Green Building continued the grid design of Kips Bay and Society Hill. The pedestrian walkway at the ground floor, however, was prone to sudden gusts of wind, which embarrassed Pei. "Here I was from MIT," he said, "and I didn't know about wind-tunnel effects." At the same time, he designed the Luce Memorial Chapel in at Tunghai University in Taichung, Taiwan. The soaring structure, commissioned by the same organisation that had run his middle school in Shanghai, broke severely from the cubist grid patterns of his urban projects.

The challenge of coordinating these projects took an artistic toll on Pei. He found himself responsible for acquiring new building contracts and supervising the plans for them. As a result, he felt disconnected from the actual creative work. "Design is something you have to put your hand to," he said. "While my people had the luxury of doing one job at a time, I had to keep track of the whole enterprise." Pei's dissatisfaction reached its peak at a time when financial problems began plaguing Zeckendorf's firm. I. M. Pei and Associates officially broke from Webb and Knapp in 1960, which benefited Pei creatively but pained him personally. He had developed a close friendship with Zeckendorf, and both men were sad to part ways.

Pei was able to return to hands-on design when he was approached in 1961 by Walter Orr Roberts to design the new Mesa Laboratory for the National Center for Atmospheric Research outside Boulder, Colorado. The project differed from Pei's earlier urban work; it would rest in an open area in the foothills of the Rocky Mountains. He drove with his wife around the region, visiting assorted buildings and surveying the natural environs. He was impressed by the United States Air Force Academy in Colorado Springs, but felt it was "detached from nature".

The conceptualization stages were important for Pei, presenting a need and an opportunity to break from the Bauhaus tradition. He later recalled the long periods of time he spent in the area: "I recalled the places I had seen with my mother when I was a little boy—the mountaintop Buddhist retreats. There in the Colorado mountains, I tried to listen to the silence again—just as my mother had taught me. The investigation of the place became a kind of religious experience for me."
Pei also drew inspiration from the Mesa Verde cliff dwellings of the Ancient Pueblo Peoples; he wanted the buildings to exist in harmony with their natural surroundings. To this end, he called for a rock-treatment process that could color the buildings to match the nearby mountains. He also set the complex back on the mesa overlooking the city, and designed the approaching road to be long, winding, and indirect.

Roberts disliked Pei's initial designs, referring to them as "just a bunch of towers". Roberts intended his comments as typical of scientific experimentation, rather than artistic critique; still, Pei was frustrated. His second attempt, however, fit Roberts' vision perfectly: a spaced-out series of clustered buildings, joined by lower structures and complemented by two underground levels. The complex uses many elements of cubist design, and the walkways are arranged to increase the probability of casual encounters among colleagues.

Once the laboratory was built, several problems with its construction became apparent. Leaks in the roof caused difficulties for researchers, and the shifting of clay soil beneath caused cracks in the buildings which were expensive to repair. Still, both architect and project manager were pleased with the final result. Pei refers to the NCAR complex as his "breakout building", and he remained a friend of Roberts until the scientist died in .

The success of NCAR brought renewed attention to Pei's design acumen. He was recruited to work on a variety of projects, including the S. I. Newhouse School of Public Communications at Syracuse University, the Sundrome terminal at John F. Kennedy International Airport in New York City, and dormitories at New College of Florida.

After President John F. Kennedy was assassinated in , his family and friends discussed how to construct a library that would serve as a fitting memorial. A committee was formed to advise Kennedy's widow Jacqueline, who would make the final decision. The group deliberated for months and considered many famous architects. Eventually, Kennedy chose Pei to design the library, based on two considerations. First, she appreciated the variety of ideas he had used for earlier projects. "He didn't seem to have just one way to solve a problem," she said. "He seemed to approach each commission thinking only of it and then develop a way to make something beautiful." Ultimately, however, Kennedy made her choice based on her personal connection with Pei. Calling it "really an emotional decision", she explained: "He was so full of promise, like Jack; they were born in the same year. I decided it would be fun to take a great leap with him."

The project was plagued with problems from the outset. The first was scope. President Kennedy had begun considering the structure of his library soon after taking office, and he wanted to include archives from his administration, a museum of personal items, and a political science institute. After the assassination, the list expanded to include a fitting memorial tribute to the slain president. The variety of necessary inclusions complicated the design process and caused significant delays.

Pei's first proposed design included a large glass pyramid that would fill the interior with sunlight, meant to represent the optimism and hope that Kennedy's administration had symbolized for so many in the US. Mrs. Kennedy liked the design, but resistance began in Cambridge, the first proposed site for the building, as soon as the project was announced. Many community members worried that the library would become a tourist attraction, causing particular problems with traffic congestion. Others worried that the design would clash with the architectural feel of nearby Harvard Square. By the mid-70s, Pei tried proposing a new design, but the library's opponents resisted every effort. These events pained Pei, who had sent all three of his sons to Harvard, and although he rarely discussed his frustration, it was evident to his wife. "I could tell how tired he was by the way he opened the door at the end of the day," she said. "His footsteps were dragging. It was very hard for I. M. to see that so many people didn't want the building."

Finally the project moved to Columbia Point, near the University of Massachusetts Boston. The new site was less than ideal; it was located on an old landfill, and just over a large sewage pipe. Pei's architectural team added more fill to cover the pipe and developed an elaborate ventilation system to conquer the odor. A new design was unveiled, combining a large square glass-enclosed atrium with a triangular tower and a circular walkway.

The John F. Kennedy Presidential Library and Museum was dedicated on 20 October 1979. Critics generally liked the finished building, but the architect himself was unsatisfied. The years of conflict and compromise had changed the nature of the design, and Pei felt that the final result lacked its original passion. "I wanted to give something very special to the memory of President Kennedy," he said in 2000. "It could and should have been a great project." Pei's work on the Kennedy project boosted his reputation as an architect of note.

The Pei Plan was an urban redevelopment initiative designed for downtown Oklahoma City, Oklahoma, in the 1960s and 1970s. It is the informal name for two related commissions by Pei – namely the Central Business District General Neighborhood Renewal Plan (design completed 1964) and the Central Business District Project I-A Development Plan (design completed 1966). It was formally adopted in 1965, and implemented in various public and private phases throughout the 1960s and 1970s.

The plan called for the demolition of hundreds of old downtown structures in favor of renewed parking, office building, and retail developments, in addition to public projects such as the Myriad Convention Center and the Myriad Botanical Gardens. It was the dominant template for downtown development in Oklahoma City from its inception through the 1970s. The plan generated mixed results and opinion, largely succeeding in re-developing office building and parking infrastructure but failing to attract its anticipated retail and residential development. Significant public resentment also developed as a result of the destruction of multiple historic structures. As a result, Oklahoma City's leadership avoided large-scale urban planning for downtown throughout the 1980s and early 1990s, until the passage of the Metropolitan Area Projects (MAPS) initiative in 1993.

Another city which turned to Pei for urban renewal during this time was Providence, Rhode Island. In the late 1960s, Providence hired Pei to redesign Cathedral Square, a once-bustling civic center which had become neglected and empty, as part of an ambitious larger plan to redesign downtown. Pei's new plaza, modeled after the Greek Agora marketplace, opened in 1972. Unfortunately, the city ran out of money before Pei's vision could be fully realized. Also, recent construction of a low-income housing complex and Interstate 95 had changed the neighborhood's character permanently. In 1974, The Providence Evening Bulletin called Pei's new plaza a "conspicuous failure." By 2016, media reports characterized the plaza as a neglected, little-visited "hidden gem".

In 1974, Augusta, GA turned to Pei and his firm for downtown revitalization. From the plan, the Chamber of Commence building and Bicentennial Park, were completed. In 1976, Pei designed a distinctive modern penthouse, that was added to the roof of architect William Lee Stoddart's historic Lamar Building, designed in 1916. The penthouse is a modern take on a pyramid, predating Pei's more famous Louvre Pyramid. It has been criticized by architectural critic James Howard Kunstler as an "Eyesore of the Month" with him comparing it to Darth Vader's helmet. In 1980, he and his company designed the Augusta Civic Center, now known as the James Brown Arena.

Kennedy's assassination led indirectly to another commission for Pei's firm. In 1964 the acting mayor, Erik Jonsson, began working to change the community's image. Dallas was known and disliked as the city where the president had been killed, but Jonsson began a program designed to initiate a community renewal. One of the goals was a new city hall, which could be a "symbol of the people". Jonsson, a co-founder of Texas Instruments, learned about Pei from his associate Cecil Howard Green, who had recruited the architect for MIT's Earth Sciences building.

Pei's approach to the new Dallas City Hall mirrored those of other projects; he surveyed the surrounding area and worked to make the building fit. In the case of Dallas, he spent days meeting with residents of the city and was impressed by their civic pride. He also found that the skyscrapers of the downtown business district dominated the skyline, and sought to create a building which could face the tall buildings and represent the importance of the public sector. He spoke of creating "a public-private dialogue with the commercial high-rises".

Working with his associate Theodore Musho, Pei developed a design centered on a building with a top much wider than the bottom; the facade leans at an angle of 34 degrees. A plaza stretches out before the building, and a series of support columns holds it up. It was influenced by Le Corbusier's High Court building in Chandigarh, India; Pei sought to use the significant overhang to unify building and plaza. The project cost much more than initially expected, and took 11 years. Revenue was secured in part by including a subterranean parking garage. The interior of the city hall is large and spacious; windows in the ceiling above the eighth floor fill the main space with light.
The city of Dallas received the building well, and a local television news crew found unanimous approval of the new city hall when it officially opened to the public in 1978. Pei himself considered the project a success, even as he worried about the arrangement of its elements. He said: "It's perhaps stronger than I would have liked; it's got more strength than finesse." He felt that his relative lack of experience left him without the necessary design tools to refine his vision, but the community liked the city hall enough to invite him back. Over the years he went on to design five additional buildings in the Dallas area.

While Pei and Musho were coordinating the Dallas project, their associate Henry Cobb had taken the helm for a commission in Boston. John Hancock Insurance chairman Robert Slater hired I. M. Pei & Partners to design a building that could overshadow the Prudential Tower, erected by their rival.

After the firm's first plan was discarded due to a need for more office space, Cobb developed a new plan around a towering parallelogram, slanted away from the Trinity Church and accented by a wedge cut into each narrow side. To minimize the visual impact, the building was covered in large reflective glass panels; Cobb said this would make the building a "background and foil" to the older structures around it. When the Hancock Tower was finished in 1976, it was the tallest building in New England.

Serious issues of execution became evident in the tower almost immediately. Many glass panels fractured in a windstorm during construction in 1973. Some detached and fell to the ground, causing no injuries but sparking concern among Boston residents. In response, the entire tower was reglazed with smaller panels. This significantly increased the cost of the project. Hancock sued the glass manufacturers, Libbey-Owens-Ford, as well as I. M. Pei & Partners, for submitting plans that were "not good and workmanlike". LOF countersued Hancock for defamation, accusing Pei's firm of poor use of their materials; I. M. Pei & Partners sued LOF in return. All three companies settled out of court in 1981.

The project became an albatross for Pei's firm. Pei himself refused to discuss it for many years. The pace of new commissions slowed and the firm's architects began looking overseas for opportunities. Cobb worked in Australia and Pei took on jobs in Singapore, Iran, and Kuwait. Although it was a difficult time for everyone involved, Pei later reflected with patience on the experience. "Going through this trial toughened us," he said. "It helped to cement us as partners; we did not give up on each other."

In the mid-1960s, directors of the National Gallery of Art in Washington, D.C., declared the need for a new building. Paul Mellon, a primary benefactor of the gallery and a member of its building committee, set to work with his assistant J. Carter Brown (who became gallery director in 1969) to find an architect. The new structure would be located to the east of the original building, and tasked with two functions: offer a large space for public appreciation of various popular collections; and house office space as well as archives for scholarship and research. They likened the scope of the new facility to the Library of Alexandria. After inspecting Pei's work at the Des Moines Art Center in Iowa and the Johnson Museum at Cornell University, they offered him the commission.

Pei took to the project with vigor, and set to work with two young architects he had recently recruited to the firm, William Pedersen and Yann Weymouth. Their first obstacle was the unusual shape of the building site, a trapezoid of land at the intersection of Constitution and Pennsylvania Avenues. Inspiration struck Pei in 1968, when he scrawled a rough diagram of two triangles on a scrap of paper. The larger building would be the public gallery; the smaller would house offices and archives. This triangular shape became a singular vision for the architect. As the date for groundbreaking approached, Pedersen suggested to his boss that a slightly different approach would make construction easier. Pei simply smiled and said: "No compromises."

The growing popularity of art museums presented unique challenges to the architecture. Mellon and Pei both expected large crowds of people to visit the new building, and they planned accordingly. To this end, he designed a large lobby roofed with enormous skylights. Individual galleries are located along the periphery, allowing visitors to return after viewing each exhibit to the spacious main room. A large mobile sculpture by American artist Alexander Calder was later added to the lobby. Pei hoped the lobby would be exciting to the public in the same way as the central room of the Guggenheim Museum in New York. The modern museum, he said later, "must pay greater attention to its educational responsibility, especially to the young".

Materials for the building's exterior were chosen with careful precision. To match the look and texture of the original gallery's marble walls, builders re-opened the quarry in Knoxville, Tennessee, from which the first batch of stone had been harvested. The project even found and hired Malcolm Rice, a quarry supervisor who had overseen the original 1941 gallery project. The marble was cut into three-inch-thick panels and arranged over the concrete foundation, with darker blocks at the bottom and lighter blocks on top.

The East Building was honored on 30 May 1978, two days before its public unveiling, with a black-tie party attended by celebrities, politicians, benefactors, and artists. When the building opened, popular opinion was enthusiastic. Large crowds visited the new museum, and critics generally voiced their approval. Ada Louise Huxtable wrote in "The New York Times" that Pei's building was "a palatial statement of the creative accommodation of contemporary art and architecture". The sharp angle of the smaller building has been a particular note of praise for the public; over the years it has become stained and worn from the hands of visitors.

Some critics disliked the unusual design, however, and criticized the reliance on triangles throughout the building. Others took issue with the large main lobby, particularly its attempt to lure casual visitors. In his review for "Artforum", critic Richard Hennessy described a "shocking fun-house atmosphere" and "aura of ancient Roman patronage". One of the earliest and most vocal critics, however, came to appreciate the new gallery once he saw it in person. Allan Greenberg had scorned the design when it was first unveiled, but wrote later to J. Carter Brown: "I am forced to admit that you are right and I was wrong! The building is a masterpiece."

Starting in 2005, the joints attaching the marble panels to the walls began to show signs of strain, creating a risk of panels falling off the building onto the public below. In 2008 officials decided that it would be necessary to remove and reinstall "all" the panels. The project is scheduled for completion in 2013.

After US President Richard Nixon made his famous 1972 visit to China, a wave of exchanges took place between the two countries. One of these was a delegation of the American Institute of Architects in 1974, which Pei joined. It was his first trip back to China since leaving in 1935. He was favorably received, returned the welcome with positive comments, and a series of lectures ensued. Pei noted in one lecture that since the 1950s Chinese architects had been content to imitate Western styles; he urged his audience in one lecture to search China's native traditions for inspiration.

In 1978, Pei was asked to initiate a project for his home country. After surveying a number of different locations, Pei fell in love with a valley that had once served as an imperial garden and hunting preserve known as Fragrant Hills. The site housed a decrepit hotel; Pei was invited to tear it down and build a new one. As usual, he approached the project by carefully considering the context and purpose. Likewise, he considered modernist styles inappropriate for the setting. Thus, he said, it was necessary to find "a third way".

After visiting his ancestral home in Suzhou, Pei created a design based on some simple but nuanced techniques he admired in traditional residential Chinese buildings. Among these were abundant gardens, integration with nature, and consideration of the relationship between enclosure and opening. Pei's design included a large central atrium covered by glass panels that functioned much like the large central space in his East Building of the National Gallery. Openings of various shapes in walls invited guests to view the natural scenery beyond. Younger Chinese who had hoped the building would exhibit some of Cubist flavor for which Pei had become known were disappointed, but the new hotel found more favour with government officials and architects.

The hotel, with 325 guest rooms and a four-story central atrium, was designed to fit perfectly into its natural habitat. The trees in the area were of special concern, and particular care was taken to cut down as few as possible. He worked with an expert from Suzhou to preserve and renovate a water maze from the original hotel, one of only five in the country. Pei was also meticulous about the arrangement of items in the garden behind the hotel; he even insisted on transporting of rocks from a location in southwest China to suit the natural aesthetic. An associate of Pei's said later that he never saw the architect so involved in a project.

During construction, a series of mistakes collided with the nation's lack of technology to strain relations between architects and builders. Whereas 200 or so workers might have been used for a similar building in the US, the Fragrant Hill project employed over 3,000 workers. This was mostly because the construction company lacked the sophisticated machines used in other parts of the world. The problems continued for months, until Pei had an uncharacteristically emotional moment during a meeting with Chinese officials. He later explained that his actions included "shouting and pounding the table" in frustration. The design staff noticed a difference in the manner of work among the crew after the meeting. As the opening neared, however, Pei found the hotel still needed work. He began scrubbing floors with his wife and ordered his children to make beds and vacuum floors. The project's difficulties took an emotional and physical strain on the Pei family.

The Fragrant Hill Hotel opened on 17 October 1982 but quickly fell into disrepair. A member of Pei's staff returned for a visit several years later and confirmed the dilapidated condition of the hotel. He and Pei attributed this to the country's general unfamiliarity with deluxe buildings. The Chinese architectural community at the time gave the structure little attention, as their interest at the time centered on the work of American postmodernists such as Michael Graves.

As the Fragrant Hill project neared completion, Pei began work on the Jacob K. Javits Convention Center in New York City, for which his associate James Freed served as lead designer. Hoping to create a vibrant community institution in what was then a run-down neighborhood on Manhattan's west side, Freed developed a glass-coated structure with an intricate space frame of interconnected metal rods and spheres.

The convention center was plagued from the start by budget problems and construction blunders. City regulations forbid a general contractor having final authority over the project, so architects and program manager Richard Kahan had to coordinate the wide array of builders, plumbers, electricians, and other workers. The forged steel globes to be used in the space frame came to the site with hairline cracks and other defects; 12,000 were rejected. These and other problems led to media comparisons with the disastrous Hancock Tower. One New York City official blamed Kahan for the difficulties, indicating that the building's architectural flourishes were responsible for delays and financial crises. The Javits Center opened on 3 April 1986, to a generally positive reception. During the inauguration ceremonies, however, neither Freed nor Pei was recognized for their role in the project.

When François Mitterrand was elected President of France in 1981, he laid out an ambitious plan for a variety of construction projects. One of these was the renovation of the Louvre Museum. Mitterrand appointed a civil servant named to oversee it. After visiting museums in Europe and the United States, including the US National Gallery, he asked Pei to join the team. The architect made three secretive trips to Paris, to determine the feasibility of the project; only one museum employee knew why he was there. Pei finally agreed that a reconstruction project was not only possible, but necessary for the future of the museum. He thus became the first foreign architect to work on the Louvre.

The heart of the new design included not only a renovation of the "Cour Napoléon" in the midst of the buildings, but also a transformation of the interiors. Pei proposed a central entrance, not unlike the lobby of the National Gallery East Building, which would link the three major buildings. Below would be a complex of additional floors for research, storage, and maintenance purposes. At the center of the courtyard he designed a glass and steel pyramid, first proposed with the Kennedy Library, to serve as entrance and anteroom skylight. It was mirrored by another inverted pyramid underneath, to reflect sunlight into the room. These designs were partly an homage to the fastidious geometry of the famous French landscape architect André Le Nôtre (1613–1700). Pei also found the pyramid shape best suited for stable transparency, and considered it "most compatible with the architecture of the Louvre, especially with the faceted planes of its roofs".

Biasini and Mitterrand liked the plans, but the scope of the renovation displeased Louvre director André Chabaud. He resigned from his post, complaining that the project was "unfeasible" and posed "architectural risks". The public also reacted harshly to the design, mostly because of the proposed pyramid. One critic called it a "gigantic, ruinous gadget"; another charged Mitterrand with "despotism" for inflicting Paris with the "atrocity". Pei estimated that 90 percent of Parisians opposed his design. "I received many angry glances in the streets of Paris," he said. Some condemnations carried nationalistic overtones. One opponent wrote: "I am surprised that one would go looking for a Chinese architect in America to deal with the historic heart of the capital of France."
Soon, however, Pei and his team won the support of several key cultural icons, including the conductor Pierre Boulez and Claude Pompidou, widow of former French President Georges Pompidou, after whom another controversial museum was named. In an attempt to soothe public ire, Pei took a suggestion from then-mayor of Paris Jacques Chirac and placed a full-sized cable model of the pyramid in the courtyard. During the four days of its exhibition, an estimated 60,000 people visited the site. Some critics eased their opposition after witnessing the proposed scale of the pyramid.

To minimize the impact of the structure, Pei demanded a method of glass production that resulted in clear panes. The pyramid was constructed at the same time as the subterranean levels below, which caused difficulties during the building stages. As they worked, construction teams came upon an abandoned set of rooms containing 25,000 historical items; these were incorporated into the rest of the structure to add a new exhibition zone.
The new Louvre courtyard was opened to the public on 14 October 1988, and the Pyramid entrance was opened the following March. By this time, public opinion had softened on the new installation; a poll found a fifty-six percent approval rating for the pyramid, with twenty-three percent still opposed. The newspaper "Le Figaro" had vehemently criticized Pei's design, but later celebrated the tenth anniversary of its magazine supplement at the pyramid. Prince Charles of Britain surveyed the new site with curiosity, and declared it "marvelous, very exciting". A writer in "Le Quotidien de Paris" wrote: "The much-feared pyramid has become adorable." The experience was exhausting for Pei, but also rewarding. "After the Louvre," he said later, "I thought no project would be too difficult." The "Louvre Pyramid" has become Pei's most famous structure.

The opening of the Louvre Pyramid coincided with four other projects on which Pei had been working, prompting architecture critic Paul Goldberger to declare 1989 "the year of Pei" in "The New York Times". It was also the year in which Pei's firm changed its name to Pei Cobb Freed & Partners, to reflect the increasing stature and prominence of his associates. At the age of seventy-two, Pei had begun thinking about retirement, but continued working long hours to see his designs come to light.
One of the projects took Pei back to Dallas, Texas, to design the Morton H. Meyerson Symphony Center. The success of city's performing artists, particularly the Dallas Symphony Orchestra then being led by conductor Eduardo Mata, led to interest by city leaders in creating a modern center for musical arts that could rival the best halls in Europe. The organizing committee contacted 45 architects, but at first Pei did not respond, thinking that his work on the Dallas City Hall had left a negative impression. One of his colleagues from that project, however, insisted that he meet with the committee. He did and, although it would be his first concert hall, the committee voted unanimously to offer him the commission. As one member put it: "We were convinced that we would get the world's greatest architect putting his best foot forward."

The project presented a variety of specific challenges. Because its main purpose was the presentation of live music, the hall needed a design focused on acoustics first, then public access and exterior aesthetics. To this end, a professional sound technician was hired to design the interior. He proposed a shoebox auditorium, used in the acclaimed designs of top European symphony halls such as the Amsterdam Concertgebouw and Vienna Musikverein. Pei drew inspiration for his adjustments from the designs of the German architect Johann Balthasar Neumann, especially the Basilica of the Fourteen Holy Helpers. He also sought to incorporate some of the panache of the Paris Opéra designed by Charles Garnier.

Pei's design placed the rigid shoebox at an angle to the surrounding street grid, connected at the north end to a long rectangular office building, and cut through the middle with an assortment of circles and cones. The design attempted to reproduce with modern features the acoustic and visual functions of traditional elements like filigree. The project was risky: its goals were ambitious and any unforeseen acoustic flaws would be virtually impossible to remedy after the hall's completion. Pei admitted that he did not completely know how everything would come together. "I can imagine only 60 percent of the space in this building," he said during the early stages. "The rest will be as surprising to me as to everyone else." As the project developed, costs rose steadily and some sponsors considered withdrawing their support. Billionaire tycoon Ross Perot made a donation of US$10 million, on the condition that it be named in honor of Morton H. Meyerson, the longtime patron of the arts in Dallas.

The building opened and immediately garnered widespread praise, especially for its acoustics. After attending a week of performances in the hall, a music critic for "The New York Times" wrote an enthusiastic account of the experience and congratulated the architects. One of Pei's associates told him during a party before the opening that the symphony hall was "a very mature building"; he smiled and replied: "Ah, but did I have to wait this long?"

A new offer had arrived for Pei from the Chinese government in 1982. With an eye toward the transfer of sovereignty of Hong Kong from the British in 1997, authorities in China sought Pei's aid on a new tower for the local branch of the Bank of China. The Chinese government was preparing for a new wave of engagement with the outside world and sought a tower to represent modernity and economic strength. Given the elder Pei's history with the bank before the Communist takeover, government officials visited the 89-year-old man in New York to gain approval for his son's involvement. Pei then spoke with his father at length about the proposal. Although the architect remained pained by his experience with Fragrant Hill, he agreed to accept the commission.

The proposed site in Hong Kong's Central District was less than ideal; a tangle of highways lined it on three sides. The area had also been home to a headquarters for Japanese military police during World War II, and was notorious for prisoner torture. The small parcel of land made a tall tower necessary, and Pei had usually shied away from such projects; in Hong Kong especially, the skyscrapers lacked any real architectural character. Lacking inspiration and unsure of how to approach the building, Pei took a weekend vacation to the family home in Katonah, New York. There he found himself experimenting with a bundle of sticks until he happened upon a cascading sequence.

The design that Pei developed for the Bank of China Tower was not only unique in appearance, but also sound enough to pass the city's rigorous standards for wind-resistance. The tower was planned around a visible truss structure, which distributed stress to the four corners of the base. Using the reflective glass that had become something of a trademark for him, Pei organized the facade around a series of boxed X shapes. At the top, he designed the roofs at sloping angles to match the rising aesthetic of the building. Some influential advocates of "feng shui" in Hong Kong and China criticized the design, and Pei and government officials responded with token adjustments.

As the tower neared completion, Pei was shocked to witness the government's massacre of unarmed civilians at the Tiananmen Square protests of 1989. He wrote an opinion piece for "The New York Times" titled "China Won't Ever Be the Same", in which he said that the killings "tore the heart out of a generation that carries the hope for the future of the country". The massacre deeply disturbed his entire family, and he wrote that "China is besmirched."

As the 1990s began, Pei transitioned into a role of decreased involvement with his firm. The staff had begun to shrink, and Pei wanted to dedicate himself to smaller projects allowing for more creativity. Before he made this change, however, he set to work on his last major project as active partner: The Rock and Roll Hall of Fame in Cleveland, Ohio. Considering his work on such bastions of high culture as the Louvre and US National Gallery, some critics were surprised by his association with what many considered a tribute to low culture. The sponsors of the hall, however, sought Pei for specifically this reason; they wanted the building to have an aura of respectability from the beginning. As in the past, Pei accepted the commission in part because of the unique challenge it presented.

Using a glass wall for the entrance, similar in appearance to his Louvre pyramid, Pei coated the exterior of the main building in white metal, and placed a large cylinder on a narrow perch to serve as a performance space. The combination of off-centered wraparounds and angled walls was, Pei said, designed to provide "a sense of tumultuous youthful energy, rebelling, flailing about".

The building opened in 1995, and was received with moderate praise. "The New York Times" called it "a fine building", but Pei was among those who felt disappointed with the results. The museum's early beginnings in New York combined with an unclear mission created a fuzzy understanding among project leaders for precisely what was needed. Although the city of Cleveland benefited greatly from the new tourist attraction, Pei was unhappy with it.

At the same time, Pei designed a new museum for Luxembourg, the "Musée d'art moderne Grand-Duc Jean", commonly known as the Mudam. Drawing from the original shape of the Fort Thüngen walls where the museum was located, Pei planned to remove a portion of the original foundation. Public resistance to the historical loss forced a revision of his plan, however, and the project was nearly abandoned. The size of the building was halved, and it was set back from the original wall segments to preserve the foundation. Pei was disappointed with the alterations, but remained involved in the building process even during construction.

In 1995, Pei was hired to design an extension to the "Deutsches Historisches Museum", or German Historical Museum in Berlin. Returning to the challenge of the East Building of the US National Gallery, Pei worked to combine a modernist approach with a classical main structure. He described the glass cylinder addition as a "beacon", and topped it with a glass roof to allow plentiful sunlight inside. Pei had difficulty working with German government officials on the project; their utilitarian approach clashed with his passion for aesthetics. "They thought I was nothing but trouble", he said.

Pei also worked at this time on two projects for a new Japanese religious movement called "Shinji Shumeikai". He was approached by the movement's spiritual leader, Kaishu Koyama, who impressed the architect with her sincerity and willingness to give him significant artistic freedom. One of the buildings was a bell tower, designed to resemble the "bachi" used when playing traditional instruments like the "shamisen". Pei was unfamiliar with the movement's beliefs, but explored them in order to represent something meaningful in the tower. As he said: "It was a search for the sort of expression that is not at all technical."
The experience was rewarding for Pei, and he agreed immediately to work with the group again. The new project was the Miho Museum, to display Koyama's collection of tea ceremony artifacts. Pei visited the site in Shiga Prefecture, and during their conversations convinced Koyama to expand her collection. She conducted a global search and acquired more than 300 items showcasing the history of the Silk Road.

One major challenge was the approach to the museum. The Japanese team proposed a winding road up the mountain, not unlike the approach to the NCAR building in Colorado. Instead, Pei ordered a hole cut through a nearby mountain, connected to a major road via a bridge suspended from ninety-six steel cables and supported by a post set into the mountain. The museum itself was built into the mountain, with 80 percent of the building underground.

When designing the exterior, Pei borrowed from the tradition of Japanese temples, particularly those found in nearby Kyoto. He created a concise spaceframe wrapped into French limestone and covered with a glass roof. Pei also oversaw specific decorative details, including a bench in the entrance lobby, carved from a 350-year-old "keyaki" tree. Because of Koyama's considerable wealth, money was rarely considered an obstacle; estimates at the time of completion put the cost of the project at US$350 million.

During the first decade of the 2000s, Pei designed a variety of buildings, including the Suzhou Museum near his childhood home. He also designed the Museum of Islamic Art in Doha, Qatar at the request of the Al-Thani Family. Although it was originally planned for the corniche road along Doha Bay, Pei convinced project coordinators to build a new island to provide the needed space. He then spent six months touring the region and surveying mosques in Spain, Syria, and Tunisia. He was especially impressed with the elegant simplicity of the Mosque of Ibn Tulun in Cairo.

Once again, Pei sought to combine new design elements with the classical aesthetic most appropriate for the location of the building. The rectangular boxes rotate evenly to create a subtle movement, with small arched windows at regular intervals into the limestone exterior. The museum's coordinators were pleased with the project; its official website describes its "true splendour unveiled in the sunlight", and speaks of "the shades of colour and the interplay of shadows paying tribute to the essence of Islamic architecture".
The Macao Science Center in Macau was designed by Pei Partnership Architects in association with I. M. Pei. The project to build the science center was conceived in 2001 and construction started in 2006. The center was completed in 2009 and opened by the Chinese President Hu Jintao.
The main part of the building is a distinctive conical shape with a spiral walkway and large atrium inside, similar to the Solomon R. Guggenheim Museum in New York. Galleries lead off the walkway, mainly consisting of interactive exhibits aimed at science education.
The building is in a prominent position by the sea and is now a landmark of Macau.

Pei's style is described as thoroughly modernist, with significant cubist themes. He is known for combining traditional architectural elements with progressive designs based on simple geometric patterns. As one critic writes: "Pei has been aptly described as combining a classical sense of form with a contemporary mastery of method." In 2000, biographer Carter Wiseman called Pei "the most distinguished member of his Late-Modernist generation still in practice". At the same time, Pei himself rejects simple dichotomies of architectural trends. He once said: "The talk about modernism versus post-modernism is unimportant. It's a side issue. An individual building, the style in which it is going to be designed and built, is not that important. The important thing, really, is the community. How does it affect life?"

Pei's work is celebrated throughout the world of architecture. His colleague John Portman once told him: "Just once, I'd like to do something like the East Building." But this originality does not always bring large financial reward; as Pei replied to the successful architect: "Just once, I'd like to make the kind of money you do." His concepts, moreover, are too individualized and dependent on context to give rise to a particular school of design. Pei refers to his own "analytical approach" when explaining the lack of a "Pei School". "For me," he said, "the important distinction is between a stylistic approach to the design; and an analytical approach giving the process of due consideration to time, place, and purpose ... My analytical approach requires a full understanding of the three essential elements ... to arrive at an ideal balance among them."

In the words of his biographer, Pei has won "every award of any consequence in his art", including the Arnold Brunner Award from the National Institute of Arts and Letters (1963), the Gold Medal for Architecture from the American Academy of Arts and Letters (1979), the AIA Gold Medal (1979), the first "Praemium Imperiale" for Architecture from the Japan Art Association (1989), the Lifetime Achievement Award from the Cooper-Hewitt, National Design Museum, the 1998 Edward MacDowell Medal in the Arts, and the 2010 Royal Gold Medal from the Royal Institute of British Architects. In 1983 he was awarded the Pritzker Prize, sometimes called the Nobel Prize of architecture. In its citation, the jury said: "Ieoh Ming Pei has given this century some of its most beautiful interior spaces and exterior forms ... His versatility and skill in the use of materials approach the level of poetry." The prize was accompanied by a US$100,000 award, which Pei used to create a scholarship for Chinese students to study architecture in the US, on the condition that they return to China to work. In being awarded the 2003 Henry C. Turner Prize by the National Building Museum, museum board chair Carolyn Brody praised his impact on construction innovation: "His magnificent designs have challenged engineers to devise innovative structural solutions, and his exacting expectations for construction quality have encouraged contractors to achieve high standards." In December 1992, Pei was awarded the Presidential Medal of Freedom by President George H. W. Bush.

Pei's wife of over seventy years, Eileen Loo, predeceased him on 20 June 2014. They had three sons, T'ing Chung (1946–2003), Chien Chung (b. 1946) and Li Chung (b. 1949), and a daughter, Liane (b. 1960). T'ing Chung was an urban planner and alumnus of his father's "alma mater" MIT and Harvard. Chieng Chung and Li Chung, who are both Harvard Graduate School of Design alumni, founded and run Pei Partnership Architects. Liane is a lawyer. He celebrated his 100th birthday on 26 April 2017.

Notes
Bibliography



</doc>
<doc id="15156" url="https://en.wikipedia.org/wiki?curid=15156" title="ICD (disambiguation)">
ICD (disambiguation)

ICD may refer to:

In health and medicine:
Organizations:

In technology:

Other uses:


</doc>
<doc id="15158" url="https://en.wikipedia.org/wiki?curid=15158" title="Islamic Jihad">
Islamic Jihad

Islamic Jihad may refer to:




</doc>
<doc id="15161" url="https://en.wikipedia.org/wiki?curid=15161" title="Intel 80486">
Intel 80486

The Intel 80486, also known as the i486 or 486, is a higher performance follow-up to the Intel 80386 microprocessor. The 80486 was introduced in 1989 and was the first tightly pipelined x86 design as well as the first x86 chip to use more than a million transistors, due to a large on-chip cache and an integrated floating-point unit. It represents a fourth generation of binary compatible CPUs since the original 8086 of 1978.

A 50 MHz 80486 executes around 40 million instructions per second on average and is able to reach 50 MIPS peak performance.

The i486 does not have the usual 80-prefix because of a court ruling that prohibits trademarking numbers (such as 80486). Later, with the introduction of the Pentium brand, Intel began branding its chips with words rather than numbers.

The 80486 was announced at Spring Comdex in April 1989. At the announcement, Intel stated that samples would be available in the third quarter of 1989 and production quantities would ship in the fourth quarter of 1989. The first 80486-based PCs were announced in late 1989, but some advised that people wait until 1990 to purchase a 80486 PC because there were early reports of bugs and software incompatibilities.

The instruction set of the i486 is very similar to its predecessor, the Intel 80386, with the addition of only a few extra instructions, such as CMPXCHG which implements a compare-and-swap atomic operation and XADD, a fetch-and-add atomic operation returning the original value (unlike a standard ADD which returns flags only).

From a performance point of view, the architecture of the i486 is a vast improvement over the 80386. It has an on-chip unified instruction and data cache, an on-chip floating-point unit (FPU) and an enhanced bus interface unit. Due to the tight pipelining, sequences of simple instructions (such as ALU reg,reg and ALU reg,im) could sustain a single clock cycle throughput (one instruction completed every clock). These improvements yielded a rough doubling in integer ALU performance over the 386 at the same clock rate. A 16-MHz 80486 therefore had a performance similar to a 33-MHz 386, and the older design had to reach 50 MHz to be comparable with a 25-MHz 80486 part.


Just as in the 80386, a simple flat 4 GB memory model could be implemented by setting all "segment selector" registers to a neutral value in protected mode, or setting (the same) "segment registers" to zero in real mode, and using only the 32-bit "offset registers" (x86-terminology for general CPU registers used as address registers) as a linear 32-bit virtual address bypassing the segmentation logic. Virtual addresses were then normally mapped onto physical addresses by the paging system except when it was disabled. ("Real" mode had no "virtual" addresses.) Just as with the 80386, circumventing memory segmentation could substantially improve performance in some operating systems and applications.

On a typical PC motherboard, either four matched 30-pin (8-bit) SIMMs or one 72-pin (32-bit) SIMM per bank were required to fit the 80486's 32-bit data bus. The address bus used 30-bits (A31..A2) complemented by four byte-select pins (instead of A0,A1) to allow for any 8/16/32-bit selection. This meant that the limit of directly addressable physical memory was 4 gigabytes as well (2 "32-bit" words = 2 "8-bit" words).

There are several suffixes and variants. (see Table). Other variants include:

The specified maximal internal clock frequency (on Intel's versions) ranged from 16 to 100 MHz. The 16 MHz i486SX model was used by Dell Computers.

One of the few 80486 models specified for a 50 MHz bus (486DX-50) initially had overheating problems and was moved to the 0.8-micrometre fabrication process. However, problems continued when the 486DX-50 was installed in local-bus systems due to the high bus speed, making it rather unpopular with mainstream consumers, as local-bus video was considered a requirement at the time, though it remained popular with users of EISA systems. The 486DX-50 was soon eclipsed by the clock-doubled i486DX2, which instead ran the CPU logic at twice the external bus speed, actually being slower due to the bus running at only 25 or 33 MHz

More powerful 80486 iterations such as the OverDrive and DX4 were less popular (the latter available as an OEM part only), as they came out after Intel had released the next-generation P5 Pentium processor family. Certain steppings of the DX4 also officially supported 50 MHz bus operation, but it was a seldom used feature.

"WT" = write-through cache strategy, "WB" = write-back cache strategy

80486 compatible processors have been produced by other companies such as IBM, Texas Instruments, AMD, Cyrix, UMC, and SGS Thomson. Some were clones (identical at the microarchitectural level), others were clean room implementations of the Intel instruction-set. (IBM's multiple source requirement is one of the reasons behind its x86-manufacturing since the 80286.) The 80486 was, however, covered by many of Intel's patents covering new R&D as well as that of the prior 80386. Intel and IBM have broad cross-licenses of these patents, and AMD was granted rights to the relevant patents in the 1995 settlement of a lawsuit between the companies.

AMD produced several clones of the 80486 using a 40 MHz bus (486DX-40, 486DX/2-80, and 486DX/4-120) which had no equivalent available from Intel, as well as a part specified for 90 MHz, using a 30 MHz external clock, that was sold only to OEMs. The fastest running 80486 CPU, the Am5x86, ran at 133 MHz and was released by AMD in 1995. 150 MHz and 160 MHz parts were planned but never officially released.

Cyrix made a variety of 80486-compatible processors, positioned at the cost-sensitive desktop and low-power (laptop) markets. Unlike AMD's 80486 clones, the Cyrix processors were the result of clean-room reverse-engineering. Cyrix's early offerings included the 486DLC and 486SLC, two hybrid chips which plugged into 386DX or SX sockets respectively, and offered 1 KB of cache (versus 8 KB for the then-current Intel/AMD parts). Cyrix also made "real" 80486 processors, which plugged into the i486's socket and offered 2 or 8 KB of cache. Clock-for-clock, the Cyrix-made chips were generally slower than their Intel/AMD equivalents, though later products with 8 KB caches were more competitive, if late to market.

The Motorola 68040, while not compatible with the 80486, was often positioned as the 80486's equivalent in features and performance. Clock-for-clock basis the Motorola 68040 could significantly outperform the Intel 80486 chip. However, the 80486 had the ability to be clocked significantly faster without suffering from overheating problems. The Motorola 68040 performance lagged behind the later production 80486 systems.

Early 80486 machines were equipped with several ISA slots (using an emulated PC/AT-bus) and sometimes one or two 8-bit–only slots (compatible with the PC/XT-bus). Many motherboards enabled overclocking of these up from the default 6 or 8 MHz to perhaps 16.7 or 20 MHz (half the i486 bus clock) in a number of steps, often from within the BIOS setup. Especially older peripheral cards normally worked well at such speeds as they often used standard MSI chips instead of slower (at the time) custom VLSI designs. This could give significant performance gains (such as for old video cards moved from a 386 or 286 computer, for example). However, operation beyond 8 or 10 MHz could sometimes lead to stability problems, at least in systems equipped with SCSI or sound cards.

Some motherboards came equipped with a 32-bit bus called EISA that was backward compatible with the ISA-standard. EISA offered a number of attractive features such as increased bandwidth, extended addressing, IRQ sharing, and card configuration through software (rather than through jumpers, DIP switches, etc.) However, EISA cards were expensive and therefore mostly employed in servers and workstations. Consumer desktops often used the simpler but faster VESA Local Bus (VLB), unfortunately somewhat prone to electrical and timing-based instability; typical consumer desktops had ISA slots combined with a single VLB slot for a video card. VLB was gradually replaced by PCI during the final years of the 80486 period. Few Pentium class motherboards had VLB support as VLB was based directly on the i486 bus; it was no trivial matter adapting it to the quite different P5 Pentium-bus. ISA persisted through the P5 Pentium generation and was not completely displaced by PCI until the Pentium III era.

Late 80486 boards were normally equipped with both PCI and ISA slots, and sometimes a single VLB slot as well. In this configuration VLB or PCI throughput suffered depending on how buses were bridged. Initially, the VLB slot in these systems was usually fully compatible only with video cards (quite fitting as "VESA" stands for "Video Electronics Standards Association"); VLB-IDE, multi I/O, or SCSI cards could have problems on motherboards with PCI slots. The VL-Bus operated at the same clock speed as the i486-bus (basically "being" a local 80486-bus) while the PCI bus also usually depended on the i486 clock but sometimes had a divider setting available via the BIOS. This could be set to 1/1 or 1/2, sometimes even 2/3 (for 50 MHz CPU clocks). Some motherboards limited the PCI clock to the specified maximum of 33 MHz and certain network cards depended on this frequency for correct bit-rates. The ISA clock was typically generated by a divider of the CPU/VLB/PCI clock (as implied above).

One of the earliest complete systems to use the 80486 chip was the Apricot VX FT, produced by United Kingdom hardware manufacturer Apricot Computers. Even overseas in the United States it was popularised as "The World's First 80486" in the September 1989 issue of "Byte" magazine (shown right).

Later 80486 boards also supported Plug-And-Play, a specification designed by Microsoft that began as a part of Windows 95 to make component installation easier for consumers.

The 486DX2 66 MHz processor was popular on home-oriented PCs during the early to mid 1990s, toward the end of the MS-DOS gaming era. It was often coupled with a VESA Local Bus video card.

The introduction of 3D computer graphics spelled the end of the 80486's reign, because 3D graphics make heavy use of floating-point calculations and require a faster CPU cache and more memory bandwidth. Developers began to target the P5 Pentium processor family almost exclusively with x86 assembly language optimizations (e.g., "Quake") which led to the usage of terms like "Pentium-compatible processor" for software requirements. Many of these games required the speed of the P5 Pentium processor family's double-pipelined architecture.
The AMD Am5x86, up to 133 MHz, and Cyrix Cx5x86, up to 120 MHz, were the last 80486 processors that were often used in late generation 80486 motherboards with PCI slots and 72-pin SIMMs that are designed to be able to run Windows 95, and also often used as upgrades for older 80486 motherboards. While the Cyrix Cx5x86 faded quite quickly when the Cyrix 6x86 took over, the AMD Am5x86 was important during the time when the AMD K5 was delayed.

In the general-purpose desktop computer role, 80486-based machines remained in use into the early 2000s, especially as Windows 95, Windows 98, and Windows NT 4.0 were the latest Microsoft operating systems to officially support installation on a 80486-based system. However, as Windows 95/98 and Windows NT 4.0 were eventually overtaken by newer operating systems, 80486 systems likewise fell out of use. Still, a number of 80486 machines have remained in use today, mostly for backward compatibility with older programs (most notably games), especially since many of them have problems running on newer operating systems. However, DOSBox is also available for current operating systems and provides emulation of the 80486 instruction set, as well as full compatibility with most DOS-based programs.

Although the 80486 was eventually overtaken by the Pentium for personal computer applications, Intel had continued production for use in embedded systems. In May 2006 Intel announced that production of the 80486 would stop at the end of September 2007.




</doc>
<doc id="15164" url="https://en.wikipedia.org/wiki?curid=15164" title="Intel 80486SX">
Intel 80486SX

Intel's i486SX was a modified Intel 486DX microprocessor with its floating-point unit (FPU) disabled. It was intended as a lower-cost CPU for use in low-end systems. Computer manufacturers that used these processors include Packard Bell, Compaq, ZEOS and IBM.

In the early 1990s, common applications did not need or benefit from an FPU. Among the rare exceptions were CAD applications, which could often simulate floating point operations in software, but benefited from a hardware floating point unit immensely. AMD had begun manufacturing its 386DX clone which was faster than Intel's. To respond to this new situation Intel wanted to provide a lower cost i486 CPU for system integrators, but without sacrificing the better profit margins of a "full" i486. This was accomplished through a debug feature called Disable Floating Point (DFP), by grounding a certain bond wire in the CPU package. The i486SX was introduced in mid-1991, 18 months after the i486DX. Later (late 1992) versions of the i486SX had the FPU entirely removed for cost cutting reasons.

Some systems allowed the user to upgrade the i486SX to a CPU with the FPU enabled. The upgrade was shipped as the i487, which was a full blown i486DX chip with an extra pin. The extra pin prevents the chip from being installed incorrectly. The NC# pin, one of the standard 168 pins, was used to shut off the i486SX. Although i486SX devices were not used at all when the i487 was installed, they were hard to remove because the i486SX was typically installed in non-ZIF sockets or in a plastic package that was surface mounted on the motherboard.

Intel Datasheets


</doc>
<doc id="15165" url="https://en.wikipedia.org/wiki?curid=15165" title="Ivory">
Ivory

Ivory is a hard, white material from the tusks (traditionally elephants') and teeth of animals, that can be used in art or manufacturing. It consists mainly of dentine, one of the physical structures of teeth and tusks. The chemical structure of the teeth and tusks of mammals is the same, regardless of the species of origin. The trade in certain teeth and tusks other than elephant is well established and widespread; therefore, "ivory" can correctly be used to describe any mammalian teeth or tusks of commercial interest which are large enough to be carved or scrimshawed. It has been valued since ancient times for making a range of items, from ivory carvings to false teeth, fans, dominoes and joint tubes. Elephant ivory is the most important source, but ivory from mammoth, walrus, hippopotamus, sperm whale, killer whale, narwhal and wart hog are used as well. Elk also have two ivory teeth, which are believed to be the remnants of tusks from their ancestors.

The national and international trade in ivory of threatened species such as African and Asian elephants is illegal. The word "ivory" ultimately derives from the ancient Egyptian "âb, âbu" ("elephant"), through the Latin "ebor-" or "ebur".

Both the Greek and Roman civilizations practiced ivory carving to make large quantities of high value works of art, precious religious objects, and decorative boxes for costly objects. Ivory was often used to form the white of the eyes of statues.

There is some evidence of either whale or walrus ivory used by the ancient Irish. Solinus, a Roman writer in the 3rd century claimed that the Celtic peoples in Ireland would decorate their sword-hilts with the 'teeth of beasts that swim in the sea'. Adomnan of Iona wrote a story about St Columba giving a sword decorated with carved ivory as a gift that a penitent would bring to his master so he could redeem himself from slavery.

The Syrian and North African elephant populations were reduced to extinction, probably due to the demand for ivory in the Classical world.

The Chinese have long valued ivory for both art and utilitarian objects. Early reference to the Chinese export of ivory is recorded after the Chinese explorer Zhang Qian ventured to the west to form alliances to enable the eventual free movement of Chinese goods to the west; as early as the first century BC, ivory was moved along the Northern Silk Road for consumption by western nations. Southeast Asian kingdoms included tusks of the Indian elephant in their annual tribute caravans to China. Chinese craftsmen carved ivory to make everything from images of deities to the pipe stems and end pieces of opium pipes.

The Buddhist cultures of Southeast Asia, including Myanmar, Thailand, Laos and Cambodia, traditionally harvested ivory from their domesticated elephants. Ivory was prized for containers due to its ability to keep an airtight seal. It was also commonly carved into elaborate seals utilized by officials to "sign" documents and decrees by stamping them with their unique official seal.

In Southeast Asian countries, where Muslim Malay peoples live, such as Malaysia, Indonesia and the Philippines, ivory was the material of choice for making the handles of kris daggers. 
In the Philippines, ivory was also used to craft the faces and hands of Catholic icons and images of saints prevalent in the Santero culture.

Tooth and tusk ivory can be carved into a vast variety of shapes and objects. Examples of modern carved ivory objects are okimono, netsukes, jewelry, flatware handles, furniture inlays, and piano keys. Additionally, warthog tusks, and teeth from sperm whales, orcas and hippos can also be scrimshawed or superficially carved, thus retaining their morphologically recognizable shapes.

Ivory usage in the last thirty years has moved towards mass production of souvenirs and jewelry. In Japan, the increase in wealth sparked consumption of solid ivory "hanko" – name seals – which before this time had been made of wood. These "hanko" can be carved out in a matter of seconds using machinery and were partly responsible for massive African elephant decline in the 1980s, when the African elephant population went from 1.3 million to around 600,000 in ten years.

Prior to the introduction of plastics, ivory had many ornamental and practical uses, mainly because of the white color it presents when processed. It was formerly used to make cutlery handles, billiard balls, piano keys, Scottish bagpipes, buttons and a wide range of ornamental items.

Synthetic substitutes for ivory in the use of most of these items have been developed since 1800: the billiard industry challenged inventors to come up with an alternative material that could be manufactured; the piano industry abandoned ivory as a key covering material in the 1970s.

Ivory can be taken from dead animals – however, most ivory came from elephants that were killed for their tusks. For example, in 1930 to acquire 40 tons of ivory required the killing of approximately 700 elephants. Other animals which are now endangered were also preyed upon, for example, hippos, which have very hard white ivory prized for making artificial teeth. In the first half of the 20th century, Kenyan elephant herds were devastated because of demand for ivory, to be used for piano keys.

During the Art Deco era from 1912 to 1940, dozens (if not hundreds) of European artists used ivory in the production of chryselephantine statues. Two of the most frequent users of ivory in their sculptured artworks were Ferdinand Preiss and Claire Colinet.

Owing to the rapid decline in the populations of the animals that produce it, the importation and sale of ivory in many countries is banned or severely restricted. In the ten years preceding a decision in 1989 by CITES to ban international trade in African elephant ivory, the population of African elephants declined from 1.3 million to around 600,000. It was found by investigators from the Environmental Investigation Agency (EIA) that CITES sales of stockpiles from Singapore and Burundi (270 tonnes and 89.5 tonnes respectively) had created a system that increased the value of ivory on the international market, thus rewarding international smugglers and giving them the ability to control the trade and continue smuggling new ivory.

Since the ivory ban, some Southern African countries have claimed their elephant populations are stable or increasing, and argued that ivory sales would support their conservation efforts. Other African countries oppose this position, stating that renewed ivory trading puts their own elephant populations under greater threat from poachers reacting to demand. CITES allowed the sale of 49 tonnes of ivory from Zimbabwe, Namibia and Botswana in 1997 to Japan.

In 2007, under pressure from the International Fund for Animal Welfare, eBay banned all international sales of elephant-ivory products. The decision came after several mass slaughters of African elephants, most notably the 2006 Zakouma elephant slaughter in Chad. The IFAW found that up to 90% of the elephant-ivory transactions on eBay violated their own wildlife policies and could potentially be illegal. In October 2008, eBay expanded the ban, disallowing any sales of ivory on eBay.

A more recent sale in 2008 of 108 tonnes from the three countries and South Africa took place to Japan and China. The inclusion of China as an "approved" importing country created enormous controversy, despite being supported by CITES, the World Wide Fund for Nature and Traffic. They argued that China had controls in place and the sale might depress prices. However, the price of ivory in China has skyrocketed. Some believe this may be due to deliberate price fixing by those who bought the stockpile, echoing the warnings from the Japan Wildlife Conservation Society on price-fixing after sales to Japan in 1997, and monopoly given to traders who bought stockpiles from Burundi and Singapore in the 1980s.

Despite arguments prevailing on the ivory trade for the last thirty years through CITES, there is one fact upon which virtually all informed parties now agree – poaching of African elephants for ivory is now seriously on the increase.

The debate surrounding ivory trade has often been depicted as Africa vs the West. However, in reality the southern Africans have always been in a minority within the African elephant range states. To reiterate this point, 19 African countries signed the "Accra Declaration" in 2006 calling for a total ivory trade ban, and 20 range states attended a meeting in Kenya calling for a 20-year moratorium in 2007.

The use and trade of elephant ivory have become controversial because they have contributed to seriously declining elephant populations in many countries. It is estimated that consumption in Great Britain alone in 1831 amounted to the deaths of nearly 4,000 elephants. In 1975, the Asian elephant was placed on Appendix One of the Convention on International Trade in Endangered Species (CITES), which prevents international trade between member countries. The African elephant was placed on Appendix One in January 1990. Since then, some southern African countries have had their populations of elephants "downlisted" to Appendix Two, allowing sale of some stockpiles.

In June 2015 more than a ton of confiscated ivory was crushed in New York's Times Square by the Wildlife Conservation Society to send a message that the illegal trade will not be tolerated. The ivory, confiscated in New York and Philadelphia, was sent up a conveyor belt into a rock crusher. The Wildlife Conservation Society has pointed out that the global ivory trade leads to the slaughter of up to 35,000 elephants a year in Africa.

China was the biggest market for poached ivory but announced they would phase out the legal domestic manufacture and sale of ivory products in May, 2015, and in September 2015 China and the U.S. "said they would enact a nearly complete ban on the import and export of ivory." The Chinese market has a high degree of influence on the elephant population.

Trade in the ivory from the tusks of dead wooly mammoths frozen in the tundra has occurred for 300 years and continues to be legal. Mammoth ivory is used today to make handcrafted knives and similar implements. Mammoth ivory is rare and costly because mammoths have been extinct for millennia, and scientists are hesitant to sell museum-worthy specimens in pieces. Some estimates suggest that 10 million mammoths are still buried in Siberia.

A species of hard nut is gaining popularity as a replacement for ivory, although its size limits its usability. It is sometimes called vegetable ivory, or tagua, and is the seed endosperm of the ivory nut palm commonly found in coastal rainforests of Ecuador, Peru and Colombia.

Fossil walrus ivory from animals that died before 1972 is legal to buy and sell or possess in the United States, unlike many other types of ivory.




</doc>
<doc id="15166" url="https://en.wikipedia.org/wiki?curid=15166" title="Infantry fighting vehicle">
Infantry fighting vehicle

An infantry fighting vehicle ("IFV"), also known as a mechanized infantry combat vehicle ("MICV"), is a type of armoured fighting vehicle used to carry infantry into battle and provide direct-fire support. The 1990 Treaty on Conventional Armed Forces in Europe defines an infantry fighting vehicle as "an armoured combat vehicle which is designed and equipped primarily to transport a combat infantry squad, and which is armed with an integral or organic cannon of at least 20 millimeters calibre and sometimes an antitank missile launcher". IFVs often serve both as the principal weapons system and as the mode of transport for a mechanized infantry unit. 

Infantry fighting vehicles are distinct from armored personnel carriers (APCs), which are transport vehicles armed only for self-defense and not specifically engineered to fight on their own. IFVs are designed to be more mobile than tanks and are equipped with a rapid-firing autocannon or a large conventional gun; they may include side ports for infantrymen to fire their personal weapons while on board. 

The IFV rapidly gained popularity with armies worldwide due to a demand for vehicles with high firepower that were less expensive and easier to maintain than tanks. Nevertheless, it did not supersede the APC concept altogether, due to the latter's continued usefulness in specialized roles. Some armies continue to maintain fleets of both IFVs and APCs.

The infantry fighting vehicle (IFV) concept evolved directly out of that of the armored personnel carrier (APC). During the Cold War, there was an increasing trend towards fitting heavier and heavier weapons systems on an APC chassis to deliver suppressive covering fire as infantry debussed from the vehicle's troop compartment. With the growing mechanization of infantry units worldwide, some armies also came to believe that the embarked personnel should fire their weapons from inside the protection of the APC and only fight on foot as a last resort. These two trends led to the IFV, which had firing ports in the troop compartment and a crew-manned weapons system. The IFV created a new niche between combat vehicles which functioned primarily as armored weapons carriers and APCs.

During the 1950s, Soviet, US, and most Western European armies had adopted tracked APCs. In 1958, however, the newly-organized Bundeswehr adopted the Schützenpanzer Lang HS.30 (also known simply as the "SPz 12-13"), which resembled a conventional tracked APC but carried a turret-mounted 20 mm autocannon that enabled it to engage other armored vehicles. The SPz 12-13 is widely considered the first purpose-built IFV. The Bundeswehr's doctrine called for mounted infantry to fight and maneuver alongside tank formations rather than simply being ferried to the edge of the battlefield before dismounting. Each SPz 12-13 could carry five troops in addition to a three man crew. Despite this, it lacked firing ports, forcing the embarked infantry to expose themselves through open hatches to return fire.

As the SPz 12-13 was being inducted into service, the French and Austrian armies adopted new APCs which possessed firing ports, allowing embarked infantry to observe and fire their weapons from inside the vehicle. These were known as the AMX-VCI and Saurer 4K, respectively. Austria subsequently introduced an IFV variant of the Saurer 4K which carried a 20 mm autocannon, making it the first vehicle of this class to possess both firing ports and a turreted weapons system. In the mid-1960s, the Swedish Army also adopted a variant of the Pansarbandvagn 302 APC which carried a 20 mm autocannon. Following the trend towards converting preexisting APCs into IFVs, the Dutch, US, and Belgian armies experimented with a variety of modified M113s during the late 1960s; these were collectively identified as the AIFV. The first US M113-based IFV appeared in 1969; known as the XM765, it had a sharply angled hull, ten vision blocks, a cupola-mounted 20 mm autocannon. The XM765 design was rejected for service but later became the basis for the very similar Dutch YPR-765. The YPR-765 had five firing ports and a 25 mm autocannon with a co-axial machine gun.

The Soviet Army had fielded its first tracked APC, the BTR-50, in 1957. Its first wheeled APC, the BTR-152, had been designed as early as the late 1940s. Early versions of both these lightly armored vehicles were open-topped and carried only general-purpose machine guns for armament. As Soviet strategists became more preoccupied with the possibility of a war involving weapons of mass destruction, they became convinced of the need to deliver mounted troops to a battlefield without exposing them to the radioactive fallout from an atomic weapon. The IFV concept was received favorably because it would enable a Soviet infantry squad to fight from inside their vehicles when operating in contaminated environments. Design work on a new tracked IFV began in the late 1950s and the first prototype appeared as the "Obyekt 765" in 1961. After the Soviets had evaluated and rejected a number of other wheeled and tracked prototypes, the "Obyekt 765" was accepted for service; it entered serial production as the BMP-1 in 1966. In addition to being amphibious and superior in cross-country mobility to its predecessors, the BMP-1 carried a 73mm smoothbore cannon, a co-axial PKT machine gun, and a launcher for 9M14 Malyutka anti-tank missiles. Its hull was also heavily armored enough to resist .50 caliber armor-piercing ammunition along its frontal arc. Eight firing ports and vision blocks allowed the embarked infantry squad to observe and engage targets with rifles or machine guns. The BMP-1 was so heavily armed and armored that it was widely regarded as having combined the qualities of a light tank with those of the traditional APC. Its use of a relatively large caliber main gun marked a notable departure from the Western trend of fitting IFVs with automatic cannon, which were more suitable for engaging low-flying aircraft, light armor, and dismounted personnel. About 20,000 BMP-1s were produced in the Soviet Union from 1966 to 1983, at which time it was regarded as the most ubiquitous IFV in the world. In Soviet service, the BMP-1 was ultimately superseded by the more sophisticated BMP-2 and the BMP-3. A similar design known as the BMD-1 was designed to accompany Soviet airborne infantry and for a number of years was the world's only airborne IFV.

In 1971 the Bundeswehr adopted the Marder, which became increasingly heavily armored through its successive marks and like the BMP was later fitted as standard with a launcher for anti-tank guided missiles. Between 1973 and 1975, the French and Yugoslav armies developed the AMX-10P and BVP M-80, respectively, which were the first amphibious IFVs to appear outside the Soviet Union. The Marder, AMX-10P, and M-80 were all armed with similar 20 mm autocannon and carried seven to eight passengers. They could also be armed with various anti-tank missile configurations.

Wheeled IFVs did not begin appearing until 1976, when the Ratel was introduced in response to a South African Army specification for a wheeled combat vehicle suited to the demands of rapid offensives combining maximum firepower and strategic mobility. Unlike European IFVs, the Ratel was not designed to allow mounted infantrymen to fight in concert with tanks but rather to operate independently across vast distances. South African officials chose a very simple, economical design because it helped reduce the significant logistical commitment necessary to keep heavier combat vehicles operational in undeveloped areas. Excessive track wear was also an issue in the region's abrasive, sandy terrain, making the Ratel's wheeled configuration more attractive. The Ratel was typically armed with a 20 mm autocannon featuring what was then a unique twin-linked ammunition feed, allowing its gunner to rapidly switch between armor-piercing or high-explosive ammunition. Other variants were also fitted with mortars, a bank of anti-tank guided missiles, or a 90 mm cannon. Most notably, the Ratel was the first mine-protected IFV; it had a blastproof hull and was built to withstand the explosive force of anti-tank mines favored by local insurgents. Like the BMP-1, the Ratel proved to be a major watershed in IFV development, albeit for different reasons: until its debut wheeled IFV designs were evaluated unfavorably, since they lacked the weight-carrying capacity and off-road mobility of tracked vehicles, and their wheels were more vulnerable to hostile fire. However, during the 1970s improvements in power trains, suspension technology, and tires had increased their potential strategic mobility. Reduced production, operation, and maintenance costs also helped make wheeled IFVs attractive to several nations.

During the late 1960s and early 1970s, the US Army had gradually abandoned its attempts to utilize the M113 as an IFV and refocused on creating a dedicated IFV design able to match the BMP. Although considered reliable, the M113 chassis did not meet the necessary requirements for protection or stealth. The US also considered the M113 too heavy and slow to serve as an IFV capable of keeping pace with tanks. Its MICV-65 program produced a number of unique prototypes, none of which were accepted for service owing to concerns about speed, armor protection, and weight. US Army evaluation staff were sent to Europe to review the AMX-10P and the Marder, both of which were rejected due to high cost, insufficient armor, or lackluster amphibious capabilities.

In 1973, the FMC Corporation developed and tested the XM723, which was a 21-ton tracked chassis which could accommodate three crew members and eight passengers. It initially carried a single 20 mm autocannon in a one-man turret but in 1976 a two-man turret was introduced; this carried a 25 mm autocannon, a co-axial machine gun, and a TOW anti-tank missile launcher. The XM723 possessed amphibious capability, nine firing ports, and spaced laminate armor on its hull. It was accepted for service with the US Army in 1980 as the Bradley Fighting Vehicle. Successive variants have been retrofitted with improved missile systems, gas particulate filter systems, Kevlar spall liners, and increased stowage. The amount of space taken up by the hull and stowage modifications has reduced the number of passengers to six.

By 1982 30,000 IFVs had entered service worldwide, and the IFV concept appeared in the doctrines of 30 national armies. The popularity of the IFV was increased by the growing trend on the part of many nations to mechanize armies previously dominated by light infantry. However, contrary to expectation the IFV did not render APCs obsolete. The US, Russian, French, and German armies have all retained large fleets of IFVs and APCs, finding the APC more suitable for multi-purpose or auxiliary roles. The British Army was one of the few Western armies which had neither recognized a niche for IFVs nor adopted a dedicated IFV design by the late 1970s. In 1980, it made the decision to adopt a new tracked armored vehicle, the FV510 Warrior. While normally classified as an IFV, the Warrior fills the role of an APC in British service and infantrymen do not remain embarked during combat.

The role of the IFV is closely linked to mechanized infantry doctrine. While some IFVs are armed with an organic direct fire gun or anti-tank guided missiles for close infantry support, they are not intended to assault armored and mechanized forces with any type of infantry on their own, mounted or not. Rather, the IFV's role is to give an infantry unit tactical and strategic mobility during combined arms operations. Most IFVs either complement tanks as part of an armored battalion, brigade, or division; others perform traditional infantry missions supported by tanks. Early development of IFVs in a number of Western nations was promoted primarily by armor officers who wanted to integrate tanks with supporting infantry in armored divisions. There were a few exceptions to the rule: for example, the Bundeswehr's decision to adopt the SPz 12-13 was largely due to the experiences of Wehrmacht panzergrenadiers who had been inappropriately ordered to undertake combat operations better suited for armor. Hence, the Bundeswehr concluded that infantry should only fight while mounted in their own armored vehicles, ideally supported by tanks. This doctrinal trend was later subsumed into the armies of other Western nations, including the US, leading to the widespread conclusion that IFVs should be confined largely to assisting the forward momentum of tanks. The Soviet Army granted more flexibility in this regard to its IFV doctrine, allowing for the mechanized infantry to occupy terrain that compromised an enemy defense, carry out flanking movements, or lure armor into ill-advised counterattacks. While they still performed an auxiliary role to tanks, the notion of using IFVs in these types of engagements dictated that they be heavily armed, which was reflected in the BMP-1 and its successors. Additionally, Soviet airborne doctrine made use of the BMD series of IFVs to operate in concert with paratroops rather than traditional mechanized or armored formations.

IFVs assumed a new significance after the Yom Kippur War. In addition to heralding the combat debut of the BMP-1, that conflict demonstrated the newfound significance of anti-tank guided missiles and the obsolescence of independent armored attacks. More emphasis was placed on combined arms offensives, and the importance of mechanized infantry to support tanks reemerged. As a result of the Yom Kippur War, the Soviet Union attached more infantry to its armored formations and the US accelerated its long-delayed IFV development program. An IFV capable of accompanying tanks for the purpose of suppressing anti-tank weapons and the hostile infantry which operated them was seen as necessary to avoid the devastation wreaked on purely armored Israeli formations.

The US Army defines all vehicles classed as IFVs as having three essential characteristics: they are armed with at least a medium-caliber cannon or automatic grenade launcher, at least sufficiently protected against small arms fire, and possess off-road mobility. It also identifies all IFVs as having some characteristics of an APC and a light tank. 

The United Nations Register for Conventional Arms (UNROCA) simply defines an IFV as any armored vehicle "designed to fight with soldiers on board" and "to accompany tanks". UNROCA makes a clear distinction between IFVs and APCs, as the former's primary mission is combat rather than general transport.

All IFVs possess armored hulls protected against rifle and machine gun fire. Most have lighter armor than main battle tanks to ensure mobility. Armies have generally accepted risk in reduced protection to recapitalize on an IFV's mobility, weight and speed. Their fully enclosed hulls offer protection from artillery fragments and residual environmental contaminants as well as limit exposure time to the mounted infantry during extended movements over open ground. Many IFVs also have sharply angled hulls that offer a relatively high degree of protection for their armor thickness. The BMP, Boragh, BVP M-80, and their respective variants all possess steel hulls with a distribution of armor and steep angling that protect them during frontal advances. The BMP-1 was vulnerable to heavy machine gun at close range on its flanks or rear, leading to a variety of more heavily armored marks appearing from 1979 onward. The Bradley possessed a lightweight aluminum alloy hull, which in most successive marks has been bolstered by the addition of explosive reactive and slat armor, spaced laminate belts, and steel track skirts. Throughout its life cycle, an IFV is expected to gain 30% more weight from armor additions.

As asymmetric conflicts become more common, an increasing concern with regards to IFV protection has been adequate countermeasures against land mines and improvised explosive devices. During the Iraq War, inadequate mine protection in US Bradleys forced their crews to resort to makeshift strategies such as lining the hull floors with sandbags. A few IFVs, such as the Ratel, have been specifically engineered to resist mine explosions.

IFVs may be armed with 20mm to 57mm automatic cannon, 73mm to 100mm low or medium velocity guns, anti-tank guided missiles, or automatic grenade launchers.

With a few exceptions, such as the BMP-1 and the BMP-3, designs such as the Marder and the BMP-2 have set the trend of arming IFVs with an autocannon suitable for use against lightly armored vehicles, low-flying aircraft, and dismounted infantry. This reflected the growing inclination to view IFVs as auxiliaries of armored formations: a small or medium caliber autocannon was perceived as an ideal suppressive weapon to complement large caliber tank fire. IFVs armed with miniature tank guns did not prove popular because many of the roles they were expected to perform were better performed by accompanying tanks. 

The BMP-1, which was the first IFV to carry a relatively large gun, came under criticism during the Yom Kippur War for its mediocre individual accuracy, due in part to the low velocities of its projectiles. During the Soviet–Afghan War, BMP-1 crews also complained that their armament lacked the elevation necessary to engage insurgents in mountainous terrain. The effectiveness of large caliber, low-velocity guns like the 2A28 Grom on the BMP-1 and BMD-1 was also much reduced by the appearance of Chobham armor on Western tanks. The Ratel, which included a variant armed with a 90mm low-velocity gun, was utilized in South African combat operations against Angolan and Cuban armored formations during the South African Border War, with mixed results. Although the Ratels succeeded in destroying a large number of Angolan tanks and APCs, they were hampered by many of the same problems as the BMP-1: mediocre standoff ranges, inferior fire control, and a lack of stabilized main gun. The Ratels' heavy armament also tempted South African commanders to utilize them as light tanks rather than in their intended role of infantry support.

Another design feature of the BMP-1 did prove more successful in establishing a precedent for future IFVs: its inclusion of an anti-tank missile system. This consisted of a rail-launcher firing 9M14 Malyutka missiles which had to be reloaded manually from outside the BMP's turret. Crew members had to expose themselves to enemy fire to reload the missiles, and they could not guide them effectively from inside the confines of the turret space. The BMP-2 and later variants of the BMP-1 made use of semiautonomous guided missile systems. In 1978, the Bundeswehr became the first Western army to embrace this trend when it retrofitted all its Marders with launchers for MILAN anti-tank missiles. The US Army added a launcher for TOW anti-tank missiles to its fleet of Bradleys, despite the fact that this greatly reduced the interior space available for seating the embarked infantry. This was justified on the basis that the Bradley needed to not only engage and destroy other IFVs, but support tanks in the destruction of other tanks during combined arms operations.

IFVs are designed to have the strategic and tactical mobility necessary to keep pace with tanks during rapid maneuvers. Some, like the BMD series, have airborne and amphibious capabilities. IFVs may be either wheeled or tracked; tracked IFVs are usually more heavily armored and possess greater carrying capacity. Wheeled IFVs are cheaper and simpler to produce, maintain, and operate. From a logistical perspective, they are also ideal for an army without widespread access to transporters or a developed rail network to deploy its armor.



</doc>
<doc id="15167" url="https://en.wikipedia.org/wiki?curid=15167" title="ICQ">
ICQ

ICQ is an instant messaging client that was first developed and popularized by the Israeli company Mirabilis in 1996. The name ICQ derives from the English phrase "I Seek You". Ownership of ICQ passed from Mirabilis to AOL in 1998, and from AOL to Mail.Ru Group in 2010.

The ICQ client application and service were initially released in November 1996 and the client was freely available to download. Users could register an account and would be assigned a number, like a phone number, for others to be able to contact them (users could also provide handles). ICQ was among the first stand-alone instant messenger and the first online instant messenger service — while real-time chat was not in itself new to the internet (IRC being the most common platform at the time), the concept of a fully centralized service with individual user accounts focused on one-on-one conversations set the blueprint for later instant messaging services like AIM, and its influence is seen in modern social media applications. Whilst Internet Relay Chat programs, like Pirch and mIRC, dominated the market for many years, their popularity began to fall in the face of mobile computing which had begun to take hold with PalmPilot and the fledgling Microsoft Windows Compact Edition.

At its peak around 2001, ICQ had more than 100 million accounts registered. At the time of the Mail.Ru acquisition in 2010, there were around 42 million daily users. Since 2013, ICQ has 11 million monthly users.


ICQ provides all users additional services and content products:
Also, users can choose and select their own avatars for their profile pages. In this way, they can protect their privacy.

ICQ users are identified and distinguished from one another by UIN, or User Identification Numbers, distributed in sequential order. The UIN was invented by Mirabilis, as the user name assigned to each user upon registration. Issued UINs started at '10,000' (5 digits) and every user receives a UIN when first registering with ICQ. As of ICQ6 users are also able to log in using the specific e-mail address they associated with their UIN during the initial registration process.
Unlike other instant messaging software or web applications, on ICQ the only permanent user info is the UIN, although it is possible to search for other users using their associated e-mail address or any other detail they have made public by updating it in their account's public profile. In addition the user can change all of his or her personal information, including screen name and e-mail address, without having to re-register. Since 2000 ICQ and AIM users are able to add each other to their contact list without the need for any external clients. As a response to UIN theft or sale of attractive UINs, ICQ started to store email addresses previously associated with a UIN. As such UINs that are stolen can sometimes be reclaimed. This applies only if (since 1999 onwards) a valid primary email address was entered into the user profile.

The founding company of ICQ, Mirabilis, was established in June 1996 by five Israelis: Yair Goldfinger, Sefi Vigiser, Amnon Amir, Arik Vardi, and Arik's father Yossi Vardi. They recognized that many people were accessing the internet through non-UNIX operating systems, such as Microsoft Windows, and those users were unfamiliar with established chat technologies, e.g. IRC.

The technology Mirabilis developed for ICQ was distributed free of charge. The technology's success encouraged AOL to acquire Mirabilis on June 8, 1998, for $287 million up front and $120 million in additional payments over three years based on performance levels. At the time this was the highest price ever paid to purchase an Israeli technology company. In 2002 AOL successfully patented the technology.

After the purchase the product was initially managed by Ariel Yarnitsky and Avi Shechter. ICQ's management changed at the end of 2003. Under the leadership of the new CEO, Orey Gilliam, who also assumed the responsibility for all of AOL's messaging business in 2007, ICQ resumed its growth; it was not only a highly profitable company, but one of AOL's most successful businesses. Eliav Moshe replaced Gilliam in 2009 and became ICQ's managing director.

In April 2010, AOL sold ICQ to Digital Sky Technologies, headed by Alisher Usmanov, for $187.5 million. While ICQ was displaced by AOL Instant Messenger, Google Talk, and other competitors in the U.S. and many other countries over the 2000s, it remained the most popular instant messaging network in Russian-speaking countries, and an important part of online culture. Popular UINs demanded over 11,000 rubles in 2010.

In September of that year, Digital Sky Technologies changed its name to Mail.Ru Group. Since the acquisition, Mail.ru has invested in turning ICQ from a desktop client to a mobile messaging system. As of 2013, around half of ICQ’s users were using its mobile apps, and in 2014, the number of users began growing for the first time since the purchase.

In March 2016 the source code of the client was released under the Apache license released on github.com.


When accepting "ICQ Privacy Policy" (2011), a user gives all the copyright in the posted information to ICQ Inc. This implies that ICQ Inc. may publish, distribute etc. any messages sent through the system despite any privacy intentions of the user:

Please be aware that any information you may have published in a public area (Public Area) may be accessible to ICQ or third parties and used by ICQ or such third parties. Therefore carefully consider whether you wish such information to be accessible. Public areas may include your user profile (depending on the level of your privacy settings) any blogs, message boards, forums or similar services made available through the ICQ Services.

AOL pursued an aggressive policy regarding alternative ("unauthorized") ICQ clients.

"Системное сообщение
System Message

On icq.com there is an "important message" for Russian-speaking ICQ users: "ICQ осуществляет поддержку только авторизированных версий программ: ICQ Lite и ICQ 6.5." ("ICQ supports only authorized versions of programs: ICQ Lite and ICQ 6.5.")

AOL's OSCAR network protocol used by ICQ is proprietary and using a third party client is a violation of ICQ Terms of Service, nevertheless a number of third-party clients have been created by using reverse-engineering and protocol descriptions. These clients include:

AOL supported clients include:




</doc>
<doc id="15169" url="https://en.wikipedia.org/wiki?curid=15169" title="Impressionism">
Impressionism

Impressionism is a 19th-century art movement characterised by relatively small, thin, yet visible brush strokes, open composition, emphasis on accurate depiction of light in its changing qualities (often accentuating the effects of the passage of time), ordinary subject matter, inclusion of "movement" as a crucial element of human perception and experience, and unusual visual angles. Impressionism originated with a group of Paris-based artists whose independent exhibitions brought them to prominence during the 1870s and 1880s.

The Impressionists faced harsh opposition from the conventional art community in France. The name of the style derives from the title of a Claude Monet work, "Impression, soleil levant" ("Impression, Sunrise"), which provoked the critic Louis Leroy to coin the term in a satirical review published in the Parisian newspaper "Le Charivari".

The development of Impressionism in the visual arts was soon followed by analogous styles in other media that became known as impressionist music and impressionist literature.

 Radicals in their time, early Impressionists violated the rules of academic painting. They constructed their pictures from freely brushed colours that took precedence over lines and contours, following the example of painters such as Eugène Delacroix and J. M. W. Turner. They also painted realistic scenes of modern life, and often painted outdoors. Previously, still lifes and portraits as well as landscapes were usually painted in a studio. The Impressionists found that they could capture the momentary and transient effects of sunlight by painting outdoors or "en plein air". They portrayed overall visual effects instead of details, and used short "broken" brush strokes of mixed and pure unmixed colour—not blended smoothly or shaded, as was customary—to achieve an effect of intense colour vibration.

Impressionism emerged in France at the same time that a number of other painters, including the Italian artists known as the Macchiaioli, and Winslow Homer in the United States, were also exploring "plein-air" painting. The Impressionists, however, developed new techniques specific to the style. Encompassing what its adherents argued was a different way of seeing, it is an art of immediacy and movement, of candid poses and compositions, of the play of light expressed in a bright and varied use of colour.

The public, at first hostile, gradually came to believe that the Impressionists had captured a fresh and original vision, even if the art critics and art establishment disapproved of the new style. By recreating the sensation in the eye that views the subject, rather than delineating the details of the subject, and by creating a welter of techniques and forms, Impressionism is a precursor of various painting styles, including Neo-Impressionism, Post-Impressionism, Fauvism, and Cubism.

In the middle of the 19th century—a time of change, as Emperor Napoleon III rebuilt Paris and waged war—the Académie des Beaux-Arts dominated French art. The Académie was the preserver of traditional French painting standards of content and style. Historical subjects, religious themes, and portraits were valued; landscape and still life were not. The Académie preferred carefully finished images that looked realistic when examined closely. Paintings in this style were made up of precise brush strokes carefully blended to hide the artist's hand in the work. Colour was restrained and often toned down further by the application of a golden varnish.

The Académie had an annual, juried art show, the Salon de Paris, and artists whose work was displayed in the show won prizes, garnered commissions, and enhanced their prestige. The standards of the juries represented the values of the Académie, represented by the works of such artists as Jean-Léon Gérôme and Alexandre Cabanel.

In the early 1860s, four young painters—Claude Monet, Pierre-Auguste Renoir, Alfred Sisley, and Frédéric Bazille—met while studying under the academic artist Charles Gleyre. They discovered that they shared an interest in painting landscape and contemporary life rather than historical or mythological scenes. Following a practice that had become increasingly popular by mid-century, they often ventured into the countryside together to paint in the open air, but not for the purpose of making sketches to be developed into carefully finished works in the studio, as was the usual custom. By painting in sunlight directly from nature, and making bold use of the vivid synthetic pigments that had become available since the beginning of the century, they began to develop a lighter and brighter manner of painting that extended further the Realism of Gustave Courbet and the Barbizon school. A favourite meeting place for the artists was the Café Guerbois on Avenue de Clichy in Paris, where the discussions were often led by Édouard Manet, whom the younger artists greatly admired. They were soon joined by Camille Pissarro, Paul Cézanne, and Armand Guillaumin.
During the 1860s, the Salon jury routinely rejected about half of the works submitted by Monet and his friends in favour of works by artists faithful to the approved style. In 1863, the Salon jury rejected Manet's "The Luncheon on the Grass" "(Le déjeuner sur l'herbe)" primarily because it depicted a nude woman with two clothed men at a picnic. While the Salon jury routinely accepted nudes in historical and allegorical paintings, they condemned Manet for placing a realistic nude in a contemporary setting. The jury's severely worded rejection of Manet's painting appalled his admirers, and the unusually large number of rejected works that year perturbed many French artists.

After Emperor Napoleon III saw the rejected works of 1863, he decreed that the public be allowed to judge the work themselves, and the Salon des Refusés (Salon of the Refused) was organized. While many viewers came only to laugh, the Salon des Refusés drew attention to the existence of a new tendency in art and attracted more visitors than the regular Salon.

Artists' petitions requesting a new Salon des Refusés in 1867, and again in 1872, were denied. In December 1873, Monet, Renoir, Pissarro, Sisley, Cézanne, Berthe Morisot, Edgar Degas and several other artists founded the "Société Anonyme Coopérative des Artistes Peintres, Sculpteurs, Graveurs" ("Cooperative and Anonymous Association of Painters, Sculptors, and Engravers") to exhibit their artworks independently. Members of the association were expected to forswear participation in the Salon. The organizers invited a number of other progressive artists to join them in their inaugural exhibition, including the older Eugène Boudin, whose example had first persuaded Monet to adopt "plein air" painting years before. Another painter who greatly influenced Monet and his friends, Johan Jongkind, declined to participate, as did Édouard Manet. In total, thirty artists participated in their first exhibition, held in April 1874 at the studio of the photographer Nadar.
The critical response was mixed. Monet and Cézanne received the harshest attacks. Critic and humorist Louis Leroy wrote a scathing review in the newspaper "Le Charivari" in which, making wordplay with the title of Claude Monet's "Impression, Sunrise" "(Impression, soleil levant)", he gave the artists the name by which they became known. Derisively titling his article "", Leroy declared that Monet's painting was at most, a sketch, and could hardly be termed a finished work.

He wrote, in the form of a dialog between viewers,

The term "Impressionist" quickly gained favour with the public. It was also accepted by the artists themselves, even though they were a diverse group in style and temperament, unified primarily by their spirit of independence and rebellion. They exhibited together—albeit with shifting membership—eight times between 1874 and 1886. The Impressionists' style, with its loose, spontaneous brushstrokes, would soon become synonymous with modern life.

Monet, Sisley, Morisot, and Pissarro may be considered the "purest" Impressionists, in their consistent pursuit of an art of spontaneity, sunlight, and colour. Degas rejected much of this, as he believed in the primacy of drawing over colour and belittled the practice of painting outdoors. Renoir turned away from Impressionism for a time during the 1880s, and never entirely regained his commitment to its ideas. Édouard Manet, although regarded by the Impressionists as their leader, never abandoned his liberal use of black as a colour, and never participated in the Impressionist exhibitions. He continued to submit his works to the Salon, where his painting "Spanish Singer" had won a 2nd class medal in 1861, and he urged the others to do likewise, arguing that "the Salon is the real field of battle" where a reputation could be made.
Among the artists of the core group (minus Bazille, who had died in the Franco-Prussian War in 1870), defections occurred as Cézanne, followed later by Renoir, Sisley, and Monet, abstained from the group exhibitions so they could submit their works to the Salon. Disagreements arose from issues such as Guillaumin's membership in the group, championed by Pissarro and Cézanne against opposition from Monet and Degas, who thought him unworthy. Degas invited Mary Cassatt to display her work in the 1879 exhibition, but also insisted on the inclusion of Jean-François Raffaëlli, Ludovic Lepic, and other realists who did not represent Impressionist practices, causing Monet in 1880 to accuse the Impressionists of "opening doors to first-come daubers". The group divided over invitations to Paul Signac and Georges Seurat to exhibit with them in 1886. Pissarro was the only artist to show at all eight Impressionist exhibitions.

The individual artists achieved few financial rewards from the Impressionist exhibitions, but their art gradually won a degree of public acceptance and support. Their dealer, Durand-Ruel, played a major role in this as he kept their work before the public and arranged shows for them in London and New York. Although Sisley died in poverty in 1899, Renoir had a great Salon success in 1879. Monet became secure financially during the early 1880s and so did Pissarro by the early 1890s. By this time the methods of Impressionist painting, in a diluted form, had become commonplace in Salon art.

French painters who prepared the way for Impressionism include the Romantic colourist Eugène Delacroix, the leader of the realists Gustave Courbet, and painters of the Barbizon school such as Théodore Rousseau. The Impressionists learned much from the work of Johan Barthold Jongkind, Jean-Baptiste-Camille Corot and Eugène Boudin, who painted from nature in a direct and spontaneous style that prefigured Impressionism, and who befriended and advised the younger artists.

A number of identifiable techniques and working habits contributed to the innovative style of the Impressionists. Although these methods had been used by previous artists—and are often conspicuous in the work of artists such as Frans Hals, Diego Velázquez, Peter Paul Rubens, John Constable, and J. M. W. Turner—the Impressionists were the first to use them all together, and with such consistency. These techniques include:


New technology played a role in the development of the style. Impressionists took advantage of the mid-century introduction of premixed paints in tin tubes (resembling modern toothpaste tubes), which allowed artists to work more spontaneously, both outdoors and indoors. Previously, painters made their own paints individually, by grinding and mixing dry pigment powders with linseed oil, which were then stored in animal bladders.

Many vivid synthetic pigments became commercially available to artists for the first time during the 19th century. These included cobalt blue, viridian, cadmium yellow, and synthetic ultramarine blue, all of which were in use by the 1840s, before Impressionism. The Impressionists' manner of painting made bold use of these pigments, and of even newer colours such as cerulean blue, which became commercially available to artists in the 1860s.

The Impressionists' progress toward a brighter style of painting was gradual. During the 1860s, Monet and Renoir sometimes painted on canvases prepared with the traditional red-brown or grey ground. By the 1870s, Monet, Renoir, and Pissarro usually chose to paint on grounds of a lighter grey or beige colour, which functioned as a middle tone in the finished painting. By the 1880s, some of the Impressionists had come to prefer white or slightly off-white grounds, and no longer allowed the ground colour a significant role in the finished painting.

Prior to the Impressionists, other painters, notably such 17th-century Dutch painters as Jan Steen, had emphasized common subjects, but their methods of composition were traditional. They arranged their compositions so that the main subject commanded the viewer's attention. The Impressionists relaxed the boundary between subject and background so that the effect of an Impressionist painting often resembles a snapshot, a part of a larger reality captured as if by chance. Photography was gaining popularity, and as cameras became more portable, photographs became more candid. Photography inspired Impressionists to represent momentary action, not only in the fleeting lights of a landscape, but in the day-to-day lives of people. 
The development of Impressionism can be considered partly as a reaction by artists to the challenge presented by photography, which seemed to devalue the artist's skill in reproducing reality. Both portrait and landscape paintings were deemed somewhat deficient and lacking in truth as photography "produced lifelike images much more efficiently and reliably".

In spite of this, photography actually inspired artists to pursue other means of creative expression, and rather than compete with photography to emulate reality, artists focused "on the one thing they could inevitably do better than the photograph—by further developing into an art form its very subjectivity in the conception of the image, the very subjectivity that photography eliminated". The Impressionists sought to express their perceptions of nature, rather than create exact representations. This allowed artists to depict subjectively what they saw with their "tacit imperatives of taste and conscience". Photography encouraged painters to exploit aspects of the painting medium, like colour, which photography then lacked: "The Impressionists were the first to consciously offer a subjective alternative to the photograph".

Another major influence was Japanese ukiyo-e art prints (Japonism). The art of these prints contributed significantly to the "snapshot" angles and unconventional compositions that became characteristic of Impressionism. An example is Monet's "Jardin à Sainte-Adresse", 1867, with its bold blocks of colour and composition on a strong diagonal slant showing the influence of Japanese prints

Edgar Degas was both an avid photographer and a collector of Japanese prints. His "The Dance Class" "(La classe de danse)" of 1874 shows both influences in its asymmetrical composition. The dancers are seemingly caught off guard in various awkward poses, leaving an expanse of empty floor space in the lower right quadrant. He also captured his dancers in sculpture, such as the "Little Dancer of Fourteen Years".

The central figures in the development of Impressionism in France, listed alphabetically, were:

Among the close associates of the Impressionists were several painters who adopted their methods to some degree. These include Jean-Louis Forain (who participated in Impressionist exhibitions in 1879, 1880, 1881 and 1886) and Giuseppe De Nittis, an Italian artist living in Paris who participated in the first Impressionist exhibit at the invitation of Degas, although the other Impressionists disparaged his work. Federico Zandomeneghi was another Italian friend of Degas who showed with the Impressionists. Eva Gonzalès was a follower of Manet who did not exhibit with the group. James Abbott McNeill Whistler was an American-born painter who played a part in Impressionism although he did not join the group and preferred grayed colours. Walter Sickert, an English artist, was initially a follower of Whistler, and later an important disciple of Degas; he did not exhibit with the Impressionists. In 1904 the artist and writer Wynford Dewhurst wrote the first important study of the French painters published in English, "Impressionist Painting: its genesis and development", which did much to popularize Impressionism in Great Britain.

By the early 1880s, Impressionist methods were affecting, at least superficially, the art of the Salon. Fashionable painters such as Jean Béraud and Henri Gervex found critical and financial success by brightening their palettes while retaining the smooth finish expected of Salon art. Works by these artists are sometimes casually referred to as Impressionism, despite their remoteness from Impressionist practice.

The influence of the French Impressionists lasted long after most of them had died. Artists like J.D. Kirszenbaum were borrowing Impressionist techniques throughout the twentieth century.

As the influence of Impressionism spread beyond France, artists, too numerous to list, became identified as practitioners of the new style. Some of the more important examples are:

The sculptor Auguste Rodin is sometimes called an Impressionist for the way he used roughly modeled surfaces to suggest transient light effects.

Pictorialist photographers whose work is characterized by soft focus and atmospheric effects have also been called Impressionists.

French Impressionist Cinema is a term applied to a loosely defined group of films and filmmakers in France from 1919–1929, although these years are debatable. French Impressionist filmmakers include Abel Gance, Jean Epstein, Germaine Dulac, Marcel L’Herbier, Louis Delluc, and Dmitry Kirsanoff.

Musical Impressionism is the name given to a movement in European classical music that arose in the late 19th century and continued into the middle of the 20th century. Originating in France, musical Impressionism is characterized by suggestion and atmosphere, and eschews the emotional excesses of the Romantic era. Impressionist composers favoured short forms such as the nocturne, arabesque, and prelude, and often explored uncommon scales such as the whole tone scale. Perhaps the most notable innovations of Impressionist composers were the introduction of major 7th chords and the extension of chord structures in 3rds to five- and six-part harmonies.

The influence of visual Impressionism on its musical counterpart is debatable. Claude Debussy and Maurice Ravel are generally considered the greatest Impressionist composers, but Debussy disavowed the term, calling it the invention of critics. Erik Satie was also considered in this category, though his approach was regarded as less serious, more musical novelty in nature. Paul Dukas is another French composer sometimes considered an Impressionist, but his style is perhaps more closely aligned to the late Romanticists. Musical Impressionism beyond France includes the work of such composers as Ottorino Respighi (Italy) Ralph Vaughan Williams, Cyril Scott, and John Ireland (England), and Manuel De Falla, and Isaac Albeniz (Spain).

The term Impressionism has also been used to describe works of literature in which a few select details suffice to convey the sensory impressions of an incident or scene. Impressionist literature is closely related to Symbolism, with its major exemplars being Baudelaire, Mallarmé, Rimbaud, and Verlaine. Authors such as Virginia Woolf, D.H. Lawrence, and Joseph Conrad have written works that are Impressionistic in the way that they describe, rather than interpret, the impressions, sensations and emotions that constitute a character's mental life.

Post-Impressionism developed from Impressionism. During the 1880s several artists began to develop different precepts for the use of colour, pattern, form, and line, derived from the Impressionist example: Vincent van Gogh, Paul Gauguin, Georges Seurat, and Henri de Toulouse-Lautrec. These artists were slightly younger than the Impressionists, and their work is known as post-Impressionism. Some of the original Impressionist artists also ventured into this new territory; Camille Pissarro briefly painted in a pointillist manner, and even Monet abandoned strict "plein air" painting. Paul Cézanne, who participated in the first and third Impressionist exhibitions, developed a highly individual vision emphasising pictorial structure, and he is more often called a post-Impressionist. Although these cases illustrate the difficulty of assigning labels, the work of the original Impressionist painters may, by definition, be categorised as Impressionism.



</doc>
<doc id="15172" url="https://en.wikipedia.org/wiki?curid=15172" title="Internet slang">
Internet slang

Internet slang (Internet shorthand, cyber-slang, netspeak, or chatspeak) refers to various kinds of slang used by different people on the Internet. An example of Internet slang is "codice_1" meaning "laugh out loud". It is difficult to provide a standardized definition of Internet slang due to the constant changes made to its nature. However, it can be understood to be any type of slang that Internet users have popularized, and in many cases, have coined. Such terms often originate with the purpose of saving keystrokes or to compensate for small character limits. Many people use the same abbreviations in texting and instant messaging, and social networking websites. Acronyms, keyboard symbols and abbreviations are common types of Internet slang. New dialects of slang, such as leet or Lolspeak, develop as ingroup internet memes rather than time savers.Some people only use LOL for fun. Many people use this internet slang not only on the Internet but also face-to-face.

Internet slang originated in the early days of the Internet with some terms predating the Internet. Internet slang is used in chat rooms, social networking services, online games, video games and in the online community. Since 1979, users of communications networks like Usenet created their own shorthand.

In Japanese, the term moe has come into common use among slang users to mean something extremely cute and appealing.

Aside from the more frequent abbreviations, acronyms, and emoticons, Internet slang also uses archaic words or the lesser-known meanings of mainstream terms. Regular words can also be altered into something with a similar pronunciation but altogether different meaning, or attributed new meanings altogether. Phonetic transcriptions of foreign words, such as the transformation of "impossible" into "codice_2" in Japanese and then [the transliteration of that] back to [the character set used for] English, also occur. In places where logographic languages are used, such as China, a visual Internet slang exists, giving characters dual meanings, one direct and one implied.

The primary motivation for using a slang unique to the Internet is to ease communication. However, while Internet slang shortcuts save time for the writer, they take two times as long for the reader to understand, according to a study by the University of Tasmania. On the other hand, similar to the use of slang in traditional face-to-face speech or written language, slang on the Internet is often a way of indicating group membership.

Internet slang provides a channel which facilitates and constrains our ability to communicate in ways that are fundamentally different from those found in other semiotic situations. Many of the expectations and practices which we associate with spoken and written language are no longer applicable. The Internet itself is ideal for new slang to emerge because of the richness of the medium and the availability of information. Slang is also thus motivated for the “creation and sustenance of online communities”. These communities, in turn, play a role in solidarity or identification or an exclusive or common cause.

David Crystal distinguishes among five areas of the Internet where slang is used- The Web itself, email, asynchronous chat (for example, mailing lists), synchronous chat (for example, Internet Relay Chat), and virtual worlds. The electronic character of the channel has a fundamental influence on the language of the medium. Options for communication are constrained by the nature of the hardware needed in order to gain Internet access. Thus, productive linguistic capacity (the type of information that can be sent) is determined by the preassigned characters on a keyboard, and receptive linguistic capacity (the type of information that can be seen) is determined by the size and configuration of the screen. Additionally, both sender and receiver are constrained linguistically by the properties of the internet software, computer hardware, and networking hardware linking them. Electronic discourse refers to writing that is "very often reads as if it were being spoken – that is, as if the sender were writing talking".

Internet slang does not constitute a homogeneous language variety. Rather, it differs according to the user and type of Internet situation. However, within the language of Internet slang, there is still an element of prescriptivism, as seen in style guides, for example "Wired Style", which are specifically aimed at usage on the Internet. Even so, few users consciously heed these prescriptive recommendations on CMC, but rather adapt their styles based on what they encounter online. Although it is difficult to produce a clear definition of Internet slang, the following types of slang may be observed. This list is not exhaustive.

There have been ongoing debates about how the use of slang on the Internet influences language usage outside of technology. Even though the direct causal relationship between the Internet and language has yet to be proven by any scientific research, Internet slang has invited split views on its influence on the standard of language use in non-computer-mediated communications.

Prescriptivists tend to have the widespread belief that the Internet has a negative influence on the future of language, and that it would lead to a degradation of standard. Some would even attribute any decline of standard formal English to the increase in usage of electronic communication. It has also been suggested that the linguistic differences between Standard English and CMC can have implications for literacy education. This is illustrated by the widely reported example of a school essay submitted by a Scottish teenager, which contained many abbreviations and acronyms likened to SMS language. There was great condemnation of this style by the mass media as well as educationists, who expressed that this showed diminishing literacy or linguistic abilities.

On the other hand, descriptivists have counter-argued that the Internet allows better expressions of a language. Rather than established linguistic conventions, linguistic choices sometimes reflect personal taste. It has also been suggested that as opposed to intentionally flouting language conventions, Internet slang is a result of a lack of motivation to monitor speech online. Hale and Scanlon describe language in Emails as being derived from "writing the way people talk", and that there is no need to insist on 'Standard' English. English users, in particular, have an extensive tradition of etiquette guides, instead of traditional prescriptive treatises, that offer pointers on linguistic appropriateness. Using and spreading Internet slang also adds onto the cultural currency of a language. It is important to the speakers of the language due to the foundation it provides for identifying within a group, and also for defining a person’s individual linguistic and communicative competence. The result is a specialized subculture based on its use of slang.

In scholarly research, attention has, for example, been drawn to the effect of the use of Internet slang in ethnography, and more importantly to how conversational relationships online change structurally because slang is used.

In German, there is already considerable controversy regarding the use of anglicisms outside of CMC. This situation is even more problematic within CMC, since the jargon of the medium is dominated by English terms. An extreme example of an anti-anglicisms perspective can be observed from the chatroom rules of a Christian site, which bans all anglicisms ("Das Verwenden von Anglizismen ist strengstens untersagt!" [Using anglicisms is strictly prohibited!]), and also translates even fundamental terms into German equivalents.

In April 2014, Gawker's editor-in-chief Max Read instituted new writing style guidelines banning internet slang for his writing staff.

Internet slang has crossed from being mediated by the computer into other non-physical domains. Here, these domains are taken to refer to any domain of interaction where interlocutors need not be geographically proximate to one another, and where the Internet is not primarily used. Internet slang is now prevalent in telephony, mainly through short messages (SMS) communication. Abbreviations and interjections, especially, have been popularized in this medium, perhaps due to the limited character space for writing messages on mobile phones. Another possible reason for this spread is the convenience of transferring the existing mappings between expression and meaning into a similar space of interaction.

At the same time, Internet slang has also taken a place as part of everyday offline language, among those with digital access. The nature and content of online conversation is brought forward to direct offline communication through the telephone and direct talking, as well as through written language, such as in writing notes or letters. In the case of interjections, such as numerically based and abbreviated Internet slang, are not pronounced as they are written physically or replaced by any actual action. Rather, they become lexicalized and spoken like non-slang words in a “stage direction” like fashion, where the actual action is not carried out but substituted with a verbal signal. The notions of flaming and trolling have also extended outside the computer, and are used in the same circumstances of deliberate or unintentional implicatures.

The expansion of Internet slang has been furthered through codification and the promotion of digital literacy. The subsequently existing and growing popularity of such references among those online as well as offline has thus advanced Internet slang literacy and globalized it. Awareness and proficiency in manipulating Internet slang in both online and offline communication indicates digital literacy and teaching materials have even been developed to further this knowledge. A South Korean publisher, for example, has published a textbook that details the meaning and context of use for common Internet slang instances and is targeted at young children who will soon be using the Internet. Similarly, Internet slang has been recommended as language teaching material in second language classrooms in order to raise communicative competence by imparting some of the cultural value attached to a language that is available only in slang.

Meanwhile, well-known dictionaries such as the OED and Merriam-Webster have been updated with a significant and growing body of slang jargon. Besides common examples, lesser known slang and slang with a non-English etymology have also found a place in standardized linguistic references. Along with these instances, literature in user-contributed dictionaries such as Urban Dictionary has also been added to. Codification seems to be qualified through frequency of use, and novel creations are often not accepted by other users of slang.

Although Internet slang began as a means of "opposition" to mainstream language, its popularity with today's globalized digitally literate population has shifted it into a part of everyday language, where it also leaves a profound impact.

Frequently used slang also have become conventionalised into memetic "unit[s] of cultural information". These memes in turn are further spread through their use on the Internet, prominently through websites. The Internet as an "information superhighway" is also catalysed through slang. The evolution of slang has also created a 'slang union' as part of a unique, specialised subculture. Such impacts are, however, limited and requires further discussion especially from the non-English world. This is because Internet slang is prevalent in languages more actively used on the Internet, like English, which is the Internet’s lingua franca.

The Internet has helped people from all over the world to become connected to one another, enabling "global" relationships to be formed. As such, it is important for the various types of slang used online to be recognizable for everyone. It is also important to do so because of how other languages are quickly catching up with English on the Internet, following the increase in Internet usage in predominantly non-English speaking countries. In fact, as of May 31, 2011, only approximately 27% of the online population is made up of English speakers.

Different cultures tend to have different motivations behind their choice of slang, on top of the difference in language used. For example, in China, because of the tough Internet regulations imposed, users tend to use certain slang to talk about issues deemed as sensitive to the government. These include using symbols to separate the characters of a word to avoid detection from manual or automated text pattern scanning and consequential censorship. An outstanding example is the use of the term codice_3 to denote censorship. River crab (hexie) is pronounced the same as "harmony"—the official term used to justify political discipline and censorship. As such Chinese netizens reappropriate the official terms in a sarcastic way.

Abbreviations are popular across different cultures, including countries like Japan, China, France, Portugal, etc., and are used according to the particular language the Internet users speak. Significantly, this same style of slang creation is also found in non-alphabetical languages as, for example, a form of "codice_4" or alternative political discourse.

The difference in language often results in miscommunication, as seen in an onomatopoeic example, "codice_5", which sounds like "crying" in Chinese, and "laughing" in Thai. A similar example is between the English "codice_6" and the Spanish "codice_7", where both are onomatopoeic expressions of laughter, but the difference in language also meant a different consonant for the same sound to be produced. For more examples of how other languages express "laughing out loud", see also: LOL

In terms of culture, in Chinese, the numerically based onomatopoeia "codice_8" (), which means to 'kiss and hug you', is used. This is comparable to "codice_9", which many Internet users use. In French, "codice_10" is used in the place of pourquoi, which means 'why'. This is an example of a combination of onomatopoeia and shortening of the original word for convenience when writing online.

In conclusion, every different country has their own language background and cultural differences and hence, they tend to have their own rules and motivations for their own Internet slang. However, at present, there is still a lack of studies done by researchers on some differences between the countries.

On the whole, the popular use of Internet slang has resulted in a unique online and offline community as well as a couple sub-categories of "special internet slang which is different from other slang spread on the whole internet… similar to jargon … usually decided by the sharing community". It has also led to virtual communities marked by the specific slang they use and led to a more homogenized yet diverse online culture.




Examples of other slang terms and their meanings:
Low Key: Keeping things a secrey 
Cuffed: When a individual has a boyfriend or girlfriend
Extra: Over the top 
Salty: it is to be angry or bitter when one makes a comment

</doc>
<doc id="15174" url="https://en.wikipedia.org/wiki?curid=15174" title="Impi">
Impi

Impi is a Zulu word for any armed body of men. However, in English it is often used to refer to a Zulu regiment, which is called an "ibutho" in Zulu. Its beginnings lie far back in historic tribal warfare customs, when groups of armed men called "impis" battled. They were systematised radically by the Zulu king Shaka, who was then only the exiled illegitimate son of king Senzangakhona, but already showing much prowess as a general in the army of Mthethwa king Dingiswayo in the Mthethwa-Ndwandwe war in the early 1810s.

The Zulu impi is popularly identified with the ascent of Shaka, ruler of the relatively small Zulu tribe before its explosion across the landscape of southern Africa, but its earliest shape as a purposeful instrument of statecraft lies in the innovations of the Mthethwa chieftain Dingiswayo, according to some historians (Morris 1965). These innovations in turn drew upon existing tribal customs, such as the "iNtanga". This was an age grade tradition common among many of the Bantu peoples of the continent's southern region. Youths were organised into age groups, with each cohort responsible for certain duties and tribal ceremonies. Periodically, the older age grades were summoned to the kraals of sub-chieftains, or "inDunas", for consultations, assignments, and an induction ceremony that marked their transition from boys to full-fledged adults and warriors, the "ukuButbwa". Kraal or settlement elders generally handled local disputes and issues. Above them were the inDunas, and above the inDunas stood the chief of a particular clan lineage or tribe. The inDunas handled administrative matters for their chiefs – ranging from settlement of disputes, to the collection of taxes. In time of war, the inDunas supervised the fighting men in their areas, forming leadership of the military forces deployed for combat. The age grade "iNtangas", under the guidance of the inDunas, formed the basis for the systematic regimental organisation that would become known worldwide as the impi.

Militarily warfare was mild among the Bantu prior to the rise of Shaka, though it occurred frequently. Objectives were typically limited to such matters as cattle raiding, avenging some personal insult, or resolving disputes over segments of grazing land. Generally a loose mob, called an "impi" participated in these melees. There were no campaigns of extermination against the defeated. They simply moved on to other open spaces on the veldt, and equilibrium was restored. The bow and arrow were known but seldom used. Warfare, like the hunt, depended on skilled spearmen and trackers. The primary weapon was a thin 6-foot throwing spear, the "assegai". Several were carried into combat. Defensive weapons included a small cowhide shield, which was later improved by King Shaka. Many battles were prearranged, with the clan warriors meeting at an assigned place and time, while women and children of the clan watched the festivities from some distance away. Ritualized taunts, single combats and tentative charges were the typical pattern. If the affair did not dissipate before, one side might find enough courage to mount a sustained attack, driving off their enemies. Casualties were usually light. The defeated clan might pay in lands or cattle and have captives to be ransomed, but extermination and mass casualties were rare. Tactics were rudimentary. Outside the ritual battles, the quick raid was the most frequent combat action, marked by burning kraals, seizure of captives, and the driving off of cattle. Pastoral herders and light agriculturalists, the Bantu did not usually build permanent fortifications to fend off enemies. A clan under threat simply packed their meager material possessions, rounded up their cattle and fled until the marauders were gone. If the marauders did not stay to permanently dispossess them of grazing areas, the fleeing clan might return to rebuild in a day or two. The genesis of the Zulu impi thus lies in tribal structures existing long before the coming of Europeans or the Shaka era.

In the early 19th century, a combination of factors began to change the customary pattern. These included rising populations, the growth of white settlement and slaving that dispossessed native peoples both at the Cape and in Portuguese Mozambique, and the rise of ambitious "new men." One such man, a warrior called Dingiswayo ("the Troubled One") of the Mthethwa rose to prominence. Historians such as Donald Morris hold that his political genius laid the basis for a relatively light hegemony. This was established through a combination of diplomacy and conquest, using not extermination or slavery, but strategic reconciliation and judicious force of arms. This hegemony reduced the frequent feuding and fighting among the small clans in the Mthethwa's orbit, transferring their energies to more centralised forces. Under Dingiswayo the age grades came to be regarded as military drafts, deployed more frequently to maintain the new order. It was from these small clans, including among them the eLangeni and the Zulu, that Shaka sprung.

Shaka proved himself to be one of Dingiswayo's most able warriors after the military call up of his age grade to serve in the Mthethwa forces. He fought with his iziCwe regiment wherever he was assigned during this early period, but from the beginning, Shaka's approach to battle did not fit the traditional mould. He began to implement his own individual methods and style, designing the famous short stabbing spear the "iKlwa", a larger, stronger shield, and discarding the oxhide sandals that he felt slowed him down. These methods proved effective on a small scale, but Shaka himself was restrained by his overlord. His conception of warfare was far more extreme that the reconcilitory methods of Dingiswayo. He sought to bring combat to a swift and bloody decision, as opposed to duels of individual champions, scattered raids, or limited skirmishes where casualties were comparatively light. While his mentor and overlord Dingiswayo lived, Shakan methods were reined in, but the removal of this check gave the Zulu chieftain much broader scope. It was under his rule that a much more rigorous mode of tribal warfare came into being. This newer, brutal focus demanded changes in weapons, organisation and tactics.

Shaka is credited with introducing a new variant of the traditional weapon, demoting the long, spindly throwing spear in favour of a heavy-bladed, short-shafted stabbing spear. He is also said to have introduced a larger, heavier cowhide shield ("isihlangu"), and trained his forces to thus close with the enemy in more effective hand-to-hand combat. The throwing spear was not discarded, but standardised like the stabbing implement and carried as a missile weapon, typically discharged at the foe, before close contact. These weapons changes integrated with and facilitated an aggressive mobility and tactical organisation.

As weapons, the Zulu warrior carried the "iklwa" stabbing spear (losing one could result in execution) and a club or cudgel fashioned from dense hardwood known in Zulu as the "iwisa", usually called the knobkerrie in English, for beating an enemy in the manner of a mace. Zulu officers often carried the Zulu Axe, but this weapon was more of a symbol to show their rank. The iklwa – so named because of the sucking sound it made when withdrawn from a human body – with its long (c. 25 cm [9.4 in]) and broad blade was an invention of Shaka that superseded the older thrown "ipapa" (so named because of the "pa-pa" sound it made as it flew through the air). It could theoretically be used both in melee and as a thrown weapon, but warriors were forbidden in Shaka's day from throwing it, which would disarm them and give their opponents something to throw back. Moreover, Shaka felt it discouraged warriors from closing into hand-to-hand combat. Shaka's brother, and successor, Dingane reintroduced greater use of the throwing spear, perhaps as a counter to Boer firearms.

As early as Shaka's reign small numbers of firearms, often obsolete muskets and rifles, were obtained by the Zulus from Europeans by trade. In the aftermath of the defeat of the British at the Battle of Isandlwana many Martini-Henry rifles were captured by the Zulus together with considerable amounts of ammunition. The advantage of this capture is debatable due to the alleged tendency of Zulu warriors to close their eyes when firing such weapons. The possession of firearms did little to change Zulu tactics, which continued to rely on a swift approach to the enemy to bring him into close combat.

All warriors carried a shield made of oxhide, which retained the hair, with a central stiffening shaft of wood, the "mgobo". Shields were the property of the king; they were stored in specialised structures, raised off the ground for protection from vermin, when not issued to the relevant regiment. The large "isihlangu" shield of Shaka's day was about five feet in length and was later partially replaced by the smaller "umbumbuluzo," a shield of identical manufacture but around three and a half feet in length. Close combat relied on co-ordinated use of the "iklwa" and shield. The warrior sought to get the edge of his shield behind the edge of his enemy's, so that he could pull the enemy's shield to the side thus opening him to a thrust with the "iklwa" deep into the abdomen or chest.

The fast-moving host, like all military formations, needed supplies. These were provided by young boys, who were attached to a force and carried rations, cooking pots, sleeping mats, extra weapons and other material. Cattle were sometimes driven on the hoof as a movable larder. Again, such arrangements in the local context were probably nothing unusual. What was different was the systematisation and organisation, a pattern yielding major benefits when the Zulu were dispatched on raiding missions.

Age-grade groupings of various sorts were common in the Bantu tribal culture of the day, and indeed are still important in much of Africa. Age grades were responsible for a variety of activities, from guarding the camp, to cattle herding, to certain rituals and ceremonies. It was customary in Zulu culture for young men to provide limited service to their local chiefs until they were married and recognised as official householders. Shaka manipulated this system, transferring the customary service period from the regional clan leaders to himself, strengthening his personal hegemony. Such groupings on the basis of age, did not constitute a permanent, paid military in the modern Western sense, nevertheless they did provide a stable basis for sustained armed mobilisation, much more so than ad hoc tribal levies or war parties.

Shaka organised the various age grades into regiments, and quartered them in special military kraals, with each regiment having its own distinctive names and insignia. Some historians argue that the large military establishment was a drain on the Zulu economy and necessitated continual raiding and expansion. This may be true since large numbers of the society's men were isolated from normal occupations, but whatever the resource impact, the regimental system clearly built on existing tribal cultural elements that could be adapted and shaped to fit an expansionist agenda.

After their 20th birthdays, young men would be sorted into formal "ibutho" (plural "amabutho") or regiments. They would build their "i=handa" (often referred to as a 'homestead', as it was basically a stockaded group of huts surrounding a corral for cattle), their gathering place when summoned for active service. Active service continued until a man married, a privilege only the king bestowed. The amabutho were recruited on the basis of age rather than regional or tribal origin. The reason for this was to enhance the centralised power of the Zulu king at the expense of clan and tribal leaders. They swore loyalty to the king of the Zulu nation.

Shaka discarded sandals to enable his warriors to run faster. Initially the move was unpopular, but those who objected were simply killed, a practice that quickly concentrated the minds of remaining personnel. Zulu tradition indicates that Shaka hardened the feet of his troops by having them stamp thorny tree and bush branches flat. Shaka drilled his troops frequently, implementing forced marches covering more than fifty miles a day. He also drilled the troops to carry out encirclement tactics (see below). Such mobility gave the Zulu a significant impact in their local region and beyond. Upkeep of the regimental system and training seems to have continued after Shaka's death, although Zulu defeats by the Boers, and growing encroachment by British colonists, sharply curtailed raiding operations prior to the War of 1879. Morris (1965, 1982) records one such mission under King Mpande to give green warriors of the uThulwana regiment experience: a raid into Swaziland, dubbed ""Fund' uThulwana"" by the Zulu, or "Teach the uThulwana".

Impi warriors were trained as early as age six, joining the army as "udibi" porters at first, being enrolled into same-age groups ("intanga"). Until they were "buta"'d, Zulu boys accompanied their fathers and brothers on campaign as servants. Eventually, they would go to the nearest "ikhanda" to "kleza" (literally, "to drink directly from the udder"), at which time the boys would become "inkwebane", cadets. They would spend their time training until they were formally enlisted by the king. They would challenge each other to stick fights, which had to be accepted on pain of dishonor.

In Shaka's day, warriors often wore elaborate plumes and cow tail regalia in battle, but by the Anglo-Zulu War of 1879, many warriors wore only a loin cloth and a minimal form of headdress. The later period Zulu soldier went into battle relatively simply dressed, painting his upper body and face with chalk and red ochre, despite the popular conception of elaborately panoplied warriors. Each "ibutho" had a singular arrangement of headdress and other adornments, so that the Zulu army could be said to have had regimental uniforms; latterly the 'full-dress' was only worn on festive occasions. The men of senior regiments would wear, in addition to their other headdress, the head-ring ("isicoco") denoting their married state. A gradation of shield colour was found, junior regiments having largely dark shields the more senior ones having shields with more light colouring; Shaka's personal regiment "Fasimba" (The Haze) having white shields with only a small patch of darker colour. This shield uniformity was facilitated by the custom of separating the king's cattle into herds based on their coat colours.

Certain adornments were awarded to individual warriors for conspicuous courage in action; these included a type of heavy brass arm-ring ("ingxotha") and an intricate necklace composed of interlocking wooden pegs ("iziqu").

The Zulu typically took the offensive, deploying in the well-known "buffalo horns" formation (). It comprised three elements:


Encirclement tactics are not unique in warfare, and historians note that attempts to surround an enemy were not unknown even in the ritualised battles. The use of separate manoeuvre elements to support a stronger central group is also well known in pre-mechanised tribal warfare, as is the use of reserve echelons farther back. What was unique about the Zulu was the degree of organisation, consistency with which they used these tactics, and the speed at which they executed them. Developments and refinements may have taken place after Shaka's death, as witnessed by the use of larger groupings of regiments by the Zulu against the British in 1879. Missions, available manpower and enemies varied, but whether facing native spear, or European bullet, the impis generally fought in and adhered to the classical buffalo horns pattern.

Regiments and corps. The Zulu forces were generally grouped into three levels: regiments, corps of several regiments, and "armies" or bigger formations, although the Zulu did not use these terms in the modern sense. Although size distinctions were taken account of, any grouping of men on a mission could collectively be called an impi, whether a raiding party of 100 or horde of 10,000. Numbers were not uniform but dependent on a variety of factors, including assignments by the king, or the manpower mustered by various clan chiefs or localities. A regiment might be 400 or 4000 men. These were grouped into corps that took their name from the military kraals where they were mustered, or sometimes the dominant regiment of that locality. There were 4 basic ranks: herdboy assistants, warriors, inDunas and higher ranked supremos for a particular mission.

Higher command and unit leadership. Leadership was not a complicated affair. An inDuna guided each regiment, and he in turn answered to senior izinduna who controlled the corps grouping. Overall guidance of the host was furnished by elder izinduna usually with many years of experience. One or more of these elder chiefs might accompany a big force on an important mission, but there was no single "field marshal" in supreme command of all Zulu forces. Regimental izinduna, like the non-coms of today's army, and yesterday's Roman Centurions, were extremely important to morale and discipline. This was shown during the battle of Isandhlwana. Blanketed by a hail of British bullets, rockets and artillery, the advance of the Zulu faltered. Echoing from the mountain, however, were the shouted cadences and fiery exhortations of their regimental izinduna, who reminded the warriors that their king did not send them to run away. Thus encouraged, the encircling regiments remained in place, maintaining continual pressure, until weakened British dispositions enabled the host to make a final surge forward. (See Morris ref below—"The Washing of the Spears").

As noted above, Shaka was neither the originator of the impi, or the age grade structure, nor the concept of a bigger grouping than the small clan system. His major innovations were to blend these traditional elements in a new way, to systematise the approach to battle, and to standardise organization, methods and weapons, particularly in his adoption of the "ilkwa" – the Zulu thrusting spear, unique long-term regimental units, and the "buffalo horns" formation. Dingswayo's approach was of a loose federation of allies under his hegemony, combining to fight, each with their own contingents, under their own leaders. Shaka dispensed with this, insisting instead on a standardised organisation and weapons package that swept away and replaced old clan allegiances with loyalty to himself. This uniform approach also encouraged the loyalty and identification of warriors with their own distinctive military regiments. In time, these warriors, from many conquered tribes and clans came to regard themselves as one nation- the Zulu. The Marian reforms of Rome in the military sphere are referenced by some writers as similar. While other ancient powers such as the Carthaginians maintained a patchwork of force types, and the legions retained such phalanx-style holdovers like the "triarii," Marius implemented one consistent standardised approach for all the infantry. This enabled more disciplined formations and efficient execution of tactics over time against a variety of enemies. As one military historian notes: 

The impi, in its Shakan form, is best known among Western readers from the Anglo-Zulu War of 1879, particularly the famous Zulu victory at Isandhlwana, but its development was over 60 years in coming before that great clash. To understand the full scope of the impi's performance in battle, military historians of the Zulu typically look to its early operations against internal African enemies, not merely the British interlude. In terms of numbers, the operations of the impi would change- from the Western equivalent of small company and battalion size forces, to manoeuvres in multi-divisional strength of between 10,000 and 40,000 men. The victory won by Zulu king Cetawasyo at Ndondakusuka, for example, two decades before the British invasion involved a deployment of 30,000 troops. These were sizeable formations in regional context but represented the bulk of prime Zulu fighting strength. Few impi-style formations were to routinely achieve this level of mobilisation for a single battle. For example, at Cannae, the Romans deployed 80,000 men, and generally could put tens of thousands more into smaller combat actions). The popular notion of countless attacking black spearmen is a distorted one. Manpower supplies on the continent were often limited. In the words of one historian: "The savage hordes of popular lore seldom materialized on African battlefields." This limited resource base would hurt the Zulu when they confronted technologically advanced world powers such as Britain. The advent of new weapons like firearms would also have a profound impact on the African battlefield, but as will be seen, the impi-style forces largely eschewed firearms, or used them in a minor way. Whether facing native spear or European bullet, impis largely fought as they had since the days of Shaka, from Zululand to Zimbabwe, and from Mozambique to Tanzania.

Upon his accession to power, Shaka was confronted by two potent threats, the Ndwandwes under Zwide, and the Qwabes. Both clans were twice as large as the Zulu. The first key test of the "new model" Shakan impis would be against the Ndwandwe, and the battle offers insight into both Shaka as a commander and the performance of his reorganised combat team. The Zulu king deployed his troops in a strong position on top of Gqokli Hill, using a deep depression on the summit to hide a large central reserve, while grouping his other warriors forward in defensive formation. Shaka also made a decoy gambit -- sending the Zulu cattle off with a small escort, luring Zwide into splitting his force. The battle began in the early morning as the Ndwandwe, under Zwide's son Nomahlanjana, made a series of frontal attacks up the steep hill. Slowed by the incline, and armed only with traditional throwing spears, they were badly mauled by Shaka's men in close quarters fighting. By mid-afternoon, the Ndwandwe were exhausted and their force weakened further by small groups of men going off in search of water. Shaka however had cunningly positioned himself so that his troops had access to a small stream nearby. In the late afternoon the Ndwandwe made a final attack. Leaving a part of their army surrounding the bottom of the hill, they pushed a huge column up to the top, hoping to drive the Zulu down into the blocking forces below. Shaka waited until the column was almost at the top, then ordered his fresh reserves to make a flanking "horn" attack, sprinting down both sides of the hill to encircle and liquidate the ascending Ndwandwe. The rest of the enemy force, which could not clearly see what was happening on the summit was next attacked in another encircling manoeuvre that sent it fleeing. In its first major battle, the Shakan impi had pulled off a multiple envelopment. On the negative side, the Ndwandwe remnants had been able to withdraw intact, and all the Zulu cattle were captured. Shaka furthermore was forced eventually to recall and pull back the warriors to his kraal at kwaBulawayo. Nevertheless, the impi had badly mauled an enemy force over twice its size, killing 5 of Zwide's sons in the process and succeeding in its first major test. A period of rebuilding now commenced and new recruits, either by conquest or alliance were incorporated into the growing Shakan force. Among the newcomers was one Mzilikazi, a small-time chieftain of the Kumalo, and a grandson of Zwide whose father had been killed by Zwide. Mzilikazi would eventually fall out with Shaka, and in fleeing, would extend the concept of the impi even further across the landscape of southern and eastern Africa.

In this period Shaka's power grew, defeating several powerful local rivals and creating a vast monolith that was the most powerful nation in its region.

Shaka's success was to spawn several offshoots of the impi-style formation. Chief among these was the Matebele, under Mzilkhazi, and the Shangaan, under the redoubtable Soshangane. The greatest expansion of the impi outside the Zululand/Zimbabwe area however was to come in East Africa, where bands of Ngoni fighting men, conquered large swathes of territory, using the methods first laid down by Shaka.

The impi clashed with another tactical system introduced by European settlers: the horse-gun system of the Boer Commando. This conflict is often popularly conceived of in terms of the well known battles between Zulu King Dingane and the Boers, most notably at the Battle of Blood River. As will be seen however, this tells only part of the story. The impi was to clash with the mobile commando on the open fields of the high veldt in a series of epic confrontations, in which each force both suffered defeat and enjoyed victory, and both sides acquitted themselves well.

Nearly 35,000 strong, well motivated and supremely confident, the Zulu were a formidable force on their own home ground, despite the almost total lack of modern weaponry. Their greatest assets were their morale, unit leadership, mobility and numbers. Tactically the Zulu acquitted themselves well in at least 3 encounters, Isandhlwana, Hlobane and the smaller Intombi action. Their stealthy approach march, camouflage and noise discipline at Isandhlwana, while not perfect, put them within excellent striking distance of their opponents, where they were able to exploit weaknesses in the camp layout. At Hlobane they caught a British column on the move rather than in the usual fortified position, partially cutting off its retreat and forcing it to withdraw.

Strategically (and perhaps understandably in their own traditional tribal context) they lacked any clear vision of fighting their most challenging war, aside from smashing the three British columns by the weight and speed of their regiments. Despite the Isandhlwana victory, tactically there were major problems as well. They rigidly and predictably applied their three-pronged "buffalo horns" attack, paradoxically their greatest strength, but also their greatest weakness when facing concentrated firepower. The Zulu failed to make use of their superior mobility by attacking the British rear area such as Natal or in interdicting vulnerable British supply lines. However, an important consideration, which King Cetshwayo appreciated, was that there was a clear difference between defending one's territory, and encroaching on another, regardless of the fact that they are at war with the holder of that land. The King realised that peace would be impossible if a real invasion of Natal was launched, and that it would only provoke a more concerted effort on the part of the British against them. The attack on Rorke's Drift, in Natal, was an opportunist raid, as opposed to a real invasion. When they did, they achieved some success, such as the liquidation of a supply detachment at the Intombi River. A more expansive mobile strategy might have cut British communications and brought their lumbering advance to a halt, bottling up the redcoats in scattered strongpoints while the impis ran rampant between them. Just such a scenario developed with the No. 1 British column, which was penned up static and immobile in garrison for over two months at Eshowe.

The Zulu also allowed their opponents too much time to set up fortified strongpoints, assaulting well defended camps and positions with painful losses. A policy of attacking the redcoats while they were strung out on the move, or crossing difficult obstacles like rivers, might have yielded more satisfactory results. For example, four miles past the Ineyzane River, after the British had comfortably crossed, and after they had spent a day consolidating their advance, the Zulu finally launched a typical "buffalo horn" encirclement attack that was seen off with withering fire from not only breech-loading Martini-Henry rifles, but 7-pounder artillery and Gatling guns. In fairness, the Zulu commanders could not conjure regiments out of thin air at the optimum time and place. They too needed time to marshal, supply and position their forces, and sort out final assignments to the three-prongs of attack. Still, the Battle of Hlobane Mountain offers just a glimpse of an alternative mobile scenario, where the manoeuvering Zulu "horns" cut off and drove back Buller's column when it was dangerously strung out on the mountain.

Command and control of the impis was problematic at times. Indeed, the Zulu attacks on the British strongpoints at Rorke's Drift and at Kambula, (both bloody defeats) seemed to have been carried out by over-enthusiastic leaders and warriors despite contrary orders of the Zulu King, Cetshwayo. Popular film re-enactments display a grizzled "izinduna" directing the host from a promontory with elegant sweeps of the hand. This might have happened during the initial marshaling of forces from a jump off point, or the deployment of reserves, but once the great encircling sweep of frenzied warriors in the "horns" and "chest" was in motion, the "izinduna" could not generally exercise detailed control.

Although the "loins" or reserves were on hand to theoretically correct or adjust an unfavorable situation, a shattered attack could make the reserves irrelevant. Against the Boers at Blood River, massed gunfire broke the back of the Zulu assault, and the Boers were later able to mount a cavalry sweep in counterattack that became a turkey shoot against fleeing Zulu remnants. Perhaps the Zulu threw everything forward and had little left. In similar manner, after exhausting themselves against British firepower at Kambula and Ulindi, few of the Zulu reserves were available to do anything constructive, although the tribal warriors still remained dangerous at the guerrilla level when scattered. At Isandhlwana however, the "classical" Zulu system struck gold, and after liquidating the British position, it was a relatively fresh reserve force that swept down on Rorke's Drift.

The Zulu had greater numbers than their opponents, but greater numbers massed together in compact arrays simply presented easy targets in the age of modern firearms and artillery. African tribes that fought in smaller guerrilla detachments typically held out against European invaders for a much longer time, as witnessed by the 7-year resistance of the Lobi against the French in West Africa, or the operations of the Berbers in Algeria against the French.

When the Zulu did acquire firearms, most notably captured stocks after the great victory at Isandhlwana, they lacked training and used them ineffectively, consistently firing high to give the bullets "strength." Southern Africa, including the areas near Natal, was teeming with bands like the Griquas who had learned to use guns. Indeed, one such group not only mastered the way of the gun, but became proficient horsemen as well, skills that helped build the Basotho tribe, in what is now the nation of Lesotho. In addition, numerous European renegades or adventurers (both Boer and non-Boer) skilled in firearms were known to the Zulu. Some had even led detachments for the Zulu kings on military missions.

The Zulu thus had clear scope and opportunity to master and adapt the new weaponry. They also had already experienced defeat against the Boers, by concentrated firearms. They had had at least four decades to adjust their tactics to this new threat. A well-drilled corps of gunmen or grenadiers, or a battery of artillery operated by European mercenaries for example, might have provided much needed covering fire as the regiments manoeuvred into position.

No such adjustments were on hand when they faced the redcoats. Immensely proud of their system, and failing to learn from their earlier defeats, they persisted in "human wave" attacks against well defended European positions where massed firepower devastated their ranks. The ministrations of an "isAngoma" (plural: "izAngoma") Zulu diviner or "witch doctor", and the bravery of individual regiments were ultimately of little use against the volleys of modern rifles, Gatling guns and artillery at the Ineyzane River, Rorke's Drift, Kambula, Gingingdlovu and finally Ulindi.

Undoubtedly, Cetshwayo and his war leaders faced a tough and extremely daunting task – overcoming the challenge of concentrated rifled, Gatling gun, and artillery fire on the battlefield. It was one that also taxed European military leaders, as the carnage of the American Civil War and the later Boer War attests. Nevertheless, Shaka's successors could argue that within the context of their experience and knowledge, they had done the best they could, following his classical template, which had advanced the Zulu from a small, obscure tribe to a respectable regional power known for its fierce warriors.

The demise of the impi finally came about with the success of European colonisation of Africa- first in southern Africa by the British, and finally in German East Africa as German colonialists defeated the last of the impi-style formations under Mkwawa, chief of the Hehe of Tanzania. The Boers, another major challenger to the impi, also saw defeat by imperial forces, in the Boer War of 1902. In its relatively brief history, the impi inspired both scorn (During the Anglo-Zulu War, British commander Lord Chelmsford complained that they did not 'fight fair') and admiration in its opponents, epitomised in Kipling's poem "Fuzzy Wuzzy":

Today the impi lives on in popular lore and culture, even in the West. While the term "impi" has become synonymous with the Zulu nation in international popular culture, it appears in various video games such as "Civilization III"', "", "", and "", where the Impi is the unique unit for the Zulu faction with Shaka as their leader and also as an appearance as unique unit of the Bantu nation in "Rise of Nations" (Zulus are among many tribes who make up the Bantu people) . 'Impi' is also the title of a very famous South Africa song by Johnny Clegg and the band Juluka which has become something of an unofficial national anthem, especially at major international sports events and especially when the opponent is England.

Lyrics:

Before stage seven of the 2013 Tour de France, the Orica-GreenEDGE cycling team played 'Impi' on their team bus in honor of teammate Daryl Impey, the first South African Tour de France leader.






</doc>
<doc id="15175" url="https://en.wikipedia.org/wiki?curid=15175" title="Irish mythology">
Irish mythology

The mythology of pre-Christian Ireland did not entirely survive the conversion to Christianity. However, much of it was preserved in medieval Irish literature, though it was shorn of its religious meanings. This literature represents the most extensive and best preserved of all the branches of Celtic mythology. Although many of the manuscripts have not survived and much more material was probably never committed to writing, there is enough remaining to enable the identification of distinct, if overlapping, cycles: the Mythological Cycle, the Ulster Cycle, the Fenian Cycle and the Historical Cycle. There are also a number of extant mythological texts that do not fit into any of the cycles. Additionally, there are a large number of recorded folk tales that, while not strictly mythological, feature personages from one or more of these four cycles.

Today some of the best known tales are of Tír na nÓg, Fionn MacCumhaill, Na Fianna, The Aos Sí / Aes Sídhe, Sétanta (CúChulainn), The Tuatha Dé Danann (Gods), the Children of Lir, Táin Bó Cúailnge & the Salmon of Knowledge.

The three main manuscript sources for Irish mythology are the late 11th/early 12th century "Lebor na hUidre" which is in the library of the Royal Irish Academy, the early 12th century "Book of Leinster" in the Library of Trinity College, Dublin, and the Bodleian Library, MS Rawlinson B 502 ("Rawl."), housed in the Bodleian Library at Oxford University. Despite the dates of these sources, most of the material they contain predates their composition. The earliest of the prose can be dated on linguistic grounds to the 8th century, and some of the verse may be as old as the 6th century.

Other important sources include a group of four manuscripts originating in the west of Ireland in the late 14th or early 15th century: "The Yellow Book of Lecan", "The Great Book of Lecan", and "The Book of Ballymote". The first of these contains part of the earliest known version of the "Táin Bó Cúailnge" ("The Driving-off of Cattle of Cooley") and is housed in Trinity College. The other three are in the Royal Academy. Other 15th-century manuscripts, such as "The Book of Fermoy" also contain interesting materials, as do such later syncretic works such as Geoffrey Keating's "Foras Feasa ar Éirinn" ("The History of Ireland") (ca. 1640), particularly as these later compilers and writers may have had access to manuscript sources that have since disappeared.

When using these sources, it is, as always, important to question the impact of the circumstances in which they were produced. Most of the manuscripts were created by Christian monks, who may well have been torn between the desire to record their native culture and their religious hostility to pagan beliefs resulting in some of the gods being euhemerised. Many of the later sources may also have formed part of a propaganda effort designed to create a history for the people of Ireland that could bear comparison with the mythological descent of their British invaders from the founders of Rome that was promulgated by Geoffrey of Monmouth and others. There was also a tendency to rework Irish genealogies to fit into the known schema of Greek or Biblical genealogy.

It was once unquestioned that medieval Irish literature preserved truly ancient traditions in a form virtually unchanged through centuries of oral tradition back to the ancient Celts of Europe. Kenneth Jackson famously described the Ulster Cycle as a "window on the Iron Age", and Garret Olmsted has attempted to draw parallels between "Táin Bó Cuailnge", the Ulster Cycle epic, and the iconography of the Gundestrup Cauldron. However, this "nativist" position has been challenged by "revisionist" scholars who believe that much of it was created in Christian times in deliberate imitation of the epics of classical literature that came with Latin learning. The revisionists would indicate passages apparently influenced by the Iliad in "Táin Bó Cuailnge", and the existence of "Togail Troí", an Irish adaptation of Dares Phrygius' "De excidio Troiae historia", found in the Book of Leinster, and note that the material culture of the stories is generally closer to the time of the stories' composition than to the distant past. A consensus has emerged which encourages the critical reading of the material.

The Mythological Cycle, comprising stories of the former gods and origins of the Irish, is the least well preserved of the four cycles. The most important sources are the "Metrical Dindshenchas" or "Lore of Places" and the "Lebor Gabála Érenn" or "Book of Invasions". Other manuscripts preserve such mythological tales as "The Dream of Aengus", "The Wooing Of Étain" and "Cath Maige Tuireadh", "The (second) Battle of Magh Tuireadh". One of the best known of all Irish stories, "Oidheadh Clainne Lir", or "The Tragedy of the Children of Lir", is also part of this cycle.
"Lebor Gabála Érenn" is a pseudo-history of Ireland, tracing the ancestry of the Irish back to before Noah. It tells of a series of invasions or "takings" of Ireland by a succession of peoples, the fifth of whom was the people known as the Tuatha Dé Danann ("Peoples of the Goddess Danu"), who were believed to have inhabited the island before the arrival of the Gaels, or Milesians. They faced opposition from their enemies, the Fomorians, led by Balor of the Evil Eye. Balor was eventually slain by Lug Lámfada (Lug of the Long Arm) at the second battle of Magh Tuireadh. With the arrival of the Gaels, the Tuatha Dé Danann retired underground to become the fairy people of later myth and legend.

The "Metrical Dindshenchas" is the great onomastics work of early Ireland, giving the naming legends of significant places in a sequence of poems. It includes a lot of important information on Mythological Cycle figures and stories, including the Battle of Tailtiu, in which the Tuatha Dé Danann were defeated by the Milesians.

It is important to note that by the Middle Ages the Tuatha Dé Danann were not viewed so much as gods as the shape-shifting magician population of an earlier Golden Age Ireland. Texts such as "Lebor Gabála Érenn" and "Cath Maige Tuireadh" present them as kings and heroes of the distant past, complete with death-tales. However, there is considerable evidence, both in the texts and from the wider Celtic world, that they were once considered deities.

Even after they are displaced as the rulers of Ireland, characters such as Lug, the Mórrígan, Aengus and Manannán Mac Lir appear in stories set centuries later, betraying their immortality. A poem in the Book of Leinster lists many of the Tuatha Dé, but ends "Although [the author] enumerates them, he does not worship them". Goibniu, Creidhne and Luchta are referred to as "Trí Dé Dána" ("three gods of craftsmanship"), and the Dagda's name is interpreted in medieval texts as "the good god". Nuada is cognate with the British god Nodens; Lug is a reflex of the pan-Celtic deity Lugus, the name of whom may indicate "Light"; Tuireann may be related to the Gaulish Taranis; Ogma to Ogmios; the Badb to Catubodua.

The Ulster Cycle is traditionally set around the first century CE, and most of the action takes place in the provinces of Ulster and Connacht. It consists of a group of heroic tales dealing with the lives of Conchobar mac Nessa, king of Ulster, the great hero Cú Chulainn, the son of Lug (Lugh), and of their friends, lovers, and enemies. These are the Ulaid, or people of the North-Eastern corner of Ireland and the action of the stories centres round the royal court at Emain Macha (known in English as Navan Fort), close to the modern town of Armagh. The Ulaid had close links with the Irish colony in Scotland, and part of Cú Chulainn's training takes place in that colony.

The cycle consists of stories of the births, early lives and training, wooings, battles, feastings, and deaths of the heroes and reflects a warrior society in which warfare consists mainly of single combats and wealth is measured mainly in cattle. These stories are written mainly in prose. The centrepiece of the Ulster Cycle is the "Táin Bó Cúailnge". Other important Ulster Cycle tales include "The Tragic Death of Aife's only Son", "Bricriu's Feast", and "The Destruction of Da Derga's Hostel". "The Exile of the Sons of Usnach", better known as the tragedy of Deirdre and the source of plays by John Millington Synge, William Butler Yeats, and Vincent Woods, is also part of this cycle.

This cycle is, in some respects, close to the mythological cycle. Some of the characters from the latter reappear, and the same sort of shape-shifting magic is much in evidence, side by side with a grim, almost callous realism. While we may suspect a few characters, such as Medb or Cú Roí, of once being deities, and Cú Chulainn in particular displays superhuman prowess, the characters are mortal and associated with a specific time and place. If the Mythological Cycle represents a Golden Age, the Ulster Cycle is Ireland's Heroic Age.

Like the Ulster Cycle, the Fenian Cycle is concerned with the deeds of Irish heroes. The stories of the Fenian Cycle appear to be set around the 3rd century and mainly in the provinces of Leinster and Munster. They differ from the other cycles in the strength of their links with the Irish-speaking community in Scotland and there are many extant Fenian texts from that country. They also differ from the Ulster Cycle in that the stories are told mainly in verse and that in tone they are nearer to the tradition of romance than the tradition of epic. The stories concern the doings of Fionn mac Cumhaill and his band of soldiers, the Fianna.
The single most important source for the Fenian Cycle is the "Acallam na Senórach" ("Colloquy of the Old Men"), which is found in two 15th-century manuscripts, the "Book of Lismore" and Laud 610, as well as a 17th-century manuscript from Killiney, County Dublin. The text is dated from linguistic evidence to the 12th century. The text records conversations between Caílte mac Rónáin and Oisín, the last surviving members of the Fianna, and Saint Patrick, and consists of about 8,000 lines. The late dates of the manuscripts may reflect a longer oral tradition for the Fenian stories.

The Fianna of the story are divided into the Clann Baiscne, led by Fionn mac Cumhaill (often rendered as "Finn MacCool", Finn Son of Cumhall), and the Clann Morna, led by his enemy, Goll mac Morna. Goll killed Fionn's father, Cumhal, in battle and the boy Fionn was brought up in secrecy. As a youth, while being trained in the art of poetry, he accidentally burned his thumb while cooking the Salmon of Knowledge, which allowed him to suck or bite his thumb to receive bursts of stupendous wisdom. He took his place as the leader of his band and numerous tales are told of their adventures. Two of the greatest of the Irish tales, "Tóraigheacht Dhiarmada agus Ghráinne" ("The Pursuit of Diarmuid and Gráinne)" and "Oisín in Tír na nÓg" form part of the cycle. The Diarmuid and Grainne story, which is one of the few Fenian prose tales, is a probable source of "Tristan and Iseult".

The world of the Fenian Cycle is one in which professional warriors spend their time hunting, fighting, and engaging in adventures in the spirit world. New entrants into the band are expected to be knowledgeable in poetry as well as undergo a number of physical tests or ordeals.
There is not any religious element in these tales unless it is one of hero-worship.

It was part of the duty of the medieval Irish bards, or court poets, to record the history of the family and the genealogy of the king they served. This they did in poems that blended the mythological and the historical to a greater or lesser degree. The resulting stories form what has come to be known as the Historical Cycle or Cycles of the Kings, or more correctly Cycles, as there are a number of independent groupings.

The kings that are included range from the almost entirely mythological Labraid Loingsech, who allegedly became High King of Ireland around 431 BC, to the entirely historical Brian Boru. However, the greatest glory of the Historical Cycle is the "Buile Shuibhne" ("The Frenzy of Sweeney"), a 12th-century tale told in verse and prose. Suibhne, king of Dál nAraidi, was cursed by St Ronan and became a kind of half-man, half bird, condemned to live out his life in the woods, fleeing from his human companions. The story has captured the imaginations of contemporary Irish poets and has been translated by Trevor Joyce and Seamus Heaney.

The adventures, or "echtrae", are a group of stories of visits to the Irish Other World (which may be westward across the sea, underground, or simply invisible to mortals). The most famous, "Oisin in Tir na nÓg" belongs to the Fenian Cycle, but several free-standing adventures survive, including "The Adventure of Conle", "The Voyage of Bran mac Ferbail", and "The Adventure of Lóegaire".

The voyages, or "immrama", are tales of sea journeys and the wonders seen on them that may have resulted from the combination of the experiences of fishermen combined and the Other World elements that inform the adventures. Of the seven "immrama" mentioned in the manuscripts, only three have survived: the "Voyage of Mael Dúin", the "Voyage of the Uí Chorra", and the "Voyage of Snedgus and Mac Riagla". "The Voyage of Mael Duin" is the forerunner of the later "Voyage of St. Brendan". While not as ancient, later 8th century AD works, that influenced European literature, include The Vision of Adamnán.

During the first few years of the 20th Century, Herminie T. Kavanagh wrote down many Irish folk tales which she published in magazines and in two books. Twenty-six years after her death, the tales from her two books, "Darby O'Gill and the Good People", and "Ashes of Old Wishes" were made into the film Darby O'Gill and the Little People. Noted Irish playwright Lady Gregory also collected folk stories to preserve Irish history.







</doc>
<doc id="15176" url="https://en.wikipedia.org/wiki?curid=15176" title="Insurance">
Insurance

Insurance is a means of protection from financial loss. It is a form of risk management, primarily used to hedge against the risk of a contingent or uncertain loss.

An entity which provides insurance is known as an insurer, insurance company, insurance carrier or underwriter. A person or entity who buys insurance is known as an insured or as a policyholder. The insurance transaction involves the insured assuming a guaranteed and known relatively small loss in the form of payment to the insurer in exchange for the insurer's promise to compensate the insured in the event of a covered loss. The loss may or may not be financial, but it must be reducible to financial terms, and usually involves something in which the insured has an insurable interest established by ownership, possession, or preexisting relationship.

The insured receives a contract, called the insurance policy, which details the conditions and circumstances under which the insurer will compensate the insured. The amount of money charged by the insurer to the insured for the coverage set forth in the insurance policy is called the premium. If the insured experiences a loss which is potentially covered by the insurance policy, the insured submits a claim to the insurer for processing by a claims adjuster. The insurer may hedge its own risk by taking out reinsurance, whereby another insurance company agrees to carry some of the risk, especially if the primary insurer deems the risk too large for it to carry.

Methods for transferring or distributing risk were practiced by Chinese and Babylonian traders as long ago as the 3rd and 2nd millennia BC, respectively. Chinese merchants travelling treacherous river rapids would redistribute their wares across many vessels to limit the loss due to any single vessel's capsizing. The Babylonians developed a system which was recorded in the famous Code of Hammurabi, c. 1750 BC, and practiced by early Mediterranean sailing merchants. If a merchant received a loan to fund his shipment, he would pay the lender an additional sum in exchange for the lender's guarantee to cancel the loan should the shipment be stolen, or lost at sea.

Circa 800 BC, the inhabitants of Rhodes created the 'general average'. This allowed groups of merchants to pay to insure their goods being shipped together. The collected premiums would be used to reimburse any merchant whose goods were jettisoned during transport, whether due to storm or sinkage.

Separate insurance contracts (i.e., insurance policies not bundled with loans or other kinds of contracts) were invented in Genoa in the 14th century, as were insurance pools backed by pledges of landed estates. The first known insurance contract dates from Genoa in 1347, and in the next century maritime insurance developed widely and premiums were intuitively varied with risks. These new insurance contracts allowed insurance to be separated from investment, a separation of roles that first proved useful in marine insurance.

Insurance became far more sophisticated in Enlightenment era Europe, and specialized varieties developed.
Property insurance as we know it today can be traced to the Great Fire of London, which in 1666 devoured more than 13,000 houses. The devastating effects of the fire converted the development of insurance "from a matter of convenience into one of urgency, a change of opinion reflected in Sir Christopher Wren's inclusion of a site for 'the Insurance Office' in his new plan for London in 1667." A number of attempted fire insurance schemes came to nothing, but in 1681, economist Nicholas Barbon and eleven associates established the first fire insurance company, the "Insurance Office for Houses," at the back of the Royal Exchange to insure brick and frame homes. Initially, 5,000 homes were insured by his Insurance Office.

At the same time, the first insurance schemes for the underwriting of business ventures became available. By the end of the seventeenth century, London's growing importance as a center for trade was increasing demand for marine insurance. In the late 1680s, Edward Lloyd opened a coffee house, which became the meeting place for parties in the shipping industry wishing to insure cargoes and ships, and those willing to underwrite such ventures. These informal beginnings led to the establishment of the insurance market Lloyd's of London and several related shipping and insurance businesses.
The first life insurance policies were taken out in the early 18th century. The first company to offer life insurance was the Amicable Society for a Perpetual Assurance Office, founded in London in 1706 by William Talbot and Sir Thomas Allen. Edward Rowe Mores established the Society for Equitable Assurances on Lives and Survivorship in 1762.

It was the world's first mutual insurer and it pioneered age based premiums based on mortality rate laying "the framework for scientific insurance practice and development" and "the basis of modern life assurance upon which all life assurance schemes were subsequently based."

In the late 19th century "accident insurance" began to become available. The first company to offer accident insurance was the Railway Passengers Assurance Company, formed in 1848 in England to insure against the rising number of fatalities on the nascent railway system.

By the late 19th century governments began to initiate national insurance programs against sickness and old age. Germany built on a tradition of welfare programs in Prussia and Saxony that began as early as in the 1840s. In the 1880s Chancellor Otto von Bismarck introduced old age pensions, accident insurance and medical care that formed the basis for Germany's welfare state. In Britain more extensive legislation was introduced by the Liberal government in the 1911 National Insurance Act. This gave the British working classes the first contributory system of insurance against illness and unemployment. This system was greatly expanded after the Second World War under the influence of the Beveridge Report, to form the first modern welfare state.

Insurance involves pooling funds from "many" insured entities (known as exposures) to pay for the losses that some may incur. The insured entities are therefore protected from risk for a fee, with the fee being dependent upon the frequency and severity of the event occurring. In order to be an insurable risk, the risk insured against must meet certain characteristics. Insurance as a financial intermediary is a commercial enterprise and a major part of the financial services industry, but individual entities can also self-insure through saving money for possible future losses.

Risk which can be insured by private companies typically shares seven common characteristics:


When a company insures an individual entity, there are basic legal requirements and regulations. Several commonly cited legal principles of insurance include:

To "indemnify" means to make whole again, or to be reinstated to the position that one was in, to the extent possible, prior to the happening of a specified event or peril. Accordingly, life insurance is generally not considered to be indemnity insurance, but rather "contingent" insurance (i.e., a claim arises on the occurrence of a specified event). There are generally three types of insurance contracts that seek to indemnify an insured:

From an insured's standpoint, the result is usually the same: the insurer pays the loss and claims expenses.

If the Insured has a "reimbursement" policy, the insured can be required to pay for a loss and then be "reimbursed" by the insurance carrier for the loss and out of pocket costs including, with the permission of the insurer, claim expenses.

Under a "pay on behalf" policy, the insurance carrier would defend and pay a claim on behalf of the insured who would not be out of pocket for anything. Most modern liability insurance is written on the basis of "pay on behalf" language which enables the insurance carrier to manage and control the claim.

Under an "indemnification" policy, the insurance carrier can generally either "reimburse" or "pay on behalf of", whichever is more beneficial to it and the insured in the claim handling process.

An entity seeking to transfer risk (an individual, corporation, or association of any type, etc.) becomes the 'insured' party once risk is assumed by an 'insurer', the insuring party, by means of a contract, called an insurance policy. Generally, an insurance contract includes, at a minimum, the following elements: identification of participating parties (the insurer, the insured, the beneficiaries), the premium, the period of coverage, the particular loss event covered, the amount of coverage (i.e., the amount to be paid to the insured or beneficiary in the event of a loss), and exclusions (events not covered). An insured is thus said to be "indemnified" against the loss covered in the policy.

When insured parties experience a loss for a specified peril, the coverage entitles the policyholder to make a claim against the insurer for the covered amount of loss as specified by the policy. The fee paid by the insured to the insurer for assuming the risk is called the premium. Insurance premiums from many insureds are used to fund accounts reserved for later payment of claims – in theory for a relatively few claimants – and for overhead costs. So long as an insurer maintains adequate funds set aside for anticipated losses (called reserves), the remaining margin is an insurer's profit.

Insurance can have various effects on society through the way that it changes who bears the cost of losses and damage. On one hand it can increase fraud; on the other it can help societies and individuals prepare for catastrophes and mitigate the effects of catastrophes on both households and societies.

Insurance can influence the probability of losses through moral hazard, insurance fraud, and preventive steps by the insurance company. Insurance scholars have typically used moral hazard to refer to the increased loss due to unintentional carelessness and insurance fraud to refer to increased risk due to intentional carelessness or indifference. Insurers attempt to address carelessness through inspections, policy provisions requiring certain types of maintenance, and possible discounts for loss mitigation efforts. While in theory insurers could encourage investment in loss reduction, some commentators have argued that in practice insurers had historically not aggressively pursued loss control measures—particularly to prevent disaster losses such as hurricanes—because of concerns over rate reductions and legal battles. However, since about 1996 insurers have begun to take a more active role in loss mitigation, such as through building codes.

According to the study books of The Chartered Insurance Institute, there are variant methods of insurance as follows:

The business model is to collect more in premium and investment income than is paid out in losses, and to also offer a competitive price which consumers will accept. Profit can be reduced to a simple equation:

Insurers make money in two ways:

The most complicated aspect of the insurance business is the actuarial science of ratemaking (price-setting) of policies, which uses statistics and probability to approximate the rate of future claims based on a given risk. After producing rates, the insurer will use discretion to reject or accept risks through the underwriting process.

At the most basic level, initial ratemaking involves looking at the frequency and severity of insured perils and the expected average payout resulting from these perils. Thereafter an insurance company will collect historical loss data, bring the loss data to present value, and compare these prior losses to the premium collected in order to assess rate adequacy. Loss ratios and expense loads are also used. Rating for different risk characteristics involves at the most basic level comparing the losses with "loss relativities"—a policy with twice as many losses would therefore be charged twice as much. More complex multivariate analyses are sometimes used when multiple characteristics are involved and a univariate analysis could produce confounded results. Other statistical methods may be used in assessing the probability of future losses.

Upon termination of a given policy, the amount of premium collected minus the amount paid out in claims is the insurer's underwriting profit on that policy. Underwriting performance is measured by something called the "combined ratio", which is the ratio of expenses/losses to premiums. A combined ratio of less than 100% indicates an underwriting profit, while anything over 100 indicates an underwriting loss. A company with a combined ratio over 100% may nevertheless remain profitable due to investment earnings.

Insurance companies earn investment profits on "float". Float, or available reserve, is the amount of money on hand at any given moment that an insurer has collected in insurance premiums but has not paid out in claims. Insurers start investing insurance premiums as soon as they are collected and continue to earn interest or other income on them until claims are paid out. The Association of British Insurers (gathering 400 insurance companies and 94% of UK insurance services) has almost 20% of the investments in the London Stock Exchange.

In the United States, the underwriting loss of property and casualty insurance companies was $142.3 billion in the five years ending 2003. But overall profit for the same period was $68.4 billion, as the result of float. Some insurance industry insiders, most notably Hank Greenberg, do not believe that it is forever possible to sustain a profit from float without an underwriting profit as well, but this opinion is not universally held.

Naturally, the float method is difficult to carry out in an economically depressed period. Bear markets do cause insurers to shift away from investments and to toughen up their underwriting standards, so a poor economy generally means high insurance premiums. This tendency to swing between profitable and unprofitable periods over time is commonly known as the underwriting, or insurance, cycle.

Claims and loss handling is the materialized utility of insurance; it is the actual "product" paid for. Claims may be filed by insureds directly with the insurer or through brokers or agents. The insurer may require that the claim be filed on its own proprietary forms, or may accept claims on a standard industry form, such as those produced by ACORD.

Insurance company claims departments employ a large number of claims adjusters supported by a staff of records management and data entry clerks. Incoming claims are classified based on severity and are assigned to adjusters whose settlement authority varies with their knowledge and experience. The adjuster undertakes an investigation of each claim, usually in close cooperation with the insured, determines if coverage is available under the terms of the insurance contract, and if so, the reasonable monetary value of the claim, and authorizes payment.

The policyholder may hire their own public adjuster to negotiate the settlement with the insurance company on their behalf. For policies that are complicated, where claims may be complex, the insured may take out a separate insurance policy add-on, called loss recovery insurance, which covers the cost of a public adjuster in the case of a claim.

Adjusting liability insurance claims is particularly difficult because there is a third party involved, the plaintiff, who is under no contractual obligation to cooperate with the insurer and may in fact regard the insurer as a deep pocket. The adjuster must obtain legal counsel for the insured (either inside "house" counsel or outside "panel" counsel), monitor litigation that may take years to complete, and appear in person or over the telephone with settlement authority at a mandatory settlement conference when requested by the judge.

If a claims adjuster suspects under-insurance, the condition of average may come into play to limit the insurance company's exposure.

In managing the claims handling function, insurers seek to balance the elements of customer satisfaction, administrative handling expenses, and claims overpayment leakages. As part of this balancing act, fraudulent insurance practices are a major business risk that must be managed and overcome. Disputes between insurers and insureds over the validity of claims or claims handling practices occasionally escalate into litigation (see insurance bad faith).

Insurers will often use insurance agents to initially market or underwrite their customers. Agents can be captive, meaning they write only for one company, or independent, meaning that they can issue policies from several companies. The existence and success of companies using insurance agents is likely due to improved and personalized service. Companies also use Broking firms, Banks and other corporate entities (like Self Help Groups, Microfinance Institutions, NGOs etc.) to market their products.

Any risk that can be quantified can potentially be insured. Specific kinds of risk that may give rise to claims are known as perils. An insurance policy will set out in detail which perils are covered by the policy and which are not. Below are non-exhaustive lists of the many different types of insurance that exist. A single policy that may cover risks in one or more of the categories set out below. For example, vehicle insurance would typically cover both the property risk (theft or damage to the vehicle) and the liability risk (legal claims arising from an accident). A home insurance policy in the United States typically includes coverage for damage to the home and the owner's belongings, certain legal claims against the owner, and even a small amount of coverage for medical expenses of guests who are injured on the owner's property.

Business insurance can take a number of different forms, such as the various kinds of professional liability insurance, also called professional indemnity (PI), which are discussed below under that name; and the business owner's policy (BOP), which packages into one policy many of the kinds of coverage that a business owner needs, in a way analogous to how homeowners' insurance packages the coverages that a homeowner needs.

Auto insurance protects the policyholder against financial loss in the event of an incident involving a vehicle they own, such as in a traffic collision.

Coverage typically includes:

Gap insurance covers the excess amount on your auto loan in an instance where your insurance company does not cover the entire loan. Depending on the company's specific policies it might or might not cover the deductible as well. This coverage is marketed for those who put low down payments, have high interest rates on their loans, and those with 60-month or longer terms. Gap insurance is typically offered by a finance company when the vehicle owner purchases their vehicle, but many auto insurance companies offer this coverage to consumers as well.

Health insurance policies cover the cost of medical treatments. Dental insurance, like medical insurance, protects policyholders for dental costs. In most developed countries, all citizens receive some health coverage from their governments, paid for by taxation. In most countries, health insurance is often part of an employer's benefits.


Casualty insurance insures against accidents, not necessarily tied to any specific property. It is a broad spectrum of insurance that a number of other types of insurance could be classified, such as auto, workers compensation, and some liability insurances.

Life insurance provides a monetary benefit to a decedent's family or other designated beneficiary, and may specifically provide for income to an insured person's family, burial, funeral and other final expenses. Life insurance policies often allow the option of having the proceeds paid to the beneficiary either in a lump sum cash payment or an annuity. In most states, a person cannot purchase a policy on another person without their knowledge.

Annuities provide a stream of payments and are generally classified as insurance because they are issued by insurance companies, are regulated as insurance, and require the same kinds of actuarial and investment management expertise that life insurance requires. Annuities and pensions that pay a benefit for life are sometimes regarded as insurance against the possibility that a retiree will outlive his or her financial resources. In that sense, they are the complement of life insurance and, from an underwriting perspective, are the mirror image of life insurance.

Certain life insurance contracts accumulate cash values, which may be taken by the insured if the policy is surrendered or which may be borrowed against. Some policies, such as annuities and endowment policies, are financial instruments to accumulate or liquidate wealth when it is needed.

In many countries, such as the United States and the UK, the tax law provides that the interest on this cash value is not taxable under certain circumstances. This leads to widespread use of life insurance as a tax-efficient method of saving as well as protection in the event of early death.

In the United States, the tax on interest income on life insurance policies and annuities is generally deferred. However, in some cases the benefit derived from tax deferral may be offset by a low return. This depends upon the insuring company, the type of policy and other variables (mortality, market return, etc.). Moreover, other income tax saving vehicles (e.g., IRAs, 401(k) plans, Roth IRAs) may be better alternatives for value accumulation.

Burial insurance is a very old type of life insurance which is paid out upon death to cover final expenses, such as the cost of a funeral. The Greeks and Romans introduced burial insurance c. 600 CE when they organized guilds called "benevolent societies" which cared for the surviving families and paid funeral expenses of members upon death. Guilds in the Middle Ages served a similar purpose, as did friendly societies during Victorian times.

Property insurance provides protection against risks to property, such as fire, theft or weather damage. This may include specialized forms of insurance such as fire insurance, flood insurance, earthquake insurance, home insurance, inland marine insurance or boiler insurance.
The term "property insurance" may, like casualty insurance, be used as a broad category of various subtypes of insurance, some of which are listed below:


Liability insurance is a very broad superset that covers legal claims against the insured. Many types of insurance include an aspect of liability coverage. For example, a homeowner's insurance policy will normally include liability coverage which protects the insured in the event of a claim brought by someone who slips and falls on the property; automobile insurance also includes an aspect of liability insurance that indemnifies against the harm that a crashing car can cause to others' lives, health, or property. The protection offered by a liability insurance policy is twofold: a legal defense in the event of a lawsuit commenced against the policyholder and indemnification (payment on behalf of the insured) with respect to a settlement or court verdict. Liability policies typically cover only the negligence of the insured, and will not apply to results of wilful or intentional acts by the insured.
Often a commercial insured's liability insurance program consists of several layers. The first layer of insurance generally consists of primary insurance, which provides first dollar indemnity for judgments and settlements up to the limits of liability of the primary policy. Generally, primary insurance is subject to a deductible and obligates the insured to defend the insured against lawsuits, which is normally accomplished by assigning counsel to defend the insured. In many instances, a commercial insured may elect to self-insure. Above the primary insurance or self-insured retention, the insured may have one or more layers of excess insurance to provide coverage additional limits of indemnity protection. There are a variety of types of excess insurance, including "stand-alone" excess policies (policies that contain their own terms, conditions, and exclusions), "follow form" excess insurance (policies that follow the terms of the underlying policy except as specifically provided), and "umbrella" insurance policies (excess insurance that in some circumstances could provide coverage that is broader than the underlying insurance).

Credit insurance repays some or all of a loan when the borrower is insolvent.




Some communities prefer to create virtual insurance amongst themselves by other means than contractual risk transfer, which assigns explicit numerical values to risk. A number of religious groups, including the Amish and some Muslim groups, depend on support provided by their communities when disasters strike. The risk presented by any given person is assumed collectively by the community who all bear the cost of rebuilding lost property and supporting people whose needs are suddenly greater after a loss of some kind. In supportive communities where others can be trusted to follow community leaders, this tacit form of insurance can work. In this manner the community can even out the extreme differences in insurability that exist among its members. Some further justification is also provided by invoking the moral hazard of explicit insurance contracts.

In the United Kingdom, The Crown (which, for practical purposes, meant the civil service) did not insure property such as government buildings. If a government building was damaged, the cost of repair would be met from public funds because, in the long run, this was cheaper than paying insurance premiums. Since many UK government buildings have been sold to property companies, and rented back, this arrangement is now less common and may have disappeared altogether.

In the United States, the most prevalent form of self-insurance is governmental risk management pools. They are self-funded cooperatives, operating as carriers of coverage for the majority of governmental entities today, such as county governments, municipalities, and school districts. Rather than these entities independently self-insure and risk bankruptcy from a large judgment or catastrophic loss, such governmental entities form a risk pool. Such pools begin their operations by capitalization through member deposits or bond issuance. Coverage (such as general liability, auto liability, professional liability, workers compensation, and property) is offered by the pool to its members, similar to coverage offered by insurance companies. However, self-insured pools offer members lower rates (due to not needing insurance brokers), increased benefits (such as loss prevention services) and subject matter expertise. Of approximately 91,000 distinct governmental entities operating in the United States, 75,000 are members of self-insured pools in various lines of coverage, forming approximately 500 pools. Although a relatively small corner of the insurance market, the annual contributions (self-insured premiums) to such pools have been estimated up to 17 billion dollars annually.

Insurance companies may be classified into two groups:

General insurance companies can be further divided into these sub categories.

In most countries, life and non-life insurers are subject to different regulatory regimes and different tax and accounting rules. The main reason for the distinction between the two types of company is that life, annuity, and pension business is very long-term in nature – coverage for life assurance or a pension can cover risks over many decades. By contrast, non-life insurance cover usually covers a shorter period, such as one year.

Insurance companies are generally classified as either mutual or proprietary companies. Mutual companies are owned by the policyholders, while shareholders (who may or may not own policies) own proprietary insurance companies.

Demutualization of mutual insurers to form stock companies, as well as the formation of a hybrid known as a mutual holding company, became common in some countries, such as the United States, in the late 20th century. However, not all states permit mutual holding companies.

Other possible forms for an insurance company include reciprocals, in which policyholders reciprocate in sharing risks, and Lloyd's organizations.

Insurance companies are rated by various agencies such as A. M. Best. The ratings include the company's financial strength, which measures its ability to pay claims. It also rates financial instruments issued by the insurance company, such as bonds, notes, and securitization products.

Reinsurance companies are insurance companies that sell policies to other insurance companies, allowing them to reduce their risks and protect themselves from very large losses. The reinsurance market is dominated by a few very large companies, with huge reserves. A reinsurer may also be a direct writer of insurance risks as well.

Captive insurance companies may be defined as limited-purpose insurance companies established with the specific objective of financing risks emanating from their parent group or groups. This definition can sometimes be extended to include some of the risks of the parent company's customers. In short, it is an in-house self-insurance vehicle. Captives may take the form of a "pure" entity (which is a 100% subsidiary of the self-insured parent company); of a "mutual" captive (which insures the collective risks of members of an industry); and of an "association" captive (which self-insures individual risks of the members of a professional, commercial or industrial association). Captives represent commercial, economic and tax advantages to their sponsors because of the reductions in costs they help create and for the ease of insurance risk management and the flexibility for cash flows they generate. Additionally, they may provide coverage of risks which is neither available nor offered in the traditional insurance market at reasonable prices.

The types of risk that a captive can underwrite for their parents include property damage, public and product liability, professional indemnity, employee benefits, employers' liability, motor and medical aid expenses. The captive's exposure to such risks may be limited by the use of reinsurance.

Captives are becoming an increasingly important component of the risk management and risk financing strategy of their parent. This can be understood against the following background:

There are also companies known as "insurance consultants". Like a mortgage broker, these companies are paid a fee by the customer to shop around for the best insurance policy amongst many companies. Similar to an insurance consultant, an 'insurance broker' also shops around for the best insurance policy amongst many companies. However, with insurance brokers, the fee is usually paid in the form of commission from the insurer that is selected rather than directly from the client.

Neither insurance consultants nor insurance brokers are insurance companies and no risks are transferred to them in insurance transactions. Third party administrators are companies that perform underwriting and sometimes claims handling services for insurance companies. These companies often have special expertise that the insurance companies do not have.

The financial stability and strength of an insurance company should be a major consideration when buying an insurance contract. An insurance premium paid currently provides coverage for losses that might arise many years in the future. For that reason, the viability of the insurance carrier is very important. In recent years, a number of insurance companies have become insolvent, leaving their policyholders with no coverage (or coverage only from a government-backed insurance pool or other arrangement with less attractive payouts for losses). A number of independent rating agencies provide information and rate the financial viability of insurance companies.

Global insurance premiums grew by 2.7% in inflation-adjusted terms in 2010 to $4.3 trillion, climbing above pre-crisis levels. The return to growth and record premiums generated during the year followed two years of decline in real terms. Life insurance premiums increased by 3.2% in 2010 and non-life premiums by 2.1%. While industrialised countries saw an increase in premiums of around 1.4%, insurance markets in emerging economies saw rapid expansion with 11% growth in premium income. The global insurance industry was sufficiently capitalised to withstand the financial crisis of 2008 and 2009 and most insurance companies restored their capital to pre-crisis levels by the end of 2010. With the continuation of the gradual recovery of the global economy, it is likely the insurance industry will continue to see growth in premium income both in industrialised countries and emerging markets in 2011.

Advanced economies account for the bulk of global insurance. With premium income of $1.62 trillion, Europe was the most important region in 2010, followed by North America $1.409 trillion and Asia $1.161 trillion. Europe has however seen a decline in premium income during the year in contrast to the growth seen in North America and Asia. The top four countries generated more than a half of premiums. The United States and Japan alone accounted for 40% of world insurance, much higher than their 7% share of the global population. Emerging economies accounted for over 85% of the world's population but only around 15% of premiums. Their markets are however growing at a quicker pace. The country expected to have the biggest impact on the insurance share distribution across the world is China. According to Sam Radwan of ENHANCE International LLC, low premium penetration (insurance premium as a % of GDP), an ageing population and the largest car market in terms of new sales, premium growth has averaged 15–20% in the past five years, and China is expected to be the largest insurance market in the next decade or two.

In the United States, insurance is regulated by the states under the McCarran-Ferguson Act, with "periodic proposals for federal intervention", and a nonprofit coalition of state insurance agencies called the National Association of Insurance Commissioners works to harmonize the country's different laws and regulations. The National Conference of Insurance Legislators (NCOIL) also works to harmonize the different state laws.

In the European Union, the Third Non-Life Directive and the Third Life Directive, both passed in 1992 and effective 1994, created a single insurance market in Europe and allowed insurance companies to offer insurance anywhere in the EU (subject to permission from authority in the head office) and allowed insurance consumers to purchase insurance from any insurer in the EU. As far as insurance in the United Kingdom, the Financial Services Authority took over insurance regulation from the General Insurance Standards Council in 2005; laws passed include the Insurance Companies Act 1973 and another in 1982, and reforms to warranty and other aspects under discussion .

The insurance industry in China was nationalized in 1949 and thereafter offered by only a single state-owned company, the People's Insurance Company of China, which was eventually suspended as demand declined in a communist environment. In 1978, market reforms led to an increase in the market and by 1995 a comprehensive Insurance Law of the People's Republic of China was passed, followed in 1998 by the formation of China Insurance Regulatory Commission (CIRC), which has broad regulatory authority over the insurance market of China.

In India IRDA is insurance regulatory authority. As per the section 4 of IRDA Act 1999, Insurance Regulatory and Development Authority (IRDA), which was constituted by an act of parliament. National Insurance Academy, Pune is apex insurance capacity builder institute promoted with support from Ministry of Finance and by LIC, Life & General Insurance companies.

In 2017, within the framework of the joint project of the Bank of Russia and Yandex, a special check mark (a green circle with a tick and ‘Реестр ЦБ РФ’ (Unified state register of insurance entities) text box) appeared in the search for Yandex system, informing the consumer that the company's financial services are offered on the marked website, which has the status of an insurance company, a broker or a mutual insurance association.

Insurance is just a risk transfer mechanism wherein the financial burden which may arise due to some fortuitous event is transferred to a bigger entity called an Insurance Company by way of paying premiums. This only reduces the financial burden and not the actual chances of happening of an event. Insurance is a risk for both the insurance company and the insured. The insurance company understands the risk involved and will perform a risk assessment when writing the policy. As a result, the premiums may go up if they determine that the policyholder will file a claim. If a person is financially stable and plans for life's unexpected events, they may be able to go without insurance. However, they must have enough to cover a total and complete loss of employment and of their possessions. Some states will accept a surety bond, a government bond, or even making a cash deposit with the state.

An insurance company may inadvertently find that its insureds may not be as risk-averse as they might otherwise be (since, by definition, the insured has transferred the risk to the insurer), a concept known as moral hazard. This 'insulates' many from the
true costs of living with risk, negating measures that can mitigate or adapt to risk and leading some to describe insurance schemes as potentially maladaptive. To reduce their own financial exposure, insurance companies have contractual clauses that mitigate their obligation to provide coverage if the insured engages in behavior that grossly magnifies their risk of loss or liability.

For example, life insurance companies may require higher premiums or deny coverage altogether to people who work in hazardous occupations or engage in dangerous sports. Liability insurance providers do not provide coverage for liability arising from intentional torts committed by or at the direction of the insured. Even if a provider desired to provide such coverage, it is against the public policy of most countries to allow such insurance to exist, and thus it is usually illegal.

Insurance policies can be complex and some policyholders may not understand all the fees and coverages included in a policy. As a result, people may buy policies on unfavorable terms. In response to these issues, many countries have enacted detailed statutory and regulatory regimes governing every aspect of the insurance business, including minimum standards for policies and the ways in which they may be advertised and sold.

For example, most insurance policies in the English language today have been carefully drafted in plain English; the industry learned the hard way that many courts will not enforce policies against insureds when the judges themselves cannot understand what the policies are saying. Typically, courts construe ambiguities in insurance policies against the insurance company and in favor of coverage under the policy.

Many institutional insurance purchasers buy insurance through an insurance broker. While on the surface it appears the broker represents the buyer (not the insurance company), and typically counsels the buyer on appropriate coverage and policy limitations, in the vast majority of cases a broker's compensation comes in the form of a commission as a percentage of the insurance premium, creating a conflict of interest in that the broker's financial interest is tilted towards encouraging an insured to purchase more insurance than might be necessary at a higher price. A broker generally holds contracts with many insurers, thereby allowing the broker to "shop" the market for the best rates and coverage possible.

Insurance may also be purchased through an agent. A tied agent, working exclusively with one insurer, represents the insurance company from whom the policyholder buys (while a free agent sells policies of various insurance companies). Just as there is a potential conflict of interest with a broker, an agent has a different type of conflict. Because agents work directly for the insurance company, if there is a claim the agent may advise the client to the benefit of the insurance company. Agents generally cannot offer as broad a range of selection compared to an insurance broker.

An independent insurance consultant advises insureds on a fee-for-service retainer, similar to an attorney, and thus offers completely independent advice, free of the financial conflict of interest of brokers or agents. However, such a consultant must still work through brokers or agents in order to secure coverage for their clients.

In the United States, economists and consumer advocates generally consider insurance to be worthwhile for low-probability, catastrophic losses, but not for high-probability, small losses. Because of this, consumers are advised to select high deductibles and to not insure losses which would not cause a disruption in their life. However, consumers have shown a tendency to prefer low deductibles and to prefer to insure relatively high-probability, small losses over low-probability, perhaps due to not understanding or ignoring the low-probability risk. This is associated with reduced purchasing of insurance against low-probability losses, and may result in increased inefficiencies from moral hazard.

Redlining is the practice of denying insurance coverage in specific geographic areas, supposedly because of a high likelihood of loss, while the alleged motivation is unlawful discrimination. Racial profiling or redlining has a long history in the property insurance industry in the United States. From a review of industry underwriting and marketing materials, court documents, and research by government agencies, industry and community groups, and academics, it is clear that race has long affected and continues to affect the policies and practices of the insurance industry.

In July 2007, The Federal Trade Commission (FTC) released a report presenting the results of a study concerning credit-based insurance scores in automobile insurance. The study found that these scores are effective predictors of risk. It also showed that African-Americans and Hispanics are substantially overrepresented in the lowest credit scores, and substantially underrepresented in the highest, while Caucasians and Asians are more evenly spread across the scores. The credit scores were also found to predict risk within each of the ethnic groups, leading the FTC to conclude that the scoring models are not solely proxies for redlining. The FTC indicated little data was available to evaluate benefit of insurance scores to consumers. The report was disputed by representatives of the Consumer Federation of America, the National Fair Housing Alliance, the National Consumer Law Center, and the Center for Economic Justice, for relying on data provided by the insurance industry.

All states have provisions in their rate regulation laws or in their fair trade practice acts that prohibit unfair discrimination, often called redlining, in setting rates and making insurance available.

In determining premiums and premium rate structures, insurers consider quantifiable factors, including location, credit scores, gender, occupation, marital status, and education level. However, the use of such factors is often considered to be unfair or unlawfully discriminatory, and the reaction against this practice has in some instances led to political disputes about the ways in which insurers determine premiums and regulatory intervention to limit the factors used.

An insurance underwriter's job is to evaluate a given risk as to the likelihood that a loss will occur. Any factor that causes a greater likelihood of loss should theoretically be charged a higher rate. This basic principle of insurance must be followed if insurance companies are to remain solvent. Thus, "discrimination" against (i.e., negative differential treatment of) potential insureds in the risk evaluation and premium-setting process is a necessary by-product of the fundamentals of insurance underwriting. For instance, insurers charge older people significantly higher premiums than they charge younger people for term life insurance. Older people are thus treated differently from younger people (i.e., a distinction is made, discrimination occurs). The rationale for the differential treatment goes to the heart of the risk a life insurer takes: Old people are likely to die sooner than young people, so the risk of loss (the insured's death) is greater in any given period of time and therefore the risk premium must be higher to cover the greater risk. However, treating insureds differently when there is no actuarially sound reason for doing so is unlawful discrimination.

New assurance products can now be protected from copying with a business method patent in the United States.

A recent example of a new insurance product that is patented is Usage Based auto insurance. Early versions were independently invented and patented by a major US auto insurance company, Progressive Auto Insurance () and a Spanish independent inventor, Salvador Minguijon Perez ().

Many independent inventors are in favor of patenting new insurance products since it gives them protection from big companies when they bring their new insurance products to market. Independent inventors account for 70% of the new U.S. patent applications in this area.

Many insurance executives are opposed to patenting insurance products because it creates a new risk for them. The Hartford insurance company, for example, recently had to pay $80 million to an independent inventor, Bancorp Services, in order to settle a patent infringement and theft of trade secret lawsuit for a type of corporate owned life insurance product invented and patented by Bancorp.

There are currently about 150 new patent applications on insurance inventions filed per year in the United States. The rate at which patents have been issued has steadily risen from 15 in 2002 to 44 in 2006.

The first insurance patent to be granted was including another example of an application posted was US2009005522 "risk assessment company". It was posted on March 6, 2009. This patent application describes a method for increasing the ease of changing insurance companies.

Insurance on demand (also IoD) is an insurance service that provides clients with insurance protection when they need, i.e. only episodic rather than on 24/7 basis as typically provided by traditional insurers (e.g. clients can purchase an insurance for one single flight rather than a longer-lasting travel insurance plan).

Certain insurance products and practices have been described as rent-seeking by critics. That is, some insurance products or practices are useful primarily because of legal benefits, such as reducing taxes, as opposed to providing protection against risks of adverse events. Under United States tax law, for example, most owners of variable annuities and variable life insurance can invest their premium payments in the stock market and defer or eliminate paying any taxes on their investments until withdrawals are made. Sometimes this tax deferral is the only reason people use these products. Another example is the legal infrastructure which allows life insurance to be held in an irrevocable trust which is used to pay an estate tax while the proceeds themselves are immune from the estate tax.

Muslim scholars have varying opinions about life insurance. Life insurance policies that earn interest (or guaranteed bonus/NAV) are generally considered to be a form of "riba" (usury) and some consider even policies that do not earn interest to be a form of "gharar" (speculation). Some argue that "gharar" is not present due to the actuarial science behind the underwriting.
Jewish rabbinical scholars also have expressed reservations regarding insurance as an avoidance of God's will but most find it acceptable in moderation.

Some Christians believe insurance represents a lack of faith and there is a long history of resistance to commercial insurance in Anabaptist communities (Mennonites, Amish, Hutterites, Brethren in Christ) but many participate in community-based self-insurance programs that spread risk within their communities.

Country-specific articles:



</doc>
<doc id="15179" url="https://en.wikipedia.org/wiki?curid=15179" title="Indira Gandhi">
Indira Gandhi

Indira Priyadarshini Gandhi (; née Nehru; 19 November 1917 – 31 October 1984) was an Indian politician, stateswoman and a central figure of the Indian National Congress. She was the first and, to date, the only female Prime Minister of India. Indira Gandhi was the daughter of Jawaharlal Nehru, the first prime minister of India. Despite her surname "Gandhi", she is not related to the family of Mahatma Gandhi; Gandhi is a common surname in Gujarat. She served as Prime Minister from January 1966 to March 1977 and again from January 1980 until her assassination in October 1984, making her the second longest-serving Indian prime minister after her father. 
Gandhi served as her father's personal assistant and hostess during his tenure as Prime Minister between 1947 and 1964. She was elected Congress President in 1959. Upon her father's death in 1964 she was appointed as a member of the Rajya Sabha (upper house) and became a member of Lal Bahadur Shastri's cabinet as Minister of Information and Broadcasting. In the Congress Party's parliamentary leadership election held in early 1966 (upon the death of Shastri), she defeated her rival, Morarji Desai, to become leader, and thus succeeded Shastri as Prime Minister of India.

As Prime Minister, Gandhi was known for her political ruthlessness and unprecedented centralisation of power. She went to war with Pakistan in support of the independence movement and war of independence in East Pakistan, which resulted in an Indian victory and the creation of Bangladesh, as well as increasing India's influence to the point where it became the regional hegemon of South Asia. Citing fissiparous tendencies and in response to a call for revolution, Gandhi instituted a state of emergency from 1975 to 1977 where basic civil liberties were suspended and the press was censored. Widespread atrocities were carried out during the emergency. In 1980, she returned to power after free and fair elections. She was assassinated by her own bodyguards and Sikh nationalists in 1984. The assassins, Beant Singh and Satwant Singh, were both shot by other security guards. Satwant Singh recovered from his injuries and was executed after being convicted of murder.

In 1999, Indira Gandhi was named "Woman of the Millennium" in an online poll organised by the BBC.

Indira Gandhi was born as Indira Nehru in a Kashmiri Pandit family on 19 November, 1917 in Allahabad. Her father, Jawaharlal Nehru, was a leading figure in India's political struggle for independence from British rule, and became the first Prime Minister of the Dominion (and later Republic) of India. She was the only child (a younger brother was born, but died young), and grew up with her mother, Kamala Nehru, at the Anand Bhavan; a large family estate in Allahabad. She had a lonely and unhappy childhood. Her father was often away, directing political activities or incarcerated, while her mother was frequently bed-ridden with illness, and later suffered an early death from tuberculosis. She had limited contact with her father, mostly through letters.
Indira was mostly taught at home by tutors, and intermittently attended school until matriculation in 1934. She was a student at the Modern School in Delhi, St Cecilia's and St Mary's Christian convent schools in Allahabad, the International School of Geneva, the Ecole Nouvelle in Bex, and the Pupils' Own School in Poona and Bombay, which is affiliated to University of Mumbai. She and her mother Kamala Nehru moved to Belur Math headquarters of Ramakrishna Mission where Swami Ranganathananda was her guardian later she went on to study at the Visva-Bharati University in Santiniketan. It was during her interview that Rabindranath Tagore named her Priyadarshini, and she came to be known as Indira Priyadarshini Nehru. A year later, however, she had to leave university to attend to her ailing mother in Europe. While there, it was decided that Indira would continue her education at the University of Oxford. After her mother died, she briefly attended the Badminton School before enrolling at Somerville College in 1937 to study history. Indira had to take the entrance examination twice, having failed at her first attempt with a poor performance in Latin. At Oxford, she did well in history, political science and economics, but her grades in Latin—a compulsory subject—remained poor. She did, however, have an active part within the student life of the university, such as the Oxford Majlis Asian Society.

During her time in Europe, Indira was plagued with ill-health and was constantly attended to by doctors. She had to make repeated trips to Switzerland to recover, disrupting her studies. She was being treated there in 1940, when the Nazi armies rapidly conquered Europe. Gandhi tried to return to England through Portugal but was left stranded for nearly two months. She managed to enter England in early 1941, and from there returned to India without completing her studies at Oxford. The university later awarded her an honorary degree. In 2010, Oxford further honoured her by selecting her as one of the ten Oxasians, illustrious Asian graduates from the University of Oxford. 
During her stay in Great Britain, Indira frequently met her future husband Feroze Gandhi (no relation to Mahatma Gandhi), whom she knew from Allahabad, and who was studying at the London School of Economics. The marriage took place in Allahabad according to Adi Dharm rituals though Feroze belonged to a Zoroastrian Parsi family of Gujarat. The couple had two sons, Rajiv Gandhi (born 1944) and Sanjay Gandhi (born 1946).

In the 1950s, Indira, now Mrs Indira Gandhi after her marriage, served her father unofficially as a personal assistant during his tenure as the first Prime Minister of India. Towards the end of the 1950s, Indira Gandhi served as the President of the Congress. In that capacity, she was instrumental in getting the Communist led Kerala State Government dismissed in 1959. That government had the distinction of being India's first ever elected Communist Government. After her father's death in 1964 she was appointed as a member of the Rajya Sabha (upper house) and served in Prime Minister Lal Bahadur Shastri's cabinet as Minister of Information and Broadcasting. In January 1966, after Shastri's death, the Congress legislative party elected Indira Gandhi over Morarji Desai as their leader. Congress party veteran K. Kamaraj was instrumental in achieving Indira's victory. Because she was a woman, other political leaders in India saw Gandhi as weak and hoped to use her as a puppet once elected: Congress President Kamaraj orchestrated Mrs. Gandhi's selection as prime minister because he perceived her to be weak enough that he and the other regional party bosses could control her, and yet strong enough to beat Desai [her political opponent] in a party election because of the high regard for her father…a woman would be an ideal tool for the Syndicate.

The first eleven years of Indira's position as prime minister saw her evolving from the perception of Congress party leaders as their puppet to a strong leader with the iron resolve to split the party for her policy positions or to go to war with Pakistan to liberate Bangladesh. At the end of this term in 1977, she was such a dominating figure in Indian politics that a Congress party president had coined the phrase "India is Indira and Indira is India."

Indira formed her government with Morarji Desai as deputy prime minister and finance minister. At the beginning of her first term as prime minister, Indira was widely criticized by the media and the opposition as a "Goongi goodiya" (Hindi word for a dumb doll or puppet) of the Congress party bosses who had got her elected and tried to constrain her.

The first electoral test for Indira was the 1967 general elections for the Lok sabha and state assemblies. The Congress Party won a reduced majority for the Lok sabha in these elections owing to widespread disenchantment over rising prices of commodities, unemployment, economic stagnation and a food crisis. Indira Gandhi had started on a rocky note after agreeing to a devaluation of the rupee, which created much hardship for Indian businesses and consumers, and the import of wheat from the United States fell through due to political disputes.

The party also for the first time lost power or lost majority in a number of states across the country. Following the 1967 elections, Indira Gandhi gradually started moving towards socialist policies. In 1969, she fell out with senior Congress party leaders on a number of issues. Chief among them was the decision by Indira to support V. V. Giri, the independent candidate rather than the official Congress party candidate Neelam Sanjiva Reddy for the vacant position of President of India. The other was the announcement by the prime minister of Bank nationalization without consulting the finance minister, Morarji Desai. These steps culminated in Party president S. Nijalingappa expelling her from the party for indiscipline. Gandhi, in turn floated her own faction of the Congress party and managed to retain most of the Congress MPs on her side with only 65 on the side of Congress (O) faction. The Indira faction, called Congress (R), lost its majority in the parliament but remained in power with the support of regional parties such as DMK. The policies of the Congress under Indira Gandhi, prior to the 1971 elections, also included proposals for the abolition of Privy Purse to former rulers of the Princely states and the 1969 nationalization of the fourteen largest banks in India.

Garibi Hatao (Eradicate Poverty) was the theme for Gandhi's 1971 bid. On the other hand, the combined opposition alliance had a two word manifesto of "Indira Hatao" (Remove Indira). The Garibi Hatao slogan and the proposed anti-poverty programs that came with it were designed to give Gandhi an independent national support, based on rural and urban poor. This would allow her to bypass the dominant rural castes both in and of state and local governments; likewise the urban commercial class. And, for their part, the previously voiceless poor would at last gain both political worth and political weight. The programs created through Garibi Hatao, though carried out locally, were funded and developed by the Central Government in New Delhi. The program was supervised and staffed by the Indian National Congress party. "These programs also provided the central political leadership with new and vast patronage resources to be disbursed... throughout the country.",

The biggest achievement of Indira Gandhi after the 1971 election came in December 1971 with India's decisive victory over Pakistan in the liberation war, that led to the formation of independent Bangladesh. She was hailed as Goddess Durga by opposition leader Atal Bihari Vajpayee at that time. In the elections held for State assemblies across India in March 1972, the Congress (R) swept to power in most states riding on the post-war "Indira wave".

Despite the victory against Pakistan, the Congress government faced numerous problems during this term. Some of these were due to high inflation which was in turn caused by war time expenses, drought in some parts of the country and more importantly, the 1973 oil crisis. The opposition to Gandhi in 1973–75 period, after the Indira wave had receded, was strongest in the states of Bihar and Gujarat. In Bihar, Jayaprakash Narayan, the veteran leader came out of retirement to lead the protest movement there.

On 12 June 1975, the Allahabad High Court declared Indira Gandhi's election to the Lok Sabha in 1971 void on grounds of electoral malpractice. In an election petition filed by her 1971 opponent, Raj Narain (who later on defeated her in 1977 parliamentary election from Raebareli), alleged several major as well as minor instances of using government resources for campaigning. The court ordered her stripped of her parliamentary seat and banned from running for any office for six years. According to constitution, the Prime Minister must be a member of either the Lok Sabha (the lower house in the Parliament of India) or a member of the Rajya Sabha (the upper house). Thus, this decision effectively removed her from office. Gandhi had asked one of her colleagues in government, Mr. Ashoke Kumar Sen to defend her in court.

But Gandhi rejected calls to resign and announced plans to appeal to the Supreme Court. The verdict was delivered by Mr Justice Jagmohanlal Sinha at Allahabad High Court. It came almost four years after the case was brought by Raj Narain, the premier's defeated opponent in the 1971 parliamentary election. Gandhi, who gave evidence in her defence during the trial, was found guilty of dishonest election practices, excessive election expenditure, and of using government machinery and officials for party purposes. The judge, however, rejected more serious charges of bribery against her.

Gandhi insisted that the conviction did not undermine her position, despite having been unseated from the lower house of parliament, Lok Sabha, by order of the High Court. She said: "There is a lot of talk about our government not being clean, but from our experience the situation was very much worse when [opposition] parties were forming governments". And she dismissed criticism of the way her Congress Party raised election campaign money, saying all parties used the same methods. The prime minister retained the support of her party, which issued a statement backing her. After news of the verdict spread, hundreds of supporters demonstrated outside her house, pledging their loyalty. Indian High Commissioner BK Nehru said Gandhi's conviction would not harm her political career. "Mrs Gandhi has still today overwhelming support in the country," he said. "I believe the prime minister of India will continue in office until the electorate of India decides otherwise".

Gandhi moved to restore order by ordering the arrest of most of the opposition participating in the unrest. Her Cabinet and government then recommended that President Fakhruddin Ali Ahmed declare a state of emergency because of the disorder and lawlessness following the Allahabad High Court decision. Accordingly, Ahmed declared a State of Emergency caused by internal disorder, based on the provisions of Article 352(1) of the Constitution, on 25 June 1975.

Within a few months, President's rule was imposed on the two opposition party ruled states of Gujarat and Tamil Nadu thereby bringing the entire country under direct Central rule or by governments led by the ruling Congress party. Police were granted powers to impose curfews and indefinitely detain citizens and all publications were subjected to substantial censorship by the Ministry of Information and Broadcasting. Finally, the impending legislative assembly elections were indefinitely postponed, with all opposition-controlled state governments being removed by virtue of the constitutional provision allowing for a dismissal of a state government on recommendation of the state's governor.

Indira Gandhi used the emergency provisions to change conflicting party members.
President Ahmed issued ordinances that did not require debate in the Parliament, allowing Gandhi to rule by decree.

The Emergency saw the entry of Gandhi's younger son, Sanjay Gandhi, into Indian Politics. Sanjay wielded tremendous power during the emergency without holding any Government office. According to Mark Tully, "His inexperience did not stop him from using the Draconian powers his mother, Indira Gandhi, had taken to terrorise the administration, setting up what was in effect a police state."

It was said that during the Emergency he virtually ran India along with his friends, especially Bansi Lal. It was also quipped that Sanjay Gandhi had total control over his mother and that the government was run by the PMH (Prime Minister House) rather than the PMO (Prime Minister Office).

In 1977, after extending the state of emergency twice, Indira Gandhi called elections to give the electorate a chance to vindicate her rule. Gandhi may have grossly misjudged her popularity by reading what the heavily censored press wrote about her. In any case, she was opposed by the Janata alliance of Opposition parties. The alliance was made up of right-wing Hindu leaning Bharatiya Jana Sangh, Congress (O), The Socialist parties, and Charan Singh's Bharatiya Kranti Dal representing northern peasant and farmers. Janata alliance, with Jai Prakash Narayan as its spiritual guide, claimed the elections were the last chance for India to choose between "democracy and dictatorship." The Congress Party split during the election campaign of 1977: veteran Indira supporters like Jagjivan Ram, Hemvati Nandan Bahuguna and Nandini Satpathy were compelled to part ways and form a new political entity, CFD (Congress for Democracy), primarily due to intra-party politicking and also due to circumstances created by Sanjay Gandhi. The prevailing rumour was that Sanjay had intentions of dislodging Gandhi and the trio stood between that. Gandhi's Congress party was crushed soundly in the elections. The public realized the statement and motto of the Janata Party alliance. Indira and Sanjay Gandhi both lost their seats, and Congress was cut down to 153 seats (compared with 350 in the previous Lok Sabha), 92 of which were in the South. The Janata alliance, under the leadership of Morarji Desai, came into power after the State of Emergency was lifted. The alliance parties later merged to form the Janata Party under the guidance of Gandhian leader, Jayaprakash Narayan. The other leaders of the Janata Party were Charan Singh, Raj Narain, George Fernandes and Atal Bihari Vajpayee.

Since Gandhi had lost her seat in the election, the defeated Congress party appointed Yashwantrao Chavan as their parliamentary party leader. Soon afterwards, the Congress party split again with Gandhi floating her own Congress faction. She won a by-election from the Chikmagalur Constituency to the Lok Sabha in November 1978. However, the Janata government's Home Minister, Choudhary Charan Singh, ordered the arrest of her and Sanjay Gandhi on several charges, none of which would be easy to prove in an Indian court. The arrest meant that Indira Gandhi was automatically expelled from Parliament. These allegations included that she "had planned or thought of killing all opposition leaders in jail during the Emergency". In response to her arrest, Indira Gandhi's supporters hijacked an Indian Airlines jet and demanded her immediate release. However, this strategy backfired disastrously. Her arrest and long-running trial gained her great sympathy from many people. The Janata coalition was only united by its hatred of Gandhi (or "that woman" as some called her). The party included right wing Hindu Nationalists, Socialists and former Congress party members. With so little in common, the Morarji Desai government was bogged down by infighting. In 1979, the government started to unravel over the issue of dual loyalties of some members to Janata and the RSS. The ambitious Union Finance minister, Charan Singh, who as the Union Home Minister during the previous year had ordered arrest of Gandhi, took advantage of this and started courting the Congress. After a significant exodus from the party to Charan Singh's faction, Desai resigned in July 1979. Charan Singh was appointed Prime Minister, by President Reddy, after Indira and Sanjay Gandhi promised Singh that Congress would support his government from outside on certain conditions. The conditions included dropping all charges against Indira and Sanjay. Since Charan Singh refused to drop the charges, Congress withdrew its support and President Reddy dissolved Parliament in August 1979.

Before the 1980 elections Gandhi approached the then Shahi Imam of Jama Masjid, Syed Abdullah Bukhari and entered into an agreement with him on the basis of 10-point programme to secure the support of the Muslim votes. In the elections held in January, Congress returned to power with a landslide majority.

The Congress under Gandhi swept back to power in January 1980. Elections soon after to State assemblies across the country also brought back Congress ministries in the state with Indira's son Sanjay Gandhi choosing loyalists to lead the states. On 23 June, Gandhi's son Sanjay was killed in an air crash while performing an aerobatic manoeuvre in New Delhi. Gandhi by this stage only trusted family members, and therefore persuaded her reluctant son, Rajiv, to enter politics.

In the 1977 elections, a coalition led by the Sikh-majority Akali Dal came to power in the northern Indian state of Punjab. In an effort to split the Akali Dal and gain popular support among the Sikhs, Indira Gandhi's Congress helped bring the orthodox religious leader Jarnail Singh Bhindranwale to prominence in Punjab politics. Later, Bhindranwale's organisation Damdami Taksal became embroiled in violence with another religious sect called the Sant Nirankari Mission, and he was accused of instigating the murder of Jagat Narain, the owner of "Punjab Kesari" newspaper After being arrested in this matter, Bhindranwale disassociated himself from Congress and joined hands with the Akali Dal. In July 1982, he led the campaign for the implementation of the Anandpur Resolution, which demanded greater autonomy for the Sikh-majority state. Meanwhile, a small section of the Sikhs, including some of Bhindranwale's followers, turned to militancy after being targeted by government officials and police in support of the Resolution. In 1982, Bhindranwale and approximately 200 armed followers moved into a guest house called the Guru Nanak Niwas, in the precinct of the Golden Temple

By 1983, the Temple complex had become a fort for a large number of militants. "The Statesman" later reported that light machine guns and semi-automatic rifles were known to have been brought into the compound. On 23 April 1983, the Punjab Police Deputy Inspector General A. S. Atwal was shot dead as he left the Temple compound. The following day, after the murder, Harchand Singh Longowal (then president of Shiromani Akali Dal) confirmed the involvement of Bhindranwale in the murder.
After several futile negotiations, Indira Gandhi ordered the Indian army in June 1984 to enter the Golden Temple in order to remove Bhindranwale and his supporters from the complex. The army used heavy artillery, including tanks, in the action code-named Operation Blue Star. The operation badly damaged or destroyed parts of the Temple complex, including the Akal Takht shrine and the Sikh library. It also led to the deaths of a large number of Sikh fighters and innocent pilgrims. The number of casualties remain disputed with estimates ranging from many hundreds to many thousands

Gandhi was accused of using the attack for political ends. Dr. Harjinder Singh Dilgeer stated that Indira Gandhi attacked the temple complex to present herself as a great hero in order to win general elections planned towards the end of 1984. There was fierce criticism of the action by Sikhs in India and overseas. There was also incidents of mutiny by Sikh soldiers in the aftermath of the attack.

The day before her death (30 October 1984) Indira Gandhi visited Orissa where she gave her last speech at the then Parade Ground in front of the Secretariat of Orissa. In that speech she strikingly associated her blood with the health of the nation

On 31 October 1984, two of Gandhi's bodyguards, Satwant Singh and Beant Singh, shot her with their service weapons in the garden of the Prime Minister's residence at 1 Safdarjung Road, New Delhi. The shooting occurred as she was walking past a wicket gate guarded by Satwant and Beant. She was to have been interviewed by the British actor Peter Ustinov, who was filming a documentary for Irish television. Beant Singh shot her three times using his side-arm and Satwant Singh fired 30 rounds. Beant Singh and Satwant Singh dropped their weapons and surrendered. Afterwards they were taken away by other guards into a closed room where Beant Singh was shot dead. Kehar Singh was later arrested for conspiracy in the attack. Both Satwant and Kehar were sentenced to death and hanged in Delhi's Tihar Jail.

Indira Gandhi was brought at 9:30 AM to the All India Institutes of Medical Sciences where doctors operated on her. She was declared dead at 2:20 PM. The post-mortem examination was conducted by a team of doctors headed by Dr. Tirath Das Dogra. Dr Dogra stated that as many as 30 bullet wounds were sustained by Indira Gandhi, from two sources, a Sterling submachine gun and a pistol. The assailants had fired 31 bullets at her, of which 30 had hit; 23 had passed through her body while 7 were trapped inside her. Dr Dogra extracted bullets to establish the identity of the weapons and to match each weapon with the bullets recovered by ballistic examination. The bullets were matched with respective weapons at CFSL Delhi. Subsequently, Dr Dogra appeared in the court of Shri Mahesh Chandra as an expert witness (PW-5), and his testimony lasted several sessions. The cross examination was conducted by Shri Pran Nath Lekhi, the defence counsel. Salma Sultan gave the first news of the assassination of Indira Gandhi on Doordarshan's evening news on 31 October 1984, more than 10 hours after she was shot. She died two weeks and five days before her 67th birthday.
Gandhi was cremated on 3 November near Raj Ghat. The site where she was cremated is today known as Shakti Sthal.

Her funeral was televised live on domestic and international stations, including the BBC. Following her cremation, millions of Sikhs were displaced and nearly three thousand were killed in anti-Sikh riots. Rajiv Gandhi on a live TV show said of the carnage, "When a big tree falls, the earth shakes."

Indira Gandhi is remembered for her ability to effectively promote Indian foreign policy measures.

In early 1971, disputed elections in Pakistan led the then East Pakistan to declare independence as Bangladesh. Repression and violence by the Pakistani army led 10 million refugees to cross border in to India over the coming months. Finally in December 1971, Gandhi directly intervened in the conflict to liberate Bangladesh. India emerged victorious in the resulting conflict to become the dominant power of South Asia. India had signed a treaty with the Soviet Union promising mutual assistance in the case of war, while Pakistan received active support from the United States during the conflict. U.S. President Richard Nixon disliked Gandhi personally, referring to her as a "witch" and "clever fox" in his private communication with Secretary of State Henry Kissinger. Nixon later wrote of the war: "[Gandhi] suckered [America]. Suckered us...this woman suckered us.". Relations with the U.S. became distant as Gandhi developed closer ties with the Soviet Union after the war. The latter grew to become India's largest trading partner and its biggest arms supplier for much of Gandhi's premiership. India's new hegemonic position as articulated under the "Indira Doctrine" led to attempts to bring the Himalayan states under the Indian sphere of influence. Nepal and Bhutan remained aligned with India, while in 1975, after years of building up support, Gandhi incorporated Sikkim into India, after a referendum in which a majority of Sikkimese voted to join India. This was denounced as a "despicable act" by China.

India maintained close ties with neighbouring Bangladesh (formerly East Pakistan) following the Liberation War. Prime Minister Sheikh Mujibur Rahman recognized Gandhi's contributions to the independence of Bangladesh. However, Mujibur Rahman's pro-India policies antagonised many in Bangladeshi politics and the military, who feared that Bangladesh had become a client state of India. The Assassination of Mujibur Rahman in 1975 led to the establishment of Islamist military regimes that sought to distance the country from India. Gandhi's relationship with the military regimes was strained, due to her alleged support of anti-Islamist leftist guerrilla forces in Bangladesh. Generally, however, there was a rapprochement between Gandhi and the Bangladeshi regimes, although issues such as border disputes and the Farakka Dam remained an irritant in bilateral ties. In 2011, the Government of Bangladesh conferred its highest state award posthumously on Gandhi for her "outstanding contribution" to the country's independence.
Gandhi's approach to dealing with Sri Lanka's ethnic problems was initially accommodating. She enjoyed cordial relations with Prime Minister Sirimavo Bandaranaike. In 1974, India ceded the tiny islet of Katchatheevu to Sri Lanka in order to save Bandaranaike's socialist government from a political disaster. However, relations soured over Sri Lanka's turn away from socialism under J. R. Jayewardene, whom Gandhi despised as a "western puppet." India under Gandhi was alleged to have supported LTTE militants in the 1980s to put pressure on Jayewardene to abide by Indian interests. Nevertheless, Gandhi rejected demands to invade Sri Lanka in the aftermath of Black July 1983, an anti-Tamil pogrom carried out by Sinhalese mobs. Gandhi made a statement emphasizing that she stood for the territorial integrity of Sri Lanka, although she also stated that India cannot "remain a silent spectator to any injustice done to the Tamil community."

India's relationship with Pakistan remained strained following the Shimla Accord in 1972. Gandhi's authorization of the detonation of a nuclear device at Pokhran in 1974 was viewed by Pakistani leader Zulfikar Ali Bhutto as an attempt to intimidate Pakistan into accepting India's hegemony in the subcontinent. However, in May 1976, Gandhi and Bhutto both agreed to reopen diplomatic establishments and normalize relations. Following the rise to power of General Muhammad Zia-ul-Haq in Pakistan in 1978, India's relations with its neighbour reached a nadir. Gandhi accused General Zia of supporting Khalistani militants in Punjab. Military hostilities recommenced in 1984 following Gandhi's authorization of Operation Meghdoot. India was victorious in the resulting Siachen conflict against Pakistan.

In order to keep the Soviet Union and the United States out of South Asia, Mrs Gandhi was instrumental in establishing the South Asian Association for Regional Cooperation (SAARC) in 1983

Gandhi remained a staunch supporter of Palestinians in the Arab–Israeli conflict and was critical of the Middle East diplomacy sponsored by the United States. Israel was viewed as a religious state and thus an analogue to India's archrival Pakistan. Indian diplomats also hoped to win Arab support in countering Pakistan in Kashmir. Nevertheless, Gandhi authorized the development of a secret channel of contact and security assistance with Israel in the late 1960s. Her lieutenant, P. V. Narasimha Rao, later became Prime Minister and approved full diplomatic ties with Israel in 1992.

India's pro-Arab policy had mixed success. Establishment of close ties with the socialist and secular Baathist regimes to some extent neutralized Pakistani propaganda against India. However, the Indo-Pakistani War of 1971 put the Arab and Muslim states of the Middle East in a dilemma as the war was fought by two states both friendly to the Arabs. The progressive Arab regimes in Egypt, Syria, and Algeria chose to remain neutral, while the conservative pro-American Arab monarchies in Jordan, Saudi Arabia, Kuwait, and United Arab Emirates openly supported Pakistan. Egypt's stance was met with dismay by the Indians, who had come to expect close co-operation with the Baathist regimes. But, the death of Nasser in 1970 and Sadat's growing friendship with Riyadh, and his mounting differences with Moscow, constrained Egypt to a policy of neutrality. Gandhi's overtures to Muammar Gaddafi were rebuffed. Libya agreed with the Arab monarchies in believing that Gandhi's intervention in East Pakistan was an attack against Islam.

The 1971 war temporarily became a stumbling block in growing Indo-Iranian ties. Although Iran had earlier characterized the Indo-Pakistani war in 1965 as Indian aggression, the Shah had launched an effort at rapprochement with India in 1969 as part of his effort to secure support for a larger Iranian role in the Persian Gulf. Gandhi's tilt towards Moscow and her dismemberment of Pakistan was perceived by the Shah as part of a larger anti-Iran conspiracy involving India, Iraq, and the Soviet Union. Nevertheless, Iran had resisted Pakistani pressure to activate the Baghdad Pact and draw in the Central Treaty Organisation (CENTO) into the conflict. Gradually, Indian and Iranian disillusionment with their respective regional allies led to a renewed partnership between the nations. Gandhi was unhappy with the lack of support from India's Arab allies during the war with Pakistan, while the Shah was apprehensive at the growing friendship between Pakistan and Arab states of the Persian Gulf, specially Saudi Arabia, and the growing influence of Islam in Pakistani society. There was an increase in Indian economic and military co-operation with Iran during the 1970s. The 1974 India-Iranian agreement led to Iran supplying nearly 75 percent of India's crude oil demands. Gandhi appreciated the Shah's disregard of Pan-Islamism in diplomacy.

One of the major developments in Southeast Asia during Gandhi's premiership was the formation of the Association of Southeast Asian Nations (ASEAN) in 1967. Relations between ASEAN and India was mutually antagonistic. ASEAN in the Indian perception was linked to the Southeast Asia Treaty Organization (SEATO), and it was therefore, seen as a pro-American organisation. On their part, the ASEAN nations were unhappy with Gandhi's sympathy for the Viet Cong and India's strong links with the USSR. Furthermore, they were also apprehensions in the region about Gandhi's future plans, particularly after India played a big role in breaking up Pakistan and facilitating in the emergence of Bangladesh as a sovereign country in 1971. India's entry into the nuclear weapons club in 1974 contributed to tensions in Southeast Asia. Relations only began to improve following Gandhi's endorsement of the ZOPFAN declaration and the disintegration of the SEATO alliance in the aftermath of Pakistani and American defeats in the region. Nevertheless, Gandhi's close relations with reunified Vietnam and her decision to recognize the Vietnam installed Government of Cambodia in 1980 meant that India and ASEAN were not able to develop a viable partnership.

Although independent India was initially viewed as a champion of anti-colonialism, its cordial relationship with the Commonwealth of Nations and liberal views of British colonial policies in East Africa had harmed its image as a staunch supporter of the anti-colonial movements. Indian condemnation of militant struggles in Kenya and Algeria was in sharp contrast to China, who had supported armed struggle to win African independence. After reaching a high diplomatic point in the aftermath of Nehru's role in the Suez Crisis, India's isolation from Africa was complete when only four nations; Ethiopia, Kenya, Nigeria and Libya supported her during the Sino-Indian War in 1962. After Gandhi became Prime Minister, diplomatic and economic relations with the states which had sided with India during the Sino-Indian War were expanded. Gandhi began negotiations with the Kenyan government to establish the Africa-India Development Cooperation. The Indian government also started considering the possibility of bringing Indians settled in Africa within the framework of its policy goals to help recover its declining geo-strategic influence. Gandhi declared the people of Indian origin settled in Africa as "Ambassadors of India." Efforts to rope in the Asian community to join Indian diplomacy, however, came to naught, partly because of the unwillingness of Indians to remain in politically insecure surroundings and partly due to the exodus of African Indians to Britain with the passing of the Commonwealth Immigrants Act in 1968. In Uganda, the African Indian community even suffered persecution and eventually expulsion under the government of Idi Amin.

Foreign and domestic policy successes in the 1970s enabled Gandhi to rebuild India's image in the eyes of African states. Victory over Pakistan and India's possession of nuclear weapons showed the degree of India's progress. Furthermore, the conclusion of the Indo-Soviet treaty in 1971 and threatening gestures by the major western power, the United States, to send its nuclear armed Task Force 74 into the Bay of Bengal at the height of the East Pakistan crisis had enabled India to regain its anti-imperialist image. Gandhi firmly tied Indian anti-imperialist interests in Africa to those of the Soviet Union. Unlike Nehru, she openly and enthusiastically supported liberation struggles in Africa. At the same time, Chinese influence in Africa had declined owing to its incessant quarrels with the Soviet Union. These developments permanently halted India's decline in Africa and helped reestablish its geo-strategic presence.

The Commonwealth is voluntary association of mainly former British colonies. India maintained cordial relations with most of the members during her time in power. In the 1980s, Indira Gandhi along with Canadian Prime Minister Pierre Trudeau, Zambia's President Kenneth Kaunda, Australian prime minister Malcolm Fraser and Singapore Prime Minister Lee Kuan Yew was regarded as one of the pillars of the commonwealth India under Indira also hosted the 1983 Commonwealth heads of Government summit in New Delhi in 1983. Gandhi used to use the Commonwealth meetings as a forum to put pressure on member countries to cut economic, sports, and cultural ties with Apartheid South Africa 

In the early 1980s under Gandhi, India attempted to reassert its prominent role in the Non-Aligned Movement by focusing on the relationship between disarmament and economic development. By appealing to the economic grievances of developing countries, Gandhi and her successors exercised a moderating influence on the Non-aligned movement, diverting it from some of the Cold War issues that marred the controversial 1979 Havana meeting where Cuban leader Fidel Castro attempted to steer the movement towards the Soviet Union. Although hosting the 1983 summit at Delhi boosted Indian prestige within the movement, its close relations with the Soviet Union and its pro-Soviet positions on Afghanistan and Cambodia limited its influence

Indira spent a number of years in Europe during her youth and formed many friendships during her stay there. During her premiership she formed friendship with many socialist leaders such as German chancellor, Willy Brandt and Austrian chancellor Bruno Kreisky. She also enjoyed closed working relationship with many British leaders including conservative premiers, Edward Heath and Margaret Thatcher.

The relationship between India and the Soviet Union deepened during Gandhi's rule. The main reason was the perceived bias of United States and China, the rivals of USSR, towards Pakistan. The support of USSR with arms supplies and casting of veto at United Nations helped in winning and consolidating the victory over Pakistan in the 1971 Bangladesh liberation war. Prior to the war Indira signed a treaty of friendship with the USSR. The USSR was not happy with the 1974 nuclear test conducted by India but did not support further action because of the ensuing cold war with the United States. Indira was not happy with the Soviet invasion of Afghanistan but once again calculations involving relations with Pakistan and China kept from criticizing the Soviet Union harshly. USSR became the main arms supplier during the Indira years by offering cheap credit and transactions in rupees rather than in dollars. The easy trade deals also applied to non-military goods. Under Indira by the early 1980s the USSR became the largest trading partner of India.

When Indira came to power in 1966, Lyndon Johnson was the US President. At that time, India was reliant on USA for food aid. Indira resented the US policy of food aid as a tool in forcing India to adopt policies favored by the US. She also resolutely refused to sign the NPT (Treaty on the Non-Proliferation of Nuclear Weapons). Relations with US strained badly under President Richard Nixon and his favoring of Pakistan during the Bangladesh liberation war. Nixon despised Indira politically and personally. Indira met President Ronald Reagan in 1981 for the first time at the North–South Summit held to discuss global poverty. Indira had been described to him as an 'Ogre' but he found her charming and easy to work with and they formed a close working relationship during her premiership in the 1980s.

Gandhi presided over three Five-Year plans as Prime Minister, two of which succeeded in meeting the targeted growth.

There is considerable debate regarding whether Gandhi was a socialist on principle or out of political expediency. Sunanda K. Datta-Ray described her as "a master of rhetoric...often more posture than policy", while "The Times" journalist, Peter Hazelhurst, famously quipped that Gandhi's socialism was "slightly left of self-interest." Critics have focused on the contradictions in the evolution of her stance towards communism; Gandhi being known for her anti-communist stance in the 1950s with Meghnad Desai even describing her as "the scourge of [India's] Communist Party." Yet, she later forged close relations with Indian communists even while using the army to break the Naxalites. In this context, Gandhi was accused of formulating populist policies to suit her political needs; being seemingly against the rich and big business while preserving the status quo in order to manipulate the support of the left at times of political insecurity, such as the late 1960s. Although Gandhi came to be viewed in time as the scourge of the right-wing and reactionary political elements of India, leftist opposition to her policies emerged. As early as 1969, critics had begun accusing her of insincerity and machiavellianism. "The Indian Libertarian" wrote that: "it would be difficult to find a more machiavellian leftist than Mrs Indira Gandhi...for here is Machiavelli at its best in the person of a suave, charming and astute politician." Rosser wrote that "some have even seen the declaration of emergency rule in 1975 as a move to suppress [leftist] dissent against Gandhi's policy shift to the right." In the 1980s, Gandhi was accused of "betraying socialism" after the beginning of "Operation Forward", an attempt at economic reform. Nevertheless, others were more convinced of Gandhi's sincerity and devotion to socialism. Pankaj Vohra noted that "even the late prime minister's critics would concede that the maximum number of legislations of social significance was brought about during her tenure...[and that] she lives in the hearts of millions of Indians who shared her concern for the poor and weaker sections and who supported her politics."

In summarizing the biographical works on Gandhi, Blema S. Steinberg concluded she was decidedly non-ideological. Only 7.4% (24) of the total 330 biographical extractions posit ideology as a reason for her policy choices. Steinberg noted Gandhi's association with socialism was superficial; only having a general and traditional commitment to the ideology, by way of her political and family ties. Gandhi personally had a fuzzy concept of socialism. In one of the early interviews she had given as Prime Minister, Gandhi had ruminated: "I suppose you could call me a socialist, but you have understand what we mean by that term...we used the word [socialism] because it came closest to what we wanted to do here – which is to eradicate poverty. You can call it socialism; but if by using that word we arouse controversy, I don't see why we should use it. I don't believe in words at all." Regardless of the debate over her ideology or lack of thereof, Gandhi remains a left-wing icon. She has been described by Hindustan Times columnist, Pankaj Vohra as "arguably the greatest mass leader of the last century." Her campaign slogan, Garibi Hatao (Eng: Remove Poverty), has become the iconic motto of the Indian National Congress. To the rural and urban poor, untouchables, minorities and women in India, Gandhi was "Indira Amma or Mother Indira."

Gandhi inherited a weak and troubled economy. Fiscal problems associated with the war with Pakistan in 1965, along with a drought-induced food crisis that spawned famines, had plunged India into the sharpest recession since independence. The government responded by taking steps to liberalize the economy, and by agreeing to the devaluation of the currency in return for the restoration of foreign aid. The economy managed to recover in 1966 and ended up growing at 4.1% over 1966–1969. But, much of that growth was offset by the fact that the external aid promised by the United States government and the International Bank for Reconstruction and Development (IBRD), meant to ease the short-run costs of adjustment to a liberalized economy, never materialized. American policy makers had complained of continued restrictions imposed on the economy. At the same time, Indo-US relations were straining due to Gandhi's criticism of the American bombing campaign in Vietnam. While it was thought, at the time, and for decades after, that President Johnson's policy of withholding food grain shipments was to coerce Indian support for the war, in fact, it was to offer India rainmaking technology that he wanted to use as a counterweight to China's possession of the atomic bomb. In light of the circumstances, liberalization became politically suspect and was soon abandoned. Grain diplomacy and currency devaluation became matters of intense national pride in India. After the bitter experience with Johnson, Gandhi decided not to request food aid in the future. Moreover, Gandhi's government resolved never again to become "so vulnerably dependent" on aid, and painstakingly began building up substantial foreign exchange reserves. When food stocks slumped after poor harvests in 1972, the government made it a point to use foreign exchange to buy US wheat commercially rather than seek resumption of food aid.

The period of 1967–75 was characterized by socialist ascendency in India which culminated in 1976 with the official declaration of state socialism. Gandhi not only abandoned the short lived liberalization programme but also aggressively expanded the public sector with new licensing requirements and other restrictions for industry. She began a new course by launching the "Fourth Five-Year Plan" in 1969. The government targeted growth at 5.7% while stating as its goals, "growth with stability and progressive achievement of self-reliance." The rationale behind the overall plan was Gandhi's "Ten Point Programme" of 1967. This had been her first economic policy formulation, six months after coming to office. The programme emphasized greater state control of the economy with the understanding that government control assured greater welfare than private control. Related to this point were a set of policies which were meant to regulate the private sector. By the end of the 1960s, the reversal of the liberalization process was complete, and India's policies were characterised as "protectionist as ever."

To deal with India's food problems, Gandhi expanded the emphasis on production of inputs to agriculture that had already been initiated by her father, Jawaharlal Nehru. The Green Revolution in India subsequently culminated under her government in the 1970s and transformed the country from a nation heavily reliant on imported grains and prone to famine to being largely able to feed itself, and become successful in achieving its goal of food security. Gandhi had a personal motive in pursuing agricultural self-sufficiency, having found India's dependency on the U.S. for shipments of grains humiliating.

The economic period of 1967–75 became significant for its major wave of nationalisations amidst the increased regulation of the private sector.

Some of the other objectives of the economic plan for the period was to provide for the minimum needs of the community through a rural works program and the removal of the privy purses of the nobility. Both these, and many other goals of the 1967 program were accomplished by 1974–75. Nevertheless, the success of the overall economic plan was tempered by the fact that annual growth at 3.3–3.4% over 1969–74 fell short of the targeted figure.

The Fifth Five Year Plan (1974–79) was enacted in the backdrop of the state of emergency and the "Twenty Point Program" of 1975. The latter was the economic rationale of the emergency, a political act which has often been justified on economic grounds. In contrast to the reception of Gandhi's earlier economic plan, this one was criticized for being a "hastily thrown together wish list." Gandhi promised to reduce poverty by targeting the consumption levels of the poor and enact wide-ranging social and economic reforms. The government additionally targeted an annual growth of 4.4% over the period of the plan.

The measures of the emergency regime was able to halt the economic trouble of the early to mid-1970s, which had been marred by harvest failures, fiscal contraction, and the breakdown of the Bretton Woods system of fixed exchanged rate; the resulting turbulence in the foreign exchange markets being further accentuated by the oil shock of 1973. The government was even able to exceed the targeted growth figure with an annual growth rate of 5.0–5.2% over the five-year period of the plan (1974–79). The economy grew at the rate of 9% in 1975–76 alone, and the Fifth Plan, became the first plan during which the per capita income of the economy grew by over 5%.

Gandhi inherited a weak economy when she again became Prime Minister in 1980. The preceding year in 1979–80 under the Janata Party government had led to the strongest recession (−5.2%) in the history of modern India with inflation rampant at 18.2%. Gandhi proceeded to abrogate the Janata Party government's Five Year Plan in 1980 and launched the Sixth Five Year Plan (1980–85). The government targeted an average growth of 5.2% over the period of the plan. Measures to check the inflation were also taken; by the early 1980s inflation was under control at an annual rate of about 5%.

Although Gandhi continued professing socialist beliefs, the Sixth Five Year Plan was markedly different from the years of Garibi Hatao. Populist programs and policies were replaced by pragmatism. There was an emphasis on tightening public expenditures, greater efficiency of the state-owned enterprises (SOE), which Gandhi qualified as a "sad thing", and in stimulating the private sector through deregulation and liberation of the capital market. The government subsequently launched "Operation Forward" in 1982, the first cautious attempt at reform. The Sixth Plan went on to become the most successful of the Five Year plans yet; showing an average growth of 5.7% over 1980–85.

During Lal Bahadur Shastri's last full year in office (1965), inflation averaged 7.7%, compared to 5.2% at the end of Gandhi's first stint in office (1977). On average, inflation in India had remained below 7% through the 1950s and 1960s. But, it then accelerated sharply in the 1970s, from 5.5% in 1970–71 to over 20% by 1973–74, due to the international oil crisis. Gandhi declared inflation the gravest of problems in 1974 (at 25.2%) and devised a severe anti-inflation program. The government was successful in bringing down inflation during the emergency; achieving negative figures of −1.1% by the end of 1975–76.

Gandhi inherited a tattered economy in her second term; harvest failures and a second oil shock in the late 1970s had again caused inflation to rise. During Charan Singh's short reign in office in the second half of 1979, inflation averaged 18.2%, compared to 6.5% during Gandhi's last year in office (1984). General economic recovery under Gandhi led to an average inflation at 6.5% from 1981–82 to 1985–86; the lowest since the beginning of India's inflation problems in the 1960s.

Unemployment stayed constant at 9% over a nine-year period (1971–80) before declining to 8.3% in 1983.

Despite the provisions, control and regulations of Reserve Bank of India, most banks in India had continued to be owned and operated by private persons. Businessmen who owned the banks were often accused of channeling the deposits into their own companies, and ignoring the priority sector. Furthermore, there was a great resentment against "class" banking in India, which had left the poor (the majority population) unbanked. After becoming Prime Minister, Gandhi expressed the intention of nationalising the banks in a paper titled, ""Stray thoughts on Bank Nationalisation"" in order to alleviate poverty. The paper received the overwhelming support of the public. In 1969, Gandhi moved to nationalise fourteen major commercial banks. After the nationalisation of banks, the branches of the public sector banks in India rose to approximate 800 percent in deposits, and advances took a huge jump by 11,000 percent. Nationalisation also resulted in a significant growth in the geographical coverage of banks; the number of bank branches rose from 8,200 to over 62,000, most of which were opened in the unbanked, rural areas. The nationalization drive not only helped to increase household savings, but it also provided considerable investments in the informal sector, in small and medium-sized enterprises, and in agriculture, and contributed significantly to regional development and to the expansion of India's industrial and agricultural base. Jayaprakash Narayan, who became famous for leading the opposition to Gandhi in the 1970s, was solid in his praise for her bank nationalisations.

Having been re-elected in 1971 on a nationalisation platform, Gandhi proceeded to nationalise the coal, steel, copper, refining, cotton textiles, and insurance industries. Most of these nationalisations were made to protect employment and the interest of the organised labour. The remaining private sector industries were placed under strict regulatory control.

During the Indo-Pakistani War of 1971, foreign-owned private oil companies had refused to supply fuel to the Indian Navy and Indian Air Force. In response, Gandhi nationalised oil companies in 1973. After nationalisation the oil majors such as the Indian Oil Corporation (IOC), the Hindustan Petroleum Corporation (HPCL) and the Bharat Petroleum Corporation (BPCL) had to keep a minimum stock level of oil, to be supplied to the military when needed.

In 1966, Gandhi accepted the demands of the Akalis to reorganize Punjab on linguistic lines. The Hindi-speaking southern half of Punjab became a separate state, Haryana, while the Pahari speaking hilly areas in the north east were joined to Himachal Pradesh. In doing so, she had hoped to ward off the growing political conflict between Hindu and Sikh groups in the region. However, a contentious issue that was considered unresolved by the Akalis was the status of Chandigarh, a prosperous city on the Punjab-Haryana border, which Gandhi declared a union territory to be shared as a capital by both the states.

Victory over Pakistan in 1971 consolidated Indian power in Kashmir. Gandhi indicated that she would make no major concessions on Kashmir. The most prominent of the Kashmiri separatists, Sheikh Abdullah, had to recognize India's control over Kashmir in light of the new order in South Asia. The situation was normalized in the years following the war after Abdullah agreed to an accord with Gandhi, by giving up the demand for a plebiscite in return for a special autonomous status for Kashmir. In 1975, Gandhi declared the state of Jammu and Kashmir as a constituent unit of India. The Kashmir conflict remained largely peaceful if frozen under Gandhi's premiership.

In 1972, Gandhi granted statehood to Meghalaya, Manipur and Tripura, while the North-East Frontier Agency was declared a union territory and renamed Arunachal Pradesh. The transition to statehood for these territories was successfully overseen by her administration. This was followed by the annexation of Sikkim in 1975.

The principle of equal pay for equal work for both men and women was enshrined in the Indian Constitution under the Gandhi administration.

Gandhi questioned the continued existence of a privy purse for former rulers of princely states. She argued the case for abolition based on equal rights for all citizens and the need to reduce the government's revenue deficit. The nobility responded by rallying around the Jana Sangh and other right-wing parties that stood in opposition to Gandhi's attempts to abolish royal privileges. The motion to abolish privy purses, and the official recognition of the titles, was originally brought before the Parliament in 1970. It was passed in the Lok Sabha but felt short of the two-thirds majority in the Rajya Sabha by a single vote. Gandhi responded by having a Presidential proclamation issued; de-recognizing the princes; with this withdrawal of recognition, their claims to privy purses were also legally lost. However, the proclamation was struck down by the Supreme Court of India. In 1971, Gandhi again motioned to abolish the privy purse. This time, it was successfully passed as the 26th Amendment to the Constitution of India. Many royals tried to protest the abolition of the privy purse, primarily through campaigns to contest seats in elections. They, however, received a final setback when many of them were defeated by huge margins.

Gandhi claimed that only "clear vision, iron will and the strictest discipline" can remove poverty. She justified the imposition of the state of emergency in 1975 in the name of the socialist mission of the Congress. Armed with the power to rule by decree and without constitutional constraints, Gandhi embarked on a massive redistribution program. The provisions included rapid enforcement of land ceilings, housing for landless labourers, the abolition of bonded labour and a moratorium on the debts of the poor. North India was at the centre of the reforms; millions of acres of land were acquired and redistributed. The government was also successful in procuring houses for landless labourers; according to Frankel, three-fourths of the targeted four million houses was achieved in 1975 alone. Nevertheless, others have disputed the success of the program and criticized Gandhi for not doing enough to reform land ownership. The political economist, Jyotindra Das Gupta, cryptically questioned "...whether or not the real supporters of land-holders were in jail or in power?" Critics also accused Gandhi of choosing to "talk left and act right", referring to her concurrent pro-business decisions and endeavours. Rosser wrote that "some have even seen the declaration of emergency rule in 1975 as a move to suppress dissent against Gandhi's policy shift to the right." Regardless of the controversy over the nature of the reforms, the long-term effects of the social changes gave rise to prominence of middle-ranking farmers from intermediate and lower castes in North India. The rise of these newly empowered social classes challenged the political establishment of the Hindi Belt in the years to come.

Under the Constitution of India of 1950, Hindi was to have become the official national language by 1965. This was not acceptable to many non-Hindi speaking states, who wanted the continued use of English in government. In 1967, Gandhi made a constitutional amendment that guaranteed the de facto use of both Hindi and English as official languages. This established the official government policy of bilingualism in India and satisfied the non-Hindi speaking Indian states. Gandhi thus put herself forward as a leader with a pan-Indian vision. Nevertheless, critics alleged that her stance was actually meant to weaken the position of rival Congress leaders from the northern states such as Uttar Pradesh, where there had been strong, sometimes violent, pro-Hindi agitations. Gandhi came out of the language conflicts with the strong support of the south Indian populace.

In the late 1960s and 1970s, Gandhi had the Indian army crush militant Communist uprisings in the Indian state of West Bengal. The communist insurgency in India was completely suppressed during the state of emergency.

Gandhi considered the north-eastern regions important, because of its strategic situation. In 1966, the Mizo uprising took place against the government of India and overran almost the whole of the Mizoram region. Gandhi ordered the Indian Army to launch massive retaliatory strikes in response. The rebellion was suppressed with the Indian Air Force even carrying out airstrikes in Aizawl; this remains the only instance of India carrying out an airstrike in its own civilian territory. The defeat of Pakistan in 1971 and the secession of East Pakistan as pro-India Bangladesh led to the collapse of the Mizo separatist movement. In 1972, after the less extremist Mizo leaders came to the negotiating table, Gandhi upgraded Mizoram to the status of a union territory. A small-scale insurgency by some militants continued into the late 1970s but was successfully dealt with by the government. The Mizo conflict was definitively resolved during the administration of Gandhi's son Rajiv. Today, Mizoram is considered as one of the most peaceful states in the north-east.

Responding to the insurgency in Nagaland, Gandhi "unleashed a powerful military offensive" in the 1970s. Finally, a massive crackdown on the insurgents took place during the state of emergency ordered by Gandhi. The insurgents soon agreed to surrender and signed the Shillong Accord in 1975. While the agreement was considered a victory for the Indian government and ended large-scale conflicts, there has since been spurts of violence by rebel holdouts and ethnic conflict amongst the tribes.

Gandhi contributed and further carried out the vision of Jawaharlal Nehru, former Premier of India to develop the program. Gandhi authorised the development of nuclear weapons in 1967, in response to the "Test No. 6" by People's Republic of China. Gandhi saw this test as Chinese nuclear intimidation, therefore, Gandhi promoted the views of Nehru to establish India's stability and security interests as independent from those of the nuclear superpowers.

The program became fully mature in 1974, when Dr. Raja Ramanna reported to Gandhi that India had the ability to test its first nuclear weapon. Gandhi gave verbal authorisation of this test, and preparations were made in a long-constructed army base, the Indian Army Pokhran Test Range. In 1974, India successfully conducted an underground nuclear test, unofficially code named as ""Smiling Buddha"", near the desert village of Pokhran in Rajasthan. As the world was quiet by this test, a vehement protest came forward from Pakistan. Great ire was raised in Pakistan and its Prime Minister, Zulfikar Ali Bhutto, described this test as ""Indian hegemony"" to intimidate Pakistan. In response to this Bhutto launched a massive campaign all over the Pakistan to make Pakistan a nuclear power. In these campaigns Bhutto asked the nation to get united and great slogans were raised such as "hum ghaas aur pattay kha lay gay magar nuclear power ban k rhe gay (We will eat grass or leaves even go hungry but will get nuclear power)". Gandhi directed a letter to Bhutto and, later to the world, describing the test for peaceful purposes and India's commitment to develop its programme for industrial and scientific use.

A member of the Nehru-Gandhi family, she married Feroze Gandhi at the age of 25, in 1942. Their marriage lasted 18 years, until Feroze died of a heart attack in 1960. They had two sons – Rajiv (b. 1944) and Sanjay (b. 1946). Her younger son Sanjay had initially been her chosen heir; but after his death in a flying accident in June 1980, Gandhi persuaded her reluctant elder son Rajiv to quit his job as a pilot and enter politics in February 1981. Rajiv took office as prime minister following his mother's assassination in 1984; he served until December 1989. Rajiv Gandhi himself was assassinated by a suicide bomber working on behalf of LTTE on 21 May 1991.

Gandhi's yoga guru, Dhirendra Brahmachari, helped her in making certain decisions and also executed certain top level political tasks on her behalf, especially from 1975 to 1977 when Gandhi "declared a state of emergency and suspended civil liberties."

In January 2017, a woman called Priya Singh Paul claimed to be Indira's granddaughter as Sanjay Gandhi's biological daughter. She claims that she was given away for adoption because Indira Gandhi hid her as a child and her mother as a wife of Sanjay Gandhi for political reasons.

In 1952 in a letter to her American friend Dorothy Norman, Gandhi wrote: "I am in no sense a feminist, but I believe in women being able to do everything...Given the opportunity to develop, capable Indian women have come to the top at once." While this statement appears paradoxical, it reflects Gandhi's complex feelings toward her gender and feminism. Her egalitarian upbringing with her cousins helped contribute to her sense of natural equality. "Flying kites, climbing trees, playing marbles with her boy cousins, Indira said she hardly knew the difference between a boy and a girl until the age of twelve.",

Indira Gandhi did not often discuss her gender, but she did involve herself in women's issues before becoming the prime minister. Before her election as the Prime Minister, she became active in the organizational wing of the Congress party, working in part in the Women's Department. In 1956, Gandhi had an active role in setting up the Congress Party's Women's Section. Unsurprisingly, a lot of her involvement stemmed from her father. As an only child, Gandhi naturally stepped into the political light. And, as a woman, Gandhi naturally helped head the Women's section of the Congress Party. She often tried to organize women to involve themselves in politics. Although rhetorically Gandhi may have attempted to separate her political success from her gender, Gandhi did involve herself in women's organizations. The political parties in India paid substantial attention to Gandhi's gender before she became prime minister, hoping to use her for political gain. 
Even though men surrounded Gandhi during her upbringing, she still had a female role model as a child. Several books on Gandhi mention her interest in Joan of Arc. In her own accounts through her letters she wrote to her friend Dorothy Norman, in 1952 she wrote: "At about eight or nine I was taken to France; Jeanne d'Arc became a great heroine of mine. She was one of the first people I read about with enthusiasm." Another historian recounts Indira's comparison of herself to Joan of Arc: "Indira developed a fascination for Joan of Arc, telling her aunt, 'Someday I am going to lead my people to freedom just as Joan of Arc did! Gandhi's linking of herself to Joan of Arc presents a nice model for historians to assess Gandhi. As one writer said: "The Indian people were her children; members of her family were the only people capable of leading them."

Gandhi had been swept up in the call for Indian independence since she was born in 1917. Thus by 1947 she was already well immersed in politics, and by 1966, when she first assumed the position of prime minister, she had held several cabinet positions in her father's office.

Gandhi's advocacy for women's rights began with her help in establishing the Congress Party's Women's Section. In 1956, she wrote in a letter: "It is because of this that I am taking a much more active part in politics. I have to do a great deal of touring in order to set up the Congress Party Women's Section, and am on numerous important committees." Gandhi spent a great deal of time throughout the 1950s helping organize women. She wrote to Norman in 1959, irritable that women had organized around the communist cause but had not mobilized for the Indian cause: "The women, whom I have been trying to organize for years, had always refused to come into politics. Now they are out in the field." Once appointed president in 1959, she "travelled relentlessly, visiting remote parts of the country that had never before received a VIP...she talked to women, asked about child health and welfare, inquired after the crafts of the region" Gandhi's actions throughout her ascent to power clearly reflect a desire to mobilize women. Gandhi did not see the purpose of feminism. Gandhi saw her own success as a woman, and also noted that "Given the opportunity to develop, capable Indian women have come to the top at once."

Gandhi felt guilty about her inability to fully devote her time to her children. She noted that her main problem in office was how to balance her political duties with tending to her children, and "stressed that motherhood was the most important part of her life." At another point, she went into more detail: "To a woman, motherhood is the highest fulfilment…To bring a new being into this world, to see its perfection and to dream of its future greatness is the most moving of all experiences and fills one with wonder and exaltation."

Her domestic initiatives did not necessarily reflect favourably on Indian women. Gandhi did not make a special effort to appoint women to cabinet positions. She did not appoint any women to full cabinet rank during her terms in office. Yet despite this, many women saw Gandhi as a symbol for feminism and an image of women's power.

After leading India to victory against Pakistan in the Bangladesh liberation war in 1971, Prime Minister (Mrs. Indira Gandhi) recommended & President V. V. Giri awarded Mrs. Gandhi India's highest civilian honour, the Bharat Ratna. In 2011, the Bangladesh Freedom Honour (Bangladesh Swadhinata Sammanona), Bangladesh's highest civilian award, was posthumously conferred on Indira Gandhi for her "outstanding contributions" to Bangladesh's Liberation War.

Indira Gandhi's main legacy was standing firm in face of American pressure to defeat Pakistan and turn East Pakistan into independent Bangladesh. She was also responsible for India joining the club of countries with nuclear weapons. Despite India being officially part of the Non-Aligned Movement, she gave Indian foreign policy a tilt towards the Soviet bloc.

Being at the forefront of Indian politics for decades, Gandhi left a powerful but controversial legacy on Indian politics. The main legacy of her rule was destroying internal party democracy in the Congress party. Her detractors accuse her of weakening State chief ministers and thereby weakening the federal structure, weakening independence of judiciary, and weakening her cabinet by vesting power in her secretariat and her sons. Gandhi is also associated with fostering a culture of nepotism in Indian politics and in India's institutions. She is also almost singularly associated with the period of Emergency rule and the dark period in Indian Democracy that it entailed. Her actions in storming the Golden Temple alienated Sikhs for a very long time. She remains the only woman ever to occupy the office of the Prime Minister of India.






</doc>
<doc id="15180" url="https://en.wikipedia.org/wiki?curid=15180" title="Intergovernmentalism">
Intergovernmentalism

In political science, intergovernmentalism treats states, and national governments in particular, as the primary actors in the integration process. Intergovernmentalist approaches claim to be able to explain both periods of radical change in the European Union because of converging governmental preferences and periods of inertia because of diverging national interests. Intergovernmentalism is distinguishable from realism and neorealism because of its recognition of the significance of institutionalisation in international politics and the impact of domestic politics upon governmental preferences.

The best-known example of regional integration is the European Union (EU), an economic and political intergovernmental organisation of 28 member states, all in Europe. The EU operates through a system of supranational independent institutions and intergovernmental negotiated decisions by the member states. Institutions of the EU include the European Commission, the Council of the European Union, the European Council, the Court of Justice of the European Union, the European Central Bank, the Court of Auditors, and the European Parliament. The European Parliament is elected every five years by EU citizens. The EU's "de facto" capital is Brussels.

The EU has developed a single market through a standardised system of laws that apply in all member states. Within the Schengen Area (which includes 22 EU and 4 non-EU European states) passport controls have been abolished. EU policies favour the free movement of people, goods, services, and capital within its boundaries, enact legislation in justice and home affairs, and maintain common policies on trade, agriculture, fisheries and regional development.

A monetary union, the eurozone, was established in 1999 and is composed of 17 member states. Through the Common Foreign and Security Policy the EU has developed a role in external relations and defence. Permanent diplomatic missions have been established around the world. The EU is represented at the United Nations, the World Trade Organisation, the G8 and the G-20.

Intergovernmentalism represents a way for limiting the conferral of powers upon supranational institutions, halting the emergence of common policies. in the current institutional system of the EU, the European Council and the Council play the role of the institutions which have the last word about decisions and policies of the EU, institutionalizing a de facto intergovernmental control over the EU as a whole, with the possibility to give more power to a small group of states. This extreme consequence can create the condition of supremacy of someone over someone else violating the principle of a "Union of Equals".

The African Union (AU, or, in its other official languages, UA) is a continental intergovernmental union, similar but less integrated to the EU, consisting of 54 African states. The AU was established on 26 May 2001 in Addis Ababa, Ethiopia, and launched on 9 July 2002 in South Africa to replace the Organisation of African Unity (OAU). The most important decisions of the AU are made by the Assembly of the African Union, a semi-annual meeting of the heads of state and government of its member states. The AU's secretariat, the African Union Commission, is based in Addis Ababa, Ethiopia.



</doc>
<doc id="15181" url="https://en.wikipedia.org/wiki?curid=15181" title="Individualism">
Individualism

Individualism is the moral stance, political philosophy, ideology, or social outlook that emphasizes the moral worth of the individual. Individualists promote the exercise of one's goals and desires and so value independence and self-reliance and advocate that interests of the individual should achieve precedence over the state or a social group, while opposing external interference upon one's own interests by society or institutions such as the government. Individualism is often defined in contrast to totalitarianism, collectivism, authoritarianism, communitarianism, statism, cosmopolitanism, tribalism, altruism, and more corporate social forms.

Individualism makes the individual its focus and so starts "with the fundamental premise that the human individual is of primary importance in the struggle for liberation." Classical liberalism, existentialism, and anarchism are examples of movements that take the human individual as a central unit of analysis. Individualism thus involves "the right of the individual to freedom and self-realization".

It has also been used as a term denoting "The quality of being an individual; individuality" related to possessing "An individual characteristic; a quirk." Individualism is thus also associated with artistic and bohemian interests and lifestyles where there is a tendency towards self-creation and experimentation as opposed to tradition or popular mass opinions and behaviors, as with humanist philosophical positions and ethics.

In the English language, the word "individualism" was first introduced, as a pejorative, by the Owenites in the late 1830s, although it is unclear if they were influenced by Saint-Simonianism or came up with it independently. A more positive use of the term in Britain came to be used with the writings of James Elishama Smith, who was a millenarian and a Christian Israelite. Although an early Owenite socialist, he eventually rejected its collective idea of property, and found in individualism a "universalism" that allowed for the development of the "original genius." Without individualism, Smith argued, individuals cannot amass property to increase one's happiness. William Maccall, another Unitarian preacher, and probably an acquaintance of Smith, came somewhat later, although influenced by John Stuart Mill, Thomas Carlyle, and German Romanticism, to the same positive conclusions, in his 1847 work "Elements of Individualism".

An individual is a person or any specific object in a collection. In the 15th century and earlier, and also today within the fields of statistics and metaphysics, individual means "indivisible", typically describing any numerically singular thing, but sometimes meaning "a person." (q.v. "The problem of proper names"). From the 17th century on, individual indicates separateness, as in individualism. Individuality is the state or quality of being an individual; a person separate from other persons and possessing his or her own needs, goals, and desires.

Individualism holds that a person taking part in society attempts to learn and discover what his or her own interests are on a personal basis, without a presumed following of the interests of a societal structure (an individualist need not be an egoist). The individualist does not follow one particular philosophy, rather creates an amalgamation of elements of many, based on personal interests in particular aspects that he/she finds of use. On a societal level, the individualist participates on a personally structured political and moral ground. Independent thinking and opinion is a common trait of an individualist. Jean-Jacques Rousseau, claims that his concept of "general will" in the "social contract" is not the simple collection of individual wills and that it furthers the interests of the individual (the constraint of law itself would be beneficial for the individual, as the lack of respect for the law necessarily entails, in Rousseau's eyes, a form of ignorance and submission to one's passions instead of the preferred autonomy of reason).

Societies and groups can differ in the extent to which they are based upon predominantly "self-regarding" (individualistic, and/or self-interested) behaviors, rather than "other-regarding" (group-oriented, and group, or society-minded) behaviors. Ruth Benedict made a distinction, relevant in this context, between "guilt" societies (e.g., medieval Europe) with an "internal reference standard", and "shame" societies (e.g., Japan, "bringing shame upon one's ancestors") with an "external reference standard", where people look to their peers for feedback on whether an action is "acceptable" or not (also known as "group-think").

Individualism is often contrasted either with totalitarianism or with collectivism, but in fact, there is a spectrum of behaviors at the societal level ranging from highly individualistic societies through mixed societies to collectivist.

The principle of individuation
, or "", describes the manner in which a thing is identified as distinguished from other things. For Carl Jung, individuation is a process of transformation, whereby the personal and collective unconscious is brought into consciousness (by means of dreams, active imagination or free association to take some examples) to be assimilated into the whole personality. It is a completely natural process necessary for the integration of the psyche to take place. Jung considered individuation to be the central process of human development. In "L'individuation psychique et collective", Gilbert Simondon developed a theory of individual and collective individuation in which the individual subject is considered as an effect of individuation rather than a cause. Thus, the individual atom is replaced by a never-ending ontological process of individuation. Individuation is an always incomplete process, always leaving a "pre-individual" left-over, itself making possible future individuations. The philosophy of Bernard Stiegler draws upon and modifies the work of Gilbert Simondon on individuation and also upon similar ideas in Friedrich Nietzsche and Sigmund Freud. For Stiegler "the "I", as a psychic individual, can only be thought in relationship to "we", which is a collective individual. The "I" is constituted in adopting a collective tradition, which it inherits and in which a plurality of "I" 's acknowledge each other's existence."

Methodological individualism is the view that phenomena can only be understood by examining how they result from the motivations and actions of individual agents. In economics, people's behavior is explained in terms of rational choices, as constrained by prices and incomes. The economist accepts individuals' preferences as givens. Becker and Stigler provide a forceful statement of this view:

Individualists are chiefly concerned with protecting individual autonomy against obligations imposed by social institutions (such as the state or religious morality). For L. Susan Brown "Liberalism and anarchism are two political philosophies that are fundamentally concerned with individual freedom yet differ from one another in very distinct ways. Anarchism shares with liberalism a radical commitment to individual freedom while rejecting liberalism's competitive property relations."

Civil libertarianism is a strain of political thought that supports civil liberties, or which emphasizes the supremacy of individual rights and personal freedoms over and against any kind of authority (such as a state, a corporation, social norms imposed through peer pressure, etc.). Civil libertarianism is not a complete ideology; rather, it is a collection of views on the specific issues of civil liberties and civil rights. Because of this, a civil libertarian outlook is compatible with many other political philosophies, and civil libertarianism is found on both the right and left in modern politics. For scholar Ellen Meiksins Wood "there are doctrines of individualism that are opposed to Lockean individualism ... and non-lockean individualism may encompass socialism".

British Historians Emily Robinson, Camilla Schofield, Florence Sutcliffe-Braithwaite, and Natalie Thomlinson have argued that by the 1970s Britons were keen about defining and claiming their individual rights, identities and perspectives. They demanded greater personal autonomy and self-determination and less outside control. They angrily complained that the 'establishment' was withholding it. They argue this shift in concerns helped cause Thatcherism, and was incorporated into Thatcherism's appeal.

Liberalism (from the Latin "liberalis", "of freedom; worthy of a free man, gentlemanlike, courteous, generous") is the belief in the importance of individual freedom. This belief is widely accepted in the United States, Europe, Australia and other Western nations, and was recognized as an important value by many Western philosophers throughout history, in particular since the Enlightenment. It is often rejected by collectivist, Islamic, or confucian societies in Asia or the Middle East (though Taoists were and are known to be individualists). The Roman Emperor Marcus Aurelius wrote praising "the idea of a polity administered with regard to equal rights and equal freedom of speech, and the idea of a kingly government which respects most of all the freedom of the governed".

Liberalism should not be confused with modern liberalism in the United States, and should be referred to as classical liberalism to avoid ambiguity.

Liberalism has its roots in the Age of Enlightenment and rejects many foundational assumptions that dominated most earlier theories of government, such as the Divine Right of Kings, hereditary status, and established religion. John Locke is often credited with the philosophical foundations of classical liberalism. He wrote "no one ought to harm another in his life, health, liberty, or possessions."

In the 17th century, liberal ideas began to influence governments in Europe, in nations such as The Netherlands, Switzerland, England and Poland, but they were strongly opposed, often by armed might, by those who favored absolute monarchy and established religion. In the 18th century, in America, the first modern liberal state was founded, without a monarch or a hereditary aristocracy. The American "Declaration of Independence" includes the words (which echo Locke) "all men are created equal; that they are endowed by their Creator with certain unalienable rights; that among these are life, liberty, and the pursuit of happiness; that to insure these rights, governments are instituted among men, deriving their just powers from the consent of the governed."

Liberalism comes in many forms. According to John N. Gray, the essence of liberalism is toleration of different beliefs and of different ideas as to what constitutes a good life.

Anarchism is a set of political philosophies that hold the state to be undesirable, unnecessary, or harmful, and often advocate stateless societies. While anti-statism is central, some argue that anarchism entails opposing authority or hierarchical organisation in the conduct of human relations, including, but not limited to, the state system.

For influential Italian anarchist Errico Malatesta "All anarchists, whatever tendency they belong to, are individualists in some way or other. But the opposite is not true; not by any means. The individualists are thus divided into two distinct categories: one which claims the right to full development for all human individuality, their own and that of others; the other which only thinks about its own individuality and has absolutely no hesitation in sacrificing the individuality of others. The Tsar of all the Russias belongs to the latter category of individualists. We belong to the former."

Individualist anarchism refers to several traditions of thought within the anarchist movement that emphasize the individual and their will over any kinds of external determinants such as groups, society, traditions, and ideological systems. Individualist anarchism is not a single philosophy but refers to a group of individualistic philosophies that sometimes are in conflict.

In 1793, William Godwin, who has often been cited as the first anarchist, wrote "Political Justice", which some consider to be the first expression of anarchism.
Godwin, a philosophical anarchist, from a rationalist and utilitarian basis opposed revolutionary action and saw a minimal state as a present "necessary evil" that would become increasingly irrelevant and powerless by the gradual spread of knowledge.
Godwin advocated individualism, proposing that all cooperation in labour be eliminated on the premise that this would be most conducive with the general good.

An influential form of individualist anarchism, called "egoism," or egoist anarchism, was expounded by one of the earliest and best-known proponents of individualist anarchism, the German Max Stirner. Stirner's "The Ego and Its Own", published in 1844, is a founding text of the philosophy. According to Stirner, the only limitation on the rights of the individual is their power to obtain what they desire, without regard for God, state, or morality. To Stirner, rights were "spooks" in the mind, and he held that society does not exist but "the individuals are its reality". Stirner advocated self-assertion and foresaw unions of egoists, non-systematic associations continually renewed by all parties' support through an act of will, which Stirner proposed as a form of organization in place of the state. Egoist anarchists claim that egoism will foster genuine and spontaneous union between individuals. "Egoism" has inspired many interpretations of Stirner's philosophy. It was re-discovered and promoted by German philosophical anarchist and LGBT activist John Henry Mackay.

Josiah Warren is widely regarded as the first American anarchist, and the four-page weekly paper he edited during 1833, "The Peaceful Revolutionist", was the first anarchist periodical published. For American anarchist historian Eunice Minette Schuster "It is apparent...that Proudhonian Anarchism was to be found in the United States at least as early as 1848 and that it was not conscious of its affinity to the Individualist Anarchism of Josiah Warren and Stephen Pearl Andrews...William B. Greene presented this Proudhonian Mutualism in its purest and most systematic form.". Henry David Thoreau (1817–1862) was an important early influence in individualist anarchist thought in the United States and Europe. Thoreau was an American author, poet, naturalist, tax resister, development critic, surveyor, historian, philosopher, and leading transcendentalist. He is best known for his books "Walden", a reflection upon simple living in natural surroundings, and his essay, "Civil Disobedience", an argument for individual resistance to civil government in moral opposition to an unjust state. Later Benjamin Tucker fused Stirner's egoism with the economics of Warren and Proudhon in his eclectic influential publication "Liberty".

From these early influences individualist anarchism in different countries attracted a small but diverse following of bohemian artists and intellectuals, free love and birth control advocates (see Anarchism and issues related to love and sex), individualist naturists nudists (see anarcho-naturism), freethought and anti-clerical activists as well as young anarchist outlaws in what came to be known as illegalism and individual reclamation (see European individualist anarchism and individualist anarchism in France). These authors and activists included Oscar Wilde, Emile Armand, Han Ryner, Henri Zisly, Renzo Novatore, Miguel Gimenez Igualada, Adolf Brand and Lev Chernyi among others. In his important essay "The Soul of Man under Socialism" from 1891 Oscar Wilde defended socialism as the way to guarantee individualism and so he saw that "With the abolition of private property, then, we shall have true, beautiful, healthy Individualism. Nobody will waste his life in accumulating things, and the symbols for things. One will live. To live is the rarest thing in the world. Most people exist, that is all." For anarchist historian George Woodcock "Wilde's aim in "The Soul of Man under Socialism" is to seek the society most favorable to the artist ... for Wilde art is the supreme end, containing within itself enlightenment and regeneration, to which all else in society must be subordinated ... Wilde represents the anarchist as aesthete." Woodcock finds that "The most ambitious contribution to literary anarchism during the 1890s was undoubtedly Oscar Wilde "The Soul of Man under Socialism"" and finds that it is influenced mainly by the thought of William Godwin.

Ethical egoism (also called simply egoism) is the normative ethical position that moral agents ought to do what is in their own self-interest. It differs from psychological egoism, which claims that people "do" only act in their self-interest. Ethical egoism also differs from rational egoism, which holds merely that it is rational to act in one's self-interest. However, these doctrines may occasionally be combined with ethical egoism.

Ethical egoism contrasts with ethical altruism, which holds that moral agents have an obligation to help and serve others. Egoism and altruism both contrast with ethical utilitarianism, which holds that a moral agent should treat one's self (also known as the subject) with no higher regard than one has for others (as egoism does, by elevating self-interests and "the self" to a status not granted to others), but that one also should not (as altruism does) sacrifice one's own interests to help others' interests, so long as one's own interests (i.e. one's own desires or well-being) are substantially-equivalent to the others' interests and well-being. Egoism, utilitarianism, and altruism are all forms of consequentialism, but egoism and altruism contrast with utilitarianism, in that egoism and altruism are both agent-focused forms of consequentialism (i.e. subject-focused or subjective), but utilitarianism is called agent-neutral (i.e. objective and impartial) as it does not treat the subject's (i.e. the self's, i.e. the moral "agent's") own interests as being more or less important than if the same interests, desires, or well-being were anyone else's.

Ethical egoism does not, however, require moral agents to harm the interests and well-being of others when making moral deliberation; e.g. what is in an agent's self-interest may be incidentally detrimental, beneficial, or neutral in its effect on others. Individualism allows for others' interest and well-being to be disregarded or not, as long as what is chosen is efficacious in satisfying the self-interest of the agent. Nor does ethical egoism necessarily entail that, in pursuing self-interest, one ought always to do what one wants to do; e.g. in the long term, the fulfilment of short-term desires may prove detrimental to the self. Fleeting pleasance, then, takes a back seat to protracted eudaemonia. In the words of James Rachels, "Ethical egoism [...] endorses selfishness, but it doesn't endorse foolishness."
Ethical egoism is sometimes the philosophical basis for support of libertarianism or individualist anarchism as in Max Stirner, although these can also be based on altruistic motivations. These are political positions based partly on a belief that individuals should not coercively prevent others from exercising freedom of action.

Egoist anarchism is a school of anarchist thought that originated in the philosophy of Max Stirner, a nineteenth-century Hegelian philosopher whose "name appears with familiar regularity in historically orientated surveys of anarchist thought as one of the earliest and best-known exponents of individualist anarchism." According to Stirner, the only limitation on the rights of the individual is their power to obtain what they desire, without regard for God, state, or morality. Stirner advocated self-assertion and foresaw unions of egoists, non-systematic associations continually renewed by all parties' support through an act of will, which Stirner proposed as a form of organisation in place of the state. Egoist anarchists argue that egoism will foster genuine and spontaneous union between individuals. "Egoism" has inspired many interpretations of Stirner's philosophy but within anarchism it has also gone beyond Stirner. It was re-discovered and promoted by German philosophical anarchist and LGBT activist John Henry Mackay. John Beverley Robinson wrote an essay called "Egoism" in which he states that "Modern egoism, as propounded by Stirner and Nietzsche, and expounded by Ibsen, Shaw and others, is all these; but it is more. It is the realization by the individual that they are an individual; that, as far as they are concerned, they are the only individual." Nietzsche (see Anarchism and Friedrich Nietzsche) and Stirner were frequently compared by French "literary anarchists" and anarchist interpretations of Nietzschean ideas appear to have also been influential in the United States. Anarchists who adhered to egoism include Benjamin Tucker, Émile Armand, John Beverley Robinson, Adolf Brand, Steven T. Byington, Renzo Novatore, James L. Walker, Enrico Arrigoni, Biofilo Panclasta, Jun Tsuji, André Arru and contemporary ones such as Hakim Bey, Bob Black and Wolfi Landstreicher.

Existentialism is a term applied to the work of a number of 19th- and 20th-century philosophers who, despite profound doctrinal differences, generally held that the focus of philosophical thought should be to deal with the conditions of existence of the individual person and his or her emotions, actions, responsibilities, and thoughts. The early 19th century philosopher Søren Kierkegaard, posthumously regarded as the father of existentialism, maintained that the individual solely has the responsibilities of giving one's own life meaning and living that life passionately and sincerely, in spite of many existential obstacles and distractions including despair, angst, absurdity, alienation, and boredom.

Subsequent existential philosophers retain the emphasis on the individual, but differ, in varying degrees, on how one achieves and what constitutes a fulfilling life, what obstacles must be overcome, and what external and internal factors are involved, including the potential consequences of the existence or non-existence of God. Many existentialists have also regarded traditional systematic or academic philosophy, in both style and content, as too abstract and remote from concrete human experience. Existentialism became fashionable after World War II as a way to reassert the importance of human individuality and freedom.

Freethought holds that individuals should not accept ideas proposed as truth without recourse to knowledge and reason. Thus, freethinkers strive to build their opinions on the basis of facts, scientific inquiry, and logical principles, independent of any logical fallacies or intellectually limiting effects of authority, confirmation bias, cognitive bias, conventional wisdom, popular culture, prejudice, sectarianism, tradition, urban legend, and all other dogmas. Regarding religion, freethinkers hold that there is insufficient evidence to scientifically validate the existence of supernatural phenomena.

Humanism is a perspective common to a wide range of ethical stances that attaches importance to human dignity, concerns, and capabilities, particularly rationality. Although the word has many senses, its meaning comes into focus when contrasted to the supernatural or to appeals to authority. Since the 19th century, humanism has been associated with an anti-clericalism inherited from the 18th-century Enlightenment "philosophes". 21st century Humanism tends to strongly endorse human rights, including reproductive rights, gender equality, social justice, and the separation of church and state. The term covers organized non-theistic religions, secular humanism, and a humanistic life stance.

Philosophical hedonism is a meta-ethical theory of value which argues that pleasure is the only intrinsic good and pain is the only intrinsic bad. The basic idea behind hedonistic thought is that pleasure (an umbrella term for all inherently likable emotions) is the only thing that is good in and of itself or by its very nature. This implies evaluating the moral worth of character or behavior according to the extent that the pleasure it produces exceeds the pain it entails. 

A libertine is one devoid of most moral restraints, which are seen as unnecessary or undesirable, especially one who ignores or even spurns accepted morals and forms of behaviour sanctified by the larger society. Libertines place value on physical pleasures, meaning those experienced through the senses. As a philosophy, libertinism gained new-found adherents in the 17th, 18th, and 19th centuries, particularly in France and Great Britain. Notable among these were John Wilmot, 2nd Earl of Rochester, and the Marquis de Sade. During the Baroque era in France, there existed a freethinking circle of philosophers and intellectuals who were collectively known as "libertinage érudit" and which included Gabriel Naudé, Élie Diodati and François de La Mothe Le Vayer. The critic Vivian de Sola Pinto linked John Wilmot, 2nd Earl of Rochester's libertinism to Hobbesian materialism.

Objectivism is a system of philosophy created by philosopher and novelist Ayn Rand (1905–1982) that holds: reality exists independent of consciousness; human beings gain knowledge rationally from perception through the process of concept formation and inductive and deductive logic; the moral purpose of one's life is the pursuit of one's own happiness or rational self-interest. Rand thinks the only social system consistent with this morality is full respect for individual rights, embodied in pure "laissez faire" capitalism; and the role of art in human life is to transform man's widest metaphysical ideas, by selective reproduction of reality, into a physical form—a work of art—that he can comprehend and to which he can respond emotionally. Objectivism celebrates man as his own hero, "with his own happiness as the moral purpose of his life, with productive achievement as his noblest activity, and reason as his only absolute."

Philosophical anarchism is an anarchist school of thought which contends that the State lacks moral legitimacy and – in contrast to revolutionary anarchism – does not advocate violent revolution to eliminate it but advocate peaceful evolution to superate it. Though philosophical anarchism does not necessarily imply any action or desire for the elimination of the State, philosophical anarchists do not believe that they have an obligation or duty to obey the State, or conversely, that the State has a right to command.

Philosophical anarchism is a component especially of individualist anarchism. Philosophical anarchists of historical note include Mohandas Gandhi, William Godwin, Pierre-Joseph Proudhon, Max Stirner, Benjamin Tucker, and Henry David Thoreau. Contemporary philosophical anarchists include A. John Simmons and Robert Paul Wolff.

Subjectivism is a philosophical tenet that accords primacy to subjective experience as fundamental of all measure and law. In extreme forms like Solipsism, it may hold that the nature and existence of every object depends solely on someone's subjective awareness of it. For example, Wittgenstein wrote in Tractatus Logico-Philosophicus: "The subject doesn't belong to the world, but it is a limit of the world" (proposition 5.632). Metaphysical subjectivism is the theory that reality is what we perceive to be real, and that there is no underlying true reality that exists independently of perception. One can also hold that it is consciousness rather than perception that is reality (subjective idealism). In probability, a subjectivism stands for the belief that probabilities are simply degrees-of-belief by rational agents in a certain proposition, and which have no objective reality in and of themselves.

Ethical subjectivism stands in opposition to moral realism, which claims that moral propositions refer to objective facts, independent of human opinion; to error theory, which denies that any moral propositions are true in any sense; and to non-cognitivism, which denies that moral sentences express propositions at all. The most common forms of ethical subjectivism are also forms of moral relativism, with moral standards held to be relative to each culture or society (c.f. cultural relativism), or even to every individual. The latter view, as put forward by Protagoras, holds that there are as many distinct scales of good and evil as there are subjects in the world. Moral subjectivism is that species of moral relativism that relativizes moral value to the individual subject.

Horst Matthai Quelle was a Spanish language German anarchist philosopher influenced by Max Stirner. He argued that since the individual gives form to the world, he is those objects, the others and the whole universe. One of his main views was a "theory of infinite worlds" which for him was developed by pre-socratic philosophers.

Solipsism is the philosophical idea that only one's own mind is sure to exist. The term comes from Latin "solus" (alone) and "ipse" (self). Solipsism as an epistemological position holds that knowledge of anything outside one's own mind is unsure. The external world and other minds cannot be known, and might not exist outside the mind. As a metaphysical position, solipsism goes further to the conclusion that the world and other minds do not exist. As such it is the only epistemological position that, by its own postulate, is both irrefutable and yet indefensible in the same manner. Although the number of individuals sincerely espousing solipsism has been small, it is not uncommon for one philosopher to accuse another's arguments of entailing solipsism as an unwanted consequence, in a kind of reductio ad absurdum. In the history of philosophy, solipsism has served as a skeptical hypothesis.

The doctrine of economic individualism holds that each individual should be allowed autonomy in making his or her own economic decisions as opposed to those decisions being made by the state, the community, the corporation etc. for him or her.

Classical liberalism is a political ideology that developed in the 19th century in England, Western Europe, and the Americas. It followed earlier forms of liberalism in its commitment to personal freedom and popular government, but differed from earlier forms of liberalism in its commitment to free markets and classical economics. Notable classical liberals in the 19th century include Jean-Baptiste Say, Thomas Malthus, and David Ricardo. Classical liberalism was revived in the 20th century by Ludwig von Mises and Friedrich Hayek, and further developed by Milton Friedman, Robert Nozick, Loren Lomasky, and Jan Narveson. The phrase "classical liberalism" is also sometimes used to refer to all forms of liberalism before the 20th century.

In regards to economic questions within individualist anarchism there are adherents to mutualism (Pierre Joseph Proudhon, Emile Armand, early Benjamin Tucker); natural rights positions (early Benjamin Tucker, Lysander Spooner, Josiah Warren); and egoistic disrespect for "ghosts" such as private property and markets (Max Stirner, John Henry Mackay, Lev Chernyi, later Benjamin Tucker, Renzo Novatore, illegalism). Contemporary individualist anarchist Kevin Carson characterizes American individualist anarchism saying that "Unlike the rest of the socialist movement, the individualist anarchists believed that the natural wage of labor in a free market was its product, and that economic exploitation could only take place when capitalists and landlords harnessed the power of the state in their interests. Thus, individualist anarchism was an alternative both to the increasing statism of the mainstream socialist movement, and to a classical liberal movement that was moving toward a mere apologetic for the power of big business." 

Mutualism is an anarchist school of thought which can be traced to the writings of Pierre-Joseph Proudhon, who envisioned a society where each person might possess a means of production, either individually or collectively, with trade representing equivalent amounts of labor in the free market. Integral to the scheme was the establishment of a mutual-credit bank which would lend to producers at a minimal interest rate only high enough to cover the costs of administration. Mutualism is based on a labor theory of value which holds that when labor or its product is sold, in exchange, it ought to receive goods or services embodying "the amount of labor necessary to produce an article of exactly similar and equal utility". Receiving anything less would be considered exploitation, theft of labor, or usury.

Libertarian socialism (sometimes dubbed socialist libertarianism, or left-libertarianism) is a group of anti-authoritarian political philosophies inside the socialist movement that rejects socialism as centralized state ownership and control of the economy, as well as the state itself. It criticizes wage labour relationships within the workplace. Instead, it emphasizes workers' self-management of the workplace and decentralized structures of political organization. It asserts that a society based on freedom and justice can be achieved through abolishing authoritarian institutions that control certain means of production and subordinate the majority to an owning class or political and economic elite. Libertarian socialists advocate for decentralized structures based on direct democracy and federal or confederal associations such as libertarian municipalism, citizens' assemblies, trade unions, and workers' councils. All of this is generally done within a general call for libertarian and voluntary human relationships through the identification, criticism, and practical dismantling of illegitimate authority in all aspects of human life. As such libertarian socialism, within the larger socialist movement, seeks to distinguish itself both from Leninism/Bolshevism and from social democracy.

Past and present political philosophies and movements commonly described as libertarian socialist include anarchism (especially anarchist communism, anarchist collectivism, anarcho-syndicalism, and mutualism) as well as autonomism, communalism, participism, guild socialism, revolutionary syndicalism, and libertarian Marxist philosophies such as council communism and Luxemburgism; as well as some versions of "utopian socialism" and individualist anarchism.

Left-libertarianism (or left-wing libertarianism) names several related but distinct approaches to politics, society, culture, and political and social theory, which stress both individual freedom and social justice. Unlike right-libertarians, they believe that neither claiming nor mixing one's labor with natural resources is enough to generate full private property rights, and maintain that natural resources (land, oil, gold, trees) ought to be held in some egalitarian manner, either unowned or owned collectively. Those left-libertarians who support private property do so under the condition that recompense is offered to the local community.

Left-libertarianism can refer generally to these related and overlapping schools of thought:

Right-libertarianism or right libertarianism is a phrase used by some to describe either non-collectivist forms of libertarianism or a variety of different libertarian views some label "right" of mainstream libertarianism including "libertarian conservatism".

Stanford Encyclopedia of Philosophy calls it "right libertarianism" but states: "Libertarianism is often thought of as 'right-wing' doctrine. This, however, is mistaken for at least two reasons. First, on social—rather than economic—issues, libertarianism tends to be 'left-wing'. It opposes laws that restrict consensual and private sexual relationships between adults (e.g., gay sex, non-marital sex, and deviant sex), laws that restrict drug use, laws that impose religious views or practices on individuals, and compulsory military service. Second, in addition to the better-known version of libertarianism—right-libertarianism—there is also a version known as 'left-libertarianism'. Both endorse full self-ownership, but they differ with respect to the powers agents have to appropriate unappropriated natural resources (land, air, water, etc.)."

The anarchist writer and bohemian Oscar Wilde wrote in his famous essay "The Soul of Man under Socialism" that "Art is individualism, and individualism is a disturbing and disintegrating force. There lies its immense value. For what it seeks is to disturb monotony of type, slavery of custom, tyranny of habit, and the reduction of man to the level of a machine." For anarchist historian George Woodcock "Wilde's aim in "The Soul of Man under Socialism" is to seek the society most favorable to the artist...for Wilde art is the supreme end, containing within itself enlightenment and regeneration, to which all else in society must be subordinated...Wilde represents the anarchist as aesthete." The word individualism in this way has been used to denote a personality with a strong tendency towards self-creation and experimentation as opposed to tradition or popular mass opinions and behaviors.

Anarchist writer Murray Bookchin describes a lot of individualist anarchists as people who "expressed their opposition in uniquely personal forms, especially in fiery tracts, outrageous behavior, and aberrant lifestyles in the cultural ghettos of fin de siècle New York, Paris, and London. As a credo, individualist anarchism remained largely a bohemian lifestyle, most conspicuous in its demands for sexual freedom ('free love') and enamored of innovations in art, behavior, and clothing."

In relation to this view of individuality, French Individualist anarchist Emile Armand advocates egoistical denial of social conventions and dogmas to live in accord to one's own ways and desires in daily life since he emphasized anarchism as a way of life and practice. In this way he opines "So the anarchist individualist tends to reproduce himself, to perpetuate his spirit in other individuals who will share his views and who will make it possible for a state of affairs to be established from which authoritarianism has been banished. It is this desire, this will, not only to live, but also to reproduce oneself, which we shall call "activity".

In the book "Imperfect garden : the legacy of humanism", humanist philosopher Tzvetan Todorov identifies individualism as an important current of socio-political thought within modernity and as examples of it he mentions Michel de Montaigne, François de La Rochefoucauld, Marquis de Sade, and Charles Baudelaire. In La Rochefoucauld, he identifies a tendency similar to stoicism in which "the honest person works his being in the manner of a sculptor who searches the liberation of the forms which are inside a block of marble, to extract the truth of that matter." In Baudelaire he finds the dandy trait in which one searches to cultivate "the idea of beauty within oneself, of satisfying one's passions of feeling and thinking."

The Russian-American poet Joseph Brodsky once wrote that "The surest defense against Evil is extreme individualism, originality of thinking, whimsicality, even—if you will—eccentricity. That is, something that can't be feigned, faked, imitated; something even a seasoned imposter couldn't be happy with." Ralph Waldo Emerson famously declared, "Whoso would be a man must be a nonconformist"—a point of view developed at length in both the life and work of Henry David Thoreau. Equally memorable and influential on Walt Whitman is Emerson's idea that "a foolish consistency is the hobgoblin of small minds, adored by little statesmen and philosophers and divines." Emerson opposes on principle the reliance on civil and religious social structures precisely because through them the individual approaches the divine second-hand, mediated by the once original experience of a genius from another age. "An institution," he explains, "is the lengthened shadow of one man." To achieve this original relation one must "Insist on one's self; never imitate" for if the relationship is secondary the connection is lost.




</doc>
<doc id="15187" url="https://en.wikipedia.org/wiki?curid=15187" title="In vivo">
In vivo

Studies that are in vivo (Latin for "within the living"; often not italicized in English) are those in which the effects of various biological entities are tested on whole, living organisms or cells, usually animals, including humans, and plants, as opposed to a tissue extract or dead organism. This is not to be confused with experiments done "in vitro" ("within the glass"), i.e., in a laboratory environment using test tubes, petri dishes, etc. Examples of investigations "in vivo" include: the pathogenesis of disease by comparing the effects of bacterial infection with the effects of purified bacterial toxins; the development of antibiotics, antiviral drugs, and new drugs generally; and new surgical procedures. Consequently, animal testing and clinical trials are major elements of "in vivo" research. "In vivo" testing is often employed over "in vitro" because it is better suited for observing the overall effects of an experiment on a living subject. In drug discovery, for example, verification of efficacy "in vivo" is crucial, because "in vitro" assays can sometimes yield misleading results with drug candidate molecules that are irrelevant "in vivo" (e.g., because such molecules cannot reach their site of "in vivo" action, for example as a result of rapid catabolism in the liver).

The English microbiologist Professor Harry Smith and his colleagues in the mid-1950s showed the importance of "in vivo" studies. They found that sterile filtrates of serum from animals infected with "Bacillus anthracis" were lethal for other animals, whereas extracts of culture fluid from the same organism grown "in vitro" were not. This discovery of anthrax toxin through the use of "in vivo" experiments had a major impact on studies of the pathogenesis of infectious disease.

The maxim "in vivo veritas" ("in a living thing [there is] truth") is used to describe this type of testing and is a play on "in vino veritas", ("in wine [there is] truth"), a well-known proverb.

In microbiology, "in vivo" is often used to refer to experimentation done in live isolated cells rather than in a whole organism, for example, cultured cells derived from biopsies. In this situation, the more specific term is "ex vivo". Once cells are disrupted and individual parts are tested or analyzed, this is known as "in vitro".

According to Christopher Lipinski and Andrew Hopkins, "Whether the aim is to discover drugs or to gain knowledge of biological systems, the nature and properties of a chemical tool cannot be considered independently of the system it is to be tested in. Compounds that bind to isolated recombinant proteins are one thing; chemical tools that can perturb cell function another; and pharmacological agents that can be tolerated by a live organism and perturb its systems are yet another. If it were simple to ascertain the properties required to develop a lead discovered "in vitro" to one that is active "in vivo", drug discovery would be as reliable as drug manufacturing."



</doc>
<doc id="15188" url="https://en.wikipedia.org/wiki?curid=15188" title="In vitro">
In vitro

In vitro (meaning: "in the glass") studies are performed with microorganisms, cells, or biological molecules outside their normal biological context. Colloquially called "test-tube experiments", these studies in biology and its subdisciplines are traditionally done in labware such as test tubes, flasks, Petri dishes, and microtiter plates. Studies conducted using components of an organism that have been isolated from their usual biological surroundings permit a more detailed or more convenient analysis than can be done with whole organisms; however, results obtained from "in vitro" experiments may not fully or accurately predict the effects on a whole organism. In contrast to "in vitro" experiments, "in vivo" studies are those conducted in animals, including humans, and whole plants.

"In vitro" (; often not italicized in English) studies are conducted using components of an organism that have been isolated from their usual biological surroundings, such as microorganisms, cells, or biological molecules. For example, microorganisms or cells can be studied in artificial culture media, and proteins can be examined in solutions. Colloquially called "test-tube experiments", these studies in biology, medicine, and their subdisciplines are traditionally done in test tubes, flasks, Petri dishes, etc. They now involve the full range of techniques used in molecular biology, such as the omics. 
In contrast, studies conducted in living beings (microorganisms, animals, humans, or whole plants) are called "in vivo" .

Examples of "in vitro" studies include: the isolation, growth and identification of cells derived from multicellular organisms in (cell or tissue culture); subcellular components (e.g. mitochondria or ribosomes); cellular or subcellular extracts (e.g. wheat germ or reticulocyte extracts); purified molecules such as proteins, DNA, or RNA); and the commercial production of antibiotics and other pharmaceutical products. Viruses, which only replicate in living cells, are studied in the laboratory in cell or tissue culture, and many animal virologists refer to such work as being "in vitro" to distinguish it from "in vivo" work in whole animals.


"In vitro" studies permit a species-specific, simpler, more convenient, and more detailed analysis than can be done with the whole organism. Just as studies in whole animals more and more replace human trials, so are "in vitro" studies replacing studies in whole animals.

Living organisms are extremely complex functional systems that are made up of, at a minimum, many tens of thousands of genes, protein molecules, RNA molecules, small organic compounds, inorganic ions, and complexes in an environment that is spatially organized by membranes, and in the case of multicellular organisms, organ systems. These myriad components interact with each other and with their environment in a way that processes food, removes waste, moves components to the correct location, and is responsive to signalling molecules, other organisms, light, sound, heat, taste, touch, and balance.

This complexity makes it difficult to identify the interactions between individual components and to explore their basic biological functions. "In vitro" work simplifies the system under study, so the investigator can focus on a small number of components.

For example, the identity of proteins of the immune system (e.g. antibodies), and the mechanism by which they recognize and bind to foreign antigens would remain very obscure if not for the extensive use of "in vitro" work to isolate the proteins, identify the cells and genes that produce them, study the physical properties of their interaction with antigens, and identify how those interactions lead to cellular signals that activate other components of the immune system.

Another advantage of "in vitro" methods is that human cells can be studied without "extrapolation" from an experimental animal's cellular response.

"In vitro" methods can be miniaturized and automated, yielding high-throughput screening methods for testing molecules in pharmacology or toxicology 

The primary disadvantage of "in vitro" experimental studies is that it may be challenging to extrapolate from the results of "in vitro" work back to the biology of the intact organism. Investigators doing "in vitro" work must be careful to avoid over-interpretation of their results, which can lead to erroneous conclusions about organismal and systems biology.

For example, scientists developing a new viral drug to treat an infection with a pathogenic virus (e.g. HIV-1) may find that a candidate drug functions to prevent viral replication in an "in vitro" setting (typically cell culture). However, before this drug is used in the clinic, it must progress through a series of "in vivo" trials to determine if it is safe and effective in intact organisms (typically small animals, primates, and humans in succession). Typically, most candidate drugs that are effective "in vitro" prove to be ineffective "in vivo" because of issues associated with delivery of the drug to the affected tissues, toxicity towards essential parts of the organism that were not represented in the initial "in vitro" studies, or other issues.

Results obtained from "in vitro" experiments cannot usually be transposed, as is, to predict the reaction of an entire organism "in vivo". Building a consistent and reliable extrapolation procedure from "in vitro" results to "in vivo" is therefore extremely important. Solutions include:


These two approaches are not incompatible; better "in vitro" systems provide better data to mathematical models. However, increasingly sophisticated "in vitro" experiments collect increasingly numerous, complex, and challenging data to integrate. Mathematical models, such as systems biology models, are much needed here.

In pharmacology, IVIVE can be used to approximate pharmacokinetics (PK) or pharmacodynamics (PD).
Since the timing and intensity of effects on a given target depend on the concentration time course of candidate drug (parent molecule or metabolites) at that target site, "in vivo" tissue and organ sensitivities can be completely different or even inverse of those observed on cells cultured and exposed "in vitro". That indicates that extrapolating effects observed "in vitro" needs a quantitative model of "in vivo" PK. Physiologically based PK (PBPK) models are generally accepted to be central to the extrapolations.

In the case of early effects or those without intercellular communications, the same cellular exposure concentration is assumed to cause the same effects, both qualitatively and quantitatively, "in vitro" and "in vivo". In these conditions, developing a simple PD model of the dose–response relationship observed "in vitro", and transposing it without changes to predict "in vivo" effects is not enough.



</doc>
<doc id="15189" url="https://en.wikipedia.org/wiki?curid=15189" title="IEEE 754-1985">
IEEE 754-1985

IEEE 754-1985 was an industry standard for representing floating-point numbers in computers, officially adopted in 1985 and superseded in 2008 by IEEE 754-2008. During its 23 years, it was the most widely used format for floating-point computation. It was implemented in software, in the form of floating-point libraries, and in hardware, in the instructions of many CPUs and FPUs. The first integrated circuit to implement the draft of what was to become IEEE 754-1985 was the Intel 8087.

IEEE 754-1985 represents numbers in binary, providing definitions for four levels of precision, of which the two most commonly used are:

The standard also defines representations for positive and negative infinity, a "negative zero", five exceptions to handle invalid results like division by zero, special values called NaNs for representing those exceptions, denormal numbers to represent numbers smaller than shown above, and four rounding modes.

Floating-point numbers in IEEE 754 format consist of three fields: a sign bit, a biased exponent, and a fraction. The following example illustrates the meaning of each.

The decimal number 0.15625 represented in binary is 0.00101 (that is, 1/8 + 1/32). (Subscripts indicate the number base.) Analogous to scientific notation, where numbers are written to have a single non-zero digit to the left of the decimal point, we rewrite this number so it has a single 1 bit to the left of the "binary point". We simply multiply by the appropriate power of 2 to compensate for shifting the bits left by three positions:

Now we can read off the fraction and the exponent: the fraction is .01 and the exponent is −3.

As illustrated in the pictures, the three fields in the IEEE 754 representation of this number are:

IEEE 754 adds a bias to the exponent so that numbers can in many cases be compared conveniently by the same hardware that compares signed 2's-complement integers. Using a biased exponent, the lesser of two positive floating-point numbers will come out "less than" the greater following the same ordering as for sign and magnitude integers. If two floating-point numbers have different signs, the sign-and-magnitude comparison also works with biased exponents. However, if both biased-exponent floating-point numbers are negative, then the ordering must be reversed. If the exponent were represented as, say, a 2's-complement number, comparison to see which of two numbers is greater would not be as convenient.

The leading 1 bit is omitted since all numbers except zero start with a leading 1; the leading 1 is implicit and doesn't actually need to be stored which gives an extra bit of precision for "free."

The number zero is represented specially:

The number representations described above are called "normalized," meaning that the implicit leading binary digit is a 1. To reduce the loss of precision when an underflow occurs, IEEE 754 includes the ability to represent fractions smaller than are possible in the normalized representation, by making the implicit leading digit a 0. Such numbers are called denormal. They don't include as many significant digits as a normalized number, but they enable a gradual loss of precision when the result of an arithmetic operation is not exactly zero but is too close to zero to be represented by a normalized number.

A denormal number is represented with a biased exponent of all 0 bits, which represents an exponent of −126 in single precision (not −127), or −1022 in double precision (not −1023). In contrast, the smallest biased exponent representing a normal number is 1 (see examples below).

The biased-exponent field is filled with all 1 bits to indicate either infinity or an invalid result of a computation.

Positive and negative infinity are represented thus:

Some operations of floating-point arithmetic are invalid, such as taking the square root of a negative number. The act of reaching an invalid result is called a floating-point "exception." An exceptional result is represented by a special code called a NaN, for "Not a Number". All NaNs in IEEE 754-1985 have this format:

Precision is defined as the minimum difference between two successive mantissa representations; thus it is a function only in the mantissa; while the gap is defined as the difference between two successive numbers.

Single-precision numbers occupy 32 bits. In single precision:

Some example range and gap values for given exponents in single precision:

As an example, 16,777,217 can not be encoded as a 32-bit float as it will be rounded to 16,777,216. This shows why floating point arithmetic is unsuitable for accounting software. However, all integers within the representable range that are a power of 2 can be stored in a 32-bit float without rounding.

Double-precision numbers occupy 64 bits. In double precision:

Some example range and gap values for given exponents in double precision:

The standard also recommends extended format(s) to be used to perform internal computations at a higher precision than that required for the final result, to minimise round-off errors: the standard only specifies minimum precision and exponent requirements for such formats. The x87 80-bit extended format is the most commonly implemented extended format that meets these requirements.

Here are some examples of single-precision IEEE 754 representations:

Every possible bit combination is either a NaN or a number with a unique value in the affinely extended real number system with its associated order, except for the two bit combinations negative zero and positive zero, which sometimes require special attention (see below). The binary representation has the special property that, excluding NaNs, any two numbers can be compared as sign and magnitude integers (endianness issues apply). When comparing as 2's-complement integers: If the sign bits differ, the negative number precedes the positive number, so 2's complement gives the correct result (except that negative zero and positive zero should be considered equal). If both values are positive, the 2's complement comparison again gives the correct result. Otherwise (two negative numbers), the correct FP ordering is the opposite of the 2's complement ordering.

Rounding errors inherent in floating point calculations often make comparison of results for exact equality not useful. Choosing an acceptable range is a complex topic.

Although negative zero and positive zero are generally considered equal for comparison purposes, some programming language relational operators and similar constructs might or do treat them as distinct. According to the Java Language Specification, comparison and equality operators treat them as equal, but codice_1 and codice_2 distinguish them (officially starting with Java version 1.1 but actually with 1.1.1), as do the comparison methods codice_3, codice_4 and even codice_5 of classes codice_6 and codice_7.

The IEEE standard has four different rounding modes; the first is the default; the others are called "directed roundings".


The IEEE standard employs (and extends) the affinely extended real number system, with separate positive and negative infinities. During drafting, there was a proposal for the standard to incorporate the projectively extended real number system, with a single unsigned infinity, by providing programmers with a mode selection option. In the interest of reducing the complexity of the final standard, the projective mode was dropped, however. The Intel 8087 and Intel 80287 floating point co-processors both support this projective mode.

The following functions must be provided:


In 1976 Intel began planning to produce a floating point coprocessor. John Palmer, the manager of the effort, persuaded them that they should try to develop a standard for all their floating point operations. William Kahan was hired as a consultant; he had helped improve the accuracy of Hewlett-Packard's calculators. Kahan initially recommended that the floating point base be decimal but the hardware design of the coprocessor was too far along to make that change.

The work within Intel worried other vendors, who set up a standardization effort to ensure a 'level playing field'. Kahan attended the second IEEE 754 standards working group meeting, held in November 1977. Here, he received permission from Intel to put forward a draft proposal based on the standard arithmetic part of their design for a coprocessor. The arguments over gradual underflow lasted until 1981 when an expert hired by DEC to assess it sided against the dissenters.

Even before it was approved, the draft standard had been implemented by a number of manufacturers. The Intel 8087, which was announced in 1980, was the first chip to implement the draft standard.





</doc>
<doc id="15190" url="https://en.wikipedia.org/wiki?curid=15190" title="Intel 80186">
Intel 80186

The Intel 80186, also known as the iAPX 186, or just 186, is a microprocessor and microcontroller introduced in 1982. It was based on the Intel 8086 and, like it, had a 16-bit external data bus multiplexed with a 20-bit address bus. It was also available as the 80188, with an 8-bit external data bus.

The 80186 series was generally intended for embedded systems, as microcontrollers with external memory. Therefore, to reduce the number of integrated circuits required, it included features such as clock generator, interrupt controller, timers, wait state generator, DMA channels, and external chip select lines.

The initial clock rate of the 80186 was 6 MHz, but due to more hardware available for the microcode to use, especially for address calculation, many individual instructions ran faster than on an 8086 at the same clock frequency. For instance, the common "register+immediate" addressing mode was significantly faster than on the 8086, especially when a memory location was both (one of the) operand(s) and the destination. Multiply and divide also showed great improvement being several times as fast as on the original 8086 and multi-bit shifts were done almost four times as quickly as in the 8086.

A few new instructions were introduced with the 80186 (referred to as the 8086-2 instruction set in some datasheets): enter/leave (replacing several instructions when handling stack frames), pusha/popa (push/pop all general registers), bound (check array index against bounds), and ins/outs (input/output of string). A useful "immediate" mode was added for the push, imul, and multi-bit shift instructions. These instructions were also included in the contemporary 80286 and in successor chips. (The instruction set of the 80286 is exactly the instruction set of the 80186 with instructions added only for operations related to the 80286 Protected mode.)
The (redesigned) CMOS version, 80C186, introduced DRAM refresh, a power-save mode, and a direct interface to the 8087 or 80187 floating point numeric coprocessor.

The 80186 would have been a natural successor to the 8086 in personal computers. However, because its integrated hardware was incompatible with the hardware used in the original IBM PC, the 80286 was used as the successor instead in the IBM PC/AT.

A few notable personal computers used the 80186: the Australian Dulmont Magnum laptop, one of the first laptops; the Wang Office Assistant, marketed as a PC-like stand-alone word processor; the Mindset; the Siemens (not 100% IBM PC-compatible but using MS-DOS 2.11); the Compis (a Swedish school computer); the French SMT-Goupil G4; the RM Nimbus (a British school computer); the Unisys ICON (a Canadian school computer); ORB Computer by ABS; the HP 100LX, HP 200LX, HP 1000CX, and HP OmniGo 700LX; the Tandy 2000 desktop (a somewhat PC-compatible workstation with sharp graphics for its day); the Telex 1260 (a desktop PC-XT compatible); the ; the Nokia MikroMikko 2. Acorn created a plug-in for the BBC Master range of computers containing an 80186-10 with 512 KB of RAM, the BBC Master 512 system.

In addition to the above examples of stand-alone implementations of the 80186 for personal computers, there was at least one example of an "add-in" accelerator card implementation: the Orchid Technology PC Turbo 186, released in 1985. It was intended for use with the original 8088-based IBM PC (Model 5150).

The Intel 80186 is intended to be embedded in electronic devices that are not primarily computers. For example:

In May 2006, Intel announced that production of the 186 would cease at the end of September 2007. Pin- and instruction-compatible replacements might still be manufactured by various third party sources.





</doc>
<doc id="15191" url="https://en.wikipedia.org/wiki?curid=15191" title="Inquisition">
Inquisition

The Inquisition was a group of institutions within the government system of the Catholic Church whose aim was to combat public heresy committed by baptized Christians. It started in 12th-century France to combat religious dissent, in particular the Cathars and the Waldensians. Other groups investigated later included the Spiritual Franciscans, the Hussites (followers of Jan Hus) and the Beguines. Beginning in the 1250s, inquisitors were generally chosen from members of the Dominican Order, replacing the earlier practice of using local clergy as judges. The term Medieval Inquisition covers these courts up to mid-15th century.

During the Late Middle Ages and early Renaissance, the concept and scope of the Inquisition significantly expanded in response to the Protestant Reformation and the Catholic Counter-Reformation. It expanded to other European countries, resulting in the Spanish Inquisition and Portuguese Inquisition. The Spanish and Portuguese operated inquisitorial courts throughout their empires in Africa, Asia, and the Americas (resulting in the Peruvian Inquisition and Mexican Inquisition). The Spanish and Portuguese inquisitions focused particularly on the issue of Jewish anusim and Muslim converts to Catholicism, partly because these minority groups were more numerous in Spain and Portugal than in many other parts of Europe, and partly because they were often considered suspect due to the assumption that they had secretly reverted to their previous religions.

Except within the Papal States, the institution of the Inquisition was abolished in the early 19th century, after the Napoleonic Wars in Europe and after the Spanish American wars of independence in the Americas. The institution survived as part of the Roman Curia, but in 1908 was given the new name of "Supreme Sacred Congregation of the Holy Office". In 1965 it became the Congregation for the Doctrine of the Faith.

The term "Inquisition" comes from Medieval Latin "inquisitio", which referred to any court process that was based on Roman law, which had gradually come back into usage in the late medieval period. Today, the English term "Inquisition" can apply to any one of several institutions that worked against heretics (or other offenders against canon law) within the judicial system of the Roman Catholic Church. Although the term "Inquisition" is usually applied to ecclesiastical courts of the Catholic Church, it has several different usages:


"[T]he Inquisition, as a church-court, had no jurisdiction over Moors and Jews as such." Generally, the Inquisition was concerned only with the heretical behaviour of Catholic adherents or converts.

"The overwhelming majority of sentences seem to have consisted of penances like wearing a cross sewn on one's clothes, going on pilgrimage, etc." When a suspect was convicted of unrepentant heresy, the inquisitorial tribunal was required by law to hand the person over to the secular authorities for final sentencing, at which point a magistrate would determine the penalty, which was usually burning at the stake although the penalty varied based on local law. The laws were inclusive of proscriptions against certain religious crimes (heresy, etc.), and the punishments included death by burning, although usually the penalty was banishment or imprisonment for life, which was generally commuted after a few years. Thus the inquisitors generally knew what would be the fate of anyone so remanded, and cannot be considered to have divorced the means of determining guilt from its effects.

The 1578 edition of the Directorium Inquisitorum (a standard Inquisitorial manual) spelled out the purpose of inquisitorial penalties: "... quoniam punitio non refertur primo & per se in correctionem & bonum eius qui punitur, sed in bonum publicum ut alij terreantur, & a malis committendis avocentur" (translation: "... for punishment does not take place primarily and "per se" for the correction and good of the person punished, but for the public good in order that others may become terrified and weaned away from the evils they would commit").

Before 1100, the Catholic Church suppressed what they believed to be heresy, usually through a system of ecclesiastical proscription or imprisonment, but without using torture,
and seldom resorting to executions. Such punishments were opposed by a number of clergymen and theologians, although some countries punished heresy with the death penalty.
In the 12th century, to counter the spread of Catharism, prosecution of heretics became more frequent. The Church charged councils composed of bishops and archbishops with establishing inquisitions (the Episcopal Inquisition). The first Inquisition was temporarily established in Languedoc (south of France) in 1184. The murder of Pope Innocent's papal legate Pierre de Castelnau in 1208 sparked the Albigensian Crusade (1209–1229). The Inquisition was permanently established in 1229, run largely by the Dominicans in Rome and later at Carcassonne in Languedoc.

Historians use the term "Medieval Inquisition" to describe the various inquisitions that started around 1184, including the Episcopal Inquisition (1184–1230s) and later the Papal Inquisition (1230s). These inquisitions responded to large popular movements throughout Europe considered apostate or heretical to Christianity, in particular the Cathars in southern France and the Waldensians in both southern France and northern Italy. Other Inquisitions followed after these first inquisition movements. The legal basis for some inquisitorial activity came from Pope Innocent IV's papal bull "Ad extirpanda" of 1252, which explicitly authorized (and defined the appropriate circumstances for) the use of torture by the Inquisition for eliciting confessions from heretics. However, Nicholas Eymerich, the inquisitor who wrote the "Directorium Inquisitorum", stated: 'Quaestiones sunt fallaces et ineficaces' ("interrogations via torture are misleading and futile"). By 1256 inquisitors were given absolution if they used instruments of torture.

In the 13th century, Pope Gregory IX (reigned 1227–1241) assigned the duty of carrying out inquisitions to the Dominican Order and Franciscan Order. By the end of the Middle Ages, England and Castile were the only large western nations without a papal inquisition.
Most inquisitors were friars who taught theology and/or law in the universities. They used inquisitorial procedures, a common legal practice adapted from the earlier Ancient Roman court procedures. They judged heresy along with bishops and groups of "assessors" (clergy serving in a role that was roughly analogous to a jury or legal advisers), using the local authorities to establish a tribunal and to prosecute heretics. After 1200, a Grand Inquisitor headed each Inquisition. Grand Inquisitions persisted until the mid 19th century.

With the sharpening of debate and of conflict between the Protestant Reformation and the Catholic Counter-Reformation, Protestant societies came to see/use the Inquisition as a terrifying "Other", while staunch Catholics regarded the Holy Office as a necessary bulwark against the spread of reprehensible heresies.

While belief in witchcraft, and persecutions directed at or excused by it, were widespread in pre-Christian Europe, and reflected in Germanic law, the influence of the Church in the early medieval era resulted in the revocation of these laws in many places, bringing an end to traditional pagan witch hunts. Throughout the medieval era mainstream Christian teaching had denied the existence of witches and witchcraft, condemning it as pagan superstition. However, Christian influence on popular beliefs in witches and "maleficium" (harm committed by magic) failed to entirely eradicate folk belief in witches.

The fierce denunciation and persecution of supposed sorceresses that characterized the cruel witchhunts of a later age were not generally found in the first thirteen hundred years of the Christian era. The medieval Church distinguished between "white" and "black" magic. Local folk practice often mixed chants, incantations, and prayers to the appropriate patron saint to ward off storms, to protect cattle, or ensure a good harvest. Bonfires on Midsummer's Eve were intended to deflect natural catastrophes or the influence of fairies, ghosts, and witches. Plants, often harvested under particular conditions, were deemed effective in healing.

Black magic was that which was used for a malevolent purpose. This was generally dealt with through confession, repentance, and charitable work assigned as penance. Early Irish canons treated sorcery as a crime to be visited with excommunication until adequate penance had been performed. In 1258 Pope Alexander IV ruled that inquisitors should limit their involvement to those cases in which there was some clear presumption of heretical belief.

The prosecution of witchcraft generally became more prominent throughout the late medieval and Renaissance era, perhaps driven partly by the upheavals of the era – the Black Death, Hundred Years' War, and a gradual cooling of the climate that modern scientists call the Little Ice Age (between about the 15th and 19th centuries). Witches were sometimes blamed. Since the years of most intense witch-hunting largely coincide with the age of the Reformation, some historians point to the influence of the Reformation on the European witch-hunt.

Dominican priest Heinrich Kramer was assistant to the Archbishop of Salzburg. In 1484 Kramer requested that Pope Innocent VIII clarify his authority to prosecute witchcraft in Germany, where he had been refused assistance by the local ecclesiastical authorities. They maintained that Kramer could not legally function in their areas.

The bull "Summis desiderantes affectibus" sought to remedy this jurisdictional dispute by specifically identifying the dioceses of Mainz, Köln, Trier, Salzburg, and Bremen. Some scholars view the bull as "clearly political". The bull failed to ensure that Kramer obtained the support he had hoped for, in fact he was subsequently expelled from the city of Innsbruck by the local bishop, George Golzer, who ordered Kramer to stop making false accusations. Golzer described Kramer as senile in letters written shortly after the incident. This rebuke led Kramer to write a justification of his views on witchcraft in his book "Malleus Maleficarum", written in 1486. In the book, Kramer stated his view that witchcraft was to blame for bad weather. The book is also noted for its animus against women. Despite Kramer's claim that the book gained acceptance from the clergy at the university of Cologne, it was in fact condemned by the clergy at Cologne for advocating views that violated Catholic doctrine and standard inquisitorial procedure. In 1538 the Spanish Inquisition cautioned its members not to believe everything the Malleus said.

Portugal and Spain in the late Middle Ages consisted largely of multicultural territories of Muslim and Jewish influence, reconquered from Islamic control, and the new Christian authorities could not assume that all their subjects would suddenly become and remain orthodox Roman Catholics. So the Inquisition in Iberia, in the lands of the Reconquista counties and kingdoms like León, Castile and Aragon, had a special socio-political basis as well as more fundamental religious motives.

In some parts of Spain towards the end of the 14th century, there was a wave of violent anti-Judaism, encouraged by the preaching of Ferrand Martinez, Archdeacon of Ecija. In the pogroms of June 1391 in Seville, hundreds of Jews were killed, and the synagogue was completely destroyed. The number of people killed was also high in other cities, such as Córdoba, Valencia and Barcelona.

One of the consequences of these pogroms was the mass conversion of thousands of surviving Jews. Forced baptism was contrary to the law of the Catholic Church, and theoretically anybody who had been forcibly baptized could legally return to Judaism. However, this was very narrowly interpreted. Legal definitions of the time theoretically acknowledged that a forced baptism was not a valid sacrament, but confined this to cases where it was literally administered by physical force. A person who had consented to baptism under threat of death or serious injury was still regarded as a voluntary convert, and accordingly forbidden to revert to Judaism. After the public violence, many of the converted "felt it safer to remain in their new religion." Thus, after 1391, a new social group appeared and were referred to as "conversos" or "New Christians".

King Ferdinand II of Aragon and Queen Isabella I of Castile established the Spanish Inquisition in 1478. In contrast to the previous inquisitions, it operated completely under royal Christian authority, though staffed by clergy and orders, and independently of the Holy See. It operated in Spain and in all Spanish colonies and territories, which included the Canary Islands, the Spanish Netherlands, the Kingdom of Naples, and all Spanish possessions in North, Central, and South America. It primarily targeted forced converts from Islam (Moriscos, Conversos and "secret Moors") and from Judaism (Conversos, Crypto-Jews and Marranos) — both groups still resided in Spain after the end of the Islamic control of Spain — who came under suspicion of either continuing to adhere to their old religion or of having fallen back into it.

In 1492 all Jews who had not converted were expelled from Spain; those who converted became nominal Catholics and thus subject to the Inquisition.

In the Americas, King Philip II set up three tribunals (each formally titled "Tribunal del Santo Oficio de la Inquisición") in 1569, one in Mexico, Cartagena de Indias (in modern-day Colombia) and Peru. The Mexican office administered Mexico (central and southeastern Mexico), Nueva Galicia (northern and western Mexico), the Audiencias of Guatemala (Guatemala, Chiapas, El Salvador, Honduras, Nicaragua, Costa Rica), and the Spanish East Indies. The Peruvian Inquisition, based in Lima, administered all the Spanish territories in South America and Panama.

The Portuguese Inquisition formally started in Portugal in 1536 at the request of King João III. Manuel I had asked Pope Leo X for the installation of the Inquisition in 1515, but only after his death in 1521 did Pope Paul III acquiesce. At its head stood a "Grande Inquisidor", or General Inquisitor, named by the Pope but selected by the Crown, and always from within the royal family. The Portuguese Inquisition principally targeted the Sephardic Jews, whom the state forced to convert to Christianity. Spain had expelled its Sephardic population in 1492; many of these Spanish Jews left Spain for Portugal but eventually were targeted there as well.

The Portuguese Inquisition held its first "auto-da-fé" in 1540. The Portuguese inquisitors mostly targeted the Jewish New Christians (i.e. "conversos" or "marranos"). The Portuguese Inquisition expanded its scope of operations from Portugal to its colonial possessions, including Brazil, Cape Verde, and Goa. In the colonies, it continued as a religious court, investigating and trying cases of breaches of the tenets of orthodox Roman Catholicism until 1821. King João III (reigned 1521–57) extended the activity of the courts to cover censorship, divination, witchcraft, and bigamy. Originally oriented for a religious action, the Inquisition exerted an influence over almost every aspect of Portuguese society: political, cultural, and social.

The Goa Inquisition, an inquisition largely aimed at Catholic converts from Hinduism or Islam who were thought to have returned to their original ways, started in 1560. In addition, the Inquisition prosecuted non-converts who broke prohibitions against the observance of Hindu or Muslim rites or interfered with Portuguese attempts to convert non-Christians to Catholicism. Aleixo Dias Falcão and Francisco Marques set it up in the palace of the Sabaio Adil Khan.

According to Henry Charles Lea, between 1540 and 1794, tribunals in Lisbon, Porto, Coimbra, and Évora resulted in the burning of 1,175 persons, the burning of another 633 in effigy, and the penancing of 29,590. But documentation of 15 out of 689 autos-da-fé has disappeared, so these numbers may slightly understate the activity.

With the Protestant Reformation, Catholic authorities became much more ready to suspect heresy in any new ideas,
including those of Renaissance humanism, previously strongly supported by many at the top of the Church hierarchy. The extirpation of heretics became a much broader and more complex enterprise, complicated by the politics of territorial Protestant powers, especially in northern Europe. The Catholic Church could no longer exercise direct influence in the politics and justice-systems of lands that officially adopted Protestantism. Thus war (the French Wars of Religion, the Thirty Years' War), massacre (the St. Bartholomew's Day massacre) and the missional and propaganda work (by the "Sacra congregatio de propaganda fide") of the Counter-Reformation came to play larger roles in these circumstances, and the Roman law type of a "judicial" approach to heresy represented by the Inquisition became less important overall.
In 1542 Pope Paul III established the Congregation of the Holy Office of the Inquisition as a permanent congregation staffed with cardinals and other officials. It had the tasks of maintaining and defending the integrity of the faith and of examining and proscribing errors and false doctrines; it thus became the supervisory body of local Inquisitions. Arguably the most famous case tried by the Roman Inquisition was that of Galileo Galilei in 1633.

The penances and sentences for those who confessed or were found guilty were pronounced together in a public ceremony at the end of all the processes. This was the "sermo generalis" or "auto-da-fé".
Penances (not matters for the civil authorities) might consist of a pilgrimage, a public scourging, a fine, or the wearing of a cross. The wearing of two tongues of red or other brightly colored cloth, sewn onto an outer garment in an "X" pattern, marked those who were under investigation. The penalties in serious cases were confiscation of property by the Inquisition or imprisonment. This led to the possibility of false charges to enable confiscation being made against those over a certain income, particularly rich "marranos". Following the French invasion of 1798, the new authorities sent 3,000 chests containing over 100,000 Inquisition documents to France from Rome.

The wars of independence of the former Spanish colonies in the Americas concluded with the abolition of the Inquisition in every quarter of Hispanic America between 1813 and 1825.

In Portugal, in the wake of the Liberal Revolution of 1820, the "General Extraordinary and Constituent Courts of the Portuguese Nation" abolished the Portuguese inquisition in 1821.

The last execution of the Inquisition was in Spain in 1826. This was the execution by garroting of the school teacher Cayetano Ripoll for purportedly teaching Deism in his school. In Spain the practices of the Inquisition were finally outlawed in 1834.

In Italy, after the restoration of the Pope as the ruler of the Papal States in 1814, the activity of the Papal States Inquisition continued on until the mid-19th century, notably in the well-publicised Mortara Affair (1858–1870). In 1908 the name of the Congregation became "The Sacred Congregation of the Holy Office", which in 1965 further changed to "Congregation for the Doctrine of the Faith", as retained to .

Beginning in the 19th century, historians have gradually compiled statistics drawn from the surviving court records, from which estimates have been calculated by adjusting the recorded number of convictions by the average rate of document loss for each time period. Gustav Henningsen and Jaime Contreras studied the records of the Spanish Inquisition, which list 44,674 cases of which 826 resulted in executions "in person" and 778 "in effigy" (i.e. a straw dummy was burned in place of the person). William Monter estimated there were 1000 executions between 1530–1630 and 250 between 1630–1730. Jean-Pierre Dedieu studied the records of Toledo's tribunal, which put 12,000 people on trial. For the period prior to 1530, Henry Kamen estimated there were about 2,000 executions in all of Spain's tribunals. Italian Renaissance history professor and Inquisition expert Carlo Ginzburg had his doubts about using statistics to reach a judgment about the period. "In many cases, we don’t have the evidence, the evidence has been lost," said Ginzburg.

Series 2 Episode 2 of "Monty Python's Flying Circus" is entitled "The Spanish Inquisition", and features Michael Palin, Terry Jones, and Terry Gilliam as an inept—not to mention anachronistic—team of Inquisitors attempting to menace 20th-Century Britons, who seem unfazed by their inane torture implements (such as "the rack", which was taken out of a dishwasher, or even the dreaded "Comfy Chair") and woefully unpolished theatrics (Palin's character discovers to his horror that having more "chief weapons" rather than fewer seems to take some of the punch out of his diabolical monologue). Their signature gimmick is bursting into the room whenever someone says they "didn't expect the Spanish Inquisition", and declaring that ""nobody" expects the Spanish Inquisition", a line which has itself inspired numerous homages and parodies since.

The 1982 novel "Baltasar and Blimunda" by José Saramago, portrays how the Portuguese Inquisition impacts the fortunes of the title characters as well as several others from history, including the priest and aviation pioneer Bartolomeu de Gusmão.

The 1981 comedy film "History of the World, Part I", produced and directed by Mel Brooks, features a segment on the Spanish Inquisition.

"Inquisitio" is a French television series set in the Middle Ages.

In the novel "Name of the Rose" by Umberto Eco, there is some discussion about various sects of Christianity and inquisition, a small discussion about the ethics and purpose of inquisition, and a scene of Inquisition. In the movie by the same name, The Inquisition plays a prominent role including torture and a burning at the stake. <Jean-Jacques Annaud, director>
In the novel La Catedral del Mar by Ildefonso Falcones, there are scenes of inquisition investigations in small towns and a great scene in Barcelona.









</doc>
<doc id="15192" url="https://en.wikipedia.org/wiki?curid=15192" title="Isaac">
Isaac

Isaac is one of the three patriarchs of the Israelites, according to the biblical Book of Genesis. In the biblical narrative, he is the son of Abraham and Sarah and father of Jacob; his name means "he will laugh", reflecting when both Abraham and Sarah laughed in disbelief when told that they would have a child. He is the only patriarch whose name was not changed, and the only one who did not move out of Canaan. According to the narrative, he died when he was 180 years old, the longest-lived of the three.

The story of Isaac is important in the Abrahamic religions including Judaism, Christianity and Islam. The consensus of modern scholarship doubts the existence of figures from Genesis, including Isaac.

The anglicized name Isaac is a transliteration of the Hebrew term "Yiṣḥāq" (יִצְחָק) which literally means "He laughs/will laugh." Ugaritic texts dating from the 13th century BCE refer to the benevolent smile of the Canaanite deity El. Genesis, however, ascribes the laughter to Isaac's parents, Abraham and Sarah, rather than El. According to the biblical narrative, Abraham fell on his face and laughed when God (Hebrew, "Elohim") imparted the news of their son's eventual birth. He laughed because Sarah was past the age of childbearing; both she and Abraham were advanced in age. Later, when Sarah overheard three messengers of the Lord renew the promise, she laughed inwardly for the same reason. Sarah denied laughing when God questioned Abraham about it.

In Amos Isaac is spelled not with a צ but with a ש - Amos 7:9 ישחק

It was prophesied to the patriarch Abraham that he would have a son and that his name should be Isaac. When Abraham became one hundred years old, this son was born to him by his first wife Sarah. Though this was Abraham's second son it was Sarah's first and only child.

On the eighth day from his birth, Isaac was circumcised, as was necessary for all males of Abraham's household, in order to be in compliance with Yahweh's covenant.

After Isaac had been weaned, Sarah saw Ishmael mocking, and urged her husband to cast out Hagar the bondservant and her son, so that Isaac would be Abraham's sole heir. Abraham was hesitant, but at God's order he listened to his wife's request.

At some point in Isaac's youth, his father Abraham brought him to Mount Moriah. At God's command, Abraham was to build a sacrificial altar and sacrifice his son Isaac upon it. After he had bound his son to the altar and drawn his knife to kill him, at the very last moment an angel of God prevented Abraham from proceeding. Rather, he was directed to sacrifice instead a nearby ram that was stuck in thickets.
Before Isaac was 40 (Gen 25:20) Abraham sent Eliezer, his steward, into Mesopotamia to find a wife for Isaac, from his nephew Bethuel's family. Eliezer chose the Aramean Rebekah for Isaac. After many years of marriage to Isaac, Rebekah had still not given birth to a child and was believed to be barren. Isaac prayed for her and she conceived. Rebekah gave birth to twin boys, Esau and Jacob. Isaac was 60 years old when his two sons were born. Isaac favored Esau, and Rebekah favored Jacob.

The narratives about Isaac do not mention him having concubines.

Isaac moved to "Beer-lahai-roi" after his father died. When the land experienced famine, he removed to the Philistine land of Gerar where his father once lived. This land was still under the control of King Abimelech as it was in the days of Abraham. Like his father, Isaac also deceived Abimelech about his wife and also got into the well business. He had gone back to all of the wells that his father dug and saw that they were all stopped up with earth. The Philistines did this after Abraham died. So, Isaac unearthed them and began to dig for more wells all the way to Beersheba, where he made a pact with Abimelech, just like in the day of his father.
Isaac grew old and became blind. He called his son Esau and directed him to procure some venison for him, in order to receive Isaac's blessing. While Esau was hunting, Jacob, after listening to his mother's advice, deceived his blind father by misrepresenting himself as Esau and thereby obtained his father's blessing, such that Jacob became Isaac's primary heir and Esau was left in an inferior position. According to Genesis 25:29–34, Esau had previously sold his birthright to Jacob for "bread and stew of lentils". Thereafter, Isaac sent Jacob into Mesopotamia to take a wife of his mother's brother's house. After 20 years working for his uncle Laban, Jacob returned home. He reconciled with his twin brother Esau, then he and Esau buried their father, Isaac, in Hebron after he died at the age of 180.

According to local tradition, the graves of Isaac and Rebekah, along with the graves of Abraham and Sarah and Jacob and Leah, are in the Cave of the Patriarchs.

In rabbinical tradition, the age of Isaac at the time of binding is taken to be 37, which contrasts with common portrayals of Isaac as a child. The rabbis also thought that the reason for the death of Sarah was the news of the intended sacrifice of Isaac. The sacrifice of Isaac is cited in appeals for the mercy of God in later Jewish traditions. The post-biblical Jewish interpretations often elaborate the role of Isaac beyond the biblical description and primarily focus on Abraham's intended sacrifice of Isaac, called the "aqedah" ("binding"). According to a version of these interpretations, Isaac died in the sacrifice and was revived. According to many accounts of Aggadah, unlike the Bible, it is Satan who is testing Isaac as an agent of God. Isaac's willingness to follow God's command at the cost of his death has been a model for many Jews who preferred martyrdom to violation of the Jewish law.

According to the Jewish tradition, Isaac instituted the afternoon prayer. This tradition is based on Genesis chapter 24, verse 63 ("Isaac went out to meditate in the field at the eventide").

Isaac was the only patriarch who stayed in Canaan during his whole life and though once he tried to leave, God told him not to do so. Rabbinic tradition gave the explanation that Isaac was almost sacrificed and anything dedicated as a sacrifice may not leave the Land of Israel. Isaac was the oldest of the biblical patriarchs at the time of his death, and the only patriarch whose name was not changed.

Rabbinic literature also linked Isaac's blindness in old age, as stated in the Bible, to the sacrificial binding: Isaac's eyes went blind because the tears of angels present at the time of his sacrifice fell on Isaac's eyes.

The early Christian church continued and developed the New Testament theme of Isaac as a type of Christ and the Church being both "the son of the promise" and the "father of the faithful". Tertullian draws a parallel between Isaac's bearing the wood for the sacrificial fire with Christ's carrying his cross. and there was a general agreement that, while all the sacrifices of the Old Law were anticipations of that on Calvary, the sacrifice of Isaac was so "in a pre-eminent way".

The Eastern Orthodox Church and the Roman Catholic Church consider Isaac as a Saint along with other biblical patriarchs. Along with those of other patriarchs and the Old Testament Righteous, his feast day is celebrated in the Eastern Orthodox Church and the Byzantine rite of the Catholic Church on the Second Sunday before Christmas (December 11–17), under the title "the Sunday of the Forefathers".

The New Testament states Isaac was "offered up" by his father Abraham, and that Isaac blessed his sons. Paul contrasted Isaac, symbolizing Christian liberty, with the rejected older son Ishmael, symbolizing slavery; Hagar is associated with the Sinai covenant, while Sarah is associated with the covenant of grace, into which her son Isaac enters. The Epistle of James chapter 2, verses 21–24, states that the sacrifice of Isaac shows that justification (in the Johannine sense) requires both faith and works.

In the Epistle to the Hebrews, Abraham's willingness to follow God's command to sacrifice Isaac is used as an example of faith as is Isaac's action in blessing Jacob and Esau with reference to the future promised by God to Abraham In verse 19, the author views the release of Isaac from sacrifice as analogous to the resurrection of Jesus, the idea of the sacrifice of Isaac being a prefigure of the sacrifice of Jesus on the cross.

Islam considers Isaac as a prophet of Islam, and describes him as the father of the Israelites and a righteous servant of God.

Isaac, along with Ishmael, is highly important for Muslims for continuing to preach the message of monotheism after his father Abraham. Among Isaac's children was the follow-up Israelite patriarch Jacob, who too is venerated an Islamic prophet.

Isaac is mentioned fifteen times by name in the Quran, often with his father and his son, Jacob. The Quran states that Abraham received "good tidings of Isaac, a prophet, of the righteous", and that God blessed them both (37: 12). In a fuller description, when angels came to Abraham to tell him of the future punishment to be imposed on Sodom and Gomorrah, his wife, Sarah, "laughed, and We gave her good tidings of Isaac, and after Isaac of (a grandson) Jacob" (11: 71–74); and it is further explained that this event will take place despite Abraham and Sarah's old age. Several verses speak of Isaac as a "gift" to Abraham (6: 84; 14: 49–50), and 24: 26–27 adds that God made "prophethood and the Book to be among his offspring", which has been interpreted to refer to Abraham's two prophetic sons, his prophetic grandson Jacob, and his prophetic great-grandson Joseph. In the Qur'an, it later narrates that Abraham also praised God for giving him Ishmael and Isaac in his old age (14: 39–41).

Elsewhere in the Quran, Isaac is mentioned in lists: Joseph follows the religion of his forefathers Abraham, Isaac and Jacob (12: 38) and speaks of God's favor to them (12: 6); Jacob's sons all testify their faith and promise to worship the God that their forefathers, "Abraham, Ishmael and Isaac", worshiped (2: 127); and the Qur'an commands Muslims to believe in the revelations that were given to "Abraham, Ishmael, Isaac, Jacob and the Patriarchs" (2: 136; 3: 84). In the Quran's narrative of Abraham's near-sacrifice of his son (37: 102), the name of the son is not mentioned and debate has continued over the son's identity, though many feel that the identity is the least important element in a story which is given to show the courage that one develops through faith.

The Quran mentions Isaac as a prophet and a righteous man of God. Isaac and Jacob are mentioned as being bestowed upon Abraham as gifts of God, who then worshipped God only and were righteous leaders in the way of God:

Some scholars have described Isaac as "a legendary figure" or "as a figure representing tribal history, or "as a seminomadic leader." The stories of Isaac, like other patriarchal stories of Genesis, are generally believed to have "their origin in folk memories and oral traditions of the early Hebrew pastoralist experience." "The Cambridge Companion to the Bible" makes the following comment on the biblical stories of the patriarchs:

According to Martin Noth, a scholar of the Hebrew Bible, the narratives of Isaac date back to an older cultural stage than that of the West-Jordanian Jacob. At that era, the Israelite tribes were not yet sedentary. In the course of looking for grazing areas, they had come in contact in southern Philistia with the inhabitants of the settled countryside. The biblical historian A. Jopsen believes in the connection between the Isaac traditions and the north, and in support of this theory adduces Amos 7:9 ("the high places of Isaac").

Albrecht Alt and Martin Noth hold that, "The figure of Isaac was enhanced when the theme of promise, previously bound to the cults of the 'God the Fathers' was incorporated into the Israelite creed during the southern-Palestinian stage of the growth of the Pentateuch tradition." According to Martin Noth, at the Southern Palestinian stage of the growth of the Pentateuch tradition, Isaac became established as one of the biblical patriarchs, but his traditions were receded in the favor of Abraham.

The earliest Christian portrayal of Isaac is found in the Roman catacomb frescoes. Excluding the fragments, Alison Moore Smith classifies these artistic works in three categories:





</doc>
<doc id="15193" url="https://en.wikipedia.org/wiki?curid=15193" title="Italian Football League">
Italian Football League

Italian Football League (IFL) is an American football league in Italy.

The league was founded in 2008, taking over previous league's significance (National Football League Italy). The league was born as a result of the escape of several of the best clubs of the old championship organized by the Italian federation, such as Milano Rhinos, Parma Panthers, Bologna Doves and Bolzano Giants. However some of the historic Italian clubs have not joined the new league and continue to participate in different tournaments organized by other federations. This is the case of Legnano Frogs, Torino Giaguari, etc.
In the following years a lot of teams moved to the FIDAF (the federation where the IFL belongs to) and most of the biggest teams are now part of the IFL that is the First Division or in the other 2 divisions.


† defunct
♦ due to league expansion the Napoli team can play the 2015 IFL season and is not relegated to the second division
‡ Roma Grizzlies won the second division championship and earned the right to play the 2015 IFL season

Until 2014 the championship game was called "Superbowl italiano".



</doc>
<doc id="15195" url="https://en.wikipedia.org/wiki?curid=15195" title="Iduna">
Iduna

Iduna can mean several things:


</doc>
<doc id="15198" url="https://en.wikipedia.org/wiki?curid=15198" title="Indic">
Indic

Indic is an adjective that may refer to:



</doc>
<doc id="15199" url="https://en.wikipedia.org/wiki?curid=15199" title="Papua (province)">
Papua (province)

Papua is the largest and easternmost province of Indonesia, comprising most of Western New Guinea. Papua is bordered by the nation of Papua New Guinea to the east, and by West Papua province to the west. Since 2002, Papua province has special autonomy status, making it a special region. Its capital is Jayapura. It was formerly called Irian Jaya (and earlier West Irian or Irian Barat) and comprised all of Indonesian New Guinea. In 2002 the current name was adopted and in 2003 West Papua province was created from western parts of Papua province.

"Papua" is the official Indonesian and internationally recognised name for the province.

During the Dutch colonial era the region was known as part of "Dutch New Guinea" or "Netherlands New Guinea". Since its annexation in 1969, it became known as "West Irian" or "Irian Barat" until 1973, and thereafter renamed "Irian Jaya" (roughly translated, "Glorious Irian") by the Suharto administration. This was the official name until the name "Papua" was adopted in 2002. Today, the indigenous inhabitants of this province prefer to call themselves Papuans.

The name "West Papua" was adopted in 1961 by the New Guinea Council until the United Nations Temporary Executive Authority (UNTEA) transferred administration to the Republic of Indonesia in 1963. "West Papua" has since been used by Papuans as a self-identifying term, especially by those demanding self-determination, and usually refers to the whole of the Indonesian portion of New Guinea. The other Indonesian province that shares New Guinea, West Irian Jaya, has been officially renamed as "West Papua", or "Papua Barat". The entire western New Guinea is often referred to as "West Papua" internationally – especially among networks of international solidarity with the West Papuan independence movement.

The province of Papua is governed by a directly elected governor (currently Lukas Enembe) and a regional legislature, People's Representative Council of province of Papua ("Dewan Perwakilan Rakyat Papua"). A government organisation that only exists in Papua is the Papuan People's Council ("Majelis Rakyat Papua") Papuan People's Council), which was formed by the Indonesian Government in 2005 as a coalition of Papuan tribal chiefs, tasked with arbitration and speaking on behalf of Papuan tribal customs.

Indonesian sovereignty over Papua dates back to 1969, when Indonesia conducted a referendum (referred to by the Indonesian government as the Act of Free Choice) on the self-determination of the peoples of Papua under an agreement with the United Nations to respect any result. Instead of conducting a democratic referendum amongst the general population, Indonesian security forces forcibly coerced a small number of tribal elders to vote to join Indonesia; some elders were not even made aware that a referendum was to be conducted beforehand. Nevertheless, the agreement with the UN was nominally upheld, and was recognised by the international community in spite of protests. This intensified the independence movement among indigenous West Papuans, deepening the Papua conflict, which began when the Dutch withdrew from the East Indies in 1963. The conflict has continued to the present, with Indonesian security forces being accused of numerous human rights abuses in their suppression of the independence movement. The Indonesian government maintains tight control over the region, barring foreign journalists or rights monitors from entering; those who do must do so covertly.

In 1999 it was proposed to split the province into three government-controlled sectors, sparking Papuan protests. In January 2003 President Megawati Sukarnoputri signed an order dividing Papua into three provinces: Central Irian Jaya (Irian Jaya Tengah), Papua (or East Irian Jaya, Irian Jaya Timur), and West Papua (Irian Jaya Barat). The formality of installing a local government for Jaraka in Irian Jaya Barat (West) took place in February 2003 and a governor was appointed in November; a government for Irian Jaya Tengah (Central Irian Jaya) was delayed from August 2003 due to violent local protests. The creation of this separate Central Irian Jaya Province was blocked by Indonesian courts, who declared it to be unconstitutional and in contravention of the Papua's special autonomy agreement. The previous division into two provinces was allowed to stand as an established fact.

The province of Papua is one of three provinces to have obtained special autonomy status, the others being Aceh and West Papua. According to Law 21/2001 on Special Autonomy Status "(UU Nomor 21 Tahun 2001 tentang Otonomi khusus Papua)," the provincial government of Papua is provided with authority within all sectors of administration, except for the five strategic areas of foreign affairs, security and defense, monetary and fiscal affairs, religion and justice. The provincial government is authorised to issue local regulations to further stipulate the implementation of the special autonomy, including regulating the authority of districts and municipalities within the province. Due to its special autonomy status, Papua province is provided with significant amount of special autonomy funds, which can be used to benefit its indigenous peoples. But the province has low fiscal capacity and it is highly dependent on unconditional transfers and the above-mentioned special autonomy fund, which accounted for about 55% of total revenues in 2008.

After obtaining its special autonomy status, in order to allow the local population access to timber production benefits, the Papuan provincial government issued a number of decrees, enabling:

As of 2010 (following the separation of West Papua Province in 2003), the residual Papua Province consisted of 28 regencies ("kabupaten") and one autonomous city ("kota"); these regencies and the city are together subdivided into 385 districts ("kecamatan"), and thence into "villages" ("kelurahan" and "desa"). In Papua, as well as in the province of West Papua, "kecamatan" are commonly referred to as "distrik".

The regencies ("kabupaten") and the city ("kota") are listed below with their areas and their populations at the 2010 Census and according to the latest official (January 2014) Estimates.

In 2000, the present area of Papua Province originally consisted of nine regencies:


On 12 November 2002, the following regencies were created:


On 8 January 2004, Supiori Regency was split from Biak Numfor Regency.

On 15 March 2007, under Law No. 19/2007, the following regencies were created:


On 4 January 2008, five other new regencies were created by Home Affairs Minister Mardiyanto who also installed five temporary regents. These five new regencies were:


Within 2008, Intan Jaya and Deiyai Regencies were split from Paniai Regency.

On 25 October 2013 the Indonesian House of Representatives (DPR) began reviewing draft laws on the establishment of 57 prospective regencies/cities (and 8 new provinces). This included two new provinces to be formed from parts of the existing Papua Province (and one new province from West Papua Province), as well as the creation of seventeen new regencies and two new cities (independent municipalities). The new regencies will be:


And the new cities will be the municipalities of:

The two new provinces from parts of the existing Papua province have recently been approved by Indonesia's House of Representatives:


Another new province, to be split from West Papua will be Southwest Papua.

The proposed South Papua ("Papua Selatan") Province would cover an area of 119,749 square km, which is rich in natural resources. It will encompass four existing regencies:


And will thus equate closely to the "original" Merauke Regency prior to the splitting of that entity in 2002. Within the existing regencies, new regencies to be added are the following:


And a new municipality of Merauke City (which is scheduled to be created from Merauke Regency)

According to a 20 January 2012 report in the "Cenderawasih Pos Jakarta", the central government is moving forward with the creation of "Central Papua". At that time it was envisaged that the new province would comprise ten existing regencies:


The new Central Papua Province, and the residual Papua Province, would together include the new regencies.

First of, for Papua Proper:


And a new municipality of Lembah Baliem (Baliem Valley, created from Jayawijaya Regency)

And for Central Papua:


The city of Jayapura also has the status of an autonomous city, equal to a regency. It was founded on 7 March 1910 as "Hollandia" and is the capital. Since Indonesian administration the name of the city has been changed to Kotabaru, then to Sukarnopura before its current name, Jayapura. Jayapura is also the largest city of Papua Province, with a small but active tourism industry. It is built on a slope overlooking the bay. Cenderawasih University ("UNCEN") campus at Abepura houses the University Museum where some of the Asmat artifacts collected by Michael Rockefeller is stored. Both Tanjung Ria beach, near the market at Hamadi – site of 22 April 1944 Allied invasion during World War II – and the site of General Douglas MacArthur's World War II headquarters at Ifar Gunung have monuments commemorating the events.

A central east–west mountain range dominates the geography of the island of New Guinea, over in total length. The western section is around long and across. The province contains the highest mountains between the Himalayas and the Andes, rising up to high, and ensuring a steady supply of rain from the tropical atmosphere. The tree line is around elevation and the tallest peaks contain permanent equatorial glaciers, increasingly melting due to a changing climate. Various other smaller mountain ranges occur both north and west of the central ranges. Except in high elevations, most areas possess a hot humid climate throughout the year, with some seasonal variation associated with the northeast monsoon season.

The southern and northern lowlands stretch for hundreds of kilometres and include lowland rainforests, extensive wetlands, savanna grasslands, and expanses of mangrove forest. The southern lowlands are the site of Lorentz National Park, also a UNESCO World Heritage Site.

The province's largest river is the Mamberamo located in the northern part of the province. The result is a large area of lakes and rivers known as the Lakes Plains region. The Baliem Valley, home of the Dani people, is a tableland above sea level in the midst of the central mountain range. Puncak Jaya, also known by its Dutch colonial name, "Carstensz Pyramid", is a limestone mountain peak above sea level. It is the highest peak of Oceania.

The following are some of the most well-known ethnic groups of Papua:


The Yei (pronounced Yay) are sometimes known as the Jei, Je, Yei-nan people.

There are approximately 2,500 speakers of the Yei language. 40% Ethno Religionists- animistic tribal religion 60% Catholics and other Christians (blended with animistic beliefs & customs):
The Yei language is believed to have two dialects observed by a Wycliffe, SIL language survey in 2001. At home the Yei people speak their own language but use Indonesian for trade, wider communication and at school. Most Yei are literate in Indonesian.

There are elementary schools in each village. About 10–30% of children continue in middle school. Very few go to high school. 
The nearest high school is in Merauke city.
They live primarily by hunting, fishing, and gardening short and long term crops in the lowlands. The Yei diet mainly consists of rice, vegetables, fish and roasted sago.
With their land at an altitude of less than 100 meters above sea level, the Yei people can best be accessed by vehicle on the road from Merauke or by motorized canoe up the Maro River. There is no airstrip or airplane access other than float plane which is currently available from Merauke through MAF by about a 15-minute flight to Toray.
The Poo and Bupul villages have a clinic but people still use traditional medicines.
There is very little infrastructure in the area: no telephones or toilets. At night electricity is run from a generator. There are single side-band radios (SSBs) in Bupul, Tanas, Poo, and Erambu villages, mainly used by the police and military force. Most villages get their drinking water from the Maro River, but some get it from wells or by collecting rain.

The population of Papua province has a fertility rate of 2.9 children per woman The population grew from the 1.9 million recorded in the 2000 Indonesia Census, to 2.9 million as recorded by the 2010 Census, and is officially estimated to be at about 3.5 million in 2014. Since the early 1990s Papua has had the highest population growth rate of all Indonesian provinces at over 3% annually. This is partly a result of birth rates, but mainly due to migration from other parts of Indonesia. While indigenous Papuans formed the near-totality of the population in 1961, they are now roughly 50% of the population, the other half being composed of non-Papuan migrants coming from other parts of Indonesia. An overwhelming percentage of these migrants came as part of a government-sponsored transmigration program.

The densest population center, other than the large coastal cities that house Indonesian bureaucratic and commercial apparatus, is located in and around the town of Wamena in the Baliem Valley of the Central Highlands.

According to the 2010 census, 83.15% of the Papuans identified themselves as Christian with 65.48% being Protestant and 17.67% being Roman Catholic. 15.88% of the population was Muslim and less than 1% were Buddhist or Hindu. There is also substantial practice of animism, the traditional religion for many Papuans, with many blending animistic beliefs with other religions such as Christianity.

In 2011, Papuan caretaker governor Syamsul Arief Rivai claimed Papua's forests cover 42 million hectares with an estimated worth of Rp.700 trillion ($78 billion) and that if the forests were managed properly and sustainably, they could produce over 500 million cubic meters of logs per annum.

The Grasberg Mine, the world's largest gold mine and third largest copper mine, is located in the highlands near Puncak Jaya, the highest mountain in Papua.

The island has an estimated 16,000 species of plant, 124 genera of which are endemic. Papua's known forest fauna includes; marsupials (including possums, wallabies, tree-kangaroos, cuscuses); other mammals (including the endangered long-beaked echidna); bird species such as birds-of-paradise, cassowaries, parrots, and cockatoos; the world's longest lizards (Papua monitor); and the world's largest butterflies.

The waterways and wetlands of Papua are also home to salt and freshwater crocodile, tree monitors, flying foxes, osprey, bats and other animals; while the equatorial glacier fields remain largely unexplored.

Protected areas within Papua province include the World Heritage Lorentz National Park, and the Wasur National Park, a Ramsar wetland of international importance.

In February 2006, a team of scientists exploring the Foja Mountains, Sarmi, discovered new species of birds, butterflies, amphibians, and plants, including possibly the largest-flowered species of rhododendron.

Ecological threats include logging-induced deforestation, forest conversion for plantation agriculture (including oil palm), smallholder agricultural conversion, the introduction and potential spread of alien species such as the crab-eating macaque which preys on and competes with indigenous species, the illegal species trade, and water pollution from oil and mining operations.

Religion Ministry has awarded Harmoni Award 2016 to Papua Regency due to religious tolerance in this area. Jayapura as a capital city of Papua Regency has also received the same award. The ministry give 10 awards, 3 award for provinces and 7 for regencies/cities.




</doc>
<doc id="15200" url="https://en.wikipedia.org/wiki?curid=15200" title="IMF (disambiguation)">
IMF (disambiguation)

The International Monetary Fund (IMF) is an international organization.

IMF may also refer to:





</doc>
<doc id="15201" url="https://en.wikipedia.org/wiki?curid=15201" title="Interdisciplinarity">
Interdisciplinarity

Interdisciplinarity or interdisciplinary studies involves the combining of two or more academic disciplines into one activity (e.g., a research project). It draws knowledge from several other fields like sociology, anthropology, psychology, economics etc. It is about creating something new by thinking across boundaries. It is related to an "interdiscipline" or an "interdisciplinary field," which is an organizational unit that crosses traditional boundaries between academic disciplines or schools of thought, as new needs and professions emerge. Large engineering teams are usually interdisciplinary, as a power station or mobile phone or other project requires the melding of several specialties. However, the term "interdisciplinary" is sometimes confined to academic settings.

The term "interdisciplinary" is applied within education and training pedagogies to describe studies that use methods and insights of several established disciplines or traditional fields of study. Interdisciplinarity involves researchers, students, and teachers in the goals of connecting and integrating several academic schools of thought, professions, or technologies—along with their specific perspectives—in the pursuit of a common task. The epidemiology of HIV/AIDS or global warming requires understanding of diverse disciplines to solve complex problems. "Interdisciplinary" may be applied where the subject is felt to have been neglected or even misrepresented in the traditional disciplinary structure of research institutions, for example, women's studies or ethnic area studies. Interdisciplinarity can likewise be applied to complex subjects that can only be understood by combining the perspectives of two or more fields.

The adjective "interdisciplinary" is most often used in educational circles when researchers from two or more disciplines pool their approaches and modify them so that they are better suited to the problem at hand, including the case of the team-taught course where students are required to understand a given subject in terms of multiple traditional disciplines. For example, the subject of land use may appear differently when examined by different disciplines, for instance, biology, chemistry, economics, geography, and politics.

Although “interdisciplinary” and “interdisciplinarity” are frequently viewed as twentieth century terms, the concept has historical antecedents, most notably Greek philosophy. Julie Thompson Klein attests that "the roots of the concepts lie in a number of ideas that resonate through modern discourse—the ideas of a unified science, general knowledge, synthesis and the integration of knowledge", while Giles Gunn says that Greek historians and dramatists took elements from other realms of knowledge (such as medicine or philosophy) to further understand their own material. The building of Roman roads required men who understood surveying, material science, logistics and several other disciplines. Any broadminded humanist project involves interdisciplinarity, and history shows a crowd of cases, as seventeenth-century Leibniz's task to create a system of universal justice, which required linguistics, economics, management, ethics, law philosophy, politics, and even sinology.

Interdisciplinary programs sometimes arise from a shared conviction that the traditional disciplines are unable or unwilling to address an important problem. For example, social science disciplines such as anthropology and sociology paid little attention to the social analysis of technology throughout most of the twentieth century. As a result, many social scientists with interests in technology have joined science, technology and society programs, which are typically staffed by scholars drawn from numerous disciplines. They may also arise from new research developments, such as nanotechnology, which cannot be addressed without combining the approaches of two or more disciplines. Examples include quantum information processing, an amalgamation of quantum physics and computer science, and bioinformatics, combining molecular biology with computer science. Sustainable development as a research area deals with problems requiring analysis and synthesis across economic, social and environmental spheres; often an integration of multiple social and natural science disciplines. Interdisciplinary research is also key to the study of health sciences, for example in studying optimal solutions to diseases. Some institutions of higher education offer accredited degree programs in Interdisciplinary Studies.

At another level, interdisciplinarity is seen as a remedy to the harmful effects of excessive specialization and isolation in information silos. On some views, however, interdisciplinarity is entirely indebted to those who specialize in one field of study—that is, without specialists, interdisciplinarians would have no information and no leading experts to consult. Others place the focus of interdisciplinarity on the need to transcend disciplines, viewing excessive specialization as problematic both epistemologically and politically. When interdisciplinary collaboration or research results in new solutions to problems, much information is given back to the various disciplines involved. Therefore, both disciplinarians and interdisciplinarians may be seen in complementary relation to one another.

Because most participants in interdisciplinary ventures were trained in traditional disciplines, they must learn to appreciate differing of perspectives and methods. For example, a discipline that places more emphasis on quantitative "rigor" may produce practitioners who think of themselves (and their discipline) as "more scientific" than others; in turn, colleagues in "softer" disciplines may associate quantitative approaches with an inability to grasp the broader dimensions of a problem. An interdisciplinary program may not succeed if its members remain stuck in their disciplines (and in disciplinary attitudes). On the other hand, and from the disciplinary perspective, much interdisciplinary work may be seen as "soft", lacking in rigor, or ideologically motivated; these beliefs place barriers in the career paths of those who choose interdisciplinary work. For example, interdisciplinary grant applications are often refereed by peer reviewers drawn from established disciplines; not surprisingly, interdisciplinary researchers may experience difficulty getting funding for their research. In addition, untenured researchers know that, when they seek promotion and tenure, it is likely that some of the evaluators will lack commitment to interdisciplinarity. They may fear that making a commitment to interdisciplinary research will increase the risk of being denied tenure.

Interdisciplinary programs may fail if they are not given sufficient autonomy. For example, interdisciplinary faculty are usually recruited to a joint appointment, with responsibilities in both an interdisciplinary program (such as women's studies) and a traditional discipline (such as history). If the traditional discipline makes the tenure decisions, new interdisciplinary faculty will be hesitant to commit themselves fully to interdisciplinary work. Other barriers include the generally disciplinary orientation of most scholarly journals, leading to the perception, if not the fact, that interdisciplinary research is hard to publish. In addition, since traditional budgetary practices at most universities channel resources through the disciplines, it becomes difficult to account for a given scholar or teacher's salary and time. During periods of budgetary contraction, the natural tendency to serve the primary constituency (i.e., students majoring in the traditional discipline) makes resources scarce for teaching and research comparatively far from the center of the discipline as traditionally understood. For these same reasons, the introduction of new interdisciplinary programs is often resisted because it is perceived as a competition for diminishing funds.

Due to these and other barriers, interdisciplinary research areas are strongly motivated to become disciplines themselves. If they succeed, they can establish their own research funding programs and make their own tenure and promotion decisions. In so doing, they lower the risk of entry. Examples of former interdisciplinary research areas that have become disciplines, many of them named for their parent disciplines, include neuroscience, cybernetics, biochemistry and biomedical engineering. These new fields are occasionally referred to as "interdisciplines". On the other hand, even though interdisciplinary activities are now a focus of attention for institutions promoting learning and teaching, as well as organizational and social entities concerned with education, they are practically facing complex barriers, serious challenges and criticism. The most important obstacles and challenges faced by interdisciplinary activities in the past two decades can be divided into "professional", "organizational", and "cultural" obstacles.

An initial distinction should be made between interdisciplinary studies, which can be found spread across the academy today, and the study of interdisciplinarity, which involves a much smaller group of researchers. The former is instantiated in thousands of research centers across the US and the world. The latter has one US organization, the Association for Interdisciplinary Studies (founded in 1979), two international organizations, the International Network of Inter- and Transdisciplinarity (founded in 2010) and the Philosophy of/as Interdisciplinarity Network (founded in 2009), and one research institute devoted to the theory and practice of interdisciplinarity, the Center for the Study of Interdisciplinarity at the University of North Texas (founded in 2008). As of September 1, 2014, the Center for the Study of Interdisciplinarity has ceased to exist. This is the result of administrative decisions at the University of North Texas.

An interdisciplinary study is an academic program or process seeking to synthesize broad perspectives, knowledge, skills, interconnections, and epistemology in an educational setting. Interdisciplinary programs may be founded in order to facilitate the study of subjects which have some coherence, but which cannot be adequately understood from a single disciplinary perspective (for example, women's studies or medieval studies). More rarely, and at a more advanced level, interdisciplinarity may itself become the focus of study, in a critique of institutionalized disciplines' ways of segmenting knowledge.

In contrast, studies of interdisciplinarity raise to self-consciousness questions about how interdisciplinarity works, the nature and history of disciplinarity, and the future of knowledge in post-industrial society. Researchers at the Center for the Study of Interdisciplinarity have made the distinction between philosophy 'of' and 'as' interdisciplinarity, the former identifying a new, discrete area within philosophy that raises epistemological and metaphysical questions about the status of interdisciplinary thinking, with the latter pointing toward a philosophical practice that is sometimes called 'field philosophy'.

Perhaps the most common complaint regarding interdisciplinary programs, by supporters and detractors alike, is the lack of synthesis—that is, students are provided with multiple disciplinary perspectives, but are not given effective guidance in resolving the conflicts and achieving a coherent view of the subject. Others have argued that the very idea of synthesis or integration of disciplines presupposes questionable politico-epistemic commitments. Critics of interdisciplinary programs feel that the ambition is simply unrealistic, given the knowledge and intellectual maturity of all but the exceptional undergraduate; some defenders concede the difficulty, but insist that cultivating interdisciplinarity as a habit of mind, even at that level, is both possible and essential to the education of informed and engaged citizens and leaders capable of analyzing, evaluating, and synthesizing information from multiple sources in order to render reasoned decisions.

While much has been written on the philosophy and promise of interdisciplinarity in academic programs and professional practice, social scientists are increasingly interrogating academic discourses on interdisciplinarity, as well as how interdisciplinarity actually works—and does not—in practice. Some have shown, for example, that some interdisciplinary enterprises that aim to serve society can produce deleterious outcomes for which no one can be held to account.

Since 1998, there has been an ascendancy in the value of interdisciplinary research and teaching and a growth in the number of bachelor's degrees awarded at U.S. universities classified as multi- or interdisciplinary studies. The number of interdisciplinary bachelor's degrees awarded annually rose from 7,000 in 1973 to 30,000 a year by 2005 according to data from the National Center of Educational Statistics (NECS). In addition, educational leaders from the Boyer Commission to Carnegie's President Vartan Gregorian to Alan I. Leshner, CEO of the American Association for the Advancement of Science have advocated for interdisciplinary rather than disciplinary approaches to problem solving in the 21st century. This has been echoed by federal funding agencies, particularly the National Institutes of Health under the direction of Elias Zerhouni, who has advocated that grant proposals be framed more as interdisciplinary collaborative projects than single researcher, single discipline ones.

At the same time, many thriving longstanding bachelor's in interdisciplinary studies programs in existence for 30 or more years, have been closed down, in spite of healthy enrollment. Examples include Arizona International (formerly part of the University of Arizona), the School of Interdisciplinary Studies at Miami University, and the Department of Interdisciplinary Studies at Wayne State University; others such as the Department of Interdisciplinary Studies at Appalachian State University, and George Mason University's New Century College, have been cut back. Stuart Henry has seen this trend as part of the hegemony of the disciplines in their attempt to recolonize the experimental knowledge production of otherwise marginalized fields of inquiry. This is due to threat perceptions seemingly based on the ascendancy of interdisciplinary studies against traditional academia.

There are many examples of when a particular idea, almost on the same period, arises in different disciplines. One case is the shift from the approach of focusing on "specialized segments of attention" (adopting one particular perspective), to the idea of "instant sensory awareness of the whole", an attention to the "total field", a "sense of the whole pattern, of form and function as a unity", an "integral idea of structure and configuration". This has happened in painting (with cubism), physics, poetry, communication and educational theory. According to Marshall McLuhan, this paradigm shift was due to the passage from an era shaped by mechanization, which brought sequentiality, to the era shaped by the instant speed of electricity, which brought simultaneity.

An article in the "Social Science Journal" attempts to provide a simple, common-sense, definition of interdisciplinarity, bypassing the difficulties of defining that concept and obviating the need for such related concepts as transdisciplinarity, pluridisciplinarity, and multidisciplinarity:"To begin with, a discipline can be conveniently defined as any comparatively self-contained and isolated domain of human experience which possesses its own community of experts. Interdisciplinarity is best seen as bringing together distinctive components of two or more disciplines. In academic discourse, interdisciplinarity typically applies to four realms: knowledge, research, education, and theory. Interdisciplinary knowledge involves familiarity with components of two or more disciplines. Interdisciplinary research combines components of two or more disciplines in the search or creation of new knowledge, operations, or artistic expressions. Interdisciplinary education merges components of two or more disciplines in a single program of instruction. Interdisciplinary theory takes interdisciplinary knowledge, research, or education as its main objects of study."In turn, interdisciplinary "richness" of any two instances of knowledge, research, or education can be ranked by weighing four variables: number of disciplines involved, the "distance" between them, the novelty of any particular combination, and their extent of integration.

Interdisciplinary knowledge and research are important because:

"The modern mind divides, specializes, thinks in categories: the Greek instinct was the opposite, to take the widest view, to see things as an organic whole ... It was arete that the Olympic games were designed to test the arete of the whole man, not a merely specialized skill ... The great event was the pentathlon, if you won this, you were a man. Needless to say, the Marathon race was never heard of until modern times: the Greeks would have regarded it as a monstrosity.""Previously, men could be divided simply into the learned and the ignorant, those more or less the one, and those more or less the other. But your specialist cannot be brought in under either of these two categories. He is not learned, for he is formally ignorant of all that does not enter into his specialty; but neither is he ignorant, because he is 'a scientist,' and 'knows' very well his own tiny portion of the universe. We shall have to say that he is a learned ignoramus, which is a very serious matter, as it implies that he is a person who is ignorant, not in the fashion of the ignorant man, but with all the petulance of one who is learned in his own special line.""It is the custom among those who are called "practical" men to condemn any man capable of a wide survey as a visionary: no man is thought worthy of a voice in politics unless he ignores or does not know nine tenths of the most important relevant facts."





</doc>
<doc id="15205" url="https://en.wikipedia.org/wiki?curid=15205" title="Insertion sort">
Insertion sort

Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:


When people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.

Insertion sort iterates, consuming one input element each repetition, and growing a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.

Sorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.

The resulting array after "k" iterations has the property where the first "k" + 1 entries are sorted ("+1" because the first entry is skipped). In each iteration the first remaining entry of the input is removed, and inserted into the result at the correct position, thus extending the result:

becomes

with each element greater than "x" copied to the right as it is compared against "x".

The most common variant of insertion sort, which operates on arrays, can be described as follows:

Pseudocode of the complete algorithm follows, where the arrays are zero-based:

The outer loop runs over all the elements except the first one, because the single-element prefix codice_1 is trivially sorted, so the invariant that the first codice_2 entries are sorted is true from the start. The inner loop moves element codice_3 to its correct place so that after the loop, the first codice_4 elements are sorted. Note that the codice_5-operator in the test must use short-circuit evaluation, otherwise the test might get stuck with an array bounds error, when codice_6 and it tries to evaluate codice_7 (i.e. accessing codice_8 fails).

After expanding the codice_9 operation in-place as codice_10 (where codice_11 is a temporary variable), a slightly faster version can be produced that moves codice_3 to its position in one go and only performs one assignment in the inner loop body:

The new inner loop shifts elements to the right to clear a spot for codice_13.

The algorithm can also be implemented in a recursive way. The recursion just replaces the outer loop, calling itself and storing successively smaller values of "n" on the stack until "n" equals 0, where the function then returns back up the call chain to execute the code after each recursive call starting with "n" equal to 1, with "n" increasing by 1 as each instance of the function returns to the prior instance. The initial call would be "insertionSortR(A, length(A)-1)" .

The best case input is an array that is already sorted. In this case insertion sort has a linear running time (i.e., O("n")). During each iteration, the first remaining element of the input is only compared with the right-most element of the sorted subsection of the array.

The simplest worst case input is an array sorted in reverse order. The set of all worst case inputs consists of all arrays where each element is the smallest or second-smallest of the elements before it. In these cases every iteration of the inner loop will scan and shift the entire sorted subsection of the array before inserting the next element. This gives insertion sort a quadratic running time (i.e., O("n")).

The average case is also quadratic, which makes insertion sort impractical for sorting large arrays. However, insertion sort is one of the fastest algorithms for sorting very small arrays, even faster than quicksort; indeed, good quicksort implementations use insertion sort for arrays smaller than a certain threshold, also when arising as subproblems; the exact threshold must be determined experimentally and depends on the machine, but is commonly around ten.

Example:
The following table shows the steps for sorting the sequence {3, 7, 4, 9, 5, 2, 6, 1}. In each step, the key under consideration is underlined. The key that was moved (or left in place because it was biggest yet considered) in the previous step is marked with an asterisk.

Insertion sort is very similar to selection sort. As in selection sort, after "k" passes through the array, the first "k" elements are in sorted order. However, the fundamental difference between the two algorithms is that for selection sort these are the "k" smallest elements of the unsorted input, while in insertion sort they are simply the first "k" elements of the input. The primary advantage of insertion sort over selection sort is that selection sort must always scan all remaining elements to find the absolute smallest element in the unsorted portion of the list, while insertion sort requires only a single comparison when the "k"+1th element is greater than the "k"th element; when this is frequently true (such as if the input array is already sorted or partially sorted), insertion sort is distinctly more efficient compared to selection sort. On average (assuming the rank of the "k"+1th element rank is random), insertion sort will require comparing and shifting half of the previous "k" elements, meaning insertion sort will perform about half as many comparisons as selection sort on average. In the worst case for insertion sort (when the input array is reverse-sorted), insertion sort performs just as many comparisons as selection sort. However, a disadvantage of insertion sort over selection sort is that it requires more writes due to the fact that, on each iteration, inserting the "k"+1th element into the sorted portion of the array requires many element swaps to shift all of the following elements, while only a single swap is required for each iteration of selection sort. In general, insertion sort will write to the array O("n") times, whereas selection sort will write only O() times. For this reason selection sort may be preferable in cases where writing to memory is significantly more expensive than reading, such as with EEPROM or flash memory.

While some divide-and-conquer algorithms such as quicksort and mergesort outperform insertion sort for larger arrays, non-recursive sorting algorithms such as insertion sort or selection sort are generally faster for very small arrays (the exact size varies by environment and implementation, but is typically between seven and fifty elements). Therefore, a useful optimization in the implementation of those algorithms is a hybrid approach, using the simpler algorithm when the array has been divided to a small size.

D.L. Shell made substantial improvements to the algorithm; the modified version is called Shell sort. The sorting algorithm compares elements separated by a distance that decreases on each pass. Shell sort has distinctly improved running times in practical work, with two simple variants requiring O("n") and O("n") running time.

If the cost of comparisons exceeds the cost of swaps, as is the case for example with string keys stored by reference or with human interaction (such as choosing one of a pair displayed side-by-side), then using "binary insertion sort" may yield better performance. Binary insertion sort employs a binary search to determine the correct location to insert new elements, and therefore performs ⌈log("n")⌉ comparisons in the worst case, which is O("n" log "n"). The algorithm as a whole still has a running time of O("n") on average because of the series of swaps required for each insertion.

The number of swaps can be reduced by calculating the position of multiple elements before moving them. For example, if the target position of two elements is calculated before they are moved into the right position, the number of swaps can be reduced by about 25% for random data. In the extreme case, this variant works similar to merge sort.

A variant named "binary merge sort" uses a "binary insertion sort" to sort groups of 32 elements, followed by a final sort using merge sort. It combines the speed of insertion sort on small data sets with the speed of merge sort on large data sets.

To avoid having to make a series of swaps for each insertion, the input could be stored in a linked list, which allows elements to be spliced into or out of the list in constant-time when the position in the list is known. However, searching a linked list requires sequentially following the links to the desired position: a linked list does not have random access, so it cannot use a faster method such as binary search. Therefore, the running time required for searching is O("n") and the time for sorting is O("n"). If a more sophisticated data structure (e.g., heap or binary tree) is used, the time required for searching and insertion can be reduced significantly; this is the essence of heap sort and binary tree sort.

In 2006 Bender, Martin Farach-Colton, and Mosteiro published a new variant of insertion sort called "library sort" or "gapped insertion sort" that leaves a small number of unused spaces (i.e., "gaps") spread throughout the array. The benefit is that insertions need only shift elements over until a gap is reached. The authors show that this sorting algorithm runs with high probability in O("n" log "n") time.

If a skip list is used, the insertion time is brought down to O(log "n"), and swaps are not needed because the skip list is implemented on a linked list structure. The final running time for insertion would be O("n" log "n").

"List insertion sort" is a variant of insertion sort. It reduces the number of movements.

If the items are stored in a linked list, then the list can be sorted with O(1) additional space. The algorithm starts with an initially empty (and therefore trivially sorted) list. The input items are taken off the list one at a time, and then inserted in the proper place in the sorted list. When the input list is empty, the sorted list has the desired result.

The algorithm below uses a trailing pointer for the insertion into the sorted list. A simpler recursive method rebuilds the list each time (rather than splicing) and can use O("n") stack space.



</doc>
<doc id="15207" url="https://en.wikipedia.org/wiki?curid=15207" title="Ig Nobel Prize">
Ig Nobel Prize

The Ig Nobel Prize is a parody of the Nobel Prize, which is awarded every autumn to celebrate ten unusual or trivial achievements in scientific research. Since 1991, the Ig Nobel Prizes have been awarded to “honor achievements that first make people laugh, and then make them think.” The name of the award, the “Ig Nobel Prize” ( ) is a pun on the word "ignoble", an achievement “characterized by baseness, lowness, or meanness”, and is satirical social criticism that identifies "absurd" research (although on occasion yielding useful knowledge).

Organized by the scientific humor magazine, the "Annals of Improbable Research" (AIR), the Ig Nobel Prizes are presented by Nobel laureates in a ceremony at the Sanders Theater, Harvard University, and are followed by the winners’ public lectures at the Massachusetts Institute of Technology.

The Ig Nobels were created in 1991 by Marc Abrahams, editor and co-founder of the "Annals of Improbable Research" and the master of ceremonies at all subsequent awards ceremonies. Awards were presented at that time for discoveries "that cannot, or should not, be reproduced". Ten prizes are awarded each year in many categories, including the Nobel Prize categories of physics, chemistry, physiology/medicine, literature, and peace, but also other categories such as public health, engineering, biology, and interdisciplinary research. The Ig Nobel Prizes recognize genuine achievements, with the exception of three prizes awarded in the first year to fictitious scientists Josiah S. Carberry, Paul DeFanti, and Thomas Kyle.

The awards are sometimes criticism via satire, as in the two awards given for homeopathy research, prizes in "science education" to the Kansas State Department of Education and Colorado State Board of Education for their stance regarding the teaching of evolution, and the prize awarded to "Social Text" after the Sokal affair. Most often, however, they draw attention to scientific articles that have some humorous or unexpected aspect. Examples range from the discovery that the presence of humans tends to sexually arouse ostriches, to the statement that black holes fulfill all the technical requirements to be the location of Hell, to research on the "five-second rule", a tongue-in-cheek belief that food dropped on the floor will not become contaminated if it is picked up within five seconds.

In 2010, Sir Andre Geim, who had been awarded an Ig Nobel Prize in 2000 for levitating a frog by magnetism, was awarded a Nobel Prize in physics for his work with graphene. He thereby became the first individual to have received both a Nobel and an Ig Nobel.

The prizes are presented by genuine Nobel laureates, originally at a ceremony in a lecture hall at MIT, but now in Sanders Theater at Harvard University. It contains a number of running jokes, including Miss Sweetie Poo, a little girl who repeatedly cries out, "Please stop: I'm bored", in a high-pitched voice if speakers go on too long. The awards ceremony is traditionally closed with the words: "If you didn't win a prize—and especially if you did—better luck next year!"

The ceremony is co-sponsored by the Harvard Computer Society, the Harvard–Radcliffe Science Fiction Association and the Harvard–Radcliffe Society of Physics Students.

Throwing paper planes onto the stage is a long-standing tradition at the Ig Nobels. In past years, physics professor Roy J. Glauber swept the stage clean of the airplanes as the official "Keeper of the Broom." Glauber could not attend the 2005 awards because he was traveling to Stockholm to claim a genuine Nobel Prize in Physics.

The "Parade of Ignitaries" into the hall includes supporting groups. At the 1997 ceremonies, a team of "cryogenic sex researchers" distributed a pamphlet titled "Safe Sex at Four Kelvin". And delegates from the Museum of Bad Art are often on hand to display some pieces from their collection.

The ceremony is recorded and broadcast on National Public Radio and is shown live over the Internet. The recording is broadcast every year, on the Friday after U.S. Thanksgiving, on the public radio program "Science Friday". In recognition of this, the audience chants the name of the radio show's host, Ira Flatow.

Two books have been published with write-ups on some of the winners: "The Ig Nobel Prize", and "The Ig Nobel Prize 2" which was later retitled "The Man Who Tried to Clone Himself".

An Ig Nobel Tour has been an annual part of National Science week in the United Kingdom since 2003. The tour has also traveled to Australia several times, Aarhus University in Denmark in April 2009, Italy and The Netherlands.

A September 2009 article in "The National" titled "A noble side to Ig Nobels" says that, although the Ig Nobel Awards are veiled criticism of trivial research, history shows that trivial research sometimes leads to important breakthroughs. For instance, in 2006, a study showing that one of the malaria mosquitoes ("Anopheles gambiae") is attracted equally to the smell of Limburger cheese and the smell of human feet earned the Ig Nobel Prize in the area of biology. As a direct result of these findings, traps baited with this cheese have been placed in strategic locations in some parts of Africa to combat the epidemic of malaria.




</doc>
<doc id="15208" url="https://en.wikipedia.org/wiki?curid=15208" title="Isaac Albéniz">
Isaac Albéniz

Isaac Manuel Francisco Albéniz y Pascual (; 29 May 186018 May 1909) was a Spanish virtuoso pianist, composer, and conductor. He is one of the foremost composers of the Post-Romantic era who also had a significant influence on his contemporaries and younger composers. He is best known for his piano works based on Spanish folk music idioms. 

Transcriptions of many of his pieces, such as "Asturias (Leyenda)", "Granada", "Sevilla", "Cadiz", "Córdoba", "Cataluña", and the "Tango in D", are important pieces for classical guitar, though he never composed for the guitar. The personal papers of Albéniz are preserved, among other institutions, in the "Biblioteca de Catalunya".

Born in Camprodon, province of Girona, to Ángel Albéniz (a customs official) and his wife, Dolors Pascual, Albéniz was a child prodigy who first performed at the age of four. At age seven, after apparently taking lessons from Antoine François Marmontel, he passed the entrance examination for piano at the Conservatoire de Paris, but he was refused admission because he was believed to be too young. By the time he had reached 12, he had made many attempts to run away from home.

His concert career began at the age of nine when his father toured both Isaac and his sister, Clementina, throughout northern Spain. A popular myth is that at the age of twelve Albéniz stowed away in a ship bound for Buenos Aires. He then found himself in Cuba, then to the United States, giving concerts in New York and San Francisco and then travelled to Liverpool, London and Leipzig, Germany. By age 15, he had already given concerts worldwide.This story is not entirely false, Albéniz did travel the world as a performer; however, he was accompanied by his "father", who as a customs agent was required to travel frequently. This can be attested by comparing Isaac's concert dates with his father's travel itinerary.

In 1876, after a short stay at the Leipzig Conservatory, he went to study at the Royal Conservatory of Brussels after King Alfonso's personal secretary, Guillermo Morphy, obtained him a royal grant. Count Morphy thought highly of Albéniz, who would later dedicate "Sevilla" to Morphy's wife when it premiered in Paris in January 1886.

In 1880 Albéniz went to Budapest, Hungary to study with Franz Liszt, only to find out that Liszt was in Weimar, Germany.

In 1883 he met the teacher and composer Felip Pedrell, who inspired him to write Spanish music such as the "Chants d'Espagne". The first movement (Prelude) of that suite, later retitled after the composer's death as "Asturias (Leyenda)", is probably most famous today as part of the classical guitar repertoire, even though it was originally composed for piano. (Many of Albéniz's other compositions were also transcribed for guitar, notably by Francisco Tárrega.) At the 1888 Barcelona Universal Exposition, the piano manufacturer Érard sponsored a series of 20 concerts featuring Albéniz's music.
The apex of Albéniz's concert career is considered to be 1889 to 1892 when he had concert tours throughout Europe. During the 1890s Albéniz lived in London and Paris. For London he wrote some musical comedies which brought him to the attention of the wealthy Francis Money-Coutts, 5th Baron Latymer. Money-Coutts commissioned and provided him with librettos for the opera "Henry Clifford" and for a projected trilogy of Arthurian operas. The first of these, "Merlin" (18981902), was thought to have been lost but has recently been reconstructed and performed. Albéniz never completed "Lancelot" (only the first act is finished, as a vocal and piano score), and he never began "Guinevere", the final part.

In 1900 he started to suffer from Bright's disease and returned to writing piano music. Between 1905 and 1908 he composed his final masterpiece, "Iberia" (1908), a suite of twelve piano "impressions".

In 1883 the composer married his student Rosina Jordana. They had three children: Blanca (who died in 1886), Laura (a painter), and Alfonso (who played for Real Madrid in the early 1900s before embarking on a career as a diplomat). Two other children died in infancy.

Albéniz died from his kidney disease on 18May1909 at age48 in Cambo-les-Bains, in Labourd, south-western France. Only a few weeks before his death, the government of France awarded Albéniz its highest honor, the Grand-Croix de la Légion d'honneur. He is buried at the Montjuïc Cemetery, Barcelona.

Albéniz's early works were mostly "salon style" music. Albéniz's first published composition, "Marcha Militar", appeared in 1868. A number of works written before this are now lost. He continued composing in traditional styles ranging from Jean-Philippe Rameau, Johann Sebastian Bach, Ludwig van Beethoven, Frédéric Chopin and Franz Liszt until the mid-1880s. He also wrote at least five zarzuelas, of which all but two are now lost.

Perhaps the best source on the works is Albéniz himself. He is quoted as commenting on his earlier period works as:There are among them a few things that are not completely worthless. The music is a bit infantile, plain, spirited; but in the end, the people, our Spanish people, are something of all that. I believe that the people are right when they continue to be moved by "Córdoba, Mallorca", by the copla of the "Sevillanas", by the "Serenata", and "Granada". In all of them I now note that there is less musical science, less of the grand idea, but more color, sunlight, flavor of olives. That music of youth, with its little sins and absurdities that almost point out the sentimental affectation ... appears to me like the carvings in the Alhambra, those peculiar arabesques that say nothing with their turns and shapes, but which are like the air, like the sun, like the blackbirds or like the nightingales of its gardens. They are more valuable than all else of Moorish Spain, which though we may not like it, is the true Spain.

During the late 1880s, the strong influence of Spanish style is evident in Albéniz's music. In 1883 Albéniz met the teacher and composer Felipe Pedrell. Pedrell was a leading figure in the development of nationalist Spanish music. In his book "The Music of Spain", Gilbert Chase describes Pedrell's influence on Albéniz: "What Albéniz derived from Pedrell was above all a spiritual orientation, the realization of the wonderful values inherent in Spanish music." Felipe Pedrell inspired Albéniz to write Spanish music such as the "Suite española", Op. 47, noted for its delicate, intricate melody and abrupt dynamic changes.

In addition to the Spanish spirit infused in Albéniz's music, he incorporated other qualities as well. In her biography of Albéniz, Pola Baytelman discerns four characteristics of the music from the middle period as follows:

1. The dance rhythms of Spain, of which there are a wide variety. 2. The use of cante jondo, which means deep or profound singing. It is the most serious and moving variety of flamenco or Spanish gypsy song, often dealing with themes of death, anguish, or religion. 3. The use of exotic scales also associated with flamenco music. The Phrygian mode is the most prominent in Albéniz's music, although he also used the Aeolian and Mixolydian modes as well as the whole-tone scale. 4. The transfer of guitar idioms into piano writing. 

Following his marriage, Albéniz settled in Madrid, Spain and produced a substantial quantity of music in a relatively short period. By 1886 he had written over 50piano pieces. The Albéniz biographer Walter A. Clark says that pieces from this period received enthusiastic reception in the composer's many concerts. Chase describes music from this period,

Taking the guitar as his instrumental model, and drawing his inspiration largely from the peculiar traits of Andalusian folk musicbut without using actual folk themesAlbéniz achieves a stylization of Spanish traditional idioms that while thoroughly artistic, gives a captivating impression of spontaneous improvisation... "Córdoba" is the piece that best represents the style of Albéniz in this period, with its hauntingly beautiful melody, set against the acrid dissonances of the plucked accompaniment imitating the notes of the Moorish guslas. Here is the heady scent of jasmines amid the swaying palm trees, the dream fantasy of an Andalusian "Arabian Nights" in which Albéniz loved to let his imagination dwell.

While Albéniz's crowning achievement, "Iberia", was written in the last years of his life in France, many of its preceding works are well-known and of great interest. The five pieces in "Chants d'Espagne", ("Songs of Spain", published in 1892) are a solid example of the compositional ideas he was exploring in the "middle period" of his life. The suite shows what Albéniz biographer Walter Aaron Clark describes as the "first flowering of his unique creative genius", and the beginnings of compositional exploration that became the hallmark of his later works. This period also includes his operatic works"Merlin, Henry Clifford", and "Pepita Jiménez". His orchestral works of this period include "Spanish Rhapsody" (1887) and "Catalonia" (1899), dedicated to Ramon Casas, who had painted his full-length portrait in 1894.

As one of the leading composers of his era, Albéniz's influences on both contemporary composers and on the future of Spanish music are profound. As a result of his extended stay in France and the friendship he formed with numerous composers there, his composition technique and harmonic language has influenced aspiring younger composers such as Claude Debussy and Maurice Ravel. His activities as conductor, performer and composer significantly raised the profile of Spanish music abroad and encouraged Spanish music and musicians in his own country.

Albéniz's works have become an important part of the repertoire of the classical guitar, many of which have been transcribed by Miguel Llobet and others. "Asturias (Leyenda)" in particular is heard most often on the guitar, as are "Granada", "Sevilla", "Cadiz", "Cataluña", "Córdoba" and the "Tango in D". Gordon Crosskey and Cuban-born guitarist Manuel Barrueco have both made solo guitar arrangements of six of the eight-movement "Suite española". Selections from "Iberia" have rarely been attempted on solo guitar but have been very effectively performed by guitar ensembles, such as the performance by John Williams and Julian Bream of "Iberia's " opening "Evocation". The Doors incorporated "Asturias" into their song "Spanish Caravan"; also, Iron Maiden's "To Tame a Land" uses the introduction of the piece for the song bridge. More recently, a guitar version of "Granada" functions as something of a love theme in Woody Allen's 2008 film "Vicky Cristina Barcelona". The 2008 horror film "Mirrors" incorporates the theme from "Asturias" into its score.

In 1997 the "Fundación Isaac Albéniz" was founded to promote Spanish music and musicians and to act as a research centre for Albéniz and Spanish music in general.

A film about his life entitled "Albéniz" was made in 1947. It was produced in Argentina.

References

Sources



</doc>
<doc id="15210" url="https://en.wikipedia.org/wiki?curid=15210" title="ITU-R">
ITU-R

The ITU Radiocommunication Sector (ITU-R) is one of the three sectors (divisions or units) of the International Telecommunication Union (ITU) and is responsible for radio communication. 

Its role is to manage the international radio-frequency spectrum and satellite orbit resources and to develop standards for radiocommunication systems with the objective of ensuring the effective use of the spectrum.

ITU is required, according to its Constitution, to allocate spectrum and register frequency allocation, orbital positions and other parameters of satellites, “in order to avoid harmful interference between radio stations of different countries”. The international spectrum management system is therefore based on regulatory procedures for frequency coordination, notification and registration.

ITU-R has a permanent secretariat, the Radiocommunication Bureau, based at the ITU HQ in Geneva, Switzerland. The elected Director of the Bureau is Mr. François Rancy of France; he was first elected by the ITU Membership to the Directorship in 2010.

The CCIR—Comité consultatif international pour la radio, Consultative Committee on International Radio or International Radio Consultative Committee—was founded in 1927.

In 1932 the CCIR and several other organizations (including the original ITU, which had been founded as the International Telegraph Union in 1865) merged to form what would in 1934 become known as the International Telecommunication Union. In 1992, the CCIR became the ITU-R.




</doc>
<doc id="15214" url="https://en.wikipedia.org/wiki?curid=15214" title="Irish Civil War">
Irish Civil War

The Irish Civil War (; 28 June 1922 – 24 May 1923) was a conflict that followed the Irish War of Independence and accompanied the establishment of the Irish Free State, an entity independent from the United Kingdom but within the British Empire.

The civil war was waged between two opposing groups, Irish republicans and Irish nationalists, over the Anglo-Irish Treaty. The forces of the Provisional Government (which became the Free State in December 1922) supported the Treaty, while the Republican opposition saw it as a betrayal of the Irish Republic (which had been proclaimed during the Easter Rising). Many of those who fought on both sides in the conflict had been members of the Irish Republican Army (IRA) during the War of Independence.

The Civil War was won by the Free State forces, who benefitted from substantial quantities of weapons provided by the British Government. The conflict may have claimed more lives than the War of Independence that preceded it, and left Irish society divided and embittered for generations. Today, two of the main political parties in the Republic of Ireland, Fianna Fáil and Fine Gael, are direct descendants of the opposing sides of the war.

The Anglo-Irish Treaty was agreed to end the 1919–1921 Irish War of Independence between the Irish Republic and the United Kingdom of Great Britain and Ireland. The treaty provided for a self-governing Irish state, having its own army and police. The Treaty also allowed Northern Ireland (the six north-eastern countiesFermanagh, Antrim, Tyrone, Londonderry, Armagh and Down where the majority population was of the Protestant religion) to opt out of the new state and return to the United Kingdomwhich it did immediately. However, rather than creating the independent republic favoured by most nationalists, the Irish Free State would be an autonomous dominion of the British Empire with the British monarch as head of state, in the same manner as Canada and Australia. The British suggested dominion status in secret correspondence even before treaty negotiations began, but Sinn Féin leader Éamon de Valera rejected the dominion. The treaty also stipulated that members of the new Irish Oireachtas (parliament) would have to take the following "Oath of Allegiance"

This oath was highly objectionable to many Irish Republicans. Furthermore, the partition of Ireland, which had already been decided by the Westminster parliament in the Government of Ireland Act 1920, was effectively confirmed in the Anglo-Irish treaty. The most contentious areas of the Treaty for the IRA were the disestablishment of the Irish Republic declared in 1919, the abandonment of the First Dáil, the status of the Irish Free State as a dominion in the British Commonwealth and the British retention of the so-called strategic Treaty Ports on Ireland's south coast which were to remain occupied by the Royal Navy. All these issues were the cause of a split in the IRA and ultimately civil war.

Michael Collins, the republican leader who had led the Irish negotiating team, argued that the treaty gave "not the ultimate freedom that all nations aspire and develop, but the freedom to achieve freedom". However, anti-treaty militants in 1922 believed that the treaty would never deliver full Irish independence.

The split over the treaty was deeply personal. Many of the leaders on both sides had been close friends and comrades during the War of Independence. This made their disagreement over the treaty all the more bitter. Michael Collins later said that Éamon de Valera had sent him as plenipotentiary to negotiate the treaty because he knew that the British would not concede an independent Irish republic and wanted Collins to take the blame for the compromise settlement. He said that he felt deeply betrayed when de Valera refused to stand by the agreement that the plenipotentiaries had negotiated with David Lloyd George and Winston Churchill. De Valera, for his part, was furious that Collins and Arthur Griffith had signed the treaty without consulting him or the Irish cabinet as instructed.

Dáil Éireann (the parliament of the Irish Republic) narrowly passed the Anglo-Irish Treaty by 64 votes to 57 on 7 January 1922. Following the Treaty's ratification, in accordance with article 17 of the Treaty, the British-recognised Provisional Government of the Irish Free State was established. Its authority under the Treaty was to provide a "provisional arrangement for the administration of Southern Ireland during the interval" before the establishment of the Irish Free State. In accordance with the Treaty, the British Government transferred "the powers and machinery requisite for the discharge of its duties". Before the British Government transferred such powers, the members of the Provisional Government each "signified in writing [their] acceptance of [the Treaty]".

Upon the Treaty's ratification, de Valera resigned as President of the Republic and failed to be re-elected by an even closer vote of 60–58. He challenged the right of the Dáil to approve the treaty, saying that its members were breaking their oath to the Irish Republic. De Valera continued to promote a compromise whereby the new Irish Free State would be in "external association" with the British Commonwealth rather than be a member of it (the inclusion of republics within the Commonwealth of Nations was not formally implemented until 1949).

In early March, he formed the "Cumann na Poblachta" (Republican Association) party while remaining a member of Sinn Féin and commenced a speaking tour of the more republican province of Munster on 17 March 1922. During the tour, de Valera made controversial speeches at Carrick on Suir, Lismore, Dungarvan and Waterford, saying at one point, "If the Treaty were accepted, the fight for freedom would still go on, and the Irish people, instead of fighting foreign soldiers, will have to fight the Irish soldiers of an Irish government set up by Irishmen." At Thurles, several days later, he repeated this imagery and added that the IRA "would have to wade through the blood of the soldiers of the Irish Government, and perhaps through that of some members of the Irish Government to get their freedom." 

In a letter to the Irish Independent on 23 March, de Valera accepted the accuracy of their report of his comment about "wading" through blood, but deplored that the newspaper had published it.

More seriously, many Irish Republican Army (IRA) officers were also against the treaty, and in March 1922 an ad hoc Army Convention repudiated the authority of the Dáil to accept the treaty. In contrast, the Minister of Defence, Richard Mulcahy, stated in the Dáil on 28 April that conditions in Dublin had prevented a Convention from being held, but that delegates had been selected and voted by ballot to accept the Oath. The anti-Treaty IRA formed their own "Army Executive", which they declared to be the real government of the country, despite the result of the 1921 general election. On 26 April, the Minister of Defence, Richard Mulcahy, summarised alleged illegal activities by many IRA men over the previous three months, whom he described as 'seceding volunteers', including hundreds of robberies. Yet this fragmenting army was the only police force on the ground following the disintegration of the Irish Republican Police and the disbanding of the Royal Irish Constabulary (RIC).

By putting ten questions to General Mulcahy on 28 April, Seán McEntee argued that the Army Executive had acted continuously on its own to create a republic since 1917, had an unaltered constitution, had never fallen under the control of the Dáil, and that: ""the only body competent to dissolve the Volunteer Executive was a duly convened convention of the Irish Republican Army"" – not the Dáil. By accepting the treaty in January and abandoning the republic, the Dáil majority had effectively deserted the Army Executive. In his reply, Mulcahy rejected this interpretation. Then, in a debate on defence, McEntee suggested that supporting the Army Executive ""... even if it meant the scrapping of the Treaty and terrible and immediate war with England, would be better than the civil war which we are beginning at present apparently."" McEntee's supporters added that the many robberies complained of by Mulcahy on 26 April were caused by the lack of payment and provision by the Dáil to the volunteers.

Collins established an "army re-unification committee" to re-unite the IRA and organised an election pact with de Valera's anti-treaty political followers to campaign jointly in the Free State's first election in 1922 and form a coalition government afterwards. He also tried to reach a compromise with anti-treaty IRA leaders by agreeing to a republican-type constitution (with no mention of the British monarchy) for the new state. IRA leaders such as Liam Lynch were prepared to accept this compromise. However, the proposal for a republican constitution was vetoed by the British as being contrary to the terms of the treaty and they threatened military intervention in the Free State unless the treaty were fully implemented. Collins reluctantly agreed. This completely undermined the electoral pact between the pro- and anti-treaty factions, who went into the Irish general election on 18 June 1922 as hostile parties, both calling themselves Sinn Féin.

The Pro-Treaty Sinn Féin party won the election with 239,193 votes to 133,864 for Anti-Treaty Sinn Féin. A further 247,226 people voted for other parties, most of whom supported the Treaty. Labour's 132,570 votes were ambiguous with regard to the Treaty. According to Hopkinson, "Irish labour and union leaders, while generally pro-Treaty, made little attempt to lead opinion during the Treaty conflict, casting themselves rather as attempted peacemakers." The election showed that a majority of the Irish electorate accepted the treaty and the foundation of the Irish Free State, but de Valera, his political followers and most of the IRA continued to oppose the treaty. De Valera is quoted as saying, "the majority have no right to do wrong".

Meanwhile, under the leadership of Michael Collins and Arthur Griffith, the pro-treaty Provisional Government set about establishing the Irish Free State, and organised the National Army – to replace the IRA – and a new police force. However, since it was envisaged that the new army would be built around the IRA, Anti-Treaty IRA units were allowed to take over British barracks and take their arms. In practice, this meant that by the summer of 1922, the Provisional Government of Southern Ireland controlled only Dublin and some other areas like County Longford where the IRA units supported the treaty. Fighting ultimately broke out when the Provisional Government tried to assert its authority over well-armed and intransigent Anti-Treaty IRA units around the country – particularly a hardline group in Dublin.

On 14 April 1922, 200 Anti-Treaty IRA militants, led by Rory O'Connor, occupied the Four Courts and several other buildings in central Dublin, resulting in a tense stand-off. These anti-treaty Republicans wanted to spark a new armed confrontation with the British, which they hoped would unite the two factions of the IRA against their common enemy. However, for those who were determined to make the Free State into a viable, self-governing Irish state, this was an act of rebellion that would have to be put down by them rather than the British.

Arthur Griffith was in favour of using force against these men immediately, but Michael Collins, who wanted at all costs to avoid civil war, left the Four Courts garrison alone until late June 1922. By this point, the Pro-Treaty Sinn Féin party had secured a large majority in the general election, along with other parties that supported the Treaty. Collins was also coming under continuing pressure from London to assert his government's authority in Dublin.

The British Government at this time also lost patience with the situation in Dublin as a result of the assassination of Field Marshal Henry Hughes Wilson, a prominent security adviser to the Prime Minister of Northern Ireland James Craig, by I.R.A. men on a street in London on 22 June 1922, with no responsibility for the act being publicly claimed by any IRA authority. Winston Churchill assumed that the Anti-Treaty IRA were responsible for the shooting and warned Collins that he would use British troops to attack the Four Courts unless the Provisional Government took action. In fact, the British cabinet actually resolved to attack the Four Courts themselves on 25 June, in an operation that would have involved tanks, howitzers and aeroplanes. However, on the advice of General Nevil Macready, who commanded the British garrison in Dublin, the plan was cancelled at the last minute. Macready's argument was that British involvement would have united Irish Nationalist opinion against the treaty, and instead Collins was given a last chance to clear the Four Courts himself.

The final straw for the Free State government came on 26 June, when the Four Courts republican garrison kidnapped JJ "Ginger" O'Connell in retaliation for the arrest of Leo Henderson, a general in the new National Army. Collins, after giving the Four Courts garrison a final (and according to Ernie O'Malley, only) ultimatum to leave the building on 27 June, decided to end the stand-off by bombarding the Four Courts garrison into surrender. The government then appointed Collins as Commander-in-Chief of the National Army. This attack was not the opening shot of the war, as skirmishes had taken place between pro- and anti-treaty IRA factions throughout the country when the British were handing over the barracks. However, this represented the 'point of no return', when all-out war was "ipso facto" declared and the Civil War officially began.

Collins ordered Mulcahy to accept a British offer of two 18-pounder field artillery for use by the new army of the Free State, though General Macready gave just 200 shells of the 10,000 he had in store at Richmond barracks in Inchicore. The anti-treaty forces in the Four Courts, who possessed only small arms, surrendered after three days of bombardment and the storming of the building by Provisional Government troops (28–30 June 1922). Shortly before the surrender, a massive explosion destroyed the western wing of the complex, including the Irish Public Record Office (PRO), injuring many advancing Free State soldiers and destroying the records. Government supporters alleged that the building had been deliberately mined. Historians dispute whether the PRO was intentionally destroyed by mines laid by the Republicans on their evacuation or if the explosions occurred when their ammunition store was accidentally ignited by the bombardment. Coogan, however, asserts that two lorry-loads of gelignite was exploded in the PRO, leaving priceless manuscripts floating over the city for several hours afterward.

Pitched battles continued in Dublin until 5 July, as Anti-Treaty IRA units from the Dublin Brigade, led by Oscar Traynor, occupied O'Connell Street – provoking a week's more street fighting: costing both sides 65 killed and 280 wounded. Among the dead was Republican leader Cathal Brugha, who made his last stand after exiting the Granville Hotel. In addition, the Free State took over 500 Republican prisoners. The civilian casualties are estimated to have numbered well over 250. When the fighting in Dublin died down, the Free State government was left firmly in control of the Irish capital and the anti-treaty forces dispersed around the country, mainly to the south and west.

The outbreak of the Civil War forced pro- and anti-treaty supporters to choose sides. Supporters of the treaty came to be known as "pro-treaty" or Free State Army, legally the National Army, and were often called "Staters" by their opponents. The latter called themselves Republicans and were also known as "anti-treaty" forces, or Irregulars, a term preferred by the Free State side.

The Anti-Treaty IRA claimed that it was defending the Irish Republic declared in 1916 during the Easter Rising, confirmed by the First Dáil and invalidly set aside by those who accepted the compromise of the Free State. Éamon de Valera stated that he would serve as an ordinary IRA volunteer and left the leadership of the Anti-Treaty Republicans to Liam Lynch, the IRA Chief of Staff. De Valera, though the Republican 'President as of October 1922, had little control over military operations. Military operations were directed by Liam Lynch until he was killed on 10 April 1923 and then by Frank Aiken, as of 20 April 1923.
The Civil War split the IRA. When the Civil War broke out, the Anti-Treaty IRA (concentrated in the south and west) outnumbered the pro-Free State forces by roughly 12,000 men to 8,000. Moreover, the Anti-Treaty ranks included many of the IRA's most experienced guerrilla fighters. The paper strength of the IRA in early 1922 was over 72,000 men, but most of them were recruited during the truce with the British and fought in neither the War of Independence nor the Civil War. According to Richard Mulcahy's estimate the anti-Treaty IRA at the beginning of the war, had 6,780 rifles and 12,900 men.

However, the Anti-Treaty IRA lacked an effective command structure, a clear strategy and sufficient arms. As well as rifles they had a handful of machine guns and many of their fighters were armed only with shotguns or handguns. They also took a handful of armoured cars from British troops as they were evacuating the country. Finally, they had no artillery of any kind. As a result, they were forced to adopt a defensive stance throughout the war.

By contrast, the Free State government managed to expand its forces dramatically after the start of the war. Michael Collins and his commanders were able to build up an army that was able to overwhelm their opponents in the field. British supplies of artillery, aircraft, armoured cars, machine guns, small arms, and ammunition were of much help to pro-treaty forces. The British delivered for instance, over 27,000 rifles, 250 machine guns and eight 18 pounder artillery pieces to the pro-Treaty forces between the outbreak of the Civil War and September 1922. The National Army amounted to 14,000 men by August 1922, was 38,000 strong by the end of 1922, and by the end of the war had grown to 55,000 men and 3,500 officers, far in excess of what the Irish state would need to maintain in peacetime.

Like the anti-Treaty IRA, the Free State's National Army was initially rooted in the IRA that fought against the British. Collins' most ruthless officers and men were recruited from the Dublin Active Service Unit (the elite unit of the IRA's Dublin Brigade) and from Michael Collins' Intelligence Department and assassination unit, The Squad. In the new National Army, they were known as the Dublin Guard. Towards the end of the war, they were implicated in some notorious atrocities against anti-treaty guerrillas in County Kerry. Up to the outbreak of Civil War, it had been agreed that only men with service in the IRA could be recruited into the National Army. However, once the war began, all such restrictions were lifted. A 'National Call to Arms' issued on July 7 for recruitment on a six-month basis brought in thousands of new recruits. Many of the new army's recruits were veterans of the British Army in World War I, where they had served in disbanded Irish regiments of the British Army. Many other were raw recruits without any military experience. The fact that at least 50% of the other ranks had no military experience in turn led to ill-discipline becoming a major problem.

A major problem for the National Army was a shortage of experienced officers. At least 20% of the National Army's officers had previously served as officers in the British Army while 50% of the rank and file of the National Army had served in the British Army in World War I. Former British Army officers were also recruited for their technical expertise. A number of the senior Free State commanders, such as Emmet Dalton, John T. Prout, and W.R.E. Murphy, had seen service as officers in World War One, Dalton and Murphy in the British Army and Prout in the US Army. The Republicans made much use of this fact in their propaganda — claiming that the Free State was only a proxy force for Britain itself. However, in fact, the majority of Free State soldiers were raw recruits without military experience in either World War I or the Irish War of Independence. There were also a significant number of former members of the British Armed Forces on the Republican side including such senior figures as Tom Barry, David Robinson and Erskine Childers.

With Dublin in pro-treaty hands, conflict spread throughout the country. The war started with the anti-treaty forces holding Cork, Limerick and Waterford as part of a self-styled Munster Republic. However, since the anti-treaty side were not equipped to wage conventional war, Liam Lynch was unable to take advantage of the Republicans' initial advantage in numbers and territory held. He hoped simply to hold the Munster Republic long enough to force Britain to re-negotiate the treaty.

The large towns in Ireland were all relatively easily taken by the Free State in August 1922. Michael Collins, Richard Mulcahy and Eoin O'Duffy planned a nationwide Free State offensive, dispatching columns overland to take Limerick in the west and Waterford in the south-east and seaborne forces to take counties Cork and Kerry in the south and Mayo in the west. In the south, landings occurred at Union Hall in Co. Cork and Fenit, the port of Tralee, in Co. Kerry. Limerick fell on 20 July, Waterford on the same day and Cork city on 10 August after a Free State force landed by sea at Passage West. Another seaborne expedition to Mayo in the west secured government control over that part of the country. While in some places the Republicans had put up determined resistance, nowhere were they able to defeat regular forces armed with artillery and armour. The only real conventional battle during the Free State offensive, the Battle of Killmallock, was fought when Free State troops advanced south from Limerick.

Government victories in the major towns inaugurated a period of guerrilla warfare. After the fall of Cork, Liam Lynch ordered Anti-Treaty IRA units to disperse and form flying columns as they had when fighting the British. They held out in areas such as the western part of counties Cork and Kerry in the south, county Wexford in the east and counties Sligo and Mayo in the west. Sporadic fighting also took place around Dundalk, where Frank Aiken and the Fourth Northern Division of the Irish Republican Army were based, and Dublin, where small-scale but regular attacks were mounted on Free State troops.

August and September 1922 saw widespread attacks on Free State forces in the territories that they had occupied in the July–August offensive, inflicting heavy casualties on them. Commander-in-Chief Michael Collins was killed in an ambush by anti-treaty Republicans at Béal na mBláth, near his home in County Cork, in August 1922. Collins' death increased the bitterness of the Free State leadership towards the Republicans and probably contributed to the subsequent descent of the conflict into a cycle of atrocities and reprisals. Arthur Griffith, the Free State president, had also died of a brain haemorrhage ten days before, leaving the Free State government in the hands of W. T. Cosgrave and the Free State army under the command of General Richard Mulcahy. For a brief period, with rising casualties among its troops and its two principal leaders dead, it looked as if the Free State might collapse.

However, as winter set in, the republicans found it increasingly difficult to sustain their campaign, and casualty rates among National Army troops dropped rapidly. For instance, in County Sligo, 54 people died in the conflict, of whom all but eight had been killed by the end of September.

In the autumn and winter of 1922, Free State forces broke up many of the larger Republican guerrilla units – in Sligo, Meath, and Connemara in the west, for example, and in much of Dublin city. Elsewhere, Anti-Treaty units were forced by lack of supplies and safe-houses to disperse into smaller groups, typically of nine to ten men. Despite these successes for the National Army, it took eight more months of intermittent warfare before the war was brought to an end.

By late 1922 and early 1923, the Anti-Treaty guerrillas' campaign had been reduced largely to acts of sabotage and destruction of public infrastructure such as roads and railways. It was also in this period that the Anti-Treaty IRA began burning the homes of Free State Senators and of many of the Anglo-Irish landed class.

In October 1922, Éamon de Valera and the anti-treaty TDs (Members of Parliament) set up their own "Republican government" in opposition to the Free State. However, by then the anti-treaty side held no significant territory and de Valera's "government" had no authority over the population. In any case, the IRA leaders paid no attention to it, seeing the Republican authority as vested in their own military leaders.

On 27 September 1922, three months after the outbreak of war, the Free State's Provisional Government put before the Dáil an Army Emergency Powers Resolution proposing legislation for setting up military tribunals, transferring most of the Free State's judicial powers over Irish citizens accused of anti-government activities to the Army Council. By instituting martial law, the first democratically elected Free State had in effect suspended most, if not all civil rights of the Irish population for the duration of the conflict. The legislation, commonly referred to as the Public Safety Bill, empowered military tribunals with the ability to impose life imprisonment, as well as the death penalty, for a variety of offences. By allowing appointed courts martial to execute any Irish citizen found in possession of firearms or ammunition, the Free State prevented Republican sympathizers from storing any arms or ammunition that could be used by Republican forces; possession of even a single sporting or civilian firearm or round of ammunition could result in execution by firing squad. Offences included attacks on state policy or military forces, donning army or police uniforms, publication of "seditious publications", and membership in the Republican Army.

The final phase of the Civil War degenerated into a series of atrocities that left a lasting legacy of bitterness in Irish politics. The Free State began executing Republican prisoners on 17 November 1922, when five IRA men were shot by firing squad. They were followed on 24 November by the execution of acclaimed author and treaty negotiator Robert Erskine Childers. In all, the Free State sanctioned 77 official executions of anti-treaty prisoners during the Civil War. The Anti-Treaty IRA in reprisal assassinated TD Seán Hales.

On 7 December 1922, the day after Hales' killing, four prominent Republicans (one from each province), who had been held since the first week of the war—Rory O'Connor, Liam Mellows, Richard Barrett and Joe McKelvey — were executed in revenge for the killing of Hales. In addition, Free State troops, particularly in County Kerry, where the guerrilla campaign was most bitter, began the summary execution of captured anti-treaty fighters. The most notorious example of this occurred at Ballyseedy, where nine Republican prisoners were tied to a landmine, which was detonated, killing eight and only leaving one, Stephen Fuller, who was blown clear by the blast, to escape.
The number of "unauthorised" executions of Republican prisoners during the war has been put as high as 153. Among the Republican reprisals were the assassination of Kevin O'Higgins' father and WT Cosgrave's uncle in February 1923.

The Anti-Treaty IRA were unable to maintain an effective guerrilla campaign, given the gradual loss of support. The Catholic Church also supported the Free State, deeming it the lawful government of the country, denouncing the Anti-Treaty IRA and refusing to administer the Sacraments to anti-treaty fighters. On 10 October 1922, the Catholic Bishops of Ireland issued a formal statement, describing the anti-treaty campaign as:

Churchmen were appalled by the ruthlessness and cruelty. The Church's support for the Free State aroused bitter hostility among some republicans. Although the Catholic Church in independent Ireland has often been seen as a triumphalist Church, a recent study has found that it felt deeply insecure after these events.

By early 1923, the offensive capability of the Anti-Treaty IRA had been seriously eroded and when, in February 1923, the Republican leader Liam Deasy was captured by Free State forces. He called on the republicans to end their campaign and reach an accommodation with the Free State. The State's executions of Anti-Treaty prisoners, 34 of whom were shot in January 1923, also took its toll on the Republicans' morale.

In addition, the National Army's operations in the field were slowly but steadily breaking up the remaining Republican concentrations.

March and April 1923 saw this progressive dismemberment of the Republican forces continue with the capture and sometimes killing of guerrilla columns. A National Army report of 11 April stated, "Events of the last few days point to the beginning of the end as a far as the irregular campaign is concerned".

As the conflict petered out into a "de facto" victory for the pro-treaty side, de Valera asked the IRA leadership to call a ceasefire, but they refused. The Anti-Treaty IRA executive met on 26 March in County Tipperary to discuss the war's future. Tom Barry proposed a motion to end the war, but it was defeated by 6 votes to 5. Éamon de Valera was allowed to attend, after some debate, but was given no voting rights.

Liam Lynch, the Republican leader, was killed in a skirmish in the Knockmealdown Mountains in County Tipperary on 10 April. The National Army had extracted information from Republican prisoners in Dublin that the IRA Executive was in the area and as well as killing Lynch, they also captured senior Anti-Treaty IRA officers Dan Breen, Todd Andrews, Seán Gaynor and Frank Barrett in the operation.

It is often suggested that the death of Lynch allowed the more pragmatic Frank Aiken, who took over as IRA Chief of Staff, to call a halt to what seemed a futile struggle. Aiken's accession to IRA leadership was followed on 30 April by the declaration of a ceasefire on behalf of the anti-treaty forces. On 24 May 1923, Aiken followed this with an order to IRA volunteers to dump arms rather than surrender them or continue a fight that they were incapable of winning.

Éamon de Valera supported the order, issuing a statement to Anti-Treaty fighters on 24 May:

The Free State government had started peace negotiations in early May, which broke down. The High Court of Justice in Ireland ruled on 31 July 1923 that a state of war no longer existed, and consequently the internment of republicans, permitted under common law only in wartime, was now illegal. Without a formal peace, holding 13,000 prisoners and worried that fighting could break out again at any time, the government enacted two Public Safety (Emergency Powers) Acts on 1 and 3 August 1923, to permit continued internment and other measures. Thousands of Anti-Treaty IRA members (including Éamon de Valera on 15 August) were arrested by the Free State forces in the weeks and months after the end of the war, when they had dumped their arms and returned home.

On 27 August 1923, a general election was held, which Cumann na nGaedheal, the pro-Free State party, won with about 40% of the first-preference vote. The Republicans, represented by Sinn Féin, won about 27% of the vote. Many of their candidates and supporters were still imprisoned before, during and after the election.

In October 1923, around 8,000 of the 12,000 Republican prisoners in Free State gaols went on a hunger strike. The strike lasted for 41 days and met little success (among those who died were Denny Barry and Andy O'Sullivan). However, most of the women prisoners were released shortly thereafter and the hunger strike helped concentrate the Republican movement on the prisoners and their associated organisations. In July, de Valera had recognised the Republican political interests lay with the prisoners and went so far as to say:

Although the cause of the Civil War was the Treaty, as the war developed the Republicans sought to identify their actions with the traditional Republican cause of the "men of no property" and the result was that large Anglo-Irish landowners and some less well-off former Protestant Loyalists were attacked. A total of 192 "stately homes" of the old landed class and Free State politicians were destroyed by Republicans during the war.

The stated reason for such attacks was that some landowners had become Free State senators. In October 1922, a deputation of Southern Unionists met W.T. Cosgrave to offer their support to the Free State and some of them had received positions in the State's Upper house or Senate. Among the prominent senators whose homes were attacked were: Palmerstown House near Naas, which belonged to the Earl of Mayo, Moore Hall in Mayo, Horace Plunkett (who had helped to establish the rural co-operative schemes), and Senator Henry Guinness (which was unsuccessful). Also burned was Marlfield House in Clonmel, the home of Senator John Philip Bagwell with its extensive library of historical documents. Bagwell was kidnapped and held in the Dublin Mountains, but later released when reprisals were threatened.

However, in addition to their allegiance to the Free State, there were also other factors behind Republican animosity towards the old landed class. Many, but not all of these people, had supported the Crown forces during the War of Independence. This support was often largely moral, but sometimes it took the form of actively assisting the British in the conflict. Such attacks should have ended with the Truce of 11 July 1921, but they continued after the truce and escalated during the Civil War. In July 1922, Con Moloney, the anti-treaty IRA's Deputy Chief of Staff, ordered that unionist property should be seized to accommodate their men. The "worst spell" of attacks on former unionist property came in the early months of 1923, 37 "big houses" being burnt in January and February alone.

Though the Wyndham Act of 1903 allowed tenants to buy land from their landlords, some small farmers, particularly in Mayo and Galway, simply occupied land belonging to political opponents during this period when the RIC had ceased to function. In 1919, senior Sinn Féin officials were sufficiently concerned at this unilateral action that they instituted Arbitration Courts to adjudicate disputes. Sometimes these attacks had sectarian overtones, although most Anti-Treaty IRA men made no distinction between Catholic and Protestant supporters of the Irish government.

Controversy continues to this day about the extent of intimidation of Protestants at this time. Many left Ireland during and after the Civil War. Dr Andy Bielenberg of UCC considers that about 41,000 who were not linked to the former British administration left Southern Ireland (which became the Irish Free State) between 1919 and 1923. He has found that a "high-water mark" of this 41,000 left between 1921 and 1923. In all, from 1911 to 1926, the Protestant population of the 26 counties fell from some 10.4% of the total population to 7.4%.

The Civil War, though short, was bloody. It cost the lives of many public figures, including Michael Collins, Cathal Brugha, Arthur Griffith and Liam Lynch. Both sides carried out brutal acts: the anti-treaty forces murdered TDs and burned many historic homes, while the government executed anti-treaty prisoners, officially and unofficially.

Precise figures for the dead and wounded have yet to be calculated. The pro-treaty forces suffered between 800–1,000 fatalities . It has been suggested that the anti-treaty forces' death toll was higher. but the Republican roll of honour, complied in the 1920s lists 426 anti-Treaty IRA Volunteers killed between January 1922 and April 1924 . 
The most recent county-by-county research suggests a death toll of just under 2,000. For total combatant and civilian deaths, a minimum of 1,500 and a maximum of 4,000 have been suggested, though the latter figure is now generally estimated to be too high. . 

The new police force was not involved in the war, which meant that it was well-placed to develop into an unarmed and politically neutral police service after the war. It had been disarmed by the Government in order to win public confidence in June–September 1922 and in December 1922, the IRA issued a General Order not to fire on the Civil Guard. The Criminal Investigation Department, or CID, a 350-strong, armed, plain-clothed Police Corps that had been established during the conflict for the purposes of counter-insurgency, was disbanded in October 1923, shortly after the conflict's end.

The economic costs of the war were also high. As their forces abandoned their fixed positions in July–August 1922, the Republicans burned many of the administrative buildings and businesses that they had been occupying. In addition, their subsequent guerrilla campaign caused much destruction and the economy of the Free State suffered a hard blow in the earliest days of its existence as a result. The material damage caused by the war to property came to over £30 million. Particularly damaging to the Free State's economy was the systematic destruction of railway infrastructure and roads by the Republicans. In addition, the cost to the Free State of waging the war came to another £17 million. By September 1923, Deputy Hogan estimated the cost at £50 million. The new State ended 1923 with a budget deficit of over £4 million. This weakened financial situation meant that the new state could not pay its share of Imperial debt under the treaty. This adversely affected the boundary negotiations in 1924–25, in which the Free State government acquiesced that border with Northern Ireland would remain unchanged in exchange for forgiveness of the Imperial debt. Further, the state undertook to pay for damage caused to property between the truce of July 1921 and the end of the Civil War; W.T. Cosgrave told the Dáil:

The fact that the Irish Civil War was fought between Irish Nationalist factions meant that the sporadic conflict in Northern Ireland ended. Collins and Sir James Craig signed an agreement to end it on 30 March 1922, but, despite this, Collins covertly supplied arms to the Northern IRA until a week before his death in August 1922. Because of the Irish Civil War, Northern Ireland was able to consolidate its existence and the partition of Ireland was confirmed for the foreseeable future. The continuing war also confirmed the northern Unionists' existing prejudices against the ethos of all shades of nationalism. This might have led to open hostilities between North and South had the Irish Civil War not broken out. Indeed, the Ulster Special Constabulary (the "B-Specials") that had been established in 1920 (on the foundation of Northern Ireland) was expanded in 1922 rather than being demobilised.

In the event, it was only well after their defeat in the Civil War that anti-treaty Irish Republicans seriously considered whether to take armed action against British rule in Northern Ireland (the first serious suggestion to do this came in the late 1930s). The northern units of the IRA largely supported the Free State side in the Civil War because of Collins's policies, and over 500 of them joined the new Free State's National Army.

The cost of the war and the budget deficit it caused was a difficulty for the new Free State and affected the Boundary Commission negotiations of 1925, which were to determine the border with Northern Ireland. The Free State agreed to waive its claim to predominantly Nationalist areas in Northern Ireland and in return its agreed share of the Imperial debt under the 1921 Treaty was not paid.
In 1926, having failed to persuade the majority of the Anti-Treaty IRA or the anti-treaty party of Sinn Féin to accept the new status quo as a basis for an evolving Republic, a large faction led by de Valera and Aiken left to resume constitutional politics and to found the Fianna Fáil party. Whereas Fianna Fáil was to become the dominant party in Irish politics, Sinn Féin became a small, isolated political party. The IRA, then much more numerous and influential than Sinn Féin, remained associated with Fianna Fáil (though not directly) until banned by de Valera in 1935.

In 1927, Fianna Fáil members took the Oath of Allegiance and entered the Dáil, effectively recognising the legitimacy of the Free State. The Free State was already moving towards independence by this point. Under the Statute of Westminster 1931, the British Parliament gave up its right to legislate for members of the British Commonwealth. When elected to power in 1932, Fianna Fáil under de Valera set about dismantling what they considered to be objectionable features of the treaty, abolishing the Oath of Allegiance, removing the power of the Office of Governor General (British representative in Ireland) and abolishing the Senate, which was dominated by former Unionists and pro-treaty Nationalists. In 1937, they passed a new constitution, which made a President the head of state, did not mention any allegiance to the British monarch, and which included a territorial claim to Northern Ireland. The following year, Britain returned without conditions the seaports that it had kept under the terms of the treaty. When the Second World War broke out in 1939, the Free State was able to demonstrate its independence by remaining neutral throughout the war, although Dublin did to some extent tacitly support the Allies. Finally, in 1948, a coalition government, containing elements of both sides in the Civil War (pro-treaty Fine Gael and anti-treaty Clann na Poblachta) left the British Commonwealth and renamed the Free State the Republic of Ireland. By the 1950s, the issues over which the Civil War had been fought were largely settled.

As with most civil wars, the internecine conflict left a bitter legacy, which continues to influence Irish politics to this day. The two largest political parties in the republic through most of its history (until the 2011 Irish General Election) were Fianna Fáil and Fine Gael, the descendants respectively of the anti-treaty and pro-treaty forces of 1922. Until the 1970s, almost all of Ireland's prominent politicians were veterans of the Civil War, a fact which poisoned the relationship between Ireland's two biggest parties. Examples of Civil War veterans include: Republicans Éamon de Valera, Frank Aiken, Todd Andrews, and Seán Lemass; and Free State supporters W. T. Cosgrave, Richard Mulcahy and Kevin O'Higgins. Moreover, many of these men's sons and daughters also became politicians, meaning that the personal wounds of the civil war were felt over three generations. In the 1930s, after Fianna Fáil took power for the first time, it looked possible for a while that the Civil War might break out again between the IRA and the pro-Free State Blueshirts. Fortunately, this crisis was averted, and by the 1950s violence was no longer prominent in politics in the Republic of Ireland.

However, the breakaway IRA continued (and continues in various forms) to exist. It was not until 1948 that the IRA renounced military attacks on the forces of the southern Irish state when it became the Republic of Ireland. After this point, the organisation dedicated itself primarily to the end of British rule in Northern Ireland. The IRA Army Council still makes claim to be the legitimate Provisional Government of the Irish Republic declared in 1918 and annulled by the Anglo-Irish Treaty of 1921.





</doc>
<doc id="15215" url="https://en.wikipedia.org/wiki?curid=15215" title="Internet Explorer">
Internet Explorer

Internet Explorer (formerly Microsoft Internet Explorer and Windows Internet Explorer, commonly abbreviated IE or MSIE) is a series of graphical web browsers developed by Microsoft and included in the Microsoft Windows line of operating systems, starting in 1995. It was first released as part of the add-on package Plus! for Windows 95 that year. Later versions were available as free downloads, or in service packs, and included in the original equipment manufacturer (OEM) service releases of Windows 95 and later versions of Windows. The browser is discontinued, but still maintained.

Internet Explorer was one of the most widely used web browsers, attaining a peak of about 95% usage share by 2003. This came after Microsoft used bundling to win the first browser war against Netscape, which was the dominant browser in the 1990s. Its usage share has since declined with the launch of Firefox (2004) and Google Chrome (2008), and with the growing popularity of operating systems such as Android and iOS that do not run Internet Explorer. Estimates for Internet Explorer's market share are about 3.12% across all platforms or by StatCounter's numbers ranked 6th, while on the only platform it's ever had significant share (i.e. excluding mobile, and not counting Xbox) it's ranked 3rd at 7.13%, just after Firefox (others place IE 2nd with 11.84% just ahead of), (browser market share is notoriously difficult to calculate). Microsoft spent over per year on Internet Explorer in the late 1990s, with over 1,000 people working on it by 1999.

Versions of Internet Explorer for other operating systems have also been produced, including an Xbox 360 version called Internet Explorer for Xbox and for platforms Microsoft no longer supports: Internet Explorer for Mac and Internet Explorer for UNIX (Solaris and HP-UX), and an embedded OEM version called Pocket Internet Explorer, later rebranded Internet Explorer Mobile made for Windows Phone, Windows CE, and previously, based on Internet Explorer 7 for Windows Mobile.

On March 17, 2015, Microsoft announced that Microsoft Edge would replace Internet Explorer as the default browser on its Windows 10 devices. This effectively makes Internet Explorer 11 the last release (however IE 10 and 9 also get security updates as of 2018). Internet Explorer, however, remains on Windows 10 primarily for enterprise purposes. Starting January 12, 2016, only Internet Explorer 11 is supported. Support varies based on the operating system's technical capabilities and its support lifecycle.

The browser has been scrutinized throughout its development for use of third-party technology (such as the source code of Spyglass Mosaic, used without royalty in early versions) and security and privacy vulnerabilities, and the United States and the European Union have alleged that integration of Internet Explorer with Windows has been to the detriment of fair browser competition.

The Internet Explorer project was started in the summer of 1994 by Thomas Reardon, who, according to the Massachusetts Institute of Technology Review of 2003, used source code from Spyglass, Inc. Mosaic, which was an early commercial web browser with formal ties to the pioneering National Center for Supercomputing Applications ("NCSA") Mosaic browser. In late 1994, Microsoft licensed Spyglass Mosaic for a quarterly fee plus a percentage of Microsoft's non-Windows revenues for the software. Although bearing a name similar to NCSA Mosaic, Spyglass Mosaic had used the NCSA Mosaic source code sparingly.

The first version, dubbed Microsoft Internet Explorer, made its debut on August 16, 1995. It was installed as part of the "Internet Jumpstart Kit" in Microsoft Plus! for Windows 95 and Plus!. The Internet Explorer team began with about six people in early development. Internet Explorer 1.5 was released several months later for Windows NT and added support for basic table rendering. By including it free of charge on their operating system, they did not have to pay royalties to Spyglass Inc, resulting in a lawsuit and a US$8 million settlement on January 22, 1997.

Microsoft was sued by Synet Inc. in 1996, over the trademark infringement.

Internet Explorer 11 is featured in a Windows 8.1 update which was released on October 17, 2013. It includes an incomplete mechanism for syncing tabs. It is a major update to its developer tools, enhanced scaling for high DPI screens, HTML5 prerender and prefetch, hardware-accelerated JPEG decoding, closed captioning, HTML5 full screen, and is the first Internet Explorer to support WebGL and Google's protocol SPDY (starting at v3). This version of IE has features dedicated to Windows 8.1, including cryptography (WebCrypto), adaptive bitrate streaming (Media Source Extensions) and Encrypted Media Extensions.

Internet Explorer 11 was made available for Windows 7 users to download on November 7, 2013, with Automatic Updates in the following weeks.

Internet Explorer 11's user agent string now identifies the agent as "Trident" (the underlying layout engine) instead of "MSIE". It also announces compatibility with Gecko (the layout engine of Firefox).

Microsoft claimed that Internet Explorer 11, running the WebKit SunSpider JavaScript Benchmark, was the fastest browser as of October 15, 2013.

Microsoft Edge, officially unveiled on January 21, 2015, has replaced Internet Explorer as the default browser on Windows 10. Internet Explorer is still installed in Windows 10 in order to maintain compatibility with older websites and intranet sites that require ActiveX and other Microsoft legacy web technologies.

According to Microsoft, development of new features for Internet Explorer has ceased. However, it will continue to be maintained as part of the support policy for the versions of Windows with which it is included.

Internet Explorer has been designed to view a broad range of web pages and provide certain features within the operating system, including Microsoft Update. During the heyday of the browser wars, Internet Explorer superseded Netscape only when it caught up technologically to support the progressive features of the time.

Internet Explorer, using the Trident layout engine:


Internet Explorer uses DOCTYPE sniffing to choose between standards mode and a "quirks mode" in which it deliberately mimicks nonstandard behaviours of old versions of MSIE for HTML and CSS rendering on screen (Internet Explorer always uses standards mode for printing). It also provides its own dialect of ECMAScript called JScript.

Internet Explorer was criticised by Tim Berners-Lee for its limited support for SVG which is promoted by W3C.

Internet Explorer has introduced an array of proprietary extensions to many of the standards, including HTML, CSS, and the DOM. This has resulted in a number of web pages that appear broken in standards-compliant web browsers and has introduced the need for a "quirks mode" to allow for rendering improper elements meant for Internet Explorer in these other browsers.

Internet Explorer has introduced a number of extensions to the DOM that have been adopted by other browsers. These include the innerHTML property, which provides access to the HTML string within an element; the XMLHttpRequest object, which allows the sending of HTTP request and receiving of HTTP response, and may be used to perform AJAX; and the designMode attribute of the contentDocument object, which enables rich text editing of HTML documents. Some of these functionalities were not possible until the introduction of the W3C DOM methods. Its Ruby character extension to HTML is also accepted as a module in W3C XHTML 1.1, though it is not found in all versions of W3C HTML.

Microsoft submitted several other features of IE for consideration by the W3C for standardization. These include the 'behaviour' CSS property, which connects the HTML elements with JScript behaviours (known as HTML Components, HTC); HTML+TIME profile, which adds timing and media synchronization support to HTML documents (similar to the W3C XHTML+SMIL), and the VML vector graphics file format. However, all were rejected, at least in their original forms; VML was subsequently combined with PGML (proposed by Adobe and Sun), resulting in the W3C-approved SVG format, one of the few vector image formats being used on the web, which IE did not support until version 9.

Other non-standard behaviours include: support for vertical text, but in a syntax different from W3C CSS3 candidate recommendation, support for a variety of image effects and page transitions, which are not found in W3C CSS, support for obfuscated script code, in particular JScript.Encode. Support for embedding EOT fonts in web pages.

Support for favicons was first added in Internet Explorer 5. Internet Explorer supports favicons in PNG, static GIF and native Windows icon formats. In Windows Vista and later, Internet Explorer can display native Windows icons that have embedded PNG files.

Internet Explorer makes use of the accessibility framework provided in Windows. Internet Explorer is also a user interface for FTP, with operations similar to that of Windows Explorer. Pop-up blocking and tabbed browsing were added respectively in Internet Explorer 6 and Internet Explorer 7. Tabbed browsing can also be added to older versions by installing MSN Search Toolbar or Yahoo Toolbar.

Internet Explorer caches visited content in the Temporary Internet Files folder to allow quicker access (or offline access) to previously visited pages. The content is indexed in a database file, known as Index.dat. Multiple Index.dat files exist which index different content—visited content, web feeds, visited URLs, cookies, etc.

Prior to IE7, clearing the cache used to clear the index but the files themselves were not reliably removed, posing a potential security and privacy risk. In IE7 and later, when the cache is cleared, the cache files are more reliably removed, and the index.dat file is overwritten with null bytes.

Caching has been improved in IE9.

Internet Explorer is fully configurable using Group Policy. Administrators of Windows Server domains (for domain-joined computers) or the local computer can apply and enforce a variety of settings on computers that affect the user interface (such as disabling menu items and individual configuration options), as well as underlying security features such as downloading of files, zone configuration, per-site settings, ActiveX control behaviour and others. Policy settings can be configured for each user and for each machine. Internet Explorer also supports Integrated Windows Authentication.

Internet Explorer uses a componentized architecture built on the Component Object Model (COM) technology. It consists of several major components, each of which is contained in a separate Dynamic-link library (DLL) and exposes a set of COM programming interfaces hosted by the Internet Explorer main executable, iexplore.exe:

Internet Explorer does not include any native scripting functionality. Rather, MSHTML.dll exposes an API that permits a programmer to develop a scripting environment to be plugged-in and to access the DOM tree. Internet Explorer 8 includes the bindings for the Active Scripting engine, which is a part of Microsoft Windows and allows any language implemented as an Active Scripting module to be used for client-side scripting. By default, only the JScript and VBScript modules are provided; third party implementations like ScreamingMonkey (for ECMAScript 4 support) can also be used. Microsoft also makes available the Microsoft Silverlight runtime (not supported in Windows RT) that allows CLI languages, including DLR-based dynamic languages like IronPython and IronRuby, to be used for client-side scripting.

Internet Explorer 8 introduces some major architectural changes, called "Loosely Coupled IE" (LCIE). LCIE separates the main window process (frame process) from the processes hosting the different web applications in different tabs (tab processes). A frame process can create multiple tab processes, each of which can be of a different integrity level; each tab process can host multiple web sites. The processes use asynchronous Inter-Process Communication to synchronize themselves. Generally, there will be a single frame process for all web sites. In Windows Vista with Protected Mode turned on, however, opening privileged content (such as local HTML pages) will create a new tab process as it will not be constrained by Protected Mode.

Internet Explorer exposes a set of Component Object Model (COM) interfaces that allows add-ons to extend the functionality of the browser. Extensibility is divided into two types: Browser extensibility and content extensibility. Browser extensibility involves adding context menu entries, toolbars, menu items or Browser Helper Objects (BHO). BHOs are used to extend the feature set of the browser, whereas the other extensibility options are used to expose that feature in the user interface. Content extensibility adds support for non-native content formats. It allows Internet Explorer to handle new file formats and new protocols, e.g. WebM or SPDY. In addition, web pages can integrate widgets known as ActiveX controls which run on Windows only but have vast potentials to extend the content capabilities; Adobe Flash Player and Microsoft Silverlight are examples. Add-ons can be installed either locally, or directly by a web site.

Since malicious add-ons can compromise the security of a system, Internet Explorer implements several safeguards. Internet Explorer 6 with Service Pack 2 and later feature an Add-on Manager for enabling or disabling individual add-ons, complemented by a "No Add-Ons" mode. Starting with Windows Vista, Internet Explorer and its BHOs run with restricted privileges and are isolated from the rest of the system. Internet Explorer 9 introduced a new component – Add-on Performance Advisor. Add-on Performance Advisor shows a notification when one or more of installed add-ons exceed a pre-set performance threshold. The notification appears in the Notification Bar when the user launches the browser. Windows 8 and Windows RT introduce a Metro-style version of Internet Explorer that is entirely sandboxed and does not run add-ons at all. In addition, Windows RT cannot download or install ActiveX controls at all; although existing ones bundled with Windows RT still run in the traditional version of Internet Explorer.

Internet Explorer itself can be hosted by other applications via a set of COM interfaces. This can be used to embed the browser functionality inside a computer program or create Internet Explorer shells.

Internet Explorer uses a zone-based security framework that groups sites based on certain conditions, including whether it is an Internet- or intranet-based site as well as a user-editable whitelist. Security restrictions are applied per zone; all the sites in a zone are subject to the restrictions.

Internet Explorer 6 SP2 onwards uses the "Attachment Execution Service" of Microsoft Windows to mark executable files downloaded from the Internet as being potentially unsafe. Accessing files marked as such will prompt the user to make an explicit trust decision to execute the file, as executables originating from the Internet can be potentially unsafe. This helps in preventing accidental installation of malware.

Internet Explorer 7 introduced the phishing filter, that restricts access to phishing sites unless the user overrides the decision. With version 8, it also blocks access to sites known to host malware. Downloads are also checked to see if they are known to be malware-infected.

In Windows Vista, Internet Explorer by default runs in what is called "Protected Mode", where the privileges of the browser itself are severely restricted—it cannot make any system-wide changes. One can optionally turn this mode off but this is not recommended. This also effectively restricts the privileges of any add-ons. As a result, even if the browser or any add-on is compromised, the damage the security breach can cause is limited.

Patches and updates to the browser are released periodically and made available through the Windows Update service, as well as through Automatic Updates. Although security patches continue to be released for a range of platforms, most feature additions and security infrastructure improvements are only made available on operating systems which are in Microsoft's mainstream support phase.

On December 16, 2008, Trend Micro recommended users switch to rival browsers until an emergency patch was released to fix a potential security risk which "could allow outside users to take control of a person's computer and steal their passwords". Microsoft representatives countered this recommendation, claiming that "0.02% of internet sites" were affected by the flaw. A fix for the issue was released the following day with the Security Update for Internet Explorer KB960714, on Microsoft Windows Update. 

In 2011, a report by Accuvant, funded by Google, rated the security (based on sandboxing) of Internet Explorer worse than Google Chrome but better than Mozilla Firefox. 

A more recent browser security white paper comparing Google Chrome, Microsoft Edge, and Internet Explorer 11 by X41 D-Sec in 2017 came to similar conclusions, also based on sandboxing and support of legacy web technologies.

Internet Explorer has been subjected to many security vulnerabilities and concerns: much of the spyware, adware, and computer viruses across the Internet are made possible by exploitable bugs and flaws in the security architecture of Internet Explorer, sometimes requiring nothing more than viewing of a malicious web page in order to install themselves. This is known as a "drive-by install". There are also attempts to trick the user into installing malicious software by misrepresenting the software's true purpose in the description section of an ActiveX security alert.

A number of security flaws affecting IE originated not in the browser itself, but ActiveX-based add-ons used by it. Because the add-ons have the same privilege as IE, the flaws can be as critical as browser flaws. This has led to the ActiveX-based architecture being criticized for being fault-prone. By 2005, some experts maintained that the dangers of ActiveX have been overstated and there were safeguards in place. In 2006, new techniques using automated testing found more than a hundred vulnerabilities in standard Microsoft ActiveX components. Security features introduced in Internet Explorer 7 mitigated some of these vulnerabilities.

Internet Explorer in 2008, had a number of published security vulnerabilities. According to research done by security research firm Secunia, Microsoft did not respond as quickly as its competitors in fixing security holes and making patches available. The firm also reported 366 vulnerabilities in ActiveX controls, an increase from the prior year.

According to an October 2010 report in "The Register", researcher Chris Evans had detected a known security vulnerability which, then dating back to 2008, had not been fixed for at least 600 days. Microsoft says that it had known about this vulnerability but it was of very low severity as the victim web site must be configured in a special way for this attack to be feasible at all.

In December 2010, researchers were able to bypass the "Protected Mode" feature in Internet Explorer.

In an advisory on January 14, 2010, Microsoft said that attackers targeting Google and other U.S. companies used software that exploits a security hole, which had already been patched, in Internet Explorer. The vulnerability affected Internet Explorer 6 on Windows XP and Server 2003, IE6 SP1 on Windows 2000 SP4, IE7 on Windows Vista, XP, Server 2008 and Server 2003, and IE8 on Windows 7, Vista, XP, Server 2003, and Server 2008 (R2).

The German government warned users against using Internet Explorer and recommended switching to an alternative web browser, due to the major security hole described above that was exploited in Internet Explorer. The Australian and French Government issued a similar warning a few days later.

On April 26, 2014, Microsoft issued a security advisory relating to CVE-2014-1776 (use-after-free vulnerability in Microsoft Internet Explorer 6 through 11), a vulnerability that could allow "remote code execution" in Internet Explorer versions 6 to 11. On April 28, 2014, the United States Department of Homeland Security's United States Computer Emergency Readiness Team (US-CERT) released an advisory stating that the vulnerability could result in "the complete compromise" of an affected system. US-CERT recommended reviewing Microsoft's suggestions to mitigate an attack or using an alternate browser until the bug is fixed. The UK National Computer Emergency Response Team (CERT-UK) published an advisory announcing similar concerns and for users to take the additional step of ensuring their antivirus software is up-to-date. Symantec, a cyber security firm, confirmed that "the vulnerability crashes Internet Explorer on Windows XP". The vulnerability was resolved on May 1, 2014, with a security update.

The adoption rate of Internet Explorer seems to be closely related to that of Microsoft Windows, as it is the default web browser that comes with Windows. Since the integration of Internet Explorer 2.0 with Windows 95 OSR 1 in 1996, and especially after version 4.0's release in 1997, the adoption was greatly accelerated: from below 20% in 1996, to about 40% in 1998, and over 80% in 2000. This made Microsoft the winner in the infamous 'first browser war' against Netscape. Netscape Navigator was the dominant browser during 1995 and until 1997, but rapidly lost share to IE starting in 1998, and eventually slipped behind in 1999. The integration of IE with Windows led to a lawsuit by AOL, Netscape's owner, accusing Microsoft of unfair competition. The infamous case was eventually won by AOL but by then it was too late, as Internet Explorer had already become the dominant browser.

Internet Explorer peaked during 2002 and 2003, with about 95% share. Its first notable competitor after beating Netscape was Firefox from Mozilla, which itself was an offshoot from Netscape.

Firefox 1.0 had surpassed Internet Explorer 5 in early 2005, with Firefox 1.0 at roughly 8 percent market share.

Approximate usage over time based on various usage share counters averaged for the year overall, or for the fourth quarter, or for the last month in the year depending on availability of reference.

According to StatCounter Internet Explorer's marketshare fell below 50% in September 2010. In May 2012, it was announced that Google Chrome overtook Internet Explorer as the most used browser worldwide. 
Browser Helper Objects are also used by many search engine companies and third parties for creating add-ons that access their services, such as search engine toolbars. Because of the use of COM, it is possible to embed web-browsing functionality in third-party applications. Hence, there are a number of Internet Explorer shells, and a number of content-centric applications like RealPlayer also use Internet Explorer's web browsing module for viewing web pages within the applications.

While a major upgrade of Internet Explorer can be uninstalled in a traditional way if the user has saved the original application files for installation, the matter of uninstalling the version of the browser that has shipped with an operating system remains a controversial one.

The idea of removing a stock install of Internet Explorer from a Windows system was proposed during the "United States v. Microsoft Corp." case. One of Microsoft's arguments during the trial was that removing Internet Explorer from Windows may result in system instability. Indeed, programs that depend on libraries installed by IE, including Windows help and support system, fail to function without IE. Before Windows Vista, it was not possible to run Windows Update without IE because the service used ActiveX technology, which no other web browser supports.

The popularity of Internet Explorer has led to the appearance of malware abusing its name. On January 28, 2011, a fake Internet Explorer browser calling itself "Internet Explorer – Emergency Mode" appeared. It closely resembles the real Internet Explorer, but has fewer buttons and no search bar. If a user launches any other browser such as Google Chrome, Mozilla Firefox, Opera, Safari or the real Internet Explorer, this browser will pop-up instead. It also displays a fake error message, claiming that the computer is infected with malware and Internet Explorer has entered Emergency Mode. It blocks access to legitimate sites such as Google if the user tries to access them.





</doc>
<doc id="15220" url="https://en.wikipedia.org/wiki?curid=15220" title="Imprecise language">
Imprecise language

Often, informal, spoken language, "everyday language" is less precise than any more formal or academic languages.

Language might be said to be imprecise because it exhibits one or more of the following features:

While imprecise language is not desirable in various scientific fields, it may be helpful, illustrative or discussion-stimulative in other contexts. Imprecision in a discourse may or may not be the intention of the author(s) or speaker(s). The role of imprecision may depend on audience, end goal, extended context and subject matter. Relevant players and real stakes will also bear on truth-grounds of statements.


</doc>
<doc id="15221" url="https://en.wikipedia.org/wiki?curid=15221" title="Intel 80188">
Intel 80188

The Intel 80188 microprocessor was a variant of the Intel 80186. The 80188 had an 8-bit external data bus instead of the 16-bit bus of the 80186; this made it less expensive to connect to peripherals. The 16-bit registers and the one megabyte address range were unchanged, however. It had a throughput of 1 million instructions per second.

The 80188 series was generally intended for embedded systems, as microcontrollers with external memory. Therefore, to reduce the number of chips required, it included features such as clock generator, interrupt controller, timers, wait state generator, DMA channels, and external chip select lines.
While the N80188 was compatible with the 8087 numerics co-processor, the 80C188 was not. It didn't have the ESC control codes integrated.

The initial clock rate of the 80188 was 6 MHz, but due to more hardware available for the microcode to use, especially for address calculation, many individual instructions ran faster than on an 8086 at the same clock frequency. For instance, the common "register+immediate" addressing mode was significantly faster than on the 8086, especially when a memory location was both (one of the) operand(s) and the destination. Multiply and divide also showed great improvement, being several times as fast as on the original 8086 and multi-bit shifts were done almost four times as quickly as in the 8086.

Along with hundreds of other processor models, Intel discontinued the 80188 processor 30 March 2006, after a life of about 24 years.



</doc>
<doc id="15222" url="https://en.wikipedia.org/wiki?curid=15222" title="IEEE 802.2">
IEEE 802.2

IEEE 802.2 is the original name of the ISO/IEC 8802-2 standard which defines logical link control (LLC) as the upper portion of the data link layer of the OSI Model. The original standard developed by the Institute of Electrical and Electronics Engineers (IEEE) in collaboration with the American National Standards Institute (ANSI) was adopted by the International Organization for Standardization (ISO) in 1998, but it still remains an integral part of the family of IEEE 802 standards for local and metropolitan networks.

LLC is a software component that provides a uniform interface to the user of the data link service, usually the network layer. LLC may offer three types of services:

Conversely, the LLC uses the services of the media access control (MAC), which is dependent on the specific transmission medium (Ethernet, Token Ring, FDDI, 802.11, etc.). Using LLC is compulsory for all IEEE 802 networks with the exception of Ethernet. It is also used in Fiber Distributed Data Interface (FDDI) which is not part of the IEEE 802 family.

The IEEE 802.2 sublayer adds some control information to the message created by the upper layer and passed to the LLC for transmission to another node on the same data link. The resulting packet is generally referred to as "LLC protocol data unit (PDU)" and the additional information added by the LLC sublayer is the "LLC HEADER". The LLC Header consist of "DSAP" ("Destination Service Access Point"), "SSAP" ("Source Service Access Point") and the "Control" field.

The two 8-bit fields DSAP and SSAP allow to multiplex various upper layer protocols above LLC. However, many protocols use the Subnetwork Access Protocol (SNAP) extension which allows using EtherType values to specify the protocol being transported atop IEEE 802.2. It also allows vendors to define their own protocol value spaces.

The 8 or 16 bit HDLC-style Control field serves to distinguish communication mode, to specify a specific operation and to facilitate connection control and flow control (in connection mode) or acknowledgements (in acknowledged connectionless mode).

IEEE 802.2 provides two connectionless and one connection-oriented operational modes:
The use of multicasts and broadcasts reduce network traffic when the same information needs to be propagated to all stations of the network. However the Type 1 service provides no guarantees regarding the order of the received frames compared to the order in which they have been sent; the sender does not even get an acknowledgment that the frames have been received.

Each device conforming the IEEE 802.2 standard must support service type 1. Each network node is assigned an LLC Class according to which service types it supports:

Any 802.2 LLC PDU has the following format:
When Subnetwork Access Protocol (SNAP) extension is used, it is located at the start of the Information field:
The 802.2 header includes two eight-bit address fields, called service access points (SAP) or collectively LSAP in the OSI terminology:

Although the LSAP fields are 8 bits long, the low-order bit is reserved for special purposes, leaving only 128 values available for most purposes.

The low-order bit of the DSAP indicates whether it contains an individual or a group address:

The low-order bit of the SSAP indicates whether the packet is a command or response packet:
The remaining 7 bits of the SSAP specify the LSAP (always an individual address) from which the packet was transmitted.

LSAP numbers are globally assigned by the IEEE to uniquely identify well established international standards.

The protocols or families of protocols which have assigned one or more SAPs may operate directly on top of 802.2 LLC. Other protocols may use the Subnetwork Access Protocol (SNAP) with IEEE 802.2 which is indicated by the hexadecimal value 0xAA (or 0xAB, if the source of a response) in SSAP and DSAP. The SNAP extension allows using EtherType values or private protocol ID spaces in all IEEE 802 networks. It can be used both in datagram and in connection-oriented network services.

Ethernet (IEEE 802.3) networks are an exception; the IEEE 802.3x-1997 standard explicitly allowed using of the Ethernet II framing, where the 16-bit field after the MAC addresses does not carry the length of the frame followed by the IEEE 802.2 LLC header, but the EtherType value followed by the upper layer data. With this framing only datagram services are supported on the data link layer.

Although IPv4 has been assigned an LSAP value of 6 (0x6) and ARP has been assigned an LSAP value of 152 (0x98), IPv4 is almost never directly encapsulated in 802.2 LLC frames without SNAP headers. Instead, the Internet standard RFC 1042 is usually used for encapsulating IPv4 traffic in 802.2 LLC frames with SNAP headers on FDDI and on IEEE 802 networks other than Ethernet. Ethernet networks typically use Ethernet II framing with EtherType 0x800 for IP and 0x806 for ARP.

The IPX protocol used by Novell NetWare networks supports an additional Ethernet frame type, 802.3 raw, ultimately supporting four frame types on Ethernet (802.3 raw, 802.2 LLC, 802.2 SNAP, and Ethernet II) and two frame types on FDDI and other (non-Ethernet) IEEE 802 networks (802.2 LLC and 802.2 SNAP).

It is possible to use diverse framings on a single network. It is possible to do it even for the same upper layer protocol, but in such a case the nodes using unlike framings cannot directly communicate with each other.

Following the destination and source SAP fields is a control field. IEEE 802.2 was conceptually derived from HDLC, and has the same three types of PDUs:

To carry data in the most-often used unacknowledged connectionless mode the U-format is used. It is identified by the value '11' in lower two bits of the single-byte control field.


</doc>
<doc id="15223" url="https://en.wikipedia.org/wiki?curid=15223" title="Invertebrate">
Invertebrate

Invertebrates are animals that neither possess nor develop a vertebral column (commonly known as a "backbone" or "spine"), derived from the notochord. This includes all animals apart from the subphylum Vertebrata. Familiar examples of invertebrates include insects; crabs, lobsters and their kin; snails, clams, octopuses and their kin; starfish, sea-urchins and their kin; jellyfish, and worms.

The majority of animal species are invertebrates; one estimate puts the figure at 97%. Many invertebrate taxa have a greater number and variety of species than the entire subphylum of Vertebrata.

Some of the so-called invertebrates, such as the Tunicata and Cephalochordata are more closely related to the vertebrates than to other invertebrates. This makes the invertebrates paraphyletic, so the term has little meaning in taxonomy.

The word "invertebrate" comes from the form of the Latin word "vertebra", which means a joint in general, and sometimes specifically a joint from the spinal column of a vertebrate. In turn the jointed aspect of "vertebra" derived from the concept of turning, expressed in the root "verto" or "vorto", to turn. Coupled with the prefix "in-", meaning "not" or "without".

The term "invertebrates" is not always precise among non-biologists since it does not accurately describe a taxon in the same way that Arthropoda, Vertebrata or Manidae do. Each of these terms describes a valid taxon, phylum, subphylum or family. "Invertebrata" is a term of convenience, not a taxon; it has very little circumscriptional significance except within the Chordata. The Vertebrata as a subphylum comprises such a small proportion of the Metazoa that to speak of the kingdom Animalia in terms of "Vertebrata" and "Invertebrata" has limited practicality. In the more formal taxonomy of Animalia other attributes that logically should precede the presence or absence of the vertebral column in constructing a cladogram, for example, the presence of a notochord. That would at least circumscribe the Chordata. However, even the notochord would be a less fundamental criterion than aspects of embryological development and symmetry or perhaps bauplan.

Despite this, the concept of "invertebrates" as a taxon of animals has persisted for over a century among the laity, and within the zoological community and in its literature it remains in use as a term of convenience for animals that are not members of the Vertebrata. The following text reflects earlier scientific understanding of the term and of those animals which have constituted it. According to this understanding, invertebrates do not possess a skeleton of bone, either internal or external. They include hugely varied body plans. Many have fluid-filled, hydrostatic skeletons, like jellyfish or worms. Others have hard exoskeletons, outer shells like those of insects and crustaceans. The most familiar invertebrates include the Protozoa, Porifera, Coelenterata, Platyhelminthes, Nematoda, Annelida, Echinodermata, Mollusca and Arthropoda. Arthropoda include insects, crustaceans and arachnids.

By far the largest number of described invertebrate species are insects. The following table lists the number of described extant species for major invertebrate groups as estimated in the IUCN Red List of Threatened Species", 2014.3.
The IUCN estimates that 66,178 extant vertebrate species have been described, which means that over 95% of the described animal species in the world are invertebrates.

The trait that is common to all invertebrates is the absence of a vertebral column (backbone): this creates a distinction between invertebrates and vertebrates. The distinction is one of convenience only; it is not based on any clear biologically homologous trait, any more than the common trait of having wings functionally unites insects, bats, and birds, or than not having wings unites tortoises, snails and sponges. Being animals, invertebrates are heterotrophs, and require sustenance in the form of the consumption of other organisms. With a few exceptions, such as the Porifera, invertebrates generally have bodies composed of differentiated tissues. There is also typically a digestive chamber with one or two openings to the exterior.

The body plans of most multicellular organisms exhibit some form of symmetry, whether radial, bilateral, or spherical. A minority, however, exhibit no symmetry. One example of asymmetric invertebrates include all gastropod species. This is easily seen in snails and sea snails, which have helical shells. Slugs appear externally symmetrical, but their pneumostome (breathing hole) is located on the right side. Other gastropods develop external asymmetry, such as Glaucus atlanticus that develops asymmetrical cerata as they mature. The origin of gastropod asymmetry is a subject of scientific debate.

Other examples of asymmetry are found in fiddler crabs and hermit crabs. They often have one claw much larger than the other. If a male fiddler loses its large claw, it will grow another on the opposite side after moulting. Sessile animals such as sponges are asymmetrical alongside coral colonies (with the exception of the individual polyps that exhibit radial symmetry); alpheidae claws that lack pincers; and some copepods, polyopisthocotyleans, and monogeneans which parasitize by attachment or residency within the gill chamber of their fish hosts).

Neurons differ in invertebrates from mammalian cells. Invertebrates cells fire in response to similar stimuli as mammals, such as tissue trauma, high temperature, or changes in pH. The first invertebrate in which a neuron cell was identified was the medicinal leech, "Hirudo medicinalis."

Learning and memory using nociceptors in the sea hare, "Aplysia" has been described. Mollusk neurons are able to detect increasing pressures and tissue trauma.

Neurons have been identified in a wide range of invertebrate species, including annelids, molluscs, nematodes and arthropods.

One type of invertebrate respiriatory system is the open respiratory system composed of spiracles, tracheae, and tracheoles that terrestrial arthropods have to transport metabolic gases to and from tissues. The distribution of spiracles can vary greatly among the many orders of insects, but in general each segment of the body can have only one pair of spiracles, each of which connects to an atrium and has a relatively large tracheal tube behind it. The tracheae are invaginations of the cuticular exoskeleton that branch (anastomose) throughout the body with diameters from only a few micrometres up to 0.8 mm. The smallest tubes, tracheoles, penetrate cells and serve as sites of diffusion for water, oxygen, and carbon dioxide. Gas may be conducted through the respiratory system by means of active ventilation or passive diffusion. Unlike vertebrates, insects do not generally carry oxygen in their haemolymph.

A tracheal tube may contain ridge-like circumferential rings of taenidia in various geometries such as loops or helices. In the head, thorax, or abdomen, tracheae may also be connected to air sacs. Many insects, such as grasshoppers and bees, which actively pump the air sacs in their abdomen, are able to control the flow of air through their body. In some aquatic insects, the tracheae exchange gas through the body wall directly, in the form of a gill, or function essentially as normal, via a plastron. Note that despite being internal, the tracheae of arthropods are shed during moulting (ecdysis).

Like vertebrates, most invertebrates reproduce at least partly through sexual reproduction. They produce specialized reproductive cells that undergo meiosis to produce smaller, motile spermatozoa or larger, non-motile ova. These fuse to form zygotes, which develop into new individuals. Others are capable of asexual reproduction, or sometimes, both methods of reproduction.

Social behavior is widespread in invertebrates, including cockroaches, termites, aphids, thrips, ants, bees, Passalidae, Acari, spiders, and more. Social interaction is particularly salient in eusocial species but applies to other invertebrates as well.

Insects recognize information transmitted by other insects.

The term invertebrates covers several phyla. One of these are the sponges (Porifera). They were long thought to have diverged from other animals early. They lack the complex organization found in most other phyla. Their cells are differentiated, but in most cases not organized into distinct tissues. Sponges typically feed by drawing in water through pores. Some speculate that sponges are not so primitive, but may instead be secondarily simplified. The Ctenophora and the Cnidaria, which includes sea anemones, corals, and jellyfish, are radially symmetric and have digestive chambers with a single opening, which serves as both the mouth and the anus. Both have distinct tissues, but they are not organized into organs. There are only two main germ layers, the ectoderm and endoderm, with only scattered cells between them. As such, they are sometimes called diploblastic.

The Echinodermata are radially symmetric and exclusively marine, including starfish (Asteroidea), sea urchins, (Echinoidea), brittle stars (Ophiuroidea), sea cucumbers (Holothuroidea) and feather stars (Crinoidea).

The largest animal phylum is also included within invertebrates: the Arthropoda, including insects, spiders, crabs, and their kin. All these organisms have a body divided into repeating segments, typically with paired appendages. In addition, they possess a hardened exoskeleton that is periodically shed during growth. Two smaller phyla, the Onychophora and Tardigrada, are close relatives of the arthropods and share these traits. The Nematoda or roundworms, are perhaps the second largest animal phylum, and are also invertebrates. Roundworms are typically microscopic, and occur in nearly every environment where there is water. A number are important parasites. Smaller phyla related to them are the Kinorhyncha, Priapulida, and Loricifera. These groups have a reduced coelom, called a pseudocoelom. Other invertebrates include the Nemertea or ribbon worms, and the Sipuncula.

Another phylum is Platyhelminthes, the flatworms. These were originally considered primitive, but it now appears they developed from more complex ancestors. Flatworms are acoelomates, lacking a body cavity, as are their closest relatives, the microscopic Gastrotricha. The Rotifera or rotifers, are common in aqueous environments. Invertebrates also include the Acanthocephala or spiny-headed worms, the Gnathostomulida, Micrognathozoa, and the Cycliophora.

Also included are two of the most successful animal phyla, the Mollusca and Annelida. The former, which is the second-largest animal phylum by number of described species, includes animals such as snails, clams, and squids, and the latter comprises the segmented worms, such as earthworms and leeches. These two groups have long been considered close relatives because of the common presence of trochophore larvae, but the annelids were considered closer to the arthropods because they are both segmented. Now, this is generally considered convergent evolution, owing to many morphological and genetic differences between the two phyla.

Among lesser phyla of invertebrates are the Hemichordata, or acorn worms, and the Chaetognatha, or arrow worms. Other phyla include Acoelomorpha, Brachiopoda, Bryozoa, Entoprocta, Phoronida, and Xenoturbellida.

Invertebrates can be classified into several main categories, some of which are taxonomically obsolescent or debatable, but still used as terms of convenience. Each however appears in its own article at the following links.


The earliest animal fossils appear to be those of invertebrates. 665-million-year-old fossils in the Trezona Formation at Trezona Bore, West Central Flinders, South Australia have been interpreted as being early sponges. Some paleontologists suggest that animals appeared much earlier, possibly as early as 1 billion years ago. Trace fossils such as tracks and burrows found in the Tonian era indicate the presence of triploblastic worms, like metazoans, roughly as large (about 5 mm wide) and complex as earthworms.

Around 453 MYA, animals began diversifying, and many of the important groups of invertebrates diverged from one another. Fossils of invertebrates are found in various types of sediment from the Phanerozoic. Fossils of invertebrates are commonly used in stratigraphy.

Carl Linnaeus divided these animals into only two groups, the Insecta and the now-obsolete Vermes (worms). Jean-Baptiste Lamarck, who was appointed to the position of "Curator of Insecta and Vermes" at the Muséum National d'Histoire Naturelle in 1793, both coined the term "invertebrate" to describe such animals, and divided the original two groups into ten, by splitting Arachnida and Crustacea from the Linnean Insecta, and Mollusca, Annelida, Cirripedia, Radiata, Coelenterata and Infusoria from the Linnean Vermes. They are now classified into over 30 phyla, from simple organisms such as sea sponges and flatworms to complex animals such as arthropods and molluscs.

Invertebrates are animals "without" a vertebral column. This has led to the conclusion that "in"vertebrates are a group that deviates from the normal, vertebrates. This has been said to be because researchers in the past, such as Lamarck, viewed vertebrates as a "standard": in Lamarck's theory of evolution, he believed that characteristics acquired through the evolutionary process involved not only survival, but also progression toward a "higher form", to which humans and vertebrates were closer than invertebrates were. Although goal-directed evolution has been abandoned, the distinction of invertebrates and vertebrates persists to this day, even though the grouping has been noted to be "hardly natural or even very sharp." Another reason cited for this continued distinction is that Lamarck created a precedent through his classifications which is now difficult to escape from. It is also possible that some humans believe that, they themselves being vertebrates, the group deserves more attention than invertebrates. In any event, in the 1968 edition of "Invertebrate Zoology", it is noted that "division of the Animal Kingdom into vertebrates and invertebrates is artificial and reflects human bias in favor of man's own relatives." The book also points out that the group lumps a vast number of species together, so that no one characteristic describes all invertebrates. In addition, some species included are only remotely related to one another, with some more related to vertebrates than other invertebrates (see Paraphyly).

For many centuries, invertebrates were neglected by biologists, in favor of big vertebrates and "useful" or charismatic species. Invertebrate biology was not a major field of study until the work of Linnaeus and Lamarck in the 18th century. During the 20th century, invertebrate zoology became one of the major fields of natural sciences, with prominent discoveries in the fields of medicine, genetics, palaeontology, and ecology. The study of invertebrates has also benefited law enforcement, as arthropods, and especially insects, were discovered to be a source of information for forensic investigators.

Two of the most commonly studied model organisms nowadays are invertebrates: the fruit fly "Drosophila melanogaster" and the nematode "Caenorhabditis elegans". They have long been the most intensively studied model organisms, and were among the first life-forms to be genetically sequenced. This was facilitated by the severely reduced state of their genomes, but many genes, introns, and linkages have been lost. Analysis of the starlet sea anemone genome has emphasised the importance of sponges, placozoans, and choanoflagellates, also being sequenced, in explaining the arrival of 1500 ancestral genes unique to animals. Invertebrates are also used by scientists in the field of aquatic biomonitoring to evaluate the effects of water pollution and climate change.





</doc>
<doc id="15225" url="https://en.wikipedia.org/wiki?curid=15225" title="Ivar Aasen">
Ivar Aasen

Ivar Andreas Aasen (; 5 August 1813 – 23 September 1896) was a Norwegian philologist, lexicographer, playwright, and poet. He is best known for having assembled from dialects one of the two official written versions of the Norwegian language, Nynorsk.

Aasen was born at Åsen in Ørsta (then Ørsten), in the district of Sunnmøre, on the west coast of Norway. His father, a peasant with a small farm, Ivar Jonsson, died in 1826. The younger Ivar was brought up to farmwork, but he assiduously cultivated all his leisure in reading. An early interest of his was botany. When he was eighteen, he opened an elementary school in his native parish. In 1833 he entered the household of Hans Conrad Thoresen, the husband of the eminent writer Magdalene Thoresen, in Herøy (then Herø), and there he picked up the elements of Latin. Gradually, and by dint of infinite patience and concentration, the young peasant mastered many languages, and began the scientific study of their structure. Ivar single-handedly created a new language for Norway to become the "literary" language.

About 1846 he had freed himself from all the burden of manual labour, and could occupy his thoughts with the dialect of his native district, Sunnmøre; his first publication was a small collection of folk songs in the Sunnmøre dialect (1843). His remarkable abilities now attracted general attention, and he was helped to continue his studies undisturbed. His "Grammar of the Norwegian Dialects" (, 1848) was the result of much labour, and of journeys taken to every part of the country. Aasen's famous "Dictionary of the Norwegian Dialects" () appeared in its original form in 1850, and from this publication dates all the wide cultivation of the popular language in Norwegian, since Aasen really did no less than construct, out of the different materials at his disposal, a popular language or definite "folke-maal" (people's language) for Norway. By 1853, he had created the norm for utilizing his new language, which he called Landsmaal, meaning country language. With certain modifications, the most important of which were introduced later by Aasen himself, but also through a latter policy aiming to merge this Norwegian language with Dano-Norwegian, this language has become "Nynorsk" ("New Norwegian"), the second of Norway's two official languages (the other being "Bokmål", the Dano-Norwegian descendant of the Danish language used in Norway in Aasen's time). An unofficial variety of Norwegian more close to Aasen's language is still found in Høgnorsk ("High Norwegian"). Today, some consider Nynorsk on equal footing with bokmål, as bokmål tends to be used more in radio and television and most newspapers, whereas New Norse (Nynorsk) is used equally in government work as well as approximately 17% of schools. Although it is not as common as its brother language, it needs to be looked upon as a viable language, as a large minority of Norwegians use it as their primary language including many scholars and authors. New Norse is both a written and spoken language.
Aasen composed poems and plays in the composite dialect to show how it should be used; one of these dramas, "The Heir" (1855), was frequently acted, and may be considered as the pioneer of all the abundant dialect-literature of the last half-century of the 1800s, from Vinje to Garborg. In 1856, he published "Norske Ordsprog", a treatise on Norwegian proverbs. Aasen continuously enlarged and improved his grammars and his dictionary. He lived very quietly in lodgings in Oslo (then Christiania), surrounded by his books and shrinking from publicity, but his name grew into wide political favour as his ideas about the language of the peasants became more and more the watch-word of the popular party. In 1864, he published his definitive grammar of Nynorsk and in 1873 he published the definitive dictionary.

Quite early in his career, in 1842, he had begun to receive a grant to enable him to give his entire attention to his philological investigations; and the Storting (Norwegian parliament), conscious of the national importance of his work, treated him in this respect with more and more generosity as he advanced in years. He continued his investigations to the last, but it may be said that, after the 1873 edition of his "Dictionary" (with a new title: ), he added but little to his stores. Aasen holds perhaps an isolated place in literary history as the one man who has invented, or at least selected and constructed, a language which has pleased so many thousands of his countrymen that they have accepted it for their schools, their sermons and their songs. He died in Christiania on 23 September 1896, and was buried with public honours.

Ivar Aasen-tunet, an institution devoted to the Nynorsk language, opened in June 2000. Their web page includes most of Aasens' texts, numerous other examples of Nynorsk literature (in Nettbiblioteket, the Internet Library), and some articles, including some in English, about language history in Norway.

"Språkåret 2013" (The Language Year 2013) celebrated Ivar Aasen's 200 year anniversary, as well as the 100 year anniversary of Det Norske Teateret. The year's main focus was to celebrate linguistic diversity in Norway. In a poll released in connection with the celebration, 56% of Norwegians said they held positive views of Aasen, while 7% held negative views. On Aasen's 200 anniversary, 5 August 2013, "Bergens Tidende", which is normally published mainly in bokmål, published an edition fully in nynorsk in memory of Aasen.

Aasen published a wide range of material, some of it released posthumously.




</doc>
<doc id="15226" url="https://en.wikipedia.org/wiki?curid=15226" title="Irredentism">
Irredentism

Irredentism is any political or popular movement that seeks to claim/reclaim and occupy a land that the movement's members consider to be a "lost" (or "unredeemed") territory from their nation's past.

Many states formalize their irredentist claims by including them in their constitutional documents, or through other means of legal enshrinement. Such territorial claims are justified on the basis of real or imagined national notions of historic territorial, religious or ethnic affiliations. Irredentist policies may be advocated by nationalist and pan-nationalist movements and have been a feature of identity politics, and of cultural, and political geography. Irredentism may operate as a device for a government to redirect their citizens' discontent against outsiders.

The word (from Italian "irredento" for "unredeemed") was coined in Italy from the phrase "Italia irredenta" ("unredeemed Italy"). This originally referred to rule by Austria-Hungary over territories mostly or partly inhabited by ethnic Italians, such as Trentino, Trieste, Gorizia, Istria, Fiume and Dalmatia during the 19th and early 20th centuries.

An area that may be subjected to a potential claim is sometimes called an "irredenta"; but not all irredentas are necessarily involved in irredentism.

A common way to express a claim to adjacent territories on the grounds of historical or ethnic association is by using the adjective "Greater" as a prefix to the country name. This conveys the image of national territory at its maximum conceivable extent with the country "proper" at its core. The use of "Greater" does not always convey an irredentistic meaning.

The Afghan border with Pakistan, known as the Durand Line, was agreed to by Afghanistan and British India in 1893. The Pashtun tribes inhabiting the border areas were divided between what have become two nations; Afghanistan never accepted the still-porous border and clashes broke out in the 1950s and 1960s between Afghanistan and Pakistan over the issue. All Afghan governments of the past century have declared, with varying intensity, a long-term goal of re-uniting all Pashtun-dominated areas under Afghan rule.

The Argentine government has intermittently maintained a claim over the Falkland Islands (Malvinas, in Spanish) since 1833, and renewed it as recently as January 2013. It considers the archipelago part of the Tierra del Fuego Province, along with South Georgia and the South Sandwich Islands.

The Argentine claim is included in the transitional provisions of the Constitution of Argentina as amended in 1994:

United Bengal is a political ideology of a Unified Bengali-speaking Nation in South Asia. The ideology was developed by Bengali Nationalists after the First Partition of Bengal in 1905. The British-ruled Bengal Presidency was divided into Western Bengal and Eastern Bengal and Assam to weaken the Independence Movement; after much protest, Bengal was reunited in 1911.

The second attempt by British to partition the Bengal along communal lines was in 1947. The United Bengal proposal was the bid made by Prime Minister of Bengal Huseyn Shaheed Suhrawardy and Bengali Nationalist Leader Sarat Chandra Bose to found a united and independent nation-state of Bengal. The proposal was floated as an alternative to the partition of Bengal on communal lines. The initiative failed due to British diplomacy and communal conflict between Bengali Muslims and Bengali Hindus that eventually led to the Second Partition of Bengal.

The 2009 constitution of Bolivia states that the country has an "unrenounceable right over the territory that gives it access to the Pacific Ocean and its maritime space". This is understood as territory that Bolivia and Peru ceded to Chile after the War of the Pacific, which left Bolivia as a landlocked country.

The preamble to the Constitution of the People's Republic of China states, "Taiwan is part of the sacred territory of the People's Republic of China (PRC). It is the lofty duty of the entire Chinese people, including our compatriots in Taiwan, to accomplish the great task of reunifying the motherland." The PRC claim to sovereignty over Taiwan is generally based on the theory of the succession of states, with the PRC claiming that it is the successor state to the Republic of China. It disregards the fact that the Qing Empire ceded Taiwan and the Pescadores to Japan in perpetuity in the Treaty of Shimonoseki in 1895.

The Government of the Republic of China formerly administered both mainland China and Taiwan; the government has been administering only Taiwan since its defeat in the Chinese Civil War by the armed forces of the Communist Party of China. While the official name of the state remains 'Republic of China', the country is commonly called 'Taiwan', since Taiwan makes up 99% of the controlled territory of the ROC.

Article 4 of the Constitution of the Republic of China originally stated that "[t]he territory of the Republic of China within its existing national boundaries shall not be altered except by a resolution of the National Assembly" Throughout the 1950s and 1960s, the Government of the Republic of China on Taiwan maintained itself to be the legitimate ruler of Mainland China as well. As part of its current policy of continuing the 'status quo', the ROC has not renounced claims over the territories currently controlled by the People's Republic of China, Mongolia, Russia, Burma and some Central Asian states. However, Taiwan does not actively pursue these claims in practice; the remaining claims that Taiwan is actively seeking are of uninhabited islands: the Senkaku Islands, whose sovereignty is also asserted by Japan and the PRC; and the Paracel Islands and Spratly Islands in the South China Sea, which are currently being developed by China (PRC).

Article 1 of the Constitution of the Union of the Comoros begins: "The Union of the Comoros is a republic, composed of the autonomous islands of Mohéli, Mayotte, Anjouan, and Grande Comore." Mayotte, geographically a part of the Comoro Islands, was the only island of the four to vote against independence from France (independence losing 37%–63%) in the referendum held December 22, 1974. Mayotte is currently a department of the French Republic.

All of the European colonies on the Indian subcontinent which were not part of the British Raj have been annexed by India since it gained its independence from the British Empire. An example of such territories was the 1961 Indian annexation of Goa. An example of annexation of a territory from the British Raj was the Indian integration of Junagadh.

Akhand Bharat, literally Undivided India or Whole India, is an irredentist call to reunite Pakistan and Bangladesh (and for some Sri Lanka, Maldives, Nepal and Bhutan) with India to form an "Undivided India" as it existed before partition in 1947 during the British Raj (and before that, during other periods of political unity in South Asia when most of the Indian Subcontinent was under the rule of one power, such as during the Maurya Empire, the Gupta Empire, the Mughal Empire or the Maratha Empire). The call for "Akhanda Bharata" has often been raised by mainstream Indian nationalistic cultural and political organizations such as the Rashtriya Swayamsevak Sangh (RSS) and the Bharatiya Janata Party (BJP). Other major Indian political parties such as the Indian National Congress, while maintaining positions against the partition of India on religious grounds, do not necessarily subscribe to a call to reunite South Asia in the form of Akhanda Bharata.

The region of Kashmir in north India has been the issue of a territorial dispute between India and Pakistan since 1947, the Kashmir conflict. Multiple wars have been fought over the issue, the first one immediately upon independence and partition in 1947 itself. To stave off a Pakistani and tribal invasion, Maharaja Hari Singh of the princely state of Jammu and Kashmir signed the Instrument of Accession with India. Kashmir has remained divided in three parts, administered by India, Pakistan and China, since then. However, on the basis of the instrument of accession, India continues to claim the entire Kashmir region as its integral part. All modern Indian political parties support the return of the entirety of Kashmir to India, and all official maps of India show the entire Jammu and Kashmir state (including parts under Pakistani or Chinese administration after 1947) as an integral part of India.

Indonesia claimed all territories of the former Dutch East Indies, and previously viewed British plans to group the British Malaya and Borneo into a new independent federation of Malaysia as a threat to its objective to create a united state called Greater Indonesia. The Indonesian opposition of Malaysian formation has led to the Indonesia–Malaysia confrontation in the early 1960s. It also held Portuguese Timor (modern East Timor) from 1975 to 2002 based on irredentist claims.

The idea of uniting former British and Dutch colonial possessions in Southeast Asia actually has its roots in the early 20th century, as the concept of Greater Malay ("Melayu Raya") was coined in British Malaya espoused by students and graduates of Sultan Idris Training College for Malay Teachers in the late 1920s. Some political figures in Indonesia including Mohammad Yamin and Sukarno revived the idea in the 1950s and named the political union concept as Greater Indonesia.

The nation state of Israel was established in 1948. The United Nations General Assembly passed U.N. Resolution 181, otherwise known as the United Nations Partition Plan for Palestine, with 72% of the valid votes. Eventually, Israeli independence was achieved following the liquidation of the former British-administered Mandate of Palestine, the departure of the British and the "Independence War" between the Jews in ex-Mandatory Palestine and five Arab states' armies. The Jewish claim to Palestine as a Jewish homeland can be seen as an example of irredentist reclamation of what is considered lost Jewish land by Zionists. These claims are based on ancestral inhabitance (and in some periods sovereignty) in the land and the cultural/religious significance of it in the Hebrew Bible. The latter is particularly relevant to the Israeli claim to Jerusalem. It should be noted that Mandatory Palestine had sizable Jewish and Arab populations before the Second World War.
Judea and Samaria, as they are called in the Bible, were part of the ancient Kingdom of Israel (designated the West Bank by Jordan in 1947) and the Gaza Strip, previously annexed by Jordan and occupied by Egypt respectively, were conquered and occupied by Israel in the Six-Day War in 1967. Israel withdrew from Gaza in August 2005; Judea and Samaria (West Bank) remain under Israeli control. Israel has never explicitly claimed sovereignty over any part of the West Bank apart from East Jerusalem, which it unilaterally annexed in 1980. However, the Israeli military supports and defends hundreds of thousands of Israeli citizens who have migrated to the West Bank, incurring criticism by some who otherwise support Israel. The United Nations Security Council, the United Nations General Assembly, and some countries and international organizations continue to regard Israel as occupying Gaza. "(See Israeli-Occupied Territories.)"
The Israeli annexing instrument, the Jerusalem Law—one of the Basic Laws of Israel (Israel does not have a constitution)—declares Jerusalem, "complete and united", to be the capital of Israel. Article 3 of the Basic Law of the Palestinian Authority, which was ratified in 2002 by the Palestinian National Authority and serves as an interim constitution, claims that "Jerusalem is the capital of Palestine." "De facto", the Palestinian government administers the parts of the West Bank that Israel has granted it authority over from Ramallah, while the Gaza Strip is administered by the Hamas movement from Gaza.

The United States has until now not recognized Israeli sovereignty over East Jerusalem and maintained its embassy in Tel Aviv. In Jerusalem, the United States maintained two Consulates General as a diplomatic representation to the city of Jerusalem alone, separate from representation to the state of Israel. One of the Consulates General was established before the 1967 war, and the other in a recently constructed building on the Israeli side of Jerusalem. Moreover, Congress passed the Jerusalem Embassy Act in 1995 that says the US shall move its embassy from Tel Aviv to Jerusalem, but allows the president to delay the move every year if it is deemed contrary to national security interests. Since 1995, every president delayed the move. However, President Donald Trump in December 2017 declared his intention to move the embassy to Jerusalem and by May 2018 the embassy will have officially moved to Jerusalem. 

A number of Israelis and Jews regard the East Bank of the Jordan river (which is today the Kingdom of Jordan) as the eastern parts of the Land of Israel (following the revisionist idea) because, according to the Bible, the Israelite tribes of Menasseh, Gad, and Reuben settled on the east bank of the Jordan, and because that area was designated a Jewish national home by the League of Nations in the Mandate for Palestine based upon the recognized historical connection of the Jewish people to the land. Cited as an explicit basis not to create, but to reconstitute the historical homeland of the Jewish people as a nation-state roughly analogous to the former Kingdom of Israel subject to change by treaty, capitulation, grant, usage, sufferance or other lawful means, it forms a basis for claims of sovereign jurisdiction.

Since their founding, both Korean states have disputed the legitimacy of the other. South Korea's constitution claims jurisdiction over the entire Korean peninsula. It acknowledges the division of Korea only indirectly by requiring the president to work for reunification. The Committee for the Five Northern Korean Provinces, established in 1949, is the South Korean authority charged with the administration of Korean territory north of the Military Demarcation Line (i.e., North Korea), and consists of the governors of the five provinces, who are appointed by the President. However the body is purely symbolic and largely tasked with dealing with Northern defectors; if reunification were to actually occur the Committee would be dissolved and new administrators appointed by the Ministry of Unification.

North Korea's constitution also stresses the importance of reunification, but, while it makes no similar formal provision for administering the South, it effectively claims its territory as it does not diplomatically recognise the Republic of Korea, deeming it an "entity occupying the Korean territory".

Other territories sometimes disputed to belong to Korea are Manchuria and Gando.

The Guayana Esequiba is a territory administered by Guyana but claimed by Venezuela. It was first included in the Viceroyalty of New Granada and the Captaincy General of Venezuela by Spain, but was later included in Essequibo by the Dutch and in British Guiana by the United Kingdom. Originally, parts of what is now eastern Venezuela were included in the disputed area. This territory of is the subject of a long-running boundary dispute inherited from the colonial powers and complicated by the independence of Guyana in 1966. The status of the territory is subject to the Treaty of Geneva, which was signed by the United Kingdom, Venezuela and British Guiana governments on February 17, 1966. This treaty stipulates that the parties will agree to find a practical, peaceful and satisfactory solution to the dispute.

Some of the most violent irredentist conflicts of recent times in Europe flared up as a consequence of the break-up of the former Yugoslavian federal state in the early 1990s. The conflict erupted further south with the ethnic Albanian majority in Kosovo seeking to switch allegiance to the adjoining state of Albania.

Greater Albania or "Ethnic Albania" as called by the Albanian nationalists themselves, is an irredentist concept of lands outside the borders of Albania which are considered part of a greater national homeland by most Albanians, based on claims on the present-day or historical presence of Albanian populations in those areas. The term incorporates claims to Kosovo, as well as territories in the neighbouring countries Montenegro, Greece, and the Republic of Macedonia. Albanians themselves mostly use the term "ethnic Albania" instead. According to the "Gallup Balkan Monitor" 2010 report, the idea of a Greater Albania is supported by the majority of Albanians in Albania (63%), Kosovo (81%) and Macedonia (53%). In 2012, as part of the celebrations for the 100th Anniversary of the Independence of Albania, Prime Minister Sali Berisha spoke of "Albanian lands" stretching from Preveza in Greece to Preševo in Serbia, and from the Macedonian capital of Skopje to the Montenegrin capital of Podgorica, angering Albania's neighbours. The comments were also inscribed on a parchment that will be displayed at a museum in the city of Vlore, where the country's independence from the Ottoman Empire was declared in 1912.

Based on the territorial definition of a historic Bulgarian state, a "Greater Bulgaria" nationalist movement has been active for more than a century that would annex most of Macedonia, Thrace, and Moesia.

The idea of the natural borders of France is a political theory conceptualized primarily in the late 18th and early 19th centuries that focused on widening the borders primarily based on either practical reasons or the territory that was thought to be the maximum extent that the ancient Gauls inhabited. Under this theory France's eastern border would extend to the Rhine river and would require the annexation of Belgium, Luxembourg, 27,264 km² of German territory on the left bank of the Rhine river, and 10,545 km² of Dutch territory south of Waal and Merwede rivers. If implemented today France would increase its territory by 70,923 km² and increase its population by 25,170,400.

During the unification of Germany (1871), the term "Großdeutschland" "Greater Germany" referred to a possible German nation consisting of the states that later comprised the German Empire and Austria. The term "Kleindeutschland" "Lesser Germany" referred to a possible German state without Austria. The term was also used by Germans referring to Greater Germany, a state consisting of pre–World War I Germany, Austria and the Sudetenland. This issue was known as the German Question.

A main point of Nazi ideology was to reunify all Germans either born or living outside of Germany to create an "all-German Reich". These beliefs ultimately resulted in the Munich Agreement, which ceded to Germany areas of Czechoslovakia that were mainly inhabited by those of German descent and the Anschluss, which ceded the entire country of Austria to Germany; both events occurred in 1938.

Following the Greek War of Independence in 1821–1832, Greece began to contest areas inhabited by Greeks, primarily against the Ottoman Empire. The Megali Idea (Great Idea) envisioned Greek incorporation of Greek-inhabited lands, but also historical lands in Asia Minor corresponding with the predominantly Greek and Orthodox Byzantine Empire and the dominions of the ancient Greeks.

The Greek quest began with the acquisition of Thessaly through the Convention of Constantinople in 1881, a failed war against Turkey in 1897 and the Balkan Wars (Macedonia, Epirus, some Aegean Islands). After World War I, Greece acquired Western Thrace from Bulgaria as per the Treaty of Neuilly-sur-Seine, but also Ionia/Smyrna and Eastern Thrace (excluding Constantinople) from the Ottoman Empire as ordained in the Treaty of Sèvres. Subsequently, Greece launched an unsuccessful campaign to further their gains in Asia Minor, but were halted by the Turkish revolution. The events culminated into the Great Fire of Smyrna, Population exchange between Greece and Turkey and Treaty of Lausanne which returned Eastern Thrace and Ionia to the newfound Turkish Republic. The events are known as the "Asia Minor Catastrophe" to Greeks. The Ionian Islands were ceded by Britain in 1864, and the Dodecanese by Italy in 1947.

Another concern of the Greeks is the incorporation of Cyprus which was ceded by the Ottomans to the British. As a result of the Cyprus Emergency the island gained independence as the Republic of Cyprus in 1960. The failed incorporation by Greece through coup d'état and the Turkish invasion of Cyprus in 1974 led to the formation of the mostly unrecognized Northern Cyprus and has culminated into the present-day Cyprus issue.

The Aegean islands of Imbros and Tenedos which were not ceded to Greece over the course of the 20th century and where the dominant Greek community has faced persecution are also of concern.

The restoration of the borders of Hungary to their state prior to World War I, in order to unite all ethnic Hungarians within the same country once again.

The Irish Free State achieved independence from the United Kingdom in 1922. This state did not include Northern Ireland, which comprised six counties in the north-east of the island of Ireland which remained in the United Kingdom. The Constitution of Ireland adopted in 1937 provided that the name of the state is "Ireland" and Articles 2 and 3 provided that "[t]he national territory consists of the whole island of Ireland", while stipulating that "[p]ending the re-integration of the national territory", the powers of the state were restricted to legislate only for the area which had formed part of the Irish Free State. Arising from the Northern Ireland peace process, the matter was mutually resolved as part of the Good Friday Agreement in 1998. Ireland's constitution was altered by referendum and its territorial claim to Northern Ireland was removed. The amended constitution asserts that while it is the entitlement of "every person born in the island of Ireland … to be part of the Irish Nation" and to hold Irish citizenship, "a united Ireland shall be brought about only by peaceful means with the consent of a majority of the people, democratically expressed, in both jurisdictions in the island." A North/South Ministerial Council was created between the two jurisdictions and given executive authority. The advisory and consultative role of the government of Ireland in the government of Northern Ireland granted by the United Kingdom, that had begun with the 1985 Anglo-Irish Agreement, was maintained, although that Agreement itself was ended. The two states also settled the long-running dispute concerning their respective names: "Ireland" and the "United Kingdom of Great Britain and Northern Ireland", with both governments agreeing to use those names.

Under the Irish republican theory of legitimism, the Irish Republic declared in 1916 was in existence from then on, denying the legitimacy of either the state of Ireland or the position of Northern Ireland within the United Kingdom. Through much of its history, this was the position of Sinn Féin; however, it effectively abandoned this stance after accepting the Good Friday Agreement. Small groups which split from Sinn Féin continue to adopt this stance, including Republican Sinn Féin, linked with the Continuity IRA, and the 32 County Sovereignty Movement, linked with the Real IRA.

Italy's territorial claims were on the basis of re-establishing a Romanesque Empire, a fourth shore according to the concept of Mare Nostrum (Latin for 'Our Sea') and traditional ethnic borders. Evident in Italy's rapid takeover of surrounding territories under Fascist leader Benito Mussolini and claims following the collapsed 1915 Treaty of London and 1919 Treaty of Versailles which established feelings of betrayal. Similar to the Nazis' stab-in-the-back myth, Mussolini and Hitler's similarities including a joint hatred towards the French and wanting to expand their territories brought the two leaders together, solidified in the Pact of Steel and later WW2. By 1942 Italy had conquered Abyssinia (modern day Ethiopia), Libya, Much of Egypt, Tunisia, Kenya and Somalia. And – on the European continent – Istria, Dalmatia, Albania, Slovenia, Croatia, Macedonia, the Spanish island of Majorca and France's Corsica; Malta was also bombed. Underlying tensions remained with France, over its territories of Corsica, Nice and Savoy.

Some Macedonian nationalists promoted the irredentist concept of a United Macedonia () among ethnic Macedonian nationalists, which involves territorial claims on the northern province of Macedonia in Greece, but also in Blagoevgrad Province ("Pirin Macedonia") in Bulgaria, Albania, and Serbia. The United Macedonia concept aims to unify the transnational region of Macedonia in the Balkans (which they claim as their homeland and which they assert was wrongfully divided under the Treaty of Bucharest in 1913), into a single state under Macedonian domination, with the Greek city of Thessaloniki ("Solun" in the Slavic languages) as its capital.

The Kingdom of Norway maintains some claim to territories lost at the dissolution of the Denmark–Norway union. The Norwegian Empire, which was the Norwegian territories at its maximum extent, included Iceland, the settleable areas of Greenland, the Faroe Islands and Shetland among others. Under Danish sovereignty since they established a hegemonic position in the Kalmar Union, the territories were considered as Norwegian colonies. When in the Treaty of Kiel in 1814, Norway's territories were transferred from Denmark to Sweden, the territories of Iceland, Greenland, and the Faroe Islands were maintained by Denmark. In 1919, Norway declared sovereignty over an area in Eastern Greenland in the Ihlen Declaration, which led to a dispute with Denmark that was not settled until 1933, by the Permanent Court of International Justice. Norway formerly included the provinces Jämtland, Härjedalen, Idre, Särna (lost since the Second Treaty of Brömsebro), and Bohuslän (lost since the Treaty of Roskilde), which were ceded to Sweden after Danish defeats in wars such as the Thirty Years' War and Second Northern War.

Kresy ("Borderlands") are the eastern lands that formerly belonged to Poland. In 1921, Polish troops crossed the Curzon Line, the border between ethnic Polish and ethnic Ukrainian and Belorussian territories, and seized large Ukrainian and Belorussian territories, and also seized 7 percent of Lithuania's territory in 1920. These territories were re-annexed by the Soviet Union in 1939 under the Molotov-Ribbentrop pact, and include major cities, like Lviv (Ukraine), Vilnius (the capital of Lithuania), and Hrodna (Belarus). Even though "Kresy", or the "Eastern Borderlands", are no longer Polish territories, the area is still inhabited by a significant Polish minority, and the memory of a Polish "Kresy" is still cultivated. The attachment to the "myth of Kresy", the vision of the region as a peaceful, idyllic, rural land, has been criticized in Polish discourse.

In January, February and March 2012, the Centre for Public Opinion Research conducted a survey, asking Poles about their ties to the Kresy. It turned out that almost 15% of the population of Poland (4.3–4.6 million people) declared that they had either been born in the Kresy, or had a parent or a grandparent who came from that region. Numerous treasures of Polish culture remain and there are numerous Kresy-oriented organizations. There are Polish sports clubs (Pogoń Lwów, FK Polonia Vilnius), newspapers (Gazeta Lwowska, Kurier Wileński), radio stations (in Lviv and Vilnius), numerous theatres, schools, choirs and folk ensembles. Poles living in "Kresy" are helped by Fundacja Pomoc Polakom na Wschodzie, a Polish government-sponsored organization, as well as other organizations, such as The "Association of Help of Poles in the East Kresy" (see also Karta Polaka). Money is frequently collected to help those Poles who live in "Kresy", and there are several annual events, such as a "Christmas Package for a Polish Veteran in Kresy", and "Summer with Poland", sponsored by the Association "Polish Community", in which Polish children from "Kresy" are invited to visit Poland. Polish language handbooks and films, as well as medicines and clothes are collected and sent to "Kresy". Books are most often sent to Polish schools which exist there — for example, in December 2010, The University of Wrocław organized an event called "Become a Polish Santa Claus and Give a Book to a Polish Child in Kresy". Polish churches and cemeteries (such as Cemetery of the Defenders of Lwów) are renovated with money from Poland.

Portugal does not recognize Spanish sovereignty over the territory of Olivenza, ceded to Spain during the Napoleonic Wars. Since the Rexurdimento of the mid-nineteenth century, there has been an intellectual movement pleading for the reintegration between Portugal and the region of Galicia. Although this movement has become increasingly popular on both sides of the border, there is no consensus in regard to the nature of such "reintegration": whether political, socio-cultural or merely linguistic.

Romania lays claims to Greater Romania, which include Bessarabia and Bucovina as Moldova, since they were parts of Romania between 1918 and 1940, and are still inhabited for the most by Romanians. Moldovans are ethnically Romanians, and the Moldovan language is the Soviet name for the Romanian language. There is some (but not universal) support by Moldovans for a peaceful and voluntary reunion with Romania, not least because (having joined the European Union), the economy has burgeoned and Romanian citizens have gained freedom of movement in Europe. Also Russian irredentism in Transnistria has caused alarm and resentment.

The annexation of Crimea by the Russian Federation in 2014 was based on a claim of protecting ethnic Russians residing there. Crimea was part of the Russian Empire from 1783 to 1917, after which it enjoyed a few years of autonomy until it was made part of the Russian Soviet Federative Socialist Republic (which was a part of the Soviet Union) from 1921 to 1954 and then transferred to Soviet Ukraine (which also was a part of the Soviet Union) in 1954. After the dissolution of the Soviet Union, Crimea still remained part of Ukraine until February 2014. Russia declared Crimea to be part of the Russian Federation in March 2014, and effective administration commenced. The Russian regional status is not currently recognised by the UN General Assembly and by many countries.

Russian irredentism also includes southeastern and coastal Ukraine, known as "Novorossiya", a term from the Russian Empire.

Serbian irredentism is manifested in "Greater Serbia". Used in the context of the Yugoslav wars, however, the Serbian struggle for Serbs to remain united in one country does not quite fit the term "irredentism". In the 19th century, Pan-Serbism sought to unite all of the Serb people across the Balkans, under Ottoman and Habsburg rule. Some intellectuals sought to unite all South Slavs (regardless of religion) into a Serbian state. Serbia had gained independence from the Ottoman Empire in 1878. Bosnia and Herzegovina, annexed by the Austrians in 1908, was viewed of as a part of the Serbian homeland. Serbia directed its territorial aspirations to the south, as the north and west was held by Austria. Macedonia was divided between Serbia and Greece after the Balkan Wars. In 1914 aspirations were directed towards Austria-Hungary. A government policy sought to incorporate all Serb-inhabited areas, and other South Slavic areas, thereby laying the foundation of Yugoslavia. With the establishment of the Kingdom of Serbs, Croats and Slovenes (later Yugoslavia), the Serbs now lived united in one country. During the breakup of Yugoslavia, the Serb political leadership in break-away Croatia and Bosnia and Herzegovina declared their territories to be part of the Federal Republic of Yugoslavia (Serbia and Montenegro).

The project of unification of Serb-inhabited areas in Croatia and Bosnia and Herzegovina during the Yugoslav wars (see United Serb Republic) ultimately failed. The Croatian Operation Storm ended large-scale combat and captured most of the Republic of Serbian Krajina forcing almost complete Serbian population to leave their centuries-old homeland, while the Dayton Agreement ended the Bosnian War. Bosnia and Herzegovina was established as a federal republic, made up by two separate entities, one being Serb-inhabited Republika Srpska. There has since been calls by Bosnian Serb politicians for the secession of Republika Srpska, and possible unification with Serbia.

After the Kosovo War (1998–99), Kosovo became a UN protectorate, still "de jure" part of Serbia. The Albanian-majority Kosovo assembly unilaterally declared the independence of Kosovo in 2008, and its status is since disputed.

Spain maintains a claim on Gibraltar, a British Overseas Territory near the southernmost tip of the Iberian Peninsula, which has been British since the 18th Century.

Gibraltar was captured in 1704, during the War of the Spanish Succession (1701–1714). The Kingdom of Spain formally ceded the territory in perpetuity to the British Crown in 1713, under of the Treaty of Utrecht. Spain's territorial claim was formally reasserted by the Spanish dictator Francisco Franco in the 1960s and has been continued by successive Spanish governments. In 2002 an agreement in principle on joint sovereignty over Gibraltar between the governments of the United Kingdom and Spain was decisively rejected in a referendum. The British Government now refuses to discuss sovereignty without the consent of the Gibraltarians.

Irredentism is acute in the Caucasus region, too. The Nagorno-Karabakh movement's original slogan of "miatsum" ('union') was explicitly oriented towards re-unification with Armenia as to the pre-Soviet status, feeding an Azerbaijani understanding of the conflict as a bilateral one between itself and an irredentist Armenia. According to Prof. Thomas Ambrosio, "Armenia's successful irredentist project in the Nagorno-Karabakh region of Azerbaijan" and "From 1992 to the cease-fire in 1994, Armenia encountered a highly permissive or tolerant international environment that allowed its annexation of some 15 percent of Azerbaijani territory."
In the view of Nadia Milanova, Nagorno-Karabakh represents a combination of separatism and irredentism. However, the area has historically been Armenian, known as the Kingdom of Artsakh or Khachen. When the Caucuses came under the rule of the Soviet Union, the land was given to Azerbaijan abruptly and arbitrarily due to pressure by Joseph Stalin, along with the ancient Armenian lands of Nakhichevan, to appease Turkey during 1919-1921.
Azerbaijan's irredentism, on the other hand, is quite explicit in official statements of the Azerbaijani officials by claiming the UN member-state Republic of Armenia as Azerbaijani territory despite the absence of historical evidence of Azerbaijan existing as a separate state up until 1918. On his official meeting in Gyanja on 21 January 2014, President Ilham Aliyev said in particular, "The present-day Armenia is actually located on historical lands of Azerbaijan. Therefore, we will return to all our historical lands in the future. This should be known to young people and children. We must live, we live and we will continue to live with this idea."

The Assyrian homeland is a geographic and cultural region situated in Northern Mesopotamia that has been traditionally inhabited by Assyrian people. The area with the greatest concentration of Assyrians on earth is located in the Assyrian homeland, or the "Assyrian Triangle", a region which comprises the Nineveh plains, southern Hakkari and the Barwari regions. This is where some Assyrian groups seek to create an independent nation state. The land roughly mirrors the boundaries of ancient Assyria proper, and the later Achaemenid, Seleucid, Parthian, Roman and Sassanid provinces of Assyria (Athura/Assuristan) that was extant between the 25th century BC and 7th century AD.

Whole Azerbaijan is a concept of the political and historical union of territories currently and historically inhabited by Azerbaijanis or historically controlled by them. Western Azerbaijan is an irredentist political concept that is used in Azerbaijan mostly to refer to Armenia. Azerbaijani statements claim that the territory of the modern Armenian republic were lands that once belonged to Azerbaijanis.

Pan-Iranism is an ideology that advocates solidarity and reunification of Iranian peoples living in the Iranian plateau and other regions that have significant Iranian cultural influence, including the Persians, Azerbaijanis, Ossetians, Kurds, Zazas, Tajiks of Tajikistan and Afghanistan, the Pashtuns and the Baloch of Pakistan. The first theoretician was Dr Mahmoud Afshar Yazdi.

The ideology of pan-Iranism is most often used in conjunction with the idea of forming a Greater Iran, which refers to the regions of the Caucasus, West Asia, Central Asia, and parts of South Asia that have significant Iranian cultural influence due to having been either long historically ruled by the various Iranian (Persian) empires (such as those of the Medes, Achaemenids, Parthians, Sassanians, Samanids, Timurids, Safavids, and Afsharids and the Qajar Empire), having considerable aspects of Persian culture in their own culture due to extensive contact with the various Empires based in Persia (e.g., those regions and peoples in the North Caucasus that were not under direct Iranian rule), or are simply nowadays still inhabited by a significant amount of Iranic-speaking people who patronize their respective cultures (as it goes for the western parts of South Asia, Bahrain and China). It roughly corresponds to the territory on the Iranian plateau and its bordering plains. It is also referred to as "Greater Persia", while the Encyclopædia Iranica uses the term "Iranian Cultural Continent".

Saddam Hussein's Iraq aimed to annex Khuzestan Province of Iran during the Iran–Iraq War due to the Arab population living there.

The Lebanese nationalism goes even further and incorporates irredentist views going beyond the Lebanese borders, seeking to unify all the lands of ancient Phoenicia around present day Lebanon. This comes from the fact that present day Lebanon, the Mediterranean coast of Syria, and northern Israel is the area that roughly corresponds to ancient Phoenicia and as a result the majority of the Lebanese people identify with the ancient Phoenician population of that region. The proposed Greater Lebanese country includes Lebanon, Mediterranean coast of Syria, and northern Israel.

The French Mandate of Syria handed over the Sanjak of Alexandretta to Turkey which turned it into Hatay Province. Syria disputes this and still regards the region as belonging to Syria.

The Syrian Social Nationalist Party, which operates in Lebanon and Syria, works for the unification of most modern states of the Levant and beyond in a single state referred to as Greater Syria. The proposed Syrian country includes Israel, Syria, Jordan, and parts of Turkey, and has at times been expanded to include Iraq, Cyprus, and the Sinai peninsula.

Misak-ı Millî is the set of six important decisions made by the last term of the Ottoman Parliament. Parliament met on 28 January 1920 and published their decisions on 12 February 1920. These decisions worried the occupying Allies, resulting in the Occupation of Constantinople by the British, French and Italian troops on 16 March 1920 and the establishment of a new Turkish nationalist parliament, the Grand National Assembly, in Ankara.

The Ottoman Minister of Internal Affairs, Damat Ferid Pasha, made the opening speech of parliament due to Mehmed VI's illness. A group of parliamentarians called "Felâh-ı Vatan" was established by Mustafa Kemal's friends to acknowledge the decisions taken at the Erzurum Congress and the Sivas Congress. Mustafa Kemal said "It is the nation's iron fist that writes the Nation's Oath which is the main principle of our independence to the annals of history." Decisions taken by this parliament were used as the basis for the new Turkish Republic's claims in the Treaty of Lausanne.

The Greater and Lesser Tunbs are disputed by the United Arab Emirates against Iran.

Greater Yemen is a theory giving Yemen claim to former territories that were held by various predecessor states that existed between the Himyarite period and 18th century. The areas claimed include parts of modern Saudi Arabia and Oman.

When Hong Kong and Macau were British and Portuguese territories, respectively, China considered these two territories to be Chinese territories under British and Portuguese administration. Therefore, Hong Kong people and Macau people descended from Chinese immigrants were entitled to Hong Kong Special Administrative Region passport or Macao Special Administrative Region passport after the two territories became the special administrative regions.

Japan claims the two southernmost islands of the Russian-administered Kuril Islands, the island chain north of Hokkaido, annexed by the Soviet Union following World War II. Japan also claims the South Korean-administered Liancourt Rocks, which are known as Takeshima in Japan and have been claimed since the end of the Second World War.

The 1909 Gando Convention addressed a territory dispute between China and Joseon Korea in China's favour. Both Korean states now accept the convention border as an administrative boundary. However, because the convention was made by the occupying Empire of Japan, South Korea has disputed its legality and some Koreans claim that Korea extends into "de facto" PRC territory, viz. Dandong and Liaoning. The most ambitious claims include all parts of Manchuria that the Goguryeo kingdom controlled.

The irredentist idea that advocates cultural and political solidarity of Mongols. The proposed territory usually includes the independent state of Mongolia, the Chinese regions of Inner Mongolia (Southern Mongolia) and Dzungaria (in Xinjiang), and the Russian subjects of Buryatia. Sometimes Tuva and the Altai Republic are included as well.

South Asia too is another region in which armed irredentist movements have been active for almost a century, in North-East India, Burma and Bangladesh. Most prominent amongst them are the Naga fight for Greater Nagaland, the Chin struggle for a unified Chinland, the Sri Lankan Tamil struggle for a return of their state under Tamil Eelam and other self-determinist movements by the ethnic indigenous peoples of the erstwhile Assam both under the British and post-British Assam under India. Other such movements include Beḻagāva border dispute on Maharashtra and Karnataka border with intentions to unite all Marathi speaking people under one state since the formation of the Karnataka state and dissolution of the bilingual Bombay state.

Greater Bangladesh is an assumption of several Indian intellectuals that the neighbouring country of Bangladesh has an aspiration to unite all Bengali dominated regions under their flag. These include the states of West Bengal, Tripura and Assam as well as the Andaman Islands which are currently part of India and the Burmese State of Rakhine. The theory is principally based on a widespread belief amongst Indian masses that a large number of illegal Bangladeshi immigrants reside in Indian territory. It is alleged that illegal immigration is actively encouraged by some political groups in Bangladesh as well as the state of Bangladesh to convert large parts of India's northeastern states and West Bengal into Muslim-majority areas that would subsequently seek to separate from India and join Muslim-majority Bangladesh. Scholars have reflected that under the guise of anti-Bangladeshi immigrant movement it is actually an anti-Muslim agenda pointed towards Bangladeshi Muslims by false propaganda and widely exaggerated claims on immigrant population. In 1998, Lieutenant General S.K. Sinha, then the Governor of Assam, claimed that massive illegal immigration from Bangladesh was directly linked with "the long-cherished design of Greater Bangladesh".

The call for creation of "Akhanda Bharata" or "Akhand Hindustan" has on occasion been raised by some Indian right wing Hindutvadi cultural and political organisations, such as the Hindu Mahasabha, Rashtriya Swayamsevak Sangh (RSS), Vishwa Hindu Parishad, Bharatiya Janata Party (BJP). The name of one organisation sharing this goal, the Akhand Hindustan Morcha, bears the term in its name. Other major Indian non-sectarian political parties, such as the Indian National Congress, maintain a position against the partition of India on religious grounds and do not subscribe to a call for Akhand Bharat.

Irredentism is commonplace in Africa due to the political boundaries of former European colonial nation-states passing through ethnic boundaries, and recent declarations of independence after civil war. For example, some Ethiopian nationalist circles still claim the former Ethiopian province of Eritrea (internationally recognized as the independent State of Eritrea in 1993 after a 30-year civil war).

In North Africa, the prime examples of irredentism are the concepts of Greater Morocco and Greater Mauritania. While Mauritania has since relinquished any claims to territories outside its internationally recognized borders, Morocco continues to occupy lands south of Morocco, which it refers to as its "Southern Provinces".

Greater Somalia refers to the region in the Horn of Africa in which ethnic Somalis are and have historically represented the predominant population. The territory encompasses The Republic of Somalia, the Ogaden region in Ethiopia, the North Eastern Province in Kenya and southern and eastern Djibouti. Ogaden in eastern Ethiopia has seen military and civic movements seeking to make it part of Somalia. This culminated in the 1977–78 Ogaden War between the two neighbours where the Somali military offensive between July 1977 and March 1978 over the disputed Ethiopian region Ogaden ended when the Somali Armed Forces retreated back across the border and a truce was declared. The Kenyan Northern Frontier District also saw conflict during the Shifta War (1963–1967) when a secessionist conflict in which ethnic Somalis in the Lamu, Garissa, Wajir and Mandera counties (all except Lamu formed part of the former North Eastern Province, abolished in 2013), attempted to join with their fellow Somalis in a "Greater Somalia". There has been no similar conflicts in Djibouti, which was previously known as the "French Somaliland" during colonisation. Here the apparent struggles for unification manifested itself in political strife that ended when in a referendum to join France as opposed to the Somali Republic succeeded among rumours of widespread vote rigging. and the subsequent death of Somali nationalist Mahmoud Harbi, Vice President of the Government Council, who was killed in a plane crash two years later under suspicious circumstances.
Some sources say that Somalia has also laid a claim to the Socotra archipelago, which is currently governed by Yemen.

In the Treaty of Guadalupe Hidalgo (1848) following the Mexican–American War (1845–48), Mexico ceded claims to what is now the Western and Southwestern United States to the United States (see Mexican Cession). The Cortina and Pizaña uprisings of 1859 and 1915 were influenced by irredentist ideas and the "proximity of the international boundary". The unsuccessful Pizaña uprising "was the last major armed protest on the part of Texas-Americans" (Tejanos). This 1915 uprising and the Plan of San Diego that preceded it marked the high point in Mexican irredentist sentiments.

In the early years of the Chicano Movement ("El Movimiento") in the 1960s and 1970s, some movement figures "were political nationalists who advocated the secession of the Southwest from the Anglo republic of the United States of America, if not fully, at least locally with regard to Chicano self-determination in local governance, education and means of production". For example, in the 1970s, Reies Tijerina and his group La Alianza, espoused various separatist, secessionist, or irredentist beliefs. The "Plan Espiritual de Aztlán", written during the First Chicano National Youth Conference in 1969, also stated "the fundamental Chicano nationalist goal of reclaiming Aztlán"—a reference to ancient Mexican myth—as "the rightful homeland of the Chicanos". However, "Most Chicano nationalists ... did not express the extreme desire for secession from the United States, and the nationalism they expressed weighed more heavily toward the broadly cultural than the explicitly political."

Today, there is virtually no Mexican-American support for "separatist policies of self-determination". "Ethnonational irredentism by Mexicans in territories seized by the United States" following the Mexican–American War "declined after the failure of several attempted revolts at the end of the nineteenth century, in favor of internal ... struggles for immigrant and racial civil rights" in the United States. Neither the Mexican government nor any significant Mexican-American group "makes irredentist claims upon the United States". In the modern era, there "has been no evidence of irredentist sentiments among Mexican-Americans, even in such formerly Mexican territories as Southern California, ... nor of disloyalty to the United States, nor of active interest in the politics of Mexico".




</doc>
<doc id="15227" url="https://en.wikipedia.org/wiki?curid=15227" title="Inuit languages">
Inuit languages

The Inuit languages are a closely related group of indigenous American languages traditionally spoken across the North American Arctic and to some extent in the subarctic in Labrador. The related Yupik languages are spoken in western and southern Alaska and in the far east of Russia, but are severely endangered in Russia today and spoken only in a few villages on the Chukchi Peninsula. The Inuit live primarily in three countries: Greenland, Canada (specifically in the Nunatsiavut region of Labrador, the Nunavik region of Quebec, Nunavut, and the Northwest Territories), and the United States (specifically the coast of Alaska).

The total population of Inuit speaking their traditional languages is difficult to assess with precision, since most counts rely on self-reported census data that may not accurately reflect usage or competence. Greenland census estimates place the number of speakers of varieties of Inuit there at roughly 50,000, while Canadian estimates are at roughly 35,000. These two countries count the bulk of speakers of Inuit language variants, although about 7,500 Alaskans speak varieties of Inuit out of a population of over 13,000 Inuit.

The Inuit languages have a few hundred speakers in Russia. In addition, an estimated 7,000 Greenlandic Inuit live in European Denmark, the largest group outside Greenland, Canada and Alaska. Thus, the global population of speakers of varieties of Inuit is on the order of nearly 100,000 people.

The traditional language of the Inuit is a system of closely interrelated dialects that are not readily comprehensible from one end of the Inuit world to the other, and some people do not think of it as a single language but rather as a group of languages. However, there are no clear criteria for breaking the Inuit language into specific member languages since it forms a dialect continuum. Each band of Inuit understands its neighbours, and most likely its neighbours' neighbours; but at some remove, comprehensibility drops to a very low level.

As a result, Inuit in different places use different words for its own variants and for the entire group of languages, and this ambiguity has been carried into other languages, creating a great deal of confusion over what labels should be applied to it.

In Greenland the official form of Inuit language, and the official language of the state, is called "Kalaallisut". In other languages, it is often called "Greenlandic" or some cognate term. The Eskimo languages of Alaska are called "Inupiatun", but the variants of the Seward Peninsula are distinguished from the other Alaskan variants by calling them "Qawiaraq", or for some dialects, "Bering Strait Inupiatun".

In Canada, the word "Inuktitut" is routinely used to refer to all Canadian variants of the Inuit traditional language, and it is under that name that it is recognised as one of the official languages of Nunavut and the Northwest Territories. However, one of the variants of western Nunavut is called "Inuinnaqtun" to distinguish itself from the dialects of eastern Canada, while the variants of the Northwest Territories are sometimes called "Inuvialuktun" and have in the past sometimes been called "Inuktun". In those dialects, the name is sometimes rendered as "Inuktitun" to reflect dialectal differences in pronunciation. The Inuit language of Quebec is called "Inuttitut" by its speakers, and often by other people, but this is a minor variation in pronunciation. In Labrador, the language is called "Inuttut" or, often in official documents, by the more descriptive name "Labradorimiutut". Furthermore, Canadians – both Inuit and non-Inuit – sometimes use the word "Inuktitut" to refer to "all" Inuit language variants, including those of Alaska and Greenland.

The phrase ""Inuit language"" is largely limited to professional discourse, since in each area, there is one or more conventional terms that cover all the local variants; or it is used as a descriptive term in publications where readers can't necessarily be expected to know the locally used words.

Although many people refer to the Inuit language as "Eskimo language", this is a broad term that also includes the Yupik languages, and is in addition strongly discouraged in Canada and diminishing in usage elsewhere. See the article on "Eskimo" for more information on this word.

The language of the Inuit is an Eskimo–Aleut language. It is fairly closely related to the Yupik languages and more remotely to the Aleut language. These cousin languages are all spoken in Western Alaska and Eastern Chukotka, Russia. It is not discernibly related to other indigenous languages of the Americas or northeast Asia, although some have proposed that it is related to the Uralic languages such as Finnish and the Sami languages in the proposed "Uralo-Siberian" grouping, or even Indo-European languages as part of the hypothetical "Nostratic" superphylum. Some consider it a Paleosiberian language, although that is more a geographic than a linguistic grouping.

Early forms of the Inuit language were spoken by the Thule people, who overran the Dorset culture that had previously occupied Arctic America at the beginning of the 2nd millennium. By 1300, the Inuit and their language had reached western Greenland, and finally east Greenland roughly at the same time the Viking colonies in southern Greenland disappeared. It is generally believed that it was during this centuries-long eastward migration that the Inuit language became distinct from the Yupik languages spoken in Western Alaska and Chukotka.

Until 1902, a possible enclave of the Dorset, the "Sadlermiut" (in modern Inuktitut spelling "Sallirmiut"), existed on Southampton Island. Almost nothing is known about their language, but the few eyewitness accounts tell of them speaking a "strange dialect". This suggests that they also spoke an Eskimo–Aleut language, but one quite distinct from the forms spoken in Canada today.

The Yupik and Inuit languages are very similar syntactically and morphologically. Their common origin can be seen in a number of cognates:

The western Alaskan variants retain a large number of features present in proto-Inuit language and in Yup'ik, enough so that they might be classed as Yup'ik languages if they were viewed in isolation from the larger Inuit world.

The Inuit languages are a fairly closely linked set of languages which can be broken up using a number of different criteria. Traditionally, Inuit describe dialect differences by means of place names to describe local idiosyncrasies in language: The dialect of Igloolik versus the dialect of Iqaluit, for example. However, political and sociological divisions are increasingly the principal criteria for describing different variants of the Inuit languages because of their links to different writing systems, literary traditions, schools, media sources and borrowed vocabulary. This makes any partition of the Inuit language somewhat problematic. This article will use labels that try to synthesise linguistic, sociolinguistic and political considerations in splitting up the Inuit dialect spectrum. This scheme is not the only one used or necessarily one used by Inuit themselves, but its labels do try to reflect the usages most seen in popular and technical literature.

In addition to the territories listed below, some 7,000 Greenlandic speakers are reported to live in mainland Denmark, and according to the 2001 census roughly 200 self-reported Inuktitut native speakers regularly live in parts of Canada which are outside traditional Inuit lands.

Of the roughly 13,000 Alaskan Iñupiat, as few as 3000 may still be able to speak the Iñupiaq, with most of them over the age of 40. Alaskan Inupiat speak four distinct dialects:


The Inuit languages are an official language in the Northwest Territories, and the official and dominant language of Nunavut; it enjoys a high level of official support in Nunavik, a semi-autonomous portion of Quebec; and is still spoken in some parts of Labrador. Generally, Canadians refer to all dialects spoken in Canada as "Inuktitut", but the terms "Inuvialuktun", "Inuinnaqtun", and "Inuttut" (also called "Nunatsiavummiutut" or "Labradorimiutut") have some currency in referring to the variants of specific areas.

Greenland counts approximately 50,000 speakers of the Inuit languages, of whom over 90% speak west Greenlandic dialects at home.

Greenlandic was strongly supported by the Danish Christian mission (conducted by the Danish state church) in Greenland. Several major dictionaries were created, beginning with Poul Egedes's Dictionarium Grönlandico-danico-latinum (1750) and culminating with Samuel Kleinschmidt's (1871) "Den grønlandske ordbog" (Transl. "The Greenlandic Dictionary") that contained a Greenlandic grammatical system that has formed the basis of modern Greenlandic grammar. Together with the fact that until 1925 Danish was not taught in the public schools, these policies had the consequence that Greenlandic has always and continues to enjoy a very strong position in Greenland, both as a spoken as well as written language.

Eastern Canadian Inuit language variants have fifteen consonants and three vowels (which can be long or short).

Consonants are arranged with five places of articulation: bilabial, alveolar, palatal, velar and uvular; and three manners of articulation: voiceless stops, voiced continuants, and nasals, as well as two additional sounds—voiceless fricatives. The Alaskan dialects have an additional manner of articulation, the "retroflex", which was present in proto-Inuit language. Retroflexes have disappeared in all the Canadian and Greenlandic dialects. In Natsilingmiutut, the voiced palatal stop derives from a former retroflex.

Almost all Inuit language variants have only three basic vowels and make a phonological distinction between short and long forms of all vowels. The only exceptions are at the extreme edges of the Inuit world: parts of Greenland, and in western Alaska.

The Inuit language, like other Eskimo–Aleut languages, has a very rich morphological system, in which a succession of different morphemes are added to root words (like verb endings in European languages) to indicate things that, in languages like English, would require several words to express. (See also: Agglutinative language and Polysynthetic language) All Inuit language words begin with a root morpheme to which other morphemes are suffixed. The language has hundreds of distinct suffixes, in some dialects as many as 700. Fortunately for learners, the language has a highly regular morphology. Although the rules are sometimes very complicated, they do not have exceptions in the sense that English and other Indo-European languages do.

This system makes words very long, and potentially unique. For example, in central Nunavut Inuktitut:

This long word is composed of a root word "tusaa-" "to hear" followed by five suffixes:

This sort of word construction is pervasive in the Inuit languages and makes it very unlike English. In one large Canadian corpus – the "Nunavut Hansard" – 92% of all words appear only once, in contrast to a small percentage in most English corpora of similar size. This makes the application of Zipf's law quite difficult in the Inuit language. Furthermore, the notion of a part of speech can be somewhat complicated in the Inuit languages. Fully inflected verbs can be interpreted as nouns. The word ilisaijuq can be interpreted as a fully inflected verb: "he studies", but can also be interpreted as a noun: "student". That said, the meaning is probably obvious to a fluent speaker, when put in context.

The morphology and syntax of the Inuit languages vary to some degree between dialects, and the article "Inuit grammar" describes primarily central Nunavut dialects, but the basic principles will generally apply to all of them and to some degree to Yupik languages as well.

Both the names of places and people tend to be highly prosaic when translated. "Iqaluit", for example, is simply the plural of the noun "iqaluk" "fish" ("Arctic char", "salmon" or "trout" depending on dialect). "Igloolik" ("Iglulik") means "place with houses", a word that could be interpreted as simply "town"; "Inuvik" is "place of people"; "Baffin Island", "Qikiqtaaluk" in Inuktitut, translates approximately to "big island".

Although practically all Inuit have legal names based on southern naming traditions, at home and among themselves they still use native naming traditions. There too, names tend to consist of highly prosaic words. The Inuit traditionally believed that by adopting the name of a dead person or a class of things, they could take some of their characteristics or powers, and enjoy a part of their identity. (This is why they were always very willing to accept European names: they believed that this made them equal to the Europeans.)

Common native names in Canada include "Ujarak" (rock), "Nuvuk" (headland), "Nasak" (hat, or hood), "Tupiq" or "Tupeq" in Kalaallisut (tent), and "Qajaq" (kayak). Inuit also use animal names, traditionally believing that by using those names, they took on some of the characteristics of that animal: "Nanuq" or "Nanoq" in Kalaallisut (polar-bear), "Uqalik" or "Ukaleq" in Kalaallisut (Arctic hare), and "Tiriaq" or "Teriaq" in Kalaallisut (ermine) are favourites. In other cases, Inuit are named after dead people or people in traditional tales, by naming them after anatomical traits those people are believed to have had. Examples include "Itigaituk" (has no feet), "Anana" or "Anaana" (mother), "Piujuq" (beautiful) and "Tulimak" (rib). Inuit may have any number of names, given by parents and other community members.

In the 1920s, changes in lifestyle and serious epidemics like tuberculosis made the government of Canada interested in tracking the Inuit of Canada's Arctic. Traditionally Inuit names reflect what is important in Inuit culture: environment, landscape, seascape, family, animals, birds, spirits. However these traditional names were difficult for non-Inuit to parse. Also, the agglutinative nature of Inuit language meant that names seemed long and were difficult for southern bureaucrats and missionaries to pronounce.

Thus, in the 1940s, the Inuit were given disc numbers, recorded on a special leather ID tag, like a dog tag. They were required to keep the tag with them always. (Some tags are now so old and worn that the number is polished out.) The numbers were assigned with a letter prefix that indicated location (E = east), community, and then the order in which the census-taker saw the individual. In some ways this state renaming was abetted by the churches and missionaries, who viewed the traditional names and their calls to power as related to shamanism and paganism.

They encouraged people to take Christian names. So a young woman who was known to her relatives as "Lutaaq, Pilitaq, Palluq, or Inusiq" and had been baptised as "Annie" was under this system to become Annie E7-121. People adopted the number-names, their family members' numbers, etc., and learned all the region codes (like knowing a telephone area code).

Until Inuit began studying in the south, many did not know that numbers were not normal parts of Christian and English naming systems. Then in 1969, the government started Project Surname, headed by Abe Okpik, to replace number-names with patrilineal "family surnames". But contemporary Inuit carvers and graphic artists still use their disk number as their signature on their works of art.

A popular belief exists that the Inuit have an unusually large number of words for snow. This is not accurate, and results from a misunderstanding of the nature of polysynthetic languages. In fact, the Inuit have only a few base roots for snow: 'qanniq-' ('qanik-' in some dialects), which is used most often like the verb "to snow", and 'aput', which means "snow" as a substance. Parts of speech work very differently in the Inuit language than in English, so these definitions are somewhat misleading.

The Inuit languages can form very long words by adding more and more descriptive affixes to words. Those affixes may modify the syntactic and semantic properties of the base word, or may add qualifiers to it in much the same way that English uses adjectives or prepositional phrases to qualify nouns (e.g. "falling snow", "blowing snow", "snow on the ground", "snow drift", etc.)

The "fact" that there are many Inuit words for snow has been put forward so often that it has become a journalistic cliché.

The Inuit use a base-20 counting system.

Because the Inuit languages are spread over such a large area, divided between different nations and political units and originally reached by Europeans of different origins at different times, there is no uniform way of writing the Inuit language.

Currently there are six "standard" ways to write the languages:

Though all except the syllabics use the Latin alphabet, all of them are a bit different from each other.
Most Inuktitut in Nunavut and Nunavik is written using a script called Inuktitut syllabics, based on Canadian Aboriginal syllabics. The western part of Nunavut and the Northwest Territories use Latin alphabet usually identified as Inuinnaqtun. In Alaska, two other Latin alphabets are used. Nunatsiavut uses an alphabet devised by German-speaking Moravian missionaries, which included the letter "kra". Greenland's Latin alphabet was originally much like the one used in Nunatsiavut, but underwent a spelling reform in 1973 to bring the orthography in line with changes in pronunciation and better reflect the phonemic inventory of the language.

Inuktitut syllabics, used in Canada, is based on Cree syllabics, which was devised by the missionary James Evans based on Devanagari a Brahmi script. The present form of Canadian Inuktitut syllabics was adopted by the Inuit Cultural Institute in Canada in the 1970s. The Inuit in Alaska, the Inuvialuit, Inuinnaqtun speakers, and Inuit in Greenland and Labrador use Latin alphabets.

Though presented in syllabic form, syllabics is not a true syllabary, but an abugida, since syllables starting with the same consonant are written with graphically similar letters.

All of the characters needed for Inuktitut syllabics are available in the Unicode character repertoire, in the blocks Unified Canadian Aboriginal Syllabics.







</doc>
<doc id="15229" url="https://en.wikipedia.org/wiki?curid=15229" title="Ibn Battuta">
Ibn Battuta

Ibn Battuta (or ') (; ; fully '; Arabic: ) (February 25, 13041368 or 1369) was a Muslim Moroccan scholar and explorer who widely travelled the medieval world. Over a period of thirty years, Ibn Battuta visited most of the Islamic world and many non-Muslim lands, including North Africa, the Horn of Africa, West Africa, the Middle East, Central Asia, Southeast Asia, South Asia and China. Near the end of his life, he dictated an account of his journeys, titled "A Gift to Those Who Contemplate the Wonders of Cities and the Marvels of Travelling" (, "Tuḥfat an-Nuẓẓār fī Gharāʾib al-Amṣār wa ʿAjāʾib al-Asfār"), usually simply referred to as "TheTravels" (, "Rihla"). This account of his journeys provides a picture of medieval civilisation that is still widely consulted today.
All that is known about Ibn Battuta's life comes from the autobiographical information included in the account of his travels, which records that he was of Berber descent, born into a family of Islamic legal scholars in Tangier, Morocco, on 24 February 1304, during the reign of the Marinid dynasty. He claimed descent from a Berber tribe known as the Lawata. As a young man, he would have studied at a Sunni Maliki madh'hab (Islamic jurisprudence school), the dominant form of education in North Africa at that time. Maliki Muslims requested Ibn Battuta serve as their religious judge as he was from an area where it was practised.

In June 1325, at the age of twenty-one, Ibn Battuta set off from his hometown on a "hajj", or pilgrimage, to Mecca, a journey that would ordinarily take sixteen months. He would not see Morocco again for twenty-four years.

I set out alone, having neither fellow-traveller in whose companionship I might find cheer, nor caravan whose part I might join, but swayed by an overmastering impulse within me and a desire long-cherished in my bosom to visit these illustrious sanctuaries. So I braced my resolution to quit my dear ones, female and male, and forsook my home as birds forsake their nests. My parents being yet in the bonds of life, it weighed sorely upon me to part from them, and both they and I were afflicted with sorrow at this separation.
He travelled to Mecca overland, following the North African coast across the sultanates of Abd al-Wadid and Hafsid. The route took him through Tlemcen, Béjaïa, and then Tunis, where he stayed for two months. For safety, Ibn Battuta usually joined a caravan to reduce the risk of being robbed. He took a bride in the town of Sfax, the first in a series of marriages that would feature in his travels.

In the early spring of 1326, after a journey of over , Ibn Battuta arrived at the port of Alexandria, at the time part of the Bahri Mamluk empire. He met two ascetic pious men in Alexandria. One was Sheikh Burhanuddin who is supposed to have foretold the destiny of Ibn Battuta as a world traveller saying "It seems to me that you are fond of foreign travel. You will visit my brother Fariduddin in India, Rukonuddin in Sind and Burhanuddin in China. Convey my greetings to them". Another pious man Sheikh Murshidi interpreted the meaning of a dream of Ibn Battuta that he was meant to be a world traveller. He spent several weeks visiting sites in the area, and then headed inland to Cairo, the capital of the Mamluk Sultanate and an important city. After spending about a month in Cairo, he embarked on the first of many detours within the relative safety of Mamluk territory. Of the three usual routes to Mecca, Ibn Battuta chose the least-travelled, which involved a journey up the Nile valley, then east to the Red Sea port of Aydhab. Upon approaching the town, however, a local rebellion forced him to turn back.

Ibn Battuta returned to Cairo and took a second side trip, this time to Mamluk-controlled Damascus. During his first trip he had encountered a holy man who prophesied that he would only reach Mecca by travelling through Syria. The diversion held an added advantage; because of the holy places that lay along the way, including Hebron, Jerusalem, and Bethlehem, the Mamluk authorities spared no efforts in keeping the route safe for pilgrims. Without this help many travellers would be robbed and murdered.

After spending the Muslim month of Ramadan in Damascus, he joined a caravan travelling the south to Medina, site of the Mosque of the Islamic prophet Muhammad. After four days in the town, he journeyed on to Mecca, where completing his pilgrimage he took the honorific status of "El-Hajji". Rather than returning home, Ibn Battuta decided to continue on, choosing as his next destination the Ilkhanate, a Mongol Khanate, to the northeast.

On 17 November 1326, following a month spent in Mecca, Ibn Battuta joined a large caravan of pilgrims returning to Iraq across the Arabian Peninsula. The group headed north to Medina and then, travelling at night, turned northeast across the Najd plateau to Najaf, on a journey that lasted about two weeks. In Najaf, he visited the mausoleum of Ali, the Fourth Caliph.

Then, instead of continuing on to Baghdad with the caravan, Ibn Battuta started a six-month detour that took him into Persia. From Najaf, he journeyed to Wasit, then followed the river Tigris south to Basra. His next destination was the town of Isfahan across the Zagros Mountains in Persia. He then headed south to Shiraz, a large, flourishing city spared the destruction wrought by Mongol invaders on many more northerly towns. Finally, he returned across the mountains to Baghdad, arriving there in June 1327. Parts of the city were still ruined from the damage inflicted by Hulago Khan's invading army in 1258.

In Baghdad, he found Abu Sa'id, the last Mongol ruler of the unified Ilkhanate, leaving the city and heading north with a large retinue. Ibn Battuta joined the royal caravan for a while, then turned north on the Silk Road to Tabriz, the first major city in the region to open its gates to the Mongols and by then an important trading centre as most of its nearby rivals had been razed by the Mongol invaders.

Ibn Battuta left again for Baghdad, probably in July, but first took an excursion northwards along the river Tigris. He visited Mosul, where he was the guest of the Ilkhanate governor, and then the towns of Cizre (Jazirat ibn 'Umar) and Mardin in modern-day Turkey. At a hermitage on a mountain near Sinjar, he met a Kurdish mystic who gave him some silver coins. Once back in Mosul, he joined a "feeder" caravan of pilgrims heading south to Baghdad, where they would meet up with the main caravan that crossed the Arabian Desert to Mecca. Ill with diarrhoea, he arrived in the city weak and exhausted for his second "hajj".

Ibn Battuta remained in Mecca for some time (the "Rihla" suggests about three years, from September 1327 until autumn 1330). Problems with chronology, however, lead commentators to suggest that he may have left after the 1328 "hajj".

After the "hajj" in either 1328 or 1330, he made his way to the port of Jeddah on the Red Sea coast. From there he followed the coast in a series of boats making slow progress against the prevailing south-easterly winds. Once in Yemen he visited Zabīd and later the highland town of Ta'izz, where he met the Rasulid dynasty king "(Malik)" Mujahid Nur al-Din Ali. Ibn Battuta also mentions visiting Sana'a, but whether he actually did so is doubtful. In all likelihood, he went directly from Ta'izz to the important trading port of Aden, arriving around the beginning of 1329 or 1331.

From Aden, Ibn Battuta embarked on a ship heading for Zeila on the coast of Somalia. He then moved on to Cape Guardafui further down the Somalia seaboard, spending about a week in each location. Later he would visit Mogadishu, the then pre-eminent city of the "Land of the Berbers" (بلد البربر "Balad al-Barbar", the medieval Arabic term for the Horn of Africa).

When Ibn Battuta arrived in 1331, Mogadishu stood at the zenith of its prosperity. He described it as "an exceedingly large city" with many rich merchants, noted for its high-quality fabric that was exported to other countries, including Egypt. Ibn Battuta added that the city was ruled by a Somali Sultan, Abu Bakr ibn Sayx 'Umar, who was originally from Berbera in northern Somalia and spoke both Somali (referred to by Battuta as "Mogadishan", the Benadir dialect of Somali) and Arabic with equal fluency. The Sultan also had a retinue of wazirs (ministers), legal experts, commanders, royal eunuchs, and assorted hangers-on at his beck and call.

Ibn Battuta continued by ship south to the Swahili Coast, a region then known in Arabic as the "Bilad al-Zanj" ("Land of the Zanj"), with an overnight stop at the island town of Mombasa. Although relatively small at the time, Mombasa would become important in the following century. After a journey along the coast, Ibn Battuta next arrived in the island town of Kilwa in present-day Tanzania, which had become an important transit centre of the gold trade. He described the city as "one of the finest and most beautifully built towns; all the buildings are of wood, and the houses are roofed with "dīs" reeds."

Ibn Battuta recorded his visit to the Kilwa Sultanate in 1330, and commented favorably on the humility and religion of its ruler, Sultan al-Hasan ibn Sulaiman, a descendant of the legendary Ali ibn al-Hassan Shirazi. He further wrote that the authority of the Sultan extended from Malindi in the north to Inhambane in the south and was particularly impressed by the planning of the city, believing it to be the reason for Kilwa's success along the coast. During this period, he described the construction of the Palace of Husuni Kubwa and a significant extension to the Great Mosque of Kilwa, which was made of coral stones and was the largest Mosque of its kind. With a change in the monsoon winds, Ibn Battuta sailed back to Arabia, first to Oman and the Strait of Hormuz then on to Mecca for the "hajj" of 1330 (or 1332).

After his third pilgrimage to Mecca, Ibn Battuta decided to seek employment with the Muslim Sultan of Delhi, Muhammad bin Tughluq. In the autumn of 1330 (or 1332), he set off for the Seljuk controlled territory of Anatolia with the intention of taking an overland route to India. He crossed the Red Sea and the Eastern Desert to reach the Nile valley and then headed north to Cairo. From there he crossed the Sinai Peninsula to Palestine and then travelled north again through some of the towns that he had visited in 1326. From the Syrian port of Latakia, a Genoese ship took him (and his companions) to Alanya on the southern coast of modern-day Turkey. He then journeyed westwards along the coast to the port of Antalya. In the town he met members of one of the semi-religious "fityan" associations. These were a feature of most Anatolian towns in the 13th and 14th centuries. The members were young artisans and had at their head a leader with the title of "Akhis". The associations specialised in welcoming travellers. Ibn Battuta was very impressed with the hospitality that he received and would later stay in their hospices in more than 25 towns in Anatolia. From Antalya Ibn Battuta headed inland to Eğirdir which was the capital of the Hamidids. He spent Ramadan (June 1331 or May 1333) in the city.

From this point the itinerary across Anatolia in the "Rihla" is confused. Ibn Battuta describes travelling westwards from Eğirdir to Milas and then skipping eastward past Eğirdir to Konya. He then continues travelling in an easterly direction, reaching Erzurum from where he skips back to Birgi which lies north of Milas. Historians believe that Ibn Battuta visited a number of towns in central Anatolia, but not in the order that he describes.

From Sinope he took a sea route to the Crimean Peninsula, arriving in the Golden Horde realm. He went to the port town of Azov, where he met with the emir of the Khan, then to the large and rich city of Majar. He left Majar to meet with Uzbeg Khan's travelling court ("Orda"), which was at the time near Beshtau mountain. From there he made a journey to Bolghar, which became the northernmost point he reached, and noted its unusually (for a subtropics dweller) short nights in summer. Then he returned to the Khan's court and with it moved to Astrakhan.

Ibn Battuta recorded that while in Bulghar he wanted to travel further north into the land of darkness. The land is snow-covered throughout (northern Siberia) and the only means of transport is dog-drawn sled. There lived a mysterious people who were reluctant to show themselves. They traded with southern people in a peculiar way. Southern merchants brought various goods and placed them in an open area on the snow in the night, then returned to their tents. Next morning they came to the place again and found their merchandise taken by the mysterious people, but in exchange they found fur-skins which could be used for making valuable coats, jackets, and other winter garments. The trade was done between merchants and the mysterious people without seeing each other. As Ibn Battuta was not a merchant and saw no benefit of going there he abandoned the travel to this land of darkness.
When they reached Astrakhan, Öz Beg Khan had just given permission for one of his pregnant wives, Princess Bayalun, a daughter of Byzantine emperor Andronikos III Palaiologos, to return to her home city of Constantinople to give birth. Ibn Battuta talked his way into this expedition, which would be his first beyond the boundaries of the Islamic world.

Arriving in Constantinople towards the end of 1332 (or 1334), he met the Byzantine emperor Andronikos III Palaiologos. He visited the great church of Hagia Sophia and spoke with an Eastern Orthodox priest about his travels in the city of Jerusalem. After a month in the city, Ibn Battuta returned to Astrakhan, then arrived in the capital city Sarai al-Jadid and reported the accounts of his travels to Sultan Öz Beg Khan (r. 1313–1341). Then he continued past the Caspian and Aral Seas to Bukhara and Samarkand, where he visited the court of another Mongolian king, Tarmashirin (r. 1331–1334) of the Chagatai Khanate. From there, he journeyed south to Afghanistan, then crossed into India via the mountain passes of the Hindu Kush. In the "Rihla", he mentions these mountains and the history of the range in slave trading. He wrote,

Ibn Battuta and his party reached the Indus River on 12 September 1333. From there, he made his way to Delhi and became acquainted with the sultan, Muhammad bin Tughluq.

Muhammad bin Tughluq was renowned as the wealthiest man in the Muslim world at that time. He patronized various scholars, Sufis, qadis, viziers and other functionaries in order to consolidate his rule. As with Mamluk Egypt, the Tughlaq Dynasty was a rare vestigial example of Muslim rule in Asia after the Mongol invasion. On the strength of his years of study in Mecca, Ibn Battuta was appointed a "qadi", or judge, by the sultan. However, he found it difficult to enforce Islamic law beyond the sultan's court in Delhi, due to lack of Islamic appeal in India.

It is uncertain by which route Ibn Battuta entered the Indian subcontinent. He may have entered via the Khyber Pass and Peshawar, or further south. He crossed the Sutlej River near the city of Pakpattan, in modern-day Pakistan, where he paid obeisance at the shrine of Baba Farid, before crossing southwest into Rajput country. From the Rajput Kingdom of Sarsatti, Battuta visited Hansi in India, describing it as "among the most beautiful cities, the best constructed and the most populated; it is surrounded with a strong wall, and its founder is said to be one of the great infidel kings, called Tara". Upon his arrival in Sindh, Ibn Battuta mentions the Indian rhinoceros that lived on the banks of the Indus.

The Sultan was erratic even by the standards of the time and for six years Ibn Battuta veered between living the high life of a trusted subordinate and falling under suspicion of treason for a variety of offences. His plan to leave on the pretext of taking another "hajj" was stymied by the Sultan. The opportunity for Battuta to leave Delhi finally arose in 1341 when an embassy arrived from Yuan dynasty China asking for permission to rebuild a Himalayan Buddhist temple popular with Chinese pilgrims.

Ibn Battuta was given charge of the embassy but en route to the coast at the start of the journey to China, he and his large retinue were attacked by a group of bandits. Separated from his companions, he was robbed and nearly lost his life. Despite this setback, within ten days he had caught up with his group and continued on to Khambhat in the Indian state of Gujarat. From there, they sailed to Calicut (now known as Kozhikode), where Portuguese explorer Vasco da Gama would land two centuries later. While in Calicut, Battuta was the guest of the ruling Zamorin. While Ibn Battuta visited a mosque on shore, a storm arose and one of the ships of his expedition sank. The other ship then sailed without him only to be seized by a local Sumatran king a few months later.

Afraid to return to Delhi and be seen as a failure, he stayed for a time in southern India under the protection of Jamal-ud-Din, ruler of the small but powerful Nawayath sultanate on the banks of the Sharavathi river next to the Arabian Sea. This area is today known as Hosapattana and lies in the Honavar administrative district of Uttara Kannada. Following the overthrow of the sultanate, Ibn Battuta had no choice but to leave India. Although determined to continue his journey to China, he first took a detour to visit the Maldive Islands where he worked as a judge.
He spent nine months on the islands, much longer than he had intended. As a "Chief Qadi", his skills were highly desirable in the formerly Buddhist nation that had recently converted to Islam. Half-kidnapped into staying, he became chief judge and married into the royal family of Omar I. He became embroiled in local politics and left when his strict judgments in the laissez-faire island kingdom began to chafe with its rulers. In the "Rihla" he mentions his dismay at the local women going about with no clothing above the waist, and the locals taking no notice when he complained. From the Maldives, he carried on to Sri Lanka and visited Sri Pada and Tenavaram temple.

Ibn Battuta's ship almost sank on embarking from Sri Lanka, only for the vessel that came to his rescue to suffer an attack by pirates. Stranded onshore, he worked his way back to the Madurai kingdom in India. Here he spent some time in the court of the short-lived Madurai Sultanate under Ghiyas-ud-Din Muhammad Damghani, from where he returned to the Maldives and boarded a Chinese junk, still intending to reach China and take up his ambassadorial post.

He reached the port of Chittagong in modern-day Bangladesh intending to travel to Sylhet to meet Shah Jalal, who became so renowned that Ibn Battuta, then in Chittagong, made a one-month journey through the mountains of Kamaru near Sylhet to meet him. On his way to Sylhet, Ibn Battuta was greeted by several of Shah Jalal's disciples who had come to assist him on his journey many days before he had arrived. At the meeting in 1345 CE, Ibn Battuta noted that Shah Jalal was tall and lean, fair in complexion and lived by the mosque in a cave, where his only item of value was a goat he kept for milk, butter, and yogurt. He observed that the companions of the Shah Jalal were foreign and known for their strength and bravery. He also mentions that many people would visit the Shah to seek guidance. Ibn Battuta went further north into Assam, then turned around and continued with his original plan.

In 1345, Ibn Battuta travelled on to Samudra Pasai Sultanate in present-day Aceh, Northern Sumatra, where he notes in his travel log that the ruler of Samudra Pasai was a pious Muslim named Sultan Al-Malik Al-Zahir Jamal-ad-Din, who performed his religious duties with utmost zeal and often waged campaigns against animists in the region. The island of Sumatra, according to Ibn Battuta, was rich in camphor, areca nut, cloves, and tin. The "madh'hab" he observed was Imam Al-Shafi‘i, whose customs were similar to those he had previously seen in coastal India, especially among the Mappila Muslims, who were also followers of Imam Al-Shafi‘i. At that time Samudra Pasai marked the end of Dar al-Islam, because no territory east of this was ruled by a Muslim. Here he stayed for about two weeks in the wooden walled town as a guest of the sultan, and then the sultan provided him with supplies and sent him on his way on one of his own junks to China.

Ibn Battuta first sailed to Malacca on the Malay Peninsula which he called "Mul Jawi". He met the ruler of Malacca and stayed as a guest for three days. 

Ibn Battuta then sailed to a state called Kaylukari in the land of Tawalisi, where he met Urduja, a local princess. Urduja was a brave warrior, and her people are opponents of the Yuan dynasty. She was described as an "idolater", but could write the phrase Bismillah in Islamic calligraphy. The locations of Kaylukari and Tawalisi are disputed. Kaylukari might referred to Po Klong Garai in Champa (now southern Vietnam), and Urduja might be an aristocrat of Champa or the Trần dynasty. Filipinos widely believe that Kaylukari was in present-day Pangasinan Province of the Philippines. In modern times, Urduja has been featured in Filipino textbooks and films as a national heroine. Numerous other locations have been proposed, ranging from Java to somewhere in Guangdong Province, China. However, Sir Henry Yule and William Henry Scott consider both Tawilisi and Urduja to be entirely fictitious. (See Tawalisi for details.)

From Kaylukari, Ibn Battuta finally reached Quanzhou in Fujian Province, China.

In the year 1345 Ibn Battuta arrived at Quanzhou in China's Fujian province, then under the rule of the Mongols. One of the first things he noted was that Muslims referred to the city as "Zaitun" (meaning olive), but Ibn Battuta could not find any olives anywhere. He mentioned local artists and their mastery in making portraits of newly arrived foreigners; these were for security purposes. Ibn Battuta praised the craftsmen and their silk and porcelain; as well as fruits such as plums and watermelons and the advantages of paper money. He described the manufacturing process of large ships in the city of Quanzhou. He also mentioned Chinese cuisine and its usage of animals such as frogs, pigs and even dogs which were sold in the markets, and noted that the chickens in China were larger than those in the west. Scholars however have pointed out numerous errors given in Ibn Battuta's account of China, for example confusing the Yellow River with the Grand Canal and other waterways, as well as believing that porcelain was made from coal.

In Quanzhou, Ibn Battuta was welcomed by the head of the local Muslim merchants (possibly a fānzhǎng or "Leader of Foreigners" ）and Sheikh al-Islam (Imam), who came to meet him with flags, drums, trumpets and musicians. Ibn Battuta noted that the Muslim populace lived within a separate portion in the city where they had their own mosques, bazaars and hospitals. In Quanzhou, he met two prominent Persians, Burhan al-Din of Kazerun and Sharif al-Din from Tabriz (both of whom were influential figures noted in the "Yuan History" as "A-mi-li-ding" and "Sai-fu-ding", respectively). While in Quanzhou he ascended the "Mount of the Hermit" and briefly visited a well-known Taoist monk in a cave.

He then travelled south along the Chinese coast to Guangzhou, where he lodged for two weeks with one of the city's wealthy merchants.

From Guangzhou he went north to Quanzhou and then proceeded to the city of Fuzhou, where he took up residence with Zahir al-Din and was proud to meet Kawam al-Din and a fellow countryman named Al-Bushri of Ceuta, who had become a wealthy merchant in China. Al-Bushri accompanied Ibn Battuta northwards to Hangzhou and paid for the gifts that Ibn Battuta would present to the Mongolian Emperor Togon-temür of the Yuan Dynasty.

Ibn Battuta said that Hangzhou was one of the largest cities he had ever seen, and he noted its charm, describing that the city sat on a beautiful lake surrounded by gentle green hills. He mentions the city's Muslim quarter and resided as a guest with a family of Egyptian origin. During his stay at Hangzhou he was particularly impressed by the large number of well-crafted and well-painted Chinese wooden ships, with coloured sails and silk awnings, assembling in the canals. Later he attended a banquet of the Yuan Mongol administrator of the city named Qurtai, who according to Ibn Battuta, was very fond of the skills of local Chinese conjurers. Ibn Battuta also mentions locals who worship the Solar deity.

He described floating through the Grand Canal on a boat watching crop fields, orchids, merchants in black-silk, and women in flowered-silk and priests also in silk. In Beijing, Ibn Battuta referred to himself as the long-lost ambassador from the Delhi Sultanate and was invited to the Yuan imperial court of Togon-temür (who according to Ibn Battuta was worshipped by some people in China). Ibn Batutta noted that the palace of Khanbaliq was made of wood and that the ruler's "head wife" (Empress Gi) held processions in her honour.

Ibn Battuta also wrote he had heard of "the rampart of Yajuj and Majuj" that was "sixty days' travel" from the city of Zeitun (Quanzhou); Hamilton Alexander Rosskeen Gibb notes that Ibn Battuta believed that the Great Wall of China was built by Dhul-Qarnayn to contain Gog and Magog as mentioned in the Quran. However, Ibn Battuta, who asked about the wall in China, could find no one who had either seen it or knew of anyone who had seen it, suggesting that no significant structure of the wall constructed in the earlier periods remained at that period (the present structure was built later during the Ming dynasty).

Ibn Battuta travelled from Beijing to Hangzhou, and then proceeded to Fuzhou. Upon his return to Quanzhou, he soon boarded a Chinese junk owned by the Sultan of Samudera Pasai Sultanate heading for Southeast Asia, whereupon Ibn Battuta was unfairly charged a hefty sum by the crew and lost much of what he had collected during his stay in China.

Battuta claimed that the Mongol Khan (Qan) had interned with him in his grave, six slave soldiers and four girl slaves. Silver, gold, weapons, and carpets were put into the grave.

After returning to Quanzhou in 1346, Ibn Battuta began his journey back to Morocco. In Kozhikode, he once again considered throwing himself at the mercy of Muhammad bin Tughluq in Delhi, but thought better of it and decided to carry on to Mecca. On his way to Basra he passed through the Strait of Hormuz, where he learned that Abu Sa'id, last ruler of the Ilkhanate Dynasty had died in Persia. Abu Sa'id's territories had subsequently collapsed due to a fierce civil war between the Persians and Mongols.

In 1348, Ibn Battuta arrived in Damascus with the intention of retracing the route of his first "hajj". He then learned that his father had died 15 years earlier and death became the dominant theme for the next year or so. The Black Death had struck and he was on hand as it spread through Syria, Palestine, and Arabia. After reaching Mecca he decided to return to Morocco, nearly a quarter of a century after leaving home. On the way he made one last detour to Sardinia, then in 1349, returned to Tangier by way of Fez, only to discover that his mother had also died a few months before.

After a few days in Tangier, Ibn Battuta set out for a trip to the Muslim-controlled territory of al-Andalus on the Iberian Peninsula. King Alfonso XI of Castile and León had threatened to attack Gibraltar, so in 1350, Ibn Battuta joined a group of Muslims leaving Tangier with the intention of defending the port. By the time he arrived, the Black Death had killed Alfonso and the threat of invasion had receded, so he turned the trip into a sight-seeing tour, travelling through Valencia and ending up in Granada.

After his departure from al-Andalus he decided to travel through Morocco. On his return home, he stopped for a while in Marrakech, which was almost a ghost town following the recent plague and the transfer of the capital to Fez.

Once more Ibn Battuta returned to Tangier, but only stayed for a short while. In 1324, two years before his first visit to Cairo, the West African Malian "Mansa", or king of kings, Musa had passed through the same city on his own "hajj" and caused a sensation with a display of extravagant riches brought from his gold-rich homeland. Although Ibn Battuta never mentioned this visit specifically, when he heard the story it may have planted a seed in his mind as he then decided to cross the Sahara and visit the Muslim kingdoms on its far side.

In the autumn of 1351, Ibn Battuta left Fez and made his way to the town of Sijilmasa on the northern edge of the Sahara in present-day Morocco. There he bought a number of camels and stayed for four months. He set out again with a caravan in February 1352 and after 25 days arrived at the dry salt lake bed of Taghaza with its salt mines. All of the local buildings were made from slabs of salt by the slaves of the Masufa tribe, who cut the salt in thick slabs for transport by camel. Taghaza was a commercial centre and awash with Malian gold, though Ibn Battuta did not form a favourable impression of the place, recording that it was plagued by flies and the water was brackish.

After a ten-day stay in Taghaza, the caravan set out for the oasis of Tasarahla (probably Bir al-Ksaib) where it stopped for three days in preparation for the last and most difficult leg of the journey across the vast desert. From Tasarahla, a Masufa scout was sent ahead to the oasis town of Oualata, where he arranged for water to be transported a distance of four days travel where it would meet the thirsty caravan. Oualata was the southern terminus of the trans-Saharan trade route and had recently become part of the Mali Empire. Altogether, the caravan took two months to cross the of desert from Sijilmasa.

From there, Ibn Battuta travelled southwest along a river he believed to be the Nile (it was actually the river Niger), until he reached the capital of the Mali Empire. There he met "Mansa" Suleyman, king since 1341. Ibn Battuta disapproved of the fact that female slaves, servants and even the daughters of the sultan went about exposing parts of their bodies not befitting a Muslim. He left the capital in February accompanied by a local Malian merchant and journeyed overland by camel to Timbuktu. Though in the next two centuries it would become the most important city in the region, at that time it was a small city and relatively unimportant. It was during this journey that Ibn Battuta first encountered a hippopotamus. The animals were feared by the local boatmen and hunted with lances to which strong cords were attached. After a short stay in Timbuktu, Ibn Battuta journeyed down the Niger to Gao in a canoe carved from a single tree. At the time Gao was an important commercial center.

After spending a month in Gao, Ibn Battuta set off with a large caravan for the oasis of Takedda. On his journey across the desert, he received a message from the Sultan of Morocco commanding him to return home. He set off for Sijilmasa in September 1353, accompanying a large caravan transporting 600 female slaves, and arrived back in Morocco early in 1354.

Ibn Battuta's itinerary gives scholars a glimpse as to when Islam first began to spread into the heart of west Africa.
After returning home from his travels in 1354, and at the suggestion of the Marinid ruler of Morocco, Abu Inan Faris, Ibn Battuta dictated an account of his journeys to Ibn Juzayy, a scholar whom he had previously met in Granada. The account is the only source for Ibn Battuta's adventures. The full title of the manuscript may be translated as "A Gift to Those Who Contemplate the Wonders of Cities and the Marvels of Travelling" (, "Tuḥfat an-Nuẓẓār fī Gharāʾib al-Amṣār wa ʿAjāʾib al-Asfār"). However, it is often simply referred to as "TheTravels" (, "Rihla"), in reference to a standard form of Arabic literature.

There is no indication that Ibn Battuta made any notes or had any journal during his twenty-nine years of travelling. When he came to dictate an account of his experiences he had to rely on memory and manuscripts produced by earlier travellers. Ibn Juzayy did not acknowledge his sources and presented some of the earlier descriptions as Ibn Battuta's own observations. When describing Damascus, Mecca, Medina and some other places in the Middle East, he clearly copied passages from the account by the Andalusian Ibn Jubayr which had been written more than 150 years earlier. Similarly, most of Ibn Juzayy's descriptions of places in Palestine were copied from an account by the 13th-century traveller Muhammad al-Abdari.

Scholars do not believe that Ibn Battuta visited all the places he described and argue that in order to provide a comprehensive description of places in the Muslim world, he relied on hearsay evidence and made use of accounts by earlier travellers. For example, it is considered very unlikely that Ibn Battuta made a trip up the Volga River from New Sarai to visit Bolghar and there are serious doubts about a number of other journeys such as his trip to Sana'a in Yemen, his journey from Balkh to Bistam in Khorasan and his trip around Anatolia. Ibn Battuta's claim that a Maghrebian called "Abu'l Barakat the Berber" converted the Maldives to Islam is contradicted by an entirely different story which says that the Maldives were converted to Islam after miracles were performed by a Tabrizi named Maulana Shaikh Yusuf Shams-ud-din according to the Tarikh, the official history of the Maldives. Some scholars have also questioned whether he really visited China. Ibn Battuta may have plagiarized entire sections of his descriptions of China lifted from works by other authors like "Masalik al-absar fi mamalik al-amsar" by Shihab al-Umari, Sulaiman al-Tajir, and possibly from Al Juwayni, Rashid al din and an Alexander romance. Furthermore, Ibn Battuta’s description and Marco Polo's writings share extremely similar sections and themes, with some of the same commentary, e.g. it is unlikely that the 3rd Caliph Uthman ibn Affan had someone with the exact identical name in China who was encountered by Ibn Battuta. However, even if the "Rihla" is not fully based on what its author personally witnessed, it provides an important account of much of the 14th-century world. Sex slaves were used by Ibn Battuta such as in Delhi. He wedded and divorced women and had children to sex slaves in Malabar, Delhi, and Bukhara. Ibn Battuta insulted Greeks as "enemies of Allah", drunkards and "swine eaters", while at the same time in Ephesus he purchased and used a Greek girl who was one of his many slave girls in his "harem" through Byzantium, Khorasan, Africa, and Palestine. It was two decades before he again returned to find out what happened to one of his wives and child in Damascus.

Ibn Battuta often experienced culture shock in regions he visited where the local customs of recently converted peoples did not fit in with his orthodox Muslim background. Among the Turks and Mongols, he was astonished at the freedom and respect enjoyed by women and remarked that on seeing a Turkish couple in a bazaar one might assume that the man was the woman's servant when he was in fact her husband. He also felt that dress customs in the Maldives, and some sub-Saharan regions in Africa were too revealing.

Little is known about Ibn Battuta's life after completion of his "Rihla" in 1355. He was appointed a judge in Morocco and died in 1368 or 1369.

Ibn Battuta's work was unknown outside the Muslim world until the beginning of the 19th century, when the German traveller-explorer Ulrich Jasper Seetzen (1767–1811) acquired a collection of manuscripts in the Middle East, among which was a 94-page volume containing an abridged version of Ibn Juzayy's text. Three extracts were published in 1818 by the German orientalist Johann Kosegarten. A fourth extract was published the following year. French scholars were alerted to the initial publication by a lengthy review published in the "Journal de Savants" by the orientalist Silvestre de Sacy.

Three copies of another abridged manuscript were acquired by the Swiss traveller Johann Burckhardt and bequeathed to the University of Cambridge. He gave a brief overview of their content in a book published posthumously in 1819. The Arabic text was translated into English by the orientalist Samuel Lee and published in London in 1829.

In the 1830s, during the French occupation of Algeria, the Bibliothèque Nationale (BNF) in Paris acquired five manuscripts of Ibn Battuta's travels, in which two were complete. One manuscript containing just the second part of the work is dated 1356 and is believed to be Ibn Juzayy's autograph. The BNF manuscripts were used in 1843 by the Irish-French orientalist Baron de Slane to produce a translation into French of Ibn Battuta's visit to the Sudan. They were also studied by the French scholars Charles Defrémery and Beniamino Sanguinetti. Beginning in 1853 they published a series of four volumes containing a critical edition of the Arabic text together with a translation into French. In their introduction Defrémery and Sanguinetti praised Lee's annotations but were critical of his translation which they claimed lacked precision, even in straightforward passages.

In 1929, exactly a century after the publication of Lee's translation, the historian and orientalist Hamilton Gibb published an English translation of selected portions of Defrémery and Sanguinetti's Arabic text. Gibb had proposed to the Hakluyt Society in 1922 that he should prepare an annotated translation of the entire "Rihla" into English. His intention was to divide the translated text into four volumes, each volume corresponding to one of the volumes published by Defrémery and Sanguinetti. The first volume was not published until 1958. Gibb died in 1971, having completed the first three volumes. The fourth volume was prepared by Charles Beckingham and published in 1994. Defrémery and Sanguinetti's printed text has now been translated into number of other languages.









</doc>
<doc id="15231" url="https://en.wikipedia.org/wiki?curid=15231" title="Integrated Services Digital Network">
Integrated Services Digital Network

Integrated Services Digital Network (ISDN) is a set of communication standards for simultaneous digital transmission of voice, video, data, and other network services over the traditional circuits of the public switched telephone network. It was first defined in 1988 in the CCITT red book. Prior to ISDN, the telephone system was viewed as a way to transport voice, with some special services available for data. The key feature of ISDN is that it integrates speech and data on the same lines, adding features that were not available in the classic telephone system. The ISDN standards define several kinds of access interfaces, such as Basic Rate Interface (BRI), Primary Rate Interface (PRI), Narrowband ISDN (N-ISDN), and Broadband ISDN (B-ISDN).

ISDN is a circuit-switched telephone network system, which also provides access to packet switched networks, designed to allow digital transmission of voice and data over ordinary telephone copper wires, resulting in potentially better voice quality than an analog phone can provide. It offers circuit-switched connections (for either voice or data), and packet-switched connections (for data), in increments of 64 kilobit/s. In some countries, ISDN found major market application for Internet access, in which ISDN typically provides a maximum of 128 kbit/s bandwidth in both upstream and downstream directions. Channel bonding can achieve a greater data rate; typically the ISDN B-channels of three or four BRIs (six to eight 64 kbit/s channels) are bonded.

ISDN is employed as the network, data-link and physical layers in the context of the OSI model. In common use, ISDN is often limited to usage to Q.931 and related protocols, which are a set of signaling protocols establishing and breaking circuit-switched connections, and for advanced calling features for the user. They were introduced in 1986.

In a videoconference, ISDN provides simultaneous voice, video, and text transmission between individual desktop videoconferencing systems and group (room) videoconferencing systems.

"Integrated services" refers to ISDN's ability to deliver at minimum two simultaneous connections, in any combination of data, voice, video, and fax, over a single line. Multiple devices can be attached to the line, and used as needed. That means an ISDN line can take care of most people's complete communications needs (apart from broadband Internet access and entertainment television) at a much higher transmission rate, without forcing the purchase of multiple analog phone lines. It also refers to integrated switching and transmission in that telephone switching and carrier wave transmission are integrated rather than separate as in earlier technology.

The entry level interface to ISDN is the Basic Rate Interface (BRI), a 128 kbit/s service delivered over a pair of standard telephone copper wires. The 144 kbit/s payload rate is broken down into two 64 kbit/s bearer channels ('B' channels) and one 16 kbit/s signaling channel ('D' channel or data channel). This is sometimes referred to as 2B+D.

The interface specifies the following network interfaces:

BRI-ISDN is very popular in Europe but is much less common in North America. It is also common in Japan — where it is known as INS64.

The other ISDN access available is the Primary Rate Interface (PRI), which is carried over an E1 (2048 kbit/s) in most parts of the world. An E1 is 30 'B' channels of 64 kbit/s, one 'D' channel of 64 kbit/s and a timing and alarm channel of 64 kbit/s. This is often referred to as 30B+2D.
In North America PRI service is delivered on one or more T1 carriers (often referred to as 23B+D) of 1544 kbit/s (24 channels). A PRI has 23 'B' channels and 1 'D' channel for signalling (Japan uses a circuit called a J1, which is similar to a T1). Inter-changeably but incorrectly, a PRI is referred to as T1 because it uses the T1 carrier format. A true T1 (commonly called "Analog T1" to avoid confusion) uses 24 channels of 64 kbit/s of in-band signaling. Each channel uses 56 kb for data and voice and 8 kb for signaling and messaging. PRI uses out of band signaling which provides the 23 B channels with clear 64 kb for voice and data and one 64 kb 'D' channel for signaling and messaging. In North America, Non-Facility Associated Signalling allows two or more PRIs to be controlled by a single D channel, and is sometimes called "23B+D + n*24B". D-channel backup allows for a second D channel in case the primary fails. NFAS is commonly used on a T3.

PRI-ISDN is popular throughout the world, especially for connecting private branch exchanges to the public network.

Even though many network professionals use the term "ISDN" to refer to the lower-bandwidth BRI circuit, in North America BRI is relatively uncommon whilst PRI circuits serving PBXs are commonplace.

The bearer channel (B) is a standard 64 kbit/s voice channel of 8 bits sampled at 8 kHz with G.711 encoding. B-channels can also be used to carry data, since they are nothing more than digital channels.

Each one of these channels is known as a DS0.

Most B channels can carry a 64kbit/s signal, but some were limited to 56K because they traveled over RBS lines. This was commonplace in the 20th century, but has since become less so.

The signaling channel (D) uses Q.931 for signaling with the other side of the link.

X.25 can be carried over the B or D channels of a BRI line, and over the B channels of a PRI line. X.25 over the D channel is used at many point-of-sale (credit card) terminals because it eliminates the modem setup, and because it connects to the central system over a B channel, thereby eliminating the need for modems and making much better use of the central system's telephone lines.

X.25 was also part of an ISDN protocol called "Always On/Dynamic ISDN", or AO/DI. This allowed a user to have a constant multi-link PPP connection to the internet over X.25 on the D channel, and brought up one or two B channels as needed.

In theory, Frame Relay can operate over the D channel of BRIs and PRIs, but it is seldom, if ever, used.

There is a second viewpoint: that of the telephone industry, where ISDN is a core technology. A telephone network can be thought of as a collection of wires strung between switching systems. The common electrical specification for the signals on these wires is T1 or E1. Between telephone company switches, the signaling is performed via SS7. Normally, a PBX is connected via a T1 with robbed bit signaling to indicate on-hook or off-hook conditions and MF and DTMF tones to encode the destination number. ISDN is much better because messages can be sent much more quickly than by trying to encode numbers as long (100 ms per digit) tone sequences. This results in faster call setup times. Also, a greater number of features are available and fraud is reduced.

ISDN is also used as a smart-network technology intended to add new services to the public switched telephone network (PSTN) by giving users direct access to end-to-end circuit-switched digital services and as a backup or failsafe circuit solution for critical use data circuits.

ISDN is used heavily by the broadcast industry as a reliable way of switching low-latency, high-quality, long-distance audio circuits. In conjunction with an appropriate codec using MPEG or various manufacturers' proprietary algorithms, an ISDN BRI can be used to send stereo bi-directional audio coded at 128 kbit/s with 20 Hz – 20 kHz audio bandwidth, although commonly the G.722 algorithm is used with a single 64 kbit/s B channel to send much lower latency mono audio at the expense of audio quality. Where very high quality audio is required multiple ISDN BRIs can be used in parallel to provide a higher bandwidth circuit switched connection. BBC Radio 3 commonly makes use of three ISDN BRIs to carry 320 kbit/s audio stream for live outside broadcasts. ISDN BRI services are used to link remote studios, sports grounds and outside broadcasts into the main broadcast studio. ISDN via satellite is used by field reporters around the world. It is also common to use ISDN for the return audio links to remote satellite broadcast vehicles.

In many countries, such as the UK and Australia, ISDN has displaced the older technology of equalised analogue landlines, with these circuits being phased out by telecommunications providers. Use of IP-based streaming codecs such as Comrex ACCESS and ipDTL is becoming more widespread in the broadcast sector, using broadband internet to connect remote studios.

ISDN-BRI never gained popularity as a general use telephone access technology in Canada and the US, and remains a niche product. The service was seen as "a solution in search of a problem", and the extensive array of options and features were difficult for customers to understand and use. ISDN has long been known by derogatory backronyms highlighting these issues, such as It Still Does Nothing, Innovations Subscribers Don't Need," and I Still Don't kNow."

Once the concept of "broadband Internet access" came to be associated with data rates incoming to the customer at 256 kbit/s or more, and alternatives like ADSL grew in popularity, the consumer market for BRI did not develop. Its only remaining advantage is that, while ADSL has a functional distance limitation and can use ADSL loop extenders, BRI has a greater limit and can use repeaters. As such, BRI may be acceptable for customers who are too remote for ADSL. Widespread use of BRI is further stymied by some small North American CLECs such as CenturyTel having given up on it and not providing Internet access using it. However, AT&T in most states (especially the former SBC/SWB territory) will still install an ISDN BRI line anywhere a normal analog line can be placed and the monthly charge is roughly $55.

ISDN-BRI is currently primarily used in industries with specialized and very specific needs. High-end videoconferencing hardware made by companies such as Sony, Polycom, Tandberg, and LifeSize via the LifeSize Networker can bond up to 8 B-channels together (using a BRI circuit for every 2 channels) to provide digital, circuit-switched video connections to almost anywhere in the world. This is very expensive, and is being replaced by IP-based conferencing, but where cost concern is less of an issue than predictable quality and where a QoS-enabled IP does not exist, BRI is the preferred choice.

Most modern non-VoIP PBXs use ISDN-PRI circuits. These are connected via T1 lines with the central office switch, replacing older analog two-way and direct inward dialing (DID) trunks. PRI is capable of delivering Calling Line Identification (CLID) in both directions so that the telephone number of an extension, rather than a company's main number, can be sent. It is still commonly used in recording studios, when a voice-over actor is in one studio (possibly telecommuting from home), but the director and producer are in a studio at another location. The ISDN protocol delivers channelized, not-over-the-Internet service, powerful call setup and routing features, faster setup and tear down, superior audio fidelity as compared to POTS (plain old telephone service), lower delay and, at higher densities, lower cost.

In 2013, Verizon announced it would no longer take orders for ISDN service in the Northeastern United States.

Bharat Sanchar Nigam Limited, Reliance Communications and Bharti Airtel are the largest communication service providers, and offer both ISDN BRI and PRI services across the country. Reliance Communications and Bharti Airtel uses the DLC technology for providing these services. With the introduction of broadband technology, the load on bandwidth is being absorbed by ADSL. ISDN continues to be an important backup network for point-to-point leased line customers such as banks, Eseva Centers, Life Insurance Corporation of India, and SBI ATMs.

On April 19, 1988, Japanese telecommunications company NTT began offering nationwide ISDN services trademarked INS Net 64, and INS Net 1500, a fruition of NTT's independent research and trial from the 1970s of what it referred to the INS (Information Network System).

Previously, in April 1985, Japanese digital telephone exchange hardware made by Fujitsu was used to experimentally deploy the world's first I interface ISDN. The I interface, unlike the older and incompatible Y interface, is what modern ISDN services use today.

Since 2000, NTT's ISDN offering have been known as FLET's ISDN, incorporating the "FLET's" brand that NTT uses for all of its ISP offerings.

In Japan, the number of ISDN subscribers dwindled as alternative technologies such as ADSL, cable Internet access, and fiber to the home gained greater popularity. On November 2, 2010, NTT announced plans to migrate their backend from PSTN to the IP network from around 2020 to around 2025. For this migration, ISDN services will be retired, and fiber optic services are recommended as an alternative.

In the United Kingdom, British Telecom (BT) provides ISDN2e (BRI) as well as ISDN30 (PRI). Until April 2006, they also offered services named Home Highway and Business Highway, which were BRI ISDN-based services that offered integrated analogue connectivity as well as ISDN. Later versions of the Highway products also included built-in USB sockets for direct computer access. Home Highway was bought by many home users, usually for Internet connection, although not as fast as ADSL, because it was available before ADSL and in places where ADSL does not reach.

In early 2015, BT announced their intention to retire the UK's ISDN infrastructure by 2025.

France Telecom offers ISDN services under their product name Numeris (2 B+D), of which a professional Duo and home Itoo version is available. ISDN is generally known as RNIS in France and has widespread availability. The introduction of ADSL is reducing ISDN use for data transfer and Internet access, although it is still common in more rural and outlying areas, and for applications such as business voice and point-of-sale terminals.

In Germany, ISDN was very popular with an installed base of 25 million channels (29% of all subscriber lines in Germany as of 2003 and 20% of all ISDN channels worldwide). Due to the success of ISDN, the number of installed analog lines was decreasing. Deutsche Telekom (DTAG) offered both BRI and PRI. Competing phone companies often offered ISDN only and no analog lines. However, these operators generally offered free hardware that also allows the use of POTS equipment, such as NTBAs with integrated terminal adapters. Because of the widespread availability of ADSL services, ISDN was primarily used for voice and fax traffic.

Until 2007 ISDN (BRI) and ADSL/VDSL were often bundled on the same line, mainly because the combination of ADSL with an analog line had no cost advantage over a combined ISDN-ADSL line. This advantage diminished when vendors of ISDN technology stopped manufacturing it and spare parts became hard to come by. Since then phone companies started introducing cheaper ADSL-only products using VoIP for telephony.

Since the introduction of VDSL2 using outdoor MSANs, ISDN has become obsolete. Today new ISDN lines are not available anymore in Germany and existing ISDN lines will be phased out by 2018 and replaced by G.992.3 Annex J all-digital-mode ADSL.

OTE, the incumbent telecommunications operator, offers ISDN BRI (BRA) services in Greece. Following the launch of ADSL in 2003, the importance of ISDN for data transfer began to decrease and is today limited to niche business applications with point-to-point requirements.

A study of the German Department of Science shows the following spread of ISDN-channels per 1,000 inhabitants in the year 2005:

In ISDN, there are two types of channels, "B" (for "bearer") and "D" (for "data"). "B channels" are used for data (which may include voice), and "D channels" are intended for signaling and control (but can also be used for data).

There are two ISDN implementations. Basic Rate Interface (BRI), also called basic rate access (BRA) — consists of two B channels, each with bandwidth of 64 kbit/s, and one D channel with a bandwidth of 16 kbit/s. Together these three channels can be designated as 2B+D. Primary Rate Interface (PRI), also called primary rate access (PRA) in Europe — contains a greater number of B channels and a D channel with a bandwidth of 64 kbit/s. The number of B channels for PRI varies according to the nation: in North America and Japan it is 23B+1D, with an aggregate bit rate of 1.544 Mbit/s (T1); in Europe, India and Australia it is 30B+2D, with an aggregate bit rate of 2.048 Mbit/s (E1). Broadband Integrated Services Digital Network (BISDN) is another ISDN implementation and it is able to manage different types of services at the same time. It is primarily used within network backbones and employs ATM.

Another alternative ISDN configuration can be used in which the B channels of an ISDN BRI line are bonded to provide a total duplex bandwidth of 128 kbit/s. This precludes use of the line for voice calls while the internet connection is in use. The B channels of several BRIs can be bonded, a typical use is a 384K videoconferencing channel.

Using bipolar with eight-zero substitution encoding technique, call data is transmitted over the data (B) channels, with the signaling (D) channels used for call setup and management. Once a call is set up, there is a simple 64 kbit/s synchronous bidirectional data channel (actually implemented as two simplex channels, one in each direction) between the end parties, lasting until the call is terminated. There can be as many calls as there are bearer channels, to the same or different end-points. Bearer channels may also be multiplexed into what may be considered single, higher-bandwidth channels via a process called B channel BONDING, or via use of Multi-Link PPP "bundling" or by using an H0, H11, or H12 channel on a PRI.

The D channel can also be used for sending and receiving X.25 data packets, and connection to X.25 packet network, this is specified in X.31. In practice, X.31 was only commercially implemented in the UK, France, Japan and Germany.

A set of "reference points" are defined in the ISDN standard to refer to certain points between the telco and the end user ISDN equipment.

Most NT-1 devices can perform the functions of the NT2 as well, and so the S and T reference points are generally collapsed into the S/T reference point.

In North America, the NT1 device is considered customer premises equipment (CPE) and must be maintained by the customer, thus, the U interface is provided to the customer. In other locations, the NT1 device is maintained by the telco, and the S/T interface is provided to the customer. In India, service providers provide U interface and an NT1 may be supplied by Service provider as part of service offering.

Among the kinds of data that can be moved over the 64 kbit/s channels are pulse-code modulated voice calls, providing access to the traditional voice PSTN. This information can be passed between the network and the user end-point at call set-up time. In North America, ISDN is now used mostly as an alternative to analog connections, most commonly for Internet access. Some of the services envisioned as being delivered over ISDN are now delivered over the Internet instead. In Europe, and in Germany in particular, ISDN has been successfully marketed as a phone with features, as opposed to a POTS phone with few or no features. Meanwhile, features that were first available with ISDN (such as Three-Way Calling, Call Forwarding, Caller ID, etc.) are now commonly available for ordinary analog phones as well, eliminating this advantage of ISDN. Another advantage of ISDN was the possibility of multiple simultaneous calls (one call per B channel), e.g. for big families, but with the increased popularity and reduced prices of mobile telephony this has become less interesting as well, making ISDN unappealing to the private customer. However, ISDN is typically more reliable than POTS, and has a significantly faster call setup time compared with POTS, and IP connections over ISDN typically have some 30–35ms round trip time, as opposed to 120–180ms (both measured with otherwise unused lines) over 56k or V.34/V.92 modems, making ISDN more reliable and more efficient for telecommuters.

Where an analog connection requires a modem, an ISDN connection requires a terminal adapter (TA). The function of an ISDN terminal adapter is often delivered in the form of a PC card with an S/T interface, and single-chip solutions seem to exist, considering the plethora of combined ISDN- and ADSL-routers.

ISDN is commonly used in radio broadcasting. Since ISDN provides a high quality connection this assists in delivering good quality audio for transmission in radio. Most radio studios are equipped with ISDN lines as their main form of communication with other studios or standard phone lines. Equipment made by companies such as Telos/Omnia (the popular Zephyr codec), Comrex, Tieline and others are used regularly by radio broadcasters. Almost all live sports broadcasts on radio are backhauled to their main studios via ISDN connections.

The following is an example of a Primary Rate (PRI) ISDN call showing the Q.921/LAPD and the Q.931/Network message intermixed (i.e. exactly what was exchanged on the D-channel). The call is originating from the switch where the trace was taken and goes out to some other switch, possibly an end-office LEC, who terminates the call.

The first line format is <time> <D-channel> <Transmitted/Received> <LAPD/ISDN message ID>. If the message is an ISDN level message, then a decoding of the message is attempted showing the various Information Elements that make up the message. All ISDN messages are tagged with an ID number relative to the switch that started the call (local/remote). Following this optional decoding is a dump of the bytes of the message in <offset> <hex> ... <hex> <ascii> ... <ascii> format.

The RR messages at the beginning prior to the call are the keep alive messages. SETUP message indicate the start of the call. Each message is acknowledged by the other side with a RR.


Specifications defining the physical layer and part of the data link layers of ISDN:

From the point of view of the OSI architecture, an ISDN line has a stack of three protocols



</doc>
<doc id="15235" url="https://en.wikipedia.org/wiki?curid=15235" title="Genomic imprinting">
Genomic imprinting

Genomic imprinting is an epigenetic phenomenon that causes genes to be expressed in a parent-of-origin-specific manner. Forms of genomic imprinting have been demonstrated in fungi, plants and animals. As of 2014, there are about 150 imprinted genes known in the mouse and about half that in humans.

Genomic imprinting is an inheritance process independent of the classical Mendelian inheritance. It is an epigenetic process that involves DNA methylation and histone methylation without altering the genetic sequence. These epigenetic marks are established ("imprinted") in the germline (sperm or egg cells) of the parents and are maintained through mitotic cell divisions in the somatic cells of an organism.

Appropriate imprinting of certain genes is important for normal development. Human diseases involving genomic imprinting include Angelman syndrome and Prader–Willi syndrome.

In diploid organisms (like humans), the somatic cells possess two copies of the genome, one inherited from the father and one from the mother. Each autosomal gene is therefore represented by two copies, or alleles, with one copy inherited from each parent at fertilization. For the vast majority of autosomal genes, expression occurs from both alleles simultaneously. In mammals, however, a small proportion (<1%) of genes are imprinted, meaning that gene expression occurs from only one allele (some recent studies have questioned this assertion, claiming that the number of regions of parent-of-origin methylation in, for example, the human genome, is much larger than previously thought). The expressed allele is dependent upon its parental origin. For example, the gene encoding insulin-like growth factor 2 (IGF2/Igf2) is only expressed from the allele inherited from the father.

The term "imprinting" was first used to describe events in the insect "Pseudococcus nipae". In Pseudococcids (mealybugs) (Hemiptera, Coccoidea) both the male and female develop from a fertilised egg. In females, all chromosomes remain euchromatic and functional. In embryos destined to become males, one haploid set of chromosomes becomes heterochromatinised after the sixth cleavage division and remains so in most tissues; males are thus functionally haploid.

That imprinting might be a feature of mammalian development was suggested in breeding experiments in mice carrying reciprocal chromosomal translocations. Nucleus transplantation experiments in mouse zygotes in the early 1980s confirmed that normal development requires the contribution of both the maternal and paternal genomes. The vast majority of mouse embryos derived from parthenogenesis (called parthenogenones, with two maternal or egg genomes) and androgenesis (called androgenones, with two paternal or sperm genomes) die at or before the blastocyst/implantation stage. In the rare instances that they develop to postimplantation stages, gynogenetic embryos show better embryonic development relative to placental development, while for androgenones, the reverse is true. Nevertheless, for the latter, only a few have been described (in a 1984 paper).

No naturally occurring cases of parthenogenesis exist in mammals because of imprinted genes. However, in 2004, experimental manipulation by Japanese researchers of a paternal methylation imprint controlling the "Igf2" gene led to the birth of a mouse (named Kaguya) with two maternal sets of chromosomes, though it is not a true parthenogenone since cells from two different female mice were used. The researchers were able to succeed by using one egg from an immature parent, thus reducing maternal imprinting, and modifying it to express the gene Igf2, which is normally only expressed by the paternal copy of the gene.

Parthenogenetic/gynogenetic embryos have twice the normal expression level of maternally derived genes, and lack expression of paternally expressed genes, while the reverse is true for androgenetic embryos. It is now known that there are at least 80 imprinted genes in humans and mice, many of which are involved in embryonic and placental growth and development. Hybrid offspring of two species may exhibit unusual growth due to the novel combination of imprinted genes.

Various methods have been used to identify imprinted genes. In swine, Bischoff "et al." 2009 compared transcriptional profiles using short-oligonucleotide microarrays to survey differentially expressed genes between parthenotes (2 maternal genomes) and control fetuses (1 maternal, 1 paternal genome). An intriguing study surveying the transcriptome of murine brain tissues revealed over 1300 imprinted gene loci (approximately 10-fold more than previously reported) by RNA-sequencing from F1 hybrids resulting from reciprocal crosses. The result however has been challenged by others who claimed that this is an overestimation by an order of magnitude due to flawed statistical analysis.

In domesticated livestock, single-nucleotide polymorphisms in imprinted genes influencing foetal growth and development have been shown to be associated with economically important production traits in cattle, sheep and pigs.

At the same time as the generation of the gynogenetic and androgenetic embryos discussed above, mouse embryos were also being generated that contained only small regions that were derived from either a paternal or maternal source. The generation of a series of such uniparental disomies, which together span the entire genome, allowed the creation of an imprinting map. Those regions which when inherited from a single parent result in a discernible phenotype contain imprinted gene(s). Further research showed that within these regions there were often numerous imprinted genes. Around 80% of imprinted genes are found in clusters such as these, called imprinted domains, suggesting a level of co-ordinated control. More recently, genome-wide screens to identify imprinted genes have used differential expression of mRNAs from control fetuses and parthenogenetic or androgenetic fetuses hybridized to expression arrays, allele-specific gene expression using SNP genotyping arrays, transcriptome sequencing, and in silico prediction pipelines.

Imprinting is a dynamic process. It must be possible to erase and re-establish imprints through each generation so that genes that are imprinted in an adult may still be expressed in that adult's offspring. (For example, the maternal genes that control insulin production will be imprinted in a male but will be expressed in any of the male's offspring that inherit these genes.) The nature of imprinting must therefore be epigenetic rather than DNA sequence dependent. In germline cells the imprint is erased and then re-established according to the sex of the individual, i.e. in the developing sperm (during spermatogenesis), a paternal imprint is established, whereas in developing oocytes (oogenesis), a maternal imprint is established. This process of erasure and reprogramming is necessary such that the germ cell imprinting status is relevant to the sex of the individual. In both plants and mammals there are two major mechanisms that are involved in establishing the imprint; these are DNA methylation and histone modifications.

Recently, a new study has suggested a novel inheritable imprinting mechanism in humans that would be specific of placental tissue and that is independent of DNA methylation (the main and classical mechanism for genomic imprinting). Among the hypothetical explanations for this exclusively human phenomenon, two possible mechanisms have been proposed: either a histone modification that confers imprinting at novel placental-specific imprinted "loci" or, alternatively, a recruitment of DNMTs to these loci by a specific and unknown transcription factor that would be expressed during early trophoblast differentiation.

The grouping of imprinted genes within clusters allows them to share common regulatory elements, such as non-coding RNAs and differentially methylated regions (DMRs). When these regulatory elements control the imprinting of one or more genes, they are known as imprinting control regions (ICR). The expression of non-coding RNAs, such as "Air" on mouse chromosome 17 and KCNQ1OT1 on human chromosome 11p15.5, have been shown to be essential for the imprinting of genes in their corresponding regions.

Differentially methylated regions are generally segments of DNA rich in cytosine and guanine nucleotides, with the cytosine nucleotides methylated on one copy but not on the other. Contrary to expectation, methylation does not necessarily mean silencing; instead, the effect of methylation depends upon the default state of the region.

The control of expression of specific genes by genomic imprinting is unique to therian mammals (placental mammals and marsupials) and flowering plants. Imprinting of whole chromosomes has been reported in mealybugs (Genus: "Pseudococcus"). and a fungus gnat ("Sciara"). It has also been established that X-chromosome inactivation occurs in an imprinted manner in the extra-embryonic tissues of mice and all tissues in marsupials, where it is always the paternal X-chromosome which is silenced.

The majority of imprinted genes in mammals have been found to have roles in the control of embryonic growth and development, including development of the placenta. Other imprinted genes are involved in post-natal development, with roles affecting suckling and metabolism.

A widely accepted hypothesis for the evolution of genomic imprinting is the "parental conflict hypothesis". Also known as the kinship theory of genomic imprinting, this hypothesis states that the inequality between parental genomes due to imprinting is a result of the differing interests of each parent in terms of the evolutionary fitness of their genes. The father's genes that encode for imprinting gain greater fitness through the success of the offspring, at the expense of the mother. The mother's evolutionary imperative is often to conserve resources for her own survival while providing sufficient nourishment to current and subsequent litters. Accordingly, paternally expressed genes tend to be growth-promoting whereas maternally expressed genes tend to be growth-limiting. In support of this hypothesis, genomic imprinting has been found in all placental mammals, where post-fertilisation offspring resource consumption at the expense of the mother is high; although it has also been found in oviparous birds where there is relatively little post-fertilisation resource transfer and therefore less parental conflict.

However, our understanding of the molecular mechanisms behind genomic imprinting show that it is the maternal genome that controls much of the imprinting of both its own and the paternally-derived genes in the zygote, making it difficult to explain why the maternal genes would willingly relinquish their dominance to that of the paternally-derived genes in light of the conflict hypothesis.

Another hypothesis proposed is that some imprinted genes act coadaptively to improve both fetal development and maternal provisioning for nutrition and care. In it a subset of paternally expressed genes are co-expressed in both the placenta and the mother's hypothalamus. This would come about through selective pressure from parent-infant coadaptation to improve infant survival. Paternally expressed 3 (Peg3) is a gene for which this hypothesis may apply.

Others have approached their study of the origins of genomic imprinting from a different side, arguing that natural selection is operating on the role of epigenetic marks as machinery for homologous chromosome recognition during meiosis, rather than on their role in differential expression. This argument centers on the existence of epigenetic effects on chromosomes that do not directly affect gene expression, but do depend on which parent the chromosome originated from. This group of epigenetic changes that depend on the chromosome's parent of origin (including both those that affect gene expression and those that do not) are called parental origin effects, and include phenomena such as paternal X inactivation in the marsupials, nonrandom parental chromatid distribution in the ferns, and even mating type switching in yeast. This diversity in organisms that show parental origin effects has prompted theorists to place the evolutionary origin of genomic imprinting before the last common ancestor of plants and animals, over a billion years ago.

Natural selection for genomic imprinting requires genetic variation in a population. A hypothesis for the origin of this genetic variation states that the host-defense system responsible for silencing foreign DNA elements, such as genes of viral origin, mistakenly silenced genes whose silencing turned out to be beneficial for the organism. There appears to be an over-representation of retrotransposed genes, that is to say genes that are inserted into the genome by viruses, among imprinted genes. It has also been postulated that if the retrotransposed gene is inserted close to another imprinted gene, it may just acquire this imprint.

Imprinting may cause problems in cloning, with clones having DNA that is not methylated in the correct positions. It is possible that this is due to a lack of time for reprogramming to be completely achieved. When a nucleus is added to an egg during somatic cell nuclear transfer, the egg starts dividing in minutes, as compared to the days or months it takes for reprogramming during embryonic development. If time is the responsible factor, it may be possible to delay cell division in clones, giving time for proper reprogramming to occur.

An allele of the "callipyge" (from the Greek for "beautiful buttocks"), or CLPG, gene in sheep produces large buttocks consisting of muscle with very little fat. The large-buttocked phenotype only occurs when the allele is present on the copy of chromosome 18 inherited from a sheep's father and is "not" on the copy of chromosome 18 inherited from that sheep's mother.

In vitro fertilisation, including ICSI, is associated with an increased risk of imprinting disorders, with an odds ratio of 3.7 (95% confidence interval 1.4 to 9.7).

The first imprinted genetic disorders to be described in humans were the reciprocally inherited Prader-Willi syndrome and Angelman syndrome. Both syndromes are associated with loss of the chromosomal region 15q11-13 (band 11 of the long arm of chromosome 15). This region contains the paternally expressed genes SNRPN and NDN and the maternally expressed gene UBE3A.

DIRAS3 is a paternally expressed and maternally imprinted gene located on chromosome 1 in humans. Reduced DIRAS3 expression is linked to an increased risk of ovarian and breast cancers; in 41% of breast and ovarian cancers the protein encoded by DIRAS3 is not expressed, suggesting that it functions as a tumor suppressor gene Therefore, if uniparental disomy occurs and a person inherits both chromosomes from the mother, the gene will not be expressed and the individual is put at a greater risk for breast and ovarian cancer.

Other conditions involving imprinting include Beckwith-Wiedemann syndrome, Silver-Russell syndrome, and pseudohypoparathyroidism.

Transient neonatal diabetes mellitus can also involve imprinting.

The "imprinted brain theory" argues that unbalanced imprinting may be a cause of autism and psychosis.

In insects, imprinting affects entire chromosomes. In some insects the entire paternal genome is silenced in male offspring, and thus is involved in sex determination. The imprinting produces effects similar to the mechanisms in other insects that eliminate paternally inherited chromosomes in male offspring, including arrhenotoky.

In placental species, parent-offspring conflict can result in the evolution of strategies, such as genomic imprinting, for embryos to subvert maternal nutrient provisioning. Despite several attempts to find it, genomic imprinting has not been found in the platypus, reptiles, birds or fish. The absence of genomic imprinting in a placental reptile, the southern grass skink, is interesting as genomic imprinting was thought to be associated with the evolution of viviparity and placental nutrient transport.

Studies in domestic livestock, such as dairy and beef cattle, have implicated imprinted genes (e.g. IGF2) in a range of economic traits, including dairy performance in Holstein-Friesian cattle.

A similar imprinting phenomenon has also been described in flowering plants (angiosperms). During fertilisation of the egg cell, a second, separate fertilization event gives rise to the endosperm, an extraembryonic structure that nourishes the embryo in a manner analogous to the mammalian placenta. Unlike the embryo, the endosperm is often formed from the fusion of two maternal cells with a male gamete. This results in a triploid genome. The 2:1 ratio of maternal to paternal genomes appears to be critical for seed development. Some genes are found to be expressed from both maternal genomes while others are expressed exclusively from the lone paternal copy. It has been suggested that these imprinted genes are responsible for the triploid block effect in flowering plants that prevents hybridization between diploids and autotetraploids.




</doc>
<doc id="15236" url="https://en.wikipedia.org/wiki?curid=15236" title="ICANN">
ICANN

The Internet Corporation for Assigned Names and Numbers (ICANN ) is a nonprofit organization responsible for coordinating the maintenance and procedures of several databases related to the namespaces and numericalspaces of the Internet, ensuring the network's stable and secure operation. ICANN performs the actual technical maintenance work of the Central Internet Address pools and DNS root zone registries pursuant to the Internet Assigned Numbers Authority (IANA) function contract. The contract regarding the IANA stewardship functions between ICANN and the National Telecommunications and Information Administration (NTIA) of the United States Department of Commerce ended on October 1, 2016, formally transitioning the functions to the global multistakeholder community.

Much of its work has concerned the Internet's global Domain Name System (DNS), including policy development for internationalization of the DNS system, introduction of new generic top-level domains (TLDs), and the operation of root name servers. The numbering facilities ICANN manages include the Internet Protocol address spaces for IPv4 and IPv6, and assignment of address blocks to regional Internet registries. ICANN also maintains registries of Internet Protocol identifiers.

ICANN's primary principles of operation have been described as helping preserve the operational stability of the Internet; to promote competition; to achieve broad representation of the global Internet community; and to develop policies appropriate to its mission through bottom-up, consensus-based processes.

ICANN was created on September 18, 1998, and incorporated on September 30, 1998, in the U.S. state of California. It is headquartered in the Playa Vista neighborhood of Los Angeles.

Before the establishment of ICANN, the IANA function of administering registries of Internet protocol identifiers (including the distributing top-level domains and IP addresses) was performed by Jon Postel, a Computer Science researcher who had been involved in the creation of ARPANET, first at UCLA and then at the University of Southern California's Information Sciences Institute (ISI). In 1997 Postel testified before Congress that this had come about as a "side task" to this research work. The Information Sciences Institute was funded by the U.S. Department of Defense, as was SRI International's Network Information Center, which also performed some assigned name functions.

As the Internet grew and expanded globally, the U.S. Department of Commerce initiated a process to establish a new organization to perform the IANA functions. On January 30, 1998, the National Telecommunications and Information Administration (NTIA), an agency of the U.S. Department of Commerce, issued for comment, "A Proposal to Improve the Technical Management of Internet Names and Addresses." The proposed rule making, or "Green Paper", was published in the Federal Register on February 20, 1998, providing opportunity for public comment. NTIA received more than 650 comments as of March 23, 1998, when the comment period closed.

The Green Paper proposed certain actions designed to privatize the management of Internet names and addresses in a manner that allows for the development of competition and facilitates global participation in Internet management. The Green Paper proposed for discussion a variety of issues relating to DNS management including private sector creation of a new not-for-profit corporation (the "new corporation") managed by a globally and functionally representative board of directors. ICANN was formed in response to this policy. ICANN managed the Internet Assigned Numbers Authority (IANA) under contract to the United States Department of Commerce (DOC) and pursuant to an agreement with the IETF.

ICANN was incorporated in California on September 30, 1998, with entrepreneur and philanthropist Esther Dyson as founding chairwoman. It is a nonprofit public benefit corporation "organized under the California Nonprofit Public Benefit Corporation Law for charitable and public purposes." ICANN was established in California due to the presence of Jon Postel, who was a founder of ICANN and was set to be its first Chief Technology Officer prior to his unexpected death. ICANN formerly operated from the same Marina del Rey building where Postel formerly worked, which is home to an office of the Information Sciences Institute at the University of Southern California. However, ICANN's headquarters is now located in the nearby Playa Vista neighborhood of Los Angeles.

Per its original by-laws, primary responsibility for policy formation in ICANN was to be delegated to three supporting organizations (Address Supporting Organization, Domain Name Supporting Organization, and Protocol Supporting Organization), each of which was to develop and recommend substantive policies and procedures for the management of the identifiers within their respective scope. They were also required to be financially independent from ICANN. As expected, the Regional Internet Registries and the IETF agreed to serve as the Address Supporting Organization and Protocol Supporting Organization respectively, and ICANN issued a call for interested parties to propose the structure and composition of the Domain Name Supporting Organization. In March 1999, the ICANN Board, based in part on the DNSO proposals received, decided instead on an alternate construction for the DNSO which delineated specific constituencies bodies within ICANN itself, thus adding primary responsibility for DNS policy development to ICANN's existing duties of oversight and coordination.

On July 26, 2006, the United States government renewed the contract with ICANN for performance of the IANA function for an additional one to five years. The context of ICANN's relationship with the U.S. government was clarified on September 29, 2006 when ICANN signed a new Memorandum of Understanding with the United States Department of Commerce (DOC). This document gave the DOC oversight over some of the ICANN operations.

During July 2008, the DOC reiterated an earlier statement that it has "no plans to transition management of the authoritative root zone file to ICANN". The letter also stresses the separate roles of the IANA and VeriSign.

On September 30, 2009, ICANN signed an agreement with the DOC (known as the "Affirmation of Commitments") that confirmed ICANN's commitment to a multistakeholder governance model, but did not remove it from DOC oversight and control.

On March 10, 2016, ICANN and the DOC signed a historic, culminating agreement to finally remove ICANN and IANA from the control and oversight of the DOC. On October 1, 2016, ICANN was freed from U.S. government oversight.

On March 18, 2002, publicly elected At-Large Representative for North America board member Karl Auerbach sued ICANN in Superior Court in California to gain access to ICANN's accounting records without restriction. Auerbach won.

During September and October 2003, ICANN played a crucial role in the conflict over VeriSign's "wild card" DNS service Site Finder. After an open letter from ICANN issuing an ultimatum to VeriSign, later endorsed by the Internet Architecture Board, the company voluntarily ended the service on October 4, 2003. After this action, VeriSign filed a lawsuit against ICANN on February 27, 2004, claiming that ICANN had exceeded its authority. By this lawsuit, VeriSign sought to reduce ambiguity about ICANN's authority. The antitrust component of VeriSign's claim was dismissed during August 2004. VeriSign's challenge that ICANN overstepped its contractual rights is currently outstanding. A proposed settlement already approved by ICANN's board would resolve VeriSign's challenge to ICANN in exchange for the right to increase pricing on .com domains. At the meeting of ICANN in Rome, which took place from March 2 to 6, 2004, ICANN agreed to ask approval of the U.S. Department of Commerce for the Waiting List Service of VeriSign.

On May 17, 2004, ICANN published a proposed budget for the year 2004–05. It included proposals to increase the openness and professionalism of its operations, and greatly increased its proposed spending from US$8.27 million to $15.83 million. The increase was to be funded by the introduction of new top-level domains, charges to domain registries, and a fee for some domain name registrations, renewals and transfers (initially USD 0.20 for all domains within a country-code top-level domain, and USD 0.25 for all others). The Council of European National Top Level Domain Registries (CENTR), which represents the Internet registries of 39 countries, rejected the increase, accusing ICANN of a lack of financial prudence and criticizing what it describes as ICANN's "unrealistic political and operational targets". Despite the criticism, the registry agreement for the top-level domains jobs and travel includes a US$2 fee on every domain the licensed companies sell or renew.

After a second round of negotiations during 2004, the TLDs eu, , travel, jobs, mobi, and cat were introduced during 2005.
On February 28, 2006, ICANN's board approved a settlement with VeriSign in the lawsuit resulting from SiteFinder that involved allowing VeriSign (the registry) to raise its registration fees by up to 7% a year. This was criticised by a few members of the U.S. House of Representatives' Small Business Committee.

During February 2007, ICANN began procedures to end accreditation of one of their registrars, RegisterFly amid charges and lawsuits involving fraud, and criticism of ICANN's management of the situation. ICANN has been the subject of criticism as a result of its handling of RegisterFly, and the harm caused to thousands of clients as a result of what has been termed ICANN's "laissez faire attitude toward customer allegations of fraud".

On May 23, 2008, ICANN issued enforcement notices against ten accredited registrars and announced this through a press release entitled "'Worst Spam Offenders' Notified by ICANN, Compliance system working to correct Whois and other issues." This was largely in response to a report issued by KnujOn, called "The 10 Worst Registrars" in terms of spam advertised junk product sites and compliance failure. The mention of the word "spam" in the title of the ICANN memo is somewhat misleading since ICANN does not address issues of spam or email abuse. Website content and usage are not within ICANN's mandate. However, the KnujOn report details how various registrars have not complied with their contractual obligations under the Registrar Accreditation Agreement (RAA). The main point of the KnujOn research was to demonstrate the relationships between compliance failure, illicit product traffic, and spam. The report demonstrated that out of 900 ICANN accredited registrars, fewer than 20 held 90% of the web domains advertised in spam. These same registrars were also most frequently cited by KnujOn as failing to resolve complaints made through the Whois Data Problem Reporting System (WDPRS).

On June 26, 2008, the ICANN Board started a new process of TLD naming policy to take a "significant step forward on the introduction of new generic top-level domains." This program envisions the availability of many new or already proposed domains, as well a new application and implementation process.

On October 1, 2008, ICANN issued breach notices against Joker and Beijing Innovative Linkage Technology Ltd. after further researching reports and complaints issued by KnujOn. These notices gave the registrars 15 days to fix their Whois investigation efforts.

In 2010, ICANN approved a major review of its policies with respect to accountability, transparency, and public participation by the Berkman Center for Internet and Society at Harvard University. This external review was an assistance of the work of ICANN's Accountability and Transparency Review team.

On February 3, 2011, ICANN announced that it had distributed the last batch of its remaining IPv4 addresses to the world's five Regional Internet Registries, the organizations that manage IP addresses in different regions. These registries began assigning the final IPv4 addresses within their regions until they ran out completely.

On June 20, 2011, the ICANN board voted to end most restrictions on the names of generic top-level domains (gTLD). Companies and organizations became able to choose essentially arbitrary top-level Internet domain names. The use of non-Latin characters (such as Cyrillic, Arabic, Chinese, etc.) is also allowed in gTLDs. ICANN began accepting applications for new gTLDS on January 12, 2012. The initial price to apply for a new gTLD was set at $185,000 and the annual renewal fee is $25,000.

Following the 2013 NSA spying scandal, ICANN endorsed the Montevideo Statement, although no direct connection between these can be proven.

On October 1, 2016, ICANN ended its contract with the United States Department of Commerce National Telecommunications and Information Administration (NTIA) and entered the private sector.

On May 25, 2018, the EU attempts to enforce its GDPR shall impact on ICANN operations. The deadline coincided with French President Macron's hosting of megatech bosses such as Facebook CEO Mark Zuckerberg, Microsoft CEO Satya Nadella, Uber CEO Dara Khosrowshahi, IBM CEO Ginni Rometty, Intel CEO Brian Krzanich, Samsung President Young Sohn, and SAP CEO Bill McDermott.

From its founding to the present, ICANN has been formally organized as a nonprofit corporation "for charitable and public purposes" under the California Nonprofit Public Benefit Corporation Law. It is managed by a 16-member board of directors composed of eight members selected by a nominating committee on which all the constituencies of ICANN are represented; six representatives of its Supporting Organizations, sub-groups that deal with specific sections of the policies under ICANN's purview; an at-large seat filled by an at-large organization; and the President / CEO, appointed by the board.

There are currently three supporting organizations: the Generic Names Supporting Organization (GNSO) deals with policy making on generic top-level domains (gTLDs); the Country Code Names Supporting Organization (ccNSO) deals with policy making on country-code top-level domains (ccTLDs); the Address Supporting Organization (ASO) deals with policy making on IP addresses.

ICANN also relies on some advisory committees and other advisory mechanisms to receive advice on the interests and needs of stakeholders that do not directly participate in the Supporting Organizations. These include the Governmental Advisory Committee (GAC), which is composed of representatives of a large number of national governments from all over the world; the At-Large Advisory Committee (ALAC), which is composed of individual Internet users from around the world selected by each of the Regional At-Large Organizations (RALO) and Nominating Committee; the Root Server System Advisory Committee, which provides advice on the operation of the DNS root server system; the Security and Stability Advisory Committee (SSAC), which is composed of Internet experts who study security issues pertaining to ICANN's mandate; and the Technical Liaison Group (TLG), which is composed of representatives of other international technical organizations that focus, at least in part, on the Internet.

The Governmental Advisory Committee has representatives from 111 states (108 UN members, the Holy See, Cook Islands, Niue and Taiwan), Hong Kong, Bermuda, Montserrat, the European Commission and the African Union Commission.

In addition the following organizations are GAC Observers:


In the Memorandum of understanding that set up the relationship between ICANN and the U.S. government, ICANN was given a mandate requiring that it operate "in a bottom up, consensus driven, democratic manner." However, the attempts that ICANN have made to establish an organizational structure that would allow wide input from the global Internet community did not produce results amenable to the current Board. As a result, the At-Large constituency and direct election of board members by the global Internet community were soon abandoned.

ICANN holds periodic public meetings rotated between continents for the purpose of encouraging global participation in its processes. Resolutions of the ICANN Board, preliminary reports, and minutes of the meetings, are published on the ICANN website, sometimes in real time. However, there are criticisms from ICANN constituencies including the Noncommercial Users Constituency (NCUC) and the At-Large Advisory Committee (ALAC) that there is not enough public disclosure and that too many discussions and decisions take place out of sight of the public.

During the early 2000s, there had been speculation that the United Nations might assume control of ICANN, followed by a negative reaction from the U.S. government and worries about a division of the Internet. The World Summit on the Information Society in Tunisia during November 2005 agreed not to get involved in the day-to-day and technical operations of ICANN. However it also agreed to establish an international Internet Governance Forum, with a consultative role on the future governance of the Internet. ICANN's Government Advisory Committee is currently established to provide advice to ICANN regarding public policy issues and has participation by many of the world's governments.

Some have attempted to argue that ICANN was never given the authority to decide policy, e.g., choose new TLDs or exclude other interested parties who refuse to pay ICANN's US$185,000 fee, but was to be a technical caretaker. Critics suggest that ICANN should not be allowed to impose business rules on market participants, and that all TLDs should be added on a first-come, first-served basis and the market should be the arbiter of who succeeds and who does not.

One task that ICANN was asked to do was to address the issue of domain name ownership resolution for generic top-level domains (gTLDs). ICANN's attempt at such a policy was drafted in close cooperation with the World Intellectual Property Organization (WIPO), and the result has now become known as the Uniform Dispute Resolution Policy (UDRP). This policy essentially attempts to provide a mechanism for rapid, cheap and reasonable resolution of domain name conflicts, avoiding the traditional court system for disputes by allowing cases to be brought to one of a set of bodies that arbitrate domain name disputes. According to ICANN policy, a domain registrant must agree to be bound by the UDRP—they cannot get a domain name without agreeing to this.

Examination of the UDRP decision patterns has caused some to conclude that compulsory domain name arbitration is less likely to give a fair hearing to domain name owners asserting defenses under the First Amendment and other laws, compared to the federal courts of appeal in particular.

In 2013, the initial report of ICANN's Expert Working Group has recommended that the present form of Whois, a utility that allows anyone to know who has registered a domain name on the Internet, should be "abandoned". It recommends it be replaced with a system that keeps most registration information secret (or "gated") from most Internet users, and only discloses information for "permissible purposes". ICANN's list of permissible purposes includes domain name research, domain name sale and purchase, regulatory enforcement, personal data protection, legal actions, and abuse mitigation. Whois has been a key tool of investigative journalists interested in determining who was disseminating information on the Internet. The use of whois by the free press is not included in the list of permissible purposes in the initial report.

Since its creation, ICANN has been the subject of criticism and controversy. In 2000, professor Michael Froomkin of the University of Miami School of Law argued that ICANN's relationship with the U.S. Department of Commerce is illegal, in violation of either the Constitution or federal statutes. In 2009, the new "Affirmation of Commitments" agreement between ICANN and the U.S. Department of Commerce, that aimed to create international oversight, ran into criticism.

During December 2011, the Federal Trade Commission stated ICANN had long failed to provide safeguards that protect consumers from online swindlers.

Also during 2011, seventy-nine companies, including The Coca-Cola Company, Hewlett-Packard, Samsung and others, signed a petition against ICANN's new TLD program (sometimes referred to as a "commercial landgrab"), in a group organized by the Association of National Advertisers. As of September 2014, this group, the Coalition for Responsible Internet Domain Oversight, that opposes the rollout of ICANN's TLD expansion program, has been joined by 102 associations and 79 major companies. Partly as a response to this criticism, ICANN initiated an effort to protect trademarks in domain name registrations, which eventually culminated in the establishment of the Trademark Clearinghouse.

One controversial proposal, resulting from a September 2011 summit between India, Brazil, and South Africa (IBSA), would seek to move Internet governance into a "UN Committee on Internet-Related Policy" (UN-CIRP). The action was a reaction to a perception that the principles of the 2005 Tunis Agenda for the Information Society have not been met. The statement proposed the creation of a new political organization operating as a component of the United Nations to provide policy recommendations for the consideration of technical organizations such as ICANN and international bodies such as the ITU. Subsequent to public criticisms, the Indian government backed away from the proposal.

On October 7, 2013 the Montevideo Statement on the Future of Internet Cooperation was released by the managers of a number of organizations involved in coordinating the Internet's global technical infrastructure, loosely known as the "I*" (or "I-star") group. Among other things, the statement "expressed strong concern over the undermining of the trust and confidence of Internet users globally due to recent revelations of pervasive monitoring and surveillance" and "called for accelerating the globalization of ICANN and IANA functions, towards an environment in which all stakeholders, including all governments, participate on an equal footing". This desire to reduce United States association with the internet is considered a reaction to the ongoing NSA surveillance scandal. The statement was signed by the managers of the Internet Corporation for Assigned Names and Numbers (ICANN), the Internet Engineering Task Force, the Internet Architecture Board, the World Wide Web Consortium, the Internet Society, and the five regional Internet address registries (African Network Information Center, American Registry for Internet Numbers, Asia-Pacific Network Information Centre, Latin America and Caribbean Internet Addresses Registry, and Réseaux IP Européens Network Coordination Centre).

During October 2013, Fadi Chehadé, current President and CEO of ICANN, met with Brazilian President Dilma Rousseff in Brasilia. Upon Chehadé's invitation, the two announced that Brazil would host an international summit on Internet governance during April 2014. The announcement came after the 2013 disclosures of mass surveillance by the U.S. government, and President Rousseff's speech at the opening session of the 2013 United Nations General Assembly, where she strongly criticized the American surveillance program as a "breach of international law". The "Global Multistakeholder Meeting on the Future of Internet Governance (NET mundial)" will include representatives of government, industry, civil society, and academia. At the IGF VIII meeting in Bali in October 2013 a commenter noted that Brazil intends the meeting to be a "summit" in the sense that it will be high level with decision-making authority. The organizers of the "NET mundial" meeting have decided that an online forum called "/1net", set up by the I* group, will be a major conduit of non-governmental input into the three committees preparing for the meeting in April.

The Obama administration that had joined critics of ICANN during 2011 announced in March 2014 that they intended to transition away from oversight of the IANA functions contract. The current contract that the United States Department of Commerce has with ICANN expired in 2015, in its place the NTIA will transition oversight of the IANA functions to the 'global multistakeholder community'.

The NetMundial Initiative is a plan for international governance of the Internet that was first proposed at the Global Multistakeholder Meeting on the Future of Internet Governance (GMMFIG) conference (April 23–24, 2014)
and later developed into the NetMundial Initiative by ICANN CEO Fadi Chehadé along with representatives of the World Economic Forum (WEF)
and the Brazilian Internet Steering Committee (Comitê Gestor da Internet no Brasil), commonly referred to as "CGI.br".

The meeting produced a nonbinding statement in favor of consensus-based decision-making. It represented a compromise and did not harshly condemn mass surveillance or include the words "net neutrality", despite initial endorsement for that from Brazil. The final resolution says ICANN should be controlled internationally by September 2015.
A minority of governments, including Russia, China, Iran and India, were unhappy with the final resolution and wanted multilateral management for the Internet, rather than broader multistakeholder management.

A month later, the Panel on Global Internet Cooperation and Governance Mechanisms (convened by the Internet Corporation for Assigned Names and Numbers (ICANN) and the World Economic Forum (WEF) with assistance from The Annenberg Foundation), endorsed and included the NetMundial statement in its own report.

During June 2014, France strongly attacked ICANN, saying ICANN is not a fit venue for Internet governance and that alternatives should be sought.

ICANN has received more than $60 million from gTLD auctions, and has accepted the controversial domain name ".sucks" (referring to the primarily US slang for being inferior or objectionable). When the .sucks registry announced their pricing model, "most brand owners were upset and felt like they were being penalized by having to pay more to protect their brands." The .sucks domain registrar has been described as "predatory, exploitive and coercive" by the Intellectual Property Constituency that advises the ICANN board.

Because of the low utility of the ".sucks" domain, it is expected that most of the fees will come from "Brand Protection" customers registering their trademarks to prevent domains being registered. U.S. Representative Bob Goodlatte says that trademark holders are "being shaken down" by the registry's fees. Jay Rockefeller says that .sucks is a "a predatory shakedown scheme" and "Approving '.sucks', a gTLD with little or no public interest value, will have the effect of undermining the credibility ICANN has slowly been building with skeptical stakeholders."

Canadian brands had complained that they were being charged "exorbitant" prices to register their trademarks as premium names. FTC chair Edith Ramirez has written to ICANN to say the agency will take action against the .sucks owner if "we have reason to believe an entity has engaged in deceptive or unfair practices in violation of Section 5 of the FTC Act". The Register reported that intellectual property lawyers are infuriated that "the dot-sucks registry was charging trademark holders $2,500 for .sucks domains and everyone else $10."

On July 30, 2018, whistleblower and writer for the Register Kieran McCarthy exposed the systematic refusal of ICANN to create the .islam and .halal gTLDs through the willful breaking of the organization's own bylaws. McCarthy coined the 6-year refusal as "the internet's very own Muslim ban" after ICANN kowtowed to Middle Eastern governments by not approving the domain name additions.





</doc>
<doc id="15237" url="https://en.wikipedia.org/wiki?curid=15237" title="Iterative method">
Iterative method

In computational mathematics, an iterative method is a mathematical procedure that uses an initial guess to generate a sequence of improving approximate solutions for a class of problems, in which the "n"-th approximation is derived from the previous ones. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic-based iterative methods are also common. 

In contrast, direct methods attempt to solve the problem by a finite sequence of operations. In the absence of rounding errors, direct methods would deliver an exact solution (like solving a linear system of equations formula_1 by Gaussian elimination). Iterative methods are often the only choice for nonlinear equations. However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.

If an equation can be put into the form "f"("x") = "x", and a solution x is an attractive fixed point of the function "f", then one may begin with a point "x" in the basin of attraction of x, and let "x" = "f"("x") for "n" ≥ 1, and the sequence {"x"} will converge to the solution x. Here "x" is the "n"th approximation or iteration of "x" and "x" is the next or "n" + 1 iteration of "x". Alternately, superscripts in parentheses are often used in numerical methods, so as not to interfere with subscripts with other meanings. (For example, "x" = "f"("x").) If the function "f" is continuously differentiable, a sufficient condition for convergence is that the spectral radius of the derivative is strictly bounded by one in a neighborhood of the fixed point. If this condition holds at the fixed point, then a sufficiently small neighborhood (basin of attraction) must exist.

In the case of a system of linear equations, the two main classes of iterative methods are the stationary iterative methods, and the more general Krylov subspace methods.

Stationary iterative methods solve a linear system with an operator approximating the original one; and based on a measurement of the error in the result (the residual), form a "correction equation" for which this process is repeated. While these methods are simple to derive, implement, and analyze, convergence is only guaranteed for a limited class of matrices. 

An "iterative method" is defined by
and for a given linear system formula_3 with exact solution formula_4 the "error" by
An iterative method is called "linear" if there exists a matrix formula_6 s.t.
and this matrix is called "iteration matrix".
An iterative method with a given iteration matrix formula_8 is called "convergent" if the following holds

An important theorem states that for a given iterative method and its iteration matrix formula_8 it is convergent if and only if its spectral radius formula_11 is smaller than unity, i.e.

The basic iterative methods work by a splitting up the matrix formula_13 into
and here the matrix formula_15 should be easily invertible.
The iterative methods are now defined as
From this follows that the iteration matrix is given by

Basic examples of stationary iterative methods use a splitting of the matrix formula_13 such as
where formula_20 is only the diagonal part of formula_13, and formula_22 is the strict lower triangular part of formula_13.
Respectively, formula_24 is the upper triangular part of formula_13.
Linear stationary iterative methods are also called relaxation methods.

Krylov subspace methods work by forming a basis of the sequence of successive matrix powers times the initial residual (the Krylov sequence). 
The approximations to the solution are then formed by minimizing the residual over the subspace formed. 
The prototypical method in this class is the conjugate gradient method (CG) which assumes that the system matrix formula_13 is symmetric positive-definite.
For symmetric (and possibly indefinite) formula_13 one works with the minimal residual method (MINRES).
In the case of not even symmetric matrices methods, such as the generalized minimal residual method (GMRES) and the biconjugate gradient method (BiCG), have been derived.

Since these methods form a basis, it is evident that the method converges in "N" iterations, where "N" is the system size. However, in the presence of rounding errors this statement does not hold; moreover, in practice "N" can be very large, and the iterative process reaches sufficient accuracy already far earlier. The analysis of these methods is hard, depending on a complicated function of the spectrum of the operator.

The approximating operator that appears in stationary iterative methods can also be incorporated in Krylov subspace methods such as GMRES (alternatively, preconditioned Krylov methods can be considered as accelerations of stationary iterative methods), where they become transformations of the original operator to a presumably better conditioned one. The construction of preconditioners is a large research area.

Probably the first iterative method for solving a linear system appeared in a letter of Gauss to a student of his. He proposed solving a 4-by-4 system of equations by repeatedly solving the component in which the residual was the largest. 

The theory of stationary iterative methods was solidly established with the work of D.M. Young starting in the 1950s. The Conjugate Gradient method was also invented in the 1950s, with independent developments by Cornelius Lanczos, Magnus Hestenes and Eduard Stiefel, but its nature and applicability were misunderstood at the time. Only in the 1970s was it realized that conjugacy based methods work very well for partial differential equations, especially the elliptic type.




</doc>
<doc id="15238" url="https://en.wikipedia.org/wiki?curid=15238" title="International judicial institution">
International judicial institution

International judicial institutions can be divided into courts, arbitral tribunals and quasi-judicial institutions. Courts are permanent bodies, with near the same composition for each case. Arbitral tribunals, by contrast, are constituted anew for each case. Both courts and arbitral tribunals can make binding decisions. Quasi-judicial institutions, by contrast, make rulings on cases, but these rulings are not in themselves legally binding; the main example is the individual complaints mechanisms available under the various UN human rights treaties.

Institutions can also be divided into global and regional institutions.

The listing below incorporates both currently existing institutions, defunct institutions that no longer exist, institutions which never came into existence due to non-ratification of their constitutive instruments, and institutions which do not yet exist, but for which constitutive instruments have been signed. It does not include mere proposed institutions for which no instrument was ever signed.








</doc>
<doc id="15239" url="https://en.wikipedia.org/wiki?curid=15239" title="International Prize Court">
International Prize Court

The International Prize Court was an international court proposed at the beginning of the 20th century, to hear prize cases. An international agreement to create it, the "Convention Relative to the Creation of an International Prize Court", was made at the Second Hague Conference in 1907 but never came into force.

The capturing of prizes (enemy equipment, vehicles, and especially ships) during wartime is a tradition that goes back as far as organized warfare itself. The International Prize Court was to hear appeals from national courts concerning prize cases. Even as a draft, the convention was innovative for the time, in being both the first ever treaty for a truly international court (as opposed to a mere arbitral tribunal), and in providing individuals with access to the court, going against the prevailing doctrines of international law at the time, according to which only states had rights and duties under international law. The Convention was opposed, particularly by elements within the United States and the United Kingdom, as a violation of national sovereignty.

The 1907 convention was modified by the "Additional Protocol to the Convention Relative to the Creation of an International Prize Court", done at the Hague on October 18, 1910. The protocol was an attempt to resolve some concerns expressed by the United States at the court, which felt it to be in violation of its constitutional provision that provides for the U.S. Supreme Court being the final judicial authority. However, neither the convention nor the subsequent protocol ever entered into force, since only Nicaragua ratified the agreements. As a result, the court never came into existence.

A number of ideas from the International Prize Court proposal can be seen in present day international courts, such as its provision for judges "ad hoc", later adopted in the Permanent Court of International Justice and the subsequent International Court of Justice.




</doc>
<doc id="15240" url="https://en.wikipedia.org/wiki?curid=15240" title="Imam">
Imam

Imam (; '; plural: ') is an Islamic leadership position. 

It is most commonly used as the title of a worship leader of a mosque and Muslim community among Sunni Muslims. In this context, imams may lead Islamic worship services, serve as community leaders, and provide religious guidance. 

For Shi'a Muslims, the imam has a more central meaning and role in Islam through the concept of imamah; the term is only applicable to those members of "Ahl al-Bayt", the house of the Islamic prophet Muhammad, designated as infallibles.

The Sunni branch of Islam does not have imams in the same sense as the Shi'a, an important distinction often overlooked by those outside of the Islamic faith. In everyday terms, the imam for Sunni Muslims is the one who leads Islamic formal (Fard) prayers, even in locations besides the mosque, whenever prayers are done in a group of two or more with one person leading (imam) and the others following by copying his ritual actions of worship. Friday sermon is most often given by an appointed imam. All mosques have an imam to lead the (congregational) prayers, even though it may sometimes just be a member from the gathered congregation rather than an officially appointed salaried person. The position of women as imams is controversial. The person that should be chosen, according to Hadith, is one who has most knowledge of the Quran and Sunnah (prophetic tradition) and is of good character; the age being post-puberty.

The term is also used for a recognized religious scholar or authority in Islam, often for the founding scholars of the four Sunni madhhabs, or schools of jurisprudence ("fiqh"). It may also refer to the Muslim scholars who created the analytical sciences related to Hadith or it may refer to the heads of Muhammad's family in their generational times.

The following table shows the considered imams in the context of scholarly authority by Sunni Muslims:

The Position of Imams In Turkey

Imams are appointed by the state to work at mosques and they are required to be graduates of an İmam Hatip high school or have a university degree in Theology. This is an official position regulated by the Presidency of Religious Affairs in Turkey and only males are appointed to this position while female officials under the same state organisation work as preachers and Qur'an course tutors, religious services experts. These officials are supposedly belong to the Hanafi school of the Sunni sect.

Central figure in an Islamic movement are also called as Imam like the Imam Nabahwi in Syria and Ahmad Raza Khan in India called as the Imam of Sunni Muslims.

In the Shi'a context, an imam is not only presented as the man of God "par excellence", but as participating fully in the names, attributes, and acts that theology usually reserves for God alone. Imams have a meaning more central to belief, referring to leaders of the community. Twelver and Ismaili Shi'a believe that these imams are chosen by God to be perfect examples for the faithful and to lead all humanity in all aspects of life. They also believe that all the imams chosen are free from committing any sin, impeccability which is called "ismah". These leaders must be followed since they are appointed by God.

Here follows a list of the Twelvers imams:

Fatimah, also Fatimah al-Zahraa, daughter of Muhammed (615–632), is also considered infallible but not an Imam. The Shi'a believe that the last Imam, the 12th Imam Mahdi will one day emerge on Qiyamah.

See Imamah (Ismaili doctrine) and List of Ismaili imams for Ismaili imams.

See details under Zaidiyyah, Islamic history of Yemen and Imams of Yemen.

At times, imams have held both secular and religious authority. This was the case in Oman among the Kharijite or Ibadi sects. At times, the imams were elected. At other times the position was inherited, as with the Yaruba dynasty from 1624 and 1742. The Imamate of Futa Jallon (1727-1896) was a Fulani state in West Africa where secular power alternated between two lines of hereditary Imams, or "almami".
In the Zaidi Shiite sect, imams were secular as well as spiritual leaders who held power in Yemen for more than a thousand years. In 897, a Zaidi ruler, al-Hadi ila'l-Haqq Yahya, founded a line of such imams, a theocratic form of government which survived until the second half of the 20th century. (See details under Zaidiyyah, History of Yemen, Imams of Yemen.)

Ruhollah Khomeini is officially referred to as Imam in Iran. Several Iranian places and institutions are named "Imam Khomeini", including a city, an international airport, a hospital, and a university.





</doc>
<doc id="15242" url="https://en.wikipedia.org/wiki?curid=15242" title="Instrument flight rules">
Instrument flight rules

Instrument flight rules (IFR) is one of two sets of regulations governing all aspects of civil aviation aircraft operations; the other is visual flight rules (VFR).

The U.S. Federal Aviation Administration's (FAA) "Instrument Flying Handbook" defines IFR as: "Rules and regulations established by the FAA to govern flight under conditions in which flight by outside visual reference is not safe. IFR flight depends upon flying by reference to instruments in the flight deck, and navigation is accomplished by reference to electronic signals." It is also a term used by pilots and controllers to indicate the type of flight plan an aircraft is flying, such as an IFR or VFR flight plan.

To put instrument flight rules into context, a brief overview of visual flight rules (VFR) is necessary. It is possible and fairly straightforward, in relatively clear weather conditions, to fly a plane solely by reference to outside visual cues, such as the horizon to maintain orientation, nearby buildings and terrain features for navigation, and other aircraft to maintain separation. This is known as operating the aircraft under VFR, and is the most common mode of operation for small aircraft. However, it is safe to fly VFR only when these outside references can be clearly seen from a sufficient distance; when flying through or above clouds, or in fog, rain, dust or similar low-level weather conditions, these references can be obscured. Thus, cloud ceiling and flight visibility are the most important variables for safe operations during all phases of flight.<ref name='NASA/CR-2000-210288'> 
</ref> The minimum weather conditions for ceiling and visibility for VFR flights are defined in FAR Part 91.155, and vary depending on the type of airspace in which the aircraft is operating, and on whether the flight is conducted during daytime or nighttime. However, typical daytime VFR minimums for most airspace is 3 statute miles of flight visibility and a distance from clouds of 500' below, 1,000' above, and 2,000' feet horizontally. Flight conditions reported as equal to or greater than these VFR minimums are referred to as visual meteorological conditions (VMC).

Any aircraft operating under VFR must have the required equipment on board, as described in FAR Part 91.205 (which includes some instruments necessary for IFR flight). VFR pilots "may" use cockpit instruments as secondary aids to navigation and orientation, but are not required to; the view outside of the aircraft is the primary source for keeping the aircraft straight and level (orientation), flying to the intended destination (navigation), and not hitting anything (separation).

Visual flight rules are generally simpler than instrument flight rules, and require significantly less training and practice. VFR provides a great degree of freedom, allowing pilots to go where they want, when they want, and allows them a much wider latitude in determining how they get there.

When operation of an aircraft under VFR is not safe, because the visual cues outside the aircraft are obscured by weather or darkness, instrument flight rules must be used instead. IFR permits an aircraft to operate in instrument meteorological conditions (IMC), which is essentially any weather condition less than VMC but in which aircraft can still operate safely. Use of instrument flight rules is also required when flying in "Class A" airspace regardless of weather conditions. Class A airspace extends from 18,000 feet above mean sea level to flight level 600 (60,000 feet pressure altitude) above the contiguous 48 United States and overlying the waters within 12 miles thereof. Flight in Class A airspace requires pilots and aircraft to be instrument equipped and rated and to be operating under Instrument Flight Rules (IFR). In many countries commercial airliners and their pilots must operate under IFR as the majority of flights enter Class A airspace; however, aircraft operating as commercial airliners must operate under IFR even if the flight plan does not take the craft into Class A airspace, such as with smaller regional flights. Procedures and training are significantly more complex compared to VFR instruction, as a pilot must demonstrate competency in conducting an entire cross-country flight solely by reference to instruments.

Instrument pilots must meticulously evaluate weather, create a very detailed flight plan based around specific instrument departure, en route, and arrival procedures, and dispatch the flight.

The distance by which an aircraft avoids obstacles or other aircraft is termed "separation". The most important concept of IFR flying is that separation is maintained regardless of weather conditions. In controlled airspace, air traffic control (ATC) separates IFR aircraft from obstacles and other aircraft using a flight "clearance" based on route, time, distance, speed, and altitude. ATC monitors IFR flights on radar, or through aircraft position reports in areas where radar coverage is not available. Aircraft position reports are sent as voice radio transmissions. In the United States, a flight operating under IFR is required to provide position reports unless ATC advises a pilot that the plane is in radar contact. The pilot must resume position reports after ATC advises that radar contact has been lost, or that radar services are terminated.

IFR flights in controlled airspace require an ATC "clearance" for each part of the flight. A clearance always specifies a "clearance limit", which is the farthest the aircraft can fly without a new clearance. In addition, a clearance typically provides a heading or route to follow, altitude, and communication parameters, such as frequencies and transponder codes.

In uncontrolled airspace, ATC clearances are unavailable. In some states a form of separation is provided to certain aircraft in uncontrolled airspace as far as is practical (often known under ICAO as an advisory service in class G airspace), but separation is not mandated nor widely provided.

Despite the protection offered by flight in controlled airspace under IFR, the ultimate responsibility for the safety of the aircraft rests with the pilot in command, who can refuse clearances.

It is essential to differentiate between flight plan type (VFR or IFR) and weather conditions (VMC or IMC). While current and forecast weather may be a factor in deciding which type of flight plan to file, weather conditions themselves do not affect one's filed flight plan. For example, an IFR flight that encounters visual meteorological conditions (VMC) en route does not automatically change to a VFR flight, and the flight must still follow all IFR procedures regardless of weather conditions. In the US, weather conditions are forecast broadly as VFR, MVFR (Marginal Visual Flight Rules), IFR, or LIFR (Low Instrument Flight Rules).

The main purpose of IFR is the safe operation of aircraft in instrument meteorological conditions (IMC). The weather is considered to be MVFR or IMC when it does not meet the minimum requirements for visual meteorological conditions (VMC). To operate safely in IMC ("actual instrument conditions"), a pilot controls the aircraft relying on flight instruments and ATC provides separation.

It is important not to confuse IFR with IMC. A significant amount of IFR flying is conducted in Visual Meteorological Conditions (VMC). Anytime a flight is operating in VMC and in a volume of airspace in which VFR traffic can operate, the crew is responsible for seeing and avoiding VFR traffic; however, because the flight is conducted under Instrument Flight Rules, ATC still provides separation services from other IFR traffic, and can in many cases also advise the crew of the location of VFR traffic near the flight path.

Although dangerous and illegal, a certain amount of VFR flying is conducted in IMC. A scenario is a VFR pilot taking off in VMC conditions, but encountering deteriorating visibility while en route. Continued VFR flight into IMC can lead to spatial disorientation of the pilot which is the cause of a significant number of general aviation crashes. VFR flight into IMC is distinct from "VFR-on-top", an IFR procedure in which the aircraft operates in VMC using a hybrid of VFR and IFR rules, and "VFR over the top", a VFR procedure in which the aircraft takes off and lands in VMC but flies above an intervening area of IMC. Also possible in many countries is "Special VFR" flight, where an aircraft is explicitly granted permission to operate VFR within the controlled airspace of an airport in conditions technically less than VMC; the pilot asserts they have the necessary visibility to fly despite the weather, must stay in contact with ATC, and cannot leave controlled airspace while still below VMC minimums.

During flight under IFR, there are no visibility requirements, so flying through clouds (or other conditions where there is zero visibility outside the aircraft) is legal and safe. However, there are still minimum weather conditions that must be present in order for the aircraft to take off or to land; these vary according to the kind of operation, the type of navigation aids available, the location and height of terrain and obstructions in the vicinity of the airport, equipment on the aircraft, and the qualifications of the crew. For example, Reno-Tahoe International Airport (KRNO) in a mountainous region has significantly different instrument approaches for aircraft landing on the same runway surface, but from opposite directions. Aircraft approaching from the north must make visual contact with the airport at a higher altitude than when approaching from the south because of rapidly rising terrain south of the airport. This higher altitude allows a flight crew to clear the obstacle if a landing is aborted. In general, each specific instrument approach specifies the minimum weather conditions to permit landing.

Although large airliners, and increasingly, smaller aircraft, carry their own terrain awareness and warning system (TAWS), these are primarily backup systems providing a last layer of defense if a sequence of errors or omissions causes a dangerous situation.

Because IFR flights often take place without visual reference to the ground, a means of navigation other than looking outside the window is required. A number of navigational aids are available to pilots, including ground-based systems such as DME/VORs and NDBs as well as the satellite-based GPS/GNSS system. Air traffic control may assist in navigation by assigning pilots specific headings ("radar vectors"). The majority of IFR navigation is given by ground- and satellite-based systems, while radar vectors are usually reserved by ATC for sequencing aircraft for a busy approach or transitioning aircraft from takeoff to cruise, among other things.

Autopilot allows automatic piloting.

Modern flight management systems have evolved to allow a crew to plan a flight as to route and altitude and to specific time of arrival at specific locations. This capability is used in several trial projects experimenting with "four-dimensional" approach clearances for commercial aircraft, with time as the fourth dimension. These clearances allow ATC to optimize the arrival of aircraft at major airports, which increases airport capacity and uses less fuel providing monetary and environmental benefits to airlines and the public.


Specific procedures allow IFR aircraft to transition safely through every stage of flight. These procedures specify how an IFR pilot should respond, even in the event of a complete radio failure, and loss of communications with ATC, including the expected aircraft course and altitude.

Departures are described in an IFR clearance issued by ATC prior to takeoff. The departure clearance may contain an assigned heading, one or more waypoints, and an initial altitude to fly. The clearance can also specify a departure procedure (DP) or standard instrument departure (SID) that should be followed unless "NO DP" is specified in the notes section of the filed flight plan.

Here is an example of an IFR clearance for a Cessna aircraft traveling from Palo Alto airport (KPAO) to Stockton airport (KSCK).

Detailed explanation:










The clearance scheme, used by ATC, can be easily remembered using the acronym

En route flight is described by IFR charts showing navigation aids, fixes, and standard routes called "airways". Aircraft with appropriate navigational equipment such as GPS, are also often cleared for a "direct-to" routing, where only the destination, or a few navigational waypoints are used to describe the route that the flight will follow. ATC will assign altitudes in its initial clearance or amendments thereto, and navigational charts indicate minimum safe altitudes for airways.

The approach portion of an IFR flight may begin with a standard terminal arrival route (STAR), describing common routes to fly to arrive at an initial approach fix (IAF) from which an instrument approach commences. 
An instrument approach terminates either by the pilot acquiring sufficient visual reference to proceed to the runway, or with a missed approach because the required visual reference is not seen in time.

To fly under IFR, a pilot must have an instrument rating and must be "current" (meet recency of experience requirements).
In the United States, to file and fly under IFR, a pilot must be instrument-rated and, within the preceding six months, have flown six instrument approaches, as well as holding procedures and course interception and tracking with navaids. Flight under IFR beyond six months after meeting these requirements is not permitted; however, currency may be reestablished within the next six months by completing the requirements above. Beyond the twelfth month, examination ("instrument proficiency check") by an instructor is required.

Practicing instrument approaches can be done either in the instrument meteorological conditions or in visual meteorological conditions – in the latter case, a safety pilot is required so that the pilot practicing instrument approaches can wear a view-limiting device which restricts his field of view to the instrument panel. A safety pilot's primary duty is to observe and avoid other traffic.

For all ILS Cat II or Cat III approaches, additional crew training is required and a certain number of low visibility approaches must either be performed or simulated within a fixed time for pilots to be 'current' in performing them.

In the UK, an IR (UK restricted) - formerly the "IMC rating" - which permits flight under IFR in airspace classes B to G in instrument meteorological conditions, a non-instrument-rated pilot can also elect to fly under IFR in visual meteorological conditions outside controlled airspace. Compared to the rest of the world, the UK's flight crew licensing regime is somewhat unusual in its licensing for meteorological conditions and airspace, rather than flight rules.

The aircraft must be equipped and type-certified for instrument flight, and the related navigational equipment must have been inspected or tested within a specific period of time prior to the instrument flight.

In the United States, instruments required for IFR flight in addition to those that are required for VFR flight are: heading indicator, sensitive altimeter adjustable for barometric pressure, clock with a sweep-second pointer or digital equivalent, attitude indicator, radios and suitable avionics for the route to be flown, alternator or generator, gyroscopic rate-of-turn indicator that is either a turn coordinator or the turn and bank indicator. From 1999 single-engine helicopters could not be FAA-certified for IFR, and Helicopter Association International estimates that 326 lives were lost in 133 accidents that would likely not have happened if those helicopters had been flying under IFR.




</doc>
<doc id="15245" url="https://en.wikipedia.org/wiki?curid=15245" title="Ismail Khan">
Ismail Khan

Mohammad Ismail Khan (Persian: محمد اسماعیل خان) (born 1946) is a politician in Afghanistan, who served as Minister of Water and Energy from 2005 to 2013. He was previously the Governor of Herat Province. He is widely known as a warlord because of his rise to power during the Soviet–Afghan War when he controlled a large sized mujahideen force, mainly his fellow Tajiks from western Afghanistan. He is a key member of the political party Jamiat-e Islami and was a member of the now defunct United National Front party.

Khan was born in or about 1946 in the Shindand District of Herat Province in Afghanistan. His family are Tajiks from the Chahar-Mahal neighbourhood of Shindand.

In early 1979 Ismail Khan was a Captain in the Afghan National Army based in the western city of Herat. In early March of that year, there was a protest in front of the Communist governor's palace against the arrests and assassinations being carried out in the countryside. The governor's troops opened fire on the demonstrators, who proceeded to storm the palace and hunt down Soviet advisers. The Herat garrison mutinied and joined the revolt, with Ismail Khan and other officers distributing all available weapons to the insurgents. Hundreds of civil workers and people not dressed in traditional Muslim clothes were murdered. A garrison of Soviet advisors was overtaken and all of its inhabitants: Soviet advisors along with their wives and children were massacred. The mob put severed heads of the victims on sticks and paraded them through the city of Herat. The government led by Nur Mohammed Taraki responded, pulverizing the city using Soviet supplied bombers and killing an estimated 24,000 citizens in less than a week. This event marked the opening salvo of the rebellion which led to the Soviet military intervention in Afghanistan in December 1979. Ismail Khan escaped to the countryside where he began to assemble a local rebel force.

During the ensuing war, he became the leader of the western command of Burhanuddin Rabbani's Jamiat-e-Islami, political party associated with neighboring Pakistan's Jamaat-e-Islami. With Ahmad Shah Massoud, he was one of the most respected mujahideen leaders. In 1992, two years after the Soviet withdrawal from Afghanistan, the mujahideen captured Herat and Ismail Khan became Governor.

In 1995, he successfully defended his province against the Taliban, in cooperation with defense minister Ahmad Shah Massoud. Khan even tried to attack the Taliban stronghold of Kandahar, but was repulsed. Later in September, an ally of the Jamiat, Uzbek General Abdul Rashid Dostum changed sides, and attacked Herat. Ismail Khan was forced to flee to neighboring Iran with 8,000 men and the Taliban took over Herat Province.

Two years later, while organizing opposition to the Taliban in Faryab area, he was betrayed and captured by Abdul Majid Rouzi who had defected to the Taliban along with Abdul Malik Pahlawan, then one of Dostum's deputies. Then in March 1999 he escaped from Kandahar prison. During the U.S. intervention in Afghanistan, he fought against the Taliban within the United Islamic Front for the Salvation of Afghanistan (Northern Alliance) and thus regained his position as Governor of Herat after they were victorious in December 2001.

After returning to Herat, Ismail Khan quickly consolidated his control over the region. He took over control of the city from the local ulema and quickly established control over the trade route between Herat and Iran, a large source of revenue. As Emir of Herat, Ismail Khan exercised great autonomy, providing social welfare for Heratis, expanding his power into neighbouring provinces, and maintaining direct international contacts. Although hated by the educated in Herat and often accused of human rights abuses, Ismail Khan's regime provided security, paid government employees, and made investments in public services. However, during his tenure as governor, Ismail Khan was accused of ruling his province like a private fiefdom, leading to increasing tensions with the Afghan Transitional Administration. In particular, he refused to pass on to the government the revenues gained from custom taxes on goods from Iran and Turkmenistan.

On 13 August 2003, President Karzai removed Governor Ismail Khan from his command of the 4th Corps. This was announced as part of a programme removing the ability of officials to hold both civilian and military posts.

Ismail Khan was ultimately removed from power in March 2004 due to pressure by neighbouring warlords and the central Afghan government. Various sources have presented different versions of the story, and the exact dynamics cannot be known with certainty. What is known is that Ismail Khan found himself at odds with a few regional commanders who, although theoretically his subordinates, attempted to remove him from power. Ismail Khan claims that these efforts began with a botched assassination attempt. Afterwards, these commanders moved their forces near Herat. Ismail Khan, unpopular with the Herati military class, was slow to mobilise his forces, perhaps waiting for the threat to Herat to become existential as a means to motivate his forces. However, the conflict was stopped with the intervention of International Security Assistance Force forces and soldiers of the Afghan National Army, freezing the conflict in its tracks. Ismail Khan's forces even fought skirmishes with the Afghan National Army, in which his son, Mirwais Sadiq was killed. Because Ismail Khan was contained by the Afghan National Army, the warlords who opposed him were quickly able to occupy strategic locations unopposed. Ismail Khan was forced to give up his governorship and to go to Kabul, where he served in Hamid Karzai's cabinet as the Minister of Energy.

In 2005 Ismail Khan became the Minister of Water and Energy.

In late 2012, the Government of Afghanistan accused Ismail Khan of illegally distributing weapons to his supporters. About 40 members of the country's Parliament requested Ismail Khan to answer their queries. The government believes that Khan is attempting to create some kind of disruption in the country.

On September 27, 2009, Ismail Khan survived a suicide blast that killed 4 of his bodyguards in Herat, in western Afghanistan. He was driving to Herat Airport when a powerful explosion occurred on the way there. Taliban spokesman, Zabiullah Mujahid, claimed responsibility and said the target was Khan.

Guantanamo captive Abdul Razzaq Hekmati requested Ismail Khan's testimony, when he was called before a Combatant Status Review Tribunal. 
Ismail Khan, like Afghan Minister of Defense Rahim Wardak, was one of the high-profile Afghans that those conducting the Tribunals ruled were "not reasonably available" to give a statement on a captive's behalf because they could not be located.

Hekmati had played a key role in helping Ismail Khan escape from the Taliban in 1999.
Hekmati stood accused of helping Taliban leaders escape from the custody of Hamid Karzai's government.

Carlotta Gall and Andy Worthington interviewed Ismail Khan for a new "The New York Times" article after Hekmati died of cancer in Guantanamo. 
According to the "New York Times"
Ismail Khan said he personally buttonholed the American ambassador to tell him that Hekmati was innocent, and should be released. In contrast, Hekmati was told that the State Department had been unable to locate Khan.

Ismail Khan is a controversial figure. Reporters Without Borders has charged him with muzzling the press and ordering attacks on journalists. Also Human Rights Watch has accused him of human rights abuses.

Nevertheless, he remains a popular figure for some in Afghanistan. Unlike other mujahideen commanders, Khan has not been linked to large-scale massacres and atrocities such as those committed after the capture of Kabul in 1992. Following news of his dismissal, rioting broke out in the streets of Herat, and President Karzai had to ask him to make a personal appeal for calm.



</doc>
<doc id="15250" url="https://en.wikipedia.org/wiki?curid=15250" title="Indigo">
Indigo

Indigo is a deep and rich color close to the color wheel blue (a primary color in the RGB color space), as well as to some variants of ultramarine. It is traditionally regarded as a color in the visible spectrum, as well as one of the seven colors of the rainbow: the color between violet and blue; however, sources differ as to its actual position in the electromagnetic spectrum.

The color indigo is named after the indigo dye derived from the plant "Indigofera tinctoria" and related species.

The first known recorded use of indigo as a color name in English was in 1289.

Species of "Indigofera" were cultivated in East Asia, Egypt, India, and Peru in antiquity. The earliest direct evidence for the use of indigo dates to around 4000 BC and comes from Huaca Prieta, in contemporary Peru. Pliny the Elder mentions the Indus Valley Civilization as the source of the dye after which it was named. It was imported from there in small quantities via the Silk Road.

The Ancient Greek term for the dye was ("Sindhi dye"), which, adopted to Latin as "indicum" and via Portuguese gave rise to the modern word "indigo".

Spanish explorers discovered an American species of indigo and began to cultivate the product in Guatemala. The English and French subsequently began to encourage indigo cultivation in their colonies in the West Indies.

Blue dye can be made from two different types of plants: the indigo plant, which produces the best results, and from the woad plant "Isatis tinctoria", also known as pastel. For a long time woad was the main source of blue dye in Europe. Woad was replaced by true indigo as trade routes opened up, and both plant sources have now been largely replaced by synthetic dyes.

The Early Modern English word "indigo" referred to the dye, not to the color (hue) itself, and "indigo" is not traditionally part of the basic color-naming system. Modern sources place indigo in the electromagnetic spectrum between 420 and 450 nanometers, which lies on the short-wave side of color wheel (RGB) blue, towards (spectral) violet.

However, the correspondence of this definition with colors of actual indigo dyes is disputed. Optical scientists Hardy and Perrin list indigo as between 445 and 464 nm wavelength, which occupies a spectrum segment from roughly the color wheel (RGB) blue extending to the long-wave side, towards azure.

Isaac Newton introduced indigo as one of the seven base colors of his work. In the mid-1660s, when Newton bought a pair of prisms at a fair near Cambridge, the East India Company had begun importing indigo dye into England, supplanting the homegrown woad as source of blue dye. In a pivotal experiment in the history of optics, the young Newton shone a narrow beam of sunlight through a prism to produce a rainbow-like band of colors on the wall. In describing this optical spectrum, Newton acknowledged that the spectrum had a continuum of colors, but named seven: "The originall or primary colours are Red, yellow, Green, Blew, & a violet purple; together with Orang, Indico, & an indefinite varietie of intermediate gradations." He linked the seven prismatic colors to the seven notes of a western major scale, as shown in his color wheel, with orange and indigo as the semitones. Having decided upon seven colors, he asked a friend to repeatedly divide up the spectrum that was projected from the prism onto the wall:

I desired a friend to draw with a pencil lines cross the image, or pillar of colours, where every one of the seven aforenamed colours was most full and brisk, and also where he judged the truest confines of them to be, whilst I held the paper so, that the said image might fall within a certain compass marked on it. And this I did, partly because my own eyes are not very critical in distinguishing colours, partly because another, to whom I had not communicated my thoughts about this matter, could have nothing but his eyes to determine his fancy in making those marks.

Indigo is therefore counted as one of the traditional colors of the rainbow, the order of which is given by the mnemonic "Roy G. Biv". James Clerk Maxwell and Hermann von Helmholtz accepted indigo as an appropriate name for the color flanking violet in the spectrum.

Later scientists conclude that Newton named the colors differently from current usage.
According to Gary Waldman, "A careful reading of Newton's work indicates that the color he called indigo, we would normally call blue; his blue is then what we would name blue-green, cyan or light blue." If this is true, Newton's seven spectral colors would have been:
Red: Orange: Yellow: Green: Blue: Indigo: Violet:

The human eye does not readily differentiate hues in the wavelengths between what we today call blue and violet. If this is where Newton meant indigo to lie, most individuals would have difficulty distinguishing indigo from its neighbors. According to Isaac Asimov, "It is customary to list indigo as a color lying between blue and violet, but it has never seemed to me that indigo is worth the dignity of being considered a separate color. To my eyes it seems merely deep blue."

Modern color scientists typically divide the spectrum between violet and blue at about 450 nm, with no indigo.

Like many other colors (orange, rose, and violet are the best-known), indigo gets its name from an object in the natural world—the plant named indigo once used for dyeing cloth (see also Indigo dye).

The color "electric indigo" is a bright and saturated color between the traditional indigo and violet. This is the brightest color indigo that can be approximated on a computer screen; it is a color located between the (primary) blue and the color violet of the RGB color wheel.

The web color "blue violet" or "deep indigo" is a tone of indigo brighter than pigment indigo, but not as bright as electric indigo.

The color "pigment indigo" is equivalent to the web color indigo and approximates the color indigo that is usually reproduced in pigments and colored pencils.

The color of indigo dye is a different color from either spectrum indigo or pigment indigo. This is the actual color of the dye. A vat full of this dye is a darker color, approximating the web color midnight blue.

Below are displayed these four major tones of indigo.

"Electric indigo" is brighter than the pigment indigo reproduced below. When plotted on the CIE chromaticity diagram, this color is at 435 nanometers, in the middle of the portion of the spectrum traditionally considered indigo, i.e., between 450 and 420 nanometers. This color is only an approximation of spectral indigo, since actual spectral colors are outside the gamut of the sRGB color system.
At right is displayed the web color "blue-violet", a color intermediate in brightness between electric indigo and pigment indigo. It is also known as "deep indigo".

The color box at right displays the web color indigo, the color indigo as it would be reproduced by artists' paints as opposed to the brighter indigo above (electric indigo) that is possible to reproduce on a computer screen. Its hue is closer to violet than to indigo dye for which the color is named. Pigment indigo can be obtained by mixing 55% pigment cyan with about 45% pigment magenta.

Compare the subtractive colors to the additive colors in the two primary color charts in the article on primary colors to see the distinction between electric colors as reproducible from light on a computer screen (additive colors) and the pigment colors reproducible with pigments (subtractive colors); the additive colors are significantly brighter because they are produced from light instead of pigment.

Web color indigo represents the way the color indigo was always reproduced in pigments, paints, or colored pencils in the 1950s. By the 1970s, because of the advent of psychedelic art, artists became used to brighter pigments, and pigments called "bright indigo" or "bright blue-violet" that are the pigment equivalent of the electric indigo reproduced in the section above became available in artists' pigments and colored pencils.
'Tropical Indigo' is the color that is called "añil" in the "Guía de coloraciones" (Guide to colorations) by Rosa Gallego and
Juan Carlos Sanz, a color dictionary published in 2005 that is widely popular in the Hispanophone realm.

"Indigo dye" is a greenish dark blue color.

Literature

Marina Warner's novel "Indigo" (1992) is a retelling of Shakespeare's "The Tempest" and features the production of indigo dye by Sycorax.





The French Army adopted dark blue indigo at the time of the French Revolution, as a replacement for the white uniforms previously worn by the Royal infantry regiments. In 1806, Napoleon decided to restore the white coats because of shortages of indigo dye imposed by the British continental blockade. However, the greater practicability of the blue color led to its retention, and indigo remained the dominant color of French military coats until 1914.

The Spiritualist applications use electric indigo, because the color is positioned between blue and violet on the spectrum.



</doc>
<doc id="15251" url="https://en.wikipedia.org/wiki?curid=15251" title="International Monetary Fund">
International Monetary Fund

The International Monetary Fund (IMF) is an international organization headquartered in Washington, D.C., consisting of "189 countries working to foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world." Formed in 1945 at the Bretton Woods Conference primarily by the ideas of Harry Dexter White and John Maynard Keynes, it came into formal existence in 1945 with 29 member countries and the goal of reconstructing the international payment system. It now plays a central role in the management of balance of payments difficulties and international financial crises. Countries contribute funds to a pool through a quota system from which countries experiencing balance of payments problems can borrow money. , the fund had SDR477 billion (about $666 billion).

Through the fund, and other activities such as the gathering of statistics and analysis, surveillance of its members' economies and the demand for particular policies, the IMF works to improve the economies of its member countries. The organisation's objectives stated in the Articles of Agreement are: to promote international monetary co-operation, international trade, high employment, exchange-rate stability, sustainable economic growth, and making resources available to member countries in financial difficulty.

The current Managing Director (MD) and Chairman of the International Monetary Fund is noted French lawyer and former politician, Christine Lagarde, who has held the post since 5 July 2011.

According to the IMF itself, it works to foster global growth and economic stability by providing policy, advice and financing the members, by working with developing nations to help them achieve macroeconomic stability and reduce poverty. The rationale for this is that private international capital markets function imperfectly and many countries have limited access to financial markets. Such market imperfections, together with balance-of-payments financing, provide the justification for official financing, without which many countries could only correct large external payment imbalances through measures with adverse economic consequences. The IMF provides alternate sources of financing.

Upon the founding of the IMF, its three primary functions were: to oversee the fixed exchange rate arrangements between countries, thus helping national governments manage their exchange rates and allowing these governments to prioritise economic growth, and to provide short-term capital to aid the balance of payments. This assistance was meant to prevent the spread of international economic crises. The IMF was also intended to help mend the pieces of the international economy after the Great Depression and World War II. As well, to provide capital investments for economic growth and projects such as infrastructure.

The IMF's role was fundamentally altered by the floating exchange rates post-1971. It shifted to examining the economic policies of countries with IMF loan agreements to determine if a shortage of capital was due to economic fluctuations or economic policy. The IMF also researched what types of government policy would ensure economic recovery. A particular concern of the IMF was to prevent financial crisis, such as those in Mexico 1982, Brazil in 1987, East Asia in 1997–98 and Russia in 1998, from spreading and threatening the entire global financial and currency system. The challenge was to promote and implement policy that reduced the frequency of crises among the emerging market countries, especially the middle-income countries which are vulnerable to massive capital outflows. Rather than maintaining a position of oversight of only exchange rates, their function became one of surveillance of the overall macroeconomic performance of member countries. Their role became a lot more active because the IMF now manages economic policy rather than just exchange rates.

In addition, the IMF negotiates conditions on lending and loans under their policy of conditionality, which was established in the 1950s. Low-income countries can borrow on concessional terms, which means there is a period of time with no interest rates, through the Extended Credit Facility (ECF), the Standby Credit Facility (SCF) and the Rapid Credit Facility (RCF). Nonconcessional loans, which include interest rates, are provided mainly through Stand-By Arrangements (SBA), the Flexible Credit Line (FCL), the Precautionary and Liquidity Line (PLL), and the Extended Fund Facility. The IMF provides emergency assistance via the Rapid Financing Instrument (RFI) to members facing urgent balance-of-payments needs.

The IMF is mandated to oversee the international monetary and financial system and monitor the economic and financial policies of its member countries. This activity is known as surveillance and facilitates international co-operation. Since the demise of the Bretton Woods system of fixed exchange rates in the early 1970s, surveillance has evolved largely by way of changes in procedures rather than through the adoption of new obligations. The responsibilities changed from those of guardian to those of overseer of members’ policies.

The Fund typically analyses the appropriateness of each member country’s economic and financial policies for achieving orderly economic growth, and assesses the consequences of these policies for other countries and for the global economy.

In 1995 the International Monetary Fund began work on data dissemination standards with the view of guiding IMF member countries to disseminate their economic and financial data to the public. The International Monetary and Financial Committee (IMFC) endorsed the guidelines for the dissemination standards and they were split into two tiers: The General Data Dissemination System (GDDS) and the Special Data Dissemination Standard (SDDS).

The executive board approved the SDDS and GDDS in 1996 and 1997 respectively, and subsequent amendments were published in a revised "Guide to the General Data Dissemination System". The system is aimed primarily at statisticians and aims to improve many aspects of statistical systems in a country. It is also part of the World Bank Millennium Development Goals and Poverty Reduction Strategic Papers.

The primary objective of the GDDS is to encourage member countries to build a framework to improve data quality and statistical capacity building to evaluate statistical needs, set priorities in improving the timeliness, transparency, reliability and accessibility of financial and economic data. Some countries initially used the GDDS, but later upgraded to SDDS.

Some entities that are not themselves IMF members also contribute statistical data to the systems:

IMF conditionality is a set of policies or conditions that the IMF requires in exchange for financial resources. The IMF does require collateral from countries for loans but also requires the government seeking assistance to correct its macroeconomic imbalances in the form of policy reform. If the conditions are not met, the funds are withheld. The concept of conditionality was introduced in a 1952 Executive Board decision and later incorporated into the Articles of Agreement.

Conditionality is associated with economic theory as well as an enforcement mechanism for repayment. Stemming primarily from the work of Jacques Polak, the theoretical underpinning of conditionality was the "monetary approach to the balance of payments".

Some of the conditions for structural adjustment can include:

These conditions are known as the Washington Consensus.

These loan conditions ensure that the borrowing country will be able to repay the IMF and that the country will not attempt to solve their balance-of-payment problems in a way that would negatively impact the international economy. The incentive problem of moral hazard—when economic agents maximise their own utility to the detriment of others because they do not bear the full consequences of their actions—is mitigated through conditions rather than providing collateral; countries in need of IMF loans do not generally possess internationally valuable collateral anyway.

Conditionality also reassures the IMF that the funds lent to them will be used for the purposes defined by the Articles of Agreement and provides safeguards that country will be able to rectify its macroeconomic and structural imbalances. In the judgment of the IMF, the adoption by the member of certain corrective measures or policies will allow it to repay the IMF, thereby ensuring that the resources will be available to support other members.

, borrowing countries have had a very good track record for repaying credit extended under the IMF's regular lending facilities with full interest over the duration of the loan. This indicates that IMF lending does not impose a burden on creditor countries, as lending countries receive market-rate interest on most of their quota subscription, plus any of their own-currency subscriptions that are loaned out by the IMF, plus all of the reserve assets that they provide the IMF.

The IMF was originally laid out as a part of the Bretton Woods system exchange agreement in 1944. During the Great Depression, countries sharply raised barriers to trade in an attempt to improve their failing economies. This led to the devaluation of national currencies and a decline in world trade.
This breakdown in international monetary co-operation created a need for oversight. The representatives of 45 governments met at the Bretton Woods Conference in the Mount Washington Hotel in Bretton Woods, New Hampshire, in the United States, to discuss a framework for postwar international economic co-operation and how to rebuild Europe.

There were two views on the role the IMF should assume as a global economic institution. American delegate Harry Dexter White foresaw an IMF that functioned more like a bank, making sure that borrowing states could repay their debts on time. Most of White's plan was incorporated into the final acts adopted at Bretton Woods. British economist John Maynard Keynes imagined that the IMF would be a cooperative fund upon which member states could draw to maintain economic activity and employment through periodic crises. This view suggested an IMF that helped governments and to act as the United States government had during the New Deal in response to World War II.
The IMF formally came into existence on 27 December 1945, when the first 29 countries ratified its Articles of Agreement. By the end of 1946 the IMF had grown to 39 members. On 1 March 1947, the IMF began its financial operations, and on 8 May France became the first country to borrow from it.

The IMF was one of the key organisations of the international economic system; its design allowed the system to balance the rebuilding of international capitalism with the maximisation of national economic sovereignty and human welfare, also known as embedded liberalism. The IMF's influence in the global economy steadily increased as it accumulated more members. The increase reflected in particular the attainment of political independence by many African countries and more recently the 1991 dissolution of the Soviet Union because most countries in the Soviet sphere of influence did not join the IMF.

The Bretton Woods system prevailed until 1971, when the United States government suspended the convertibility of the US$ (and dollar reserves held by other governments) into gold. This is known as the Nixon Shock. The changes to the IMF articles of agreement reflecting these changes were ratified by the 1976 Jamaica Accords.

The IMF provided two major lending packages in the early 2000s to Argentina (during the 1998–2002 Argentine great depression) and Uruguay (after the 2002 Uruguay banking crisis). However, by the mid-2000s, IMF lending was at its lowest share of world GDP since the 1970s.

In May 2010, the IMF participated, in 3:11 proportion, in the first Greek bailout that totalled €110 billion, to address the great accumulation of public debt, caused by continuing large public sector deficits. As part of the bailout, the Greek government agreed to adopt austerity measures that would reduce the deficit from 11% in 2009 to "well below 3%" in 2014. The bailout did not include debt restructuring measures such as a haircut, to the chagrin of the Swiss, Brazilian, Indian, Russian, and Argentinian Directors of the IMF, with the Greek authorities themselves (at the time, PM George Papandreou and Finance Minister Giorgos Papakonstantinou) ruling out a haircut.

A second bailout package of more than €100 billion was agreed over the course of a few months from October 2011, during which time Papandreou was forced from office. The so-called Troika, of which the IMF is part, are joint managers of this programme, which was approved by the Executive Directors of the IMF on 15 March 2012 for SDR23.8 billion, and which saw private bondholders take a haircut of upwards of 50%. In the interval between May 2010 and February 2012 the private banks of Holland, France and Germany reduced exposure to Greek debt from €122 billion to €66 billion.

As of January 2012, the largest borrowers from the IMF in order were Greece, Portugal, Ireland, Romania, and Ukraine.

On 25 March 2013, a €10 billion international bailout of Cyprus was agreed by the Troika, at the cost to the Cypriots of its agreement: to close the country's second-largest bank; to impose a one-time bank deposit levy on Bank of Cyprus uninsured deposits. No insured deposit of €100k or less were to be affected under the terms of a novel bail-in scheme.

The topic of sovereign debt restructuring was taken up by the IMF in April 2013 for the first time since 2005, in a report entitled "Sovereign Debt Restructuring: Recent Developments and Implications for the Fund’s Legal and Policy Framework". The paper, which was discussed by the board on 20 May, summarised the recent experiences in Greece, St Kitts and Nevis, Belize, and Jamaica. An explanatory interview with Deputy Director Hugh Bredenkamp was published a few days later, as was a deconstruction by Matina Stevis of the "Wall Street Journal".

In the October 2013 Fiscal Monitor publication, the IMF suggested that a capital levy capable of reducing Euro-area government debt ratios to "end-2007 levels" would require a very high tax rate of about 10%.

The Fiscal Affairs department of the IMF, headed at the time by Acting Director Sanjeev Gupta, produced a January 2014 report entitled "Fiscal Policy and Income Inequality" that stated that "Some taxes levied on wealth, especially on immovable property, are also an option for economies seeking more progressive taxation ... Property taxes are equitable and efficient, but underutilized in many economies ... There is considerable scope to exploit this tax more fully, both as a revenue source and as a redistributive instrument."

At the end of March 2014, the IMF secured an $18 billion bailout fund for the provisional government of Ukraine in the aftermath of the 2014 Ukrainian revolution.

Not all member countries of the IMF are sovereign states, and therefore not all "member countries" of the IMF are members of the United Nations. Amidst "member countries" of the IMF that are not member states of the UN are non-sovereign areas with special jurisdictions that are officially under the sovereignty of full UN member states, such as Aruba, Curaçao, Hong Kong, and Macau, as well as Kosovo. The corporate members appoint "ex-officio" voting members, who are listed below. All members of the IMF are also International Bank for Reconstruction and Development (IBRD) members and vice versa.

Former members are Cuba (which left in 1964), and the Republic of China (Taiwan), which was ejected from the UN in 1980 after losing the support of then United States President Jimmy Carter and was replaced by the People's Republic of China. However, "Taiwan Province of China" is still listed in the official IMF indices.

Apart from Cuba, the other UN states that do not belong to the IMF are Andorra, Liechtenstein, Monaco and North Korea.

The former Czechoslovakia was expelled in 1954 for "failing to provide required data" and was readmitted in 1990, after the Velvet Revolution. Poland withdrew in 1950—allegedly pressured by the Soviet Union—but returned in 1986.

Any country may apply to be a part of the IMF. Post-IMF formation, in the early postwar period, rules for IMF membership were left relatively loose. Members needed to make periodic membership payments towards their quota, to refrain from currency restrictions unless granted IMF permission, to abide by the Code of Conduct in the IMF Articles of Agreement, and to provide national economic information. However, stricter rules were imposed on governments that applied to the IMF for funding.

The countries that joined the IMF between 1945 and 1971 agreed to keep their exchange rates secured at rates that could be adjusted only to correct a "fundamental disequilibrium" in the balance of payments, and only with the IMF's agreement.

Some members have a very difficult relationship with the IMF and even when they are still members they do not allow themselves to be monitored.

Member countries of the IMF have access to information on the economic policies of all member countries, the opportunity to influence other members’ economic policies, technical assistance in banking, fiscal affairs, and exchange matters, financial support in times of payment difficulties, and increased opportunities for trade and investment.

The Board of Governors consists of one governor and one alternate governor for each member country. Each member country appoints its two governors. The Board normally meets once a year and is responsible for electing or appointing executive directors to the Executive Board. While the Board of Governors is officially responsible for approving quota increases, Special Drawing Right allocations, the admittance of new members, compulsory withdrawal of members, and amendments to the Articles of Agreement and By-Laws, in practice it has delegated most of its powers to the IMF's Executive Board.

The Board of Governors is advised by the International Monetary and Financial Committee and the Development Committee. The International Monetary and Financial Committee has 24 members and monitors developments in global liquidity and the transfer of resources to developing countries. The Development Committee has 25 members and advises on critical development issues and on financial resources required to promote economic development in developing countries. They also advise on trade and environmental issues.

24 Executive Directors make up the Executive Board. The Executive Directors represent all 189 member countries in a geographically based roster. Countries with large economies have their own Executive Director, but most countries are grouped in constituencies representing four or more countries.

Following the "2008 Amendment on Voice and Participation" which came into effect in March 2011, eight countries each appoint an Executive Director: the United States, Japan, China, Germany, France, the United Kingdom, Russia, and Saudi Arabia. The remaining 16 Directors represent constituencies consisting of 4 to 22 countries. The Executive Director representing the largest constituency of 22 countries accounts for 1.55% of the vote. This Board usually meets several times each week. The Board membership and constituency is scheduled for periodic review every eight years.

The IMF is led by a managing director, who is head of the staff and serves as Chairman of the Executive Board. The managing director is assisted by a First Deputy managing director and three other Deputy Managing Directors. Historically the IMF's managing director has been European and the president of the World Bank has been from the United States. However, this standard is increasingly being questioned and competition for these two posts may soon open up to include other qualified candidates from any part of the world.

In 2011 the world's largest developing countries, the BRIC nations, issued a statement declaring that the tradition of appointing a European as managing director undermined the legitimacy of the IMF and called for the appointment to be merit-based.

Previous managing director Dominique Strauss-Kahn was arrested in connection with charges of sexually assaulting a New York hotel room attendant and resigned on 18 May. The charges were later dropped. On 28 June 2011 Christine Lagarde was confirmed as managing director of the IMF for a five-year term starting on 5 July 2011.

Voting power in the IMF is based on a quota system. Each member has a number of basic votes (each member's number of basic votes equals 5.502% of the total votes), plus one additional vote for each Special Drawing Right (SDR) of 100,000 of a member country's quota. The Special Drawing Right is the unit of account of the IMF and represents a claim to currency. It is based on a basket of key international currencies. The basic votes generate a slight bias in favour of small countries, but the additional votes determined by SDR outweigh this bias. Changes in the voting shares require approval by a supermajority of 85% of voting power.

In December 2015, the United States Congress adopted a legislation authorising the 2010 Quota and Governance Reforms. As a result, 

The IMF's quota system was created to raise funds for loans. Each IMF member country is assigned a quota, or contribution, that reflects the country's relative size in the global economy. Each member's quota also determines its relative voting power. Thus, financial contributions from member governments are linked to voting power in the organisation.

This system follows the logic of a shareholder-controlled organisation: wealthy countries have more say in the making and revision of rules. Since decision making at the IMF reflects each member's relative economic position in the world, wealthier countries that provide more money to the IMF have more influence than poorer members that contribute less; nonetheless, the IMF focuses on redistribution.

Quotas are normally reviewed every five years and can be increased when deemed necessary by the Board of Governors. IMF voting shares are relatively inflexible: countries that grow economically have tended to become under-represented as their voting power lags behind. Currently, reforming the representation of developing countries within the IMF has been suggested. These countries' economies represent a large portion of the global economic system but this is not reflected in the IMF's decision making process through the nature of the quota system. Joseph Stiglitz argues, "There is a need to provide more effective voice and representation for developing countries, which now represent a much larger portion of world economic activity since 1944, when the IMF was created." In 2008, a number of quota reforms were passed including shifting 6% of quota shares to dynamic emerging markets and developing countries.

The IMF's membership is divided along income lines: certain countries provide the financial resources while others use these resources. Both developed country "creditors" and developing country "borrowers" are members of the IMF. The developed countries provide the financial resources but rarely enter into IMF loan agreements; they are the creditors. Conversely, the developing countries use the lending services but contribute little to the pool of money available to lend because their quotas are smaller; they are the borrowers. Thus, tension is created around governance issues because these two groups, creditors and borrowers, have fundamentally different interests.

The criticism is that the system of voting power distribution through a quota system institutionalises borrower subordination and creditor dominance. The resulting division of the IMF's membership into borrowers and non-borrowers has increased the controversy around conditionality because the borrowers are interested in increasing loan access while creditors want to maintain reassurance that the loans will be repaid.

A recent source revealed that the average overall use of IMF credit per decade increased, in real terms, by 21% between the 1970s and 1980s, and increased again by just over 22% from the 1980s to the 1991–2005 period. Another study has suggested that since 1950 the continent of Africa alone has received $300 billion from the IMF, the World Bank, and affiliate institutions.

A study by Bumba Mukherjee found that developing democratic countries benefit more from IMF programs than developing autocratic countries because policy-making, and the process of deciding where loaned money is used, is more transparent within a democracy. One study done by Randall Stone found that although earlier studies found little impact of IMF programs on balance of payments, more recent studies using more sophisticated methods and larger samples "usually found IMF programs improved the balance of payments".

The Exceptional Access Framework was created in 2003 when John B. Taylor was Under Secretary of the US Treasury for International Affairs. The new Framework became fully operational in February 2003 and it was applied in the subsequent decisions on Argentina and Brazil. Its purpose was to place some sensible rules and limits on the way the IMF makes loans to support governments with debt problem—especially in emerging markets—and thereby move away from the bailout mentality of the 1990s. Such a reform was essential for ending the crisis atmosphere that then existed in emerging markets. The reform was closely related to, and put in place nearly simultaneously with, the actions of several emerging market countries to place collective action clauses in their bond contracts.

In 2010, the framework was abandoned so the IMF could make loans to Greece in an unsustainable and political situation.

The topic of sovereign debt restructuring was taken up by IMF staff in April 2013 for the first time since 2005, in a report entitled "Sovereign Debt Restructuring: Recent Developments and Implications for the Fund's Legal and Policy Framework". The paper, which was discussed by the board on 20 May, summarised the recent experiences in Greece, St Kitts and Nevis, Belize and Jamaica. An explanatory interview with Deputy Director Hugh Bredenkamp was published a few days later, as was a deconstruction by Matina Stevis of the "Wall Street Journal".

The staff was directed to formulate an updated policy, which was accomplished on 22 May 2014 with a report entitled "The Fund's Lending Framework and Sovereign Debt: Preliminary Considerations", and taken up by the Executive Board on 13 June. The staff proposed that "in circumstances where a (Sovereign) member has lost market access and debt is considered sustainable ... the IMF would be able to provide Exceptional Access on the basis of a debt operation that involves an extension of maturities", which was labelled a "reprofiling operation". These reprofiling operations would "generally be less costly to the debtor and creditors—and thus to the system overall—relative to either an upfront debt reduction operation or a bail-out that is followed by debt reduction ... (and) would be envisaged only when both (a) a member has lost market access and (b) debt is assessed to be sustainable, but not with high probability ... Creditors will only agree if they understand that such an amendment is necessary to avoid a worse outcome: namely, a default and/or an operation involving debt reduction ... Collective action clauses, which now exist in most—but not all—bonds, would be relied upon to address collective action problems."

Globalization encompasses three institutions: global financial markets and transnational companies, national governments linked to each other in economic and military alliances led by the United States, and rising "global governments" such as World Trade Organization (WTO), IMF, and World Bank. Charles Derber argues in his book "People Before Profit," "These interacting institutions create a new global power system where sovereignty is globalized, taking power and constitutional authority away from nations and giving it to global markets and international bodies". Titus Alexander argues that this system institutionalises global inequality between western countries and the Majority World in a form of global apartheid, in which the IMF is a key pillar.

The establishment of globalised economic institutions has been both a symptom of and a stimulus for globalisation. The development of the World Bank, the IMF regional development banks such as the European Bank for Reconstruction and Development (EBRD), and multilateral trade institutions such as the WTO signals a move away from the dominance of the state as the exclusive unit of analysis in international affairs. Globalization has thus been transformative in terms of a reconceptualising of state sovereignty.

Following United States President Bill Clinton's administration's aggressive financial deregulation campaign in the 1990s, globalisation leaders overturned longstanding restrictions by governments that limited foreign ownership of their banks, deregulated currency exchange, and eliminated restrictions on how quickly money could be withdrawn by foreign investors.

Fund report in May 2015, the world's governments indirectly subsidise fossil fuel companies with $5.3tn (£3.4tn) a year. Most this is due to polluters not paying the costs imposed on governments by the burning of coal, oil and gas: air pollution, health problems, the floods, droughts and storms driven by climate change.

Overseas Development Institute (ODI) research undertaken in 1980 included criticisms of the IMF which support the analysis that it is a pillar of what activist Titus Alexander calls global apartheid.

ODI conclusions were that the IMF's very nature of promoting market-oriented approaches attracted unavoidable criticism. On the other hand, the IMF could serve as a scapegoat while allowing governments to blame international bankers. The ODI conceded that the IMF was insensitive to political aspirations of LDCs, while its policy conditions were inflexible.

Argentina, which had been considered by the IMF to be a model country in its compliance to policy proposals by the Bretton Woods institutions, experienced a catastrophic economic crisis in 2001, which some believe to have been caused by IMF-induced budget restrictions—which undercut the government's ability to sustain national infrastructure even in crucial areas such as health, education, and security—and privatisation of strategically vital national resources. Others attribute the crisis to Argentina's misdesigned fiscal federalism, which caused subnational spending to increase rapidly. The crisis added to widespread hatred of this institution in Argentina and other South American countries, with many blaming the IMF for the region's economic problems. The current—as of early 2006—trend toward moderate left-wing governments in the region and a growing concern with the development of a regional economic policy largely independent of big business pressures has been ascribed to this crisis.

In 2006, a senior ActionAid policy analyst Akanksha Marphatia stated that IMF policies in Africa undermine any possibility of meeting the Millennium Development Goals (MDGs) due to imposed restrictions that prevent spending on important sectors, such as education and health.

In an interview (2008-05-19), the former Romanian Prime Minister Călin Popescu-Tăriceanu claimed that "Since 2005, IMF is constantly making mistakes when it appreciates the country's economic performances". Former Tanzanian President Julius Nyerere, who claimed that debt-ridden African states were ceding sovereignty to the IMF and the World Bank, famously asked, "Who elected the IMF to be the ministry of finance for every country in the world?"

Former chief economist of IMF and former Reserve Bank of India (RBI) Governor Raghuram Rajan who predicted Financial crisis of 2007–08 criticised IMF for remaining a sideline player to the Developed world. He criticised IMF for praising the monetary policies of the US, which he believed were wreaking havoc in emerging markets. He had been critical of the ultra-loose money policies of the Western nations and IMF.

The IMF has been criticised for being "out of touch" with local economic conditions, cultures, and environments in the countries they are requiring policy reform. The economic advice the IMF gives might not always take into consideration the difference between what spending means on paper and how it is felt by citizens.

Jeffrey Sachs argues that the IMF's "usual prescription is 'budgetary belt tightening to countries who are much too poor to own belts'". Sachs wrote that the IMF's role as a generalist institution specialising in macroeconomic issues needs reform. Conditionality has also been criticised because a country can pledge collateral of "acceptable assets" to obtain waivers—if one assumes that all countries are able to provide "acceptable collateral".

One view is that conditionality undermines domestic political institutions. The recipient governments are sacrificing policy autonomy in exchange for funds, which can lead to public resentment of the local leadership for accepting and enforcing the IMF conditions. Political instability can result from more leadership turnover as political leaders are replaced in electoral backlashes. IMF conditions are often criticised for reducing government services, thus increasing unemployment.

Another criticism is that IMF programs are only designed to address poor governance, excessive government spending, excessive government intervention in markets, and too much state ownership. This assumes that this narrow range of issues represents the only possible problems; everything is standardised and differing contexts are ignored. A country may also be compelled to accept conditions it would not normally accept had they not been in a financial crisis in need of assistance.

On top of that, regardless of what methodologies and data sets used, it comes to same conclusion of exacerbating income inequality. With Gini coefficient, it became clear that countries with IMF programs face increased income inequality.

It is claimed that conditionalities retard social stability and hence inhibit the stated goals of the IMF, while Structural Adjustment Programs lead to an increase in poverty in recipient countries. The IMF sometimes advocates "austerity programmes", cutting public spending and increasing taxes even when the economy is weak, to bring budgets closer to a balance, thus reducing budget deficits. Countries are often advised to lower their corporate tax rate. In "Globalization and Its Discontents", Joseph E. Stiglitz, former chief economist and senior vice-president at the World Bank, criticises these policies. He argues that by converting to a more monetarist approach, the purpose of the fund is no longer valid, as it was designed to provide funds for countries to carry out Keynesian reflations, and that the IMF "was not participating in a conspiracy, but it was reflecting the interests and ideology of the Western financial community".

International politics play an important role in IMF decision making. The clout of member states is roughly proportional to its contribution to IMF finances. The United States has the greatest number of votes and therefore wields the most influence. Domestic politics often come into play, with politicians in developing countries using conditionality to gain leverage over the opposition to influence policy.

The IMF is only one of many international organisations, and it is a generalist institution that deals only with macroeconomic issues; its core areas of concern in developing countries are very narrow. One proposed reform is a movement towards close partnership with other specialist agencies such as UNICEF, the Food and Agriculture Organization (FAO), and the United Nations Development Program (UNDP).

Jeffrey Sachs argues in "The End of Poverty" that the IMF and the World Bank have "the brightest economists and the lead in advising poor countries on how to break out of poverty, but the problem is development economics". Development economics needs the reform, not the IMF. He also notes that IMF loan conditions should be paired with other reforms—e.g., trade reform in developed nations, debt cancellation, and increased financial assistance for investments in basic infrastructure. IMF loan conditions cannot stand alone and produce change; they need to be partnered with other reforms or other conditions as applicable.

The scholarly consensus is that IMF decision-making is not simply technocratic, but also guided by political and economic concerns. The United States is the IMF's most powerful member, and its influence reaches even into decision-making concerning individual loan agreements. The United States has historically been openly opposed to losing what Treasury Secretary Jacob Lew described in 2015 as its "leadership role" at the IMF, and the United States' "ability to shape international norms and practices".

Reforms to give more powers to emerging economies were agreed by the G20 in 2010. The reforms could not pass, however, until they were ratified by the US Congress, since 85% of the Fund's voting power was required for the reforms to take effect, and the Americans held more than 16% of voting power at the time. After repeated criticism, the United States finally ratified the voting reforms at the end of 2015. The OECD countries maintained their overwhelming majority of voting share, and the United States in particular retained its share at over 16%.

The role of the Bretton Woods institutions has been controversial since the late Cold War, because of claims that the IMF policy makers supported military dictatorships friendly to American and European corporations, but also other anti-communist and Communist regimes (such as Mobutu's Zaire and Ceaușescu's Romania, respectively). Critics also claim that the IMF is generally apathetic or hostile to human rights, and labour rights. The controversy has helped spark the anti-globalization movement.

An example of IMF's support for a dictatorship was its ongoing support for Mobutu's rule in Zaire, although its own envoy, Erwin Blumenthal, provided a sobering report about the entrenched corruption and embezzlement and the inability of the country to pay back any loans.

Arguments in favour of the IMF say that economic stability is a precursor to democracy; however, critics highlight various examples in which democratised countries fell after receiving IMF loans.

A 2017 study found no evidence of IMF lending programs undermining democracy in borrowing countries. To the contrary, it found "evidence for modest but definitively positive conditional differences in the democracy scores of participating and non-participating countries."

A number of civil society organisations have criticised the IMF's policies for their impact on access to food, particularly in developing countries. In October 2008, former United States president Bill Clinton delivered a speech to the United Nations on World Food Day, criticising the World Bank and IMF for their policies on food and agriculture:

A 2009 study concluded that the strict conditions resulted in thousands of deaths in Eastern Europe by tuberculosis as public health care had to be weakened. In the 21 countries to which the IMF had given loans, tuberculosis deaths rose by 16.6%.

In 2009, a book by Rick Rowden titled "The Deadly Ideas of Neoliberalism: How the IMF has Undermined Public Health and the Fight Against AIDS", claimed that the IMF's monetarist approach towards prioritising price stability (low inflation) and fiscal restraint (low budget deficits) was unnecessarily restrictive and has prevented developing countries from scaling up long-term investment in public health infrastructure. The book claimed the consequences have been chronically underfunded public health systems, leading to demoralising working conditions that have fuelled a "brain drain" of medical personnel, all of which has undermined public health and the fight against HIV/AIDS in developing countries.

In 2016, the IMF's research department published a report titled "Neoliberalism: Oversold?" which, while praising some aspects of the "neoliberal agenda," claims that the organisation has been "overselling" fiscal austerity policies and financial deregulation, which they claim has exacerbated both financial crises and economic inequality around the world.

IMF policies have been repeatedly criticised for making it difficult for indebted countries to say no to environmentally harmful projects that nevertheless generate revenues such as oil, coal, and forest-destroying lumber and agriculture projects. Ecuador, for example, had to defy IMF advice repeatedly to pursue the protection of its rainforests, though paradoxically this need was cited in the IMF argument to provide support to Ecuador. The IMF acknowledged this paradox in the 2010 report that proposed the IMF Green Fund, a mechanism to issue special drawing rights directly to pay for climate harm prevention and potentially other ecological protection as pursued generally by other environmental finance.

While the response to these moves was generally positive possibly because ecological protection and energy and infrastructure transformation are more politically neutral than pressures to change social policy. Some experts voiced concern that the IMF was not representative, and that the IMF proposals to generate only US$200 billion a year by 2020 with the SDRs as seed funds, did not go far enough to undo the general incentive to pursue destructive projects inherent in the world commodity trading and banking systems—criticisms often levelled at the World Trade Organization and large global banking institutions.

In the context of the European debt crisis, some observers noted that Spain and California, two troubled economies within Europe and the United States, and also Germany, the primary and politically most fragile supporter of a euro currency bailout would benefit from IMF recognition of their leadership in green technology, and directly from Green Fund–generated demand for their exports, which could also improve their credit ratings.

Both Lagarde and her two predecessors Strauss-Kahn and Rato have been investigated by the authorities and have either faced trial or are scheduled to go on trial for a variety of offences.

Lagarde had been accused of giving preferential treatment to businessman-turned-politician Bernard Tapie as he pursued a legal challenge against the French government. At the time, Lagarde was the French economic minister. Within hours of her conviction, in which she escaped any punishment, the fund's 24-member executive board put to rest any speculation that she might have to resign, praising her "outstanding leadership" and the "wide respect" she commands around the world.

In March 2011 the Ministers of Economy and Finance of the African Union proposed to establish an African Monetary Fund.

At the 6th BRICS summit in July 2014 the BRICS nations (Brazil, Russia, India, China, and South Africa) announced the BRICS Contingent Reserve Arrangement (CRA) with an initial size of US$100 billion, a framework to provide liquidity through currency swaps in response to actual or potential short-term balance-of-payments pressures.

In 2014, the China-led Asian Infrastructure Investment Bank was established.

"Life and Debt", a documentary film, deals with the IMF's policies' influence on Jamaica and its economy from a critical point of view. "Debtocracy", a 2011 independent Greek documentary film, also criticises the IMF. Portuguese musician 's 1982 album is inspired by the IMF's intervention in Portugal through monitored stabilisation programs in 1977–78. In the 2015 film, "Our Brand Is Crisis", the IMF is mentioned as a point of political contention.





</doc>
<doc id="15252" url="https://en.wikipedia.org/wiki?curid=15252" title="Islands of the Clyde">
Islands of the Clyde

The Islands of the Firth of Clyde are the fifth largest of the major Scottish island groups after the Inner and Outer Hebrides, Orkney and Shetland. They are situated in the Firth of Clyde between Ayrshire and Argyll and Bute. There are about forty islands and skerries, of which only four are inhabited and only nine larger than . The largest and most populous are Arran and Bute, and Great Cumbrae and Holy Isle are also served by dedicated ferry routes. Unlike the four larger Scottish archipelagos, none of the isles in this group are connected to one another or to the mainland by bridges.

The geology and geomorphology of the area is complex and the islands and the surrounding sea lochs each have distinctive features. The influence of the Atlantic Ocean and the North Atlantic Drift create a mild, damp oceanic climate.

The larger islands have been continuously inhabited since Neolithic times, were influenced by the emergence of the kingdom of Dál Riata from 500 AD and then absorbed into the emerging Kingdom of Alba under Kenneth MacAlpin. They experienced Norse incursions during the early Middle Ages and then became part of the Kingdom of Scotland in the 13th century. There is a diversity of wildlife, including three species of rare endemic tree.

The Highland Boundary Fault runs past Bute and through the northern part of Arran, so from a geological perspective some of the islands are in the Highlands and some in the Central Lowlands. As a result, Arran is sometimes referred to as "Scotland in miniature" and the island is a popular destination for geologists, who come to see intrusive igneous landforms such as sills and dykes as well as sedimentary and metasedimentary rocks ranging widely in age. Visiting in 1787, the geologist James Hutton found his first example of an unconformity there and this spot is one of the most famous places in the study of geology. A group of weakly metamorphosed rocks that form the Highland Border Complex lie discontinuously along the Highland Boundary Fault. One of the most prominent exposures is along Loch Fad on Bute. Ailsa Craig, which lies some south of Arran, has been quarried for a rare type of micro-granite containing riebeckite known as "Ailsite" which is used to make curling stones. As of 2004, 60 to 70% of all curling stones in use were made from granite from the island.

In common with the rest of Scotland the Firth of Clyde was covered by ice sheets during the Pleistocene ice ages and the landscape is much affected by glaciation. Arran's highest peaks may have been nunataks at this time. After the last retreat of the ice sea level changes and the isostatic rise of land makes charting post glacial coastlines a complex task but the resultant clifflines behind raised beaches are a prominent feature of the entire coastline.

The soils of the islands reflect the diverse geology. Bute has the most productive land, and a pattern of deposits that is typical of the southwest of Scotland. There is a mixture of boulder clay and other glacial deposits in the eroded valleys, and raised beach and marine deposits elsewhere, especially to the south and west which result in a machair landscape in places, inland from the sandy bays, such as Stravanan.

The Firth of Clyde, in which these island lie, is north of the Irish Sea and has numerous branching inlets, some of them substantial features in their own right. These include Loch Goil, Loch Long, Gare Loch, Loch Fyne and the estuary of the River Clyde. In places the effect of glaciation on the seabed is pronounced. For example, the Firth is deep between Arran and Bute, although they are only apart. The islands are all exposed to wind and tide and various lighthouses, such as those on Ailsa Craig, Pladda and Davaar act as an aid to navigation.

The Firth of Clyde lies between 55 and 56 degrees north, at the same latitude as Labrador in Canada and north of the Aleutian Islands, but the influence of the North Atlantic Drift—the northern extension of the Gulf Stream—ameliorates the winter weather and the area enjoys a mild, damp oceanic climate. Temperatures are generally cool, averaging about in January and in July at sea level. Snow seldom lies at sea level and frosts are generally less frequent than the mainland. In common with most islands of the west coast of Scotland, rainfall is generally high at between per annum on Bute, the Cumbraes and in the south of Arran and per annum in the north of Arran. The Arran mountains are wetter still with the summits receiving over annually. May, June and July are the sunniest months, with upwards of 200 hours of bright sunshine being recorded on average, southern Bute benefiting from a particularly high level of sunny days.

Mesolithic humans arrived in the Firth of the Clyde during the fourth millennium BC, probably from Ireland. This was followed by a wave of Neolithic peoples using the same route and there is some evidence that the Firth of Clyde was a significant route via which mainland Scotland was colonised at this time. A particular style of megalithic structure developed in Argyll, the Clyde estuary and elsewhere in western Scotland that has become known as the Clyde cairn. They are rectangular or trapezoidal in shape with a small enclosing chamber faced with large slabs of stone set on end and sometimes subdivided into smaller compartments. A forecourt area may have been used for displays or rituals associated with the interment of the dead, who were placed inside the chambers. They are concentrated in Arran, Bute and Kintyre and it is likely that the Clyde cairns were the earliest forms of Neolithic monument constructed by incoming settlers although few of the 100 or so examples have been given a radiocarbon dating. An example at Monamore on Arran has been dated to 3160 BC, although it was almost certainly built earlier than that, possibly c. 4000BC. There are also numerous standing stones dating from prehistoric times, including six stone circles on Machrie Moor, Arran and other examples on Great Cumbrae and Bute.

Bronze Age settlers also constructed megaliths at various sites, many of them dating from the second millennium BC, although the chambered cairns were replaced by burial cists, found on for example, Inchmarnock. Settlement evidence, especially from the early part of this era is however poor. The Queen of the Inch necklace is an article of jewellery made of jet found on Bute that dates from circa 2000 BC. During the early Iron Age Brythonic culture held sway, there being no evidence that the Roman occupation of southern Scotland extended to these islands.

During the 2nd century AD Irish influence was at work in the region and by the 6th century the kingdom of Dál Riata was established. Unlike the P-Celtic speaking Brythons, these Gaels spoke a form of Gaelic that still survives in the Hebrides. Through the efforts of Saint Ninian and others Christianity slowly supplanted Druidism. Dál Riata flourished from the time of Fergus Mór in the late fifth century until the Viking incursions that commenced in the late eighth century. Islands close to the shores of modern Ayrshire would have remained part of the Kingdom of Strathclyde during this period, whilst the main islands became part of the emerging Kingdom of Alba founded by Kenneth MacAlpin (Cináed mac Ailpín).

The Islands of the Clyde historically formed the border zone between the Norse "Suðreyjar" and Scotland. As such many of these islands fell under Norse hegemony between the 9th and 13th centuries.

The islands of the Clyde may well have formed the power base of Somhairle mac Giolla Brighde and his descendants by the last half of the 12th century. At about this time period, the authority of the Steward of Scotland seems to have encroached into the region; and there is reason to suspect that, by the turn of the 13th century, the islands were consumed by the expanding Stewart lordship. The western extension of Scottish authority appears to have been one of the factors behind a Norwegian invasion of the region in 1230, in which the invaders seized Rothesay Castle.

In 1263 Norwegian troops commanded by Haakon Haakonarson repeated the feat but the ensuing Battle of Largs between Scots and Norwegian forces, which took place on the shores of the Firth of Clyde, was inconclusive as a military contest. This marked an ultimately terminal weakening of Norwegian power in Scotland. Haakon retreated to Orkney, where he died in December 1263, entertained on his death bed by recitations of the sagas. Following this ill-fated expedition, all rights that the Norwegian Crown "had of old therein" in relation to the islands were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth.

From the mid thirteenth century to the present day all of the islands of the Clyde have remained part of Scotland.

From the commencement of the early medieval period until 1387 all of these isles were part of the Diocese of Sodor and Man, based at Peel, on the Isle of Man. Thereafter, the seat of the Bishopric of the Isles was relocated to the north, firstly to Snizort on Skye and then Iona, a state of affairs which continued until the 16th century Scottish Reformation.

The century following 1750 was time of significant change. New forms of transport, industry and agriculture brought sweeping changes, and an end to traditional ways of life that had endured for centuries. The aftermath of the Battle of Culloden marked the beginning of the end for the clan system and whilst there were marked improvements in living standards for some, these transformations came at a cost for others. In the early 19th century Alexander, 10th Duke of Hamilton (1767–1852) embarked on a programme of clearances that had a devastating effect on Arran's population. Whole villages were removed and the Gaelic culture of the island dealt a terminal blow. A memorial to this early form of ethnic cleansing has been constructed on the shore at Lamlash, paid for by a Canadian descendant of the emigrants.

From the 1850s to the late 20th century the Clyde Puffer, made famous by the "Vital Spark", was the workhorse of the islands, carrying all kinds of produce and products to and from the islands. The Caledonian Steam Packet Company (CSP) was formed in May 1889 to operate steamer services to and from Gourock for the Caledonian Railway and soon expanded by taking over rival steamer operators. David MacBrayne Ltd operated the Glasgow to Ardrishaig steamer service, as part of the "Royal Route" to Oban. During the 20th century many of the islands were developed as tourist resorts for Glaswegians who went "Doon the Watter", in parallel to mainland resorts such as Largs and Troon.
In 1973 CSP and MacBraynes commenced joint Clyde and West Highland operations under the new name of Caledonian MacBrayne. A publicly owned company, they serve Great Cumbrae, Arran and Bute as well as running mainland-to-mainland ferries across the firth. Private companies operate services from Arran to Holy Isle and from McInroy's Point (Gourock) to Hunter's Quay on the Cowal peninsula.

The majority of the islands at one time made up the traditional County of Bute. Today the islands are split more or less equally between the modern unitary authorities of Argyll and Bute and North Ayrshire with only Ailsa Craig and Lady Isle in South Ayrshire falling outwith these two council areas.

The following table gives a list of the islands of the Firth of Clyde with an area greater than 40 hectares (approximately 100 acres) plus adjacent smaller uninhabited islets, tidal islets only separated at higher stages of the tide, and skerries which are only exposed at lower stages of the tide.

Six islands were inhabited in 2001 including Davaar and Sanda with 2 and 1 residents respectively. By the time of the 2011 census neither had a usually resident population.

Some islets lie remote from the larger islands and are listed separately here by location.

Gare Loch is a small loch which hosts the Faslane Naval Base, the home of the UK's Trident nuclear submarines. At its southern end, the loch opens into the Firth of Clyde, via the Rhu narrows. It contains two islets: Green Island and Perch Rock.

The Kilbrannan Sound, which lies between Arran and the Kintyre peninsula, contains several islets: An Struthlag, Cour Island, Eilean Carrach (Carradale), Eilean Carrach (Skipness), Eilean Grianain, Eilean Sunadale, Gull Isle, Island Ross and Thorn Isle. In the late 11th century Magnus Barefoot, King of Norway, made an arrangement with King Malcolm III of Scotland that he could take possession of land on the west coast around which a ship could sail. He had his longship dragged across the long isthmus in the north of Kintyre between East Loch Tarbert and West Loch Tarbert as part of a campaign to increase his possessions. Magnus declared that Kintyre had "better land than the best of the Hebrides", and by taking command of his ship's tiller and "sailing" across the isthmus he was able to claim the entire peninsula was an island, which remained under Norse rule for more than a dozen years as a result.

Loch Fyne, which extends inland from the Sound of Bute is the longest of Scotland's sea lochs and contains several islets and skerries. These are Duncuan Island, Eilean Ardgaddan, Eilean a' Bhuic, Eilean Aoghainn, Eilean a' Chomhraig, Eilean an Dúnain, Eilean Buidhe (Ardmarnock), Eilean Buidhe (Portavadie), Eilean Fraoch, Eilean Math-ghamhna, Eilean Mór, Glas Eilean, Heather Island, Inverneil Island, Kilbride Island and Liath Eilean.

The North Ayrshire islets of Broad Rock, East Islet, Halftide Rock, High Rock and North Islet are all found surrounding Horse Isle. Lady Isle, which lies off the South Ayrshire coast near Troon once housed "ane old chapell with an excellent spring of water". However, in June 1821 someone set fire to the "turf and pasture", and permanently destroyed the island's grazing, with gales blowing much of the island's soil into the sea.

Neither Loch Goil nor Loch Long, which are fjord-like arms of the firth to the north, contain islands.

The following are places along that shores of the Firth of Clyde that are not islands and have misleading names, "eilean" being Gaelic for "island": Eilean na Beithe, Portavadie; Eilean Beag, Cove; Eilean Dubh, Dalchenna, Loch Fyne; Eilean nan Gabhar, Melldalloch, Kyles of Bute; Barmore Island, just north of Tarbert, Kintyre; Eilean Aoidh, south of Portavadie; Eilean Leathan, Kilbrannan Sound just south of Torrisdale Bay; Island Muller, Kilbrannan Sound north of Campbeltown.

There are populations of red deer, red squirrel, badger, otter, adder and common lizard. Offshore there are harbour porpoises, basking sharks and various species of dolphin. Davaar is home to a population of wild goats.

Over 200 species of bird have been recorded in the area including black guillemot, eider, peregrine falcon and the golden eagle. In 1981 there were 28 ptarmigan on Arran, but in 2009 it was reported that extensive surveys had been unable to record any. Similarly, the red-billed chough no longer breeds on the island.

Arran also has three rare endemic species of tree, the Arran Whitebeams. These are the Scottish or Arran whitebeam, the cut-leaved whitebeam and the Catacol whitebeam, which are amongst the most endangered tree species in the world. They are found in a protected national nature reserve, and are monitored by staff from Scottish Natural Heritage. Only 283 Arran whitebeam and 236 cut-leaved whitebeam were recorded as mature trees in 1980. The Catacol whitebeam was discovered in 2007 and steps have been taken to protect the two known specimens.

The Roman historian Tacitus refers to the "Clota" meaning the Clyde. The derivation is not certain but probably from the Brythonic "Clouta" which became "Clut" in Old Welsh. The name's literal meaning is "wash" but probably refers to the idea of a river goddess being "the washer" or "strongly flowing one". Bute's derivation is also uncertain. "Bót" is the Norse name and this is the Old Irish word for "fire", possibly a reference to signal fires. The etymology of Arran is no more clear—Haswell-Smith (2004) offers a Brythonic derivation and a meaning of "high place" although Watson (1926) suggests it may be pre-Celtic.




</doc>
<doc id="15253" url="https://en.wikipedia.org/wiki?curid=15253" title="International Bank Account Number">
International Bank Account Number

The International Bank Account Number (IBAN) is an internationally agreed system of identifying bank accounts across national borders to facilitate the communication and processing of cross border transactions with a reduced risk of transcription errors. It was originally adopted by the European Committee for Banking Standards (ECBS), and later as an international standard under ISO 13616:1997. The current standard is ISO 13616:2007, which indicates SWIFT as the formal registrar. Initially developed to facilitate payments within the European Union, it has been implemented by most European countries and numerous countries in the other parts of the world, mainly in the Middle East and in the Caribbean. As of February 2016, 69 countries were using the IBAN numbering system.

The IBAN consists of up to 34 alphanumeric characters comprising: a country code; two check digits; and a number that includes the domestic bank account number, branch identifier, and potential routing information. The check digits enable a sanity check of the bank account number to confirm its integrity before submitting a transaction.

Before IBAN, differing national standards for bank account identification (i.e. bank, branch, routing codes, and account number) were confusing for some users. This often led to necessary routing information being missing from payments. Routing information as specified by ISO 9362 (also known as Business Identifier Codes (BIC code), SWIFT ID or SWIFT code, and SWIFT-BIC) does not require a specific format for the transaction so the identification of accounts and transaction types is left to agreements of the transaction partners. It also does not contain check digits, so errors of transcription were not detectable and it was not possible for a sending bank to validate the routing information prior to submitting the payment. Routing errors caused delayed payments and incurred extra costs to the sending and receiving banks and often to intermediate routing banks.

In 1997, to overcome these difficulties, the International Organization for Standardization (ISO) published ISO 13616:1997. This proposal had a degree of flexibility, which the European Committee for Banking Standards (ECBS) believed would make it unworkable, and they produced a "slimmed down" version of the standard which, amongst other things, permitted only upper-case letters and required that the IBAN for each country have a fixed length. ISO 13616:1997 was subsequently withdrawn and replaced by ISO 13616:2003. The standard was revised again in 2007 when it was split into two parts. ISO 13616-1:2007 "specifies the elements of an international bank account number (IBAN) used to facilitate the processing of data internationally in data interchange, in financial environments as well as within and between other industries" but "does not specify internal procedures, file organization techniques, storage media, languages, etc. to be used in its implementation". ISO 13616-2:2007 describes "the Registration Authority (RA) responsible for the registry of IBAN formats that are compliant with ISO 13616-1 [and] the procedures for registering ISO 13616-compliant IBAN formats". The official IBAN registrar under ISO 13616-2:2007 is SWIFT.

IBAN imposes a flexible but regular format sufficient for account identification and contains validation information to avoid errors of transcription. It carries all the routing information needed to get a payment from one bank to another wherever it may be; it contains key bank account details such as country code, branch codes (known as sort codes in the UK and Ireland) and account numbers, and it contains "check digits" which can be validated at source according to a single standard procedure. Where used, IBANs have reduced trans-national money transfer errors to under 0.1% of total payments.

The IBAN consists of up to 34 alphanumeric characters, as follows:


The check digits enable a sanity check of the bank account number to confirm its integrity before submitting a transaction.

The IBAN should not contain spaces when transmitted electronically. When printed it is expressed in groups of four characters separated by a single space, the last group being of variable length as shown in the example below:

Permitted IBAN characters are the digits "0" to "9" and the 26 upper-case Latin alphabetic characters "A" to "Z". This applies even in countries (e.g., Thailand) where these characters are not used in the national language.

The Basic Bank Account Number (BBAN) format is decided by the national central bank or designated payment authority of each country. There is no consistency between the formats adopted. The national authority may register its BBAN format with SWIFT, but is not obliged to do so. It may adopt IBAN without registration. SWIFT also acts as the registration authority for the SWIFT system, which is used by most countries that have not adopted IBAN. A major difference between the two systems is that under SWIFT there is no requirement that BBANs used within a country be of a pre-defined length.

The BBAN must be of a fixed length for the country and comprise case-insensitive alphanumeric characters. It includes the domestic bank account number, branch identifier, and potential routing information. Each country can have a different national routing/account numbering system, up to a maximum of 30 alphanumeric characters.

The check digits enable the sending bank (or its customer) to perform a sanity check of the routing destination and account number from a single string of data at the time of data entry. This check is guaranteed to detect any instances where a single character has been omitted, duplicated, mistyped or where two characters have been transposed. Thus routing and account number errors are virtually eliminated.

One of the design aims of the IBAN was to enable as much validation as possible to be done at the point of data entry. In particular, the computer program that accepts an IBAN will be able to validate:


The check digits are calculated using MOD-97-10 as per ISO/IEC 7064:2003 (abbreviated to "mod-97" in this article), which specifies a set of check character systems capable of protecting strings against errors which occur when people copy or key data. In particular, the standard states that the following can be detected:


The underlying rules for IBANs is that the account-servicing financial institution should issue an IBAN, as there are a number of areas where different IBANs could be generated from the same account and branch numbers that would satisfy the generic IBAN validation rules. In particular cases where 00 is a valid check digit, 97 will not be a valid check digit, likewise, if 01 is a valid check digit, 98 will not be a valid check digit, similarly with 02 and 99.

The UN CEFACT TBG5 has published a free IBAN validation service in 32 languages for all 57 countries that have adopted the IBAN standard. They have also published the Javascript source code of the verification algorithm.

An English language IBAN checker for ECBS member country bank accounts is available on its website.

An IBAN is validated by converting it into an integer and performing a basic "mod-97" operation (as described in ISO 7064) on it. If the IBAN is valid, the remainder equals 1. The algorithm of IBAN validation is as follows:


If the remainder is 1, the check digit test is passed and the IBAN might be valid.

Example (fictitious United Kingdom bank, sort code 12-34-56, account number 98765432):

According to the ECBS "generation of the IBAN shall be the exclusive responsibility of the bank/branch servicing the account". The ECBS document replicates part of the ISO/IEC 7064:2003 standard as a method for generating check digits in the range 02 to 98. Check digits in the ranges 00 to 96, 01 to 97, and 03 to 99 will also provide validation of an IBAN, but the standard is silent as to whether or not these ranges may be used.

The preferred algorithm is:


Any computer programming language or software package that is used to compute "D" mod "97" directly must have the ability to handle integers of more than 30 digits. In practice, this can only be done by software that either supports arbitrary-precision arithmetic or that can handle 220 bit (unsigned) integers, features that are often not standard. If the application software in use does not provide the ability to handle integers of this size, the modulo operation can be performed in a piece-wise manner (as is the case with the UN CEFACT TBG5 Javascript program).

Piece-wise calculation can be done in many ways. One such way is as follows:


The result of the final calculation in step 2 will be "D" mod 97 = "N" mod "97".

In this example, the above algorithm for "D" mod 97 will be applied to "D" = 3214282912345698765432161182. (The digits are colour-coded to aid the description below.) If the result is one, the IBAN corresponding to "D" passes the check digit test.


From step 8, the final result is "D" mod 97 = 1 and the IBAN has passed this check digit test.

International bank transactions use either an IBAN or the ISO 9362 Business Identifier Code system (BIC or SWIFT code) in conjunction with the BBAN (Basic Bank Account Number).

The banks of most countries in Europe publish account numbers using both the IBAN format and the nationally recognised identifiers, this being mandatory within the European Economic Area.

Day-to-day administration of banking in British Overseas Territories varies from territory to territory; some, such as South Georgia and the South Sandwich Islands, have too small a population to warrant a banking system while others, such as Bermuda, have a thriving financial sector. The use of the IBAN is up to the local government—Gibraltar, being part of the European Union is required to use the IBAN, as are the Crown dependencies, which use the British clearing system, and the British Virgin Islands have chosen to do so. , no other British Overseas Territories have chosen to use the IBAN. Banks in the Caribbean Netherlands also do not use the IBAN.

The IBAN designation scheme was chosen as the foundation for electronic straight-through processing in the European Economic Area. The European Parliament mandated that a bank charge needs to be the same amount for domestic credit transfers as for cross-border credit transfers regulated in decision 2560/2001 (updated in 924/2009). This regulation took effect in 2003. Only payments in euro up to €12,500 to a bank account designated by its IBAN were covered by the regulation.

The Euro Payments regulation has been the foundation for the decision to create a Single Euro Payments Area (SEPA). The European Central Bank has created the TARGET2 interbank network that unifies the technical infrastructure of the 26 central banks of the European Union (although Sweden and the UK have opted out). SEPA is a self-regulatory initiative by the banking sector of Europe as represented in the European Payments Council (EPC). The European Union made the scheme mandatory through the Payment Services Directive published in 2007. Since January 2008, all countries must support SEPA credit transfer, and SEPA direct debit must be supported since November 2009. The regulation on SEPA payments increases the charge cap (same price for domestic payments as for cross-border payments) to €50,000.

With a further decision of the European Parliament, the IBAN scheme for bank accounts fully replaced the domestic numbering schemes from 31 December 2012. On 16 December 2010, the European Commission published proposed regulations that will make IBAN support mandatory for domestic credit transfer by 2013 and for domestic direct debit by 2014 (with a 12 and 24 months transition period respectively). Some countries have already replaced their traditional bank account scheme by IBAN. This includes Switzerland where IBAN was introduced for national credit transfer on 1 January 2006 and the support for the old bank account numbers has not been required from 1 January 2010.

Based on a 20 December 2011 memorandum, the EU parliament resolved the mandatory dates for the adoption of the IBAN on 14 February 2012. From 1 February 2014, all national systems for credit transfer and direct debit must be abolished and replaced by an IBAN-based system. This will be extended to all cross-border SEPA transactions from 1 February 2016 (Article 5 Section 7). After these dates the IBAN will be sufficient to identify an account for home and foreign financial transactions in SEPA countries and banks will no longer be permitted to require that the customer supply the BIC for the beneficiary's bank.

In the run-up to the 1 February 2014 deadline, it became apparent that many old bank account numbers had not been allocated IBANs—an issue that has to be addressed on a country-by-country basis. In Germany, for example, Deutsche Bundesbank and the German Banking Industry Committee require that all holders of German bank codes ("Bankleitzahl") publish the specifics of their IBAN generation format taking into account not only the generation of check digits but also the handling of legacy bank codes, thereby enabling third parties to generate IBANs independently of the bank. The first such catalogue was published in June 2013 as a variant of the old bank code catalog ("Bankleitzahlendatei").

Banks in numerous non-European countries including most states of the Middle East, North Africa and the Caribbean have implemented the IBAN format for account identification. In some countries the IBAN is used on an "ad hoc" basis, an example being Ukraine where account numbers used for international transfers of four of the national banks have additional aliases that follow the IBAN format as a precursor to formal SWIFT registration.

The degree to which bank verifies the validity of a recipient's bank account number depends of the configuration of the transmitting bank's software—many major software packages supply bank account validation as a standard function. Some banks outside Europe may not recognize IBAN, though this is expected to diminish with time. Non-European banks usually accept IBANs for accounts in Europe, although they might not treat IBANs differently from other foreign bank account numbers. In particular, they might not check the IBAN's validity prior to sending the transfer.

Banks in the United States do not use IBAN as account numbers for U.S. accounts. Any adoption of the IBAN standard by U.S. banks would likely be initiated by ANSI ASC X9, the U.S. financial services standards development organization: a working group (WGAB20) was established as an X9 subcommittee to generate an IBAN construction for U.S. bank accounts.

Canadian financial institutions have not adopted IBAN and use routing numbers issued by Payments Canada for domestic transfers, and SWIFT for international transfers. There is no formal governmental or private sector regulatory requirement in Canada for the major banks to use IBAN.

Australia and New Zealand do not use IBAN. They use Bank State Branch codes for domestic transfers and SWIFT for international transfers.

This table summarises the IBAN formats by country:


In addition to the above list, Nordea has catalogued IBANs for countries listed below.

In this list
Addition list of countries, in the process of introducing the IBAN retrieved from SWIFT partner website are listed below.

In this list

There is criticism about the length and readability of IBAN. Printed on paper the IBAN is often difficult to read. Therefore, it is popular to group the IBAN with four symbols. However, for electronic documents (e.g. PDF invoice) the copy and paste of grouped IBAN can result in errors with online banking forms. Only a few user friendly bank institutes allow and detect the copy and paste of both grouped and ungrouped IBAN.




</doc>
<doc id="15254" url="https://en.wikipedia.org/wiki?curid=15254" title="Infinitive">
Infinitive

Infinitive (abbreviated ) is a grammatical term referring to certain verb forms existing in many languages, most often used as non-finite verbs. As with many linguistic concepts, there is not a single definition applicable to all languages. The word is derived from Late Latin "[modus] infinitivus", a derivative of "infinitus" meaning "unlimited".

In traditional descriptions of English, the infinitive is the basic dictionary form of a verb when used non-finitely, with or without the particle "to". Thus "to go" is an infinitive, as is "go" in a sentence like "I must go there" (but not in "I go there", where it is a finite verb). The form without "to" is called the bare infinitive, and the form with "to" is called the full infinitive or "to"-infinitive.

In many other languages the infinitive is a single word, often with a characteristic inflective ending, like "morir" ("(to) die") in Spanish, "manger" ("(to) eat") in French, "portare" ("(to) carry") in Latin, "lieben" ("(to) love") in German, etc. However some languages have no forms which can be considered to be infinitives. Many Native American languages and some languages in Africa and Australia do not have direct equivalents to infinitives or verbal nouns; in their place they use finite verb forms in ordinary clauses or various special constructions.

Being a verb, an infinitive may take objects and other complements and modifiers to form a verb phrase (called an infinitive phrase). Like other non-finite verb forms (like participles, converbs, gerunds and gerundives), infinitives do not generally have an expressed subject; thus an infinitive verb phrase also constitutes a complete non-finite clause, called an infinitive (infinitival) clause. Such phrases or clauses may play a variety of roles within sentences, often being nouns (for example being the subject of a sentence or being a complement of another verb), and sometimes being adverbs or other types of modifier. Many verb forms known as infinitives differ from gerunds (verbal nouns) in that they do not inflect for case or occur in adpositional phrases. Instead, infinitives often originate in earlier inflectional forms of verbal nouns. Unlike finite verbs, infinitives are not usually inflected for tense, person, etc. either, although some degree of inflection sometimes occurs; for example Latin has distinct active and passive infinitives.

An "infinitive phrase" is a verb phrase constructed with the verb in infinitive form. This consists of the verb together with its objects and other complements and modifiers. Some examples of infinitive phrases in English are given below – these may be based on either the full infinitive (introduced by the particle "to") or the bare infinitive (without the particle "to").

Infinitive phrases often have an implied grammatical subject making them effectively clauses rather than phrases. Such "infinitive clauses" or "infinitival clauses", are one of several kinds of non-finite clause. They can play various grammatical roles like a constituent of a larger clause or sentence; for example it may form a noun phrase or adverb. Infinitival clauses may be embedded within each other in complex ways, like in the sentence:
Here the infinitival clause "to get married" is contained within the finite dependent clause "that Brett Favre is going to get married"; this in turn is contained within another infinitival clause, which is contained in the finite independent clause (the whole sentence).

The grammatical structure of an infinitival clause may differ from that of a corresponding finite clause. For example, in German, the infinitive form of the verb usually goes to the end of its clause, whereas a finite verb (in an independent clause) typically comes in second position.

Following certain verbs or prepositions, infinitives commonly "do" have an expressed subject, e.g. 
As these examples illustrate, the subject of the infinitive is in the objective case (them, him) in contrast to the nominative case that would be used with a finite verb, e.g. "They ate their dinner." 
Such accusative and infinitive constructions are present in Latin and Ancient Greek, as well as many modern languages. The unusual case for the subject of an infinitive is an example of exceptional case-marking, where the infinitive clause's role being an object of a verb or preposition (want, for) overpowers the pronoun's subjective role within the clause.

In some languages, infinitives may be marked for grammatical categories like voice, aspect, and to some extent tense. This may be done by inflection, like with the Latin perfect and passive infinitives, or by periphrasis (with the use of auxiliary verbs), like with the Latin future infinitives or the English perfect and progressive infinitives.

Latin has present, perfect and future infinitives, with active and passive forms of each. For details see .

English has infinitive constructions which are marked (periphrastically) for aspect: perfect, progressive (continuous), or a combination of the two (perfect progressive). These can also be marked for passive voice (as can the plain infinitive):
Further constructions can be made with other auxiliary-like expressions, like "(to) be going to eat" or "(to) be about to eat", which have future meaning. For more examples of the above types of construction, see .

Perfect infinitives are also found in other European languages which have perfect forms with auxiliaries similarly to English. For example, "avoir mangé" means "(to) have eaten" in French.

Regarding English, the term "infinitive" is traditionally applied to the unmarked form of the verb (the "plain form") when it forms a non-finite verb, whether or not introduced by the particle "to". Hence "sit" and "to sit", as used in the following sentences, would each be considered an infinitive:

The form without "to" is called the "bare infinitive"; the form introduced by "to" is called the "full infinitive" or "to-infinitive".

The other non-finite verb forms in English are the gerund or present participle (the "-ing" form), and the past participle – these are not considered infinitives. Moreover, the unmarked form of the verb is not considered an infinitive when it is forms a finite verb: like a present indicative ("I "sit" every day"), subjunctive ("I suggest that he "sit""), or imperative (""Sit" down!"). (For some irregular verbs the form of the infinitive coincides additionally with that of the past tense and/or past participle, like in the case of "put".)

Certain auxiliary verbs are defective in that they do not have infinitives (or any other non-finite forms). This applies to the modal verbs ("can", "must", etc.), as well as certain related auxiliaries like the "had" of "had better" and the "used" of "used to". (Periphrases can be employed instead in some cases, like "(to) be able to" for "can", and "(to) have to" for "must".) It also applies to the auxiliary "do", like used in questions, negatives and emphasis like described under "do"-support. (Infinitives are negated by simply preceding them with "not". Of course the verb "do" when forming a main verb can appear in the infinitive.) However, the auxiliary verbs "have" (used to form the perfect) and "be" (used to form the passive voice and continuous aspect) both commonly appear in the infinitive: "I should have finished by now"; "It's thought to have been a burial site"; "Let him be released"; "I hope to be working tomorrow."

Huddleston and Pullum's "Cambridge Grammar of the English Language" (2002) does not use the notion of the "infinitive" ("there is no form in the English verb paradigm called 'the infinitive'"), only that of the "infinitival clause", noting that English uses the same form of the verb, the "plain form", in infinitival clauses that it uses in imperative and present-subjunctive clauses.

A matter of controversy among prescriptive grammarians and style writers has been the appropriateness of separating the two words of the "to"-infinitive (as in "I expect "to" happily "sit" here"). For details of this, see split infinitive. Opposing linguistic theories typically do not consider the "to"-infinitive to be a distinct constituent, instead regarding the scope of the particle "to" to cover an entire verb phrase; thus, "to buy a car" is parsed like "<nowiki>to [buy [a car]]</nowiki>", rather not like "<nowiki>[to buy] [a car]</nowiki>".

The bare infinitive and the "to"-infinitive have a variety of uses in English. The two forms are mostly in complementary distribution – certain contexts call for one, and certain contexts for the other; they are not normally interchangeable, except in occasional instances like after the verb "help", where either can be used.

The main uses of infinitives (or infinitive phrases) are like follows:

The infinitive is also the usual dictionary form or citation form of a verb. The form listed in dictionaries is the bare infinitive, although the "to"-infinitive is often used in referring to verbs or in defining other verbs: "The word 'amble' means 'to walk slowly'"; "How do we conjugate the verb "to go"?"

For further detail and examples of the uses of infinitives in English, see Bare infinitive and "To"-infinitive in the article on uses of English verb forms.

The original Proto-Germanic ending of the infinitive was "-an", with verbs derived from other words ending in "-jan" or "-janan".

In German it is "-en" ("sagen"), with "-eln" or "-ern" endings on a few words based on -l or -r roots ("segeln", "ändern"). The use of "zu" with infinitives is similar to English "to", but is less frequent than in English. German infinitives can form nouns, often expressing abstractions of the action, in which case they are of neuter gender: "das Essen" means "the eating", but also "the food".

In Dutch infinitives also end in "-en" ("zeggen" — "to say"), sometimes used with "te" similar to English "to", e.g. "Het is niet moeilijk te begrijpen" → "It is not hard to understand." The few verbs with stems ending in "-a" have infinitives in -n ("gaan" — "to go", "slaan" — "to hit"). Afrikaans has lost the distinction between the infinitive and present forms of verbs, with the exception of the verbs "wees" (to be), which admits the present form "is", and the verb "hê" (to have), whose present form is "het".

In North Germanic languages the final "-n" was lost from the infinitive as early as 500–540 AD, reducing the suffix to "-a". Later it has been further reduced to "-e" in Danish and some Norwegian dialects (including the written majority language bokmål). In the majority of Eastern Norwegian dialects and a few bordering Western Swedish dialects the reduction to "-e" was only partial, leaving some infinitives in "-a" and others in "-e" (å laga vs. å kaste). In northern parts of Norway the infinitive suffix is completely lost (å lag’ vs. å kast’) or only the "-a" is kept (å laga vs. å kast’). The infinitives of these languages are inflected for passive voice through the addition of "-s" or "-st" to the active form. This suffix appearance in Old Norse was a contraction of "mik" (“me”, forming "-mk") or "sik" (reflexive pronoun, forming "-sk") and was originally expressing reflexive actions: (hann) "kallar" (“(he) calls”) + "-sik" (“himself”) > (hann) "kallask" (“(he) calls himself”). The suffixes "-mk" and "-sk" later merged to "-s", which evolved to "-st" in the western dialects. The loss or reduction of "-a" in active voice in Norwegian did not occur in the passive forms ("-ast", "-as"), except for some dialects that have "-es". The other North Germanic languages have the same vowel in both forms.

The formation of the infinitive in the Romance languages reflects that in their ancestor, Latin, almost all verbs had an infinitive ending with "-re" (preceded by one of various thematic vowels). For example, in Italian infinitives end in "-are", "-ere", "-rre" (rare), or "-ire" (which is still identical to the Latin forms), and in "-arsi", "-ersi", "-rsi", "-irsi" for the reflexive forms. In Spanish and Portuguese, infinitives end in "-ar", "-er", or "-ir", while similarly in French they typically end in "-re", "-er", "oir", and "-ir". In Romanian, both short and long-form infinitives exist; the so-called "long infinitives" end in "-are, -ere, -ire" and in modern speech are used exclusively as verbal nouns. Verbs that cannot be converted into the nominal long infinitive are very rare). The "short infinitives" used in verbal contexts (e.g. after an auxiliary verb) have the endings "-a","-ea", "-e", and "-i" (basically removing the ending in "-re"). In Romanian, the infinitive is usually replaced by a clause containing the conjunction "să" plus the subjunctive mood. The only verb that is modal in common modern Romanian is the verb "a putea", to be able to. However, in popular speech the infinitive after "a putea" is also increasingly replaced by the subjunctive.

In all Romance languages, infinitives can also form nouns.

Latin infinitives challenged several of the generalizations about infinitives. They did inflect for voice ("amare", "to love", "amari", to be loved) and for tense ("amare", "to love", "amavisse", "to have loved"), and allowed for an overt expression of the subject ("video Socratem currere", "I see Socrates running"). See .

Romance languages inherited from Latin the possibility of an overt expression of the subject (as in Italian "vedo Socrate correre"). Moreover, the "inflected infinitive" (or "personal infinitive") found in Portuguese and Galician inflects for person and number. These, alongside Sardinian, are the only Indo-European languages that allow infinitives to take person and number endings. This helps to make infinitive clauses very common in these languages; for example, the English finite clause "in order that you/she/we have..." would be translated to Portuguese like "para teres/ela ter/termos..." (Portuguese is a null-subject language). The Portuguese personal infinitive has no proper tenses, only aspects (imperfect and perfect), but tenses can be expressed using periphrastic structures. For instance, "even though you sing/have sung/are going to sing" could be translated to "apesar de cantares/teres cantado/ires cantar".

Other Romance languages (including Spanish, Romanian, Catalan, and some Italian dialects) allow uninflected infinitives to combine with overt nominative subjects. For example, Spanish "al abrir yo los ojos" ("when I opened my eyes") or "sin yo saberlo" ("without my knowing about it").

In Ancient Greek the infinitive has four tenses (present, future, aorist, perfect) and three voices (active, middle, passive). Present and perfect have the same infinitive for both middle and passive, while future and aorist have separate middle and passive forms.

Thematic verbs form present active infinitives by adding to the stem the thematic vowel and the infinitive ending , and contracts to , e.g. . Athematic verbs, and perfect actives and aorist passives, add the suffix instead, e.g. . In the middle and passive, the present middle infinitive ending is , e.g. and most tenses of thematic verbs add an additional between the ending and the stem, e.g. .

The infinitive "per se" does not exist in Modern Greek. To see this, consider the ancient Greek "ἐθέλω γράφειν" “I want to write”. In modern Greek this become "θέλω να γράψω" “I want that I write”. In modern Greek, the infinitive has thus changed form and function and is used mainly in the formation of periphrastic tense forms and not with an article or alone. Instead of the Ancient Greek infinitive system "γράφειν, γράψειν, γράψαι, γεγραφέναι", Modern Greek uses only the form "γράψει", a development of the ancient Greek aorist infinitive "γράψαι". This form is also invariable. The modern Greek infinitive has only two forms according to voice: for example, "γράψει" for the active voice and "γραφ(τ)εί" for the passive voice (coming from the ancient passive aorist infinitive "γραφῆναι").

The infinitive in Russian usually ends in "-t’" (ть) preceded by a thematic vowel, or "-ti" (ти), if not preceded by one; some verbs have a stem ending in a consonant and change the "t" to "č’", like "*mogt’ → moč’" (*могть → мочь) "can". Some other Balto-Slavic languages have the infinitive typically ending in, for example, "-ć" (sometimes "-c") in Polish, "-t’" in Slovak, "-t" (formerly "-ti") in Czech and Latvian (with a handful ending in -s on the latter), "-ty" (-ти) in Ukrainian, -ць ("-ts"') in Belarusian. Lithuanian infinitives end in -"ti", Slovenian end on -"ti" or -"či", and Croatian on -"ti" or -"ći".

Serbian officially retains infinitives -"ti" or -"ći", but is more flexible than the other slavic languages in breaking the infinitive through a clause. The infinitive nevertheless remains the dictionary form. 

Bulgarian and Macedonian have lost the infinitive altogether except in a handful of frozen expressions where it is the same as the 3rd person singular aorist form. Almost all expressions where an infinitive may be used in Bulgarian are listed here; neverthess in all cases a subordinate clause is the more usual form. For that reason, the present first-person singular conjugation is the dictionary form in Bulgarian, while Macedonian uses the third person singular form of the verb in present tense.

Hebrew has "two" infinitives, the infinitive absolute and the infinitive construct. The infinitive construct is used after prepositions and is inflected with pronominal endings to indicate its subject or object: "bikhtōbh hassōphēr" "when the scribe wrote", "ahare lekhtō" "after his going". When the infinitive construct is preceded by ("lə-", "li-", "lā-", "lo-") "to", it has a similar meaning to the English "to"-infinitive, and this is its most frequent use in Modern Hebrew. The infinitive absolute is used for verb focus and emphasis, like in "mōth yāmūth" (literally "a dying he will die"; figuratively, "he shall indeed/surely die"). This usage is commonplace in the Bible, but in Modern Hebrew it is restricted to high-flown literary works.

Note, however, that the "to"-infinitive of Hebrew is not the dictionary form; that is the third person singular perfect form.

The Finnish grammatical tradition includes many non-finite forms that are generally labeled as (numbered) infinitives although many of these are functionally converbs. To form the so-called first infinitive, the strong form of the root (without consonant gradation or epenthetic 'e') is used, and these changes occur:

As such, it is inconvenient for dictionary use, because the imperative would be closer to the root word. Nevertheless, dictionaries use the first infinitive.

There are also four other infinitives, plus a "long" form of the first:
Note that all of these must change to reflect vowel harmony, so the fifth infinitive (with a third-person suffix) of "hypätä" "jump" is "hyppäämäisillään" "he was about to jump", not "*hyppäämaisillaan".

The Seri language of northwestern Mexico has infinitival forms which are used in two constructions (with the verb meaning 'want' and with the verb meaning 'be able'). The infinitive is formed by adding a prefix to the stem: either "iha-" (plus a vowel change of certain vowel-initial stems) if the complement clause is transitive, or "ica-" (and no vowel change) if the complement clause is intransitive. The infinitive shows agreement in number with the controlling subject. Examples are: "icatax ihmiimzo" 'I want to go', where "icatax" is the singular infinitive of the verb 'go' (singular root is "-atax"), and "icalx hamiimcajc" 'we want to go', where "icalx" is the plural infinitive. Examples of the transitive infinitive: "ihaho" 'to see it/him/her/them' (root "-aho"), and "ihacta" 'to look at it/him/her/them' (root "-oocta").

In languages without an infinitive, the infinitive is translated either as a "that"-clause or as a verbal noun. For example, in Literary Arabic the sentence "I want to write a book" is translated as either "urīdu an aktuba kitāban" (lit. "I want that I write a book", with a verb in the subjunctive mood) or "urīdu kitābata kitābin" (lit. "I want the writing of a book", with the "masdar" or verbal noun), and in Levantine Colloquial Arabic "biddi aktub kitāb" (subordinate clause with verb in subjunctive).

Even in languages that have infinitives, similar constructions are sometimes necessary where English would allow the infinitive. For example, in French the sentence "I want you to come" translates to "Je veux que vous veniez" (lit. "I want that you come", with "come" being in the subjunctive mood). However, "I want to come" is simply "Je veux venir", using the infinitive, just as in English. In Russian, sentences such as "I want you to leave" do not use an infinitive. Rather, they use the conjunction чтобы "in order to/so that" with the past tense form (most probably remnant of subjunctive) of the verb: "Я хочу, чтобы вы ушли" (literally, "I want so that you left").



</doc>
<doc id="15256" url="https://en.wikipedia.org/wiki?curid=15256" title="Immaculate Conception">
Immaculate Conception

The Immaculate Conception is the conception of the Blessed Virgin Mary free from original sin by virtue of the merits of her son Jesus Christ. The Catholic Church teaches that God acted upon Mary in the first moment of her conception keeping her "immaculate".

The Immaculate Conception is commonly confused with the Virgin Birth of Jesus. Jesus' birth is covered by the Doctrine of Incarnation, while the Immaculate Conception deals with the conception of Mary, not that of her son.

Although the belief that Mary was sinless, or conceived without original sin, has been widely held since Late Antiquity, the doctrine was not dogmatically defined until 1854, by Pope Pius IX in his papal bull "Ineffabilis Deus". The Catholic Church celebrates the Feast of the Immaculate Conception on December 8; in many Catholic countries, it is a holy day of obligation or patronal feast, and in some a national public holiday.

The defined dogma of the Immaculate Conception states that

The definition concerns original sin only, and it makes no declaration about the Church's belief that the Blessed Virgin was sinless in the sense of freedom from actual or personal sin. 
The doctrine teaches that from her conception Mary, being always free from original sin, received the sanctifying grace that would normally come with baptism after birth.

The Encyclical "Mystici Corporis" from Pope Pius XII (1943) in addition holds that Mary was also sinless personally, "free from all sin, original or personal". 
In this, Pius XII repeats a position already expressed by the Council of Trent, which decreed "If anyone shall say that a man once justified can sin no more, nor lose grace, and that therefore he who falls and sins was never truly justified; or, on the contrary, that throughout his whole life he can avoid all sins even venial sins, except by a special privilege of God, as the Church holds in regard to the Blessed Virgin: let him be anathema."
When defining the dogma in "Ineffabilis Deus", Pope Pius IX explicitly affirmed that Mary was redeemed in a manner more sublime. He stated that Mary, rather than being cleansed after sin, was completely prevented from contracting Original Sin in view of the foreseen merits of Jesus Christ, the Savior of the human race. In , Mary proclaims: "My spirit has rejoiced in God my Saviour." This is referred to as Mary's pre-redemption by Christ. Since the Second Council of Orange against semi-pelagianism, the Catholic Church has taught that even had man never sinned in the Garden of Eden and was sinless, he would still require God's grace to remain sinless.

The doctrine of the immaculate conception (Mary being conceived free from original sin) is not to be confused with the virginal conception of her son Jesus. This misunderstanding of the term "immaculate conception" is frequently met in the mass media. 
Catholics believe that Mary was conceived of both parents traditionally known by the names of Saint Joachim and Saint Anne. In 1677, the Holy See condemned the error of Imperiali who taught that St. Anne in the conception and birth of Mary remained virgin which had been a belief surfacing occasionally since the 4th century. The Church celebrates the Feast of the Immaculate Conception (when Mary was conceived free from original sin) on 8 December, exactly nine months before celebrating the Nativity of Mary. The feast of the Annunciation (which commemorates the virginal conception and the Incarnation of Jesus) is celebrated on 25 March, nine months before Christmas Day.

A feast of the Conception of the Most Holy and All Pure Mother of God was celebrated in Syria on 8 December perhaps as early as the 5th century. Note that the title of "achrantos" (spotless, immaculate, all-pure) refers to the holiness of Mary, not specifically to the holiness of her conception.
Mary's complete sinlessness and concomitant exemption from any taint from the first moment of her existence was a doctrine familiar to Greek theologians of Byzantium. Beginning with St. Gregory Nazianzen, his explanation of the "purification" of Jesus and Mary at the circumcision (Luke 2:22) prompted him to consider the primary meaning of "purification" in Christology (and by extension in Mariology) to refer to a perfectly sinless nature that manifested itself in glory in a moment of grace (e.g., Jesus at his Baptism). St. Gregory Nazianzen designated Mary as "prokathartheisa (prepurified)." Gregory likely attempted to solve the riddle of the Purification of Jesus and Mary in the Temple through considering the human natures of Jesus and Mary as equally holy and therefore both purified in this manner of grace and glory. Gregory's doctrines surrounding Mary's purification were likely related to the burgeoning commemoration of the Mother of God in and around Constantinople very close to the date of Christmas. Nazianzen's title of Mary at the Annunciation as "prepurified" was subsequently adopted by all theologians interested in his Mariology to justify the Byzantine equivalent of the Immaculate Conception. This is especially apparent in the Fathers St. Sophronios of Jerusalem and St. John Damascene, who will be treated below in this article at the section on Church Fathers. About the time of Damascene, the public celebration of the "Conception of St. Ann [i.e., of the Theotokos in her womb]" was becoming popular. After this period, the "purification" of the perfect natures of Jesus and Mary would not only mean moments of grace and glory at the Incarnation and Baptism and other public Byzantine liturgical feasts, but purification was eventually associated with the feast of Mary's very conception (along with her Presentation in the Temple as a toddler) by Orthodox authors of the 2nd millennium (e.g., St. Nicholas Cabasilas and Joseph Bryennius).

It is admitted that the doctrine as defined by Pius IX was not explicitly mooted before the 12th century. It is also agreed that "no direct or categorical and stringent proof of the dogma can be brought forward from Scripture". But it is claimed that the doctrine is implicitly contained in the teaching of the Fathers. Their expressions on the subject of the sinlessness of Mary are, it is pointed out, so ample and so absolute that they must be taken to include original sin as well as actual. Thus in the first five centuries such epithets as "in every respect holy", "in all things unstained", "super-innocent", and "singularly holy" are applied to her; she is compared to Eve before the fall, as ancestress of a redeemed people; she is "the earth before it was accursed". The well-known words of St. Augustine (d. 430) may be cited: "As regards the mother of God," he says, "I will not allow any question whatever of sin." It is true that he is here speaking directly of actual or personal sin. But his argument is that all men are sinners; that they are so through original depravity; that this original depravity may be overcome by the grace of God, and he adds that he does not know but that Mary may have had sufficient grace to overcome sin "of every sort" ("omni ex parte").

Although the doctrine of Mary's Immaculate Conception appears only later among Latin (and particularly Frankish) theologians, it became ever more manifest among Byzantine theologians reliant on Gregory Nazianzen's Mariology in the Medieval or Byzantine East. Although hymnographers and scholars, like the Emperor Justinian I, were accustomed to call Mary "prepurified" in their poetic and credal statements, the first point of departure for more fully commenting on Nazianzen's meaning occurs in Sophronius of Jerusalem. In other places Sophronius explains that the Theotokos was already immaculate, when she was "purified" at the Annunciation and goes so far as to note that John the Baptist is literally "holier than all 'Men' born of woman" since Mary's surpassing holiness signifies that she was holier than even John after his sanctification in utero. Sophronius' teaching is augmented and incorporated by St. John Damascene (d. 749/750). John, besides many passages wherein he extolls the Theotokos for her purification at the Annunciation, grants her the unique honor of "purifying the waters of baptism by touching them." This honor was most famously and firstly attributed to Christ, especially in the legacy of Nazianzen. As such, Nazianzen's assertion of parallel holiness between the prepurified Mary and purified Jesus of the New Testament is made even more explicit in Damascene in his discourse on Mary's holiness to also imitate Christ's baptism at the Jordan. The Damascene's hymnongraphy and De fide Orthodoxa explicitly use Mary's "pre purification" as a key to understanding her absolute holiness and unsullied human nature. In fact, Damascene (along with Nazianzen) serves as the source for nearly all subsequent promotion of Mary's complete holiness from her Conception by the "all pure seed" of Joachim and the womb "wider than heaven" of St. Ann.

Bernard of Clairvaux in the 12th century raised the question of the Immaculate Conception. A feast of the Conception of the Blessed Virgin had already begun to be celebrated in some churches of the West. St Bernard blames the canons of the metropolitan church of Lyon for instituting such a festival without the permission of the Holy See. In doing so, he takes occasion to repudiate altogether the view that the conception of Mary was sinless. It is doubtful, however, whether he was using the term "conception" in the same sense in which it is used in the definition of Pope Pius IX. Bernard would seem to have been speaking of conception in the active sense of the mother's cooperation, for in his argument he says: "How can there be absence of sin where there is concupiscence ("libido")?" and stronger expressions follow, showing that he is speaking of the mother and not of the child.

Saint Thomas Aquinas refused to concede the Immaculate Conception, on the ground that, unless the Blessed Virgin had at one time or other been one of the sinful, she could not justly be said to have been redeemed by Christ.

Saint Bonaventure (d. 1274), second only to Saint Thomas in his influence on the Christian schools of his age, hesitated to accept it for a similar reason. He believed that Mary was completely free from sin, but that she was not given this grace at the instant of her conception.

The celebrated John Duns Scotus (d. 1308), a Friar Minor like Saint Bonaventure, argued, on the contrary, that from a rational point of view it was certainly as little derogatory to the merits of Christ to assert that Mary was by him preserved from all taint of sin, as to say that she first contracted it and then was delivered. Proposing a solution to the theological problem of reconciling the doctrine with that of universal redemption in Christ, he argued that Mary's immaculate conception did not remove her from redemption by Christ; rather it was the result of a more perfect redemption granted her because of her special role in salvation history.

The arguments of Scotus, combined with a better acquaintance with the language of the early Fathers, gradually prevailed in the schools of the Western Church. In 1387 the university of Paris strongly condemned the opposite view.

Scotus's arguments remained controversial, however, particularly among the Dominicans, who were willing enough to celebrate Mary's "sanctificatio" (being made free from sin) but, following the Dominican Thomas Aquinas' arguments, continued to insist that her sanctification could not have occurred until after her conception.

Popular opinion remained firmly behind the celebration of Mary's conception. In 1439, the Council of Basel, which is not reckoned an ecumenical council, stated that belief in the immaculate conception of Mary is in accord with the Catholic faith. By the end of the 15th century the belief was widely professed and taught in many theological faculties, but such was the influence of the Dominicans, and the weight of the arguments of Thomas Aquinas (who had been canonised in 1323 and declared "Doctor Angelicus" of the Church in 1567) that the Council of Trent (1545–63)—which might have been expected to affirm the doctrine—instead declined to take a position.

The papal bull defining the dogma, "Ineffabilis Deus" (1854), mentioned in particular the patrististic interpretation of as referring to a woman, Mary, who would be eternally at enmity with the evil serpent and completely triumphing over him. It said the Fathers saw foreshadowings of Mary's "wondrous abundance of divine gifts and original innocence" "in that ark of Noah, which was built by divine command and escaped entirely safe and sound from the common shipwreck of the whole world; in the ladder which Jacob saw reaching from the earth to heaven, by whose rungs the angels of God ascended and descended, and on whose top the Lord himself leaned; in that bush which Moses saw in the holy place burning on all sides, which was not consumed or injured in any way but grew green and blossomed beautifully; in that impregnable tower before the enemy, from which hung a thousand bucklers and all the armor of the strong; in that garden enclosed on all sides, which cannot be violated or corrupted by any deceitful plots; in that resplendent city of God, which has its foundations on the holy mountains; in that most august temple of God, which, radiant with divine splendours, is full of the glory of God; and in very many other biblical types of this kind."

The bull recounts that the Fathers interpreted the angel's address to Mary, "highly favoured one" or "full of grace", as indicating that "she was never subject to the curse and was, together with her Son, the only partaker of perpetual benediction"; they "frequently compare her to Eve while yet a virgin, while yet innocence, while yet incorrupt, while not yet deceived by the deadly snares of the most treacherous serpent".

The theological underpinnings of Immaculate Conception had been the subject of debate during the Middle Ages with opposition provided by figures such as Saint Thomas Aquinas, a Dominican. However, supportive arguments by Franciscans William of Ware and Duns Scotus, and general belief among Catholics made the doctrine more acceptable, so that the Council of Basel supported it in the 15th century, but the Council of Trent sidestepped the question. Pope Sixtus IV, a Franciscan, had tried to pacify the situation by forbidding either side from criticizing each other, and placed the feast of the Immaculate Conception on the Roman Calendar in 1477, but Pope Pius V, a Dominican, changed it to the feast of the Conception of Mary. Clement XI made the feast universal in 1708, but still did not call it the feast of the Immaculate Conception. Popular and theological support for the concept continued to grow and by the 18th century it was widely depicted in art.

During the reign of Pope Gregory XVI the bishops in various countries began to press for a definition as dogma of the teaching of Mary's immaculate conception.

In 1839 Mariano Spada (1796 - 1872), professor of theology at the Roman College of Saint Thomas, published "Esame Critico sulla dottrina dell’ Angelico Dottore S. Tommaso di Aquino circa il Peccato originale, relativamente alla Beatissima Vergine Maria" [A critical examination of the doctrine of St. Thomas Aquinas, the Angelic Doctor, regarding original sin with respect to the Most Blessed Virgin Mary], in which Aquinas is interpreted not as treating the question of the Immaculate Conception later formulated in the papal bull "Ineffabilis Deus" but rather the sanctification of the fetus within Mary's womb. Spada furnished an interpretation whereby Pius IX was relieved of the problem of seeming to foster a doctrine not in agreement with the Aquinas' teaching. Pope Pius IX would later appoint Spada Master of the Sacred Palace in 1867.

Pius IX, at the beginning of his pontificate, and again after 1851, appointed commissions to investigate the whole subject, and he was advised that the doctrine was one which could be defined and that the time for a definition was opportune.

It was not until 1854 that Pope Pius IX, with the support of the overwhelming majority of Roman Catholic bishops, whom he had consulted between 1851–1853, promulgated the papal bull "Ineffabilis Deus" (Latin for "Ineffable God"), which defined "ex cathedra" the dogma of the Immaculate Conception:

The dogma was defined in accordance with the conditions of papal infallibility, which would be defined in 1870 by the First Vatican Council.

The papal definition of the dogma declares with absolute certainty and authority that Mary possessed sanctifying grace from the first instant of her existence and was free from the lack of grace caused by the original sin at the beginning of human history. Mary's salvation was won by her son Jesus Christ through his passion, death, and resurrection and was not due to her own merits.

George Sale in 1734 proposed that the doctrine of immaculate conception of Mary may be alluded to in the text of the Qur'an. 
Thus, commenting in 1734 on the passage "I have called her Mary; and I commend her to thy protection, and also her issue, against Satan driven away with stones", Sale stated: "It is not improbable that the pretended immaculate conception of the virgin Mary is intimated in this passage. For according to a tradition of Mohammed, every person that comes into the world, is touched at his birth by the devil, and therefore cries out, Mary and her son only excepted; between whom, and the evil spirit God placed a veil, so that his touch did not reach them. And for this reason they say, neither of them were guilty of any sin, like the rest of the children of Adam."
Edward Gibbon in volume 5 of his "Decline and Fall of the Roman Empire", published in 1788, wrote: "The Latin Church has not disdained to borrow from the Koran the immaculate conception of his virgin mother." That he was speaking of her immaculate conception by her mother, not of her own virginal conception of Jesus, is shown by his footnote: "In the xiith century the immaculate conception was condemned by St. Bernard as a presumptuous novelty." 
In the aftermath of the definition of the dogma in 1854, this charge was repeated in an article published in 1865: "Strange as it may appear, that the doctrine which the church of Rome has promulgated, with so much pomp and ceremony, 'for the destruction of all heresies, and the confirmation of the faith of her adherents', should have its origin in the Mohametan Bible; yet the testimony of such authorities as Gibbon, and Sale, and Forster, and Gagnier, and Maracci, leave no doubt as to the marvellous fact."

Without making Islamic belief the origin of the doctrine defined in 1854, a similarity between the two has been noted also by Roman Catholic writers such as Thomas Patrick Hughes, William Bernard Ullathorne, Giancarlo Finazzo.

"The English Commentary of the Holy Quran" argues that this interpretation is misleading, as Islam does not embrace the concept of original sin to begin with, so that Mary could not have been exempt from it. 

Moreover, Hannah's prayer in the Quran for her child to remain protected from Satan (Shayṭān) was said "after" it had already been born, not before and expresses a natural concern any righteous parent would have. 

A "hadith" nevertheless states that the only children born without the "touch of Satan" were Mary and Jesus- 
The specific mention of Mary and Jesus in this hadith has been argued as taken to "represent a class of people", in keeping with the Arabic language and the Quranic verse "[O Satan,] surely thou shalt have no power over My servants, except such of the erring ones as choose to follow thee" (15:42).

For the Roman Catholic Church the dogma of the Immaculate Conception gained additional significance from the reputed apparitions of Our Lady of Lourdes in 1858. At Lourdes a 14-year-old girl, Bernadette Soubirous, claimed that a beautiful woman appeared to her and said, "I am the Immaculate Conception". Many believe the woman to have been the Blessed Virgin Mary and pray to her as such.

Pope Pius IX defined the dogma of the Immaculate Conception "not so much because of proofs in Scripture or ancient tradition, but due to a profound "sensus fidelium" and the Magisterium".

Speaking of the witness of the Church Fathers in claiming for Mary titles such as "Free from all contagion of sin", Pope Pius XII wrote:
The Roman Catholic tradition has a well-established philosophy for the study of the Immaculate Conception and the veneration of the Blessed Virgin Mary in the field of Mariology, with Pontifical schools such as the Marianum specifically devoted to this.

According to Bernard Ullathorne, a 19th-century English Roman Catholic prelate, "the expressions - The Immaculate Conception - The Immaculate Preservation - The Immunity - and Exception from original sin, are all phrases which bear the same signification, and are used equally to express one and the same mystery."

A number of countries are considered to be under the patronage of the Immaculate Conception by pontifical decree.

These include Argentina, Brazil, Korea, Nicaragua, Paraguay, Philippines, Spain (old kingdoms and the present state), the United States and Uruguay.

By royal decree under the House of Braganza, it is the principal Patroness of Portugal.

By 750, the feast of her conception (December 8th) was widely celebrated in the Byzantine East, under the name of the Conception (active) of Saint Anne. In the West it was known as the feast of the Conception (passive) of Mary, and was associated particularly with the Normans, whether these introduced it directly from the East or took it from English usage. The spread of the feast, by now with the adjective "Immaculate" attached to its title, met opposition on the part of some, on the grounds that sanctification was possible only after conception. Critics included Saints Bernard of Clairvaux, Albertus Magnus and Thomas Aquinas. Other theologians defended the expression "Immaculate Conception", pointing out that sanctification could be conferred at the first moment of conception in view of the foreseen merits of Christ, a view held especially by Franciscans.

William of Ware and Blessed John Duns Scotus pointed out that Mary’s Immaculate Conception enhances Jesus’ redemptive work. One of the chief proponents of the doctrine was the Hungarian Franciscan Pelbartus Ladislaus of Temesvár.
On 28 February 1476, Pope Sixtus IV, authorized those dioceses that wished to introduce the feast to do so, and introduced it to his own diocese of Rome in 1477, with a specially composed Mass and Office of the feast. With his bull "Cum praeexcelsa" of 28 February 1477, in which he referred to the feast as that of the Conception of Mary, without using the word "Immaculate", he granted indulgences to those who would participate in the specially composed Mass or Office on the feast itself or during its octave, and he used the word "immaculate" of Mary, but applied instead the adjective "miraculous" to her conception. On 4 September 1483, referring to the feast as that of "the Conception of Immaculate Mary ever Virgin", he condemned both those who called it mortally sinful and heretical to hold that the "glorious and immaculate mother of God was conceived "without" the stain of original sin" and those who called it mortally sinful and heretical to hold that "the glorious Virgin Mary was conceived "with" original sin", since, he said, "up to this time there has been no decision made by the Roman Church and the Apostolic See." This decree was reaffirmed by the Council of Trent.

Pope Pius V, while including the feast in the Tridentine Calendar, removed the adjective "Immaculate" and suppressed the existing special Mass for the feast, directing that the Mass for the Nativity of Mary (with the word "Nativity" replaced by "Conception") be used instead. Part of that earlier Mass was revived in the Mass that Pope Pius IX ordered to be used on the feast and that is still in use.

On 6 December 1708, Pope Clement XI made the feast of the Conception of Mary, at that time still with the Nativity of Mary formula for the Mass, a Holy Day of Obligation. Until Pope Pius X reduced in 1911 the number of Holy Days of Obligation to 8, there were in the course of the year 36 such days, apart from Sundays. Writers such as Sarah Jane Boss interpret the existence of the feast as a strong indication of the Church's traditional belief in the Immaculate Conception.
For differing reasons, belief in Mary's immaculate conception in the Catholic doctrinal form is not part of the official doctrines of the Eastern Orthodox, Oriental Orthodox, Anglican and Protestant churches.

Contemporary Eastern Orthodox Christians often object to the dogmatic declaration of her immaculate conception as an "over-elaboration" of the faith and because they see it as too closely connected with a particular interpretation of the doctrine of ancestral sin. All the same, the historical and authentic tradition of Mariology in Byzantium took its historical point of departure from Sophronios, Damascene, and their imitators. The most famous Eastern Orthodox theologian to imply Mary's Immaculate Conception was St. Gregory Palamas. Though many passages from his works were long known to extol and attribute to Mary a Christlike holiness in her human nature, traditional objections to Palamas' disposition toward the Immaculate Conception typically rely on a poor understanding of his doctrine of "the purification of Mary" at the Annunciation. Not only did he explicitly cite St. Gregory Nazianzen for his understanding of Jesus' purification at His baptism and Mary's at the Annunciation, but Theophanes of Nicaea, Joseph Bryennius, and Gennadios Scholarios all explicitly placed Mary's Conception as the first moment of her all-immaculate participation in the divine energies to such a degree that she was always completely without spot and graced. In addition to Emperor Manuel II and Gennadius Scholarius, St. Mark of Ephesus also fervently defended Mary's title as "prepurified" against the Dominican, Manuel Calecas, who was perhaps promoting thomistic Mariology that denied Mary's all-holiness from the first moment of her existence.

In the tradition of Ethiopian Orthodoxy, the Kebra Nagast says:
While Old Catholics do not reject the Immaculate Conception of Mary, and some of their parishes venerate Mary as immaculately conceived and celebrate the feast of her Immaculate Conception, they do not accept its definition as a dogma, since they reject papal infallibility and with it the Pope's authority to define dogma.

Martin Luther, who initiated the Protestant Reformation, said: "Mother Mary, like us, was born in sin of sinful parents, but the Holy Spirit covered her, sanctified and purified her so that this child was born of flesh and blood, but not with sinful flesh and blood. The Holy Spirit permitted the Virgin Mary to remain a true, natural human being of flesh and blood, just as we. However, he warded off sin from her flesh and blood so that she became the mother of a pure child, not poisoned by sin as we are. For in that moment when she conceived, she was a holy mother filled with the Holy Spirit and her fruit is a holy pure fruit, at once God and truly man, in one person." Some Lutherans, such as the members of the Anglo-Lutheran Catholic Church, support the doctrine.

Most Protestants reject the doctrine because they do not consider the development of dogmatic theology to be authoritative apart from biblical exegesis, and because the doctrine of the Immaculate Conception is not taught in the Bible. The formal pronouncement of Mary's Immaculate Conception by the Catholic Church in 1854 further alienated some Protestant churches largely due to its implication that not all have sinned.

Belief in Mary's immaculate conception is not a doctrine within Anglicanism, although it is shared by many Anglo-Catholics. In the Church of England's "Common Worship" prayer book, 8 December is designated a Lesser Festival of the "Conception of the Blessed Virgin Mary" (without the adjective "immaculate").

The report "Mary: Faith and Hope in Christ", by the Anglican-Roman Catholic International Commission, concluded that the teaching about Mary in the two definitions of the Assumption and the Immaculate Conception can be said to be consonant with the teaching of the Scriptures and the ancient common traditions. But the report expressed concerns that the Roman Catholic dogmatic definitions of these concepts implies them to be "revealed by God", stating: "The question arises for Anglicans, however, as to whether these doctrines concerning Mary are revealed by God in a way which must be held by believers as a matter of faith."

Other than Anglo-Catholics, most Anglicans reject the doctrine that Mary was sinless and conceived without original sin, often citing that it is not within the Holy Scripture and is against the redemptive role and purpose of Jesus Christ merited for all human beings.

The Roman Missal and the Roman Rite Liturgy of the Hours naturally includes references to Mary's immaculate conception in the feast of the Immaculate Conception. An example is the antiphon that begins: "Tota pulchra es, Maria, et macula originalis non est in te" ("You are all beautiful, Mary, and the original stain [of sin] is not in you." It continues: "Your clothing is white as snow, and your face is like the sun. You are all beautiful, Mary, and the original stain [of sin] is not in you. You are the glory of Jerusalem, you are the joy of Israel, you give honour to our people. You are all beautiful, Mary.") On the basis of the original Gregorian chant music, polyphonic settings have been composed by Anton Bruckner, Pablo Casals, Maurice Duruflé, Grzegorz Gerwazy Gorczycki, , José Maurício Nunes Garcia, and ,

Other prayers honouring Mary's immaculate conception are in use outside the formal liturgy. The hymn "Immaculate Mary", addressed to Mary as the Immaculately Conceived One, is closely associated with Lourdes. The Immaculata prayer, composed by Saint Maximillian Kolbe, is a prayer of entrustment to Mary as the Immaculata. A novena of prayers, with a specific prayer for each of the nine days has been composed under the title of the Immaculate Conception Novena.

The 1476 extension of the feast of the Immaculate Conception to the entire Latin Church reduced the likelihood of controversy for the artist or patron in depicting an image, so that emblems depicting "The Immaculate Conception" began to appear.

Many artists in the 15th century faced the problem of how to depict an abstract idea such as the Immaculate Conception, and the problem was not fully solved for 150 years. The Italian Renaissance artist Piero di Cosimo was among those artists who tried new solutions, but none of these became generally adopted so that the subject matter would be immediately recognisable to the faithful.

The definitive iconography for the Immaculate Conception, drawing on the emblem tradition, seems to have been finally established by the master and then father-in-law of Diego Velázquez, the painter and theorist Francisco Pacheco. Pacheco's iconography influenced other Spanish artists or artists active in Spain such as El Greco, Bartolomé Murillo, Diego Velázquez, and Francisco Zurbarán, who each produced a number of artistic masterpieces based on the use of these same symbols.

The popularity of this particular representation of "The Immaculate Conception" spread across the rest of Europe, and has since remained the best known artistic depiction of the concept: in a heavenly realm, moments after her creation, the spirit of Mary (in the form of a young woman) looks up in awe at (or bows her head to) God. The moon is under her feet and a halo of twelve stars surround her head, possibly a reference to "a woman clothed with the sun" from Revelation 12:1–2. Additional imagery may include clouds, a golden light, and putti. In some paintings the putti are holding lilies and roses, flowers often associated with Mary.





</doc>
<doc id="15260" url="https://en.wikipedia.org/wiki?curid=15260" title="Islands of the North Atlantic">
Islands of the North Atlantic

IONA (Islands of the North Atlantic) is an acronym suggested in 1980 by Sir John Biggs-Davison to refer to a loose linkage of England, Wales, Scotland, Ireland, the Isle of Man and Channel Islands, similar to the present day British-Irish Council. Its intended purpose was as a more politically acceptable alternative to British Isles, which is disliked by many people in Ireland.

The neologism has been criticised on the grounds that it excludes most of the islands in the North Atlantic, and also that the only island referred to by the term that is actually in the North Atlantic Ocean is Ireland. Great Britain is in fact in between the Irish Sea and The North Sea. It has been used particularly in the context of the Northern Irish peace process during the negotiation of the Good Friday Agreement, as a neutral name for the proposed council.

One feature of this name is that IONA has the same spelling as the island of Iona which is off the coast of Scotland but with which Irish people have strong cultural associations. It is therefore a name with which people of both main islands might identify. Taoiseach Bertie Ahern noted the symbolism in a 2006 address in Edinburgh:[The Island of] Iona is a powerful symbol of relationships between these islands, with its ethos of service not dominion. Iona also radiated out towards the Europe of the Dark Ages, not to mention Pagan England at Lindisfarne. The British-Irish Council is the expression of a relationship that at the origin of the Anglo-Irish process in 1981 was sometimes given the name Iona, islands of the North Atlantic, and sometimes Council of the Isles, with its evocation of the Lords of the Isles of the 14th and 15th centuries who spanned the North Channel. In the 17th century, Highland warriors and persecuted Presbyterian Ministers criss-crossed the North Channel.
In a Dáil Éireann debate, Proinsias De Rossa was less enthusiastic: The acronym IONA is a useful way of addressing the coming together of these two islands. However, the island of Iona is probably a green heaven in that nobody lives on it and therefore it cannot be polluted in any way.

Outside the Northern Ireland peace process the term IONA is used by the World Universities Debating Championship and in inter-varsity debating competitions throughout Britain and Ireland. In this context IONA is one of the regions which appoint a representative onto the committee of the World Universities Debating Council. Greenland, the Faroe Islands and Iceland would be included in the definition of IONA used in this context, while Newfoundland and Prince Edward Island would be in North America. However, none of these islands have yet participated in the World Universities Debating Championships. Otherwise, the term has achieved very little popular usage in any context.




</doc>
<doc id="15261" url="https://en.wikipedia.org/wiki?curid=15261" title="Intel DX4">
Intel DX4

The IntelDX4 is a clock-tripled i486 microprocessor with 16 KB L1 cache. Intel named it DX4 (rather than "DX3") as a consequence of litigation with AMD over trademarks. The product was officially named the IntelDX4, but OEMs continued using the i486 naming convention.

Intel produced IntelDX4s with two clock speed steppings: A 75 MHz version (3× 25 MHz multiplier), and a 100 MHz version (usually 3× 33.3 MHz, but sometimes also 2× 50 MHz). Both chips were released in March 1994. A version of the IntelDX4 featuring write-back cache was released in October 1994. The original write-through versions of the chip are marked with a laser embossed "&E", while the write-back enabled versions are marked "&EW". i486 OverDrive editions of the IntelDX4 had locked multipliers, and therefore can only run at 3× the external clock-speed. The 100 MHz model of the processor had an iCOMP rating of 435, while the 75 MHz processor had a rating of 319. The IntelDX4 was an OEM-only product, but the DX4 Overdrive could be purchased at a retail store.

The IntelDX4 microprocessor is mostly pin-compatible with the 80486, but requires a lower 3.3 V supply. Normal 80486 and DX2 processors use a 5 V supply; plugging a DX4 into an unmodified socket will destroy it. Motherboards lacking support for the 3.3 V CPUs can sometimes make use of them using a voltage regulator (VRM) that fits between the socket and the CPU.


</doc>
<doc id="15264" url="https://en.wikipedia.org/wiki?curid=15264" title="Iapetus (disambiguation)">
Iapetus (disambiguation)

Iapetus is a Titan in Greek mythology.
Iapetus may also refer to:


</doc>
<doc id="15266" url="https://en.wikipedia.org/wiki?curid=15266" title="Interactive Fiction Competition">
Interactive Fiction Competition

The Interactive Fiction Competition (also known as IFComp) is one of several annual competitions for works of interactive fiction. It has been held since 1995. It is intended for fairly short games, as judges are only allowed to spend two hours playing a game before deciding how many points to award it. The competition has been described as the "Super Bowl" of interactive fiction.

The competition is organized by "Stephen Granade". Although the first competition had separate sections for Inform and TADS games, subsequent competitions have not been divided into sections and are open to games produced by any method, provided that the software used to play the game is freely available. Anyone can judge the games, and anyone can donate a prize. Almost always, there are enough prizes donated that anyone who enters will get one. Entries are required to be released as freeware or public domain, reflecting the general non-profit ethos of the IF community.

In addition to the main competition, the entries take part in the Miss Congeniality contest, where the participating authors vote for three games (not including their own). This was started in 1998 to distribute that year's surplus prizes; this additional contest has remained unchanged since then, even without the original reason for its existence.

In 2016, operation of the competition was taken over by the Interactive Fiction Technology Foundation.

The competition differs from the XYZZY Awards, as authors must specifically submit games to the Interactive Fiction Competition, but all games released in the past year are eligible for the XYZZY Awards. Many games win awards in both competitions.

The following is a list of winners to date:

A reviewer for "The A.V. Club" said of the 2008 competition, "Once again, the IF Competition delivers some of the best writing in games." The 2008 competition was described as containing "some real standouts both in quality of puzzles and a willingness to stretch the definition of text adventures/interactive fiction."




</doc>
<doc id="15267" url="https://en.wikipedia.org/wiki?curid=15267" title="Immunity">
Immunity

Immunity may refer to:





</doc>
<doc id="15268" url="https://en.wikipedia.org/wiki?curid=15268" title="Inquests in England and Wales">
Inquests in England and Wales

Inquests in England and Wales are held into sudden and unexplained deaths and also into the circumstances of discovery of a certain class of valuable artefacts known as "treasure trove". In England and Wales inquests are the responsibility of a coroner, who operates under the jurisdiction of the Coroners and Justice Act 2009.

There is a general duty upon every person to report a death to the coroner if an inquest is likely to be required. However, this duty is largely unenforceable in practice and the duty falls on the responsible registrar. The registrar must report a death where:

The coroner must hold an inquest where the death was:

Where the cause of death is unknown, the coroner may order a post mortem examination in order to determine whether the death was violent. If the death is found to be non-violent, an inquest is unnecessary.

In 2004 in England and Wales, there were 514,000 deaths of which 225,500 were referred to the coroner. Of those, 115,800 resulted in post-mortem examinations and there were 28,300 inquests, 570 with a jury. In 2014 the Royal College of Pathologists claimed that up to 10,000 deaths a year recorded as being from natural causes should have been investigated by inquests. They were particularly concerned about people whose death occurred as a result of medical errors. "We believe a medical examiner would have been alerted to what was going on in Mid-Staffordshire long before this long list of avoidable deaths reached the total it did," said Archie Prentice, the pathologists' president.

A coroner must summon a jury for an inquest if the death was not a result of natural causes and occurred when the deceased was in state custody (for example in prison, police custody, or whilst detained under the Mental Health Act 1983); or if it was the result of an act or omission of a police officer; or if it was a result of a notifiable accident, poisoning or disease. The senior coroner can also call a jury at his or her own discretion. This discretion has been heavily litigated in light of the Human Rights Act 1998, which means that juries are required now in a broader range of situations than expressly required by statute.

The purpose of the inquest is to answer four questions:

Evidence must be solely for the purpose of answering these questions and no other evidence is admitted. It is not for the inquest to ascertain "how the deceased died" or "in what broad circumstances", but "how the deceased came by his death", a more limited question. Moreover, it is not the purpose of the inquest to determine, or appear to determine, criminal or civil liability, to apportion guilt or attribute blame. For example, where a prisoner hanged himself in a cell, he came by his death by hanging and it was not the role of the inquest to enquire into the broader circumstances such as the alleged neglect of the prison authorities that might have contributed to his state of mind or given him the opportunity. However, the inquest should set out as many of the facts as the public interest requires.

Under the terms of article 2 of the European Convention of Human Rights, governments are required to "establish a framework of laws, precautions, procedures and means of enforcement which will, to the greatest extent reasonably practicable, protect life." The European Court of Human Rights has interpreted this as mandating independent official investigation of any death where public servants may be implicated. Since the Human Rights Act 1998 came into force, in those cases alone, the inquest is now to consider the broader question "by what means and in what circumstances".

In disasters, such as the King's Cross fire, a single inquest may be held into several deaths. However, when several protesters were shot and killed by police in Mitchelstown in 1887, the findings of a common inquest were quashed because the killings had taken place at different times and in different places.

Inquests are governed by the Coroners Rules. The coroner gives notice to near relatives, those entitled to examine witnesses and those whose conduct is likely to be scrutinised. Inquests are held in public except where there are real issues of national security.

Individuals with an interest in the proceedings, such as relatives of the deceased, individuals appearing as witnesses, and organisations or individuals who may face some responsibility in the death of the individual, may be represented by lawyers at the discretion of the coroner. Witnesses may be compelled to testify subject to the privilege against self-incrimination.

The following verdicts are not mandatory but are strongly recommended:

In 2004, 37% of inquests recorded an outcome of death by accident / misadventure, 21% by natural causes, 13% suicide, 10% open verdicts, and 19% other outcomes.

Since 2004 it has been possible for the coroner to record a narrative verdict, recording the circumstances of a death without apportioning blame or liability. Since 2009, other possible verdicts have included "alcohol/drug related death" and "road traffic collision". The civil standard of proof, on the balance of probabilities, is needed for most verdicts, except unlawful killing and suicide where the criminal standard of beyond reasonable doubt is required.

Owing in particular to the failures to notice the serial murder committed by Harold Shipman, the Coroners and Justice Act 2009 modernised the system with:





</doc>
<doc id="15270" url="https://en.wikipedia.org/wiki?curid=15270" title="Index">
Index

Index may refer to:


















</doc>
<doc id="15271" url="https://en.wikipedia.org/wiki?curid=15271" title="Information retrieval">
Information retrieval

Information retrieval (IR) is the activity of obtaining information system resources relevant to an information need from a collection of information resources. Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for metadata that describe data, and for databases of texts, images or sounds.

Automated information retrieval systems are used to reduce what has been called information overload. An IR systems is a software that provide access to books, journals and other documents, stores them and manages the document. Web search engines are the most visible IR applications.

An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.

An object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.

Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.

Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.

The idea of using computers to search for relevant pieces of information was popularized in the article "As We May Think" by Vannevar Bush in 1945. It would appear that Bush was inspired by patents for a 'statistical machine' - filed by Emanuel Goldberg in the 1920s and '30s - that searched for documents stored on film. The first description of a computer searching for information was described by Holmstrom in 1948, detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.

In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.

For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.



The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. All measures assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.








</doc>
<doc id="15272" url="https://en.wikipedia.org/wiki?curid=15272" title="List of Italian-language poets">
List of Italian-language poets

List of poets who wrote in Italian (or Italian dialects):




















</doc>
<doc id="15274" url="https://en.wikipedia.org/wiki?curid=15274" title="International Criminal Tribunal for the former Yugoslavia">
International Criminal Tribunal for the former Yugoslavia

The International Tribunal for the Prosecution of Persons Responsible for Serious Violations of International Humanitarian Law Committed in the Territory of the Former Yugoslavia since 1991, more commonly referred to as the International Criminal Tribunal for the former Yugoslavia (ICTY), was a body of the United Nations established to prosecute serious crimes committed during the Yugoslav Wars, and to try their perpetrators. The tribunal was an ad hoc court located in The Hague, Netherlands.

The Court was established by Resolution 827 of the United Nations Security Council, which was passed on 25 May 1993. It had jurisdiction over four clusters of crimes committed on the territory of the former Yugoslavia since 1991: grave breaches of the Geneva Conventions, violations of the laws or customs of war, genocide, and crimes against humanity. The maximum sentence it could impose was life imprisonment. Various countries signed agreements with the UN to carry out custodial sentences.

A total of 161 persons were indicted; the final indictments were issued in December 2004, the last of which were confirmed and unsealed in the spring of 2005. The final fugitive, Goran Hadžić, was arrested on 20 July 2011. The final judgment was issued on 29 November 2017 and the institution formally ceased to exist on 31 December 2017.

Residual functions of the ICTY, including oversight of sentences and consideration of any appeal proceedings initiated since 1 July 2013, are under the jurisdiction of a successor body, the Mechanism for International Criminal Tribunals.

United Nations Security Council Resolution 808 of 22 February 1993 decided that "an international tribunal shall be established for the prosecution of persons responsible for serious violations of international humanitarian law committed in the territory of the former Yugoslavia since 1991", and calling on the Secretary-General to "submit for consideration by the Council ... a report on all aspects of this matter, including specific proposals and where appropriate options ... taking into account suggestions put forward in this regard by Member States".

In 1993, the ICTY built its internal infrastructure. 17 states have signed an agreement with the ICTY to carry out custodial sentences.

1993–1994: In the first year of its existence, the Tribunal laid the foundations for its existence as a judicial organ. The Tribunal established the legal framework for its operations by adopting the rules of procedure and evidence, as well as its rules of detention and directive for the assignment of defense counsel. Together these rules established a legal aid system for the Tribunal. As the ICTY is part of the United Nations and as it was the first "international" court for "criminal" justice, the development of a juridical infrastructure was considered quite a challenge. However after the first year the first ICTY judges had drafted and adopted all the rules for court proceedings.

1994–1995: The ICTY established its offices within the Aegon Insurance Building in The Hague (which was, at the time, still partially in use by Aegon) and detention facilities in Scheveningen in The Hague (the Netherlands). The ICTY hired now many staff members. By July 1994 there were sufficient staff members in the office of the prosecutor to begin field investigations and by November 1994 the first indictment was presented and confirmed. In 1995, the entire staff numbered more than 200 persons and came from all over the world. Moreover, some governments assigned their legally trained people to the ICTY.

In 1994 the first indictment was issued against the Bosnian-Serb concentration camp commander Dragan Nikolić. This was followed on 13 February 1995 by two indictments comprising 21 individuals which were issued against a group of 21 Bosnian-Serbs charged with committing atrocities against Muslim and Croat civilian prisoners. While the war in the former Yugoslavia was still raging, the ICTY prosecutors showed that an international court was viable. However, no accused was arrested.

The court confirmed eight indictments against 46 individuals and issued arrest warrants. Bosnian Serb indictee Duško Tadić became the subject of the Tribunal's first trial. Tadić was arrested by German police in Munich in 1994 for his alleged actions in the Prijedor region in Bosnia-Herzegovina (especially his actions in the Omarska, Trnopolje and Keraterm detention camps). He made his first appearance before the ICTY Trial Chamber on 26 April 1995, and pleaded not guilty to all of the charges in the indictment.

1995–1996: Between June 1995 and June 1996, 10 public indictments had been confirmed against a total of 33 individuals. Six of the newly indicted persons were transferred in the Tribunal's detention unit. In addition to Duško Tadic, by June 1996 the tribunal had Tihomir Blaškić, Dražen Erdemović, Zejnil Delalić, Zdravko Mucić, Esad Landžo and Hazim Delić in custody. Erdemović became the first person to enter a guilty plea before the tribunal's court. Between 1995 and 1996, the ICTY dealt with miscellaneous cases involving several detainees, which never reached the trial stage.

In 2004, the ICTY published a list of five accomplishments "in justice and law":


The United Nations Security Council passed resolutions 1503 in August 2003 and 1534 in March 2004, which both called for the completion of all cases at both the ICTY and its sister tribunal, the International Criminal Tribunal for Rwanda (ICTR) by 2010.

In December 2010, the Security Council adopted Resolution 1966, which established the Mechanism for International Criminal Tribunals (MICT), a body intended to gradually assume residual functions from both the ICTY and the ICTR as they wound down their mandate. Resolution 1966 called upon the Tribunal to finish its work by 31 December 2014 to prepare for its closure and transfer of its responsibilities.

In a "Completion Strategy Report" issued in May 2011, the ICTY indicated it aimed to complete all trials by the end of 2012 and all appeals by 2015, with the exception of Radovan Karadžić whose trial was expected to end in 2014 and Ratko Mladić and Goran Hadžić, who were at large at that time and were not arrested until later that year.

The MICT's ICTY branch began functioning on 1 July 2013. Per the Transitional Arrangements adopted by the UN Security Council, the ICTY was to conduct and complete all outstanding first instance trials, including those of Karadžić, Mladić and Hadžić. The ICTY would also conduct and complete all appeal proceedings for which the notice of appeal against the judgement or sentence was filed before 1 July 2013. The MICT will handle any appeals for which notice is filed after that date.

The final ICTY trial to be completed in the first instance was that of Ratko Mladić, who was convicted on 22 November 2017. The final case to be considered by the ICTY was an appeal proceeding encompassing six individuals, whose sentences were upheld on 29 November 2017.

While operating, the Tribunal employed around 900 staff. Its organisational components were Chambers, Registry and the Office of the Prosecutor (OTP).
The Prosecutor was responsible for investigating crimes, gathering evidence and prosecutions and was head of the Office of the Prosecutor (OTP). The Prosecutor was appointed by the UN Security Council upon nomination by the UN Secretary-General.

The last prosecutor was Serge Brammertz. Previous Prosecutors have been Ramón Escovar Salom of Venezuela (1993–1994), however, he never took up that office, Richard Goldstone of South Africa (1994–1996), Louise Arbour of Canada (1996–1999), and Carla Del Ponte of Switzerland (1999–2007). Richard Goldstone, Louise Arbour and Carla Del Ponte also simultaneously served as the Prosecutor of the International Criminal Tribunal for Rwanda until 2003. Graham Blewitt [Australia] served as the Deputy Prosecutor from 1994 until 2004. David Tolbert, the President of the International Center for Transitional Justice, was also appointed Deputy Prosecutor of the ICTY in 2004.

Chambers encompassed the judges and their aides. The Tribunal operated three Trial Chambers and one Appeals Chamber. The President of the Tribunal was also the presiding Judge of the Appeals Chamber.

At the time of the court's dissolution, there were seven permanent judges and one "ad hoc" judge who served on the Tribunal. A total of 86 judges have been appointed to the Tribunal from 52 United Nations member states. Of those judges, 51 were permanent judges, 36 were "ad litem" judges, and one was an "ad hoc" judge. Note that one judge served as both a permanent and "ad litem" judge, and another served as both a permanent and "ad hoc" judge.

UN member and observer states could each submit up to two nominees of different nationalities to the UN Secretary-General. The UN Secretary-General submitted this list to the UN Security Council which selected from 28 to 42 nominees and submitted these nominees to the UN General Assembly. The UN General Assembly then elected 14 judges from that list. Judges served for four years and were eligible for re-election. The UN Secretary-General appointed replacements in case of vacancy for the remainder of the term of office concerned.

On 21 October 2015, Judge Carmel Agius of Malta was elected President of the ICTY and Liu Daqun of China was elected Vice-President; they have assumed their positions on 17 November 2015. His predecessors were Antonio Cassese of Italy (1993–1997), Gabrielle Kirk McDonald of the United States (1997–1999), Claude Jorda of France (1999–2002), Theodor Meron of the United States (2002–2005), Fausto Pocar of Italy (2005–2008), Patrick Robinson of Jamaica (2008–2011), and Theodor Meron (2011–2015).

The Registry was responsible for handling the administration of the Tribunal; activities included keeping court records, translating court documents, transporting and accommodating those who appear to testify, operating the Public Information Section, and such general duties as payroll administration, personnel management and procurement. It was also responsible for the Detention Unit for indictees being held during their trial and the Legal Aid program for indictees who cannot pay for their own defence. It was headed by the Registrar, a position occupied over the years by Theo van Boven of the Netherlands (February 1994 to December 1994), Dorothée de Sampayo Garrido-Nijgh of the Netherlands (1995–2000), Hans Holthuis of the Netherlands (2001–2009), and John Hocking of Australia (May 2009 to December 2017).

Those defendants on trial and those who were denied a provisional release were detained at the United Nations Detention Unit on the premises of the Penitentiary Institution Haaglanden, location Scheveningen in Belgisch Park, a suburb of The Hague, located some 3 km by road from the courthouse. The indicted were housed in private cells which had a toilet, shower, radio, satellite TV, personal computer (without internet access) and other luxuries. They were allowed to phone family and friends daily and could have conjugal visits. There was also a library, a gym and various rooms used for religious observances. The inmates were allowed to cook for themselves. All of the inmates mixed freely and were not segregated on the basis of nationality. As the cells were more akin to a university residence instead of a jail, some had derisively referred to the ICT as the "Hague Hilton". The reason for this luxury relative to other prisons is that the first president of the court wanted to emphasise that the indictees were innocent until proven guilty.

The Tribunal indicted 161 individuals between 1997 and 2004 and completed proceedings with them as follows:

The indictees ranged from common soldiers to generals and police commanders all the way to prime ministers. Slobodan Milošević was the first sitting head of state indicted for war crimes. Other "high level" indictees included Milan Babić, former President of the Republika Srpska Krajina; Ramush Haradinaj, former Prime Minister of Kosovo; Radovan Karadžić, former President of the Republika Srpska; Ratko Mladić, former Commander of the Bosnian Serb Army; and Ante Gotovina, former General of the Croatian Army.

The very first hearing at the ICTY was referral request in the Tadić case on 8 November 1994. Croat Serb General and former President of the Republic of Serbian Krajina Goran Hadžić was the last fugitive wanted by the Tribunal to be arrested on 20 July 2011.

An additional 23 individuals have been the subject of contempt proceedings.

Skeptics argued that an international court could not function while the war in the former Yugoslavia was still going on. This would be a huge undertaking for any court, but for the ICTY it would be an even greater one, as the new tribunal still needed judges, a prosecutor, a registrar, investigative and support staff, an extensive interpretation and translation system, a legal aid structure, premises, equipment, courtrooms, detention facilities, guards and all the related funding.

Criticisms of the court include:

Supporters of the work of the ICTY responded to critics in various publications. In a response to David Harland's "Selective Justice", Jelena Subotić, an assistant professor of political science at Georgia State University and author of "Hijacked Justice: Dealing with the Past in the Balkans", responded that the critics of the Tribunal miss the point, "which is not to deliver justice for past wrongs equally for 'all sides', fostering reconciliation, but to carefully measure each case on its own merits ... We should judge the work of the tribunal by its legal expertise, not by the political outcomes we desire."

Marko Hoare claims the accusations of the tribunal's "selective justice" stem from Serbian nationalist propaganda. He wrote: "This is, of course, the claim that hardline Serb nationalists and supporters of Slobodan Milosevic have been making for about the last two decades. Instead of carrying out any research into the actual record of the ICTY in order to support his thesis, Harland simply repeats a string of cliches of the kind that frequently appear in anti-Hague diatribes by Serb nationalists."




</doc>
<doc id="15275" url="https://en.wikipedia.org/wiki?curid=15275" title="ISO 216">
ISO 216

ISO 216 specifies international standard (ISO) paper sizes used in most countries in the world today, although not in Canada, the United States, Mexico, or the Dominican Republic. The standard defines the "A" and "B" series of paper sizes, including A4, the most commonly available size. Two supplementary standards, ISO 217 and ISO 269, define related paper sizes; the ISO 269 "C" series is commonly listed alongside the A and B sizes.

All ISO 216, ISO 217 and ISO 269 paper sizes (except some envelopes) have the same aspect ratio, :1, within rounding to millimetres. This ratio has the unique property that when cut or folded in half widthways, the halves also have the same aspect ratio. Each ISO paper size is one half of the area of the next larger size in the same series.

In 1786, the German scientist Georg Christoph Lichtenberg described the advantages of basing a paper size on an aspect ratio of in a letter to Johann Beckmann. The formats that became ISO paper sizes A2, A3, B3, B4, and B5 were developed in France. They were listed in a 1798 law on taxation of publications that was based in part on page sizes.
The main advantage of this system is its scaling. Rectangular paper with an aspect ratio of has the unique property that, when cut or folded in half midway between its shorter sides, each half has the same aspect ratio and half the area of the whole sheet before it was divided. Equivalently, if one lays two same-sized sheets paper with an aspect ratio of side-by-side along their longer side, they form a larger rectangle with the aspect ratio of and double the area of each individual sheet.

The ISO system of paper sizes exploit these properties of the aspect ratio. In each series of sizes (for example, series A), the largest size is numbered 0 (for example, A0), and each successive size (for example, A1, A2, etc.) has half the area of the preceding sheet and can be cut by halving the length of the preceding size sheet. The new measurement is rounded down to the nearest millimetre. A folded brochure can be made by using a sheet of the next larger size (for example, an A4 sheet is folded in half to make a brochure with size A5 pages. An office photocopier or printer can be designed to reduce a page from A4 to A5 or to enlarge a page from A4 to A3. Similarly, two sheets of A4 can be scaled down to fit one A4 sheet without excess empty paper.

This system also simplifies calculating the weight of paper. Under ISO 536, paper's grammage is defined as a sheet's weight in grams (g) per area in square metres (abbreviated g/m or gsm). Since an A0 sheet has an area of 1 m, its weight in grams is the same as its grammage. One can derive the grammage of other sizes by arithmetic division in g/m. A standard A4 sheet made from 80 g/m paper weighs 5 g, as it is (four halvings, ignoring rounding) of an A0 page. Thus the weight, and the associated postage rate, can be easily approximated by counting the number of sheets used.

ISO 216 and its related standards were first published between 1975 and 1995:

Paper in the A series format has an aspect ratio of (≈ 1.414, when ignoring rounding). A0 is defined so that it has an area of 1 square metre before rounding to the nearest millimeter. Successive paper sizes in the series (A1, A2, A3, etc.) are defined by halving the length of the preceding paper size and rounding down, so that the long side of A("n"+1) is the same length as the short side of A"n".

The most used of this series is the size A4 which is and thus almost exactly square metres in area. For comparison, the letter paper size commonly used in North America () is about 6 mm (0.24 in) wider and 18 mm (0.71 in) shorter than A4.

The geometric rationale behind the square root of 2 is to maintain the aspect ratio of each subsequent rectangle after cutting or folding an A series sheet in half, perpendicular to the larger side. Given a rectangle with a longer side, , and a shorter side, , ensuring that its aspect ratio, , will be the same as that of a rectangle half its size, , which means that , which reduces to ; in other words, an aspect ratio of 1:.

The formula that gives the larger border of the paper size A"n" in metres and without rounding off is the geometric sequence:
The paper size A"n" thus has the dimension
and area (before rounding)

The measurement in millimetres of the long side of A"n" can be calculated as
(brackets represent the floor function).

The B series is defined in the standard as follows: "A subsidiary series of sizes is obtained by placing the geometrical means between adjacent sizes of the A series in sequence." The use of the geometric mean makes each step in size: B0, A0, B1, A1, B2 … smaller than the previous one by the same factor. As with the A series, the lengths of the B series have the ratio , and folding one in half (and rounding down to the nearest millimeter) gives the next in the series. The shorter side of B0 is exactly 1 metre.

The measurement in millimetres of the long side of B"n" can be calculated as

There is also an incompatible Japanese B series which the JIS defines to have 1.5 times the area of the corresponding JIS A series (which is identical to the ISO A series). Thus, the lengths of JIS B series paper are ≈ 1.22 times those of A-series paper. By comparison, the lengths of ISO B series paper are ≈ 1.19 times those of A-series paper.

The C series formats are geometric means between the B series and A series formats with the same number (e.g., C2 is the geometric mean between B2 and A2). The width to height ratio is as in the A and B series. The C series formats are used mainly for envelopes. An A4 page will fit into a C4 envelope. C series envelopes follow the same ratio principle as the A series pages. For example, if an A4 page is folded in half so that it is A5 in size, it will fit into a C5 envelope (which will be the same size as a C4 envelope folded in half). The lengths of ISO C series paper are therefore ≈ 1.09 times those of A-series paper.

A, B, and C paper fit together as part of a geometric progression, with ratio of successive side lengths of , though there is no size half-way between B"n" and A("n" − 1): A4, C4, B4, "D4", A3, …; there is such a D-series in the Swedish extensions to the system.

The measurement in millimetres of the long side of C"n" can be calculated as

The tolerances specified in the standard are:

These are related to comparison between series A, B and C.

The ISO 216 formats are organized around the ratio 1:; two sheets next to each other together have the same ratio, sideways. In scaled photocopying, for example, two A4 sheets reduced to A5 size fit exactly onto one A4 sheet, and an A4 sheet in magnified size onto an A3 sheet; in each case, there is neither waste nor want.

The principal countries not generally using the ISO paper sizes are the United States and Canada, which use the Letter, Legal and Executive system. Although they have also officially adopted the ISO 216 paper format, Mexico, Panama, Venezuela, Colombia, the Philippines, and Chile also use mostly U.S. paper sizes.

Rectangular sheets of paper with the ratio 1: are popular in paper folding, such as origami, where they are sometimes called "A4 rectangles" or "silver rectangles". In other contexts, the term "silver rectangle" can also refer to a rectangle in the proportion 1:(1 + ), known as the silver ratio.

An important adjunct to the ISO paper sizes, particularly the A series, are the technical drawing line widths specified in ISO 128, and the matching technical pen widths of 0.13, 0.18, 0.25, 0.35, 0.5, 0.7, 1.0, 1.40, and 2.0 mm, as specified in . Color codes are assigned to each size to facilitate easy recognition by the drafter. These sizes increase by a factor of , so that particular pens can be used on particular sizes of paper, and then the next smaller or larger size can be used to continue the drawing after it has been reduced or enlarged, respectively. For example, a continuous thick line on A0 size paper shall be drawn with a 0.7 mm pen, the same line on A1 paper shall be drawn with a 0.5 mm pen, and finally on A2, A3, or A4 paper it shall be drawn with a 0.35 mm pen.

The earlier DIN 6775 standard upon which ISO 9175-1 is based also specified a term and symbol for easy identification of pens and drawing templates compatible with the standard, called , which may still be found on some technical drafting equipment.




</doc>
<doc id="15276" url="https://en.wikipedia.org/wiki?curid=15276" title="ISO 3864">
ISO 3864

ISO 3864 specifies international standards for safety symbols. These labels are graphical, to overcome language barriers. The standard is split into four parts:
These are the colors specified in ISO Standard 3864-4 in RAL colour standard.
The corresponding American standard is ANSI Z535, which is incompatible with the ISO standard. ANSI standard ANSI Z535.6-2006 defines an optional accompanying text in one or more languages.

ISO 3864 is extended by ISO 7010, which provides a set of symbols based on the principles and properties specified in ISO 3864.



</doc>
<doc id="15281" url="https://en.wikipedia.org/wiki?curid=15281" title="Isaac Abendana">
Isaac Abendana

Isaac Abendana (ca. 1640 – 1699) was the younger brother of Jacob Abendana, and became "hakam" of the Spanish Portuguese Synagogue in London after his brother died. 

Abendana moved to England before his brother, in 1662, and taught Hebrew at Cambridge University. He completed an unpublished Latin translation of the "Mishnah" for the university in 1671. 

While he was at Cambridge, Abendana sold Hebrew books to the Bodleian Library of Oxford, and in 1689 he took a teaching position in Magdalen College. In Oxford, he wrote a series of Jewish almanacs for Christians, which he later collected and compiled as the "Discourses on the Ecclesiastical and Civil Polity of the Jews" (1706). Like his brother, he maintained an extensive correspondence with leading Christian scholars of his time, most notably with the philosopher Ralph Cudworth, master of Christ's College, Cambridge. 


</doc>
<doc id="15284" url="https://en.wikipedia.org/wiki?curid=15284" title="List of intelligence agencies">
List of intelligence agencies

This is a list of intelligence agencies. It includes only currently operational institutions.















































Wildlife Crime Control Bureau



General Staff of the Armed Forces of the Islamic Republic of Iran:

Islamic Republic of Iran Army:

Islamic Revolutionary Guard Corps:

Law Enforcement Force of the Islamic Republic of Iran:

Ministry of Defence and Armed Forces Logistics:

Kurdistan Region

Foreign & Domestic Military Intelligence (Defence Forces)
Domestic Police Intelligence ("Garda Síochána")
















In the Presidents Office
























Special Telecommunication Service, abbreviated STS
Serviciul de Protecţie şi Pază, abbreviated SPP
(DGIA) – "Direcția Generală de Informații a Apărării"























MISS (formerly DMI - Defense Military Intelligence) now Military Intelligence Security Services











</doc>
<doc id="15285" url="https://en.wikipedia.org/wiki?curid=15285" title="Internet Engineering Task Force">
Internet Engineering Task Force

The Internet Engineering Task Force (IETF) develops and promotes voluntary Internet standards, in particular the standards that comprise the Internet protocol suite (TCP/IP). It is an open standards organization, with no formal membership or membership requirements. All participants and managers are volunteers, though their work is usually funded by their employers or sponsors. 

The IETF started out as an activity supported by the U.S. federal government, but since 1993 it has operated as a standards development function under the auspices of the Internet Society, an international membership-based non-profit organization.

The IETF is organized into a large number of working groups and informal discussion groups (BoFs, or Birds of a Feather), each dealing with a specific topic and operates in a bottom-up task creation mode, largely driven by these working groups. Each working group has an appointed chairperson (or sometimes several co-chairs), along with a charter that describes its focus, and what and when it is expected to produce. It is open to all who want to participate, and holds discussions on an open mailing list or at IETF meetings, where the entry fee in July 2014 was USD $650 per person.. Midst 2018 the fees are: early bird $700, late payment $875, student $150 and a one day pass for $375.

Rough consensus is the primary basis for decision making. There are no formal voting procedures. Because the majority of the IETF's work is done via mailing lists, meeting attendance is not required for contributors. Each working group is intended to complete work on its topic and then disband. In some cases, the WG will instead have its charter updated to take on new tasks as appropriate.

The working groups are organized into areas by subject matter. Current areas are Applications, General, Internet, Operations and Management, Real-time Applications and Infrastructure, Routing, Security, and Transport. Each area is overseen by an "area director" (AD), with most areas having two co-ADs. The ADs are responsible for appointing working group chairs. The area directors, together with the IETF Chair, form the Internet Engineering Steering Group (IESG), which is responsible for the overall operation of the IETF. 

The IETF is overseen by the Internet Architecture Board (IAB), which oversees its external relationships, and relations with the RFC Editor. The IAB is also jointly responsible for the IETF Administrative Oversight Committee (IAOC), which oversees the IETF Administrative Support Activity (IASA), which provides logistical, etc. support for the IETF. The IAB also manages the Internet Research Task Force (IRTF), with which the IETF has a number of cross-group relations.

A Nominating Committee (NomCom) of ten randomly chosen volunteers who participate regularly at meetings is vested with the power to appoint, reappoint, and remove members of the IESG, IAB, IASA, and the IAOC. To date, no one has been removed by a NomCom, although several people have resigned their positions, requiring replacements.

In 1993 the IETF changed from an activity supported by the U.S. government to an independent, international activity associated with the Internet Society, an international membership-based non-profit organization. Because the IETF itself does not have members, nor is it an organization "per se", the Internet Society provides the financial and legal framework for the activities of the IETF and its sister bodies (IAB, IRTF, …). IETF activities are funded by meeting fees, meeting sponsors and by the Internet Society via its organizational membership and the proceeds of the Public Interest Registry. 

In December 2005 the IETF Trust was established to manage the copyrighted materials produced by the IETF.

The first IETF meeting was attended by 21 U.S.-government-funded researchers on 16 January 1986. It was a continuation of the work of the earlier GADS Task Force. Representatives from non-governmental entities were invited to attend starting with the fourth IETF meeting in October 1986. Since that time all IETF meetings have been open to the public.

Initially, the IETF met quarterly, but from 1991, it has been meeting three times a year. The initial meetings were very small, with fewer than 35 people in attendance at each of the first five meetings. The maximum attendance during the first 13 meetings was only 120 attendees. This occurred at the 12th meeting held during January 1989. These meetings have grown in both participation and scope a great deal since the early 1990s; it had a maximum attendance of 2,810 at the December 2000 IETF held in San Diego, CA. Attendance declined with industry restructuring during the early 2000s, and is currently around 1,200.

The location for IETF meetings vary greatly. A list of past and future meeting locations can be found on the IETF meetings page. The IETF strives to hold its meetings near where most of the IETF volunteers are located. For many years, the goal was three meetings a year, with two in North America and one in either Europe or Asia, alternating between them every other year. The current goal is to hold three meetings in North America, two in Europe and one in Asia during a two-year period. However, corporate sponsorship of the meetings is also an important factor and the schedule has been modified from time to time in order to decrease operational costs.

The IETF also organizes Hackathons during the IETF meetings. The focus is on implementing code that will improve standards in terms of quality and interoperability.

The details of IETF operations have changed considerably as the organization has grown, but the basic mechanism remains publication of proposed specifications, development based on the proposals, review and independent testing by participants, and republication as a revised proposal, a draft proposal, or eventually as an Internet Standard. IETF standards are developed in an open, all-inclusive process in which any interested individual can participate. All IETF documents are freely available over the Internet and can be reproduced at will. Multiple, working, useful, interoperable implementations are the chief requirement before an IETF proposed specification can become a standard. Most specifications are focused on single protocols rather than tightly interlocked systems. This has allowed the protocols to be used in many different systems, and its standards are routinely re-used by bodies which create full-fledged architectures (e.g. 3GPP IMS).

Because it relies on volunteers and uses "rough consensus and running code" as its touchstone, results can be slow whenever the number of volunteers is either too small to make progress, or so large as to make consensus difficult, or when volunteers lack the necessary expertise. For protocols like SMTP, which is used to transport e-mail for a user community in the many hundreds of millions, there is also considerable resistance to any change that is not fully backwards compatible, except for IPv6. Work within the IETF on ways to improve the speed of the standards-making process is ongoing but, because the number of volunteers with opinions on it is very great, consensus on improvements has been slow to develop.

The IETF cooperates with the W3C, ISO/IEC, ITU, and other standards bodies.

Statistics are available that show who the top contributors by RFC publication are. While the IETF only allows for participation by individuals, and not by corporations or governments, sponsorship information is available from these statistics.

The IETF Chairperson is selected by the Nominating Committee (NomCom) process for a 2-year renewable term. Before 1993, the IETF Chair was selected by the IAB.

A list of the past and current Chairs of the IETF follows:
It works on a broad range of networking technologies which provide foundation for Internet's growth and evolution.

It aims to improve the efficiency in management of networks as they grow in size and complexity. IETF is also standardizing protocols for autonomic networking that enables networks to be self managing.

It is a network of physical objects or things that are embedded with electronics, sensors, software and also enables objects to exchange data with operator, manufacturer and other connected devices. Several IETF working groups are developing protocols that are directly relevant to IoT.

Its development provides the ability of internet applications to send data over the internet. There are some well established transport protocols such as TCP(Transmission Control Protocol) and UDP(User Datagram Protocol) which are continuously getting extended and refined to meet the needs of global internet.

It divides its work into number of areas which has Working groups that has a relation to area's focus. Area Directors handle the primary task of area management. Area Directors may be advised by one or more Directorates. The area structure is defined by Internet Engineering Steering Group. Nominations Committee can be used to add new members.



</doc>
<doc id="15286" url="https://en.wikipedia.org/wiki?curid=15286" title="ISM band">
ISM band

The industrial, scientific and medical (ISM) radio bands are radio bands (portions of the radio spectrum) reserved internationally for the use of radio frequency (RF) energy for industrial, scientific and medical purposes other than telecommunications. 
Examples of applications in these bands include radio-frequency process heating, microwave ovens, and medical diathermy machines. The powerful emissions of these devices can create electromagnetic interference and disrupt radio communication using the same frequency, so these devices were limited to certain bands of frequencies. In general, communications equipment operating in these bands must tolerate any interference generated by ISM applications, and users have no regulatory protection from ISM device operation.

Despite the intent of the original allocations, and because there are multiple allocations, in recent years the fastest-growing uses of these bands have been for short-range, low power wireless communications systems. Cordless phones, Bluetooth devices, near field communication (NFC) devices, and wireless computer networks (WiFi) all use the ISM frequencies, although these low power emitters are not considered ISM.

The ISM bands are defined by the ITU Radio Regulations (article 5) in footnotes 5.138, 5.150, and 5.280 of the Radio Regulations. Individual countries' use of the bands designated in these sections may differ due to variations in national radio regulations. Because communication devices using the ISM bands must tolerate any interference from ISM equipment, unlicensed operations are typically permitted to use these bands, since unlicensed operation typically needs to be tolerant of interference from other devices anyway. The ISM bands share allocations with unlicensed and licensed operations; however, due to the high likelihood of harmful interference, licensed use of the bands is typically low. In the United States, uses of the ISM bands are governed by Part 18 of the Federal Communications Commission (FCC) rules, while Part 15 contains the rules for unlicensed communication devices, even those that share ISM frequencies. In Europe, the ETSI is responsible for regulating the use of Short Range Devices, some of which operate in ISM bands.

The allocation of radio frequencies is provided according to "Article 5" of the ITU Radio Regulations (edition 2012).

In order to improve harmonisation in spectrum utilisation, the majority of service-allocations stipulated in this document were incorporated in national Tables of Frequency Allocations and Utilisations which is within the responsibility of the appropriate national administration. The allocation might be primary, secondary, exclusive, and shared.

Type A (footnote 5.138) = frequency bands are designated for "ISM applications". The use of these frequency bands for ISM applications shall be subject to special authorization by the administration concerned, in agreement with other administrations whose radiocommunication services might be affected. In applying this provision, administrations shall have due regard to the latest relevant ITU-R Recommendations.

Type B (footnote 5.150) = frequency bands are also designated for ISM applications. Radiocommunication services operating within these bands must accept harmful interference which may be caused by these applications.

ITU RR, Footnote 5.280 = In Germany, Austria, Bosnia and Herzegovina, Croatia, Macedonia, Liechtenstein, Montenegro, Portugal, Serbia, Slovenia and Switzerland, the band 433.05-434.79 MHz (center frequency 433.92 MHz) is designated for "ISM applications". Radiocommunication services of these countries operating within this band must accept harmful interference which may be caused by these applications.

Footnote AU, Australia is part of ITU Region 3 the band 433.05 to 434.79 MHz is not a designated ISM band in Australia, however the operation of low powered devices in the radiofrequency band 433.05 to 434.79 MHz is supported through Radiocommunications class licence for low interference potential devices (LIPDs).

The ISM bands were first established at the International Telecommunications Conference of the ITU in Atlantic City, 1947. The American delegation specifically proposed several bands, including the now commonplace 2.4 GHz band, to accommodate the then nascent process of microwave heating; however, FCC annual reports of that time suggest that much preparation was done ahead of these presentations.

From the proceedings:
"The delegate of the United States, referring to his request that the frequency 2450 Mc/s be allocated for I.S.M., indicated that there was in existence in the United States, and working on this frequency a diathermy machine and an electronic cooker, and that the latter might eventually be installed in transatlantic ships and airplanes. There was therefore some point in attempting to reach world agreement on this subject."

Radio frequencies in the ISM bands have been used for communication purposes, although such devices may experience interference from non-communication sources. In the United States, as early as 1958 Class D Citizens Band, a Part 95 service, was allocated to frequencies that are also allocated to ISM. [1]

In the U.S., the FCC first made unlicensed spread spectrum available in the ISM bands in rules adopted on May 9, 1985.

Many other countries later developed similar regulations, enabling use of this technology. The FCC action was proposed by Michael Marcus of the FCC staff in 1980 and the subsequent regulatory action took five more years. It was part of a broader proposal to allow civil use of spread spectrum technology and was opposed at the time by mainstream equipment manufacturers and many radio system operators.

The original ISM specifications envisioned that the bands would be used primarily for noncommunication purposes, such as heating. The bands are still widely used for these purposes. For many people, the most commonly encountered ISM device is the home microwave oven operating at 2.45 GHz which uses microwaves to cook food. Industrial heating is another big application area; such as induction heating, microwave heat treating, plastic softening, and plastic welding processes. In medical settings, shortwave and microwave diathermy machines use radio waves in the ISM bands to apply deep heating to the body for relaxation and healing. More recently hyperthermia therapy uses microwaves to heat tissue to kill cancer cells. 

However, as detailed below, the increasing congestion of the radio spectrum, the increasing sophistication of microelectronics, and the attraction of unlicensed use, has in recent decades led to an explosion of uses of these bands for short range communication systems for wireless devices, which are now by far the largest uses of these bands. These are sometimes called "non ISM" uses since they do not fall under the originally envisioned "industrial", "scientific", and "medical" application areas. One of the largest applications has been wireless networking (WiFi). The IEEE 802.11 wireless networking protocols, the standards on which almost all wireless systems are based, use the ISM bands. Virtually all laptops, tablet computers, computer printers and cellphones now have 802.11 wireless modems using the 2.4 and 5.7 GHz ISM bands. Bluetooth is another networking technology using the 2.4 GHz band, which can be problematic given the probability of interference. Near field communication devices such as proximity cards and contactless smart cards use the lower frequency 13 and 27 MHz ISM bands. Other short range devices using the ISM bands are: wireless microphones, baby monitors, garage door openers, wireless doorbells, keyless entry systems for vehicles, radio control channels for UAVs (drones), wireless surveillance systems, RFID systems for merchandise, and wild animal tracking systems. 

Some electrodeless lamp designs are ISM devices, which use RF emissions to excite fluorescent tubes. Sulfur lamps are commercially available plasma lamps, which use a 2.45 GHz magnetron to heat sulfur into a brightly glowing plasma.

Long-distance wireless power systems have been proposed and experimented with which would use high-power transmitters and rectennas, in lieu of overhead transmission lines and underground cables, to send power to remote locations. NASA has studied using microwave power transmission on 2.45 GHz to send energy collected by solar power satellites back to the ground.

Also in space applications, a Helicon Double Layer ion thruster is a prototype spacecraft propulsion engine which uses a 13.56 MHz transmission to break down and heat gas into plasma.

In recent years ISM bands have also been shared with (non-ISM) license-free error-tolerant communications applications such as wireless sensor networks in the 915 MHz and 2.450 GHz bands, as well as wireless LANs and cordless phones in the 915 MHz, 2.450 GHz, and 5.800 GHz bands. Because unlicensed devices are required to be tolerant of ISM emissions in these bands, unlicensed low power users are generally able to operate in these bands without causing problems for ISM users. ISM equipment does not necessarily include a radio receiver in the ISM band (e.g. a microwave oven does not have a receiver).

In the United States, according to 47 CFR Part 15.5, low power communication devices must accept interference from licensed users of that frequency band, and the Part 15 device must not cause interference to licensed users. Note that the 915 MHz band should not be used in countries outside Region 2, except those that specifically allow it, such as Australia and Israel, especially those that use the GSM-900 band for cellphones. The ISM bands are also widely used for Radio-frequency identification (RFID) applications with the most commonly used band being the 13.56 MHz band used by systems compliant with ISO/IEC 14443 including those used by biometric passports and contactless smart cards.

In Europe, the use of the ISM band is covered by Short Range Device regulations issued by European Commission, based on technical recommendations by CEPT and standards by ETSI. In most of Europe, LPD433 band is allowed for license-free voice communication in addition to PMR446.

Wireless LAN devices use wavebands as follows:

IEEE 802.15.4, ZigBee and other personal area networks may use the and ISM bands because of frequency sharing between different allocations.

Wireless LANs and cordless phones can also use bands other than those shared with ISM, but such uses require approval on a country by country basis. DECT phones use allocated spectrum outside the ISM bands that differs in Europe and North America. Ultra-wideband LANs require more spectrum than the ISM bands can provide, so the relevant standards such as IEEE 802.15.4a are designed to make use of spectrum outside the ISM bands. Despite the fact that these additional bands are outside the official ITU-R ISM bands, because they are used for the same types of low power personal communications, they are sometimes incorrectly referred to as ISM bands as well.

Also note that several brands of radio control equipment use the band range for low power remote control of toys, from gas powered cars to miniature aircraft.

Worldwide Digital Cordless Telecommunications or WDCT is a technology that uses the radio spectrum.

Google's Project Loon uses ISM bands (specifically 2.4 and 5.8 GHz bands) for balloon-to-balloon and balloon-to-ground communications.

Pursuant to 47 CFR Part 97 some ISM bands are used by licensed amateur radio operators for communication - including amateur television.




</doc>
