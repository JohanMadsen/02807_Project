<doc id="21482" url="https://en.wikipedia.org/wiki?curid=21482" title="Nairobi">
Nairobi

Nairobi () is the capital and the largest city of Kenya. The name comes from the Maasai phrase "Enkare Nyrobi", which translates to "cool water", a reference to the Nairobi River which flows through the city. The city proper had a population of 3,138,369 in the 2009 census, while the metropolitan area has a population of 6,547,547. The city is popularly referred to as the Green City in the Sun.

Nairobi was founded in 1899 by the colonial authorities in British East Africa, as a rail depot on the Uganda Railway. The town quickly grew to replace Machakos as the capital of Kenya in 1907. After independence in 1963, Nairobi became the capital of the Republic of Kenya. During Kenya's colonial period, the city became a centre for the colony's coffee, tea and sisal industry. The city lies on the River Athi in the southern part of the country, and has an elevation of above sea level.

With a population of 3.36 million in 2011, Nairobi is the second-largest city by population in the African Great Lakes region after Dar es Salaam, Tanzania. According to the 2009 census, in the administrative area of Nairobi, 3,138,295 inhabitants lived within . Nairobi is the 10th-largest city in Africa, including the population of its suburbs.

Home to thousands of Kenyan businesses and over 100 major international companies and organisations, including the United Nations Environment Programme (UN Environment) and the United Nations Office at Nairobi (UNON), Nairobi is an established hub for business and culture. The Nairobi Securities Exchange (NSE) is one of the largest in Africa and the second-oldest exchange on the continent. It is Africa's fourth-largest exchange in terms of trading volume, capable of making 10 million trades a day.

Nairobi is found within the Greater Nairobi Metropolitan region, which consists of 5 out of 47 counties in Kenya, which generates about 60% of the entire nation's . The counties are:

The area was an uninhabited swamp until a supply depot of the Uganda Railway was built in 1896, which soon became the railway's headquarters. The decision by railway engineers irked the colonial government, who originally preferred Machakos as the ideal capital of the colony. The main issue, from the very start, was the lack of proper drainage as the selected site was swampy. But railway engineers believed that Nairobi would become nothing more than an Indian township which they argued, could "'prosper in spite of unsanitary conditions and chronic plague".'

The city was named after a water hole known in Maasai as "Enkare Nairobi", meaning "place of cool waters". Its main street at the time, Biashara Street, was completely rebuilt in the early 1900s after an outbreak of plague and the burning of the original town. The location of the Nairobi railway camp was chosen due to its central position between Mombasa and Kampala. It was also chosen because its network of rivers could supply the camp with water and its elevation would make it cool enough for residential purposes. However, malaria was a serious problem, leading to at least one attempt to have the town moved.

In 1905, Nairobi replaced Mombasa as capital of the British protectorate, and the city grew around administration and tourism, initially in the form of big game hunting. As the British occupiers started to explore the region, they started using Nairobi as their first port of call. This prompted the colonial government to encourage the building of several hotels in the city. The main occupants were British game hunters.

Nairobi continued to grow under the British and many people settled within the city's suburbs. In 1919, Nairobi was declared to be a municipality. Despite becoming one of Britain's richest colonies, a British investigating commission in 1955 found that poverty was widespread among the city's population:The wages of the majority of African workers are too low to enable them to obtain accommodation which is adequate to any standard. The high cost of housing relative to wages is in itself a cause of overcrowding, because housing is shared to lighten the cost. This, with the high cost of food in towns, makes family life impossible for the majority.

In February 1926, E. A. T. Dutton passed through Nairobi on his way to Mount Kenya, and said of the city:

The continuous expansion of the city began to anger the Maasai, as the city was devouring their land to the south. It also angered the Kikuyu people, who wanted the land returned to them. After the end of World War II, this friction developed into the Mau Mau rebellion. Jomo Kenyatta, Kenya's future president, was jailed for his involvement even though there was no evidence linking him to the rebellion. The pressure exerted from the locals onto the British resulted in Kenyan independence in 1963, with Nairobi as the capital of the new republic.

After independence, Nairobi grew rapidly and this growth put pressure on the city's infrastructure. Power cuts and water shortages were a common occurrence, though in the past few years better city planning has helped to put some of these problems in check.

On 11 September 1973, the Kenyatta International Conference Centre KICC was open to the public. The 28-storey building at the time was designed by the Norwegian architect Karl Henrik Nøstvik and a Kenyan David Mutiso, the construction was done in three phases.Phase I was the construction of the podium, Phase II consisted of the main tower and Phase III involved the Plenary. Construction was completed in 1973; with the opening ceremony occurring on 11 September and being presided over by Kenya’s founding father President Kenyatta. It is the only building within the city with a helipad that is open to the public. Of the buildings built in the Seventies, the KICC was the most eco-friendly and most environmentally conscious structure; its main frame was constructed with locally available materials Gravel, sand, cement and wood, it had wide open spaces which allowed for natural aeration and natural lighting. Cuboids made up the plenary hall, the tower consisted of a cylinder composed of several cuboids and the amphitheater and helipad both resembled cones. The tower was built around a concrete core and it had no walls but glass windows, which allowed for maximum natural lighting. It had the largest halls in eastern and central Africa.

Three years prior in 1972, the World Bank approved funds for further expansion of the then Nairobi Airport (now Jomo Kenyatta International Airport), including a new international and domestic passenger terminal building, the airport's first dedicated cargo and freight terminal, new taxiways, associated aprons, internal roads, car parks, police and fire stations, a State Pavilion, airfield and roadway lighting, fire hydrant system, water, electrical, telecommunications and sewage systems, a dual carriageway passenger access road, security, drainage and the building of the main access road to the airport (Airport South Road). The total cost of the project was more than US$29 million (US$111.8 million in 2013 dollars). On 14 March 1978, construction of the current terminal building was completed on the other side of the airport's single runway and opened by President Jomo Kenyatta less than five months before his death. The airport was renamed Jomo Kenyatta International Airport in memory of its First President.

The United States Embassy, then located in downtown Nairobi, was bombed in August 1998 by Al-Qaida, as one of a series of US embassy bombings. It is now the site of a memorial park.

On 9 November 2012, President Mwai Kibaki opened the KES 31 billion Thika Superhighway.The mega-project in East and Central Africa started in 2009 and ended in 2011. It involved expanding the four-lane carriageway to eight lanes, building underpasses, providing interchanges at roundabouts, erecting flyovers and building underpasses to ease congestion. The 50.4-kilometre road was built in three phases: Uhuru Highway to Muthaiga Roundabout; Muthaiga Roundabout to Kenyatta University and; Kenyatta University to Thika Town.

On 31 May 2017, The current president Uhuru Kenyatta inaugurated the Standard Gauge railway which runs from Nairobi to Mombasa and vice versa. It was primarily built by a Chinese firm with about 90% of total funding from China and about 10% from the Kenyan government. A second phase is also being built which will link Naivasha to the existing route and also the Uganda border.

The city is situated at and and occupies .

Nairobi is situated between the cities of Kampala and Mombasa. As Nairobi is adjacent to the eastern edge of the Rift Valley, minor earthquakes and tremors occasionally occur. The Ngong Hills, located to the west of the city, are the most prominent geographical feature of the Nairobi area. Mount Kenya is situated north of Nairobi, and Mount Kilimanjaro is towards the south-east.

The Nairobi River and its tributaries traverse through the Nairobi County. Nobel Peace Prize laureate Wangari Maathai fought fiercely to save the indigenous Karura Forest in northern Nairobi which was under threat of being replaced by housing and other infrastructure.

Nairobi's western suburbs stretch all the way from the Kenyatta National Hospital in the south to the UN headquarters at Gigiri suburb in the north, a distance of about . The city is centred on the City Square, which is located in the Central Business District. The Kenyan Parliament buildings, the Holy Family Cathedral, Nairobi City Hall, Nairobi Law Courts, and the Kenyatta Conference Centre all surround the square.

Under the Köppen climate classification, Nairobi has a subtropical highland climate (Cfb/Cwb). At above sea level, evenings may be cool, especially in the June/July season, when the temperature can drop to . The sunniest and warmest part of the year is from December to March, when temperatures average the mid-twenties during the day. The mean maximum temperature for this period is .

There are two rainy seasons, but rainfall can be moderate. The cloudiest part of the year is just after the first rainy season, when, until September, conditions are usually overcast with drizzle. As Nairobi is situated close to the equator, the differences between the seasons are minimal. The seasons are referred to as the wet season and dry season. The timing of sunrise and sunset varies little throughout the year for the same reason.

Nairobi is divided into a series of constituencies with each being represented by members of Parliament in the National Assembly. These constituencies are: Makadara, Kamukunji, Starehe, Langata, Dagoretti, Westlands, Kasarani, and Embakasi. The main administrative divisions of Nairobi are Central, Dagoretti, Embakasi, Kasarani, Kibera, Makadara, Pumwani, and Westlands. Most of the upmarket suburbs are situated to the west and north-central of Nairobi, where most European settlers resided during the colonial times AKA 'Ubabini'. These include Karen, Langata, Lavington, Gigiri, Muthaiga, Brookside, Spring Valley, Loresho, Kilimani, Kileleshwa, Hurlingham, Runda, Kitisuru, Nyari, Kyuna, Lower Kabete, Westlands, and Highridge, although Kangemi, Kawangware, and Dagoretti are lower income areas close to these affluent suburbs. The city's colonial past is commemorated by many English place-names.

Most lower-middle and upper middle income neighbourhoods are located in the north-central areas such as Highridge, Parklands, Ngara, Pangani, and areas to the southwest and southeast of the metropolitan area near the Jomo Kenyatta International Airport. The most notable ones include Avenue Park, Fedha, Pipeline, Donholm, Greenfields, Nyayo, Taasia, Baraka, Nairobi West, Madaraka, Siwaka, South B, South C, Mugoya, Riverbank, Hazina, Buru Buru, Uhuru, Harambee Civil Servants', Akiba, Kimathi, Pioneer, and Koma Rock to the centre-east and Kasarani to northeast area among others. The low and lower income estates are located mainly in far eastern Nairobi. These include, Umoja, Kariokor, Dandora, Kariobangi, Kayole, Embakasi, and Huruma. Kitengela suburb, though located further southeast, Ongata Rongai and Kiserian further southwest, and Ngong/Embulbul suburbs also known as 'Diaspora' to the far west are considered part of the Greater Nairobi Metropolitan area. More than 90% of Nairobi residents work within the Nairobi Metropolitan area, in the formal and informal sectors. Many Somali immigrants have also settled in Eastleigh, nicknamed "Little Mogadishu".

The Kibera slum in Nairobi (with an estimated population of at least 500,000 to over 1,000,000 people) was thought to be Africa's second largest slum. However, recent census results have shown that Kibera is indeed much smaller than originally thought.

Nairobi has many parks and open spaces throughout the city. Much of the city has dense tree-cover and plenty of green spaces. The most famous park in Nairobi is Uhuru Park. The park borders the central business district and the neighbourhood Upper Hill. Uhuru ("Freedom" in Swahili) Park is a centre for outdoor speeches, services, and rallies. The park was to be built over by former President Daniel arap Moi, who wanted the 62-storey headquarters of his party, the Kenya African National Union, situated in the park. However, the park was saved following a campaign by Nobel Peace Prize winner Wangari Maathai.

Central Park is adjacent to Uhuru Park, and includes a memorial for Jomo Kenyatta, the first president of Kenya, and the Moi Monument, built in 1988 to commemorate the second president's first decade in power. Other notable open spaces include Jeevanjee Gardens, City Park, 7 August Memorial Park, and Nairobi Arboretum.

The colonial 1948 Master Plan for Nairobi still acts as the governing mechanism when it comes to making decisions related to urban planning. The Master Plan at the time, which was designed for 250,000 people, allocated 28% of Nairobi's land to public space, but because of rapid population growth, much of the vitality of public spaces within the city are increasingly threatened. City Park, the only natural park in Nairobi, for example, was originally 150 acres, but has since lost approximately 50 acres of land to private development through squatting and illegal alienation which began in the 1980s.

The City of Nairobi enjoys the status of a full administrative County.

The Nairobi province differs in several ways from other Kenyan regions. The county is entirely urban. It has only one local council, Nairobi City Council. Nairobi Province was not divided into "districts" until 2007, when three districts were created. In 2010, along with the new constitution, Nairobi was renamed a County.

Nairobi County has seventeen constituencies. Constituency name may differ from division name, such that Starehe Constituency is equal to Central Division, Lang'ata Constituency to Kibera division, and Kamukunji Constituency to Pumwani Division in terms of boundaries.

Nairobi is divided into seventeen constituencies and eighty five wards, mostly named after residential estates. Kibera Division, for example, includes Kibera (Kenya's largest slum) as well as affluent estates of Karen and Langata.

Nairobi is home to the Nairobi Securities Exchange (NSE), one of Africa's largest. The NSE was officially recognised as an overseas stock exchange by the London Stock Exchange in 1953. The exchange is Africa's 4th largest in terms of trading volumes, and 5th largest in terms of Market Capitalization as a percentage of GDP.

Nairobi is the regional headquarters of several international companies and organisations. In 2007, General Electric, Young & Rubicam, Google, Coca-Cola, IBM Services, Airtel, and Cisco Systems relocated their African headquarters to the city. The United Nations Office at Nairobi hosts UN Environment and UN-Habitat headquarters.

Several of Africa's largest companies are headquartered in Nairobi. KenGen, which is the largest African stock outside South Africa, is based in the city. Kenya Airways, Africa's fourth largest airline, uses Nairobi's Jomo Kenyatta International Airport as a hub.

Goods manufactured in Nairobi include clothing, textiles, building materials, processed foods, beverages, and cigarettes. Several foreign companies have factories based in and around the city. These include Goodyear, General Motors, Toyota Motors, and Coca-Cola.

Nairobi has a large tourist industry, being both a tourist destination and a transport hub. 

Nairobi has grown around its central business district. This takes a rectangular shape, around the Uhuru Highway, Haille Selassie Avenue, Moi Avenue, and University Way. It features many of Nairobi's important buildings, including the City Hall and Parliament Building. The city square is also located within the perimeter.

Most of the skyscrapers in this region are the headquarters of businesses and corporations, such as I&M and the Kenyatta International Conference Centre. The United States Embassy bombing took place in this district, prompting the building of a new embassy building in the suburbs.

In 2011, the city was considered to be about 4 million residents. A large beautification project took place in the Central Business District, as the city prepared to host the 2006 Afri-Cities summit. Iconic buildings such as the Kenyatta International Conference Centre had their exteriors cleaned and repainted.

Nairobi downtown area or central business district is bordered to the southwest by Uhuru Park and Central Park. The Mombasa to Kampala railway runs to the southeast of the district.

Today, many businesses are considering relocating and /or establishing their headquarters outside the Central Business District area. This is because land is cheaper, and better facilities can easily be built and maintained elsewhere. Two areas that are seeing a growth in companies and office space are Upper Hill, which is located, approximately from the Central Business District and Westlands, which is also about the same distance, away from the city centre.

Companies that have moved from the Central Business District to Upper Hill include Citibank and in 2007, Coca-Cola began construction of their East and Central African headquarters in Upper Hill, cementing the district as the preferred location for office space in Nairobi. The largest office development in this area is UAP Tower, a recently completed 33-storey tower at 163 meters high. The World Bank and International Finance Corporation (part of the World Bank Group) are also located in Upper Hill at the Delta Center, Menegai Road. Earlier on, they were located in the Hill Park Building and CBA Building respectively(both also in Upper Hill), and prior to that in View Park towers in the Central Business District.

To accommodate the large demand for floor space in Nairobi, various commercial projects are being constructed. New business parks are being built in the city, including the flagship Nairobi Business Park.

Construction boom and real estate development projects

Nairobi is currently undergoing a construction boom. Major real estate projects and skyscrapers are coming up in the city. Among them are:The pinnacle twin towers which will tower at 314m, Britam Tower (200m), Avic International Africa headquarters (176m), Prism tower (140m), Pan Africa insurance towers, Pallazzo offices, and many other projects. Shopping malls are also being constructed like the recently completed Garden city Mall, Centum's Two rivers Mall, The Hub in Karen, Karen waterfront, Thika Greens, and the recently reconstructed Westgate Mall. High-class residential apartments for living are coming up like Le Mac towers, a residential tower in Westlands Nairobi with 23 floors. Avic International is also putting up a total of four residential apartments on Waiyaki way: a 28-level tower, two 24-level towers, and a 25-level tower. Hotel towers are also being erected in the city. Avic International is putting up a 30-level hotel tower of 141m in the Westlands. The hotel tower will be operated by Marriot group. Jabavu limited is constructing a 35 floor hotel tower in Upper Hill which will be high over 140 metres in the city skyline. Arcon Group Africa has also announced plans to erect a skyscraper in Upper hill which will have 66 floors and tower over 290 metres, further cementing Upper hill as the preferred metropolis for multinational corporations launching their operations in the Kenyan capital.

Nairobi is one of the few cities in the world with a national park within its boundaries, making it a prime tourist destination as well, with several other tourist attractions. The most famous is the Nairobi National Park, the only game reserve of this nature to border a capital city, or any major city. The park contains many animals including lions, giraffes, and black rhinos. The park is home to over 400 species of birds. The Nairobi Safari Walk is a major attraction to the Nairobi National Park as it offers a rare on-foot experience of the animals.

Nairobi is home to several museums, sites, and monuments. The Nairobi National Museum is the country's national museum and the largest in the city. It houses a large collection of artefacts portraying Kenya's rich heritage through history, nature, culture, and contemporary art. It also includes the full remains of a homo erectus popularly known as the Turkana boy. Other prominent museums include the Nairobi Gallery, Nairobi Railway Museum, and the Karen Blixen Museum located in the affluent Karen suburb. Uhuru Gardens, a national monument and the largest memorial park in Kenya, is also the place where the first Kenyan flag was raised at independence. It is located along Langata road near the Wilson Airport.

Nairobi is nicknamed the "Safari Capital of the World" or "the Green City in the Sun", and has many hotels to cater for safari-bound tourists. Five-star hotels in Nairobi include the Nairobi Serena, Laico Regency (formerly Grand Regency Hotel), Windsor (Karen), Holiday Inn, Nairobi Safari Club (Lilian Towers), The Sarova Stanley Hotel, Safari Park & Casino, InterContinental, Panari Hotel, Hilton, and the Norfolk Hotel. Other newer ones include the Crowne Plaza Hotel Nairobi in Upper Hill area, the Sankara Nairobi in Westlands, Tribe Hotel-Village Market, House of Wayne, The Eastland Hotel, Ole Sereni, and The Boma located along Mombasa Highway. International chains apart from the Hilton, the Intercontinental group, and Serena Hotels are also setting up properties in Nairobi city. Upcoming establishments include Radisson Blu and the upscale boutique Bidwood Suite Hotel in Westlands, which are nearing completion. The Best Western Premier-Nairobi and The Villa Rosa Kempinski have been completed and opened.

Nairobi is also home to the largest ice rink in Africa: the Solar Ice Rink at the Panari Hotel's Sky Centre. The rink, opened in 2005, covers and can accommodate 200 people.

Shopping malls in Nairobi include the Greenspan Mall (Donholm), Yaya Centre (Hurlingham), Sarit Centre (Westlands), Westgate Shopping Mall (Westlands), ABC Place (Westlands), The Village Market (Gigiri), Junction Shopping Mall (Ngong Road), Prestige Plaza (Ngong Road), Crossroads Shopping Centre (Karen), T-Mall (Langata), Garden City Mall (Thika Road) and Thika Road Mall (Thika Road). Nakumatt, Uchumi, and Tuskys, Naivas are the largest supermarket chains with modern stores throughout the city.

The Nairobi Java House is a coffee house and restaurant chain with branches located around the city including one at the Jomo Kenyatta International Airport. Other coffee chains include Art Caffe, Dormans Coffee House and Savannah, which is part of Sasini Tea.

Nairobi's night life is popular with tourists, young and old. From a collection of gourmet restaurants offering local and international cuisine, Nairobi has something to offer to every age and pocket. Most common known food establishments include The Carnivore and The Tamarind Restaurants which have outlets in Langata, City Centre, and the Village Market. For those more discerning travellers, one can choose from a wide array of local cuisine, Mediterranean, fast food, Ethiopian, and Arabian. The city's nightlife is mostly centred along friends and colleagues meeting after work especially on Fridays – commonly known as "Furahiday" (Happy Day), theme nights, events and concerts, and Shisha cafés. The most popular clubbing spots are centred in upmarket Westlands which has come to be known as "Electric Avenue", Karen, Langata, Hurlingham, and "uptown" venues in the city centre. Nairobians generally go out every day of the week and most establishments are open till late.

Other sites include Jomo Kenyatta's Mausoleum, Kenya National Theatre, and the Kenya National Archives. Art galleries in Nairobi include the Rahimtulla Museum of Modern Art (Ramoma), the Mizizi Arts Centre, and the Nairobi National Museum.

Population of Nairobi between 1906 and 2009.

Nairobi has experienced one of the highest growth rates of any city in Africa. Since its foundation in 1899, Nairobi has grown to become the second largest city in the African Great Lakes, despite being one of youngest cities in the region. The growth rate of Nairobi is currently 4.1% a year. It is estimated that Nairobi's population will reach 5 million in 2025.

These data fit remarkably closely (r^2 = 0.9994) to a logistic curve with t(0) = 1900, P(0)=8500, r = 0.059 and K = 8,000,000. This suggests a current (2011) growth rate of 3.5% (the CIA estimate of 4.5% cited above would have been true in 2005). According to this curve, the population of the city will be below 4 million in 2015, and will reach 5 million in 2025.

Given this high population growth, owing itself both to urban migration and high birth rates, the economy has yet to catch up. Unemployment is estimated at 40% within the city, mainly in the high-density, low income areas of the city which can make them seem even denser than the higher-income neighborhoods.

Nairobi is a cosmopolitan and multicultural city. The names of some of its suburbs, including Hurlingham and Parklands reflect Nairobi's early history of colonial occupation.

By the mid twentieth century, many foreigners settled in Nairobi from other parts of the British Empire, primarily India and parts of (present-day) Pakistan. These immigrants were workers who arrived to construct the Kampala – Mombasa railway, settling in Nairobi after its completion, and also merchants from Gujarat. Nairobi also has established communities from Somalia and Sudan. 

There are a number of churches, mosques, temples, and gurdwaras within the city. Prominent places of worship in Nairobi include the Cathedral Basilica of the Holy Family, All Saints Cathedral, Ismaili Jamat Khana, and Jamia Mosque.

Nairobi has two informal nicknames. The first is ""The Green City in the Sun"", which is derived from the city's foliage and warm climate. The second is the ""Safari Capital of the World"", which is used due to Nairobi's prominence as a hub for safari tourism.

There are a number of shopping malls in the Nairobi Area. These include: Garden city mall, Thika road mall (TRM), the West Gate mall, Prestige Plaza, the Village Market, the Sarit Centre, the Junction, Rosslyn Riviera mall, Two Rivers mall. A variety of amenities are provided at these malls and include: cinemas, fashion and apparel retailers, bookshops, electronics and grocery stores, coffeehouses, restaurants and bars.

"Kwani?" is Kenya's first literary journal and was established by writers living in Nairobi. Nairobi's publishing houses have also produced the works of some of Kenya's authors, including Ngũgĩ wa Thiong'o and Meja Mwangi who were part of post-colonial writing.

Many film makers also practice their craft out of Nairobi. Film-making is still young in the country, but people like producer Njeri Karago and director Judy Kibinge are paving the way for others.

Perhaps the most famous book and film set in Nairobi is "Out of Africa". The book was written by Karen Blixen, whose pseudonym was Isak Dinesen, and it is her account of living in Kenya. Karen Blixen lived in the Nairobi area from 1917 to 1931. The neighbourhood in which she lived, Karen, is named after her.

In 1985, "Out of Africa" was made into a film, directed by Sydney Pollack. The film won 28 awards, including seven Academy Awards. The popularity of the film prompted the opening of Nairobi's Karen Blixen Museum.

Nairobi is also the setting of many of the novels of Ngũgĩ wa Thiong'o, Kenya's foremost writer.

Nairobi has been the set of several other American and British films. The most recent of these was "The Constant Gardener" (2005), a large part of which was filmed in the city. The story revolves around a British diplomat in Nairobi whose wife is murdered in northern Kenya. Much of the filming was in the Kibera slum.

Among the latest Kenyan actors in Hollywood who identify with Nairobi is Lupita Nyong'o. Lupita received an Oscar award for best supporting actress in her role as Patsy in the film "12 Years a Slave" during the "86th Academy Awards" at the Dolby theatre in Los Angeles. Lupita is the daughter of Kenyan politician Peter Anyang' Nyong'o

Most new Hollywood films are nowadays screened at Nairobi's cinemas. Up until the early 1990s, there were only a few film theatres and the repertoire was limited. There are also two drive-in cinemas in Nairobi.

In 2015 and 2016, Nairobi was the focus point for the American television series "Sense8" which shot its first and second seasons partly in the city. The TV series has high reviews in The Internet Movie Database (IMDB).

In 2015 Nairobi was also featured in the British thriller film "Eye in the Sky (2015 film)", which is a story about a lieutenant general and a colonel who faced political opposition after ordering a drone missile strike to take out a group of suicide bombers in Nairobi, Kenya.

In Nairobi, there is a range of restaurants and, besides being home to "nyama choma" which is a local term used to refer to roasted meat, there are American fast food restaurants such as KFC, Subway, Domino's Pizza, Pizza Hut, Hardee's and Burger King which are popular, and the longer established South African chains, Galittos, Steers, PizzaMojo, Spur Steak Ranches. Coffee houses, doubling up as restaurants, mostly frequented by the upper middle classes, such as Artcaffe, Nairobi Java House and Dormans have become increasingly popular in recent days. Traditional food joints such as the popular K'osewe's in the city centre and Amaica, which specialise in African delicacies are also widepsread. The Kenchic franchise which specialised in old-school chicken and chips meals was also popular, particularly among the lower classes and students, with restaurants all over the city and its suburbs. However, as at February 2016, Kenchic stopped operating its eatery businesses. Upscale restaurants specialising in specific cuisines, ranging from Italian, Lebanese, Ethiopian, French and seafood are more likely to be found in five star hotels and the wealthier suburbs in the West and South of the city.

Nairobi has an annual restaurant week (NRW) at the beginning of the year, January–February. Nairobi's restaurants offer dining packages at reduced prices. NRW is managed by Eatout Kenya which is an online platform that lists and reviews restaurants in Nairobi, and provides a platform for Kenyan foodies to congregate and share.

Nairobi is the centre of Kenya's music scene. Benga is a Kenyan genre which was developed in Nairobi. The style is a fusion of jazz and Luo music forms. Mugithi is another popular genre in Kenya, with its origins in the central parts of the country. A majority of music videos of leading local musicians are also filmed in the city.

In the 1970s, Nairobi became the prominent centre for music in the African Great Lakes. During this period, Nairobi was established as a hub of soukous music. This genre was originally developed in Kinshasa and Brazzaville. After the political climate in the region deteriorated, many Congolese artists relocated to Nairobi. Artists such as Orchestra Super Mazembe moved from Congo to Nairobi and found great success. Virgin records became aware of the popularity of the genre and signed recording contracts with several soukous artists.

More recently, Nairobi has become the centre of the Kenyan hip hop scene, with Kalamashaka, Gidi Gidi Majimaji being the pioneers of urban music in Kenyan. The genre has become very popular amongst local youth, and domestic musicians have become some of the most popular in the region. Successful artists based in Nairobi include Jua Cali, Nonini, Camp Mulla, Juliani, Eric Wainaina, Suzanna Owinyo and Nameless. Popular Record labels include Ogopa DJs, Grand Pa Records, Main Switch, Red Black and Green Republik, Calif Records and Bornblack Music Group.

Many foreign musicians who tour Africa perform in Nairobi. Bob Marley's first-ever visit to Africa started in Nairobi. Acts that have performed in Nairobi include Lost Boyz, Wyclef Jean, Shaggy, Akon, Eve, T.O.K, Sean Paul, Wayne Wonder, Alaine, Konshens, Ja Rule, and Morgan Heritage, and Cabo Snoop. Other international musicians who have performed in Nairobi include the rocking show by Don Carlos, Demarco, Busy Signal, Mr. Vegas and the Elephant man crew.

Nairobi, including the coastal towns of Mombasa and Diani, have recently become the centre of Electronic Dance Music (EDM) in Kenya, which includes Trance, Techno, House, Progressive, Drum & Bass, and Dubstep. Prominent international composers & DJs have graced their presence in these cities, including Kyau & Albert, Solarity, Ronski Speed, and Boom Jinx.

Many nightclubs in and around the city have witnessed a growth in the population that exclusively listen to Electronic Dance Music, especially amongst the younger generations. These youth also support many local EDM producers & DJs, such as Jahawi, Mikhail Kuzi, Barney Barrow, Jack Rooster, HennessyLive, Trancephilic5 As well as up and comers such as L.A Dave, Eric K, Raj El Rey, Tom Parker and more.

Gospel music is also very popular in Nairobi just as in the rest of Kenya, with gospel artistes having a great impact in the mostly Christian city . Artistes such as Esther Wahome, Eunice Njeri, Daddy Owen, Emmy Kosgei and the late Angela Chibalonza, among others, have a great pull over the general population while others like MOG, Juliani, Ecko dyda, DK Kwenye Beat have great influence over the younger generation. Their concerts are also very popular and they have as much influence as the great secular artistes.The most popular being Groove tours, TSO (Totally Sold Out) new year concerts.

Musical group Sauti Sol performed for U.S. President Barack Obama when he was in the city for the 2015 Global Entrepreneurship summit.

Nairobi is the African Great Lakes region's sporting centre. The premier sports facility in Nairobi and generally in Kenya is the Moi International Sports Centre in the suburb of Kasarani. The complex was completed in 1987, and was used to host the 1987 All Africa Games. The complex comprises a 60,000 seater stadium, the second largest in the African Great Lakes (after Tanzania's new national stadium), a 5,000 seater gymnasium, and a 2,000 seater aquatics centre.

The Nyayo National Stadium is Nairobi's second largest stadium renowned for hosting global rugby event under the "Safaricom Sevens." Completed in 1983, the stadium has a capacity of 30,000. This stadium is primarily used for football. The facility is located close to the Central Business District, which makes it a convenient location for political gatherings.

Nairobi City Stadium is the city's first stadium, and used for club football. Nairobi Gymkhana is the home of the Kenyan cricket team, and was a venue for the 2003 Cricket World Cup. Notable annual events staged in Nairobi include Safari Rally (although it lost its World Rally Championship status in 2003), Safari Sevens rugby union tournament, and Nairobi Marathon.

Football is the most popular sport in the city by viewership and participation. This is highlighted by the number of football clubs in the city, including Kenyan Premier League sides Gor Mahia, A.F.C. Leopards, Tusker and Mathare United.

There are six golf courses within a 20 km radius of Nairobi. The oldest 18-hole golf course in the city is the Royal Nairobi Golf Club. It was established in 1906 by the British, just seven years after the city was founded. Other notable golf clubs include the Windsor Country Club, Karen Country Club, and Muthaiga Golf Club. The Kenya Open golf tournament, which is part of the Challenge Tour, takes place in Nairobi. The Ngong Racecourse in Nairobi is the centre of horse racing in Kenya.

Rugby is also a popular sport in Nairobi with 8 of the 12 top flight clubs based here.

Basketball is also a popular Sport played in the City's Primary, Secondary and College leagues. Most of the City's Urban youth are basketball fans and watch the American NBA.

The majority of schools follow either the Kenyan Curriculum or the British Curriculum. There is also International School of Kenya which follows the North American Curriculum and the German school in Gigiri. Kenya High School, one of the schools in Kenya, is located in Nairobi.

Nairobi is home to several Universities and Colleges.
Numerous other universities have also opened satellite campuses in Nairobi. The Railways Training Institute established in 1956, is also a notable institution of higher learning with a campus in Nairobi.

Major plans are being implemented in the need to decongest the city's traffic and the completion of Thika Road has given the city a much needed face-lift attributed to road's enhancement of global standards. Several projects have been completed (Syokimau Rail Station, the Eastern and Northern Bypasses) while numerous other projects are still underway. The country's head of state announced (when he opened Syokimau Rail Service) that Kenya was collaborating with other countries in the region to develop railway infrastructure to improve regional connectivity under the ambitious LAPPSET project which is the single largest and most expensive in the continent.

Kenya signed a bilateral agreement with Uganda to facilitate joint development of the Mombasa-Malaba-Kampala standard gauge railway. A branch line will also be extended to Kisumu.

Similarly, Kenya signed a Memorandum of Understanding with the Government of Ethiopia for the development of Lamu-Addis Ababa standard gauge railway. Under the Lamu-South Sudan and Ethiopia Transport Corridor Project, the development of a railway component is among the priority projects.

The development of these critical transport facilities will, besides reducing transport costs due to faster movement of goods and people within the region, also increase trade, improve the socio-economic welfare of Northern Kenya and boost the country's potential in attracting investments from all over the world.

The first phase of the Standard Gauge Railway project was launched on 31 May 2017 by the President of Kenya Uhuru Kenyatta in a ceremony that saw thousands of Kenyans ride on the inaugural trip free of charge. The two passenger locomotives christened Madaraka Express currently operate daily trips between Nairobi and Mombasa with the fixed departure time of 9:00 AM.

Jomo Kenyatta International Airport is the largest airport in East and Central Africa. Domestic travelers made up 40% of overall passengers in 2016. An increase of 32% in 5 yrs since 2012. JKIA had more than 7 million passengers going through it in 2016. In February 2017, JKIA received a Category One Status from the FAA boosting the airport's status as a Regional Aviation hub.

Wilson Airport is a general-aviation airport handling smaller aircraft, mostly propeller-driven. In July 2016, construction of a new Air Traffic Control Tower commenced at a cost of KES 163 Million (approximately USD 1.63 million)

Eastleigh Airport is a military base airport. In its earlier years, it was utilised as a landing strip in the pre-jet airline era. It was mostly used as a British Passenger and mail route from Southampton to Cape Town in the 1930s & 1940's. This route was served by flying boats between Britain and Kisumu and then by land-based aircraft on the routes to the south.

Matatus are the most common form of public transport in Nairobi.
Matatu, which literally translates to "three cents for a ride" (nowadays much more) are privately owned minibuses, and the most popular form of local transport. They generally seat fourteen to twenty-four. Matatus operate within Nairobi, its environs and suburbs and from Nairobi to other towns around the country. The matatu's route is imprinted along a yellow stripe on the side of the bus, and matatus plying specific routes have specific route numbers. However, in November 2014 President Uhuru Kenyatta lifted the ban on the yellow stripe and allowed matatus to maintain the colourful graphics in an effort to support the youth in creating employment. Matatus in Nairobi were easily distinguishable by their extravagant paint schemes, as owners would paint their matatu with various colourful decorations, such as their favourite football team or hip hop artist. More recently, some have even painted Barack Obama's face on their vehicle. They are notorious for their poor safety records, which are a result of overcrowding and reckless driving. Due to the intense competition between matatus, many are equipped with powerful sound systems and television screens to attract more customers.
However, in 2004, a law was passed requiring all matatus to include seat belts and speed governors and to be painted with a yellow stripe. At first, this caused a furore amongst Matatu operators, but they were pressured by government and the public to make the changes. Matatus are now limited to . However, many of the matatu vehicles have had their speed governors disabled, which is evident by them travelling at speeds well over .

In December 2010, the Government embarked on a policy to phase out matatus as a means of public transport. Consequently, no new matatus are licensed to operate from January 2011 while the current ones will be allowed to live out their lifespan; a move aimed at enhancing the safety of citizens and visitors as well. However, the matatus continue to occupy the road ways in large numbers contributing to the congestion of Nairobi.

Buses are increasingly becoming common in the city with some even going to the extents of installing complimentary WiFi systems in partnership with the leading mobile service provider. There are four major bus companies operating the city routes and are the traditional Kenya Bus Service (KBS), and newer private operators Citi Hoppa, Compliant MOA and Double M. The Citi Hoppa buses are distinguishable by their green livery, the Double M buses are painted purple, Compliant MOA by their distinctively screaming names and mix of white, blue colours while the KBS buses are painted blue.

Companies such as Easy Coach, Crown Bus, Coast Bus, Modern Coast, Eldoret Express, Chania, the Guardian Angel, Spanish and Mash Poa run scheduled buses and luxury coaches to other cities and towns.

Nairobi was founded as a railway town, and the main headquarters of Kenya Railways (KR) is still situated at Nairobi railway station, which is located near the city centre. The line runs through Nairobi, from Mombasa to Kampala. Its main use is freight traffic connecting Nairobi to Mombasa and Kisumu. A number of morning and evening commuter trains connect the centre with the suburbs, but the city has no proper light rail, tramway, or rapid transit lines. A proposal has been passed for the construction of a commuter rail line. The country's third president since independence, President Mwai Kibaki on Tuesday, 13 November 2012 launched the Syokimau Rail Service marking a major milestone in the history of railway development in the country. The opening of the station marked another milestone in efforts to realise various projects envisaged under the Vision 2030 Economic Blueprint. The new station has a train that ferries passengers from Syokimau to the city centre cutting travel time by half. Opening of the station marks the completion of the first phase of the Sh24b Nairobi Commuter Rail Network that is geared at easing traffic congestion in Nairobi, blamed for huge economic losses. Other modern stations include Imara Daima Railway Station and Makadara Railway Station.

The new Mombasa-Nairobi Standard Gauge Railway connects the port city of Mombasa and Nairobi. The new railway line has replaced the 1900 metre-gauge railway. The Nairobi Terminus is located at Syokimau, some 20 km from the city centre. Passengers travelling from Mombasa are usually transferred from the metre-gauge trains to the CBD.

Nairobi is served by highways that link Mombasa to Kampala in Uganda and Arusha in Tanzania. These are earmarked to ease the daily motor traffic within and surrounding the metro area. However, driving in Nairobi is chaotic. Most of the roads are tarmacked and there are signs showing directions to certain neighbourhoods. The city is connected to the Jomo Kenyatta International Airport by the Mombasa Highway, which passes through Industrial Area, South B, South C and Embakasi. Ongata Rongai, Langata and Karen are connected to the city centre by Langata Road, which runs to the south. Lavington, Riverside, Westlands, etc. are connected by Waiyaki Way. Kasarani, Eastlands, and Embakasi are connected by Thika Road, Jogoo Road, and Outer Ring Road.

Highways connect the city with other major towns such as Mombasa, Machakos, Voi, (A109), Kisumu, Nakuru, Eldoret, Namanga Border Tanzania(A104) etc.

Nairobi is currently undergoing major road constructions to update its infrastructure network. The new systems of roads, flyovers, and bridges would cut outrageous traffic levels caused the inability of the current infrastructure to cope with the soaring economic growth in the past few years. It is also a major component of Kenya's Vision 2030 and Nairobi Metropolis plans. Most roads now, though, are well lit and surfaced with adequate signage.

The proposed Nairobi Bypasses are currently under construction by the Kenyan government and financed by Chinese Government. Their construction seeks to ease congestion in Nairobi's downtown area and the surrounding suburbs.
The Bypasses will comprise the:

94% of the piped water supply for Nairobi comes from rivers and reservoirs in the Aberdare Range north of the city, of which the reservoir of the Thika Dam is the most important one. Water distribution losses – technically called non-revenue water – are 40%, and only 40% of those with house connections receive water continuously. Slum residents receive water through water kiosks and end up paying much higher water prices than those fortunate enough to have access to piped water at their residence. In the middle of a severe drought, the board of the Nairobi City Water and Sewerage Company was sacked July 2009 for "malpractices", following the publication of a report by Transparency International-Kenya and the Kenyan NGO Maji Na Ufanisi (Water and Development). The report had found cases of bribery for illegal connections, tampering with meter readings, and diversion of water from domestic users to industries in five cities, with the highest incidence of bribery in Nairobi.

There is wide variety regarding standards of living in Nairobi. Most wealthy Kenyans live in Nairobi, but the majority of Nairobians are average and poor. Half of the population have been estimated to live in slums which cover just 5% of the city area. The growth of these slums is a result of urbanisation, poor town planning, and the unavailability of loans for low income earners. 

Kibera is one of the largest slums in Africa, and is situated to the west of Nairobi. (Kibera comes from the Nubian word Kibra, meaning "forest" or "jungle"). The slums cover two square kilometres and are on government land. Kibera has been the setting for several films, the most recent being "The Constant Gardener".

Other notable slums include Mathare and Korogocho. Altogether, 66 areas are counted as slums within Nairobi.

Many Nairobi non-slum-dwellers live in relatively good housing conditions. Large houses can be found in many of the upmarket neighbourhoods, especially to the west of Nairobi. Historically, British occupiers have settled in Gigiri, Muthaiga, Langata and Karen. Other middle and high income estates include Parklands, Westlands, Hurlingham, Kilimani, Milimani, Spring Valley, Lavington, Rosslyn, Kitisuru, and Nairobi Hill.

To accommodate the growing middle class, many new apartments and housing developments are being built in and around the city. The most notable development is "Greenpark", at Athi River, Machakos County from Nairobi's Central Business District. Over 5,000 houses, villas and apartments are being constructed at this development, including leisure, retail and commercial facilities. The development is being marketed to families, as are most others within the city. Eastlands also houses most of the city's middle class and includes South C, South B, Embakasi, Buru Buru, Komarock, Donholm, Umoja, and various others.

Throughout the 1990s, Nairobi had struggled with rising crime, earning a reputation for being a dangerous city and the nickname "Nairobbery," a name which persists today. On 7 August 1998, the US Embassy was bombed, killing 224 people and injuring 4000. In 2001, the United Nations International Civil Service Commission rated Nairobi as among the most insecure cities in the world, classifying the city as "status C". In the United Nations report, it was stated that in 2001, nearly one third of all Nairobi residents experienced some form of robbery in the city. The head of one development agency cited the notoriously high levels of violent armed robberies, burglaries, and carjackings. Crime had risen in Nairobi as a result of unplanned urbanisation, with a minimal number of police stations and a proper security infrastructure. However, many claim that the biggest factor for the city's alarming crime rate is police corruption, which leaves many criminals unpunished. As a security precaution, most large houses have a watch guard, burglar grills, and dogs to patrol their grounds during the night. Most crimes, however, occur around the poor neighbourhoods where it gets dangerous during night hours.

In 2006, crime decreased in the city, due to increased security and an improved police presence. Despite this, in 2007, the Kenyan government and US State Department have announced that Nairobi is experiencing a greater level of violent crime than in previous years. Since then, the government has taken measures to combat crime with heavy police presence in and around the city while US government has updated its travel warning for the country.

Following a grenade attack in October 2011 by a local Kenyan man, with terrorist links, the city faced a heightened security presence. Fears spread over further promised retaliations by the Al-Shabaab group of rebels over Kenya's involvement in a coordinated operation with the Somalian military against the insurgent outfit.

There have been a spate of Blasts in Nairobi which started on 10 March 2012, where assailants threw grenades at a busy bus station and a blue-collar bar in Nairobi, killing nine and injuring more than 50. On 28 May 2012, 28 people were injured in an explosion in a shopping complex in downtown Nairobi, near Moi avenue. On 21 September 2013, Al-Shabaab-associated militants attacked the Westgate Mall. 67 people were killed.

Nairobi is home to most of Kenya's news and media organisations. The city is also home to the African Great Lakes region's largest newspapers: the "Daily Nation" and "The Standard". These are circulated within Kenya and cover a range of domestic and regional issues. Both newspapers are published in English.

Kenya Broadcasting Corporation, a state-run television and radio station, is headquartered in the city. Kenya Television Network is part of the Standard Group and was Kenya's first privately owned TV station. The Nation Media Group runs NTV which is based in Nairobi. East Africa Television Channel 5 is 24-hour music channel based in Dar es Salaam Tanzania and broadcasts in Uganda, Tanzania, and Kenya. There are also a number of prominent radio stations located in Kenya's capital including KISS 100, Capital FM, East FM, Kameme FM, Metro FM, and Family FM, among others.

Several multinational media organisations have their regional headquarters in Nairobi. These include the BBC, CNN, Agence France-Presse, Reuters, Deutsche Welle, and the Associated Press. The East African bureau of CNBC Africa is located in Nairobi's city centre, while the Nairobi bureau of "The New York Times" is located in the suburb of Gigiri. The broadcast headquarters of CCTV Africa are located in Nairobi.

Nairobi is also home to the East African School of Media Studies due to its large media focus.

Nairobi has grown since 1899. A population projection in the 21st century is listed below.
Nairobi is twinned with:



</doc>
<doc id="21483" url="https://en.wikipedia.org/wiki?curid=21483" title="Numeral (linguistics)">
Numeral (linguistics)

In linguistics, a numeral is a member of a part of speech characterized by the designation of numbers; some examples are the English word 'two' and the compound 'seventy-seventh'. Numerals function most typically as a noun (e.g., "twelve", "dozen"), adjective (e.g., "first", "single") or adverb (e.g., "once", "singly") and express numbers and relations to numbers for example: quantity, sequence, frequency, or fraction.

Numerals may be attributive, as in "two dogs", or pronominal, as in "I saw two (of them)".

Many words of different parts of speech indicate number or quantity. Such words are called Quantifiers. Examples are words such as "every", "most", "least", "some", etc. Numerals are distinguished from other quantifiers by the fact that they designate a specific number. Examples are words such as "five, ten, fifty, one hundred, etc." They may or may not be treated as a distinct part of speech; this may vary, not only with the language, but with the choice of word. For example, 'dozen' serves the function of a noun, 'first' serves the function of an adjective, and 'twice' serves the function of an adverb. In Old Church Slavonic, the cardinal numbers 5 to 10 were feminine nouns; when quantifying a noun, that noun was declined in the genitive plural like other nouns that followed a noun of quantity (one would say the equivalent of "five of people"). In English grammar, the classification ""numeral"" (viewed as a part of speech) is reserved for those words which have distinct grammatical behavior: when a numeral modifies a noun, it may replace the article: the/some dogs played in the park" → twelve dogs played in the park". (Note that *"dozen dogs played in the park" is not grammatical, so 'dozen' is not a numeral in this sense.) English numerals indicate cardinal numbers. However, not all words for cardinal numbers are necessarily numerals. For example, "million" is grammatically a noun, and must be preceded by an article or numeral itself.

Numerals may be simple, such as 'eleven', or compound, such as 'twenty-three'.

In linguistics, however, numerals are classified according to purpose: examples are ordinal numbers ("first", "second", "third", etc.; from 'third' up, these are also used for fractions), multiplicative numbers ("once", "twice", and "thrice"), multipliers ("single", "double", and "triple"), and distributive numbers ("singly", "doubly", and "triply"). Georgian, Latin, and Romanian (see Romanian distributive numbers) have regular distributive numbers, such as Latin "singuli" "one-by-one", "bini" "in pairs, two-by-two", "terni" "three each", etc. In languages other than English, there may be other kinds of number words. For example, in Slavic languages there are collective numbers which describe sets, such as "pair" or "dozen" in English (see Russian numerals, Polish numerals).

Some languages have a very limited set of numerals, and in some cases they arguably do not have any numerals at all, but instead use more generic quantifiers, such as 'pair' or 'many'. However, by now most such languages have borrowed the numeral system or part of the numeral system of a national or colonial language, though in a few cases (such as Guarani ), a numeral system has been invented internally rather than borrowed. Other languages had an indigenous system but borrowed a second set of numerals anyway. An example is Japanese, which uses either native or Chinese-derived numerals depending on what is being counted.

In many languages, such as Chinese, numerals require the use of numeral classifiers. Many sign languages, such as ASL, incorporate numerals.

Not all languages have numeral systems. Specifically, there is not much need for numeral systems among hunter-gatherers who do not engage in commerce. Many languages around the world have no numerals above two to four—or at least did not before contact with the colonial societies—and speakers of these languages may have no tradition of using the numerals they did have for counting. Indeed, several languages from the Amazon have been independently reported to have no specific number words other than 'one'. These include Nadëb, pre-contact Mocoví and Pilagá, Culina and pre-contact Jarawara, Jabutí, Canela-Krahô, Botocudo (Krenák), Chiquitano, the Campa languages, Arabela, and Achuar. Some languages of Australia, such as Warlpiri, do not have words for quantities above two, as did many Khoisan languages at the time of European contact. Such languages do not have a word class of 'numeral'.

Most languages with both numerals and counting use base 8, 10, 12, or 20. Base 10 appears to come from counting one's fingers, base 20 from the fingers and toes, base 8 from counting the spaces between the fingers (attested in California), and base 12 from counting the knuckles (3 each for the four fingers).

For very large (and very small) numbers, traditional systems have been superseded by the use of scientific notation and the system of SI prefixes. Traditional systems continue to be used in everyday life.

Many languages of Melanesia have (or once had) counting systems based on parts of the body which do not have a numeric base; there are (or were) no numerals, but rather nouns for relevant parts of the body—or simply pointing to the relevant spots—were used for quantities. For example, 1–4 may be the fingers, 5 'thumb', 6 'wrist', 7 'elbow', 8 'shoulder', etc., across the body and down the other arm, so that the opposite little finger represents a number between 17 (Torres Islands) to 23 (Eleman). For numbers beyond this, the torso, legs and toes may be used, or one might count back up the other arm and back down the first, depending on the people.

Some Austronesian and Melanesian ethnic groups, some Sulawesi and some Papua New Guineans, count with the base number four, using the term "asu" and "aso", the word for dog, as the ubiquitous village dog has four legs. This is argued by anthropologists to be also based on early humans noting the human and animal shared body feature of two arms and two legs as well as its ease in simple arithmetic and counting. As an example of the system's ease a realistic scenario could include a farmer returning from the market with fifty "asu" heads of pig (200), less 30 "asu" (120) of pig bartered for 10 "asu" (40) of goats noting his new pig count total as twenty "asu": 80 pigs remaining. The system has a correlation to the dozen counting system and is still in common use in these areas as a natural and easy method of simple arithmetic.

Quinary systems are based on the number 5. It is almost certain the quinary system developed from counting by fingers (five fingers per hand). An example are the Epi languages of Vanuatu, where 5 is "luna" 'hand', 10 "lua-luna" 'two hand', 15 "tolu-luna" 'three hand', etc. 11 is then "lua-luna tai" 'two-hand one', and 17 "tolu-luna lua" 'three-hand two'.

5 is a common "auxiliary base", or "sub-base", where 6 is 'five and one', 7 'five and two', etc. Aztec was a vigesimal (base-20) system with sub-base 5.

The Morehead-Maro languages of Southern New Guinea are examples of the rare base 6 system. Kanum is one these languages. The Sko languages on the North Coast of New Guinea follow a base-24 system with a subbase of 6.

Septenary systems are vary rare, as few natural objects consistently have seven distinctive features. Traditionally, it occurs in week-related timing. It has been suggested that the Palikur language may have a base-seven system, but this is dubious.

Octal counting systems are based on the number 8. It is used in the Yuki language of California and in the Pamean languages of Mexico, because the Yuki and Pame keep count by using the four spaces between their fingers rather than the fingers themselves.

It has been suggested that Nenets has a base-nine system.

A majority of traditional number systems are decimal. This dates back at least to the ancient Egyptians, who used a wholly decimal system. Anthropologists hypothesize this may be due to humans having five digits per hand, ten in total. There are many regional variations including:


Duodecimal systems are based on 12.

These include:

Duodecimal numeric systems have some practical advantages over decimal. It is much easier to divide the base digit twelve (which is a highly composite number) by many important divisors in market and trade settings, such as the numbers 2, 3, 4 and 6.

Because of several measurements based on twelve, many Western languages have words for base-twelve units such as "dozen", "gross" and "great gross", which allow for rudimentary duodecimal nomenclature, such as "two gross six dozen" for 360. Ancient Romans used a decimal system for integers, but switched to duodecimal for fractions, and correspondingly Latin developed a rich vocabulary for duodecimal-based fractions (see Roman numerals). A notable fictional duodecimal system was that of J. R. R. Tolkien's Elvish languages, which used duodecimal as well as decimal.

Vigesimal numbers use the number 20 as the base number for counting. Anthropologists are convinced the system originated from digit counting, as did bases five and ten, twenty being the number of human fingers and toes combined
The system is in widespread use across the world. Some include the classical Mesoamerican cultures, still in use today in the modern indigenous languages of their descendants, namely the Nahuatl and Mayan languages (see Maya numerals). A modern national language which uses a full vigesimal system is Dzongkha in Bhutan.

Partial vigesimal systems are found in some European languages: Basque, Celtic languages, French (from Celtic), Danish, and Georgian. In these languages the systems are vigesimal up to 99, then decimal from 100 up. That is, 140 is 'one hundred two score', not *seven score, and there is no numeral for 400.

The term "score" originates from tally sticks, and is perhaps a remnant of Celtic vigesimal counting. It was widely used to learn the pre-decimal British currency in this idiom: "a dozen pence and a score of bob", referring to the 20 shillings in a pound. For Americans the term is most known from the opening of the Gettysburg Address: ""Four score and seven years ago our fathers..."".

The Sko languages have a base-24 system with a subbase of 6.

Ngiti has base 32.

Ekari has a base-60 system. Sumeria had a base-60 system with a decimal subbase (perhaps a conflation of the decimal and a duodecimal systems of its constituent peoples), which was the origin of the numbering of modern degrees, minutes, and seconds.

Supyire is said to have a base-80 system; it counts in twenties (with 5 and 10 as subbases) up to 80, then by eighties up to 400, and then by 400s (great scores).
799 [i.e. 400 + (4 x 80) + (3 x 20) + {10 + (5 + 4)}]’

English has derived numerals for multiples of its base ("fifty, sixty," etc), and some languages have simplex numerals for these, or even for numbers between the multiples of its base. Balinese, for example, currently has a decimal system, with words for 10, 100, and 1000, but has additional simplex numerals for 25 (with a second word for 25 only found in a compound for 75), 35, 45, 50, 150, 175, 200 (with a second found in a compound for 1200), 400, 900, and 1600. In Hindustani, the numerals between 10 and 100 have developed to the extent that they need to be learned independently.

In many languages, numerals up to the base are a distinct part of speech, while the words for powers of the base belong to one of the other word classes. In English, these higher words are hundred 10, thousand 10, million 10, and higher powers of a thousand (short scale) or of a million (long scale—see names of large numbers). These words cannot modify a noun without being preceded by an article or numeral (*"hundred dogs played in the park"), and so are nouns.

In East Asia, the higher units are hundred, thousand, myriad 10, and powers of myriad. In India, they are hundred, thousand, lakh 10, crore 10, and so on. The Mesoamerican system, still used to some extent in Mayan languages, was based on powers of 20: "bak’" 400 (20), "pik" 8000 (20), "kalab" 160,000 (20), etc.

A database Numeral Systems of the World's Languages compiled by Eugene S.L. Chan of Hong Kong is hosted by the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany. The database currently contains data for about 4000 languages.




</doc>
<doc id="21485" url="https://en.wikipedia.org/wiki?curid=21485" title="Neutrino">
Neutrino

A neutrino ( or ) (denoted by the Greek letter ν) is a fermion (an elementary particle with half-integer spin) that interacts only via the weak subatomic force and gravity. The mass of the neutrino is much smaller than that of the other known elementary particles. Although only differences of squares of the three mass values are known as of 2016, cosmological observations imply that the sum of the three masses must be less than one millionth that of the electron. The neutrino is so named because it is electrically neutral and because its rest mass is so small ("-ino") that it was long thought to be zero. The weak force has a very short range, gravity is extremely weak on the subatomic scale, and neutrinos, as leptons, do not participate in the strong interaction. Thus, neutrinos typically pass through normal matter unimpeded and undetected.

Weak interactions create neutrinos in one of three leptonic flavors: electron neutrinos muon neutrinos (), or tau neutrinos (), in association with the corresponding charged lepton. Although neutrinos were long believed to be massless, it is now known that there are three discrete neutrino masses with different tiny values, but they do not correspond uniquely to the three flavors. A neutrino created with a specific flavor is in an associated specific quantum superposition of all three mass states. As a result, neutrinos oscillate between different flavors in flight. For example, an electron neutrino produced in a beta decay reaction may interact in a distant detector as a muon or tau neutrino.

For each neutrino, there also exists a corresponding antiparticle, called an "antineutrino", which also has half-integer spin and no electric charge. They are distinguished from the neutrinos by having opposite signs of lepton number and chirality. To conserve total lepton number, in nuclear beta decay, electron neutrinos appear together with only positrons (anti-electrons) or electron-antineutrinos, and electron antineutrinos with electrons or electron neutrinos.

Neutrinos are created by various radioactive decays, including in beta decay of atomic nuclei or hadrons, nuclear reactions such as those that take place in the core of a star or artificially in nuclear reactors, nuclear bombs or particle accelerators, during a supernova, in the spin-down of a neutron star, or when accelerated particle beams or cosmic rays strike atoms. The majority of neutrinos in the vicinity of the Earth are from nuclear reactions in the Sun. In the vicinity of the Earth, about 65 billion () solar neutrinos per second pass through every square centimeter perpendicular to the direction of the Sun.

For study, neutrinos can be created artificially with nuclear reactors and particle accelerators. There is intense research activity involving neutrinos, with goals that include the determination of the three neutrino mass values, the measurement of the degree of CP violation in the leptonic sector (leading to leptogenesis); and searches for evidence of physics beyond the Standard Model of particle physics, such as neutrinoless double beta decay, which would be evidence for violation of lepton number conservation. Neutrinos can also be used for tomography of the interior of the earth.

The neutrino was postulated first by Wolfgang Pauli in 1930 to explain how beta decay could conserve energy, momentum, and angular momentum (spin). In contrast to Niels Bohr, who proposed a statistical version of the conservation laws to explain the observed continuous energy spectra in beta decay, Pauli hypothesized an undetected particle that he called a "neutron", using the same "-on" ending employed for naming both the proton and the electron. He considered that the new particle was emitted from the nucleus together with the electron or beta particle in the process of beta decay.

James Chadwick discovered a much more massive nuclear particle in 1932 and also named it a neutron, leaving two kinds of particles with the same name. Pauli earlier (in 1930) had used the term "neutron" for both the neutral particle that conserved energy in beta decay, and a presumed neutral particle in the nucleus, and initially did not consider these two neutral particles as distinct from each other. The word "neutrino" entered the scientific vocabulary through Enrico Fermi, who used it during a conference in Paris in July 1932 and at the Solvay Conference in October 1933, where Pauli also employed it. The name (the Italian equivalent of "little neutral one") was jokingly coined by Edoardo Amaldi during a conversation with Fermi at the Institute of physics of via Panisperna in Rome, in order to distinguish this light neutral particle from Chadwick's neutron.

In Fermi's theory of beta decay, Chadwick's large neutral particle could decay to a proton, electron, and the smaller neutral particle (flavored as an electron antineutrino):

Fermi's paper, written in 1934, unified Pauli's neutrino with Paul Dirac's positron and Werner Heisenberg's neutron–proton model and gave a solid theoretical basis for future experimental work. The journal Nature rejected Fermi's paper, saying that the theory was "too remote from reality". He submitted the paper to an Italian journal, which accepted it, but the general lack of interest in his theory at that early date caused him to switch to experimental physics.

By 1934 there was experimental evidence against Bohr's idea that energy conservation is invalid for beta decay. At the Solvay conference of that year, measurements of the energy spectra of beta particles (electrons) were reported, showing that there is a strict limit on the energy of electrons from each type of beta decay. Such a limit is not expected if the conservation of energy is invalid, in which case any amount of energy would be statistically available in at least a few decays. The natural explanation of the beta decay spectrum as first measured in 1934 was that only a limited (and conserved) amount of energy was available, and a new particle was sometimes taking a varying fraction of this limited energy, leaving the rest for the beta particle. Pauli made use of the occasion to publicly emphasize that the still-undetected "neutrino" must be an actual particle.

In 1942, Wang Ganchang first proposed the use of beta capture to experimentally detect neutrinos. In the 20 July 1956 issue of "Science", Clyde Cowan, Frederick Reines, F. B. Harrison, H. W. Kruse, and A. D. McGuire published confirmation that they had detected the neutrino, a result that was rewarded almost forty years later with the 1995 Nobel Prize.

In this experiment, now known as the Cowan–Reines neutrino experiment, antineutrinos created in a nuclear reactor by beta decay reacted with protons to produce neutrons and positrons:

The positron quickly finds an electron, and they annihilate each other. The two resulting gamma rays (γ) are detectable. The neutron can be detected by its capture on an appropriate nucleus, releasing a gamma ray. The coincidence of both events – positron annihilation and neutron capture – gives a unique signature of an antineutrino interaction.

The antineutrino discovered by Cowan and Reines is the antiparticle of the electron neutrino.

In 1962, Leon M. Lederman, Melvin Schwartz and Jack Steinberger showed that more than one type of neutrino exists by first detecting interactions of the muon neutrino (already hypothesised with the name "neutretto"), which earned them the 1988 Nobel Prize in Physics.

When the third type of lepton, the tau, was discovered in 1975 at the Stanford Linear Accelerator Center, it too was expected to have an associated neutrino (the tau neutrino). First evidence for this third neutrino type came from the observation of missing energy and momentum in tau decays analogous to the beta decay leading to the discovery of the electron neutrino. The first detection of tau neutrino interactions was announced in 2000 by the DONUT collaboration at Fermilab; its existence had already been inferred by both theoretical consistency and experimental data from the Large Electron–Positron Collider.

In the 1960s, the now-famous Homestake experiment made the first measurement of the flux of electron neutrinos arriving from the core of the Sun and found a value that was between one third and one half the number predicted by the Standard Solar Model. This discrepancy, which became known as the solar neutrino problem, remained unresolved for some thirty years, while possible problems with both the experiment and the solar model were investigated, but none could be found. Eventually it was realized that both were correct, but rather it was the neutrinos themselves that were far more interesting than expected. It was postulated that the three neutrinos had nonzero and slightly but indistinguishably different masses, and could therefore oscillate into undetectable flavors on their flight to the Earth. This hypothesis was investigated by a new series of experiments, thereby opening a new major field of research that still continues. Eventual confirmation of the phenomenon of neutrino oscillation led to two Nobel prizes, to Raymond Davis, Jr., who conceived and led the Homestake experiment, and to Art McDonald, who led the SNO experiment, which could detect all of the neutrino flavors and found no deficit.

A practical method for investigating neutrino oscillations was first suggested by Bruno Pontecorvo in 1957 using an analogy with kaon oscillations; over the subsequent 10 years he developed the mathematical formalism and the modern formulation of vacuum oscillations. In 1985 Stanislav Mikheyev and Alexei Smirnov (expanding on 1978 work by Lincoln Wolfenstein) noted that flavor oscillations can be modified when neutrinos propagate through matter. This so-called Mikheyev–Smirnov–Wolfenstein effect (MSW effect) is important to understand because many neutrinos emitted by fusion in the Sun pass through the dense matter in the solar core (where essentially all solar fusion takes place) on their way to detectors on Earth.

Starting in 1998, experiments began to show that solar and atmospheric neutrinos change flavors (see Super-Kamiokande and Sudbury Neutrino Observatory). This resolved the solar neutrino problem: the electron neutrinos produced in the Sun had partly changed into other flavors which the experiments could not detect.

Although individual experiments, such as the set of solar neutrino experiments, are consistent with non-oscillatory mechanisms of neutrino flavor conversion, taken altogether, neutrino experiments imply the existence of neutrino oscillations. Especially relevant in this context are the reactor experiment KamLAND and the accelerator experiments such as MINOS. The KamLAND experiment has indeed identified oscillations as the neutrino flavor conversion mechanism involved in the solar electron neutrinos. Similarly MINOS confirms the oscillation of atmospheric neutrinos and gives a better determination of the mass squared splitting. Takaaki Kajita of Japan and Arthur B. McDonald of Canada received the 2015 Nobel Prize for Physics for their landmark finding, theoretical and experimental, that neutrinos can change flavors.

Raymond Davis, Jr. and Masatoshi Koshiba were jointly awarded the 2002 Nobel Prize in Physics. Both conducted pioneering work on solar neutrino detection, and Koshiba's work also resulted in the first real-time observation of neutrinos from the SN 1987A supernova in the nearby Large Magellanic Cloud. These efforts marked the beginning of neutrino astronomy.

The neutrino has half-integer spin (½ħ) and is therefore a fermion. Also being leptons, neutrinos have been observed to interact through only the weak force, although it is assumed that they also interact gravitationally.

Weak interactions create neutrinos in one of three leptonic flavors: electron neutrinos (), muon neutrinos (), or tau neutrinos (), in association with the corresponding electron, muon, and tau charged leptons, respectively.

Although neutrinos were long believed to be massless, it is now known that there are also three discrete neutrino masses, but they don't correspond uniquely to the three flavors. Although only differences of squares of the three mass values are known as of 2016, experiments have shown that these masses are tiny in magnitude. From cosmological measurements, it has been calculated that the sum of the three neutrino masses must be less than one millionth that of the electron.

More formally, neutrino flavor eigenstates are not the same as the neutrino mass eigenstates (simply labelled 1, 2, 3). As of 2016, it is not known which of these three is the heaviest. In analogy with the mass hierarchy of the charged leptons, the configuration with mass being lighter than mass is conventionally called the "normal hierarchy", while in the "inverted hierarchy", the opposite would hold. Several major experimental efforts are underway to help establish which is correct.

A neutrino created in a specific flavor eigenstate is in an associated specific quantum superposition of all three mass eigenstates. This is possible because the three masses differ so little that they cannot be experimentally distinguished within any practical flight path, due to the uncertainty principle. The proportion of each mass state in the produced pure flavor state has been found to depend strongly on that flavor. The relationship between flavor and mass eigenstates is encoded in the PMNS matrix. Experiments have established values for the elements of this matrix.

The existence of a neutrino mass allows the possibility of a tiny neutrino magnetic moment, in which case neutrinos could interact electromagnetically as well; no such interaction has been discovered.

Neutrinos oscillate between different flavors in flight. For example, an electron neutrino produced in a beta decay reaction may interact in a distant detector as a muon or tau neutrino, as defined by the flavor of the charged lepton produced in the detector. This oscillation occurs because the three mass state components of the produced flavor travel at slightly different speeds, so that their quantum mechanical wave packets develop relative phase shifts that change how they combine to produce a varying superposition of three flavors. Each flavor component thereby oscillates sinusoidally as the neutrino travels, with the flavors varying in relative strengths. The relative flavor proportions when the neutrino interacts represent the relative probabilities for that flavor of interaction to produce the corresponding flavor of charged lepton.

There are other possibilities in which neutrino could oscillate even if they were massless. If Lorentz symmetry were not an exact symmetry, neutrinos could experience Lorentz-violating oscillations.

Neutrinos traveling through matter, in general, undergo a process analogous to light traveling through a transparent material. This process is not directly observable because it does not produce ionizing radiation, but gives rise to the MSW effect. Only a small fraction of the neutrino's energy is transferred to the material.

For each neutrino, there also exists a corresponding antiparticle, called an "antineutrino", which also has no electric charge and half-integer spin. They are distinguished from the neutrinos by having opposite signs of lepton number and opposite chirality. As of 2016, no evidence has been found for any other difference. In all observations so far of leptonic processes (despite extensive and continuing searches for exceptions), there is no overall change in lepton number; for example, if total lepton number is zero in the initial state, electron neutrinos appear in the final state together with only positrons (anti-electrons) or electron-antineutrinos, and electron antineutrinos with electrons or electron neutrinos.

Antineutrinos are produced in nuclear beta decay together with a beta particle, in which, e.g., a neutron decays into a proton, electron, and antineutrino. All antineutrinos observed thus far possess right-handed helicity (i.e. only one of the two possible spin states has ever been seen), while neutrinos are left-handed. Nevertheless, as neutrinos have mass, their helicity is frame-dependent, so it is the related frame-independent property of chirality that is relevant here.

Antineutrinos were first detected as a result of their interaction with protons in a large tank of water. This was installed next to a nuclear reactor as a controllable source of the antineutrinos (See: Cowan–Reines neutrino experiment).
Researchers around the world have begun to investigate the possibility of using antineutrinos for reactor monitoring in the context of preventing the proliferation of nuclear weapons.

Because antineutrinos and neutrinos are neutral particles, it is possible that they are the same particle. Particles that have this property are known as Majorana particles, after the Italian physicist Ettore Majorana who first proposed the concept. For the case of neutrinos this theory has gained popularity as it can be used, in combination with the seesaw mechanism, to explain why neutrino masses are so small compared to those of the other elementary particles, such as electrons or quarks. Majorana neutrinos have the property that the neutrino and antineutrino could be distinguished only by chirality; what experiments observe as a difference between the neutrino and antineutrino could simply be due to one particle with two possible chiralities.

It is not yet known whether neutrinos are Majorana or Dirac particles; it is possible to test this property experimentally. For example, if neutrinos are indeed Majorana particles, then lepton-number violating processes such as neutrinoless double beta decay would be allowed, while they would not if neutrinos are Dirac particles. Several experiments have been and are being conducted to search for this process, e.g. GERDA. The cosmic neutrino background is also a probe of whether neutrinos are Majorana particles, since there should be a different number of cosmic neutrinos detected in either the Dirac or Majorana case.

Neutrinos can interact with a nucleus, changing it to another nucleus. This process is used in radiochemical neutrino detectors. In this case, the energy levels and spin states within the target nucleus have to be taken into account to estimate the probability for an interaction. In general the interaction probability increases with the number of neutrons and protons within a nucleus.

It is very hard to uniquely identify neutrino interactions among the natural background of radioactivity. For this reason, in early experiments a special reaction channel was chosen to facilitate the identification: the interaction of an antineutrino with one of the hydrogen nuclei in the water molecules. A hydrogen nucleus is a single proton, so simultaneous nuclear interactions, which would occur within a heavier nucleus, don't need to be considered for the detection experiment. Within a cubic metre of water placed right outside a nuclear reactor, only relatively few such interactions can be recorded, but the setup is now used for measuring the reactor's plutonium production rate.

Very much like neutrons do in nuclear reactors, neutrinos can induce fission reactions within heavy nuclei. So far, this reaction has not been measured in a laboratory, but is predicted to happen within stars and supernovae. The process affects the abundance of isotopes seen in the universe. Neutrino fission of deuterium nuclei has been observed in the Sudbury Neutrino Observatory, which uses a heavy water detector.

Observations of the cosmic microwave background suggest that neutrinos do not interact with themselves.

There are three known types ("flavors") of neutrinos: electron neutrino , muon neutrino and tau neutrino , named after their partner leptons in the Standard Model (see table at right). The current best measurement of the number of neutrino types comes from observing the decay of the Z boson. This particle can decay into any light neutrino and its antineutrino, and the more types of light neutrinos available, the shorter the lifetime of the Z boson. Measurements of the Z lifetime have shown that the number of light neutrino flavors that couple to the Z is 3. The correspondence between the six quarks in the Standard Model and the six leptons, among them the three neutrinos, suggests to physicists' intuition that there should be exactly three types of neutrino. Proof that there are only three kinds of neutrinos remains an elusive goal of particle physics.

There are several active research areas involving the neutrino. Some are concerned with testing predictions of neutrino behavior. Other research is focused on measurement of unknown properties of neutrinos, especially their masses and CP violation, as they cannot be predicted with existing theories.

International scientific collaborations install large neutrino detectors near nuclear reactors or in neutrino beams from particle accelerators to better constrain the neutrino masses and the values for the magnitude and rates of oscillations between neutrino flavors. These experiments are thereby searching for the existence of CP violation in the neutrino sector; that is, whether or not the laws of physics treat neutrinos and antineutrinos differently.

The KATRIN experiment in Germany has begun to acquire data in June 2018 to determine the value of the mass of the electron neutrino, with other approaches to this problem in the planning stages.

On 19 July 2013, the results from the T2K experiment presented at the European Physical Society Conference on High Energy Physics in Stockholm, Sweden, confirmed neutrino oscillation theory.

Despite their tiny masses, neutrinos are so numerous that their gravitational force can influence other matter in the universe.

The three known neutrino flavors are the only established elementary particle candidates for dark matter, specifically hot dark matter, although that possibility appears to be largely ruled out by observations of the cosmic microwave background. If heavier sterile neutrinos exist, they might serve as warm dark matter, which still seems plausible.

Other efforts search for evidence of a sterile neutrino – a fourth neutrino flavor that does not interact with matter like the three known neutrino flavors. The possibility of "sterile" neutrinos is unaffected by the Z-boson decay measurements described above: If their mass is greater than half the Z-boson's mass, they would not be a decay product. Therefore, heavy sterile neutrinos would have a mass of at least 45.6 GeV.

The existence of such particles is in fact hinted by experimental data from the LSND experiment. On the other hand, the currently running MiniBooNE experiment suggested that sterile neutrinos are not required to explain the experimental data, although the latest research into this area is on-going and anomalies in the MiniBooNE data may allow for exotic neutrino types, including sterile neutrinos. A recent re-analysis of reference electron spectra data from the Institut Laue-Langevin has also hinted at a fourth, sterile neutrino.

According to an analysis published in 2010, data from the Wilkinson Microwave Anisotropy Probe of the cosmic background radiation is compatible with either three or four types of neutrinos.

There are also experiments searching for neutrinoless double-beta decay, which, if it exists, would violate lepton number conservation, and imply a minuscule splitting or difference between the physical masses of what are now conventionally called a “neutrino” and corresponding “antineutrino”, with opposite signs of lepton number.

If this were discovered the two could no longer be mutual antiparticles, and each of the resulting "six" distinct neutrinos would have no distinct antiparticle partner. Cosmic ray neutrino experiments detect neutrinos from space to study both the nature of neutrinos and the cosmic sources producing them.

Before neutrinos were found to oscillate, they were generally assumed to be massless, propagating at the speed of light. According to the theory of special relativity, the question of neutrino velocity is closely related to their mass: if neutrinos are massless, they must travel at the speed of light, and if they have mass they cannot reach the speed of light. Due to their tiny mass, the predicted speed is extremely close to the speed of light in all experiments, and current detectors are not sensitive to the expected difference.

Also some Lorentz-violating variants of quantum gravity might allow faster-than-light neutrinos. A comprehensive framework for Lorentz violations is the Standard-Model Extension (SME).

In the early 1980s, first measurements of neutrino speed were done using pulsed pion beams (produced by pulsed proton beams hitting a target). The pions decayed producing neutrinos, and the neutrino interactions observed within a time window in a detector at a distance were consistent with the speed of light. This measurement was repeated in 2007 using the MINOS detectors, which found the speed of neutrinos to be, at the 99% confidence level, in the range between and . The central value of 1.000051"c" is higher than the speed of light but is also consistent with a velocity of exactly "c" or even slightly less. This measurement set an upper bound on the mass of the muon neutrino of at 99% confidence. After the detectors for the project were upgraded in 2012, MINOS refined their initial result and found agreement with the speed of light, with the difference in the arrival time of neutrinos and light of −0.0006% (±0.0012%).

A similar observation was made, on a much larger scale, with supernova 1987A (SN 1987A). 10 MeV antineutrinos from the supernova were detected within a time window that was consistent with the speed of light for the neutrinos. So far, all measurements of neutrino speed have been consistent with the speed of light.

In September 2011, the OPERA collaboration released calculations showing velocities of 17 GeV and 28 GeV neutrinos exceeding the speed of light in their experiments (see Faster-than-light neutrino anomaly). In November 2011, OPERA repeated its experiment with changes so that the speed could be determined individually for each detected neutrino. The results showed the same faster-than-light speed. In February 2012, reports came out that the results may have been caused by a loose fiber optic cable attached to one of the atomic clocks which measured the departure and arrival times of the neutrinos. An independent recreation of the experiment in the same laboratory by ICARUS found no discernible difference between the speed of a neutrino and the speed of light.

In June 2012, CERN announced that new measurements conducted by all four Gran Sasso experiments (OPERA, ICARUS, Borexino and LVD) found agreement between the speed of light and the speed of neutrinos, finally refuting the initial OPERA claim.

The Standard Model of particle physics assumed that neutrinos are massless. The experimentally established phenomenon of neutrino oscillation, which mixes neutrino flavour states with neutrino mass states (analogously to CKM mixing), requires neutrinos to have nonzero masses. Massive neutrinos were originally conceived by Bruno Pontecorvo in the 1950s. Enhancing the basic framework to accommodate their mass is straightforward by adding a right-handed Lagrangian.

Providing for neutrino mass can be done in two ways, and some proposals use both:

The strongest upper limit on the masses of neutrinos comes from cosmology: the Big Bang model predicts that there is a fixed ratio between the number of neutrinos and the number of photons in the cosmic microwave background. If the total energy of all three types of neutrinos exceeded an average of per neutrino, there would be so much mass in the universe that it would collapse. This limit can be circumvented by assuming that the neutrino is unstable, but there are limits within the Standard Model that make this difficult. A much more stringent constraint comes from a careful analysis of cosmological data, such as the cosmic microwave background radiation, galaxy surveys, and the Lyman-alpha forest. These indicate that the summed masses of the three neutrinos must be less than .

The Nobel prize in Physics 2015 was awarded to both Takaaki Kajita and Arthur B. McDonald for their experimental discovery of neutrino oscillations, which demonstrates that neutrinos have mass.

In 1998, research results at the Super-Kamiokande neutrino detector determined that neutrinos can oscillate from one flavor to another, which requires that they must have a nonzero mass. While this shows that neutrinos have mass, the absolute neutrino mass scale is still not known. This is because neutrino oscillations are sensitive only to the difference in the squares of the masses. The best estimate of the difference in the squares of the masses of mass eigenstates 1 and 2 was published by KamLAND in 2005: |Δ"m"| = . In 2006, the MINOS experiment measured oscillations from an intense muon neutrino beam, determining the difference in the squares of the masses between neutrino mass eigenstates 2 and 3. The initial results indicate |Δ"m"| = 0.0027 eV, consistent with previous results from Super-Kamiokande. Since |Δ"m"| is the difference of two squared masses, at least one of them has to have a value which is at least the square root of this value. Thus, there exists at least one neutrino mass eigenstate with a mass of at least .

In 2009, lensing data of a galaxy cluster were analyzed to predict a neutrino mass of about . This surprisingly high value requires that the three neutrino masses be nearly equal, with neutrino oscillations on the order of "milli" electron-Volts. In 2016 this was updated to a mass of . It predicts 3 sterile neutrinos of the same mass, stems with the Planck dark matter fraction and the non-observation of neutrinoless double beta decay. The masses lie below the Mainz-Troitsk upper bound of for the electron antineutrino. The latter is being tested since June 2018 in the KATRIN experiment, that searches for a mass between and .

A number of efforts are under way to directly determine the absolute neutrino mass scale in laboratory experiments. The methods applied involve nuclear beta decay (KATRIN and MARE).

On 31 May 2010, OPERA researchers observed the first tau neutrino candidate event in a muon neutrino beam, the first time this transformation in neutrinos had been observed, providing further evidence that they have mass.

In July 2010, the 3-D MegaZ DR7 galaxy survey reported that they had measured a limit of the combined mass of the three neutrino varieties to be less than . A tighter upper bound yet for this sum of masses, , was reported in March 2013 by the Planck collaboration, whereas a February 2014 result estimates the sum as 0.320 ± 0.081 eV based on discrepancies between the cosmological consequences implied by Planck's detailed measurements of the Cosmic Microwave Background and predictions arising from observing other phenomena, combined with the assumption that neutrinos are responsible for the observed weaker gravitational lensing than would be expected from massless neutrinos.

If the neutrino is a Majorana particle, the mass may be calculated by finding the half-life of neutrinoless double-beta decay of certain nuclei. The current lowest upper limit on the Majorana mass of the neutrino has been set by KamLAND-Zen: 0.060–0.161 eV.

Standard Model neutrinos are fundamental point-like particles, without any width or volume. Since the neutrino is an elementary particle it does not have a size in the same sense as everyday objects. An effective size can be defined using their electroweak cross section (apparent size in electroweak interaction). The characteristic areas for the electroweak interaction are measured in units called nanobarns (nb) which are 10 cm² or 10 m², roughly the area of a disc a little more than 0.3 attometer in diameter, or about 1 billionth of the size of a uranium nucleus. The electron neutrino cross section is 3.2  nanobarns the muon neutrino cross section is 1.7 nanobarns, and the tau neutrino 1.0 nanobarn. These scattering cross sections depend on no other properties than the masses of the corresponding charged leptons. This size is relevant only to the probability of scattering. Properties associated with conventional "size" are absent: neutrinos cannot be condensed to form a separate uniform substance and they have no minimal distance between them.

Experimental results show that (nearly) all produced and observed neutrinos have left-handed helicities (spins antiparallel to momenta), and all antineutrinos have right-handed helicities, within the margin of error. In the massless limit, it means that only one of two possible chiralities is observed for either particle. These are the only chiralities included in the Standard Model of particle interactions.

It is possible that their counterparts (right-handed neutrinos and left-handed antineutrinos) simply do not exist. If they do, their properties are substantially different from observable neutrinos and antineutrinos. It is theorized that they are either very heavy (on the order of GUT scale—see "Seesaw mechanism"), do not participate in weak interaction (so-called "sterile neutrinos"), or both.

The existence of nonzero neutrino masses somewhat complicates the situation. Neutrinos are produced in weak interactions as chirality eigenstates. Chirality of a massive particle is not a constant of motion; helicity is, but the chirality operator does not share eigenstates with the helicity operator. Free neutrinos propagate as mixtures of left- and right-handed helicity states, with mixing amplitudes on the order of "m"/"E". This does not significantly affect the experiments, because neutrinos involved are nearly always ultrarelativistic, and thus mixing amplitudes are vanishingly small. Effectively, they travel so quickly and time passes so slowly in their rest-frames that they do not have enough time to change over any observable path. For example, most solar neutrinos have energies on the order of –, so the fraction of neutrinos with "wrong" helicity among them cannot exceed .

An unexpected series of experimental results for the rate of decay of heavy highly charged radioactive ions circulating in a storage ring has provoked theoretical activity in an effort to find a convincing explanation. The rates of weak decay of two radioactive species with half lives of about 40 s and 200 s are found to have a significant oscillatory modulation, with a period of about 7 s.
The observed phenomenon is known as the GSI anomaly, as the storage ring is a facility at the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt Germany. As the decay process produces an electron neutrino, some of the proposed explanations for the observed oscillation rate invoke neutrino properties. Initial ideas related to flavour oscillation were met with skepticism. A more recent proposal involves mass differences between neutrino mass eigenstates.

Nuclear reactors are the major source of human-generated neutrinos. The majority of energy in a nuclear reactor is generated by fission (the four main fissile isotopes in nuclear reactors are , , and ), the resultant neutron-rich daughter nuclides rapidly undergo additional beta decays, each converting one neutron to a proton and an electron and releasing an electron antineutrino (). Including these subsequent decays, the average nuclear fission releases about of energy, of which roughly 95.5% is retained in the core as heat, and roughly 4.5% (or about ) is radiated away as antineutrinos. For a typical nuclear reactor with a thermal power of , the total power production from fissioning atoms is actually , of which is radiated away as antineutrino radiation and never appears in the engineering. This is to say, of fission energy is "lost" from this reactor and does not appear as heat available to run turbines, since antineutrinos penetrate all building materials practically without interaction.

The antineutrino energy spectrum depends on the degree to which the fuel is burned (plutonium-239 fission antineutrinos on average have slightly more energy than those from uranium-235 fission), but in general, the "detectable" antineutrinos from fission have a peak energy between about 3.5 and , with a maximum energy of about . There is no established experimental method to measure the flux of low-energy antineutrinos. Only antineutrinos with an energy above threshold of can trigger inverse beta decay and thus be unambiguously identified (see below). An estimated 3% of all antineutrinos from a nuclear reactor carry an energy above this threshold. Thus, an average nuclear power plant may generate over antineutrinos per second above this threshold, but also a much larger number (97%/3% ≈ 30 times this number) below the energy threshold, which cannot be seen with present detector technology. The ND280 detector has been proposed as a viable safeguard unit.

Some particle accelerators have been used to make neutrino beams. The technique is to collide protons with a fixed target, producing charged pions or kaons. These unstable particles are then magnetically focused into a long tunnel where they decay while in flight. Because of the relativistic boost of the decaying particle, the neutrinos are produced as a beam rather than isotropically. Efforts to construct an accelerator facility where neutrinos are produced through muon decays are ongoing. Such a setup is generally known as a neutrino factory.

Nuclear weapons also produce very large quantities of neutrinos. Fred Reines and Clyde Cowan considered the detection of neutrinos from a bomb prior to their search for reactor neutrinos; a fission reactor was recommended as a better alternative by Los Alamos physics division leader J.M.B. Kellogg. Fission weapons produce antineutrinos (from the fission process), and fusion weapons produce both neutrinos (from the fusion process) and antineutrinos (from the initiating fission explosion).

Neutrinos are produced together with the natural background radiation. In particular, the decay chains of and isotopes, as well as, include beta decays which emit antineutrinos. These so-called geoneutrinos can provide valuable information on the Earth's interior. A first indication for geoneutrinos was found by the KamLAND experiment in 2005, updated results have been presented by KamLAND and Borexino. The main background in the geoneutrino measurements are the antineutrinos coming from reactors.
Atmospheric neutrinos result from the interaction of cosmic rays with atomic nuclei in the Earth's atmosphere, creating showers of particles, many of which are unstable and produce neutrinos when they decay. A collaboration of particle physicists from Tata Institute of Fundamental Research (India), Osaka City University (Japan) and Durham University (UK) recorded the first cosmic ray neutrino interaction in an underground laboratory in Kolar Gold Fields in India in 1965.

Solar neutrinos originate from the nuclear fusion powering the Sun and other stars.
The details of the operation of the Sun are explained by the Standard Solar Model. In short: when four protons fuse to become one helium nucleus, two of them have to convert into neutrons, and each such conversion releases one electron neutrino.

The Sun sends enormous numbers of neutrinos in all directions. Each second, about 65 billion () solar neutrinos pass through every square centimeter on the part of the Earth orthogonal to the direction of the Sun. Since neutrinos are insignificantly absorbed by the mass of the Earth, the surface area on the side of the Earth opposite the Sun receives about the same number of neutrinos as the side facing the Sun.

In 1966, Colgate and White
calculated that neutrinos carry away most of the gravitational energy released by the collapse of massive stars, events now categorized as Type Ib and Ic and Type II supernovae. When such stars collapse, matter densities at the core become so high () that the degeneracy of electrons is not enough to prevent protons and electrons from combining to form a neutron and an electron neutrino. A second and more important neutrino source is the thermal energy (100 billion kelvins) of the newly formed neutron core, which is dissipated via the formation of neutrino–antineutrino pairs of all flavors.

Colgate and White's theory of supernova neutrino production was confirmed in 1987, when neutrinos from Supernova 1987A were detected. The water-based detectors Kamiokande II and IMB detected 11 and 8 antineutrinos (lepton number = −1) of thermal origin, respectively, while the scintillator-based Baksan detector found 5 neutrinos (lepton number = +1) of either thermal or electron-capture origin, in a burst less than 13 seconds long. The neutrino signal from the supernova arrived at earth several hours before the arrival of the first electromagnetic radiation, as expected from the evident fact that the latter emerges along with the shock wave. The exceptionally feeble interaction with normal matter allowed the neutrinos to pass through the churning mass of the exploding star, while the electromagnetic photons were slowed.

Because neutrinos interact so little with matter, it is thought that a supernova's neutrino emissions carry information about the innermost regions of the explosion. Much of the "visible" light comes from the decay of radioactive elements produced by the supernova shock wave, and even light from the explosion itself is scattered by dense and turbulent gases, and thus delayed. The neutrino burst is expected to reach Earth before any electromagnetic waves, including visible light, gamma rays, or radio waves. The exact time delay of the electromagnetic waves' arrivals depends on the velocity of the shock wave and on the thickness of the outer layer of the star. For a Type II supernova, astronomers expect the neutrino flood to be released seconds after the stellar core collapse, while the first electromagnetic signal may emerge hours later, after the explosion shock wave has had time to reach the surface of the star. The Supernova Early Warning System project uses a network of neutrino detectors to monitor the sky for candidate supernova events; the neutrino signal will provide a useful advance warning of a star exploding in the Milky Way.

Although neutrinos pass through the outer gases of a supernova without scattering, they provide information about the deeper supernova core with evidence that here, even neutrinos scatter to a significant extent. In a supernova core the densities are those of a neutron star (which is expected to be formed in this type of supernova), becoming large enough to influence the duration of the neutrino signal by delaying some neutrinos. The 13 second-long neutrino signal from SN 1987A lasted far longer than it would take for unimpeded neutrinos to cross through the neutrino-generating core of a supernova, expected to be only 3200 kilometers in diameter for SN 1987A.

The number of neutrinos counted was also consistent with a total neutrino energy of , which was estimated to be nearly all of the total energy of the supernova.

For an average supernova, approximately 10 (an octodecillion) neutrinos are released, but the actual number detected at a terrestrial detector formula_1 will be far smaller, at the level of

formula_2,

where formula_3 is the mass of the detector (with e.g. Super Kamiokande having a mass of 50 kton) and formula_4 is the distance to the supernova. Hence in practice it will only be possible to detect neutrino bursts from supernovae within or nearby the Milky Way (our own galaxy). In addition to the detection of neutrinos from individual supernovae, it should also be possible to detect the diffuse supernova neutrino background, which originates from all supernovae in the Universe.

The energy of supernova neutrinos ranges from a few to several tens of MeV. The sites where cosmic rays are accelerated are expected to produce neutrinos that are at least one million times more energetic, produced from turbulent gaseous environments left over by supernova explosions: the supernova remnants. The origin of the cosmic rays was attributed to supernovas by Walter Baade and Fritz Zwicky; this hypothesis was refined by Vitaly L. Ginzburg and Sergei I. Syrovatsky who attributed the origin to supernova remnants, and supported their claim by the crucial remark, that the cosmic ray losses of the Milky Way is compensated, if the efficiency of acceleration in supernova remnants is about 10 percent. Ginzburg and Syrovatskii's hypothesis is supported by the specific mechanism of "shock wave acceleration" happening in supernova remnants, which is consistent with the original theoretical picture drawn by Enrico Fermi, and is receiving support from observational data. The very-high-energy neutrinos are still to be seen, but this branch of neutrino astronomy is just in its infancy. The main existing or forthcoming experiments that aim at observing very-high-energy neutrinos from our galaxy are Baikal, AMANDA, IceCube, ANTARES, NEMO and Nestor. Related information is provided by very-high-energy gamma ray observatories, such as VERITAS, HESS and MAGIC. Indeed, the collisions of cosmic rays are supposed to produce charged pions, whose decay give the neutrinos, and also neutral pions, whose decay give gamma rays: the environment of a supernova remnant is transparent to both types of radiation.

Still-higher-energy neutrinos, resulting from the interactions of extragalactic cosmic rays, could be observed with the Pierre Auger Observatory or with the dedicated experiment named ANITA.

It is thought that, just like the cosmic microwave background radiation left over from the Big Bang, there is a background of low-energy neutrinos in our Universe. In the 1980s it was proposed that these may be the explanation for the dark matter thought to exist in the universe. Neutrinos have one important advantage over most other dark matter candidates: it is known that they exist. This idea also has serious problems.

From particle experiments, it is known that neutrinos are very light. This means that they easily move at speeds close to the speed of light. For this reason, dark matter made from neutrinos is termed "hot dark matter". The problem is that being fast moving, the neutrinos would tend to have spread out evenly in the universe before cosmological expansion made them cold enough to congregate in clumps. This would cause the part of dark matter made of neutrinos to be smeared out and unable to cause the large galactic structures that we see.

These same galaxies and groups of galaxies appear to be surrounded by dark matter that is not fast enough to escape from those galaxies. Presumably this matter provided the gravitational nucleus for formation. This implies that neutrinos cannot make up a significant part of the total amount of dark matter.

From cosmological arguments, relic background neutrinos are estimated to have density of 56 of each type per cubic centimeter and temperature () if they are massless, much colder if their mass exceeds . Although their density is quite high, they have not yet been observed in the laboratory, as their energy is below thresholds of most detection methods, and due to extremely low neutrino interaction cross-sections at sub-eV energies. In contrast, boron-8 solar neutrinos—which are emitted with a higher energy—have been detected definitively despite having a space density that is lower than that of relic neutrinos by some 6 orders of magnitude.

Neutrinos cannot be detected directly, because they do not ionize the materials they are passing through (they do not carry electric charge and other proposed effects, like the MSW effect, do not produce traceable radiation). A unique reaction to identify antineutrinos, sometimes referred to as inverse beta decay, as applied by Reines and Cowan (see below), requires a very large detector to detect a significant number of neutrinos. All detection methods require the neutrinos to carry a minimum threshold energy. So far, there is no detection method for low-energy neutrinos, in the sense that potential neutrino interactions (for example by the MSW effect) cannot be uniquely distinguished from other causes. Neutrino detectors are often built underground to isolate the detector from cosmic rays and other background radiation.

Antineutrinos were first detected in the 1950s near a nuclear reactor. Reines and Cowan used two targets containing a solution of cadmium chloride in water. Two scintillation detectors were placed next to the cadmium targets. Antineutrinos with an energy above the threshold of caused charged current interactions with the protons in the water, producing positrons and neutrons. This is very much like decay, where energy is used to convert a proton into a neutron, a positron () and an electron neutrino () is emitted:

From known decay:

In the Cowan and Reines experiment, instead of an outgoing neutrino, you have an incoming antineutrino () from a nuclear reactor:

The resulting positron annihilation with electrons in the detector material created photons with an energy of about . Pairs of photons in coincidence could be detected by the two scintillation detectors above and below the target. The neutrons were captured by cadmium nuclei resulting in gamma rays of about that were detected a few microseconds after the photons from a positron annihilation event.

Since then, various detection methods have been used. Super Kamiokande is a large volume of water surrounded by photomultiplier tubes that watch for the Cherenkov radiation emitted when an incoming neutrino creates an electron or muon in the water. The Sudbury Neutrino Observatory is similar, but uses heavy water as the detecting medium, which uses the same effects, but also allows the additional reaction any-flavor neutrino photo-dissociation of deuterium, resulting in a free neutron which is then detected from gamma radiation after chlorine-capture. Other detectors have consisted of large volumes of chlorine or gallium which are periodically checked for excesses of argon or germanium, respectively, which are created by electron-neutrinos interacting with the original substance. MINOS used a solid plastic scintillator coupled to photomultiplier tubes, while Borexino uses a liquid pseudocumene scintillator also watched by photomultiplier tubes and the NOνA detector uses liquid scintillator watched by avalanche photodiodes. The IceCube Neutrino Observatory uses of the Antarctic ice sheet near the south pole with photomultiplier tubes distributed throughout the volume.
The University of Liverpool ND280 detector employs the novel use of gadolinium encased light detectors in a temperature controlled magnetic field capturing double light pulse events. The T2K experiment developed the technology and practical experiments were successful in both Japan and at Wylfa power station.

Neutrinos' low mass and neutral charge mean they interact exceedingly weakly with other particles and fields. This feature of weak interaction interests scientists because it means neutrinos can be used to probe environments that other radiation (such as light or radio waves) cannot penetrate.

Using neutrinos as a probe was first proposed in the mid-20th century as a way to detect conditions at the core of the Sun. The solar core cannot be imaged directly because electromagnetic radiation (such as light) is diffused by the great amount and density of matter surrounding the core. On the other hand, neutrinos pass through the Sun with few interactions. Whereas photons emitted from the solar core may require 40,000 years to diffuse to the outer layers of the Sun, neutrinos generated in stellar fusion reactions at the core cross this distance practically unimpeded at nearly the speed of light.

Neutrinos are also useful for probing astrophysical sources beyond the Solar System because they are the only known particles that are not significantly attenuated by their travel through the interstellar medium. Optical photons can be obscured or diffused by dust, gas, and background radiation. High-energy cosmic rays, in the form of swift protons and atomic nuclei, are unable to travel more than about 100 megaparsecs due to the Greisen–Zatsepin–Kuzmin limit (GZK cutoff). Neutrinos, in contrast, can travel even greater distances barely attenuated.

The galactic core of the Milky Way is fully obscured by dense gas and numerous bright objects. Neutrinos produced in the galactic core might be measurable by Earth-based neutrino telescopes.

Another important use of the neutrino is in the observation of supernovae, the explosions that end the lives of highly massive stars. The core collapse phase of a supernova is an extremely dense and energetic event. It is so dense that no known particles are able to escape the advancing core front except for neutrinos. Consequently, supernovae are known to release approximately 99% of their radiant energy in a short (10-second) burst of neutrinos. These neutrinos are a very useful probe for core collapse studies.

The rest mass of the neutrino is an important test of cosmological and astrophysical theories (see "Dark matter"). The neutrino's significance in probing cosmological phenomena is as great as any other method, and is thus a major focus of study in astrophysical communities.

The study of neutrinos is important in particle physics because neutrinos typically have the lowest mass, and hence are examples of the lowest-energy particles theorized in extensions of the Standard Model of particle physics.

In November 2012, American scientists used a particle accelerator to send a coherent neutrino message through 780 feet of rock. This marks the first use of neutrinos for communication, and future research may permit binary neutrino messages to be sent immense distances through even the densest materials, such as the Earth's core.

In July 2018, the IceCube Neutrino Observatory announced that they have traced an extremely-high-energy neutrino that hit their Antarctica-based research station in September 2017 back to its point of origin in the blazar TXS 0506 +056 located 3.7 billion light-years away in the direction of the constellation Orion. This is the first time that a neutrino detector has been used to locate an object in space and that a source of cosmic rays has been identified.





</doc>
<doc id="21488" url="https://en.wikipedia.org/wiki?curid=21488" title="Nanotechnology">
Nanotechnology

Nanotechnology ("nanotech") is manipulation of matter on an atomic, molecular, and supramolecular scale. The earliest, widespread description of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabrication of macroscale products, also now referred to as molecular nanotechnology. A more generalized description of nanotechnology was subsequently established by the National Nanotechnology Initiative, which defines nanotechnology as the manipulation of matter with at least one dimension sized from 1 to 100 nanometers. This definition reflects the fact that quantum mechanical effects are important at this quantum-realm scale, and so the definition shifted from a particular technological goal to a research category inclusive of all types of research and technologies that deal with the special properties of matter which occur below the given size threshold. It is therefore common to see the plural form "nanotechnologies" as well as "nanoscale technologies" to refer to the broad range of research and applications whose common trait is size. Because of the variety of potential applications (including industrial and military), governments have invested billions of dollars in nanotechnology research. Through 2012, the USA has invested $3.7 billion using its National Nanotechnology Initiative, the European Union has invested $1.2 billion, and Japan has invested $750 million.

Nanotechnology as defined by size is naturally very broad, including fields of science as diverse as surface science, organic chemistry, molecular biology, semiconductor physics, energy storage, microfabrication, molecular engineering, etc. The associated research and applications are equally diverse, ranging from extensions of conventional device physics to completely new approaches based upon molecular self-assembly, from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale.

Scientists currently debate the future implications of nanotechnology. Nanotechnology may be able to create many new materials and devices with a vast range of applications, such as in nanomedicine, nanoelectronics, biomaterials energy production, and consumer products. On the other hand, nanotechnology raises many of the same issues as any new technology, including concerns about the toxicity and environmental impact of nanomaterials, and their potential effects on global economics, as well as speculation about various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted.

The concepts that seeded nanotechnology were first discussed in 1959 by renowned physicist Richard Feynman in his talk "There's Plenty of Room at the Bottom", in which he described the possibility of synthesis via direct manipulation of atoms. The term "nano-technology" was first used by Norio Taniguchi in 1974, though it was not widely known.

Inspired by Feynman's concepts, K. Eric Drexler used the term "nanotechnology" in his 1986 book "Engines of Creation: The Coming Era of Nanotechnology", which proposed the idea of a nanoscale "assembler" which would be able to build a copy of itself and of other items of arbitrary complexity with atomic control. Also in 1986, Drexler co-founded The Foresight Institute (with which he is no longer affiliated) to help increase public awareness and understanding of nanotechnology concepts and implications.

Thus, emergence of nanotechnology as a field in the 1980s occurred through convergence of Drexler's theoretical and public work, which developed and popularized a conceptual framework for nanotechnology, and high-visibility experimental advances that drew additional wide-scale attention to the prospects of atomic control of matter. In the 1980s, two major breakthroughs sparked the growth of nanotechnology in modern era.

First, the invention of the scanning tunneling microscope in 1981 which provided unprecedented visualization of individual atoms and bonds, and was successfully used to manipulate individual atoms in 1989. The microscope's developers Gerd Binnig and Heinrich Rohrer at IBM Zurich Research Laboratory received a Nobel Prize in Physics in 1986. Binnig, Quate and Gerber also invented the analogous atomic force microscope that year.

Second, Fullerenes were discovered in 1985 by Harry Kroto, Richard Smalley, and Robert Curl, who together won the 1996 Nobel Prize in Chemistry. C was not initially described as nanotechnology; the term was used regarding subsequent work with related graphene tubes (called carbon nanotubes and sometimes called Bucky tubes) which suggested potential applications for nanoscale electronics and devices.

In the early 2000s, the field garnered increased scientific, political, and commercial attention that led to both controversy and progress. Controversies emerged regarding the definitions and potential implications of nanotechnologies, exemplified by the Royal Society's report on nanotechnology. Challenges were raised regarding the feasibility of applications envisioned by advocates of molecular nanotechnology, which culminated in a public debate between Drexler and Smalley in 2001 and 2003.

Meanwhile, commercialization of products based on advancements in nanoscale technologies began emerging. These products are limited to bulk applications of nanomaterials and do not involve atomic control of matter. Some examples include the Silver Nano platform for using silver nanoparticles as an antibacterial agent, nanoparticle-based transparent sunscreens, carbon fiber strengthening using silica nanoparticles, and carbon nanotubes for stain-resistant textiles.

Governments moved to promote and fund research into nanotechnology, such as in the U.S. with the National Nanotechnology Initiative, which formalized a size-based definition of nanotechnology and established funding for research on the nanoscale, and in Europe via the European Framework Programmes for Research and Technological Development.

By the mid-2000s new and serious scientific attention began to flourish. Projects emerged to produce nanotechnology roadmaps which center on atomically precise manipulation of matter and discuss existing and projected capabilities, goals, and applications.

Nanotechnology is the engineering of functional systems at the molecular scale. This covers both current work and concepts that are more advanced. In its original sense, nanotechnology refers to the projected ability to construct items from the bottom up, using techniques and tools being developed today to make complete, high performance products.

One nanometer (nm) is one billionth, or 10, of a meter. By comparison, typical carbon-carbon bond lengths, or the spacing between these atoms in a molecule, are in the range , and a DNA double-helix has a diameter around 2 nm. On the other hand, the smallest cellular life-forms, the bacteria of the genus "Mycoplasma", are around 200 nm in length. By convention, nanotechnology is taken as the scale range following the definition used by the National Nanotechnology Initiative in the US. The lower limit is set by the size of atoms (hydrogen has the smallest atoms, which are approximately a quarter of a nm kinetic diameter) since nanotechnology must build its devices from atoms and molecules. The upper limit is more or less arbitrary but is around the size below which phenomena not observed in larger structures start to become apparent and can be made use of in the nano device. These new phenomena make nanotechnology distinct from devices which are merely miniaturised versions of an equivalent macroscopic device; such devices are on a larger scale and come under the description of microtechnology.

To put that scale in another context, the comparative size of a nanometer to a meter is the same as that of a marble to the size of the earth. Or another way of putting it: a nanometer is the amount an average man's beard grows in the time it takes him to raise the razor to his face.

Two main approaches are used in nanotechnology. In the "bottom-up" approach, materials and devices are built from molecular components which assemble themselves chemically by principles of molecular recognition. In the "top-down" approach, nano-objects are constructed from larger entities without atomic-level control.

Areas of physics such as nanoelectronics, nanomechanics, nanophotonics and nanoionics have evolved during the last few decades to provide a basic scientific foundation of nanotechnology.

Several phenomena become pronounced as the size of the system decreases. These include statistical mechanical effects, as well as quantum mechanical effects, for example the "quantum size effect" where the electronic properties of solids are altered with great reductions in particle size. This effect does not come into play by going from macro to micro dimensions. However, quantum effects can become significant when the nanometer size range is reached, typically at distances of 100 nanometers or less, the so-called quantum realm. Additionally, a number of physical (mechanical, electrical, optical, etc.) properties change when compared to macroscopic systems. One example is the increase in surface area to volume ratio altering mechanical, thermal and catalytic properties of materials. Diffusion and reactions at nanoscale, nanostructures materials and nanodevices with fast ion transport are generally referred to nanoionics. "Mechanical" properties of nanosystems are of interest in the nanomechanics research. The catalytic activity of nanomaterials also opens potential risks in their interaction with biomaterials.

Materials reduced to the nanoscale can show different properties compared to what they exhibit on a macroscale, enabling unique applications. For instance, opaque substances can become transparent (copper); stable materials can turn combustible (aluminium); insoluble materials may become soluble (gold). A material such as gold, which is chemically inert at normal scales, can serve as a potent chemical catalyst at nanoscales. Much of the fascination with nanotechnology stems from these quantum and surface phenomena that matter exhibits at the nanoscale.

Modern synthetic chemistry has reached the point where it is possible to prepare small molecules to almost any structure. These methods are used today to manufacture a wide variety of useful chemicals such as pharmaceuticals or commercial polymers. This ability raises the question of extending this kind of control to the next-larger level, seeking methods to assemble these single molecules into supramolecular assemblies consisting of many molecules arranged in a well defined manner.

These approaches utilize the concepts of molecular self-assembly and/or supramolecular chemistry to automatically arrange themselves into some useful conformation through a bottom-up approach. The concept of molecular recognition is especially important: molecules can be designed so that a specific configuration or arrangement is favored due to non-covalent intermolecular forces. The Watson–Crick basepairing rules are a direct result of this, as is the specificity of an enzyme being targeted to a single substrate, or the specific folding of the protein itself. Thus, two or more components can be designed to be complementary and mutually attractive so that they make a more complex and useful whole.

Such bottom-up approaches should be capable of producing devices in parallel and be much cheaper than top-down methods, but could potentially be overwhelmed as the size and complexity of the desired assembly increases. Most useful structures require complex and thermodynamically unlikely arrangements of atoms. Nevertheless, there are many examples of self-assembly based on molecular recognition in biology, most notably Watson–Crick basepairing and enzyme-substrate interactions. The challenge for nanotechnology is whether these principles can be used to engineer new constructs in addition to natural ones.

Molecular nanotechnology, sometimes called molecular manufacturing, describes engineered nanosystems (nanoscale machines) operating on the molecular scale. Molecular nanotechnology is especially associated with the molecular assembler, a machine that can produce a desired structure or device atom-by-atom using the principles of mechanosynthesis. Manufacturing in the context of productive nanosystems is not related to, and should be clearly distinguished from, the conventional technologies used to manufacture nanomaterials such as carbon nanotubes and nanoparticles.

When the term "nanotechnology" was independently coined and popularized by Eric Drexler (who at the time was unaware of an earlier usage by Norio Taniguchi) it referred to a future manufacturing technology based on molecular machine systems. The premise was that molecular scale biological analogies of traditional machine components demonstrated molecular machines were possible: by the countless examples found in biology, it is known that sophisticated, stochastically optimised biological machines can be produced.

It is hoped that developments in nanotechnology will make possible their construction by some other means, perhaps using biomimetic principles. However, Drexler and other researchers have proposed that advanced nanotechnology, although perhaps initially implemented by biomimetic means, ultimately could be based on mechanical engineering principles, namely, a manufacturing technology based on the mechanical functionality of these components (such as gears, bearings, motors, and structural members) that would enable programmable, positional assembly to atomic specification. The physics and engineering performance of exemplar designs were analyzed in Drexler's book "Nanosystems".

In general it is very difficult to assemble devices on the atomic scale, as one has to position atoms on other atoms of comparable size and stickiness. Another view, put forth by Carlo Montemagno, is that future nanosystems will be hybrids of silicon technology and biological molecular machines. Richard Smalley argued that mechanosynthesis are impossible due to the difficulties in mechanically manipulating individual molecules.

This led to an exchange of letters in the ACS publication Chemical & Engineering News in 2003. Though biology clearly demonstrates that molecular machine systems are possible, non-biological molecular machines are today only in their infancy. Leaders in research on non-biological molecular machines are Dr. Alex Zettl and his colleagues at Lawrence Berkeley Laboratories and UC Berkeley. They have constructed at least three distinct molecular devices whose motion is controlled from the desktop with changing voltage: a nanotube nanomotor, a molecular actuator, and a nanoelectromechanical relaxation oscillator. See nanotube nanomotor for more examples.

An experiment indicating that positional molecular assembly is possible was performed by Ho and Lee at Cornell University in 1999. They used a scanning tunneling microscope to move an individual carbon monoxide molecule (CO) to an individual iron atom (Fe) sitting on a flat silver crystal, and chemically bound the CO to the Fe by applying a voltage.

The nanomaterials field includes subfields which develop or study materials having unique properties arising from their nanoscale dimensions.

These seek to arrange smaller components into more complex assemblies.

These seek to create smaller devices by using larger ones to direct their assembly.

These seek to develop components of a desired functionality without regard to how they might be assembled.


These subfields seek to anticipate what inventions nanotechnology might yield, or attempt to propose an agenda along which inquiry might progress. These often take a big-picture view of nanotechnology, with more emphasis on its societal implications than the details of how such inventions could actually be created.

Nanomaterials can be classified in 0D, 1D, 2D and 3D nanomaterials. The dimensionality play a major role in determining the characteristic of nanomaterials including physical, chemical and biological characteristics. With the decrease in dimensionality, an increase in surface-to-volume ratio is observed. This indicate that smaller dimensional nanomaterials have higher surface area compared to 3D nanomaterials. Recently, two dimensional (2D) nanomaterials are extensively investigated for electronic, biomedical, drug delivery and biosensor applications.

There are several important modern developments. The atomic force microscope (AFM) and the Scanning Tunneling Microscope (STM) are two early versions of scanning probes that launched nanotechnology. There are other types of scanning probe microscopy. Although conceptually similar to the scanning confocal microscope developed by Marvin Minsky in 1961 and the scanning acoustic microscope (SAM) developed by Calvin Quate and coworkers in the 1970s, newer scanning probe microscopes have much higher resolution, since they are not limited by the wavelength of sound or light.

The tip of a scanning probe can also be used to manipulate nanostructures (a process called positional assembly). Feature-oriented scanning methodology may be a promising way to implement these nanomanipulations in automatic mode. However, this is still a slow process because of low scanning velocity of the microscope.

Various techniques of nanolithography such as optical lithography, X-ray lithography, dip pen nanolithography, electron beam lithography or nanoimprint lithography were also developed. Lithography is a top-down fabrication technique where a bulk material is reduced in size to nanoscale pattern.

Another group of nanotechnological techniques include those used for fabrication of nanotubes and nanowires, those used in semiconductor fabrication such as deep ultraviolet lithography, electron beam lithography, focused ion beam machining, nanoimprint lithography, atomic layer deposition, and molecular vapor deposition, and further including molecular self-assembly techniques such as those employing di-block copolymers. The precursors of these techniques preceded the nanotech era, and are extensions in the development of scientific advancements rather than techniques which were devised with the sole purpose of creating nanotechnology and which were results of nanotechnology research.

The top-down approach anticipates nanodevices that must be built piece by piece in stages, much as manufactured items are made. Scanning probe microscopy is an important technique both for characterization and synthesis of nanomaterials. Atomic force microscopes and scanning tunneling microscopes can be used to look at surfaces and to move atoms around. By designing different tips for these microscopes, they can be used for carving out structures on surfaces and to help guide self-assembling structures. By using, for example, feature-oriented scanning approach, atoms or molecules can be moved around on a surface with scanning probe microscopy techniques. At present, it is expensive and time-consuming for mass production but very suitable for laboratory experimentation.

In contrast, bottom-up techniques build or grow larger structures atom by atom or molecule by molecule. These techniques include chemical synthesis, self-assembly and positional assembly. Dual polarisation interferometry is one tool suitable for characterisation of self assembled thin films. Another variation of the bottom-up approach is molecular beam epitaxy or MBE. Researchers at Bell Telephone Laboratories like John R. Arthur. Alfred Y. Cho, and Art C. Gossard developed and implemented MBE as a research tool in the late 1960s and 1970s. Samples made by MBE were key to the discovery of the fractional quantum Hall effect for which the 1998 Nobel Prize in Physics was awarded. MBE allows scientists to lay down atomically precise layers of atoms and, in the process, build up complex structures. Important for research on semiconductors, MBE is also widely used to make samples and devices for the newly emerging field of spintronics.

However, new therapeutic products, based on responsive nanomaterials, such as the ultradeformable, stress-sensitive Transfersome vesicles, are under development and already approved for human use in some countries.

As of August 21, 2008, the Project on Emerging Nanotechnologies estimates that over 800 manufacturer-identified nanotech products are publicly available, with new ones hitting the market at a pace of 3–4 per week. The project lists all of the products in a publicly accessible online database. Most applications are limited to the use of "first generation" passive nanomaterials which includes titanium dioxide in sunscreen, cosmetics, surface coatings, and some food products; Carbon allotropes used to produce gecko tape; silver in food packaging, clothing, disinfectants and household appliances; zinc oxide in sunscreens and cosmetics, surface coatings, paints and outdoor furniture varnishes; and cerium oxide as a fuel catalyst.

Further applications allow tennis balls to last longer, golf balls to fly straighter, and even bowling balls to become more durable and have a harder surface. Trousers and socks have been infused with nanotechnology so that they will last longer and keep people cool in the summer. Bandages are being infused with silver nanoparticles to heal cuts faster. Video game consoles and personal computers may become cheaper, faster, and contain more memory thanks to nanotechnology. Also, to build structures for on chip computing with light, for example on chip optical quantum information processing, and picosecond transmission of information.

Nanotechnology may have the ability to make existing medical applications cheaper and easier to use in places like the general practitioner's office and at home. Cars are being manufactured with nanomaterials so they may need fewer metals and less fuel to operate in the future.

Scientists are now turning to nanotechnology in an attempt to develop diesel engines with cleaner exhaust fumes. Platinum is currently used as the diesel engine catalyst in these engines. The catalyst is what cleans the exhaust fume particles. First a reduction catalyst is employed to take nitrogen atoms from NOx molecules in order to free oxygen. Next the oxidation catalyst oxidizes the hydrocarbons and carbon monoxide to form carbon dioxide and water. Platinum is used in both the reduction and the oxidation catalysts. Using platinum though, is inefficient in that it is expensive and unsustainable. Danish company InnovationsFonden invested DKK 15 million in a search for new catalyst substitutes using nanotechnology. The goal of the project, launched in the autumn of 2014, is to maximize surface area and minimize the amount of material required. Objects tend to minimize their surface energy; two drops of water, for example, will join to form one drop and decrease surface area. If the catalyst's surface area that is exposed to the exhaust fumes is maximized, efficiency of the catalyst is maximized. The team working on this project aims to create nanoparticles that will not merge. Every time the surface is optimized, material is saved. Thus, creating these nanoparticles will increase the effectiveness of the resulting diesel engine catalyst—in turn leading to cleaner exhaust fumes—and will decrease cost. If successful, the team hopes to reduce platinum use by 25%.

Nanotechnology also has a prominent role in the fast developing field of Tissue Engineering. When designing scaffolds, researchers attempt to the mimic the nanoscale features of a Cell's microenvironment to direct its differentiation down a suitable lineage. For example, when creating scaffolds to support the growth of bone, researchers may mimic osteoclast resorption pits.

Researchers have successfully used DNA origami-based nanobots capable of carrying out logic functions to achieve targeted drug delivery in cockroaches. It is said that the computational power of these nanobots can be scaled up to that of a Commodore 64.

An area of concern is the effect that industrial-scale manufacturing and use of nanomaterials would have on human health and the environment, as suggested by nanotoxicology research. For these reasons, some groups advocate that nanotechnology be regulated by governments. Others counter that overregulation would stifle scientific research and the development of beneficial innovations. Public health research agencies, such as the National Institute for Occupational Safety and Health are actively conducting research on potential health effects stemming from exposures to nanoparticles.

Some nanoparticle products may have unintended consequences. Researchers have discovered that bacteriostatic silver nanoparticles used in socks to reduce foot odor are being released in the wash. These particles are then flushed into the waste water stream and may destroy bacteria which are critical components of natural ecosystems, farms, and waste treatment processes.

Public deliberations on risk perception in the US and UK carried out by the Center for Nanotechnology in Society found that participants were more positive about nanotechnologies for energy applications than for health applications, with health applications raising moral and ethical dilemmas such as cost and availability.

Experts, including director of the Woodrow Wilson Center's Project on Emerging Nanotechnologies David Rejeski, have testified that successful commercialization depends on adequate oversight, risk research strategy, and public engagement. Berkeley, California is currently the only city in the United States to regulate nanotechnology; Cambridge, Massachusetts in 2008 considered enacting a similar law, but ultimately rejected it. Relevant for both research on and application of nanotechnologies, the insurability of nanotechnology is contested. Without state regulation of nanotechnology, the availability of private insurance for potential damages is seen as necessary to ensure that burdens are not socialised implicitly.

Nanofibers are used in several areas and in different products, in everything from aircraft wings to tennis rackets. Inhaling airborne nanoparticles and nanofibers may lead to a number of pulmonary diseases, e.g. fibrosis. Researchers have found that when rats breathed in nanoparticles, the particles settled in the brain and lungs, which led to significant increases in biomarkers for inflammation and stress response and that nanoparticles induce skin aging through oxidative stress in hairless mice.

A two-year study at UCLA's School of Public Health found lab mice consuming nano-titanium dioxide showed DNA and chromosome damage to a degree "linked to all the big killers of man, namely cancer, heart disease, neurological disease and aging".

A major study published more recently in Nature Nanotechnology suggests some forms of carbon nanotubes – a poster child for the "nanotechnology revolution" – could be as harmful as asbestos if inhaled in sufficient quantities. Anthony Seaton of the Institute of Occupational Medicine in Edinburgh, Scotland, who contributed to the article on carbon nanotubes said "We know that some of them probably have the potential to cause mesothelioma. So those sorts of materials need to be handled very carefully." In the absence of specific regulation forthcoming from governments, Paull and Lyons (2008) have called for an exclusion of engineered nanoparticles in food. A newspaper article reports that workers in a paint factory developed serious lung disease and nanoparticles were found in their lungs.

Calls for tighter regulation of nanotechnology have occurred alongside a growing debate related to the human health and safety risks of nanotechnology. There is significant debate about who is responsible for the regulation of nanotechnology. Some regulatory agencies currently cover some nanotechnology products and processes (to varying degrees) – by "bolting on" nanotechnology to existing regulations – there are clear gaps in these regimes. Davies (2008) has proposed a regulatory road map describing steps to deal with these shortcomings.

Stakeholders concerned by the lack of a regulatory framework to assess and control risks associated with the release of nanoparticles and nanotubes have drawn parallels with bovine spongiform encephalopathy ("mad cow" disease), thalidomide, genetically modified food, nuclear energy, reproductive technologies, biotechnology, and asbestosis. Dr. Andrew Maynard, chief science advisor to the Woodrow Wilson Center's Project on Emerging Nanotechnologies, concludes that there is insufficient funding for human health and safety research, and as a result there is currently limited understanding of the human health and safety risks associated with nanotechnology. As a result, some academics have called for stricter application of the precautionary principle, with delayed marketing approval, enhanced labelling and additional safety data development requirements in relation to certain forms of nanotechnology.

The Royal Society report identified a risk of nanoparticles or nanotubes being released during disposal, destruction and recycling, and recommended that "manufacturers of products that fall under extended producer responsibility regimes such as end-of-life regulations publish procedures outlining how these materials will be managed to minimize possible human and environmental exposure" (p. xiii).

The Center for Nanotechnology in Society has found that people respond to nanotechnologies differently, depending on application – with participants in public deliberations more positive about nanotechnologies for energy than health applications – suggesting that any public calls for nano regulations may differ by technology sector.



</doc>
<doc id="21489" url="https://en.wikipedia.org/wiki?curid=21489" title="NetHack">
NetHack

NetHack is a single-player roguelike video game originally released in 1987 with ASCII graphics. It is a descendant of an earlier game called "Hack" (), which is a clone of "Rogue" (). Comparing it with "Rogue", "Engadget"s Justin Olivetti wrote that it took its exploration aspect and "made it far richer with an encyclopedia of objects, a larger vocabulary, a wealth of pop culture mentions, and a puzzler's attitude." In 2000, "Salon" described it as "one of the finest gaming experiences the computing world has to offer."

The player chooses a character race and class for the mission of retrieving the Amulet of Yendor in a randomly generated dungeon.

The first version of "NetHack" was released by Mike Stephenson on July 28, 1987.

A core development team emerged with the release of "NetHack" 3.0 in July 1989. Over the next 14 years of development they established a tight-lipped culture, revealing little, if anything, between releases. Owing to the ever-increasing depth and complexity found in each release, the development team enjoys a near-mythical status among some fans. This perceived omniscience is captured in the initialism TDTTOE, "The DevTeam Thinks of Everything".

Stephenson licensed the software under the NetHack General Public License, allowing other developers to release their own versions. The license is certified as open source by the Open Source Initiative.

For 12 years, the last version to include new gameplay features was "NetHack" 3.4.3, released in December 2003. Subsequent updates included new tilesets and compatibility with variants of Mac OS. In the absence of new releases from the developers, several variants developed by fans emerged.

On December 7, 2015, version 3.6.0 was released, the first major release in over a decade. While the patch did not add major new gameplay features, the update was designed to prepare the game for expansion in the future. NetHack remains "...one of the oldest games still being developed". A public read-only mirror of "NetHack"'s git repository was made available on February 10, 2016. Version 3.6.1 was announced on April 28th, 2018, a return to a somewhat more frequent release schedule.

Before starting a game, players are asked to choose their character's race, role, gender, and alignment, or allow the game to assign the attributes randomly. There are traditional fantasy roles such as knight, barbarian, wizard, rogue, valkyrie, priest, monk, and samurai, but there are also unusual ones, including archaeologist, tourist, and caveman. The player character's role and alignment dictate which deity the character serves in the game and "how other monsters react toward you".

After the player character is created, the main objective is introduced. To win the game, the player must retrieve the Amulet of Yendor, found at the lowest level of the dungeon, and offer it to their deity. Successful completion of this task rewards the player with the gift of immortality, and the player is said to "ascend", attaining the status of demigod. In addition, a number of sub-quests must be completed, including one class-specific quest.

The player's character is, unless they opt not to be, accompanied by a pet animal, typically a kitten or little dog, although knights begin with a saddled pony. Pets grow from fighting, and they can be changed by various means. Most of the other monsters may also be tamed using magic or tempting food.

"NetHack"'s dungeon spans about 50 primary levels, of which most are randomly generated when the player character first encounters them. A typical level contains a way "up" and "down" (these may be stairways, ladders, trapdoors etc.), along with several "rooms" joined by corridors. The "rooms" are randomly generated rectangles (as opposed to the linear corridors) and may contain features such as altars, shops, fountains, traps, thrones, pools of water, and sinks based upon the randomly generated features of the room. Some "special" levels are of fixed design in every game session.

"NetHack" features a variety of items: weapons (either ranged or melee), armor to protect the player; scrolls and spellbooks to read, potions to quaff, wands, rings, amulets, and an assortment of tools such as keys and lamps.

"NetHack"<nowiki>'</nowiki>s identification of items is almost identical to "Rogue"<nowiki>'</nowiki>s. For example, a newly discovered potion may be referred to as a 'pink potion' with no other clues as to its identity. Players can perform a variety of actions and tricks to deduce, or at least narrow down, the identity of the potion. The most obvious is the somewhat risky tactic of simply drinking it. All items of a certain type will have the same description; for instance, all scrolls of enchant weapon may be labeled 'TEMOV', and once one has been identified, all scrolls of enchant weapon found will be labeled unambiguously as such. Starting a new game will scramble the items' descriptions again, so the 'silver ring' that is a ring of levitation in one game might be a ring of hunger in another.

As in many other roguelike games, all items in "NetHack" are either "blessed", "uncursed", or "cursed". The majority of items are found uncursed, but the blessed or cursed status of an item is unknown until it is identified or detected through other means.

Generally, a blessed item will be more powerful than an uncursed item, and a cursed item will be less powerful, with the added disadvantage that once it has been equipped by the player, it cannot be easily unequipped. Where an object would bestow an effect upon the character, a curse will generally make the effect harmful, or increase the amount of harm done. There are exceptions, however, which are usually very specific – for example, drinking a cursed potion of gain level will make the character literally rise through the ceiling to the level above, instead of gaining an experience level.

As in other rogue-like games, "NetHack" features permadeath: expired characters cannot be revived without having made backup copies of the actual save files.

Although "NetHack" can be completed by new or intermediate players without any artificial limitations, experienced players can attempt "conducts" for an additional challenge. These are voluntary restrictions on actions taken, such as using no wishes, following a vegetarian or even vegan diet, or even killing no monsters. While conducts are generally tracked by the game and are displayed at death or ascension, unofficial conducts are practiced within the community.

When a player dies, the cause of death and score is created and added to the list where the player's character is ranked against other previous characters. The prompt "Do you want your possessions identified?" is given by default at the end of any game, allowing the player to learn any unknown properties of the items in the inventory at death. Player's attributes (such as resistances, luck, and others), conduct (usually self-imposed challenges, such as playing as an atheist or a vegetarian), and a tally of creatures killed, may also be displayed.

The game sporadically saves a level on which a character has died and then integrates that level into a later game. This is done via "bones files", which are saved on the computer hosting the game. A player using a publicly hosted copy of the game can thus encounter the remains and possessions of many other players, although many of these possessions may have become cursed.

"NetHack" is largely based on discovering secrets and tricks during gameplay. It can take years for one to become well-versed in them, and even experienced players routinely discover new ones. A number of "NetHack" fan sites and discussion forums offer lists of game secrets known as "spoilers". Fans of "NetHack" consider an ascension without having read spoilers very prestigious; the achievement is so difficult that some question whether it has been or can be accomplished.

"NetHack" was originally created with only a simple ASCII graphical user interface, although the option to use something more elaborate was added later in its development. Interface elements – environment, entities, and objects – are represented by arrangements of ASCII or Extended ASCII glyphs used in plain text, "DEC graphics" or "IBM graphics" mode. In addition to the environment, the interface also displays character and situational information.

A detailed example:
The player (the '@' sign, a wizard in this case) has entered the level via the stairs (the '<' sign) and killed a few monsters, leaving their corpses (the '%' signs) behind. Exploring, the wizard has uncovered three rooms joined by corridors (the '#' signs): one with an altar (the '_' sign), another empty, and the final one (that the wizard is currently in) containing a potion (the '!' sign), chest (the '(' sign), and has just moved onto a square containing a silver ring. Parts of the level are still unexplored (probably accessible through the door to the west (the '+' sign)) and the player has yet to find the downstairs (a '>' sign) to the next level.

Apart from the original termcap interface shown above, there are interfaces that replace standard screen representations with two-dimensional images, or tiles, collectively known as "tiles mode". Graphic interfaces of this kind have been successfully implemented on the Amiga, the X Window System, the similar Microsoft Windows GUI, the Qt toolkit, or the GNOME libraries.

Enhanced graphical options also exist, such as the isometric perspective of "Falcon's Eye" and "Vulture's Eye", or the three-dimensional rendering that noegnud offers. "Vulture's Eye" is a fork of the now defunct Falcon's Eye project. "Vulture's Eye" adds additional graphics, sounds, bug fixes and performance enhancements and is under active development in an open collaborative environment.

Bugs, humorous messages, stories, experiences, and ideas for the next version are discussed on the Usenet newsgroup rec.games.roguelike.nethack.

A public server at nethack.alt.org, commonly known as NAO, gives access to NetHack through a Telnet or SSH interface. A Java Telnet emulator is also available on the same site. Ebonhack connects to NAO with a graphical tiles-based interface.

During the whole month of November, the annual /dev/null NetHack Tournament takes place. It has been held every year since 1999. The Junethack Cross-Variant Summer Tournament has taken place annually since 2011.

The NetHack General Public License allows anyone to create a fork or port it to a platform not supported by the official DevTeam, provided that they use the same license. Over the years this licensing has led to a large number of ports and forks (e.g. "Slash'EM" and "UnNetHack") as well as versions in German, Japanese and Spanish.

The "NetHack General Public License" is a copyleft software license certified as an open source license by the Open Source Initiative.

The license was written in 1989 by Mike Stephenson, who patterned it after the GNU bison license (which was written by Richard Stallman in 1988). Like the Bison license, and Stallman's later GNU General Public License, the "NetHack" license was written to allow the free sharing and modification of the source code under its protection. At the same time, the license explicitly states that the source code is not covered by any warranty, thus protecting the original authors from litigation.

The NetHack GPL requires all derivative works to be distributed under the same license, except that the creator of a derivative work is allowed to offer warranty protection on the new work. The derivative work is required to indicate the modifications made and the dates of changes. In addition, the source code of the derivative work must be made available, free of charge except for nominal distribution fees.




</doc>
<doc id="21490" url="https://en.wikipedia.org/wiki?curid=21490" title="Nylon">
Nylon

Nylon is a generic designation for a family of synthetic polymers, based on aliphatic or semi-aromatic polyamides. 
Nylon is a thermoplastic silky material 
that can be melt-processed into fibers, films or shapes.

Nylon was the first commercially successful synthetic thermoplastic polymer. DuPont began its research project in 1930.
The first example of nylon (nylon 6,6) was produced using diamines on February 28, 1935, by Wallace Hume Carothers at DuPont's research facility at the DuPont Experimental Station. In response to Carothers' work, Paul Schlack at IG Farben developed nylon 6, a different molecule based on caprolactam, on January 29, 1938.

Nylon was first used commercially in a nylon-bristled toothbrush in 1938, followed more famously in women's stockings or "nylons" which were shown at the 1939 New York World's Fair and first sold commercially in 1940. During World War II, almost all nylon production was diverted to the military for use in parachutes and parachute cord. Wartime uses of nylon and other plastics greatly increased the market for the new materials.

Nylon is made of repeating units linked by amide links similar to the peptide bonds in proteins.
Commercially, nylon polymer is made by reacting monomers which are either lactams, acid/amines or stoichiometric mixtures of diamines (-NH) and diacids (-COOH). Mixtures of these can be polymerized together to make copolymers. Nylon polymers can be mixed with a wide variety of additives to achieve many different property variations.
Nylon polymers have found significant commercial applications in fabric and fibers (apparel, flooring and rubber reinforcement), in shapes (molded parts for cars, electrical equipment, etc.), and in films (mostly for food packaging).

DuPont, founded by Éleuthère Irénée du Pont, first produced gunpowder and later cellulose-based paints. Following WWI, DuPont produced synthetic ammonia and other chemicals. DuPont began experimenting with the development of cellulose based fibres, eventually producing the synthetic fibre rayon. DuPont's experience with rayon was an important precursor to its development and marketing of nylon.

DuPont's invention of nylon spanned a nine-year period, ranging from the start of the project in 1930 to its exhibition at the World Fair in New York in 1939. The project grew from a new structure at DuPont, suggested by Charles Stine in 1927, in which the chemical department would be composed of several small research teams that would focus on “pioneering research” in chemistry and would “lead to practical applications”. Harvard instructor Wallace Hume Carothers was hired to direct the polymer research group. Initially he was allowed to focus on pure research, building on and testing the theories of German chemist Hermann Staudinger. He was very successful as research he undertook greatly improved the knowledge of polymers and contributed to science.

In the spring of 1930, Carothers and his team had already synthesized two new polymers. One was neoprene, a synthetic rubber greatly used during the war. The other was a white elastic but strong paste that would later become nylon. After these discoveries Carother’s team was made to shift its research from a more pure research approach investigating general polymerization to a more practically-focused goal of finding “one chemical combination that would lend itself to industrial applications”.

It wasn’t until the beginning of 1935 that a polymer called "polymer 6-6" was finally produced. The first example of nylon (nylon 6,6) was produced by Wallace Carothers on February 28, 1935, at DuPont's research facility at the DuPont Experimental Station. It had all the desired properties of elasticity and strength.
However, it also required a complex manufacturing process that would become the basis of industrial production in the future. DuPont obtained a patent for the polymer in September 1938, and quickly achieved a monopoly of the fibre.

The production of nylon required interdepartmental collaboration between three departments at DuPont: the Department of Chemical Research, the Ammonia Department, and the Department of Rayon. Some of the key ingredients of nylon had to be produced using high pressure chemistry, the main area of expertise of the Ammonia Department. Nylon was considered a “godsend to the Ammonia Department”, which had been in financial difficulties. The reactants of nylon soon constituted half of the Ammonia department’s sales and helped them come out of the period of the Great Depression by creating jobs and revenue at DuPont.

DuPont's nylon project demonstrated the importance of chemical engineering in industry, helped create jobs, and furthered the advancement of chemical engineering techniques. In fact, it developed a chemical plant that provided 1800 jobs and used the latest technologies of the time, which are still used as a model for chemical plants today. “The success of the nylon project thus had to do with its ability to achieve the rapid mobilization of a large number of DuPont’s chemists and engineers”. The first nylon plant was located at Seaford, Delaware, beginning commercial production on December 15, 1939. On October 26, 1995, the Seaford plant was designated a National Historic Chemical Landmark by the American Chemical Society.

DuPont went through an extensive process to generate names for its new product. 
In 1940, John W. Eckelberry of DuPont stated that the letters "nyl" were arbitrary and the "on" was copied from the suffixes of other fibers such as cotton and Rayon. A later publication by DuPont ("Context", vol. 7, no. 2, 1978) explained that the name was originally intended to be "No-Run" ("run" meaning "unravel"), but was modified to avoid making such an unjustified claim. Since the products were not really run-proof, the vowels were swapped to produce "nuron", which was changed to "nilon" "to make it sound less like a nerve tonic". For clarity in pronunciation, the "i" was changed to "y."

An important part of nylon’s popularity stems from DuPont’s marketing strategy. The fibre was promoted to increase demand before the product was available to the general market. Nylon’s commercial announcement occurred on October 27, 1938, at the final session of the "Herald Tribune"s yearly "Forum on Current Problems", on the site of the approaching New York City world's fair. The “first man-made organic textile fiber” which was derived from “coal, water and air” and promised to be “as strong as steel, as fine as the spider’s web” was received enthusiastically by the audience, many of them middle-class women, and made the headlines of most newspapers. Nylon was introduced as part of "The world of tomorrow" at the 1939 New York World's Fair and was featured at DuPont's "Wonder World of Chemistry" at the Golden Gate International Exposition in San Francisco in 1939. Actual nylon stockings were not shipped to selected stores in the national market until May 15, 1940. However, a limited number were released for sale in Delaware before that. The first public sale of nylon stockings occurred on October 24, 1939, in Wilmington, Delaware. 4,000 pairs of stockings were available, all of which were sold within three hours.

Another added bonus to the campaign was that it meant reducing silk imports from Japan, an argument that won over many wary customers. Nylon was even mentioned by President Roosevelt’s cabinet, which addressed its “vast and interesting economic possibilities” five days after the material was formally announced.

However, the early excitement over nylon also caused problems. It fueled unreasonable expectations that nylon would be better than silk, a miracle fabric as strong as steel that would last forever and never run. Realizing the danger of claims such as “New Hosiery Held Strong as Steel” and “No More Runs”, Du Pont scaled back the terms of the original announcement, especially those stating that nylon would possess the strength of steel.

Also, DuPont executives marketing nylon as a revolutionary man-made material did not at first realize that some consumers experienced a sense of unease and distrust, even fear, towards synthetic fabrics.
A particularly damaging news story, drawing on DuPont's 1938 patent for the new polymer, suggested that one method of producing nylon might be to use cadaverine, a chemical extracted from corpses. Although scientists asserted that cadaverine was also extracted by heating coal, the public often refused to listen, as in the case of a woman who confronted one of the lead scientists at DuPont and refused to accept that the rumour was not true.

DuPont changed its campaign strategy, emphasizing that nylon was made from “coal, air and water”, and started focusing on the personal and aesthetic aspects of nylon, rather than its intrinsic qualities. Nylon was thus domesticated, and attention shifted to the material and consumer aspect of the fiber with slogans like “If it’s nylon, it’s prettier, and oh! How fast it dries!”.

After nylon’s nationwide release in 1940, production was increased. 1300 tons of the fabric were produced during 1940. During their first year on the market, 64 million pairs of nylon stockings were sold.
While nylon was marketed as the durable and indestructible material of the people, it was sold at almost twice the price of silk stockings ($4.27 per pound of nylon versus $2.79 per pound of silk). Sales of nylon stockings were strong in part due to changes in women’s fashion. As Lauren Olds explains: “by 1939 [hemlines] had inched back up to the knee, closing the decade just as it started off”. The shorter skirts were accompanied by a demand for stockings that offered fuller coverage without the use of garters to hold them up.

However, as of February 11, 1942, nylon production was redirected from being a consumer material to one used by the military. DuPont's production of nylon stockings and other lingerie stopped, and most manufactured nylon was used to make parachutes and tents for World War II.

Once the war ended, the return of nylon was awaited with great anticipation. Although DuPont projected yearly production of 360 million pairs of stockings, there were delays in converting back to consumer rather than wartime production. In 1946, the demand for nylon stockings could not be satisfied, which led to the Nylon Riots. In one case, an estimated 40,000 people lined up in Pittsburgh to buy 13,000 pairs of nylons. In the meantime, women cut up nylon tents and parachutes left from the war in order to make blouses and wedding dresses. Between the end of the war and 1952, production of stockings and lingerie used 80% of the world’s nylon. DuPont put a lot of focus on catering to the civilian demand, and continually expanded its production.

As pure nylon hosiery was sold in a wider market, problems became apparent. Nylon stockings were found to be fragile, in the sense that the thread often tended to unravel lengthwise, creating ‘runs’. People also reported that pure nylon textiles could be uncomfortable due to nylon's lack of absorbency. Moisture stayed inside the fabric near the skin under hot or moist conditions instead of being "wicked" away. Nylon fabric could also be itchy, and tended to cling and sometimes spark as a result of static electrical charge built up by friction.
Also, under some conditions stockings could decompose turning back into nylon's original components of air, coal, and water. Scientists explained this as a result of air pollution, attributing it to London smog in 1952, as well as poor air quality in New York and Los Angeles.

The solution found to problems with pure nylon fabric was to blend nylon with other existing fibres or polymers such as cotton, polyester, and spandex. This led to the development of a wide array of blended fabrics. The new nylon blends retained the desirable properties of nylon (elasticity, durability, ability to be dyed) and kept clothes prices low and affordable.
As of 1950, the New York Quartermaster Procurement Agency (NYQMPA), which developed and tested textiles for the army and navy, had committed to developing a wool-nylon blend. They were not the only ones to introduce blends of both natural and synthetic fibres. "America's Textile Reporter" referred to 1951 as the "Year of the blending of the fibers". Fabric blends included mixes like "Bunara" (wool-rabbit-nylon) and "Casmet" (wool-nylon-fur). In Britain in November 1951, the inaugural address of the 198th session of the Royal Society for the Encouragement of Arts, Manufactures and Commerce focused on the blending of textiles.

DuPont's Fabric Development Department cleverly targeted French fashion designers, supplying them with fabric samples. In 1955, designers such as Coco Chanel, Jean Patou, and Christian Dior showed gowns created with DuPont fibers, and fashion photographer Horst P. Horst was hired to document their use of DuPont fabrics. "American Fabrics" credited blends with providing "creative possibilities and new ideas for fashions which had been hitherto undreamed of."

In spite of oil shortages in the 1970s, consumption of nylon textiles continued to grow by 7.5 per cent per annum between the 1960s and 1980s. 
Overall production of synthetic fibres, however, dropped from 63% of the worlds textile production in 1965, to 45% of the world's textile production in early 1970s. The appeal of "new" technologies wore off, and nylon fabric “was going out of style in the 1970s”. Also, consumers became concerned about environmental costs throughout the production cycle: obtaining the raw materials (oil), energy use during production, waste produced during creation of the fibre, and eventual waste disposal of materials that were not biodegradable. 
Synthetic fibres have not dominated the market since the 1950s and 1960s. , nylon continued to represent about 12% (8 million pounds) of the world’s production of synthetic fibres.

Although pure nylon has many flaws and is now rarely used, its derivatives have greatly influenced and contributed to society. From scientific discoveries relating to the production of plastics and polymerization, to economic impact during the depression and the changing of women’s fashion, nylon was a revolutionary product. The first flag planted on the moon, in a symbolic gesture of celebration, was made of nylon. The flag itself cost $5.50, but had to have a specially-designed flagpole with a horizontal bar so that it would appear to "fly".
“Nylon is one of the great symbols of the American century, on a par no doubt with Coca-Cola in the consumer dreams of 20th century men and women. [...] it is not only a technologically advanced product, it has also captured the public’s imagination [...]: nylon as an object of desire”.

Nylons are condensation polymers or copolymers, formed by reacting difunctional monomers containing equal parts of amine and carboxylic acid, so that amides are formed at both ends of each monomer in a process analogous to polypeptide biopolymers. Most nylons are made from the reaction of a dicarboxylic acid with a diamine (e.g. PA66) or a lactam or amino acid with itself (e.g. PA6). In the first case, the "repeating unit" consists of one of each monomer, so that they alternate in the chain, similar to the so-called ABAB structure of polyesters and polyurethanes. Since each monomer in this copolymer has the same reactive group on both ends, the direction of the amide bond reverses between each monomer, unlike natural polyamide proteins, which have overall directionality: C terminal → N terminal. In the second case (so called AA), the repeating unit corresponds to the single monomer.

In common usage, the prefix 'PA' (polyamide) or the name 'Nylon' are used interchangeably and are equivalent in meaning.

The nomenclature used for nylon polymers was devised during the synthesis of the first simple aliphatic nylons and uses numbers to describe the number of carbons in each monomer unit, including the carbon(s) of the carboxylic acid(s). Subsequent use of cyclic and aromatic monomers required the use of letters or sets of letters. One number after 'PA' or 'Nylon' indicates a homopolymer which is "monadic" or based on one amino acid (minus HO) as monomer:


Two numbers or sets of letters indicate a "dyadic" homopolymer formed from two monomers: one diamine and one dicarboxylic acid. The first number indicates the number of carbons in the diamine. The two numbers should be separated by a comma for clarity, but the comma is often omitted.

For copolymers the comonomers or pairs of comonomers are separated by slashes:

The term polyphthalamide (abbreviated to PPA) is used when 60% or more moles of the carboxylic acid portion of the repeating unit in the polymer chain is composed of a combination of terephthalic acid (TPA) and isophthalic acid (IPA).

Wallace Carothers at DuPont patented nylon 66 using amides. 
In the case of nylons that involve reaction of a diamine and a dicarboxylic acid, it is difficult to get the proportions exactly correct, and deviations can lead to chain termination at molecular weights less than a desirable 10,000 daltons (u). To overcome this problem, a crystalline, solid "nylon salt" can be formed at room temperature, using an exact 1:1 ratio of the acid and the base to neutralize each other. The salt is crystallized to purify it and obtain the desired precise stoichiometry. Heated to 285 °C (545 °F), the salt reacts to form nylon polymer with the production of water.

Carothers overlooked the possibility of using lactams. That synthetic route was developed by Paul Schlack at IG Farben, leading to nylon 6, or polycaprolactam — formed by a ring-opening polymerization. The peptide bond within the caprolactam is broken with the exposed active groups on each side being incorporated into two new bonds as the monomer becomes part of the polymer backbone.

The 428 °F (220 °C) melting point of nylon 6 is lower than the 509 °F (265 °C) melting point of nylon 66.

Nylon 510, made from pentamethylene diamine and sebacic acid, was studied by Carothers even before nylon 66 and has superior properties, but is more expensive to make. In keeping with this naming convention, "nylon 6,12" or "PA 612" is a copolymer of a 6C diamine and a 12C diacid. Similarly for PA 510 PA 611; PA 1012, etc. Other nylons include copolymerized dicarboxylic acid/diamine products that are "not" based upon the monomers listed above. For example, some fully aromatic nylons (known as "aramids") are polymerized with the addition of diacids like terephthalic acid (→ Kevlar, Twaron) or isophthalic acid (→ Nomex), more commonly associated with polyesters. There are copolymers of PA 66/6; copolymers of PA 66/6/12; and others. In general linear polymers are the most useful, but it is possible to introduce branches in nylon by the condensation of dicarboxylic acids with polyamines having three or more amino groups.

The general reaction is:
Two molecules of water are given off and the nylon is formed. Its properties are determined by the R and R' groups in the monomers. In nylon 6,6, R = 4C and R' = 6C alkanes, but one also has to include the two carboxyl carbons in the diacid to get the number it donates to the chain. In Kevlar, both R and R' are benzene rings.

Industrial synthesis is usually done by heating the acids, amines or lactams to remove water, but in the laboratory, diacid chlorides can be reacted with diamines. For example, a popular demonstration of interfacial polymerization (the "nylon rope trick") is the synthesis of nylon 66 from adipoyl chloride and hexamethylene diamine.

Nylons can also be synthesized from dinitriles using acid catalysis. For example, this method is applicable for preparation of nylon 1,6 from adiponitrile, formaldehyde and water. Additionally, nylons can be synthesized from diols and dinitriles using this method as well.

Nylon monomers are manufactured by a variety of routes, starting in most cases from crude oil but sometimes from biomass. Those in current production are described below.



2-methyl pentamethylene diamine is a by product of HMD production

With increasing interest in biobased materials other monomers are being investigated: 1,5-pentanediamine (cadaverine) (PMD): starch (e.g. cassava) → glucose → lysine → PMD.

Due to the large number of diamines, diacids and aminoacids that can be synthesized, many nylon polymers have been made experimentally and characterized to varying degrees. A smaller number have been scaled up and offered commercially, and these are detailed below.

Homopolymer nylons derived from one monomer

Examples of these polymers that are or were commercially available

Homopolymer polyamides derived from pairs of diamines and diacids (or diacid derivatives). Shown in the table below are polymers which are or have been offered commercially either as homopolymers or as a part of a copolymer.

Examples of these polymers that are or were commercially available

It is easy to make mixtures of the monomers or sets of monomers used to make nylons to obtain copolymers. This lowers crystallinity and can therefore lower the melting point.

Some copolymers that have been or are commercially available are listed below: 

Most nylon polymers are miscible with each other allowing a range of blends to be made. The two polymers can react with one another by transamidation to form random copolymers.

According to their crystallinity, polyamides can be:

According to this classification, PA66, for example, is an aliphatic semi-crystalline homopolyamide.

Nylon clothing tends to be less flammable than cotton and rayon, but nylon fibres may melt and stick to skin.

Above their melting temperatures, "T", thermoplastics like nylon are amorphous solids or viscous fluids in which the chains approximate random coils. Below "T", amorphous regions alternate with regions which are lamellar crystals. The amorphous regions contribute elasticity and the crystalline regions contribute strength and rigidity. The planar amide (-CO-NH-) groups are very polar, so nylon forms multiple hydrogen bonds among adjacent strands. Because the nylon backbone is so regular and symmetrical, especially if all the amide bonds are in the "trans" configuration, nylons often have high crystallinity and make excellent fibers. The amount of crystallinity depends on the details of formation, as well as on the kind of nylon.

Nylon 66 can have multiple parallel strands aligned with their neighboring peptide bonds at coordinated separations of exactly 6 and 4 carbons for considerable lengths, so the carbonyl oxygens and amide hydrogens can line up to form interchain hydrogen bonds repeatedly, without interruption (see the figure opposite). Nylon 510 can have coordinated runs of 5 and 8 carbons. Thus parallel (but not antiparallel) strands can participate in extended, unbroken, multi-chain β-pleated sheets, a strong and tough supermolecular structure similar to that found in natural silk fibroin and the β-keratins in feathers. (Proteins have only an amino acid α-carbon separating sequential -CO-NH- groups.) Nylon 6 will form uninterrupted H-bonded sheets with mixed directionalities, but the β-sheet wrinkling is somewhat different. The three-dimensional disposition of each alkane hydrocarbon chain depends on rotations about the 109.47° tetrahedral bonds of singly bonded carbon atoms.

When extruded into fibers through pores in an industry spinneret, the individual polymer chains tend to align because of viscous flow. If subjected to cold drawing afterwards, the fibers align further, increasing their crystallinity, and the material acquires additional tensile strength. In practice, nylon fibers are most often drawn using heated rolls at high speeds.

Block nylon tends to be less crystalline, except near the surfaces due to shearing stresses during formation. Nylon is clear and colorless, or milky, but is easily dyed. Multistranded nylon cord and rope is slippery and tends to unravel. The ends can be melted and fused with a heat source such as a flame or electrode to prevent this.

Nylons are hygroscopic, and will absorb or desorb moisture as a function of the ambient humidity. Variations in moisture content have several effects on the polymer. Firstly, the dimensions will change, but more importantly moisture acts as a plasticizer, lowering the glass transition temperature ("T"), and consequently the elastic modulus at temperatures below the "T"

When dry, polyamide is a good electrical insulator. However, polyamide is hygroscopic. The absorption of water will change some of the material's properties such as its electrical resistance. Nylon is less absorbent than wool or cotton.

The characteristic features of nylon 6,6 include:

On the other hand, nylon 6 is easy to dye, more readily fades; it has a higher impact resistance, a more rapid moisture absorption, greater elasticity and elastic recovery.

Bill Pittendreigh, DuPont, and other individuals and corporations worked diligently during the first few months of World War II to find a way to replace Asian silk and hemp with nylon in parachutes. It was also used to make tires, tents, ropes, ponchos, and other military supplies. It was even used in the production of a high-grade paper for U.S. currency. At the outset of the war, cotton accounted for more than 80% of all fibers used and manufactured, and wool fibers accounted for nearly all of the rest. By August 1945, manufactured fibers had taken a market share of 25%, at the expense of cotton. After the war, because of shortages of both silk and nylon, nylon parachute material was sometimes repurposed to make dresses.

Nylon 6 and 66 fibers are used in carpet manufacture.

Nylon is one kind of fiber used in tire cord. Herman E. Schroeder pioneered application of nylon in tires.

Nylon resins are widely used in the automobile industry especially in the engine compartment.

Molded nylon is used in hair combs and mechanical parts such as machine screws, gears and other low- to medium-stress components previously cast in metal. Engineering-grade nylon is processed by extrusion, casting, and injection molding. Type 6,6 Nylon 101 is the most common commercial grade of nylon, and Nylon 6 is the most common commercial grade of molded nylon. For use in tools such as spudgers, nylon is available in glass-filled variants which increase structural and impact strength and rigidity, and molybdenum disulfide-filled variants which increase lubricity. Its various properties also make it very useful as a material in additive manufacturing; specifically as a filament in consumer and professional grade fused deposition modeling 3D printers. Nylon can be used as the matrix material in composite materials, with reinforcing fibers like glass or carbon fiber; such a composite has a higher density than pure nylon. Such thermoplastic composites (25% to 30% glass fiber) are frequently used in car components next to the engine, such as intake manifolds, where the good heat resistance of such materials makes them feasible competitors to metals.

Nylon was used to make the stock of the Remington Nylon 66 rifle. The frame of the modern Glock pistol is made of a nylon composite.

Nylon resins are used as a component of food packaging films where an oxygen barrier is needed. Some of the terpolymers based upon nylon are used every day in packaging. Nylon has been used for meat wrappings and sausage sheaths. The high temperature resistance of nylon makes it useful for oven bags.

Nylon filaments are primarily used in brushes especially toothbrushes and string trimmers. They are also used as monofilaments in fishing line. Nylon 610 and 612 are the most used polymers for filaments.

Its various properties also make it very useful as a material in additive manufacturing; specifically as a filament in consumer and professional grade fused deposition modeling 3D printers.

Nylon resins can be extruded into rods, tubes and sheets.

Nylon powders are used to powder coat metals. Nylon 11 and nylon 12 are the most widely used.

In the mid-1940s, classical guitarist Andrés Segovia mentioned the shortage of good guitar strings in the United States, particularly his favorite Pirastro catgut strings, to a number of foreign diplomats at a party, including General Lindeman of the British Embassy. A month later, the General presented Segovia with some nylon strings which he had obtained via some members of the DuPont family. Segovia found that although the strings produced a clear sound, they had a faint metallic timbre which he hoped could be eliminated.

Nylon strings were first tried on stage by Olga Coelho in New York in January, 1944.

In 1946, Segovia and string maker Albert Augustine were introduced by their mutual friend Vladimir Bobri, editor of Guitar Review. On the basis of Segovia's interest and Augustine's past experiments, they decided to pursue the development of nylon strings. DuPont, skeptical of the idea, agreed to supply the nylon if Augustine would endeavor to develop and produce the actual strings. After three years of development, Augustine demonstrated a nylon first string whose quality impressed guitarists, including Segovia, in addition to DuPont.

Wound strings, however, were more problematic. Eventually, however, after experimenting with various types of metal and smoothing and polishing techniques, Augustine was also able to produce high quality nylon wound strings.

All nylons are susceptible to hydrolysis, especially by strong acids, a reaction essentially the reverse of the synthetic reaction shown above. The molecular weight of nylon products so attacked drops, and cracks form quickly at the affected zones. Lower members of the nylons (such as nylon 6) are affected more than higher members such as nylon 12. This means that nylon parts cannot be used in contact with sulfuric acid for example, such as the electrolyte used in lead–acid batteries.

When being molded, nylon must be dried to prevent hydrolysis in the molding machine barrel since water at high temperatures can also degrade the polymer. The reaction is of the type:

Berners-Lee calculates the average greenhouse gas footprint of nylon in manufacturing carpets at 5.43 kg CO equivalent per kg, when produced in Europe. This gives it almost the same carbon footprint as wool, but with greater durability and therefore a lower overall carbon footprint.

Data published by PlasticsEurope indicates for nylon 66 a greenhouse gas footprint of 6.4 kg CO equivalent per kg, and an energy consumption of 138 kJ/kg. When considering the environmental impact of nylon, it is important to consider the use phase. In particular when cars are lightweighted, significant savings in fuel consumption and CO emissions are realised.

Various nylons break down in fire and form hazardous smoke, and toxic fumes or ash, typically containing hydrogen cyanide. Incinerating nylons to recover the high energy used to create them is usually expensive, so most nylons reach the garbage dumps, decaying slowly. Discarded nylon fabric takes 30-40 years to decompose. Nylon is a robust polymer and lends itself well to recycling. Much nylon resin is recycled directly in a closed loop at the injection molding machine, by grinding sprues and runners and mixing them with the virgin granules being consumed by the molding machine.

As one of the largest engineering polymer families, the global demand of nylon resins and compounds was valued at roughly US$20.5 billion in 2013. The market is expected to reach US$30 billion by 2020 by following an average annual growth of 5.5%.




</doc>
<doc id="21491" url="https://en.wikipedia.org/wiki?curid=21491" title="Nucleus">
Nucleus

Nucleus (pl: "nuclei") is a Latin word for the seed inside a fruit. It may refer to:











</doc>
<doc id="21494" url="https://en.wikipedia.org/wiki?curid=21494" title="Nerd">
Nerd

A nerd is a person seen as overly intellectual, obsessive, introverted or lacking social skills. Such a person may spend inordinate amounts of time on unpopular, little known, or non-mainstream activities, which are generally either highly technical, abstract, or relating to topics of fiction or fantasy, to the exclusion of more mainstream activities. Additionally, many so-called nerds are described as being shy, quirky, pedantic, and unattractive.

Originally derogatory, the term "nerd" was a stereotype, but as with other pejoratives, it has been reclaimed and redefined by some as a term of pride and group identity.

The first documented appearance of the word "nerd" is as the name of a creature in Dr. Seuss's book "If I Ran the Zoo" (1950), in which the narrator Gerald McGrew claims that he would collect "a Nerkle, a Nerd, and a Seersucker too" for his imaginary zoo. The slang meaning of the term dates to the next year, 1951, when "Newsweek" magazine reported on its popular use as a synonym for "drip" or "square" in Detroit, Michigan. By the early 1960s, usage of the term had spread throughout the United States, and even as far as Scotland. At some point, the word took on connotations of bookishness and social ineptitude.

An alternate spelling, as "nurd" or "gnurd", also began to appear in the mid-1960s or early 1970s. Author Philip K. Dick claimed to have coined the "nurd" spelling in 1973, but its first recorded use appeared in a 1965 student publication at Rensselaer Polytechnic Institute (RPI). Oral tradition there holds that the word is derived from "knurd" ("drunk" spelled backward), which was used to describe people who studied rather than partied. The term "gnurd " (spelled with the "g") was in use at the Massachusetts Institute of Technology (MIT) by 1965. The term "nurd" was also in use at the Massachusetts Institute of Technology as early as 1971 but was used in the context for the proper name of a fictional character in a satirical "news" article.

The Online Etymology Dictionary speculates that the word is an alteration of the 1940s term ""nert"" (meaning "stupid or crazy person"), which is itself an alteration of "nut" (nutcase).

The term was popularized in the 1970s by its heavy use in the sitcom "Happy Days".

Because of the nerd stereotype, many smart people are often thought of as nerdy. This belief can be harmful, as it can cause high-school students to "switch off their lights" out of fear of being branded as a nerd, and cause otherwise appealing people to be considered nerdy simply for their intellect. It was once thought that intellectuals were nerdy because they were envied. However, Paul Graham stated in his essay, "Why Nerds are Unpopular", that intellect is neutral, meaning that you are neither loved nor despised for it. He also states that it is only the correlation that makes smart teens automatically seem nerdy, and that a nerd is someone that is not socially adept enough. Additionally, he says that the reason why many smart kids are unpopular is that they "don't have time for the activities required for popularity."
Stereotypical nerd appearance, often lampooned in caricatures, includes very large glasses, braces, severe acne and pants worn high at the waist. In the media, many nerds are males, portrayed as being physically unfit, either overweight or skinny due to lack of physical exercise. It has been suggested by some, such as linguist Mary Bucholtz, that being a nerd may be a state of being "hyperwhite" and rejecting African-American culture and slang that "cool" white children use. However, after the "Revenge of the Nerds" movie franchise (with multicultural nerds), and the introduction of the Steve Urkel character on the television series "Family Matters", nerds have been seen in all races and colors as well as more recently being a frequent young Asian or Indian male stereotype in North America. Portrayal of "nerd girls", in films such as "She's Out of Control", "Welcome to the Dollhouse" and "She's All That" depicts that smart but nerdy women might suffer later in life if they do not focus on improving their physical attractiveness.

In the United States, a 2010 study published in the "Journal of International and Intercultural Communication" indicated that Asian Americans are perceived as most likely to be nerds, followed by White Americans, while non-White Hispanics and Black Americans were perceived as least likely to be nerds. These stereotypes stem from concepts of Orientalism and Primitivism, as discussed in Ron Eglash's essay "Race, Sex, and Nerds": "From Black Geeks to Asian American Hipsters".

The rise of Silicon Valley and the American computer industry at large has allowed many so-called nerdy people to accumulate large fortunes. Many stereotypically nerdy interests, such as superhero and science fiction works, are now popular culture hits. Some measures of nerdiness are now allegedly considered desirable, as, to some, it suggests a person who is intelligent, respectful, interesting, and able to earn a large salary. Stereotypical nerd qualities are evolving, going from awkwardness and social ostracism to an allegedly more widespread acceptance and sometimes even celebration of their differences.

Johannes Grenzfurthner, researcher, self-proclaimed nerd and director of nerd documentary "Traceroute", reflects on the emergence of nerds and nerd culture:
In the 1984 film "Revenge of the Nerds" Robert Carradine worked to embody the nerd stereotype; in doing so, he helped create a definitive image of nerds. Additionally, the storyline presaged, and may have helped inspire, the "nerd pride" that emerged in the 1990s. "American Splendor" regular Toby Radloff claims this was the movie that inspired him to become "The Genuine Nerd from Cleveland, Ohio." In the "American Splendor" film, Toby's friend, "American Splendor" author Harvey Pekar, was less receptive to the movie, believing it to be hopelessly idealistic, explaining that Toby, an adult low income file clerk, had nothing in common with the middle class kids in the film who would eventually attain college degrees, success, and cease being perceived as nerds. Many, however, seem to share Radloff's view, as "nerd pride" has become more widespread in the years since. MIT professor Gerald Sussman, for example, seeks to instill pride in nerds:
The popular computer-related news website Slashdot uses the tagline "News for nerds. Stuff that matters." The Charles J. Sykes quote "Be nice to nerds. Chances are you'll end up working for one" has been popularized on the Internet and incorrectly attributed to Bill Gates. In Spain, Nerd Pride Day has been observed on May 25 since 2006, the same day as Towel Day, another somewhat nerdy holiday.<ref name="geek/nerd"></ref> The date was picked as it is the anniversary of the release of "".

An episode from the animated series "Freakazoid", titled "Nerdator", includes the use of nerds to power the mind of a Predator-like enemy. Towards the middle of the show, he gave this speech. :

The Danish reality TV show "FC Zulu", known in the internationally franchised format as "FC Nerds", established a format wherein a team of nerds, after two or three months of training, competes with a professional soccer team.

Some commentators consider that the word is devalued when applied to people who adopt a sub-cultural pattern of behaviour, rather than being reserved for people with a marked ability.

Although originally being predominately an American stereotype, Nerd culture has grown across the globe and is now more acceptable and common than ever. Australian events such as Oz Comic-Con (a large comic book and Cosplay convention, similar to San Diego Comic-Con International) and Supernova, are incredibly popular events among the culture of people who identify themselves as nerds. In 2016, Oz Comic-Con in Perth saw almost 20,000 cos-players and comic book fans meet to celebrate the event, hence being named a "professionally organised Woodstock for geeks".

Nerds are often the target of bullying due to a range of reasons that may include physical appearance or social background. Paul Graham has suggested that the reason nerds are frequently singled out for bullying is their indifference to popularity, in the face of a youth culture that views popularity as paramount. However, research findings suggest that bullies are often as socially inept as their academically better-performing victims, and that popularity fails to confer protection from bullying. Other commentators have pointed out that pervasive harassment of intellectually-oriented youth began only in the mid-twentieth century and some have suggested that its cause involves jealousy over future employment opportunities and earning potential.






</doc>
<doc id="21496" url="https://en.wikipedia.org/wiki?curid=21496" title="Nucleic acid">
Nucleic acid

Nucleic acids are biopolymers, or small biomolecules, essential to all known forms of life. They are composed of nucleotides, which are monomers made of three components: a 5-carbon sugar, a phosphate group and a nitrogenous base. If the sugar is a compound ribose, the polymer is RNA (ribonucleic acid); if the sugar is derived from ribose as deoxyribose, the polymer is DNA (deoxyribonucleic acid).

Nucleic acids are the most important of all biomolecules. They are found in abundance in all living things, where they function to create and encode and then store information in the nucleus of every living cell of every life-form organism on Earth. In turn, they function to transmit and express that information inside and outside the cell nucleus—to the interior operations of the cell and ultimately to the next generation of each living organism. The encoded information is contained and conveyed via the nucleic acid sequence, which provides the 'ladder-step' ordering of nucleotides within the molecules of RNA and DNA.

Strings of nucleotides are bonded to form helical backbones—typically, one for RNA, two for DNA—and assembled into chains of base-pairs selected from the five primary, or canonical, nucleobases, which are: adenine, cytosine, guanine, thymine, and uracil; note, thymine occurs only in DNA and uracil only in RNA. Using amino acids and the process known as protein synthesis, the specific sequencing in DNA of these nucleobase-pairs enables storing and transmitting coded instructions as genes. In RNA, base-pair sequencing provides for manufacturing new proteins that determine the frames and parts and most chemical processes of all life forms.


Experimental studies of nucleic acids constitute a major part of modern biological and medical research, and form a foundation for genome and forensic science, and the biotechnology and pharmaceutical industries.

The term "nucleic acid" is the overall name for DNA and RNA, members of a family of biopolymers, and is synonymous with "polynucleotide". Nucleic acids were named for their initial discovery within the nucleus, and for the presence of phosphate groups (related to phosphoric acid). Although first discovered within the nucleus of eukaryotic cells, nucleic acids are now known to be found in all life forms including within bacteria, archaea, mitochondria, chloroplasts, viruses, and viroids. (note: there is debate as to whether viruses are living or non-living). All living cells contain both DNA and RNA (except some cells such as mature red blood cells), while viruses contain either DNA or RNA, but usually not both.
The basic component of biological nucleic acids is the nucleotide, each of which contains a pentose sugar (ribose or deoxyribose), a phosphate group, and a nucleobase.
Nucleic acids are also generated within the laboratory, through the use of enzymes (DNA and RNA polymerases) and by solid-phase chemical synthesis. The chemical methods also enable the generation of altered nucleic acids that are not found in nature, for example peptide nucleic acids.

Nucleic acids are generally very large molecules. Indeed, DNA molecules are probably the largest individual molecules known. Well-studied biological nucleic acid molecules range in size from 21 nucleotides (small interfering RNA) to large chromosomes (human chromosome 1 is a single molecule that contains 247 million base pairs).

In most cases, naturally occurring DNA molecules are double-stranded and RNA molecules are single-stranded. There are numerous exceptions, however—some viruses have genomes made of double-stranded RNA and other viruses have single-stranded DNA genomes, and, in some circumstances, nucleic acid structures with three or four strands can form.

Nucleic acids are linear polymers (chains) of nucleotides. Each nucleotide consists of three components: a purine or pyrimidine nucleobase (sometimes termed "nitrogenous base" or simply "base"), a pentose sugar, and a phosphate group. The substructure consisting of a nucleobase plus sugar is termed a nucleoside. Nucleic acid types differ in the structure of the sugar in their nucleotides–DNA contains 2'-deoxyribose while RNA contains ribose (where the only difference is the presence of a hydroxyl group). Also, the nucleobases found in the two nucleic acid types are different: adenine, cytosine, and guanine are found in both RNA and DNA, while thymine occurs in DNA and uracil occurs in RNA.

The sugars and phosphates in nucleic acids are connected to each other in an alternating chain (sugar-phosphate backbone) through phosphodiester linkages. In conventional nomenclature, the carbons to which the phosphate groups attach are the 3'-end and the 5'-end carbons of the sugar. This gives nucleic acids directionality, and the ends of nucleic acid molecules are referred to as 5'-end and 3'-end. The nucleobases are joined to the sugars via an N-glycosidic linkage involving a nucleobase ring nitrogen (N-1 for pyrimidines and N-9 for purines) and the 1' carbon of the pentose sugar ring.

Non-standard nucleosides are also found in both RNA and DNA and usually arise from modification of the standard nucleosides within the DNA molecule or the primary (initial) RNA transcript. Transfer RNA (tRNA) molecules contain a particularly large number of modified nucleosides.

Double-stranded nucleic acids are made up of complementary sequences, in which extensive Watson-Crick base pairing results in a highly repeated and quite uniform double-helical three-dimensional structure. In contrast, single-stranded RNA and DNA molecules are not constrained to a regular double helix, and can adopt highly complex three-dimensional structures that are based on short stretches of intramolecular base-paired sequences including both Watson-Crick and noncanonical base pairs, and a wide range of complex tertiary interactions.

Nucleic acid molecules are usually unbranched, and may occur as linear and circular molecules. For example, bacterial chromosomes, plasmids, mitochondrial DNA, and chloroplast DNA are usually circular double-stranded DNA molecules, while chromosomes of the eukaryotic nucleus are usually linear double-stranded DNA molecules. Most RNA molecules are linear, single-stranded molecules, but both circular and branched molecules can result from RNA splicing reactions. The total amount of pyrimidine is equal to the total amount of purines. The diameter of the helix is about 20A.

One DNA or RNA molecule differs from another primarily in the sequence of nucleotides. Nucleotide sequences are of great importance in biology since they carry the ultimate instructions that encode all biological molecules, molecular assemblies, subcellular and cellular structures, organs, and organisms, and directly enable cognition, memory, and behavior ("see Genetics"). Enormous efforts have gone into the development of experimental methods to determine the nucleotide sequence of biological DNA and RNA molecules, and today hundreds of millions of nucleotides are sequenced daily at genome centers and smaller laboratories worldwide. In addition to maintaining the GenBank nucleic acid sequence database, the National Center for Biotechnology Information (NCBI, https://www.ncbi.nlm.nih.gov) provides analysis and retrieval resources for the data in GenBank and other biological data made available through the NCBI web site.

Deoxyribonucleic acid (DNA) is a nucleic acid containing the genetic instructions used in the development and functioning of all known living organisms. The DNA segments carrying this genetic information are called genes. Likewise, other DNA sequences have structural purposes, or are involved in regulating the use of this genetic information. Along with RNA and proteins, DNA is one of the three major macromolecules that are essential for all known forms of life.
DNA consists of two long polymers of simple units called nucleotides, with backbones made of sugars and phosphate groups joined by ester bonds. These two strands run in opposite directions to each other and are, therefore, anti-parallel. Attached to each sugar is one of four types of molecules called nucleobases (informally, bases). It is the sequence of these four nucleobases along the backbone that encodes information. This information is read using the genetic code, which specifies the sequence of the amino acids within proteins. The code is read by copying stretches of DNA into the related nucleic acid RNA in a process called transcription.
Within cells DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi, and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.

Ribonucleic acid (RNA) functions in converting genetic information from genes into the amino acid sequences of proteins. The three universal types of RNA include transfer RNA (tRNA), messenger RNA (mRNA), and ribosomal RNA (rRNA). Messenger RNA acts to carry genetic sequence information between DNA and ribosomes, directing protein synthesis. Ribosomal RNA is a major component of the ribosome, and catalyzes peptide bond formation. Transfer RNA serves as the carrier molecule for amino acids to be used in protein synthesis, and is responsible for decoding the mRNA. In addition, many other classes of RNA are now known.

Artificial nucleic acid analogues have been designed and synthesized by chemists, and include peptide nucleic acid, morpholino- and locked nucleic acid, glycol nucleic acid, and threose nucleic acid. Each of these is distinguished from naturally occurring DNA or RNA by changes to the backbone of the molecules.



</doc>
<doc id="21497" url="https://en.wikipedia.org/wiki?curid=21497" title="Nitrate">
Nitrate

Nitrate is a polyatomic ion with the molecular formula and a molecular mass of 62.0049 u. Nitrates also describe the organic functional group RONO. These nitrate esters are a specialized class of explosives.

The anion is the conjugate base of nitric acid, consisting of one central nitrogen atom surrounded by three identically bonded oxygen atoms in a trigonal planar arrangement. The nitrate ion carries a formal charge of −1. This results from a combination formal charge in which each of the three oxygens carries a − charge, whereas the nitrogen carries a +1 charge, all these adding up to formal charge of the polyatomic nitrate ion. This arrangement is commonly used as an example of resonance. Like the isoelectronic carbonate ion, the nitrate ion can be represented by resonance structures:

Almost all inorganic nitrate salts are soluble in water at standard temperature and pressure. A common example of an inorganic nitrate salt is potassium nitrate (saltpeter). A rich source of inorganic nitrate in the human body comes from diets rich in leafy green foods, such as spinach and arugula. NO3- (inorganic nitrate) is the viable active component within beetroot juice and other vegetables.

Dietary nitrate may be found in cured meats, various leafy vegetables, and drinking water; nitrite consumption is primarily determined by the amount of processed meats eaten, and the concentration of nitrates in these meats. Nitrite and water are converted in the body to nitric oxide, which could reduce hypertension. Anti-hypertensive diets, such as the DASH diet, typically contain high levels of nitrates, which are first reduced to nitrite in the saliva, as detected in saliva testing, prior to forming nitric oxide.

Nitrate salts are found naturally on earth as large deposits, particularly of nitratine, a major source of sodium nitrate.

Nitrites are produced by a number of species of nitrifying bacteria, and the nitrate compounds for gunpowder (see this topic for more) were historically produced, in the absence of mineral nitrate sources, by means of various fermentation processes using urine and dung.

Nitrates are found in fertilizers.

As a byproduct of lightning strikes in earth's nitrogen-oxygen rich atmosphere, nitric acid is produced when nitrogen dioxide reacts with water vapor.

Nitrates are mainly produced for use as fertilizers in agriculture because of their high solubility and biodegradability. The main nitrate fertilizers are ammonium, sodium, potassium, and calcium salts. Several million kilograms are produced annually for this purpose.

The second major application of nitrates is as oxidizing agents, most notably in explosives where the rapid oxidation of carbon compounds liberates large volumes of gases (see gunpowder for an example). Sodium nitrate is used to remove air bubbles from molten glass and some ceramics. Mixtures of the molten salt are used to harden some metals.

Explosives and table tennis balls are made from celluloid.

Although nitrites are the nitrogen compound chiefly used in meat curing, nitrates are used in certain specialty curing processes where a long release of nitrite from parent nitrate stores is needed. The use of nitrates in food preservation is controversial. This is due to the potential for the formation of nitrosamines when nitrates are present in high concentrations and the product is cooked at high temperatures. The effect is seen for red or processed meat, but not for white meat or fish. Potently, the production of carcinogenic nitrosamines may be inhibited by the use of the antioxidants vitamin C and the alpha-tocopherol form of vitamin E during curing.

Under simulated gastric conditions, nitrosothiols rather than nitrosamines are the main nitroso species being formed. The use of either compound is therefore regulated; for example, in the United States, the concentration of nitrates and nitrites is generally limited to 200 ppm or lower. They are considered irreplaceable in the prevention of botulinum poisoning from consumption of cured dry sausages by preventing spore germination.

Research has shown that dietary nitrate supplementation delivers positive results when testing endurance exercise performance.

The historical standard method of testing for nitrate is the Cadmium Reduction Method, which is reliable and accurate although it is dependent on a toxic metal cadmium and thus not suitable for all applications. An alternative method for nitrate and nitrite analysis is enzymatic reduction using nitrate reductase, which has recently been proposed by the US Environmental Protection Agency as an alternate test procedure for determining nitrate. An open source photometer has been developed for this method to accurately detect nitrate in water, soils, forage, etc. According to Hackaday this device can be built for US$65 to use this method to quantify nitrate accurately.

Free nitrate ions in solution can be detected by a nitrate ion selective electrode. Such electrodes function analogously to the pH selective electrode. This response is partially described by the Nernst equation.

Nitrate poisoning can occur through enterohepatic metabolism of nitrate due to nitrite being an intermediate. Nitrites oxidize the iron atoms in hemoglobin from ferrous iron(II) to ferric iron(III), rendering it unable to carry oxygen. This process can lead to generalized lack of oxygen in organ tissue and a dangerous condition called methemoglobinemia. Although nitrite converts to ammonia, if there is more nitrite than can be converted, the animal slowly suffers from a lack of oxygen.

Humans are subject to nitrate toxicity, with infants being especially vulnerable to methemoglobinemia due to nitrate metabolizing triglycerides present at higher concentrations than at other stages of development. Methemoglobinemia in infants is known as blue baby syndrome. Although nitrates in drinking water once were thought to be a contributing factor, there now are significant scientific doubts as to whether there is a causal link. Blue baby syndrome now is thought to be the product of a number of factors, which can include any factor that causes gastric upset, such as diarrhoeal infection, protein intolerance, heavy metal toxicity etc., with nitrates playing a minor role. Nitrates, if a factor in a specific case, would most often be ingested by infants in high nitrate drinking water, however, nitrate exposure also may occur if eating, for instance, vegetables containing high levels of nitrate. Lettuce may contain elevated nitrate under growth conditions such as reduced sunlight, undersupply of the essential micronutrients molybdenum (Mo), and iron (Fe), or high concentrations of nitrate due to reduced assimilation of nitrate in the plant. High levels of nitrate fertilization also contribute to elevated levels of nitrate in the harvested plant.

Some adults may be more susceptible to the effects of nitrates than others. The methemoglobin reductase enzyme may be under-produced or absent in certain people who have an inherited mutation. Such individuals cannot break down methemoglobin so rapidly as those who do have the enzyme, leading to increased circulating levels of methemoglobin (the implication being that their blood is not so oxygen-rich as that of the others). Those with insufficient stomach acid, including some vegetarians and vegans, may also be at risk. It is the increased consumption of green, leafy vegetables that typically accompanies these types of diets, that may lead to increased nitrate intake. A wide variety of medical conditions, including food allergies, asthma, hepatitis, and gallstones may be linked with low stomach acid; these individuals also may be highly sensitive to the effects of nitrate.

Methemoglobinemia may be treated with methylene blue, which reduces ferric iron(III) in affected blood cells back to ferrous iron(II).

In freshwater or estuarine systems close to land, nitrate can reach high levels that can potentially cause the death of fish. While nitrate is much less toxic than ammonia, levels over 30 ppm of nitrate can inhibit growth, impair the immune system and cause stress in some aquatic species. However, in light of inherent problems with past protocols on acute nitrate toxicity experiments, the extent of nitrate toxicity has been the subject of recent debate.

In most cases of excess nitrate concentrations in aquatic systems, the primary source is surface runoff from agricultural or landscaped areas that have received excess nitrate fertilizer. This is called eutrophication and can lead to algae blooms. As well as leading to water anoxia and dead zones, these blooms may cause other changes to ecosystem function, favouring some groups of organisms over others. As a consequence, as nitrate forms a component of total dissolved solids, they are widely used as an indicator of water quality.

Symptoms of nitrate poisoning include increased heart rate and respiration; in advanced cases blood and tissue may turn a blue or brown color. Feed can be tested for nitrate; treatment consists of supplementing or substituting existing supplies with lower nitrate material. Safe levels of nitrate for various types of livestock are as follows:

The values above are on a dry (moisture-free) basis.

Nitrate formation with elements of the periodic table.



</doc>
<doc id="21502" url="https://en.wikipedia.org/wiki?curid=21502" title="Nike">
Nike

Nike often refers to:
Nike may also refer to:






</doc>
<doc id="21503" url="https://en.wikipedia.org/wiki?curid=21503" title="Nevis">
Nevis

Nevis is a small island in the Caribbean Sea that forms part of the inner arc of the Leeward Islands chain of the West Indies. Nevis and the neighbouring island of Saint Kitts constitute one country: the Federation of Saint Kitts and Nevis. Nevis is located near the northern end of the Lesser Antilles archipelago, about 350 km east-southeast of Puerto Rico and 80 km west of Antigua. Its area is and the capital is Charlestown.

Saint Kitts and Nevis are separated by a shallow channel known as "The Narrows". Nevis is roughly conical in shape with a volcano known as Nevis Peak at its centre. The island is fringed on its western and northern coastlines by sandy beaches which are composed of a mixture of white coral sand with brown and black sand which is eroded and washed down from the volcanic rocks that make up the island. The gently-sloping coastal plain ( wide) has natural freshwater springs as well as non-potable volcanic hot springs, especially along the western coast.

The island was named "Oualie" ("Land of Beautiful Waters") by the Caribs and "Dulcina" ("Sweet Island") by the early British settlers. The name "Nevis" is derived from the Spanish "Nuestra Señora de las Nieves" (which means Our Lady of the Snows); the name first appears on maps in the 16th century. Nevis is also known by the sobriquet "Queen of the Caribees", which it earned in the 18th century, when its sugar plantations created much wealth for the British.

Nevis is of particular historical significance to Americans because it was the birthplace and early childhood home of Alexander Hamilton. For the British, Nevis is the place where Horatio Nelson was stationed as a young sea captain, and is where he met and married a Nevisian, Frances Nisbet, the young widow of a plantation-owner.

The majority of the approximately 12,000 citizens of Nevis are of primarily African descent, with notable British, Portuguese and Lebanese minority communities. English is the official language, and the literacy rate, 98 percent, is one of the highest in the Western Hemisphere.

In 1498, Christopher Columbus gave the island the name "San Martin" (Saint Martin). However, the confusion of numerous poorly-charted small islands in the Leeward Island chain meant that this name ended up being accidentally transferred to another island, which is still known as Saint-Martin/Sint Maarten.

The current name "Nevis" was derived from a Spanish name "Nuestra Señora de las Nieves" by a process of abbreviation and anglicisation. The Spanish name means Our Lady of the Snows. It is not known who chose this name for the island, but it is a reference to the story of a 4th-century Catholic miracle: a snowfall on the Esquiline Hill in Rome. Presumably the white clouds that usually cover the top of Nevis Peak reminded someone of this story of a miraculous snowfall in a hot climate.

Nevis was part of the Spanish claim to the Caribbean islands, a claim pursued until the Treaty of Madrid (1670), even though there were no Spanish settlements on the island. According to Vincent Hubbard, author of "Swords, Ships & Sugar: History of Nevis", the Spanish ruling caused many of the Arawak groups who were not ethnically Caribs to "be redefined as Caribs overnight". Records indicate that the Spanish enslaved large numbers of the native inhabitants on the more accessible of the Leeward Islands and sent them to Cubagua, Venezuela to dive for pearls. Hubbard suggests that the reason the first European settlers found so few "Caribs" on Nevis is that they had already been rounded up by the Spanish and shipped off to be used as slaves.

Nevis was first sighted by Columbus in 1493; the island had been settled for more than two thousand years by Amerindian people. The indigenous people of Nevis during these periods belonged to the Leeward Island Amerindian groups popularly referred to as Arawaks and Caribs, a complex mosaic of ethnic groups with similar culture and language. Dominican anthropologist Lennox Honychurch traces the European use of the term "Carib" to refer to the Leeward Island aborigines to Columbus, who picked it up from the Taínos on Hispaniola. It was not a name the Caribs called themselves. "Carib Indians" was the generic name used for all groups believed involved in cannibalistic war rituals, more particularly, the consumption of parts of a killed enemy's body.

The Amerindian name for Nevis was "Oualie", land of beautiful waters. The structure of the Island Carib language has been linguistically identified as Arawakan.

In spite of the Spanish claim, Nevis continued to be a popular stop-over point for English and Dutch ships on their way to the North American continent. Captain Bartholomew Gilbert of Plymouth visited the island in 1603, spending two weeks to cut twenty tons of lignum vitae wood. Gilbert sailed on to Virginia to seek out survivors of the Roanoke settlement in what is now North Carolina. Captain John Smith visited Nevis also on his way to Virginia in 1607. This was the voyage which founded Jamestown, the first permanent English settlement in the New World.

On 30 August 1620, James 6th of Scotland – James I of England asserted sovereignty over Nevis by giving a Royal Patent for colonisation to the Earl of Carlisle. However, actual European settlement did not happen until 1628, when Anthony Hilton moved from nearby Saint Kitts following a murder plot against him. He was accompanied by 80 other settlers, soon to be boosted by a further 100 settlers from London who had originally hoped to settle Barbuda. Hilton became the first Governor of Nevis. After the Treaty of Madrid (1670) between Spain and England, Nevis became the seat of the British colony and the Admiralty Court also sat in Nevis. Between 1675 and 1730, the island was the headquarters for the slave trade for the Leeward Islands, with approximately 6,000–7,000 enslaved West Africans passing through en route to other islands each year. The Royal African Company brought all its ships through Nevis. A 1678 census shows a community of Irish people – 22% of the population – existing as either indentured servants or freemen.

Due to the profitable Slave Trade and the high quality of Nevisian sugar cane, the island soon became a dominant source of wealth for Great Britain and the slave-owning British plantocracy. When the Leeward Islands were separated from Barbados in 1671, Nevis became the seat of the Leeward Islands colony and was given the nickname "Queen of the Caribees". It remained colonial capital for the Leeward Islands until the seat was transferred to Antigua for military reasons in 1698. During this period, Nevis was the richest of the British Leeward Islands. The island outranked larger islands like Jamaica in sugar production in the late 17th century. The wealth of the planters on the island is evident in the tax records preserved at the Calendar State Papers in the British Colonial Office Public Records, where the amount of tax collected on the Leeward Islands was recorded. The sums recorded for 1676 as "head tax on slaves", a tax payable in sugar, amounted to 384,600 pounds in Nevis, as opposed to 67,000 each in Antigua and Saint Kitts, 62,500 in Montserrat, and 5,500 total in the other five islands. The profits on sugar cultivation in Nevis was enhanced by the fact that the cane juice from Nevis yielded an unusually high amount of sugar. A gallon (3.79 litres) of cane juice from Nevis yielded 24 ounces (0.71 litres) of sugar, whereas a gallon from Saint Kitts yielded 16 ounces (0.47 litres). Twenty percent of the British Empire's total sugar production in 1700 was derived from Nevisian plantations. Exports from West Indian colonies like Nevis were worth more than all the exports from all the mainland Thirteen Colonies of North America combined at the time of the American Revolution.

The enslaved families formed the large labour force required to work the sugar plantations. After the 1650s the supply of white indentured servants began to dry up due to increased wages in England and less incentive to migrate to the colonies. By the end of the 17th century, the population of Nevis consisted of a small, rich planter elite in control, a marginal population of poor whites, a great majority of African-descended slaves, and an unknown number of maroons, escaped slaves living in the mountains. In 1780, 90 percent of the 10,000 people living on Nevis were black. Some of the maroons joined with the few remaining Caribs in Nevis to form a resistance force. Memories of the Nevisian maroons' struggle under the plantation system are preserved in place names such as Maroon Hill, an early centre of resistance.

The great wealth generated by the colonies of the West Indies led to wars between Spain, Britain, and France. The formation of the United States can be said to be a partial by-product of these wars and the strategic trade aims that often ignored North America. Three privateers (William Kidd being one of them) were employed by the British Crown to help protect ships in Nevis' waters.

During the 17th century, the French, based on Saint Kitts, launched many attacks on Nevis, sometimes assisted by the Island Caribs, who in 1667 sent a large fleet of canoes along in support. In the same year a Franco-Dutch invasion fleet was repelled off Nevis by an English fleet. Letters and other records from the era indicate that the English on Nevis hated and feared the Amerindians. In 1674 and 1683 they participated in attacks on Carib villages in Dominica and St. Vincent, in spite of a lack of official approval from the Crown for the attack.

On Nevis, the English built Fort Charles and a series of smaller fortifications to aid in defending the island. This included Saddle Hill Battery, built in 1740 to replace a deodand on Mount Nevis.

In 1706, Pierre Le Moyne d'Iberville, the French Canadian founder of Louisiana in North America, decided to drive the English out of Nevis and thus also stop pirate attacks on French ships; he considered Nevis the region's headquarters for piracy against French trade. During d'Iberville's invasion of Nevis, French buccaneers were used in the front line, infamous for being ruthless killers after the pillaging during the wars with Spain where they gained a reputation for torturing and murdering non-combatants. In the face of the invading force, the English militiamen of Nevis fled. Some planters burned the plantations, rather than letting the French have them, and hid in the mountains. It was the enslaved Africans who held the French at bay by taking up arms to defend their families and the island. The slave quarters had been looted and burned as well, as the main reward promised the men fighting on the French side in the attack was the right to capture as many slaves as possible and resell them in Martinique.

During the fighting, 3,400 enslaved Nevisians were captured and sent off to Martinique, but about 1,000 more, poorly armed and militarily untrained, held the French troops at bay, by "murderous fire" according to an eyewitness account by an English militiaman. He wrote that "the slaves' brave behaviour and defence there shamed what some of their masters did, and they do not shrink to tell us so." After 18 days of fighting, the French were driven off the island. Among the Nevisian men, women and children carried away on d'Iberville's ships, six ended up in Louisiana, the first persons of African descent to arrive there.

One consequence of the French attack was a collapsed sugar industry and during the ensuing hardship on Nevis, small plots of land on the plantations were made available to the enslaved families in order to control the loss of life due to starvation. With less profitability for the absentee plantation owners, the import of food supplies for the plantation workers dwindled. Between 1776 and 1783, when the food supplies failed to arrive altogether due to the rebellion in North America, 300–400 enslaved Nevisians starved to death. On 1 August 1834, slavery was abolished in the British Empire. In Nevis, 8,815 slaves were freed. The first Monday in August is celebrated as Emancipation Day and is part of the annual Nevis Culturama festival.

A four-year apprenticeship programme followed the abolishment of slavery on the plantations. In spite of the continued use of the labour force, the Nevisian slave owners were paid over £150,000 in compensation from the British Government for the loss of property, whereas the enslaved families received nothing for 200 years of labour. One of the wealthiest planter families in Nevis, the Pinneys of Montravers Plantation, claimed £36,396 (worth close to £1,800,000 today) in compensation for the slaves on the family-owned plantations around the Caribbean.

Because of the early distribution of plots and because many of the planters departed from the island when sugar cultivation became unprofitable, a relatively large percentage of Nevisians already owned or controlled land at emancipation. Others settled on crown land. This early development of a society with a majority of small, landowning farmers and entrepreneurs created a stronger middleclass in Nevis than in Saint Kitts where the sugar industry continued until 2006. Even though the 15 families in the wealthy planter elite no longer control the arable land, Saint Kitts still has a large, landless working class population.

Nevis was united with Saint Kitts and Anguilla in 1882, and they became an associated state with full internal autonomy in 1967, though Anguilla seceded in 1971. Together, Saint Kitts and Nevis became independent on 19 September 1983. On 10 August 1998, a referendum on Nevis to separate from Saint Kitts had 2,427 votes in favour and 1,498 against, falling short of the two-thirds majority needed.

Before 1967, the local government of Saint Kitts was also the government of Nevis and Anguilla. Nevis had two seats and Anguilla one seat in the government. The economic and infrastructural development of the two smaller islands was not a priority to the colonial federal government.

When the hospital in Charlestown was destroyed in a hurricane in 1899, planting of trees in the squares of Saint Kitts and refurbishing of government buildings, also in Saint Kitts, took precedence over the rebuilding of the only hospital in Nevis. After five years without any proper medical facilities, the leaders in Nevis initiated a campaign, threatening to seek independence from Saint Kitts. The British Administrator in Saint Kitts, Charles Cox, was unmoved. He stated that Nevis did not need a hospital since there had been no significant rise in the number of deaths during the time Nevisians had been without a hospital. Therefore, no action was needed on behalf of the government, and besides, Cox continued, the Legislative Council regarded "Nevis and Anguilla as a drag on St. Kitts and would willingly see a separation". Finally, a letter of complaint to the metropolitan British Foreign Office gave result and the federal government in Saint Kitts was ordered by their superiors in London to take speedy action. The Legislative Council took another five years to consider their options. The final decision by the federal government was to not rebuild the old hospital after all, but to instead convert the old Government House in Nevis into a hospital, named Alexandra Hospital after Queen Alexandra, wife of King Edward VII. A majority of the funds assigned for the hospital could thus spent on the construction of a new official residence in Nevis.

After d'Iberville's invasion in 1704, records show Nevis’ sugar industry in ruins and a decimated population begging the English Parliament and relatives for loans and monetary assistance to stave off island-wide starvation. The sugar industry on the island never fully recovered and during the general depression that followed the loss of the West Indian sugar monopoly, Nevis fell on hard times and the island became one of the poorest in the region. The island remained poorer than Saint Kitts until 1991, when the fiscal performance of Nevis edged ahead of the fiscal performance of Saint Kitts for the first time since the French invasion. 

Electricity was introduced in Nevis in 1954, when two generators were shipped in to provide electricity to the area around Charlestown. In this regard, Nevis fared better than Anguilla, where there were no paved roads, no electricity and no telephones up until 1967. However, electricity did not become available island-wide on Nevis until 1971.

An ambitious infrastructure development programme was introduced in the early 2000s which included a transformation of the Charlestown port, construction of a new deep-water harbour, resurfacing and widening the Island Main Road, a new airport terminal and control tower, and a major airport expansion, which required the relocation of an entire village in order to make room for the runway extension.

Modernised classrooms and better equipped schools, as well as improvements in the educational system, have contributed to a leap in academic performance on the island. The pass rate among the Nevisian students sitting for the Caribbean Examination Council (CXC) exams, the Cambridge General Certificate of Education Examination (GCE) and the Caribbean Advance Proficiency Examinations is now consistently among the highest in the English-speaking Caribbean.


The formation of the island began in mid-Pliocene times, approximately 3.45 million years ago. Nine distinct eruptive centres from different geological ages, ranging from mid-Pliocene to Pleistocene, have contributed to the formation. No single model of the island's geological evolution can therefore be ascertained.

Nevis Peak (985 m /3,232 ft) is the dormant remnant of one of these ancient stratovolcanoes. The last activity took place about 100,000 years ago, but active fumaroles and hot springs are still found on the island, the most recent formed in 1953. The composite cone of Nevis volcano has two overlapping summit craters that are partially filled by a lava dome, created in recent, pre-Columbian time. Pyroclastic flows and mudflows were deposited on the lower slopes of the cone simultaneously. Nevis Peak is located on the outer crater rim. Four other lava domes were constructed on the flanks of the volcano, one on the northeast flank (Madden's Mount), one on the eastern flank (Butlers Mountain), one on the northwest coast (Mount Lily) and one on the south coast (Saddle Hill, with a height of 375 metres). The southernmost point on the island is Dogwood Point which is also the southernmost point of the Federation of Saint Kitts and Nevis.

During the last ice age, when the sea level was 60 m lower, the three islands of Saint Kitts, Nevis and Sint Eustatius (also known as Statia) were connected as one island. Saba, however, is separated from these three by a deeper channel.

There are visible wave-breaking reefs along the northern and eastern shorelines. To the south and west, the reefs are located in deeper water and are suitable for scuba diving. The most developed beach on Nevis is the 6.5 km long Pinney's Beach, on the western or Caribbean coast. There are sheltered swimming beaches in Oualie Bay and Cades Bay. The eastern coast of the island faces into the Atlantic Ocean, and can have strong surf in parts of the shore which are unprotected by fringing coral reefs. The colour of the sand on the beaches of Nevis is variable: on a lot of the bigger beaches the sand is a yellow grey in colour, but some beaches on the southern coast have darker, reddish, or even black sand. Under a microscope it becomes clear that Nevis sand is a mixture of tiny fragments of coral, many foraminifera, and small crystals of the various mineral constituents of the volcanic rock of which the island is made.

Seven volcanic centers make up Nevis. These include Round Hill (3.43 Ma), Cades Bay (3.22 Ma), Hurricane Hill (2.7 Ma), Saddle Hill (1.8 Ma), Butlers Mountain (1.1 Ma), Red Cliff and Nevis Peak (0.98 Ma). These are mainly andesite and dacite lava domes, with associated block and ash flows, plus lahars.Nevis Peak has the highest elevation at 984 m. Cades Bay and Farm Estate Soufriere are noted areas of hydrothermal activity.

Water has been piped since 1911 from a spring called the "Source", located 1800 feet up the mountain, to storage tanks at Rawlins Village, and since 1912, to Butler's Village. Additional drinking water comes from Nelson's Spring near Cotton Ground and Bath Spring. Groundwater has been extracted since the 1990s, and mixed with the Source water.

During the 17th and 18th centuries, massive deforestation was undertaken by the planters as the land was initially cleared for sugar cultivation. This intense land exploitation by the sugar and cotton industry lasted almost 300 years, and greatly changed the island's ecosystem.

In some places along the windswept southeast or "Windward" coast of the island, the landscape is radically altered compared with how it used to be in pre-colonial times. Due to extreme land erosion, the top soil was swept away, and in some places at the coast, sheer cliffs as high as 25 metres (82 ft) have developed.

Thick forest once covered the eastern coastal plain, where the Amerindians built their first settlements during the Aceramic period, complimenting the ecosystem surrounding the coral reef just offshore. It was the easy access to fresh water on the island and the rich food source represented by the ocean life sheltered by the reef that made it feasible for the Amerindians to settle this area around 600 BC. With the loss of the natural vegetation, the balance in runoff nutrients to the reef was disturbed, eventually causing as much as 80 percent of the large eastern fringing reef to become inactive. As the reef broke apart, it in turn provided less protection for the coastline.

During times of maximum cultivation, sugar cane fields stretched from the coastline of Nevis up to an altitude at which the mountain slopes were too steep and rocky to farm. Nonetheless, once the sugar industry was finally abandoned, vegetation on the leeward side of the island regrew reasonably well, as scrub and secondary forest.

Nevis has several natural freshwater springs (including Nelson's Spring). The island also has numerous non-potable volcanic hot springs, including most notably the Bath Spring near Bath village, just south of the capital Charlestown.

After heavy rains, powerful rivers of rainwater pour down the numerous ravines (known as ghauts). When the water reaches the coastline, the corresponding coastal ponds, both freshwater and brackish, fill to capacity and beyond, spilling over into the sea.

With modern development, the existing freshwater springs are no longer enough to supply water to the whole island. The water supply now comes mostly from Government wells. The major source of potable water for the island is groundwater, obtained from 14 active wells. Water is pumped from the wells, stored and allowed to flow by gravity to the various locations.

The climate is tropical with little variation, tempered all year round (but particularly from December through February) by the steady north-easterly winds, called the trade winds. There is a slightly hotter and somewhat rainier season from May to November.

Nevis lies within the track area of tropical storms and occasional hurricanes. These storms can develop between August and October. This time of year has the heaviest rainfalls.

The official currency is the Eastern Caribbean dollar (EC$), which is shared by eight other territories in the region.

The European Commission's Delegation in Barbados and the Eastern Caribbean estimates the annual per capita Gross Domestic Product (GDP) on Nevis to be about 10 percent higher than on St. Kitts.

The major source of revenue for Nevis today is tourism. During the 2003–2004 season, approximately 40,000 tourists visited Nevis. A five star hotel "(The Four Seasons Resort Nevis, West Indies)", four exclusive restored plantation inns, and several smaller hotels, are currently in operation. Larger developments along the west coast have recently been approved and are in the process of being developed.

The introduction of secrecy legislation has made offshore financial services a rapidly growing economic sector in Nevis. Incorporation of companies, international insurance and reinsurance, as well as several international banks, trust companies, asset management firms, have created a boost in the economy. During 2005, the Nevis Island Treasury collected $94.6 million in annual revenue, compared to $59.8 million during 2001. In 1998, 17,500 international banking companies were registered in Nevis. Registration and annual filing fees paid in 1999 by these entities amounted to over 10 percent of Nevis’ revenues. The offshore financial industry gained importance during the financial disaster of 1999 when Hurricane Lenny damaged the major resort on the island, causing the hotel to be closed down for a year and 400 of the 700 employees to be laid off.

In 2000, the Financial Action Task Force, part of the Organisation for Economic Co-operation and Development (OECD), issued a blacklist of 35 nations which were said to be non-cooperative in the campaign against tax evasion and money laundering. The list included the Federation of Saint Kitts and Nevis, as well as Liechtenstein, Monaco, Luxembourg, the British Channel Islands, Israel, and Russia.

The political structure for the Federation of Saint Kitts and Nevis is based on the Westminster Parliamentary system, but it is a unique structure in that Nevis has its own unicameral legislature, consisting of Her Majesty's representative (the Deputy Governor General) and members of the Nevis Island Assembly. Nevis has considerable autonomy in its legislative branch. The constitution actually empowers the Nevis Island Legislature to make laws that cannot be abrogated by the National Assembly. In addition, Nevis has a constitutionally protected right to secede from the federation, should a two-third majority of the island's population vote for independence in a local referendum. Section 113.(1) of the constitution states: "The Nevis Island Legislature may provide that the island of Nevis shall cease to be federated with the island of Saint Christopher and accordingly that this Constitution shall no longer have effect in the island of Nevis."

Nevis has its own premier and its own government, the Nevis Island Administration. It collects its own taxes and has a separate budget, with a current account surplus. According to a statement released by the Nevis Ministry of Finance in 2005, Nevis had one of the highest growth rates in gross national product and per capita income in the Caribbean at that point.

The federal prime minister, Timothy Harris, is the leader of the majority party of the federal House of Representatives in Saint Kitts, and his cabinet conducts the affairs of state. The Federation of Saint Kitts and Nevis has a 14 or 15-member unicameral legislature or parliament (the Senate and House of Representatives sit and vote together): A Senate, with three or four members appointed by the governor general on the advice of the prime minister and the leader of the opposition; and a popularly elected House of Representatives with 11 members, eight Saint Kitts seats and three Nevis seats. The prime minister and the cabinet are responsible to the Parliament.

Nevis elections are scheduled every five years. The Nevis elections of 2013, called on 23 January 2013, was won by the party in opposition, the Concerned Citizens Movement (CCM), led by Vance Amory. The CCM won three of the five seats in the Nevis Island Assembly, while the incumbent party, the Nevis Reformation Party (NRP), won two.

In the federal elections of 2010, the CCM won two of the three Nevis assigned Federal seats, while the NRP won one. Of the eight Saint Kitts assigned federal seats, the St Kitts-Nevis Labour Party won six and the People's Action Movement (PAM) two.

Joseph Parry, leader of the opposition, has indicated that he favours constitutional reform over secession for Nevis. His party, the NRP, has historically been the strongest and most ardent proponent for Nevis independence; the party came to power with secession as the main campaign issue. In 1975, the NRP manifesto declared that: "The Nevis Reformation Party will strive at all costs to gain secession for Nevis from St. Kitts – a privilege enjoyed by the island of Nevis prior to 1882."

A cursory proposal for constitutional reform was presented by the NRP in 1999, but the issue was not prominent in the 2006 election campaign and it appears a detailed proposal has yet to be worked out and agreed upon within the party.

In "Handbook of Federal Countries" published by Forum of Federations, the authors consider the constitution problematic because it does not "specifically outline" the federal financial arrangements or the means by which the central government and Nevis Island Administration can raise revenue: "In terms of the NIA, the constitution only states (in s. 108(1)) that 'all revenues...raised or received by the Administration...shall be paid into and form a fund styled the Nevis Island Consolidated Fund.' [...] Section 110(1) states that the proceeds of all 'takes' collected in St. Kitts and Nevis under any law are to be shared between the federal government and the Nevis Island Administration based on population. The share going to the NIA, however, is subject to deductions (s. 110(2)), such as the cost of common services and debt charges, as determined by the Governor-General (s.110(3)) on the advice of the Prime Minister who can also take advice from the Premier of Nevis (s.110(4))."

According to a 1995 report by the Commonwealth Observer Group of the Commonwealth Secretariat, "the federal government is also the local government of St Kitts and this has resulted in a perception among the political parties in Nevis that the interests of the people of Nevis are being neglected by the federal government which is more concerned with the administration of St Kitts than with the federal administration."

Simeon Daniel, Nevis' first Premier and former leader of the Nevis Reformation Party (NRP) and Vance Amory, Premier and leader of the Concerned Citizens Movement (CCM), made sovereign independence for Nevis from the Federation of Saint Kitts and Nevis part of their parties' agenda. Since independence from the United Kingdom in 1983, the Nevis Island Administration and the Federal Government have been involved in several conflicts over the interpretation of the new constitution which came into effect at independence. During an interview on Voice of America in March 1998, repeated in a government issued press release headlined "PM Douglas Maintains 1983 Constitution is Flawed", Prime Minister Denzil Douglas called the constitution a "recipe for disaster and disharmony among the people of both islands".

A crisis developed in 1984 when the People's Action Movement (PAM) won a majority in the Federal elections and temporarily ceased honouring the Federal Government's financial obligations to Nevis. Consequently, cheques issued by the Nevis Administration were not honoured by the Bank, public servants in Nevis were not paid on time and the Nevis Island Administration experienced difficulties in meeting its financial obligations.

There is also substantial support in Nevis for British Overseas Territory status similarly to Anguilla, which was formerly the third of the tri-state Saint Christopher-Nevis-Anguilla colony.

In 1996, four new bills were introduced in the National Assembly in Saint Kitts, one of which made provisions to have revenue derived from activities in Nevis paid directly to the treasury in Saint Kitts instead of to the treasury in Nevis. Another bill, The Financial Services Committee Act, contained provisions that all investments in Saint Kitts and Nevis would require approval by an investment committee in Saint Kitts. This was controversial, because ever since 1983 the Nevis Island Administration had approved all investments for Nevis, on the basis that the constitution vests legislative authority for industries, trades and businesses and economic development in Nevis to the Nevis Island Administration.

All three representatives from Nevis, including the leader of the opposition in the Nevis Island Assembly, objected to the introduction of these bills into the National Assembly in Saint Kitts, arguing that the bills would affect the ability of Nevis to develop its offshore financial services sector and that the bills would be detrimental to the Nevis economy. All the representatives in opposition in the National Assembly shared the conviction that the bills, if passed into law, would be unconstitutional and undermine the constitutional and legislative authority of the Nevis Island Administration, as well as result in the destruction of the economy of Nevis.

The constitutional crisis initially developed when the newly appointed Attorney General refused to grant permission for the Nevis Island Administration to assert its legal right in the Courts. After a decision of the High Court in favour of the Nevis Island Administration, the Prime Minister gave newspaper interviews stating that he "refused to accept the decision of the High Court". Due to the deteriorating relationship between the Nevis Island Administration and the Federal Government, a Constitutional Committee was appointed in April 1996 to advise on whether or not the present constitutional arrangement between the islands should continue. The committee recommended constitutional reform and the establishment of an island administration for Saint Kitts, separate from the Federal Government.

The Federal Government in Saint Kitts fills both functions today and Saint Kitts does not have an equivalent to the Nevis Island Administration. Disagreements between the political parties in Nevis and between the Nevis Island Administration and the Federal Government have prevented the recommendations by the electoral committee from being implemented. The problematic political arrangement between the two islands therefore continues to date.

Nevis has continued developing its own legislation, such as The Nevis International Insurance Ordinance and the Nevis International Mutual Funds Ordinance of 2004, but calls for secession are often based on concerns that the legislative authority of the Nevis Island Administration might be challenged again in the future.

The issues of political dissension between Saint Kitts and Nevis are often centred around perceptions of imbalance in the economic structure. As noted by many scholars, Nevisians have often referred to a structural imbalance in Saint Kitts' favour in how funds are distributed between the two islands and this issue has made the movement for Nevis secession a constant presence in the island's political arena, with many articles appearing in the local press expressing concerns such as those compiled by Everton Powell in "What Motivates Our Call for Independence": 

A referendum on secession from the Federation of St. Kitts and Nevis was held in 1998. Although 62% voted in favor of a secession, a two-thirds majority would have been necessary for the referendum to succeed.

The island of Nevis is divided into five administrative subdivisions called parishes, each of which has an elected representative in the Nevis Island Assembly. The division of this almost round island into parishes was done in a circular sector pattern, so each parish is shaped like a pie slice, reaching from the highest point of Nevis Peak down to the coastline.

The parishes have double names, for example Saint George Gingerland. The first part of the name is the name of the patron saint of the parish church, and the second part of the name is the traditional common name of the parish. Often the parishes are referred to simply by their common names. The religious part of a parish name is sometimes written or pronounced in the possessive: Saint George's Gingerland.

The five parishes of Nevis are:

"Culturama", the annual cultural festival of Nevis, is celebrated during the Emancipation Day weekend, the first week of August. The festivities include many traditional folk dances, such as the masquerade, the Moko jumbies on stilts, Cowboys and Indians, and Plait the Ribbon, a May pole dance. The celebration was given a more organised form in 1974, including a Miss Culture Show and a Calypso Competition, as well as drama performances, old fashion Troupes (including Johnny Walkers, Giant and Spear, Bulls, Red Cross and Blue Ribbon), arts and crafts exhibitions and recipe competitions. According to the Nevis Department of Culture, the aim is to protect and encourage indigenous folklore, in order to make sure that the uniquely Caribbean culture can "reassert itself and flourish".

The official language is English and Saint Kitts Creole (known on the island as 'Nevisian' or 'Nevis creole') is also widely spoken. The local creole is actually more widely spoken on Nevis than on the neighbouring island.

Nevisian culture has since the 17th century incorporated African, European and East Indian cultural elements, creating a distinct Afro-Caribbean culture. Several historical anthropologists have done field research Nevis and in Nevisian migrant communities in order to trace the creation and constitution of a Nevisian cultural community. Karen Fog Olwig published her research about Nevis in 1993, writing that the areas where the Afro-Caribbean traditions were especially strong and flourishing relate to kinship and subsistence farming. However, she adds, Afro-Caribbean cultural impulses were not recognised or valued in the colonial society and were therefore often expressed through Euro-Caribbean cultural forms. Examples of European forms appropriated to express Afro-Caribbean culture are the Nevisian and Kittitian "Tea Meetings" and "Christmas Sports". According to anthropologist Roger D. Abrahams, these traditional performance art forms are "Nevisian approximation of British performance codes, techniques, and patterns". He writes that the Tea Meetings were staged as theatrical "battles between decorum and chaos", decorum represented by the ceremony chairmen and chaos the hecklers in the audience, with a diplomatic King or a Queen presiding over the battle to ensure fairness.

The Christmas Sports included a form of comedy and satire based on local events and gossip. They were historically an important part of the Christmas celebrations in Nevis, performed on Christmas Eve by small troupes consisting of five or six men accompanied by string bands from different parts of the island. One of the men in the troupe was dressed as a woman, playing all the female parts in the dramatisations. The troupes moved from yard to yard to perform their skits, using props, face paint and costumes to play the roles of well-known personalities in the community. Examples of gossip about undesired behaviour that could surface in the skits for comic effect were querulous neighbours, adulterous affairs, planters mistreating workers, domestic disputes or abuse, crooked politicians and any form of stealing or cheating experienced in the society. Even though no names were mentioned in these skits, the audience would usually be able to guess who the heckling message in the troupe's dramatised portrayals was aimed at, as it was played out right on the person's own front yard. The acts thus functioned as social and moral commentaries on current events and behaviours in Nevisian society. This particular form is called "Bazzarding" by many locals. Abrahams theorises that Christmas Sports are rooted in the pre-emancipation Christmas and New Year holiday celebrations, when the enslaved population had several days off.

American folklorist and musicologist Alan Lomax visited Nevis in 1962 in order to conduct long-term research into the black folk culture of the island. His field trip to Nevis and surrounding islands resulted in the anthology "Lomax Caribbean Voyage" series. 
Among the Nevisians recorded were chantey-singing fishermen in a session organised in a rum shop in Newcastle; Santoy, the Calypsonian, performing calypsos by Nevisian ballader and local legend Charles Walters to guitar and cuatro; and string bands, fife players and drummers from Gingerland, performing quadrilles.

The island is also known for "Jamband music", which is the kind of music performed by local bands during the "Culturama Festival" and is key to "Jouvert" dancing. The sounds of the so-called "Iron Band" are also popular within the culture; many locals come together using any old pans, sinks, or other kits of any sort; which they use to create sounds and music. This form of music is played throughout the villages during the Christmas and carnival seasons.

A series of earthquakes during the 18th century severely damaged most of the colonial-era stone buildings of Charlestown. The Georgian stone buildings in Charlestown that are visible today had to be partially rebuilt after the earthquakes, and this led to the development of a new architectural style, consisting of a wooden upper floor over a stone ground floor; the new style resisted earthquake damage much more effectively.

Two famous Nevisian buildings from the 18th century are Hermitage Plantation, built of lignum vitae wood in 1740, the oldest surviving wooden house still in use in the Caribbean today, and the Bath Hotel, the first hotel in the Caribbean, a luxury hotel and spa built by John Huggins in 1778. The soothing waters of the hotel's hot spring and the lively social life on Nevis attracted many famous Europeans including Antigua-based Admiral Nelson, and Prince William Henry, Duke of Clarence, (future William IV of the United Kingdom), who attended balls and private parties at the Bath Hotel. Today, the building serves as government offices, and there are two outdoor hot-spring bathing spots which were specially constructed in recent years for public use.

An often repeated legend appears to suggest that a destructive 1680 or 1690 earthquake and tsunami destroyed the buildings of the original capital Jamestown on the west coast. Folk tales say that the town sank beneath the ocean, and the tsunami is blamed for the escape of (possibly fictional) pirate Red Legs Greaves. However, archaeologists from the University of Southampton who have done excavations in the area, have found no evidence to indicate that the story is true. They state that this story may originate with an over-excited Victorian letter writer sharing somewhat exaggerated accounts of his exotic life in the tropical colony with a British audience back home. One such letter recounts that so much damage was done to the town that it was completely evacuated, and was engulfed by the sea. Early maps do not, however, actually show a settlement called "Jamestown", only "Morton's Bay", and later maps show that all that was left of Jamestown/Morton's Bay in 1818 was a building labelled "Pleasure House". Very old bricks that wash up on Pinney's Beach after storms may have contributed to this legend of a sunken town; however these bricks are thought to be dumped ballast from 17th and 18th century sailing ships.





</doc>
<doc id="21504" url="https://en.wikipedia.org/wiki?curid=21504" title="Nicole Kidman">
Nicole Kidman

Nicole Mary Kidman, (born 20 June 1967) is an Australian-American actress and producer. She is the recipient of multiple awards, including an Academy Award, two Primetime Emmy Awards, five Golden Globe Awards, a SAG Award, and the Silver Bear for Best Actress.

Kidman began her acting career in Australia with the 1983 film "Bush Christmas". Her breakthrough came in 1989 with the thriller "Dead Calm" and the television miniseries "Bangkok Hilton". In 1990, she made her Hollywood debut in the racing film "Days of Thunder", opposite Tom Cruise. She went on to achieve wide recognition with leading roles in the romantic drama "Far and Away" (1992), the superhero film "Batman Forever" (1995), the crime comedy-drama "To Die For" (1995), for which she won a Golden Globe Award, and the erotic thriller "Eyes Wide Shut" (1999). She received two consecutive Academy Award for Best Actress nominations for playing a courtesan in the musical "Moulin Rouge!" (2001) and the writer Virginia Woolf in the drama film "The Hours" (2002); she won the award for the latter, and received Golden Globes for both films.

Kidman continued to star in commercially successful films, including the horror-thriller "The Others" (2001) and the epic war drama film "Cold Mountain" (2003), as she achieved critical acclaim for roles in independent films, including the dramas "Dogville" (2003), "Birth" (2004), and "Rabbit Hole" (2010), for which she received a third Best Actress Oscar nomination, and the thriller "The Paperboy" (2012). Following a period of critical and commercial failures, Kidman earned a fourth Oscar nomination for playing a troubled mother in the biopic "Lion" (2016). She returned to television in 2017 with an acclaimed performance as a victim of domestic abuse in the HBO series "Big Little Lies", for which she won a Golden Globe Award, a SAG Award, and two Primetime Emmy Awards for Outstanding Lead Actress in a Limited Series and Outstanding Limited Series as a producer.

Kidman has been a Goodwill ambassador for UNICEF since 1994 and for UNIFEM since 2006. In 2006, Kidman was appointed Companion of the Order of Australia and was the highest-paid actress in the motion picture industry for that year. As a result of being born to Australian parents in Hawaii, Kidman has dual citizenship in Australia and the United States. Kidman founded and owns the production company Blossom Films. Following her divorce from actor Tom Cruise, Kidman has been married to singer Keith Urban since 2006.

Kidman was born 20 June 1967 in Honolulu, Hawaii, while her Australian parents were temporarily in the United States on student visas. Her father was Antony Kidman (1938–2014), a biochemist, clinical psychologist and author, who died of a heart attack in Singapore aged 75. Her mother, Janelle Ann (née Glenny), is a nursing instructor who edited her husband's books and was a member of the Women's Electoral Lobby. Kidman's ancestry includes Irish, Scottish and English heritage.

Being born in Hawaii, she was given the Hawaiian name "Hōkūlani". The inspiration for the name came from a baby elephant born around the same time at the Honolulu Zoo, but the name is also a commonly used Hawaiian name for girls, Hokulani meaning 'Heavenly Star'. 

At the time of Kidman's birth, her father was a graduate student at the University of Hawaiʻi at Mānoa. He became a visiting fellow at the National Institute of Mental Health of the United States. Opposed to the war in Vietnam, Kidman's parents participated in anti-war protests while living in Washington, D.C. The family returned to Australia when Kidman was four and her mother now lives on Sydney's North Shore. Kidman has a younger sister, Antonia Kidman, a journalist and TV presenter.

Kidman attended Lane Cove Public School and North Sydney Girls' High School. She was enrolled in ballet at three and showed her natural talent for acting in her primary and high school years. She says that she was first inspired to become an actress upon seeing Margaret Hamilton's performance as the Wicked Witch of the West in "The Wizard of Oz". Kidman has revealed that she was timid as a child, saying, "I am very shy – really shy – I even had a stutter as a kid, which I slowly got over, but I still regress into that shyness. So I don't like walking into a crowded restaurant by myself; I don't like going to a party by myself."

She initially studied at the Phillip Street Theatre in Sydney. At Philip Street, Kidman studied alongside Naomi Watts who had attended the same high school. She also attended the Australian Theatre for Young People. Here she took up drama, mime and performing in her teens, finding acting to be a refuge. Owing to her fair skin and naturally red hair, the Australian sun forced the young Kidman to rehearse in halls of the theatre. A regular at the Phillip Street Theatre, she received both encouragement and praise to pursue acting full-time.

In 1983, aged 16, Kidman made her film debut in a remake of the Australian holiday season favourite "Bush Christmas". By the end of 1983, she had a supporting role in the television series "Five Mile Creek". In 1984, her mother was diagnosed with breast cancer, which caused Kidman to halt her acting work temporarily while she studied massage so she could help her mother with physical therapy. She began gaining popularity in the mid-1980s after appearing in several film roles, including "BMX Bandits" (1983), "Watch the Shadows Dance" (1987 aka "Nightmaster"), and the romantic comedy "Windrider" (1986), which earned Kidman attention due to her racy scenes. Also during the decade, she appeared in several Australian productions, including the soap opera "A Country Practice" and the 1987 miniseries "Vietnam". She also made guest appearances on Australian television programs and TV movies.

In 1988, Kidman appeared in "Emerald City", based on the play of the same name. The Australian film earned her an Australian Film Institute award for Best Supporting Actress. Kidman next starred with Sam Neill in "Dead Calm" (1989) as Rae Ingram, playing the wife of a naval officer. The thriller brought Kidman to international recognition; "Variety" commented: "Throughout the film, Kidman is excellent. She gives the character of Rae real tenacity and energy." Meanwhile, critic Roger Ebert noted the excellent chemistry between the leads, stating, "Kidman and Zane do generate real, palpable hatred in their scenes together." She followed that up with the Australian miniseries "Bangkok Hilton". She next moved on to star alongside her then-boyfriend and future husband, Tom Cruise, in the 1990 auto racing film "Days of Thunder", as a young doctor who falls in love with a NASCAR driver. It is Kidman's American debut and was among the highest-grossing films of the year.

In 1991, she co-starred with Thandie Newton and former classmate Naomi Watts in the Australian independent film "Flirting". They portrayed high school girls in this coming of age story, which won the Australian Film Institute Award for Best Film. That same year, her work in the film "Billy Bathgate" earned Kidman her first Golden Globe Award nomination, for Best Supporting Actress. "The New York Times", in its film review, called her "a beauty with, it seems, a sense of humor". The following year, she and Cruise re-teamed for Ron Howard's Irish epic "Far and Away" (1992), which was a modest critical and commercial success. In 1993, she starred in the thriller "Malice" opposite Alec Baldwin and the drama "My Life" opposite Michael Keaton.

In 1995, Kidman appeared her highest-grossing live-action film (as of 2018), playing Dr. Chase Meridian, the damsel in distress, in the superhero film "Batman Forever", opposite Val Kilmer as the film's title character. The same year Kidman starred in Gus Van Sant's critically acclaimed dark comedy "To Die For", in which she played the murderous newcaster Suzanne Stone. Of Kidman's Golden Globe award-winning performance, Mick LaSalle of the "San Francisco Chronicle" said "[she] brings to the role layers of meaning, intention and impulse. Telling her story in close-up – as she does throughout the film – Kidman lets you see the calculation, the wheels turning, the transparent efforts to charm that succeed in charming all the same." Kidman next appeared in "The Portrait of a Lady" (1996), based on the novel of the same name, alongside Barbara Hershey, John Malkovich, and Mary-Louise Parker. The following year, she starred in the action-thriller "The Peacemaker" (1997) as White House nuclear expert Dr. Julia Kelly, opposite George Clooney. The film grossed $110,000,000 worldwide. In 1998, she co-starred with Sandra Bullock in the poorly received fantasy "Practical Magic" as a modern-day witch. Kidman returned to her work on stage the same year in the David Hare play "The Blue Room", which opened in London.
In 1999, Kidman reunited with then husband, Tom Cruise, to portray a married couple in "Eyes Wide Shut", the final film of director Stanley Kubrick. The film was subject to censorship controversies due to the explicit nature of its sex scenes. The film received further attention following Kubrick's death shortly before its release. After brief hiatus and a highly publicized divorce from Cruise, Kidman returned to the screen to play a mail-order bride in the British-American drama "Birthday Girl". In 2001, Kidman played the cabaret actress and courtesan Satine in Baz Luhrmann's musical "Moulin Rouge!", opposite Ewan McGregor. Her performance and her singing received positive reviews; Paul Clinton of "CNN.com" called it her best work since "To Die For", and wrote "[she] is smoldering and stunning as Satine. She moves with total confidence throughout the film [...] Kidman seems to specialize in "ice queen" characters, but with Satine, she allows herself to thaw, just a bit." Subsequently, Kidman received her second Golden Globe Award, for Best Actress in a Motion Picture Musical or Comedy, as well as many other acting awards and nominations. She also received her first Academy Award nomination, for Best Actress.

Also in 2001, she had a starring role in Alejandro Amenábar's horror film "The Others" as Grace Stewart, a mother living in the Channel Islands during World War II who suspects her house is haunted. Grossing over $210,947,037 worldwide, the film also earned several Goya Awards award nominations, including a Best Actress nomination for Kidman. She received her second BAFTA and fifth Golden Globe nominations. Roger Ebert commented that "Alejandro Amenábar has the patience to create a languorous, dreamy atmosphere, and Nicole Kidman succeeds in convincing us that she is a normal person in a disturbing situation, and not just a standard-issue horror movie hysteric." Kidman was named the World's Most Beautiful Person by People magazine

In 2002, Kidman won critical praise for her portrayal of Virginia Woolf in Stephen Daldry's "The Hours", which stars Meryl Streep and Julianne Moore. Kidman famously wore prosthetics that were applied to her nose making her almost unrecognisable playing the author during her time in 1920s England, and her bouts with depression and mental illness while trying to write her novel, "Mrs. Dalloway". The film earned positive notices and several nominations, including for an Academy Award for Best Picture. "The New York Times" wrote that, "Ms. Kidman, in a performance of astounding bravery, evokes the savage inner war waged by a brilliant mind against a system of faulty wiring that transmits a searing, crazy static into her brain". Kidman won numerous critics' awards, including her first BAFTA, third Golden Globe, and the Academy Award for Best Actress. As the first Australian actress to win an Academy Award, Kidman made a teary acceptance speech about the importance of art, even during times of war, saying, "Why do you come to the Academy Awards when the world is in such turmoil? Because art is important. And because you believe in what you do and you want to honour that, and it is a tradition that needs to be upheld."

Following her Oscar win, Kidman appeared in three very different films in 2003. The first, a leading role in "Dogville", by Danish director Lars von Trier, was an experimental film set on a bare soundstage. Though the film divided critics in the United States, Kidman still earned praise for her performance. Peter Travers of "Rolling Stones" magazine stated: "Kidman gives the most emotionally bruising performance of her career in Dogville, a movie that never met a cliche it didn't stomp on." The second was an adaptation of Philip Roth's novel "The Human Stain", opposite Anthony Hopkins. Her third film was Anthony Minghella's war drama "Cold Mountain". Kidman appeared opposite Jude Law and Renée Zellweger, playing Southerner Ada Monroe, who is in love with Law's character and separated by the Civil War. "TIME" magazine wrote, "Kidman takes strength from Ada's plight and grows steadily, literally luminous. Her sculptural pallor gives way to warm radiance in the firelight". The film garnered several award nominations and wins for its actors; Kidman received her sixth Golden Globe nomination at the 61st Golden Globe Awards for Best Actress.

In 2004 she appeared in the film "Birth", which received controversy over a scene in which Kidman shares a bath with her co-star, 10-year-old Cameron Bright. At a press conference at the Venice Film Festival, Kidman addressed the controversy saying, "It wasn't that I wanted to make a film where I kiss a 10-year-old boy. I wanted to make a film where you understand love". Kidman earned her seventh Golden Globe nomination, for Best Actress – Motion Picture Drama. That same year she appeared in the black comedy-science-fiction film "The Stepford Wives", a remake of the 1975 film of the same name. Kidman appeared in the lead role as Joanna Eberhart, a successful producer. The film, directed by Frank Oz, was critically panned and a commercial failure. The following year, Kidman appeared opposite Sean Penn in the Sydney Pollack thriller "The Interpreter", playing UN translator Silvia Broome. Also that year, she starred in "Bewitched", based on the 1960s TV sitcom of the same name, opposite Will Ferrell. Both Kidman and Ferrell earned that year's Razzie Award for "Worst Screen Couple". Neither film fared well in the United States, with box office sales falling well short of the production costs, but both films performed well internationally.

In conjunction with her success in the film industry, Kidman became the face of the "Chanel No. 5" perfume brand. She starred in a campaign of television and print ads with Rodrigo Santoro, directed by "Moulin Rouge!" director Baz Luhrmann, to promote the fragrance during the holiday seasons of 2004, 2005, 2006, and 2008. The three-minute commercial produced for "Chanel No. 5" made Kidman the record holder for the most money paid per minute to an actor after she reportedly earned US$12million for the three-minute advert. During this time, Kidman was also listed as the 45th Most Powerful Celebrity on the 2005 "Forbes" Celebrity 100 List. She made a reported US$14.5 million in 2004–2005. On "People" magazine's list of 2005's highest-paid actresses, Kidman was second behind Julia Roberts, with US$16–17 million per-film price tag. Nintendo in 2007 announced that Kidman would be the new face of Nintendo's advertising campaign for the Nintendo DS game More Brain Training in its European market.
Kidman portrayed photographer Diane Arbus in the biography "Fur" (2006), opposite Robert Downey, Jr.. Both Kidman and Downey Jr. received praise for their performances. She also lent her voice to the animated film "Happy Feet" (2006), which grossed over US$384 million worldwide. In 2007, she starred in the science-fiction movie "The Invasion" directed by Oliver Hirschbiegel, a remake of the 1956 "Invasion of the Body Snatchers" that proved a critical and commercial failure. She also played opposite Jennifer Jason Leigh and Jack Black in Noah Baumbach's comedy-drama "Margot at the Wedding", which earned Kidman a Satellite Award nomination for Best Actress – Musical or Comedy. She then starred in the fantasy-adventure, "The Golden Compass" (2007), playing the villainous Marisa Coulter.

In 2008, she reunited with "Moulin Rouge!" director Baz Luhrmann in the Australian period film "Australia", set in the remote Northern Territory during the Japanese attack on Darwin during World War II. Kidman played opposite Hugh Jackman as an Englishwoman feeling overwhelmed by the continent. The acting was praised and the movie was a box office success worldwide. Kidman was originally set to star in the post-World War II German drama, "The Reader", working with previous collaborators Sydney Pollack and Anthony Minghella, but due to her pregnancy prior to filming she had to back out. The role went to Kate Winslet, who ultimately won the Oscar for Best Actress, which Kidman presented to her during the 81st Academy Awards. Kidman appeared in the 2009 Rob Marshall musical "Nine", portraying the Federico Fellini-like character's muse, Claudia Jenssen. She was featured alongside fellow Oscar winners Daniel Day-Lewis, Judi Dench, Marion Cotillard, Penélope Cruz and Sophia Loren. Kidman, whose screen time was brief compared to the other actresses, performed the musical number "Unusual Way" alongside Day-Lewis. The film received several Golden Globe and Academy Award nominations, and earned Kidman a fourth Screen Actors Guild Award nomination, as part of the Outstanding Cast.

In 2010, Kidman starred with Aaron Eckhart in the film adaptation of the Pulitzer Prize-winning play "Rabbit Hole", for which she vacated her role in the Woody Allen picture "You Will Meet a Tall Dark Stranger". Her work on "Rabbit Hole" earned her critical acclaim, and received nominations for the Academy Awards, Golden Globe Awards, and Screen Actors Guild Awards, Kidman also produced this film. She lent her voice to a promotional video that Australia used to support its bid to host the 2018 FIFA World Cup. "TV Guide" reported in 2008 that Kidman will star in "The Danish Girl", a film adaptation of the novel of the same name, playing Lili Elbe, the world's first postoperative transsexual. "Screen Daily" reported that shooting would begin in Germany in July 2011. However the project has been delayed following the exit of the director, Lasse Hallström and Kidman's co-star Rachel Weisz. In 2009, "Variety" said that she would produce and star in a film adaptation of the Chris Cleave novel "Little Bee", in association with BBC Films.

In June 2010, "TV Guide" announced that Kidman and Clive Owen will star in an HBO film about Ernest Hemingway and his relationship with Martha Gellhorn. entitled "Hemingway & Gellhorn". The film, directed by Philip Kaufman, began shooting in March 2011, with an air date scheduled for 2012. She also starred alongside Nicolas Cage in director Joel Schumacher's action-thriller "Trespass", with the stars playing a married couple taken hostage.
On 17 September 2010, ContactMusic. com said Kidman would return to Broadway to portray Alexandra Del Lago in David Cromer's revival of Tennessee Williams' "Sweet Bird of Youth", with Scott Rudin producing. On 30 August 2011, Cromer spoke to "The New York Times" and explained that the production would not meet its original fall 2011 revival date but that it remains an active project.

In June 2011, Kidman was cast in Lee Daniels' adaptation of the Pete Dexter novel, "The Paperboy"; she began filming on the thriller on 1 August 2011, and "The Paperboy" was released in 2012. In the film, she portrayed death row groupie Charlotte Bless, and performed sex scenes that she claims not to have remembered until seeing the finished film. "I was like okay, so that's what I did", she said. The film competed in the 2012 Cannes Film Festival, and Kidman's performance drew critical acclaim and among nominations for the SAG and the Saturn Award for Best Supporting Actress, gave Kidman her second Golden Globe nomination for Best Supporting Actress and her tenth nomination overall. In 2012, Kidman's audiobook recording of Virginia Woolf's "To the Lighthouse" was released at Audible.com. Kidman also co-starred in Park Chan-wook's "Stoker" (2013) to positive critical response and a Saturn Award nomination for Best Supporting Actress. In April 2013 she was selected as a member of the main competition jury at the 2013 Cannes Film Festival.
In 2014, Kidman starred in the biopic, "Grace of Monaco" in the title role that chronicles the 1962 crisis, in which Charles de Gaulle blockaded the tiny principality, angered by Monaco's status as a tax haven for wealthy French subjects and Kelly's contemplating a Hollywood return to star in Alfred Hitchcock's "Marnie". Opening out of competition at the 2014 Cannes Film Festival, the film received largely negative reviews. Kidman also starred in two films with Colin Firth, the first, the British-Australian historical drama, "The Railway Man" in which Kidman played officer's wife Patti Lomax received positive critical reviews. Katherine Monk of the Montreal Gazette said of Kidman's performance, "It's a truly masterful piece of acting that transcends Teplitzky's store-bought framing, but it's Kidman who delivers the biggest surprise: For the first time since her eyebrows turned into solid marble arches, the Australian Oscar winner is truly terrific". The second, the British thriller film "Before I Go To Sleep" drew positive critical response of Kidman's performance, as Christine Lucas, a car crash survivor with brain damage. Kidman also appeared in the family film "Paddington" (2014) as a villain.

On 23 January, she starred in the Australian-Irish drama-thriller "Strangerland", which opened at the 2015 Sundance Film Festival to a "rapturous" audience response to Kidman's performance. Kidman also co-starred in the Jason Bateman-directed "The Family Fang", produced by Kidman's production company, Blossom Films, which premiered at the 2015 Toronto International Film Festival. Other projects include the biographical drama "Queen of the Desert", with Kidman portraying the lead role of traveller, writer, archaeologist, explorer, cartographer and political officer Gertrude Bell and "Genius" alongside Colin Firth and Guy Pearce. Kidman played a lead role in the 2015 thriller "Secret in Their Eyes", directed by Billy Ray and co-starring Julia Roberts and Chiwetel Ejiofor. After more than 15 years, Kidman returned to the West End in the UK premiere of "Photograph 51" at the Noël Coward Theatre. She starred as British scientist Rosalind Franklin in the production from 5 September to 21 November 2015, directed by Michael Grandage. The play focuses on Franklin's role in the discovery of the structure of DNA. Kidman and the play earned "glowing reviews". Her return to the West End has been hailed a success, especially after having won an acting award for her role in "Photograph 51".

In 2016, Kidman's performance in "Lion" earned rave reviews, as well as nominations for the Academy Award for Best Supporting Actress, her fourth nomination overall, the Critics Choice for Best Supporting Actress, the Screen Actors Guild for Outstanding Performance by a Female Actor in a Supporting Role, the BAFTA Award for Best Actress in a Supporting Role, a win in the same category at the Hollywood Film Awards as well as her third Golden Globe nomination for Best Supporting Actress, and her eleventh nomination overall. Kidman portrayed Sue Brierly, the adoptive mother of Saroo, an Indian boy who was separated from his birth family, a role she felt connected to as she herself is the mother of adopted children. Richard Roeper of "The Chicago Sun-Times" felt that, "Kidman gives a powerful and moving performance as Saroo's adoptive mother, who loves her son with every molecule of her being, but comes to understand his quest. It's as good as anything she's done in the last decade."

In 2017, Kidman returned to television in the seven-part miniseries adaptation of the Liane Moriarty bestseller "Big Little Lies", which premiered on HBO. She produced the miniseries along with her co-star, Reese Witherspoon, and the show's director, Jean-Marc Vallée. She plays Celeste Wright, a former lawyer and housewife, who is concealing her abusive relationship with her younger husband, played by Alexander Skarsgård. Kidman has garnered critical acclaim for her performance, with Matthew Jacobs of "The Huffington Post" stating that she "delivered a career-defining performance". Ann Hornaday of "The Washington Post" wrote that "Kidman belongs in the pantheon of great actresses". She has received a nomination from the Television Critics Association and won the Primetime Emmy Award for Outstanding Lead Actress in a Limited Series or Movie for her performance, as well as winning the Primetime Emmy Award for Outstanding Limited Series as a producer. She went on to win a Golden Globe and SAG Award for her role.

She then played Martha Farnsworth, the headmistress of an all-girls school during the Civil War, in Sofia Coppola's drama "The Beguiled", an adaptation of the novel written by Thomas P. Cullinan, which premiered at the 2017 Cannes Film Festival, competing for the Palme d'Or. The film received positive reviews, as did Kidman's performance, with Katie Walsh of "Tribune News Service" noting, "Nicole Kidman is particularly, unsurprisingly excellent in her performance as the steely Miss Martha. She is controlled and in control, unflappable. Her genteel manners and femininity co-exist easily with her toughness." Kidman had two other films premiere at the festival, the science-fiction romantic comedy "How to Talk to Girls at Parties", reuniting her with director John Cameron Mitchell, and the psychological thriller "The Killing of a Sacred Deer", directed by Yorgos Lanthimos, which also competed for the Palme d'Or. Also in 2017, Kidman played a supporting role in the television series "".

Kidman is set to portray Queen Atlanna, the mother of the title character, in the Warner Bros. film "Aquaman". She will also star alongside Bryan Cranston and Kevin Hart in "The Upside", a remake of the 2011 French comedy "The Intouchables". She is also set to star in the upcoming animated film, "The Guardian Brothers", alongside Meryl Streep, Mel Brooks, Edward Norton, and Dan Fogler.

Kidman has been married twice: previously to actor Tom Cruise, and currently to country singer Keith Urban. She has an adopted son and daughter with Cruise as well as two biological daughters with Urban. Kidman met Cruise in November 1989, while filming "Days of Thunder", they were married on Christmas Eve 1990 in Telluride, Colorado. The couple adopted a daughter, Isabella Jane (born 1992), and a son, Connor Anthony (born 1995). On 5 February 2001, the couple's spokesperson announced their separation. Cruise filed for divorce two days later, and the marriage was dissolved in August of that year, with Cruise citing irreconcilable differences. In a 2007 interview with "Marie Claire", Kidman noted the incorrect reporting of the ectopic pregnancy early in her marriage. "It was wrongly reported as miscarriage, by everyone who picked up the story." "So it's huge news, and it didn't happen."
In the June 2006 issue of "Ladies' Home Journal", she said she still loved Cruise: "He was huge; still is. To me, he was just Tom, but to everybody else, he is huge. But he was lovely to me and I loved him. I still love him." In addition, she has expressed shock about their divorce. In 2015, former Church of Scientology executive Mark Rathbun claimed in a documentary film that he was instructed to "facilitate [Cruise's] break-up with Nicole Kidman". Cruise's auditor further claimed Kidman had been wiretapped on Cruise's suggestion.

Prior to marrying Cruise, Kidman had been involved in relationships with Australian actor Marcus Graham and "Windrider" (1986) co-star Tom Burlinson. She was also said to be involved with Adrien Brody. The film "Cold Mountain" brought rumours that an affair between Kidman and co-star Jude Law was responsible for the break-up of his marriage. Both denied the allegations, and Kidman won an undisclosed sum from the British tabloids that published the story. She met musician Lenny Kravitz in 2003, and dated him into 2004. Robbie Williams confirmed he had a short romance with Kidman on her yacht in summer 2004.

In a 2007 "Vanity Fair" interview, Kidman revealed that she had been secretly engaged to someone prior to her present relationship to New Zealand-Australian country singer Keith Urban, whom she met at , an event honouring Australians, in January 2005. Kidman married Urban on 25 June 2006, at Cardinal Cerretti Memorial Chapel in the grounds of St Patrick's Estate, Manly in Sydney. In an interview in 2015, Kidman said, "We didn't really know each other – we got to know each other during our marriage." They maintain homes in Sydney, Sutton Forest (New South Wales, Australia), Los Angeles, and Nashville (Tennessee, USA). The couple's first daughter was born in 2008, in Nashville. In 2010, Kidman and Urban had their second daughter via surrogacy at Nashville's Centennial Women's Hospital. In an interview by Tina Brown at the 2015 Women in the World conference, she stated that her attention turned to her career after her divorce from Cruise: "Out of my divorce came work that was applauded so that was an interesting thing for me", leading to her Academy Award in 2003.

Kidman is Catholic. She attended Mary Mackillop Chapel in North Sydney. Following criticism of "The Golden Compass" by Catholic leaders as anti-Catholic, Kidman told "Entertainment Weekly" that the Catholic Church is part of her "essence", and that her religious beliefs would prevent her from taking a role in a film she perceived as anti-Catholic. During her divorce from Tom Cruise, she stated that she did not want their children raised as Scientologists. She has been reluctant to discuss Scientology since her divorce. In 2014, Kidman said she had been practicing Transcendental Meditation since her early twenties. Kidman has donated to U.S. Democratic party candidates.

In 2002, Kidman first appeared on the Australian rich list published annually in the "Business Review Weekly" with an estimated net worth of A$122 million. In the 2011 published list, Kidman's wealth was estimated at A$304 million, down from A$329 million in 2010. Kidman has raised money for, and drawn attention to, disadvantaged children around the world. In 1994, she was appointed a goodwill ambassador for UNICEF, and in 2004, she was honoured as a "Citizen of the World" by the United Nations. Kidman joined the Little Tee Campaign for breast cancer care to design T-shirts or vests to raise money to fight the disease; motivated by her mother's own battle with breast cancer in 1984.

In the 2006 Australia Day Honours, Kidman was appointed Companion of Order of Australia (AC) for "service to the performing arts as an acclaimed motion picture performer, to health care through contributions to improve medical treatment for women and children and advocacy for cancer research, to youth as a principal supporter of young performing artists, and to humanitarian causes in Australia and internationally". However, due to film commitments and her wedding to Urban, it wasn't until 13 April 2007 that she was presented with the honour. It was presented by the Governor-General of Australia, Major General Michael Jeffery, in a ceremony at Government House, Canberra. Kidman was appointed goodwill ambassador of the United Nations Development Fund for Women (UNIFEM) in 2006. In this capacity, Kidman has addressed international audiences at UN events, raised awareness through the media and testified before the United States House of Representatives Committee on Foreign Affairs to support the International Violence against Women Act. Kidman visited Kosovo in 2006 to learn about women's experiences of conflict and UNIFEM's support efforts. She is the international spokesperson for UNIFEM's Say NO – UNiTE to End Violence against Women initiative. Kidman and the UNIFEM executive director presented over five million signatures collected during the first phase of this to the UN Secretary-General on 25 November 2008.

In the beginning of 2009, Kidman appeared in a series of postage stamps featuring Australian actors. She, Geoffrey Rush, Russell Crowe, and Cate Blanchett each appear twice in the series: once as themselves, and once as their Academy Award-nominated character; Kidman's second stamp showed her as Satine from Moulin Rouge!. On 8 January 2010, alongside Nancy Pelosi, Joan Chen and Joe Torre, Kidman attended the ceremony to help Family Violence Prevention Fund break ground on a new international center located in the Presidio of San Francisco.

In 2015, Kidman became the brand ambassador for Etihad Airways.

Kidman supports the Nashville Predators, being seen and photographed almost nightly throughout the season. Additionally, she supports Sydney Swans in the Australian Football League, and once served as a club ambassador.

As of 2018, Kidman's films have grossed more than US$3.9 billion at the worldwide box-office, according to Box Office Mojo.

Kidman's discography consists of one spoken word album, one extended play, three singles, three music videos, ten other appearances, a number of unreleased tracks, and two tribute songs recorded by various artists.
Kidman, primarily known in the field of acting, entered the music industry in the 2000s after recording a number of tracks for the soundtrack album to Baz Luhrmann's 2001 motion picture "Moulin Rouge!", which she starred in. Her duet with Ewan McGregor entitled "Come What May" was released as her debut and the second single of the OST through Interscope on 24 September 2001. The composition became the eighth-highest selling single by an Australian artist for that year, being certified Gold by Australian Recording Industry Association, while reaching on the UK Singles Chart at number twenty-seven. In addition, the song received a nomination at the 59th Golden Globe Awards as the Best Original Song, and has been listed as the eighty-fifth within AFI's 100 Years...100 Songs by American Film Institute.
"Somethin' Stupid", a cover version of Frank and Nancy Sinatra followed soon. The track, recorded as a duet with English singer-songwriter Robbie Williams, was issued on 14 December, 2001, by Chrysalis Records as the lead single of his fourth studio album "Swing When You're Winning". Kidman's second single topped the official music charts in Italy, New Zealand, Portugal, and England, as well as scored top ten placings all over Europe, including Australia, Austria, Belgium, Denmark, Germany, Netherlands, Norway, and Switzerland. Apart from being certified either Gold or Silver in a number of countries, it was classified as the eleventh best-selling single of 2002 in Italy, thirtieth in the UK, the fifty-ninth in Australia, and the ninety-third in France, respectively. The song peaked at No. 8 in the Australian ARIAnet Singles Chart and at No. 1, for three weeks, in the UK.

On 5 April 2002, Kidman released, through Interscope, her third single, a cover of Randy Crawford's "One Day I'll Fly Away". The song, a Tony Philips remix, was promoted as the pilot single of a follow-up to the original soundtrack of the same name, "Moulin Rouge! Vol. 2". In 2006, she contributed with her vocal for the OST "Happy Feet" on a rendition of the Prince song "Kiss". In 2009, she was featured on the soundtrack of Rob Marshall's 2009 movie musical "Nine", singing the song 
"Unusual Way".

Her name was later been credited on a track called "What's the Procedure", issued on 14 March 2013, on the compilation album "I Know Why They Call It Pop: Volume 2" by Rok Lok Records. Among others, Kidman also narrated an audiobook in 2012.

In 2017, she and Nicolle Gaylon sung backing vocals on her husband, country music singer Keith Urban’s song "Female".

In 2003, Kidman received a Star on the Hollywood Walk of Fame. In addition to her 2003 Academy Award for Best Actress, Kidman has received Best Actress awards from the following critics' groups or award-granting organisations: the Hollywood Foreign Press (Golden Globes), the Australian Film Institute, Blockbuster Entertainment Awards, Empire Awards, Golden Satellite Awards, Hollywood Film Festival, London Critics Circle, Russian Guild of Film Critics, and the Southeastern Film Critics Association. 

Kidman also received recognition from the National Association of Theatre Owners at the ShoWest Convention in 1992 as the Female Star of Tomorrow, and in 2002 for a Distinguished Decade of Achievement in Film. In 2003, she was given the American Cinematheque Award.




</doc>
<doc id="21505" url="https://en.wikipedia.org/wiki?curid=21505" title="Nucleotide">
Nucleotide

Nucleotides are organic molecules that serve as the monomer units for forming the nucleic acid polymers deoxyribonucleic acid (DNA) and ribonucleic acid (RNA), both of which are essential biomolecules within all life-forms on Earth. Nucleotides are the building blocks of nucleic acids; they are composed of three subunit molecules: a nitrogenous base, a five-carbon sugar (ribose or deoxyribose), and at least one phosphate group.

A nucleoside is a nitrogenous base and a 5-carbon sugar. Thus a nucleoside plus a phosphate group yields a nucleotide.

Nucleotides also play a central role in metabolism at a fundamental, cellular level. They carry packets of chemical energy——in the form of the nucleoside triphosphates Adenosine triphosphate(ATP), Guanosine triphosphate(GTP), Cytidine triphosphate(CTP) and Uridine triphosphate(UTP)——throughout the cell to the many cellular functions that demand energy, which include: synthesizing amino acids, proteins and cell membranes and parts, moving the cell and moving cell parts (both internally and intercellularly), dividing the cell, etc. In addition, nucleotides participate in cell signaling (cyclic guanosine monophosphate or cGMP and cyclic adenosine monophosphate or cAMP), and are incorporated into important cofactors of enzymatic reactions (e.g. coenzyme A, FAD, FMN, NAD, and NADP).

In experimental biochemistry, nucleotides can be radiolabeled with radionuclides to yield radionucleotides.

A nucleotide is composed of three distinctive chemical sub-units: a five-carbon sugar molecule, a nitrogenous base—which two together are called a nucleoside—and one phosphate group. With all three joined, a nucleotide is also termed a "nucleoside "mono"phosphate". The chemistry sources ACS Style Guide and IUPAC Gold Book prescribe that a nucleotide should contain only one phosphate group, but common usage in molecular biology textbooks often extends the definition to include molecules with two, or with three, phosphates. Thus, the terms "nucleoside "di"phosphate" or "nucleoside "tri"phosphate" may also indicate nucleotides.

Nucleotides contain either a purine or a pyrimidine base—i.e., the nitrogenous base molecule, also known as a nucleobase—and are termed "ribo"nucleotides if the sugar is ribose, or "deoxyribo"nucleotides if the sugar is deoxyribose. Individual phosphate molecules repetitively connect the sugar-ring molecules in two adjacent nucleotide monomers, thereby connecting the nucleotide monomers of a nucleic acid end-to-end into a long chain. These chain-joins of sugar and phosphate molecules create a 'backbone' strand for a single- or double helix. In any one strand, the chemical orientation (directionality) of the chain-joins runs from the 5'-end to the 3'-end ("read": 5 prime-end to 3 prime-end)—referring to the five carbon sites on sugar molecules in adjacent nucleotides. In a double helix, the two strands are oriented in opposite directions, which permits base pairing and complementarity between the base-pairs, all which is essential for replicating or transcribing the encoded information found in DNA.

Unlike in nucleic acid nucleotides, singular cyclic nucleotides are formed when the phosphate group is bound twice to the same sugar molecule, i.e., at the corners of the sugar hydroxyl groups. These individual nucleotides function in cell metabolism rather than the nucleic acid structures of long-chain molecules.

Nucleic acids then are polymeric macromolecules assembled from nucleotides, the monomer-units of nucleic acids. The purine bases adenine and guanine and pyrimidine base cytosine occur in both DNA and RNA, while the pyrimidine bases thymine (in DNA) and uracil (in RNA) in just one. Adenine forms a base pair with thymine with two hydrogen bonds, while guanine pairs with cytosine with three hydrogen bonds.

Nucleotides can be synthesized by a variety of means both in vitro and in vivo.

In vitro, protecting groups may be used during laboratory production of nucleotides. A purified nucleoside is protected to create a phosphoramidite, which can then be used to obtain analogues not found in nature and/or to synthesize an oligonucleotide.

In vivo, nucleotides can be synthesized de novo or recycled through salvage pathways. The components used in de novo nucleotide synthesis are derived from biosynthetic precursors of carbohydrate and amino acid metabolism, and from ammonia and carbon dioxide. The liver is the major organ of de novo synthesis of all four nucleotides. De novo synthesis of pyrimidines and purines follows two different pathways. Pyrimidines are synthesized first from aspartate and carbamoyl-phosphate in the cytoplasm to the common precursor ring structure orotic acid, onto which a phosphorylated ribosyl unit is covalently linked. Purines, however, are first synthesized from the sugar template onto which the ring synthesis occurs. For reference, the syntheses of the purine and pyrimidine nucleotides are carried out by several enzymes in the cytoplasm of the cell, not within a specific organelle. Nucleotides undergo breakdown such that useful parts can be reused in synthesis reactions to create new nucleotides.

The synthesis of the pyrimidines CTP and UTP occurs in the cytoplasm and starts with the formation of carbamoyl phosphate from glutamine and CO. Next, aspartate carbamoyltransferase catalyzes a condensation reaction between aspartate and carbamoyl phosphate to form carbamoyl aspartic acid, which is cyclized into 4,5-dihydroorotic acid by dihydroorotase. The latter is converted to orotate by dihydroorotate oxidase. The net reaction is:

Orotate is covalently linked with a phosphorylated ribosyl unit. The covalent linkage between the ribose and pyrimidine occurs at position C of the ribose unit, which contains a pyrophosphate, and N of the pyrimidine ring. Orotate phosphoribosyltransferase (PRPP transferase) catalyzes the net reaction yielding orotidine monophosphate (OMP):

Orotidine 5'-monophosphate is decarboxylated by orotidine-5'-phosphate decarboxylase to form uridine monophosphate (UMP). PRPP transferase catalyzes both the ribosylation and decarboxylation reactions, forming UMP from orotic acid in the presence of PRPP. It is from UMP that other pyrimidine nucleotides are derived. UMP is phosphorylated by two kinases to uridine triphosphate (UTP) via two sequential reactions with ATP. First the diphosphate form UDP is produced, which in turn is phosphorylated to UTP. Both steps are fueled by ATP hydrolysis:

CTP is subsequently formed by amination of UTP by the catalytic activity of CTP synthetase. Glutamine is the NH donor and the reaction is fueled by ATP hydrolysis, too:

Cytidine monophosphate (CMP) is derived from cytidine triphosphate (CTP) with subsequent loss of two phosphates.

The atoms that are used to build the purine nucleotides come from a variety of sources: 

The de novo synthesis of purine nucleotides by which these precursors are incorporated into the purine ring proceeds by a 10-step pathway to the branch-point intermediate IMP, the nucleotide of the base hypoxanthine. AMP and GMP are subsequently synthesized from this intermediate via separate, two-step pathways. Thus, purine moieties are initially formed as part of the ribonucleotides rather than as free bases.

Six enzymes take part in IMP synthesis. Three of them are multifunctional:

The pathway starts with the formation of PRPP. PRPS1 is the enzyme that activates R5P, which is formed primarily by the pentose phosphate pathway, to PRPP by reacting it with ATP. The reaction is unusual in that a pyrophosphoryl group is directly transferred from ATP to C of R5P and that the product has the α configuration about C1. This reaction is also shared with the pathways for the synthesis of Trp, His, and the pyrimidine nucleotides. Being on a major metabolic crossroad and requiring much energy, this reaction is highly regulated.

In the first reaction unique to purine nucleotide biosynthesis, PPAT catalyzes the displacement of PRPP's pyrophosphate group (PP) by an amide nitrogen donated from either glutamine (N), glycine (N&C), aspartate (N), folic acid (C), or CO. This is the committed step in purine synthesis. The reaction occurs with the inversion of configuration about ribose C, thereby forming β-5-phosphorybosylamine (5-PRA) and establishing the anomeric form of the future nucleotide.

Next, a glycine is incorporated fueled by ATP hydrolysis and the carboxyl group forms an amine bond to the NH previously introduced. A one-carbon unit from folic acid coenzyme N-formyl-THF is then added to the amino group of the substituted glycine followed by the closure of the imidazole ring. Next, a second NH group is transferred from a glutamine to the first carbon of the glycine unit. A carboxylation of the second carbon of the glycin unit is concomittantly added. This new carbon is modified by the additional of a third NH unit, this time transferred from an aspartate residue. Finally, a second one-carbon unit from formyl-THF is added to the nitrogen group and the ring covalently closed to form the common purine precursor inosine monophosphate (IMP).

Inosine monophosphate is converted to adenosine monophosphate in two steps. First, GTP hydrolysis fuels the addition of aspartate to IMP by adenylosuccinate synthase, substituting the carbonyl oxygen for a nitrogen and forming the intermediate adenylosuccinate. Fumarate is then cleaved off forming adenosine monophosphate. This step is catalyzed by adenylosuccinate lyase.

Inosine monophosphate is converted to guanosine monophosphate by the oxidation of IMP forming xanthylate, followed by the insertion of an amino group at C. NAD is the electron acceptor in the oxidation reaction. The amide group transfer from glutamine is fueled by ATP hydrolysis.

In humans, pyrimidine rings (C, T, U) can be degraded completely to CO and NH (urea excretion). That having been said, purine rings (G, A) cannot. Instead they are degraded to the metabolically inert uric acid which is then excreted from the body. Uric acid is formed when GMP is split into the base guanine and ribose. Guanine is deaminated to xanthine which in turn is oxidized to uric acid. This last reaction is irreversible. Similarly, uric acid can be formed when AMP is deaminated to IMP from which the ribose unit is removed to form hypoxanthine. Hypoxanthine is oxidized to xanthine and finally to uric acid. Instead of uric acid secretion, guanine and IMP can be used for recycling purposes and nucleic acid synthesis in the presence of PRPP and aspartate (NH donor).

An unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature. In 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP). The two new artificial nucleotides or "Unnatural Base Pair" (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS–dNaM) complex or base pair in DNA. In 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed, and inserted it into cells of the common bacterium "E. coli" that successfully replicated the unnatural base pairs through multiple generations. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into "E. coli" bacteria. Then, the natural bacterial replication pathways use them to accurately replicate the plasmid containing d5SICS–dNaM.

The successful incorporation of a third base pair is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, from the existing 21 amino acids to a theoretically possible 172, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses.

Nucleotide (abbreviated "nt") is a common unit of length for single-stranded nucleic acids, similar to how base pair is a unit of length for double-stranded nucleic acids.

A study done by the Department of Sports Science at the University of Hull in Hull, UK has shown that nucleotides have significant impact on cortisol levels in saliva. Post exercise, the experimental nucleotide group had lower cortisol levels in their blood than the control or the placebo. Additionally, post supplement values of Immunoglobulin A were significantly higher than either the placebo or the control. The study concluded, "nucleotide supplementation blunts the response of the hormones associated with physiological stress."

Another study conducted in 2013 looked at the impact nucleotide supplementation had on the immune system in athletes. In the study, all athletes were male and were highly skilled in taekwondo. Out of the twenty athletes tested, half received a placebo and half received 480 mg per day of nucleotide supplement. After thirty days, the study concluded that nucleotide supplementation may counteract the impairment of the body's immune function after heavy exercise.

The IUPAC has designated the symbols for nucleotides. Apart from the five (A, G, C, T/U) bases, often degenerate bases are used especially for designing PCR primers. These nucleotide codes are listed here. Some primer sequences may also include the character "I", which codes for the non-standard nucleotide inosine. Inosine occurs in tRNAs, and will pair with adenine, cytosine, or thymine. This character does not appear in the following table however, because it does not represent a degeneracy. While inosine can serve a similar function as the degeneracy "D", it is an actual nucleotide, rather than a representation of a mix of nucleotides that covers each possible pairing needed.



</doc>
<doc id="21506" url="https://en.wikipedia.org/wiki?curid=21506" title="Numerical analysis">
Numerical analysis

Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).

One of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.

Numerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of the square root of 2, modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.

Numerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century also the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.

Before the advent of modern computers numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.

The overall goal of the field of numerical analysis is the design and analysis of techniques to give approximate but accurate solutions to hard problems, the variety of which is suggested by the following:


The rest of this section outlines several important themes of numerical analysis.

The field of numerical analysis predates the invention of modern computers by many centuries. Linear interpolation was already in use more than 2000 years ago. Many great mathematicians of the past were preoccupied by numerical analysis, as is obvious from the names of important algorithms like Newton's method, Lagrange interpolation polynomial, Gaussian elimination, or Euler's method.

To facilitate computations by hand, large books were produced with formulas and tables of data such as interpolation points and function coefficients. Using these tables, often calculated out to 16 decimal places or more for some functions, one could look up values to plug into the formulas given and achieve very good numerical estimates of some functions. The canonical work in the field is the NIST publication edited by Abramowitz and Stegun, a 1000-plus page book of a very large number of commonly used formulas and functions and their values at many points. The function values are no longer very useful when a computer is available, but the large listing of formulas can still be very handy.

The mechanical calculator was also developed as a tool for hand computation. These calculators evolved into electronic computers in the 1940s, and it was then found that these computers were also useful for administrative purposes. But the invention of the computer also influenced the field of numerical analysis, since now longer and more complicated calculations could be done.

Direct methods compute the solution to a problem in a finite number of steps. These methods would give the precise answer if they were performed in infinite precision arithmetic. Examples include Gaussian elimination, the QR factorization method for solving systems of linear equations, and the simplex method of linear programming. In practice, finite precision is used and the result is an approximation of the true solution (assuming stability).

In contrast to direct methods, iterative methods are not expected to terminate in a finite number of steps. Starting from an initial guess, iterative methods form successive approximations that converge to the exact solution only in the limit. A convergence test, often involving the residual, is specified in order to decide when a sufficiently accurate solution has (hopefully) been found. Even using infinite precision arithmetic these methods would not reach the solution within a finite number of steps (in general). Examples include Newton's method, the bisection method, and Jacobi iteration. In computational matrix algebra, iterative methods are generally needed for large problems.

Iterative methods are more common than direct methods in numerical analysis. Some methods are direct in principle but are usually used as though they were not, e.g. GMRES and the conjugate gradient method. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted in the same manner as for an iterative method.

Furthermore, continuous problems must sometimes be replaced by a discrete problem whose solution is known to approximate that of the continuous problem; this process is called "discretization". For example, the solution of a differential equation is a function. This function must be represented by a finite amount of data, for instance by its value at a finite number of points at its domain, even though this domain is a continuum.

The study of errors forms an important part of numerical analysis. There are several ways in which error can be introduced in the solution of the problem.

Round-off errors arise because it is impossible to represent all real numbers exactly on a machine with finite memory (which is what all practical digital computers are).

Truncation errors are committed when an iterative method is terminated or a mathematical procedure is approximated, and the approximate solution differs from the exact solution. Similarly, discretization induces a discretization error because the solution of the discrete problem does not coincide with the solution of the continuous problem. For instance, in the iteration in the sidebar to compute the solution of formula_1, after 10 or so iterations, we conclude that the root is roughly 1.99 (for example). We therefore have a truncation error of 0.01.

Once an error is generated, it will generally propagate through the calculation. For instance, we have already noted that the operation + on a calculator (or a computer) is inexact. It follows that a calculation of the type is even more inexact.

What does it mean when we say that the truncation error is created when we approximate a mathematical procedure? We know that to integrate a function exactly requires one to find the sum of infinite trapezoids. But numerically one can find the sum of only finite trapezoids, and hence the approximation of the mathematical procedure. Similarly, to differentiate a function, the differential element approaches zero but numerically we can only choose a finite value of the differential element.

Numerical stability is an important notion in numerical analysis. An algorithm is called "numerically stable" if an error, whatever its cause, does not grow to be much larger during the calculation. This happens if the problem is "well-conditioned", meaning that the solution changes by only a small amount if the problem data are changed by a small amount. To the contrary, if a problem is "ill-conditioned", then any small error in the data will grow to be a large error.

Both the original problem and the algorithm used to solve that problem can be "well-conditioned" and/or "ill-conditioned", and any combination is possible.

So an algorithm that solves a well-conditioned problem may be either numerically stable or numerically unstable. An art of numerical analysis is to find a stable algorithm for solving a well-posed mathematical problem. For instance, computing the square root of 2 (which is roughly 1.41421) is a well-posed problem. Many algorithms solve this problem by starting with an initial approximation "x" to formula_2, for instance "x" = 1.4, and then computing improved guesses "x", "x", etc. One such method is the famous Babylonian method, which is given by "x" = "x"/2 + 1/"x". Another method, which we will call Method X, is given by "x" = ("x" − 2) + "x". We have calculated a few iterations of each scheme in table form below, with initial guesses "x" = 1.4 and "x" = 1.42.

Observe that the Babylonian method converges quickly regardless of the initial guess, whereas Method X converges extremely slowly with initial guess "x" = 1.4 and diverges for initial guess "x" = 1.42. Hence, the Babylonian method is numerically stable, while Method X is numerically unstable.


The field of numerical analysis includes many sub-disciplines. Some of the major ones are:

One of the simplest problems is the evaluation of a function at a given point. The most straightforward approach, of just plugging in the number in the formula is sometimes not very efficient. For polynomials, a better approach is using the Horner scheme, since it reduces the necessary number of multiplications and additions. Generally, it is important to estimate and control round-off errors arising from the use of floating point arithmetic.

Interpolation solves the following problem: given the value of some unknown function at a number of points, what value does that function have at some other point between the given points?

Extrapolation is very similar to interpolation, except that now we want to find the value of the unknown function at a point which is outside the given points.

Regression is also similar, but it takes into account that the data is imprecise. Given some points, and a measurement of the value of some function at these points (with an error), we want to determine the unknown function. The least squares-method is one popular way to achieve this.

Another fundamental problem is computing the solution of some given equation. Two cases are commonly distinguished, depending on whether the equation is linear or not. For instance, the equation formula_7 is linear while formula_8 is not.

Much effort has been put in the development of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. Iterative methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and conjugate gradient method are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.

Root-finding algorithms are used to solve nonlinear equations (they are so named since a root of a function is an argument for which the function yields zero). If the function is differentiable and the derivative is known, then Newton's method is a popular choice. Linearization is another technique for solving nonlinear equations.

Several important problems can be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image compression algorithm is based on the singular value decomposition. The corresponding tool in statistics is called principal component analysis.

Optimization problems ask for the point at which a given function is maximized (or minimized). Often, the point also has to satisfy some constraints.

The field of optimization is further split in several subfields, depending on the form of the objective function and the constraint. For instance, linear programming deals with the case that both the objective function and the constraints are linear. A famous method in linear programming is the simplex method.

The method of Lagrange multipliers can be used to reduce optimization problems with constraints to unconstrained optimization problems.

Numerical integration, in some instances also known as numerical quadrature, asks for the value of a definite integral. Popular methods use one of the Newton–Cotes formulas (like the midpoint rule or Simpson's rule) or Gaussian quadrature. These methods rely on a "divide and conquer" strategy, whereby an integral on a relatively large set is broken down into integrals on smaller sets. In higher dimensions, where these methods become prohibitively expensive in terms of computational effort, one may use Monte Carlo or quasi-Monte Carlo methods (see Monte Carlo integration), or, in modestly large dimensions, the method of sparse grids.

Numerical analysis is also concerned with computing (in an approximate way) the solution of differential equations, both ordinary differential equations and partial differential equations.

Partial differential equations are solved by first discretizing the equation, bringing it into a finite-dimensional subspace. This can be done by a finite element method, a finite difference method, or (particularly in engineering) a finite volume method. The theoretical justification of these methods often involves theorems from functional analysis. This reduces the problem to the solution of an algebraic equation.

Since the late twentieth century, most algorithms are implemented in a variety of programming languages. The Netlib repository contains various collections of software routines for numerical problems, mostly in Fortran and C. Commercial products implementing many different numerical algorithms include the IMSL and NAG libraries; a free-software alternative is the GNU Scientific Library.

There are several popular numerical computing applications such as MATLAB, TK Solver, S-PLUS, and IDL as well as free and open source alternatives such as FreeMat, Scilab, GNU Octave (similar to Matlab), and IT++ (a C++ library). There are also programming languages such as R (similar to S-PLUS) and Python with libraries such as NumPy, SciPy and SymPy. Performance varies widely: while vector and matrix operations are usually fast, scalar loops may vary in speed by more than an order of magnitude.

Many computer algebra systems such as Mathematica also benefit from the availability of arbitrary precision arithmetic which can provide more accurate results.

Also, any spreadsheet software can be used to solve simple problems relating to numerical analysis.



Journals

Online texts

Online course material


</doc>
<doc id="21508" url="https://en.wikipedia.org/wiki?curid=21508" title="Noosphere">
Noosphere

The noosphere (; sometimes noösphere) is the sphere of human thought. The word derives from the Greek νοῦς (nous "mind") and σφαῖρα (sphaira "sphere"), in lexical analogy to "atmosphere" and "biosphere". It was introduced by Pierre Teilhard de Chardin in 1922 in his "Cosmogenesis". Another possibility is the first use of the term by Édouard Le Roy (1870–1954), who together with Teilhard was listening to lectures of Vladimir Ivanovich Vernadsky at the Sorbonne. In 1936, Vernadsky accepted the idea of the noosphere in a letter to Boris Leonidovich Lichkov (though he states that the concept derives from Le Roy. Citing the work of Teilhard's biographer—Rene Cuenot—Sampson and Pitt stated that although the concept was jointly developed by all three men (Vernadsky, LeRoy, and Teilhard), Teilhard believed that he actually invented the word: "I believe, so far as one can ever tell, that the word 'noosphere' was my invention: but it was he [Le Roy] who launched it."

In the theory of Vernadsky, the noosphere is the third in a succession of phases of development of the Earth, after the geosphere (inanimate matter) and the biosphere (biological life). Just as the emergence of life fundamentally transformed the geosphere, the emergence of human cognition fundamentally transforms the biosphere. In contrast to the conceptions of the Gaia theorists, or the promoters of cyberspace, Vernadsky's noosphere emerges at the point where humankind, through the mastery of nuclear processes, begins to create resources through the transmutation of elements. It is also currently being researched as part of the Princeton Global Consciousness Project.

Teilhard perceived a directionality in evolution along an axis of increasing "Complexity/Consciousness". For Teilhard, the noosphere is the sphere of thought encircling the earth that has emerged through evolution as a consequence of this growth in complexity / consciousness. The noosphere is therefore as much part of nature as the barysphere, lithosphere, hydrosphere, atmosphere, and biosphere. As a result, Teilhard sees the "social phenomenon [as] the culmination of and not the attenuation of the biological phenomenon." These social phenomena are part of the noosphere and include, for example, legal, educational, religious, research, industrial and technological systems. In this sense, the noosphere emerges through and is constituted by the interaction of human minds. The noosphere thus grows in step with the organization of the human mass in relation to itself as it populates the earth. Teilhard argued the noosphere evolves towards ever greater personalisation, individuation and unification of its elements. He saw the Christian notion of love as being the principal driver of noogenesis. Evolution would culminate in the Omega Point—an apex of thought/consciousness—which he identified with the eschatological return of Christ.

One of the original aspects of the noosphere concept deals with evolution. Henri Bergson, with his "L'évolution créatrice" (1907), was one of the first to propose evolution is "creative" and cannot necessarily be explained solely by Darwinian natural selection. "L'évolution créatrice" is upheld, according to Bergson, by a constant vital force which animates life and fundamentally connects mind and body, an idea opposing the dualism of René Descartes. In 1923, C. Lloyd Morgan took this work further, elaborating on an "emergent evolution" which could explain increasing complexity (including the evolution of mind). Morgan found many of the most interesting changes in living things have been largely discontinuous with past evolution. Therefore, these living things did not necessarily evolve through a gradual process of natural selection. Rather, he posited, the process of evolution experiences jumps in complexity (such as the emergence of a self-reflective universe, or noosphere). Finally, the complexification of human cultures, particularly language, facilitated a quickening of evolution in which cultural evolution occurs more rapidly than biological evolution. Recent understanding of human ecosystems and of human impact on the biosphere have led to a link between the notion of sustainability with the "co-evolution" and harmonization of cultural and biological evolution.





</doc>
<doc id="21511" url="https://en.wikipedia.org/wiki?curid=21511" title="Niccolò Paganini">
Niccolò Paganini

Niccolò (or Nicolò) Paganini (; 27 October 178227 May 1840) was an Italian violinist, violist, guitarist, and composer. He was the most celebrated violin virtuoso of his time, and left his mark as one of the pillars of modern violin technique. His 24 Caprices for Solo Violin Op. 1 are among the best known of his compositions, and have served as an inspiration for many prominent composers.

Niccolò Paganini was born in Genoa, then capital of the Republic of Genoa, the third of the six children of Antonio and Teresa (née Bocciardo) Paganini. Paganini's father was an unsuccessful trader, but he managed to supplement his income through playing music on the mandolin. At the age of five, Paganini started learning the mandolin from his father, and moved to the violin by the age of seven. His musical talents were quickly recognized, earning him numerous scholarships for violin lessons. The young Paganini studied under various local violinists, including Giovanni Servetto and Giacomo Costa, but his progress quickly outpaced their abilities. Paganini and his father then traveled to Parma to seek further guidance from Alessandro Rolla. But upon listening to Paganini's playing, Rolla immediately referred him to his own teacher, Ferdinando Paer and, later, Paer's own teacher, Gasparo Ghiretti. Though Paganini did not stay long with Paer or Ghiretti, the two had considerable influence on his composition style.

The French invaded northern Italy in March 1796, and Genoa was not spared. The Paganinis sought refuge in their country property in Romairone, near Bolzaneto. It was in this period that Paganini is thought to have developed his relationship with the guitar. He mastered the guitar, but preferred to play it in exclusively intimate, rather than public concerts. He later described the guitar as his "constant companion" on his concert tours. By 1800, Paganini and his father traveled to Livorno, where Paganini played in concerts and his father resumed his maritime work. In 1801, the 18-year-old Paganini was appointed first violin of the Republic of Lucca, but a substantial portion of his income came from freelancing. His fame as a violinist was matched only by his reputation as a gambler and womanizer.

In 1805, Lucca was annexed by Napoleonic France, and the region was ceded to Napoleon's sister, Elisa Baciocchi. Paganini became a violinist for the Baciocchi court, while giving private lessons to Elisa's husband, Felice. In 1807, Baciocchi became the Grand Duchess of Tuscany and her court was transferred to Florence. Paganini was part of the entourage, but, towards the end of 1809, he left Baciocchi to resume his freelance career.

For the next few years, Paganini returned to touring in the areas surrounding Parma and Genoa. Though he was very popular with the local audience, he was still not very well known in the rest of Europe. His first break came from an 1813 concert at La Scala in Milan. The concert was a great success. As a result, Paganini began to attract the attention of other prominent, though more conservative, musicians across Europe. His early encounters with Charles Philippe Lafont and Louis Spohr created intense rivalry. His concert activities, however, were still limited to Italy for the next few years.

In 1827, Pope Leo XII honoured Paganini with the Order of the Golden Spur. His fame spread across Europe with a concert tour that started in Vienna in August 1828, stopping in every major European city in Germany, Poland, and Bohemia until February 1831 in Strasbourg. This was followed by tours in Paris and Britain. His technical ability and his willingness to display it received much critical acclaim. In addition to his own compositions, theme and variations being the most popular, Paganini also performed modified versions of works (primarily concertos) written by his early contemporaries, such as Rodolphe Kreutzer and Giovanni Battista Viotti.

Paganini's travels also brought him into contact with eminent guitar virtuosi of the day, including Ferdinando Carulli in Paris and Mauro Giuliani in Vienna. But this experience did not inspire him to play public concerts with guitar, and even performances of his own guitar trios and quartets were private to the point of being behind closed doors.
Throughout his life, Paganini was no stranger to chronic illnesses. Although no definite medical proof exists, he was reputed to have been affected by Marfan syndrome or Ehlers–Danlos syndrome. In addition, his frequent concert schedule, as well as his extravagant lifestyle, took their toll on his health. He was diagnosed with syphilis as early as 1822, and his remedy, which included mercury and opium, came with serious physical and psychological side effects. In 1834, while still in Paris, he was treated for tuberculosis. Though his recovery was reasonably quick, after the illness his career was marred by frequent cancellations due to various health problems, from the common cold to depression, which lasted from days to months.

In September 1834, Paganini put an end to his concert career and returned to Genoa. Contrary to popular beliefs involving his wishing to keep his music and techniques secret, Paganini devoted his time to the publication of his compositions and violin methods. He accepted students, of whom two enjoyed moderate success: violinist Camillo Sivori and cellist Gaetano Ciandelli. Neither, however, considered Paganini helpful or inspirational. In 1835, Paganini returned to Parma, this time under the employ of Archduchess Marie Louise of Austria, Napoleon's second wife. He was in charge of reorganizing her court orchestra. However, he eventually conflicted with the players and court, so his visions never saw completion. In Paris, he befriended the 11-year-old Polish virtuoso Apollinaire de Kontski, giving him some lessons and a signed testimonial. It was widely put about, falsely, that Paganini was so impressed with de Kontski's skills that he bequeathed him his violins and manuscripts.

In 1836, Paganini returned to Paris to set up a casino. Its immediate failure left him in financial ruin, and he auctioned off his personal effects, including his musical instruments, to recoup his losses. At Christmas of 1838, he left Paris for Marseilles and, after a brief stay, travelled to Nice where his condition worsened. In May 1840, the Bishop of Nice sent Paganini a local parish priest to perform the last rites. Paganini assumed the sacrament was premature, and refused.

A week later, on 27 May 1840, Paganini died from internal hemorrhaging before a priest could be summoned. Because of this, and his widely rumored association with the devil, the Church denied his body a Catholic burial in Genoa. It took four years and an appeal to the Pope before the Church let his body be transported to Genoa, but it was still not buried. His body was finally buried in 1876, in a cemetery in Parma. In 1893, the Czech violinist František Ondříček persuaded Paganini's grandson, Attila, to allow a viewing of the violinist's body. After this episode, Paganini's body was finally reinterred in a new cemetery in Parma in 1896.

Though having no shortage of romantic conquests, Paganini was seriously involved with a singer named Antonia Bianchi from Como, whom he met in Milan in 1813. The two gave concerts together throughout Italy. They had a son, Achilles Cyrus Alexander, born on 23 July 1825 in Palermo and baptized at San Bartolomeo's. They never legalized their union and it ended around April 1828 in Vienna. Paganini brought Achilles on his European tours, and Achilles later accompanied his father until the latter's death. He was instrumental in dealing with his father's burial, years after his death.

Throughout his career, Paganini also became close friends with composers Gioachino Rossini and Hector Berlioz. Rossini and Paganini met in Bologna in the summer of 1818. In January 1821, on his return from Naples, Paganini met Rossini again in Rome, just in time to become the substitute conductor for Rossini's opera "Matilde di Shabran", upon the sudden death of the original conductor. Paganini's efforts earned gratitude from Rossini.

Paganini met Berlioz in Paris, and was a frequent correspondent as a penfriend. He commissioned a piece from the composer, but was not satisfied with the resultant four-movement piece for orchestra and viola obbligato "Harold en Italie". He never performed it, and instead it was premiered a year later by violist Christian Urhan. He did however write his own "Sonata per Gran Viola" Op. 35 (with orchestra or guitar accompaniment). Despite his alleged lack of interest in "Harold", Paganini often referred to Berlioz as the resurrection of Beethoven and, towards the end of his life, he gave large sums to the composer. They shared an active interest in the guitar, which they both played and used in compositions. Paganini gave Berlioz a guitar, which they both signed on its sound box.

Paganini was in possession of a number of fine string instruments. More legendary than these were the circumstances under which he obtained (and lost) some of them. While Paganini was still a teenager in Livorno, a wealthy businessman named Livron lent him a violin, made by the master luthier Giuseppe Guarneri, for a concert. Livron was so impressed with Paganini's playing that he refused to take it back. This particular violin came to be known as "Il Cannone Guarnerius". On a later occasion in Parma, he won another valuable violin (also by Guarneri) after a difficult sight-reading challenge from a man named Pasini.
Other instruments associated with Paganini include the "Antonio Amati" 1600, the "Nicolò Amati" 1657, the "Paganini-Desaint" 1680 Stradivari, the Guarneri-filius "Andrea" 1706, the "Le Brun" 1712 Stradivari, the "Vuillaume" c. 1720 Bergonzi, the "Hubay" 1726 Stradivari, and the "Comte Cozio di Salabue" 1727 violins; the "Countess of Flanders" 1582 da Salò-di Bertolotti, and the "Mendelssohn" 1731 Stradivari violas; the "Piatti" 1700 Goffriller, the "Stanlein" 1707 Stradivari, and the "Ladenburg" 1736 Stradivari cellos; and the "Grobert of Mirecourt" 1820 (guitar). Four of these instruments were played by the Tokyo String Quartet.

Of his guitars, there is little evidence remaining of his various choices of instrument. The aforementioned guitar that he gave to Berlioz is a French instrument made by one Grobert of Mirecourt. The luthier made his instrument in the style of René Lacote, a more well-known Paris-based guitar-maker. It is preserved and on display in the Musée de la Musique in Paris.

Of the guitars he owned through his life, there was an instrument by Gennaro Fabricatore that he had refused to sell even in his periods of financial stress, and was among the instruments in his possession at the time of his death. There is an unsubstantiated rumour that he also played Stauffer guitars; he may certainly have come across these in his meetings with Giuliani in Vienna.

Paganini composed his own works to play exclusively in his concerts, all of which profoundly influenced the evolution of violin technique. His 24 Caprices were likely composed in the period between 1805 and 1809, while he was in the service of the Baciocchi court. Also during this period, he composed the majority of the solo pieces, duo-sonatas, trios and quartets for the guitar, either as a solo instrument or with strings. These chamber works may have been inspired by the publication, in Lucca, of the guitar quintets of Boccherini. Many of his variations, including "Le Streghe", "The Carnival of Venice", and "Nel cor più non mi sento", were composed, or at least first performed, before his European concert tour.

Generally speaking, Paganini's compositions were technically imaginative, and the timbre of the instrument was greatly expanded as a result of these works. Sounds of different musical instruments and animals were often imitated. One such composition was titled "Il Fandango Spanolo" (The Spanish Dance), which featured a series of humorous imitations of farm animals. Even more outrageous was a solo piece "Duetto Amoroso", in which the sighs and groans of lovers were intimately depicted on the violin. There survives a manuscript of the "Duetto", which has been recorded. The existence of the "Fandango" is known only through concert posters.

However, his works were criticized for lacking characteristics of true polyphonism, as pointed out by Eugène Ysaÿe. Yehudi Menuhin, on the other hand, suggested that this might have been the result of his reliance on the guitar (in lieu of the piano) as an aid in composition. The orchestral parts for his concertos were often polite, unadventurous, and clearly supportive of the soloist. In this, his style is consistent with that of other Italian composers such as Paisiello, Rossini and Donizetti, who were influenced by the guitar-song milieu of Naples during this period.

Paganini was also the inspiration of many prominent composers. Both "La Campanella" and the A minor Caprice (No. 24) have been an object of interest for a number of composers. Franz Liszt, Robert Schumann, Johannes Brahms, Sergei Rachmaninoff, Boris Blacher, Andrew Lloyd Webber, George Rochberg and Witold Lutosławski, among others, wrote well-known variations on these themes.

The Israeli violinist Ivry Gitlis once referred to Paganini as a phenomenon rather than a development. Though some of the techniques frequently employed by Paganini were already present, most accomplished violinists of the time focused on intonation and bowing techniques. Arcangelo Corelli (1653–1713) was considered a pioneer in transforming the violin from an ensemble instrument to a solo instrument. In the meantime, the polyphonic capability of the violin was firmly established through the Sonatas and Partitas BWV 1001–1006 of Johann Sebastian Bach (1685–1750). Other notable violinists included Antonio Vivaldi (1678–1741) and Giuseppe Tartini (1692–1770), who, in their compositions, reflected the increasing technical and musical demands on the violinist. Although the role of the violin in music drastically changed through this period, progress in violin technique was steady but slow. Techniques requiring agility of the fingers and the bow were still considered unorthodox and discouraged by the established community of violinists.

Much of Paganini's playing (and his violin composition) was influenced by two violinists, Pietro Locatelli (1693–1746) and August Duranowski (Auguste Frédéric Durand) (1770–1834). During Paganini's study in Parma, he came across the 24 Caprices of Locatelli (entitled "L'arte di nuova modulazione – Capricci enigmatici" or "The art of the new style – the enigmatic caprices"). Published in the 1730s, they were shunned by the musical authorities for their technical innovations, and were forgotten by the musical community at large. Around the same time, Durand, a former student of Giovanni Battista Viotti (1755–1824), became a celebrated violinist. He was renowned for his use of harmonics and the left hand pizzicato in his performance. Paganini was impressed by Durand's innovations and showmanship, which later also became the hallmarks of the young violin virtuoso. Paganini was instrumental in the revival and popularization of these violinistic techniques, which are now incorporated into regular compositions.

Another aspect of Paganini's violin techniques concerned his flexibility. He had exceptionally long fingers and was capable of playing three octaves across four strings in a hand span, an extraordinary feat even by today's standards. His seemingly unnatural ability may have been a result of Marfan syndrome.

Notable works inspired by compositions of Paganini include:
The "Caprice No. 24 in A minor", Op. 1, ("Tema con variazioni") has been the basis of works by many other composers. Notable examples include Brahms's "Variations on a Theme of Paganini" and Rachmaninoff's "Rhapsody on a Theme of Paganini".

The Paganini Competition ("Premio Paganini") is an international violin competition created in 1954 in his home city of Genoa and named in his honour.

In 1972 the State of Italy purchased a large collection of Niccolò Paganini manuscripts from the W. Heyer Library of Cologne. They are housed at the Biblioteca Casanatense in Rome.

In 1982 the city of Genoa commissioned a thematic catalogue of music by Paganini, edited by Maria Rosa Moretti and Anna Sorrento, hence the abbreviation "MS" assigned to his catalogued works.
A minor planet 2859 Paganini discovered in 1978 by Soviet astronomer Nikolai Chernykh is named after him.

Although no photographs of Paganini are known to exist, in 1900 Italian violin maker Giuseppe Fiorini forged the now famous fake daguerreotype of the celebrated violinist. So well in fact, that even the great classical author and conversationalist Arthur M. Abell was led to believe it to be true, reprinting the image in the 22 January 1901 issue of the "Musical Courier".

Paganini has been portrayed by a number of actors in film and television productions, including Stewart Granger in the 1946 biographical portrait "The Magic Bow", Roxy Roth in "A Song to Remember" (1945), Klaus Kinski in "Kinski Paganini" (1989) and David Garrett in "The Devil's Violinist" (2013).

In the Soviet 1982 miniseries "Niccolo Paganini" the musician is portrayed by the Armenian actor Vladimir Msryan. The series focuses on Paganini's relationship with the Roman Catholic Church. Another Soviet actor, Armen Dzhigarkhanyan, plays Paganini's fictionalized arch-rival, an insidious Jesuit official. The information in the series is generally spurious and it also plays to some of the myths and legends rampant during the musician's lifetime. One memorable scene shows Paganini's adversaries sabotaging his violin before a high-profile performance, causing all strings but one to break during the concert. An undeterred Paganini continues to perform on three, two, and finally on a single string. In actuality, Paganini himself occasionally broke strings during his performances on purpose so he could further display his virtuosity. He did this by carefully filing notches into them to weaken them, so that they would break when in use.

In Don Nigro's satirical comedy "Paganini" (1995), the great violinist seeks vainly for his salvation, claiming that he unknowingly sold his soul to the Devil. "Variation upon variation," he cries at one point, "but which variation leads to salvation and which to damnation? Music is a question for which there is no answer." Paganini is portrayed as having killed three of his lovers and sinking repeatedly into poverty, prison, and drink. Each time he is "rescued" by the Devil who appears in different guises, returning Paganini's violin so he can continue playing. In the end, Paganini's salvation—administered by a god-like Clockmaker—turns out to be imprisonment in a large bottle where he plays his music for the amusement of the public through all eternity. "Do not pity him, my dear," the Clockmaker tells Antonia, one of Paganini's murdered wives. "He is alone with the answer for which there is no question. The saved and the damned are the same."



Images


</doc>
<doc id="21512" url="https://en.wikipedia.org/wiki?curid=21512" title="North Atlantic Current">
North Atlantic Current

The North Atlantic Current (NAC), also known as North Atlantic Drift and North Atlantic Sea Movement, is a powerful warm western boundary current within the Atlantic Ocean that extends the Gulf Stream northeastward.

The NAC originates from where the Gulf Stream turns north at the Southeast Newfoundland Rise, a submarine ridge that stretches southeast from the Grand Banks. The NAC flows northward east of the Grand Banks, from 40°N to 51°N, before turning sharply east to cross the Atlantic. It transports more warm tropical water to northern latitudes than any other boundary current; more than 40 Sv in the south and 20 Sv as it crosses the Mid-Atlantic Ridge. It reaches speeds of 2 knots near the North American coast. Directed by topography, the NAC meanders heavily, but in contrast to the meanders of the Gulf Stream, the NAC meanders remain stable without breaking off into eddies.

The colder parts of the Gulf Stream turn northward near the "tail" of the Grand Banks at 50°W where the Azores Current branches off to flow south of the Azores. From there the NAC flows northeastward, east of the Flemish Cap (47°N, 45°W). Approaching the Mid-Atlantic Ridge, it then turns eastward and becomes much broader and more diffuse. It then splits into a colder northeastern branch and a warmer eastern branch. As the warmer branch turns southward, most of the subtropical component of the Gulf Stream is diverted southward, and as a consequence, the North Atlantic is mostly supplied by subpolar waters, including a contribution from the Labrador Current recirculated into the NAC at 45°N.

West of Continental Europe, it splits into two major branches. One branch goes southeast, becoming the Canary Current as it passes northwest Africa and turns southwest. The other major branch continues north along the coast of Northwestern Europe.
Other branches include the Irminger Current and the Norwegian Current. Driven by the global thermohaline circulation, the North Atlantic Current is part of the wind-driven Gulf Stream, which goes further east and north from the North American coast across the Atlantic and into the Arctic Ocean.

The North Atlantic Current, together with the Gulf Stream, have a long-lived reputation for having a considerable warming influence on European climate. However, the principal cause for differences in winter climate between North America and Europe seems to be winds rather than ocean currents (although the currents do exert influence at very high latitudes by preventing the formation of sea ice).





</doc>
<doc id="21513" url="https://en.wikipedia.org/wiki?curid=21513" title="North Atlantic Deep Water">
North Atlantic Deep Water

North Atlantic Deep Water (NADW) is a deep water mass formed in the North Atlantic Ocean. Thermohaline circulation of the world's oceans involves the flow of warm surface waters from the southern hemisphere into the North Atlantic. Water flowing northward becomes modified through evaporation and mixing with other water masses, leading to increased salinity. When this water reaches the North Atlantic it cools and sinks through convection, due to its decreased temperature and increased salinity resulting in increased density. NADW is the outflow of this thick deep layer, which can be detected by its high salinity, high oxygen content, nutrient minima, high C/C, and chlorofluorocarbons (CFCs). 

CFCs are anthropogenic substances that enter the surface of the ocean from gas exchange with the atmosphere. This distinct composition allows its path to be traced as it mixes with Circumpolar Deep Water (CDW), which in turn fills the deep Indian Ocean and part of the South Pacific. NADW and its formation is essential to the Atlantic Meridional Overturning Circulation (AMOC), which is responsible for transporting large amounts of water, heat, salt, carbon, nutrients and other substances from the Tropical Atlantic to the Mid and High Latitude Atlantic. 

In the conveyor belt model of thermohaline circulation of the world's oceans, the sinking of NADW pulls the waters of the North Atlantic drift northward. However, this is almost certainly an oversimplification of the actual relationship between NADW formation and the strength of the Gulf Stream/North Atlantic drift.

NADW has a temperature of 2-4 °C with a salinity of 34.9-35.0 psu found at a depth between 1500 and 4000m.

The NADW is a complex of several water masses formed by deep convection and also by overflow of dense water across the Greenland-Iceland-Scotland Ridge.

The upper layers are formed by deep open ocean convection during winter. Labrador Sea Water (LSW), formed in the Labrador Sea can reach depths of 2000 m as dense water sinks downward. Classical Labrador Sea Water (CLSW) production is dependent on preconditioning of water in the Labrador Sea from the previous year, and the strength of the North Atlantic Oscillation.

During a positive NAO phase, conditions exist for strong winter storms to develop. These storms freshen the surface water, and their winds increase cyclonic flow, which allows denser waters to sink. As a result, the temperature, salinity, and density vary yearly. In some years these conditions do not exist and CLSW is not formed. CLSW has characteristic potential temperature of 3 °C, salinity of 34.88 psu, and density of 34.66. 

Another component of LSW is the Upper Labrador Sea Water (ULSW). ULSW forms at a density lower than CLSW and has a CFC maximum between 1200 and 1500 m in the subtropical North Atlantic. Eddies of cold less saline ULSW have similar densities of warmer saltier water and flow along the DWBC, but maintain their high CFCs. The ULSW eddies erode rapidly as they mix laterally with this warmer saltier water.

The lower waters mass of NADW form from overflow of the Greenland-Iceland-Scotland Ridge. They are Iceland-Scotland Overflow Water (ISOW) and Denmark Strait Overflow Water (DSOW). The overflows are a combination of dense Arctic Ocean water (18%), modified Atlantic water (32%), and intermediate water from the Nordic seas (20%), that entrain and mix with other water masses (contributing 30%) as they flow over the Greenland-Iceland-Scotland Ridge. 

The formation of both of these waters involves the conversion of warm salty northward flowing surface waters to cold dense deep waters behind the Greenland-Iceland-Scotland Ridge. Water flow from the North Atlantic current enters the Arctic Ocean through the Norwegian Current which splits into the Fram Strait and Barents Sea Branch. Water from the Fram Strait recirculates, reaching a density of DSOW, sinks, and flows towards the Denmark Strait. Water flowing into the Barent Sea feeds ISOW.

ISOW enters the eastern North Atlantic over the Iceland-Scotland Ridge through the Faeroe Bank Channel at a depth of 850 m, with some water flowing over the shallower Iceland-Faeroe Rise. ISOW has a low CFC concentrations and it has been estimated from these concentrations that ISOW resides behind the ridge for 45 years. As the water flows southward at the bottom of the channel, it entrains surrounding water of the eastern North Atlantic, and flows to the western North Atlantic through the Charlie-Gibbs Fracture Zone, entraining with LSW. This water is less dense than (DSOW) and lays above it as it flows cyclonically in the Irminger Basin.

DSOW is the coldest, densest, and freshest water mass of NADW. DSOW formed behind the ridge flows over the Denmark Strait at a depth of 600m. The most significant water mass contributing to DSOW is Arctic Intermediate Water (AIW). Winter cooling and convection allow AIW to sink and pool behind the Denmark Strait. Upper AIW has a high amount of anthropogenic tracers due its exposure to the atmosphere. AIW's tritium and CFC signature is observed in DSOW at the base of the Greenland continental slope. This also showed that the DSOW flowing 450 km to the south was no older than 2 years. Both the DSOW and ISOW flow around the Irminger Basin and Labrador Sea in a deep boundary current. Leaving the Greenland Sea with 2.5 Sv its flow increases to 10 Sv south of Greenland. It is cold and relatively fresh, flowing below 3500 m in the DWBC and spreading inward the deep Atlantic basins.

The southward spread of NADW along the Deep Western Boundary current (DWBC) can be traced by its high oxygen content, high CFCs, and density.

ULSW is the major source of upper NADW. ULSW advects southward from the Labrador Sea in small eddies that mix into the DWBC. A CFC maxima associated with ULSW has been observed along 24°N in the DWBC at 1500 m. Some of the upper ULSW recirculates into the Gulf Stream, while some remains in the DWBC. High CFCs in the subtropics indicate recirculation in the subtropics.

ULSW that remains in the DWBC dilutes as it moves equatorward. Deep convection in the Labrador Sea during the late 1980s and early 1990s resulted in CLSW with a lower CFC concentration due to downward mixing. Convection allowed the CFCs to penetrate further downward to 2000m. These minimum could be tracked, and were first observed in the subtropics in the early 1990s.

ISOW and DSOW flow around the Irminger Basin and DSOW entering the DWBC. These are the two lower portions of the NADW. Another CFC maximum is seen at 3500 m in the subtropics from the DSOW contribution to NADW. Some of the NADW recirculates with the northern gyre. To the south of the gyre NADW flows under the Gulf Stream where it continues along the DWBC until it reaches another gyre in the subtropics.

Lower North Atlantic Deep Water (LNADW), originating in the Greenland and Norwegian Seas, brings high salinity, oxygen, and freon concentrations towards to the Romanche Trench, an equatorial fracture zone in the Mid-Atlantic Ridge (MAR). Found at depths around , LNADW flow east through the trench over AABW, the trench being the only opening in the MAR where inter-basin exchange is possible for these two water masses.

It is believed that North Atlantic Deep Water formation has been dramatically reduced at times during the past (such as during the Younger Dryas or during Heinrich events), and that this might correlate with a decrease in the strength of the Gulf Stream and the North Atlantic drift, in turn cooling the climate of northwestern Europe. 

There is concern that global warming might cause this to happen again. It is also hypothesized that during the Last Glacial Maximum (LGM), NADW was replaced with an analogous watermass that occupied a shallower depth known as Glacial North Atlantic Intermediate Water (GNAIW).




</doc>
<doc id="21514" url="https://en.wikipedia.org/wiki?curid=21514" title="Nanomedicine">
Nanomedicine

Nanomedicine is the medical application of nanotechnology. Nanomedicine ranges from the medical applications of nanomaterials and biological devices, to nanoelectronic biosensors, and even possible future applications of molecular nanotechnology such as biological machines. Current problems for nanomedicine involve understanding the issues related to toxicity and environmental impact of nanoscale materials (materials whose structure is on the scale of nanometers, i.e. billionths of a meter).

Functionalities can be added to nanomaterials by interfacing them with biological molecules or structures. The size of nanomaterials is similar to that of most biological molecules and structures; therefore, nanomaterials can be useful for both in vivo and in vitro biomedical research and applications.
Thus far, the integration of nanomaterials with biology has led to the development of diagnostic devices, contrast agents, analytical tools, physical therapy applications, and drug delivery vehicles.

Nanomedicine seeks to deliver a valuable set of research tools and clinically useful devices in the near future. The National Nanotechnology Initiative expects new commercial applications in the pharmaceutical industry that may include advanced drug delivery systems, new therapies, and in vivo imaging. Nanomedicine research is receiving funding from the US National Institutes of Health Common Fund program, supporting four nanomedicine development centers.

Nanomedicine sales reached $16 billion in 2015, with a minimum of $3.8 billion in nanotechnology R&D being invested every year. Global funding for emerging nanotechnology increased by 45% per year in recent years, with product sales exceeding $1 trillion in 2013. As the nanomedicine industry continues to grow, it is expected to have a significant impact on the economy.

 Nanotechnology has provided the possibility of delivering drugs to specific cells using nanoparticles. The overall drug consumption and side-effects may be lowered significantly by depositing the active agent in the morbid region only and in no higher dose than needed. Targeted drug delivery is intended to reduce the side effects of drugs with concomitant decreases in consumption and treatment expenses. Drug delivery focuses on maximizing bioavailability both at specific places in the body and over a period of time. This can potentially be achieved by molecular targeting by nanoengineered devices. A benefit of using nanoscale for medical technologies is that smaller devices are less invasive and can possibly be implanted inside the body, plus biochemical reaction times are much shorter. These devices are faster and more sensitive than typical drug delivery. The efficacy of drug delivery through nanomedicine is largely based upon: a) efficient encapsulation of the drugs, b) successful delivery of drug to the targeted region of the body, and c) successful release of the drug.

Drug delivery systems, lipid- or polymer-based nanoparticles, can be designed to improve the pharmacokinetics and biodistribution of the drug. However, the pharmacokinetics and pharmacodynamics of nanomedicine is highly variable among different patients. When designed to avoid the body's defence mechanisms, nanoparticles have beneficial properties that can be used to improve drug delivery. Complex drug delivery mechanisms are being developed, including the ability to get drugs through cell membranes and into cell cytoplasm. Triggered response is one way for drug molecules to be used more efficiently. Drugs are placed in the body and only activate on encountering a particular signal. For example, a drug with poor solubility will be replaced by a drug delivery system where both hydrophilic and hydrophobic environments exist, improving the solubility. Drug delivery systems may also be able to prevent tissue damage through regulated drug release; reduce drug clearance rates; or lower the volume of distribution and reduce the effect on non-target tissue. However, the biodistribution of these nanoparticles is still imperfect due to the complex host's reactions to nano- and microsized materials and the difficulty in targeting specific organs in the body. Nevertheless, a lot of work is still ongoing to optimize and better understand the potential and limitations of nanoparticulate systems. While advancement of research proves that targeting and distribution can be augmented by nanoparticles, the dangers of nanotoxicity become an important next step in further understanding of their medical uses.

Nanoparticles are under research for their potential to decrease antibiotic resistance or for various antimicrobial uses. Nanoparticles might also used to circumvent multidrug resistance (MDR) mechanisms.
Two forms of nanomedicine that have already been tested in mice and are awaiting human testing will use gold nanoshells to help diagnose and treat cancer, along with liposomes as vaccine adjuvants and drug transport vehicles. Similarly, drug detoxification is also another application for nanomedicine which has shown promising results in rats. Advances in Lipid nanotechnology was also instrumental in engineering medical nanodevices and novel drug delivery systems as well as in developing sensing applications. Another example can be found in dendrimers and nanoporous materials. Another example is to use block co-polymers, which form micelles for drug encapsulation.

Polymeric nanoparticles are a competing technology to lipidic (based mainly on Phospholipids) nanoparticles. There is an additional risk of toxicity associated with polymers not widely studied or understood. The major advantages of polymers is stability, lower cost and predictable characterisation. However, in the patient's body this very stability (slow degradation) is a negative factor. Phospholipids on the other hand are membrane lipids (already present in the body and surrounding each cell), have a GRAS (Generally Recognised As Safe) status from FDA and are derived from natural sources without any complex chemistry involved. They are not metabolised but rather absorbed by the body and the degradation products are themselves nutrients (fats or micronutrients).

Protein and peptides exert multiple biological actions in the human body and they have been identified as showing great promise for treatment of various diseases and disorders. These macromolecules are called biopharmaceuticals. Targeted and/or controlled delivery of these biopharmaceuticals using nanomaterials like nanoparticles
and Dendrimers is an emerging field called nanobiopharmaceutics, and these products are called nanobiopharmaceuticals.

Another highly efficient system for microRNA delivery for example are nanoparticles formed by the self-assembly of two different microRNAs deregulated in cancer.

Another vision is based on small electromechanical systems; nanoelectromechanical systems are being investigated for the active release of drugs and sensors. Some potentially important applications include cancer treatment with iron nanoparticles or gold shells or cancer early diagnosis. Nanotechnology is also opening up new opportunities in implantable delivery systems, which are often preferable to the use of injectable drugs, because the latter frequently display first-order kinetics (the blood concentration goes up rapidly, but drops exponentially over time). This rapid rise may cause difficulties with toxicity, and drug efficacy can diminish as the drug concentration falls below the targeted range.

Some nanotechnology-based drugs that are commercially available or in human clinical trials include:

Existing and potential drug nanocarriers have been reviewed.

Nanoparticles have high surface area to volume ratio. This allows for many functional groups to be attached to a nanoparticle, which can seek out and bind to certain tumor cells. Additionally, the small size of nanoparticles (10 to 100 nanometers), allows them to preferentially accumulate at tumor sites (because tumors lack an effective lymphatic drainage system). Limitations to conventional cancer chemotherapy include drug resistance, lack of selectivity, and lack of solubility. Nanoparticles have the potential to overcome these problems.

In photodynamic therapy, a particle is placed within the body and is illuminated with light from the outside. The light gets absorbed by the particle and if the particle is metal, energy from the light will heat the particle and surrounding tissue. Light may also be used to produce high energy oxygen molecules which will chemically react with and destroy most organic molecules that are next to them (like tumors). This therapy is appealing for many reasons. It does not leave a "toxic trail" of reactive molecules throughout the body (chemotherapy) because it is directed where only the light is shined and the particles exist. Photodynamic therapy has potential for a noninvasive procedure for dealing with diseases, growth and tumors. Kanzius RF therapy is one example of such therapy (nanoparticle hyperthermia) . Also, gold nanoparticles have the potential to join numerous therapeutic functions into a single platform, by targeting specific tumor cells, tissues and organs.

"In vivo" imaging is another area where tools and devices are being developed. Using nanoparticle contrast agents, images such as ultrasound and MRI have a favorable distribution and improved contrast. In cardiovascular imaging, nanoparticles have potential to aid visualization of blood pooling, ischemia, angiogenesis, atherosclerosis, and focal areas where inflammation is present.

The small size of nanoparticles endows them with properties that can be very useful in oncology, particularly in imaging. Quantum dots (nanoparticles with quantum confinement properties, such as size-tunable light emission), when used in conjunction with MRI (magnetic resonance imaging), can produce exceptional images of tumor sites. Nanoparticles of cadmium selenide (quantum dots) glow when exposed to ultraviolet light. When injected, they seep into cancer tumors. The surgeon can see the glowing tumor, and use it as a guide for more accurate tumor removal.These nanoparticles are much brighter than organic dyes and only need one light source for excitation. This means that the use of fluorescent quantum dots could produce a higher contrast image and at a lower cost than today's organic dyes used as contrast media. The downside, however, is that quantum dots are usually made of quite toxic elements, but this concern may be addressed by use of fluorescent dopants.

Tracking movement can help determine how well drugs are being distributed or how substances are metabolized. It is difficult to track a small group of cells throughout the body, so scientists used to dye the cells. These dyes needed to be excited by light of a certain wavelength in order for them to light up. While different color dyes absorb different frequencies of light, there was a need for as many light sources as cells. A way around this problem is with luminescent tags. These tags are quantum dots attached to proteins that penetrate cell membranes. The dots can be random in size, can be made of bio-inert material, and they demonstrate the nanoscale property that color is size-dependent. As a result, sizes are selected so that the frequency of light used to make a group of quantum dots fluoresce is an even multiple of the frequency required to make another group incandesce. Then both groups can be lit with a single light source. They have also found a way to insert nanoparticles into the affected parts of the body so that those parts of the body will glow showing the tumor growth or shrinkage or also organ trouble.

Nanotechnology-on-a-chip is one more dimension of lab-on-a-chip technology. Magnetic nanoparticles, bound to a suitable antibody, are used to label specific molecules, structures or microorganisms. In particular silica nanoparticles are inert from the photophysical point of view and might accumulate a large number of dye(s) within the nanoparticle shell. Gold nanoparticles tagged with short segments of DNA can be used for detection of genetic sequence in a sample. Multicolor optical coding for biological assays has been achieved by embedding different-sized quantum dots into polymeric microbeads. Nanopore technology for analysis of nucleic acids converts strings of nucleotides directly into electronic signatures.

Sensor test chips containing thousands of nanowires, able to detect proteins and other biomarkers left behind by cancer cells, could enable the detection and diagnosis of cancer in the early stages from a few drops of a patient's blood. Nanotechnology is helping to advance the use of arthroscopes, which are pencil-sized devices that are used in surgeries with lights and cameras so surgeons can do the surgeries with smaller incisions. The smaller the incisions the faster the healing time which is better for the patients. It is also helping to find a way to make an arthroscope smaller than a strand of hair.

Research on nanoelectronics-based cancer diagnostics could lead to tests that can be done in pharmacies. The results promise to be highly accurate and the product promises to be inexpensive. They could take a very small amount of blood and detect cancer anywhere in the body in about five minutes, with a sensitivity that is a thousand times better a conventional laboratory test. These devices that are built with nanowires to detect cancer proteins; each nanowire detector is primed to be sensitive to a different cancer marker. The biggest advantage of the nanowire detectors is that they could test for anywhere from ten to one hundred similar medical conditions without adding cost to the testing device. Nanotechnology has also helped to personalize oncology for the detection, diagnosis, and treatment of cancer. It is now able to be tailored to each individual’s tumor for better performance. They have found ways that they will be able to target a specific part of the body that is being affected by cancer.

Magnetic micro particles are proven research instruments for the separation of cells and proteins from complex media. The technology is available under the name Magnetic-activated cell sorting or Dynabeads among others. More recently it was shown in animal models that magnetic nanoparticles can be used for the removal of various noxious compounds including toxins, pathogens, and proteins from whole blood in an extracorporeal circuit similar to dialysis. In contrast to dialysis, which works on the principle of the size related diffusion of solutes and ultrafiltration of fluid across a semi-permeable membrane, the purification with nanoparticles allows specific targeting of substances. Additionally larger compounds which are commonly not dialyzable can be removed.

The purification process is based on functionalized iron oxide or carbon coated metal nanoparticles with ferromagnetic or superparamagnetic properties. Binding agents such as proteins, antibodies, antibiotics, or synthetic ligands are covalently linked to the particle surface. These binding agents are able to interact with target species forming an agglomerate. Applying an external magnetic field gradient allows exerting a force on the nanoparticles. Hence the particles can be separated from the bulk fluid, thereby cleaning it from the contaminants.

The small size (< 100 nm) and large surface area of functionalized nanomagnets leads to advantageous properties compared to hemoperfusion, which is a clinically used technique for the purification of blood and is based on surface adsorption. These advantages are high loading and accessible for binding agents, high selectivity towards the target compound, fast diffusion, small hydrodynamic resistance, and low dosage.

This approach offers new therapeutic possibilities for the treatment of systemic infections such as sepsis by directly removing the pathogen. It can also be used to selectively remove cytokines or endotoxins or for the dialysis of compounds which are not accessible by traditional dialysis methods. However the technology is still in a preclinical phase and first clinical trials are not expected before 2017.

Nanotechnology may be used as part of tissue engineering to help reproduce or repair or reshape damaged tissue using suitable nanomaterial-based scaffolds and growth factors. Tissue engineering if successful may replace conventional treatments like organ transplants or artificial implants. Nanoparticles such as graphene, carbon nanotubes, molybdenum disulfide and tungsten disulfide are being used as reinforcing agents to fabricate mechanically strong biodegradable polymeric nanocomposites for bone tissue engineering applications. The addition of these nanoparticles in the polymer matrix at low concentrations (~0.2 weight %) leads to significant improvements in the compressive and flexural mechanical properties of polymeric nanocomposites. Potentially, these nanocomposites may be used as a novel, mechanically strong, light weight composite as bone implants.

For example, a flesh welder was demonstrated to fuse two pieces of chicken meat into a single piece using a suspension of gold-coated nanoshells activated by an infrared laser. This could be used to weld arteries during surgery.
Another example is nanonephrology, the use of nanomedicine on the kidney.

Neuro-electronic interfacing is a visionary goal dealing with the construction of nanodevices that will permit computers to be joined and linked to the nervous system. This idea requires the building of a molecular structure that will permit control and detection of nerve impulses by an external computer. A refuelable strategy implies energy is refilled continuously or periodically with external sonic, chemical, tethered, magnetic, or biological electrical sources, while a nonrefuelable strategy implies that all power is drawn from internal energy storage which would stop when all energy is drained. A nanoscale enzymatic biofuel cell for self-powered nanodevices have been developed that uses glucose from biofluids including human blood and watermelons. One limitation to this innovation is the fact that electrical interference or leakage or overheating from power consumption is possible. The wiring of the structure is extremely difficult because they must be positioned precisely in the nervous system. The structures that will provide the interface must also be compatible with the body's immune system.

Molecular nanotechnology is a speculative subfield of nanotechnology regarding the possibility of engineering molecular assemblers, machines which could re-order matter at a molecular or atomic scale. Nanomedicine would make use of these nanorobots, introduced into the body, to repair or detect damages and infections. Molecular nanotechnology is highly theoretical, seeking to anticipate what inventions nanotechnology might yield and to propose an agenda for future inquiry. The proposed elements of molecular nanotechnology, such as molecular assemblers and nanorobots are far beyond current capabilities. Future advances in nanomedicine could give rise to life extension through the repair of many processes thought to be responsible for aging. K. Eric Drexler, one of the founders of nanotechnology, postulated cell repair machines, including ones operating within cells and utilizing as yet hypothetical molecular machines, in his 1986 book Engines of Creation, with the first technical discussion of medical nanorobots by Robert Freitas appearing in 1999. Raymond Kurzweil, a futurist and transhumanist, stated in his book "The Singularity Is Near" that he believes that advanced medical nanorobotics could completely remedy the effects of aging by 2030. According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a "medical" use for Feynman's theoretical micromachines (see nanotechnology). Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) "swallow the doctor". The idea was incorporated into Feynman's 1959 essay "There's Plenty of Room at the Bottom."


</doc>
<doc id="21518" url="https://en.wikipedia.org/wiki?curid=21518" title="NMR (disambiguation)">
NMR (disambiguation)

NMR may refer to:






</doc>
<doc id="21520" url="https://en.wikipedia.org/wiki?curid=21520" title="Null set">
Null set

In set theory, a null set formula_1 is a set that can be covered by a countable union of intervals of arbitrarily small total length. The notion of null set in set theory anticipates the development of Lebesgue measure since a null set necessarily has measure zero. More generally, on a given measure space formula_2 a null set is a set formula_3 such that formula_4.

Suppose formula_5 is a subset of the real line formula_6 such that 
where the "U" are intervals and |"U"| is the length of "U", then "A" is a null set. Also known as a set of zero-content. 

In terminology of mathematical analysis, this definition requires that there be a sequence of open covers of "A" for which the limit of the lengths of the covers is zero.

Null sets include all finite sets, all countable sets, and even some uncountable sets such as the Cantor set.

The empty set is always a null set. More generally, any countable union of null sets is null. Any measurable subset of a null set is itself a null set. Together, these facts show that the "m"-null sets of "X" form a sigma-ideal on "X". Similarly, the measurable "m"-null sets form a sigma-ideal of the sigma-algebra of measurable sets. Thus, null sets may be interpreted as negligible sets, defining a notion of almost everywhere.

The Lebesgue measure is the standard way of assigning a length, area or volume to subsets of Euclidean space.

A subset "N" of formula_6 has null Lebesgue measure and is considered to be a null set in formula_6 if and only if:
This condition can be generalised to formula_11, using "n"-cubes instead of intervals. In fact, the idea can be made to make sense on any Riemannian manifold, even if there is no Lebesgue measure there.

For instance:

if λ is Lebesgue measure for formula_6 and π is Lebesgue measure for formula_17, then the product measure formula_20. In terms of null sets, the following equivalence has been styled a Fubini's theorem: 

Null sets play a key role in the definition of the Lebesgue integral: if functions "f" and "g" are equal except on a null set, then "f" is integrable if and only if "g" is, and their integrals are equal.

A measure in which all subsets of null sets are measurable is "complete". Any non-complete measure can be completed to form a complete measure by asserting that subsets of null sets have measure zero. Lebesgue measure is an example of a complete measure; in some constructions, it is defined as the completion of a non-complete Borel measure.

The Borel measure is not complete. One simple construction is to start with the standard Cantor set "K", which is closed hence Borel measurable, and which has measure zero, and to find a subset "F" of "K" which is not Borel measurable. (Since the Lebesgue measure is complete, this "F" is of course Lebesgue measurable.)

First, we have to know that every set of positive measure contains a nonmeasurable subset. Let "f" be the Cantor function, a continuous function which is locally constant on "K", and monotonically increasing on [0, 1], with "f"(0) = 0 and "f"(1) = 1. Obviously, "f"("K") is countable, since it contains one point per component of "K". Hence "f"("K") has measure zero, so "f"("K") has measure one. We need a strictly monotonic function, so consider "g"("x") = "f"("x") + "x". Since "g"("x") is strictly monotonic and continuous, it is a homeomorphism. Furthermore, "g"("K") has measure one. Let "E" ⊂ "g"("K") be non-measurable, and let "F" = "g"("E"). Because "g" is injective, we have that "F" ⊂ "K", and so "F" is a null set. However, if it were Borel measurable, then "g"("F") would also be Borel measurable (here we use the fact that the preimage of a Borel set by a continuous function is measurable; "g"("F") = ("g")("F") is the preimage of "F" through the continuous function "h" = "g".) Therefore, "F" is a null, but non-Borel measurable set.

In a separable Banach space ("X", +), the group operation moves any subset "A" ⊂ "X" to the translates "A" + "x" for any "x" ∈ "X". When there is a probability measure μ on the σ-algebra of Borel subsets of "X", such that for all "x", μ("A" + "x") = 0, then "A" is a Haar null set.

The term refers to the null invariance of the measures of translates, associating it with the complete invariance found with Haar measure.

Some algebraic properties of topological groups have been related to the size of subsets and Haar null sets.
Haar null sets have been used to in Polish groups to show that when "A" is not a meagre set then "A""A" contains an open neighborhood of the identity element. This property is named for Hugo Steinhaus since it is the conclusion of the Steinhaus theorem.




</doc>
<doc id="21522" url="https://en.wikipedia.org/wiki?curid=21522" title="November 24">
November 24





</doc>
<doc id="21523" url="https://en.wikipedia.org/wiki?curid=21523" title="Artificial neural network">
Artificial neural network

Artificial neural networks (ANNs) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems "learn" to perform tasks by considering examples, generally without being programmed with any task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the results to identify cats in other images. They do this without any prior knowledge about cats, e.g., that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the learning material that they process.

An ANN is based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.

In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called 'edges'. Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.

The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. ANNs have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
Warren McCulloch and Walter Pitts (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.

In the late 1940s, D.O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines.

Farley and Clark (1954) first used computational machines, then called "calculators", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).

Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time.

In 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.

The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, becoming the Group Method of Data Handling.

Neural network research stagnated after machine learning research by Minsky and Papert (1969), who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second was that computers didn't have enough processing power to effectively handle the work required by large neural networks. Neural network research slowed until computers achieved far greater processing power.

Much of artificial intelligence had focused on high-level (symbolic) models that are processed by using algorithms, characterized for example by expert systems with knowledge embodied in "if-then" rules, until in the late 1980s research expanded to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a cognitive model.

A key trigger for renewed interest in neural networks and learning was Werbos's (1975) backpropagation algorithm that effectively solved the exclusive-or problem and more generally accelerated the training of multi-layer networks. Backpropagation distributed the error term back up through the layers, by modifying the weights at each node.

In the mid-1980s, parallel distributed processing became popular under the name connectionism. Rumelhart and McClelland (1986) described the use of connectionism to simulate neural processes.

Support vector machines and other, much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity. However, using neural networks transformed some domains, such as the prediction of protein structures.

In 1992, max-pooling was introduced to help with least shift invariance and tolerance to deformation to aid in 3D object recognition.
In 2010, Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants.

The vanishing gradient problem affects many-layered feedforward networks that used backpropagation and also recurrent neural networks (RNNs). As errors propagate from layer to layer, they shrink exponentially with the number of layers, impeding the tuning of neuron weights that is based on those errors, particularly affecting deep networks.

To overcome this problem, Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation. Behnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization.

Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an "ancestral pass") from the top level feature activations. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.

Earlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pre-training, while available computing power increased through the use of GPUs and distributed computing. Neural networks were deployed on a large scale, particularly in image and visual recognition problems. This became known as "deep learning".

Computational devices were created in CMOS, for both biophysical simulation and neuromorphic computing. Nanodevices for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices). Ciresan and colleagues (2010) in Schmidhuber's group showed that despite the vanishing gradient problem, GPUs makes back-propagation feasible for many-layered feedforward neural networks.

Between 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three languages to be learned.

Ciresan and colleagues won pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge and others. Their neural networks were the first pattern recognizers to achieve human-competitive or even superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem.

Researchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.

GPU-based implementations of this approach won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge, the ImageNet Competition and others.

Deep, highly nonlinear neural architectures similar to the neocognitron and the "standard architecture of vision", inspired by simple and complex cells, were pre-trained by unsupervised methods by Hinton. A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs.

As of 2011, the state of the art in deep learning feedforward networks alternated between convolutional layers and max-pooling layers, topped by several fully or sparsely connected layers followed by a final classification layer. Learning is usually done without unsupervised pre-training.

Such supervised deep learning methods were the first to achieve human-competitive performance on certain tasks.

ANNs were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes, only when invariance extended beyond shift, to all ANN-learned concepts, such as location, type (object class label), scale, lighting and others. This was realized in Developmental Networks (DNs) whose embodiments are Where-What Networks, WWN-1 (2008) through WWN-7 (2013).

An "artificial neural network" is a network of simple elements called "artificial neurons", which receive input, change their internal state ("activation") according to that input, and produce output depending on the input and activation. The "network" forms by connecting the output of certain neurons to the input of other neurons forming a directed, weighted graph. The weights as well as the functions that compute the activation can be modified by a process called "learning" which is governed by a "learning rule".

A neuron with label formula_1 receiving an input formula_2 from predecessor neurons consists of the following components:

Often the output function is simply the Identity function.

An "input neuron" has no predecessor but serves as input interface for the whole network. Similarly an "output neuron" has no successor and thus serves as output interface of the whole network.

The "network" consists of connections, each connection transferring the output of a neuron formula_13 to the input of a neuron formula_1. In this sense formula_13 is the predecessor of formula_1 and formula_1 is the successor of formula_13. Each connection is assigned a weight formula_19.

The "propagation function" computes the "input" formula_2 to the neuron formula_1 from the outputs formula_22 of predecessor neurons and typically has the form

The "learning rule" is a rule or an algorithm which modifies the parameters of the neural network, in order for a given input to the network to produce a favored output. This "learning" process typically amounts to modifying the weights and thresholds of the variables within the network.

Neural network models can be viewed as simple mathematical models defining a function formula_24 or a distribution over formula_25 or both formula_25 and formula_27. Sometimes models are intimately associated with a particular learning rule. A common use of the phrase "ANN model" is really the definition of a "class" of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity).

Mathematically, a neuron's network function formula_28 is defined as a composition of other functions formula_29, that can further be decomposed into other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between functions. A widely used type of composition is the "nonlinear weighted sum", where formula_30, where formula_31 (commonly referred to as the activation function) is some predefined function, such as the hyperbolic tangent or sigmoid function or softmax function or rectifier function. The important characteristic of the activation function is that it provides a smooth transition as input values change, i.e. a small change in input produces a small change in output. The following refers to a collection of functions formula_32 as a vector formula_33.

This figure depicts such a decomposition of formula_34, with dependencies between variables indicated by arrows. These can be interpreted in two ways.

The first view is the functional view: the input formula_35 is transformed into a 3-dimensional vector formula_36, which is then transformed into a 2-dimensional vector formula_37, which is finally transformed into formula_34. This view is most commonly encountered in the context of optimization.

The second view is the probabilistic view: the random variable formula_39 depends upon the random variable formula_40, which depends upon formula_41, which depends upon the random variable formula_25. This view is most commonly encountered in the context of graphical models.

The two views are largely equivalent. In either case, for this particular architecture, the components of individual layers are independent of each other (e.g., the components of formula_37 are independent of each other given their input formula_36). This naturally enables a degree of parallelism in the implementation.

Networks such as the previous one are commonly called feedforward, because their graph is a directed acyclic graph. Networks with cycles are commonly called recurrent. Such networks are commonly depicted in the manner shown at the top of the figure, where formula_34 is shown as being dependent upon itself. However, an implied temporal dependence is not shown.

The possibility of learning has attracted the most interest in neural networks. Given a specific "task" to solve, and a class of functions formula_46, learning means using a set of observations to find formula_47 which solves the task in some optimal sense.

This entails defining a cost function formula_48 such that, for the optimal solution formula_49, formula_50 formula_51 i.e., no solution has a cost less than the cost of the optimal solution (see mathematical optimization).

The cost function formula_52 is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost.

For applications where the solution is data dependent, the cost must necessarily be a function of the observations, otherwise the model would not relate to the data. It is frequently defined as a statistic to which only approximations can be made. As a simple example, consider the problem of finding the model formula_34, which minimizes formula_54, for data pairs formula_55 drawn from some distribution formula_56. In practical situations we would only have formula_57 samples from formula_56 and thus, for the above example, we would only minimize formula_59. Thus, the cost is minimized over a sample of the data rather than the entire distribution.

When formula_60 some form of online machine learning must be used, where the cost is reduced as each new example is seen. While online machine learning is often used when formula_56 is fixed, it is most useful in the case where the distribution changes slowly over time. In neural network methods, some form of online machine learning is frequently used for finite datasets.

While it is possible to define an ad hoc cost function, frequently a particular cost (function) is used, either because it has desirable properties (such as convexity) or because it arises naturally from a particular formulation of the problem (e.g., in a probabilistic formulation the posterior probability of the model can be used as an inverse cost). Ultimately, the cost function depends on the task.

A DNN can be discriminatively trained with the standard backpropagation algorithm. Backpropagation is a method to calculate the gradient of the loss function (produces the cost associated with a given state) with respect to the weights in an ANN.

The basics of continuous backpropagation were derived in the context of control theory by Kelley in 1960 and by Bryson in 1961, using principles of dynamic programming. In 1962, Dreyfus published a simpler derivation based only on the chain rule. Bryson and Ho described it as a multi-stage dynamic system optimization method in 1969. In 1970, Linnainmaa finally published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. This corresponds to the modern version of backpropagation which is efficient even when the networks are sparse. In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. In 1974, Werbos mentioned the possibility of applying this principle to ANNs, and in 1982, he applied Linnainmaa's AD method to neural networks in the way that is widely used today. In 1986, Rumelhart, Hinton and Williams noted that this method can generate useful internal representations of incoming data in hidden layers of neural networks. In 1993, Wan was the first to win an international pattern recognition contest through backpropagation.

The weight updates of backpropagation can be done via stochastic gradient descent using the following equation:
where, formula_63 is the learning rate, formula_64 is the cost (loss) function and formula_65 a stochastic term. The choice of the cost function depends on factors such as the learning type (supervised, unsupervised, reinforcement, etc.) and the activation function. For example, when performing supervised learning on a multiclass classification problem, common choices for the activation function and cost function are the softmax function and cross entropy function, respectively. The softmax function is defined as formula_66 where formula_67 represents the class probability (output of the unit formula_68) and formula_69 and formula_70 represent the total input to units formula_68 and formula_72 of the same level respectively. Cross entropy is defined as formula_73 where formula_74 represents the target probability for output unit formula_68 and formula_67 is the probability output for formula_68 after applying the activation function.

These can be used to output object bounding boxes in the form of a binary mask. They are also used for multi-scale regression to increase localization precision. DNN-based regression can learn features that capture geometric information in addition to serving as a good classifier. They remove the requirement to explicitly model parts and their relations. This helps to broaden the variety of objects that can be learned. The model consists of multiple layers, each of which has a rectified linear unit as its activation function for non-linear transformation. Some layers are convolutional, while others are fully connected. Every convolutional layer has an additional max pooling. The network is trained to minimize "L" error for predicting the mask ranging over the entire training set containing bounding boxes represented as masks.

Alternatives to backpropagation include Extreme Learning Machines, "No-prop" networks, training without backtracking, "weightless" networks, and non-connectionist neural networks.

The three major learning paradigms each correspond to a particular learning task. These are supervised learning, unsupervised learning and reinforcement learning.

Supervised learning uses a set of example pairs formula_78 and the aim is to find a function formula_79 in the allowed class of functions that matches the examples. In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain.

A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output, formula_80, and the target value formula_81 over all the example pairs. Minimizing this cost using gradient descent for the class of neural networks called multilayer perceptrons (MLP), produces the backpropagation algorithm for training neural networks.

Tasks that fall within the paradigm of supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). The supervised learning paradigm is also applicable to sequential data (e.g., for hand writing, speech and gesture recognition). This can be thought of as learning with a "teacher", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.

In unsupervised learning, some data formula_35 is given and the cost function to be minimized, that can be any function of the data formula_35 and the network's output, formula_34.

The cost function is dependent on the task (the model domain) and any "a priori" assumptions (the implicit properties of the model, its parameters and the observed variables).

As a trivial example, consider the model formula_85 where formula_86 is a constant and the cost formula_87. Minimizing this cost produces a value of formula_86 that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between formula_35 and formula_28, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples those quantities would be maximized rather than minimized).

Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.

In reinforcement learning, data formula_35 are usually not given, but generated by an agent's interactions with the environment. At each point in time formula_92, the agent performs an action formula_93 and the environment generates an observation formula_94 and an instantaneous cost formula_95, according to some (usually unknown) dynamics. The aim is to discover a policy for selecting actions that minimizes some measure of a long-term cost, e.g., the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated.

More formally the environment is modeled as a Markov decision process (MDP) with states formula_96 and actions formula_97 with the following probability distributions: the instantaneous cost distribution formula_98, the observation distribution formula_99 and the transition formula_100, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two then define a Markov chain (MC). The aim is to discover the policy (i.e., the MC) that minimizes the cost.

ANNs are frequently used in reinforcement learning as part of the overall algorithm. Dynamic programming was coupled with ANNs (giving neurodynamic programming) by Bertsekas and Tsitsiklis and applied to multi-dimensional nonlinear problems such as those involved in vehicle routing, natural resources management or medicine because of the ability of ANNs to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems.

Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.

This is a learning method specially designed for cerebellar model articulation controller (CMAC) neural networks. In 2004 a recursive least squares algorithm was introduced to train CMAC neural network online. This algorithm can converge in one step and update all weights in one step with any new input data. Initially, this algorithm had computational complexity of "O"("N"). Based on QR decomposition, this recursive learning algorithm was simplified to be "O"("N").

Training a neural network model essentially means selecting one model from the set of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost. Numerous algorithms are available for training neural network models; most of them can be viewed as a straightforward application of optimization theory and statistical estimation.

Most employ some form of gradient descent, using backpropagation to compute the actual gradients. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction. Backpropagation training algorithms fall into three categories: 

Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other methods for training neural networks.

The Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov-Gabor polynomials that permit additions and multiplications. It used a deep feedforward multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size and depth of the resulting network depends on the task.

A convolutional neural network (CNN) is a class of deep, feed-forward networks, composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular, max-pooling is often structured via Fukushima's convolutional architecture. This architecture allows CNNs to take advantage of the 2D structure of input data.

CNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular, deep, feed-forward neural networks and have many fewer parameters to estimate. Examples of applications in computer vision include DeepDream and robot navigation.

Long short-term memory (LSTM) networks are RNNs that avoid the vanishing gradient problem. LSTM is normally augmented by recurrent gates called forget gates. LSTM networks prevent backpropagated errors from vanishing or exploding. Instead errors can flow backwards through unlimited numbers of virtual layers in space-unfolded LSTM. That is, LSTM can learn "very deep learning" tasks that require memories of events that happened thousands or even millions of discrete time steps ago. Problem-specific LSTM-like topologies can be evolved. LSTM can handle long delays and signals that have a mix of low and high frequency components.

Stacks of LSTM RNNs trained by Connectionist Temporal Classification (CTC) can find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.

In 2003, LSTM started to become competitive with traditional speech recognizers. In 2007, the combination with CTC achieved first good results on speech data. In 2009, a CTC-trained LSTM was the first RNN to win pattern recognition contests, when it won several competitions in connected handwriting recognition. In 2014, Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark, without traditional speech processing methods. LSTM also improved large-vocabulary speech recognition, text-to-speech synthesis, for Google Android, and photo-real talking heads. In 2015, Google's speech recognition experienced a 49% improvement through CTC-trained LSTM.

LSTM became popular in Natural Language Processing. Unlike previous models based on HMMs and similar concepts, LSTM can learn to recognise context-sensitive languages. LSTM improved machine translation, language modeling and multilingual language processing. LSTM combined with CNNs improved automatic image captioning.

Deep Reservoir Computing and Deep Echo State Networks (deepESNs) provide a framework for efficiently trained models for hierarchical processing of temporal data, while enabling the investigation of the inherent role of RNN layered composition.

A deep belief network (DBN) is a probabilistic, generative model made up of multiple layers of hidden units. It can be considered a composition of simple learning modules that make up each layer.

A DBN can be used to generatively pre-train a DNN by using the learned DBN weights as the initial DNN weights. Backpropagation or other discriminative algorithms can then tune these weights. This is particularly helpful when training data are limited, because poorly initialized weights can significantly hinder model performance. These pre-trained weights are in a region of the weight space that is closer to the optimal weights than were they randomly chosen. This allows for both improved modeling and faster convergence of the fine-tuning phase.

Large memory storage and retrieval neural networks (LAMSTAR) are fast deep learning neural networks of many layers that can use many filters simultaneously. These filters may be nonlinear, stochastic, logic, non-stationary, or even non-analytical. They are biologically motivated and learn continuously.

A LAMSTAR neural network may serve as a dynamic neural network in spatial or time domains or both. Its speed is provided by Hebbian link-weights that integrate the various and usually different filters (preprocessing functions) into its many layers and to dynamically rank the significance of the various layers and functions relative to a given learning task. This grossly imitates biological learning which integrates various preprocessors (cochlea, retina, "etc.") and cortexes (auditory, visual, "etc.") and their various regions. Its deep learning capability is further enhanced by using inhibition, correlation and its ability to cope with incomplete data, or "lost" neurons or layers even amidst a task. It is fully transparent due to its link weights. The link-weights allow dynamic determination of innovation and redundancy, and facilitate the ranking of layers, of filters or of individual neurons relative to a task.

LAMSTAR has been applied to many domains, including medical and financial predictions, adaptive filtering of noisy speech in unknown noise, still-image recognition, video image recognition, software security and adaptive control of non-linear systems. LAMSTAR had a much faster learning speed and somewhat lower error rate than a CNN based on ReLU-function filters and max pooling, in 20 comparative studies.

These applications demonstrate delving into aspects of the data that are hidden from shallow learning networks and the human senses, such as in the cases of predicting onset of sleep apnea events, of an electrocardiogram of a fetus as recorded from skin-surface electrodes placed on the mother's abdomen early in pregnancy, of financial prediction or in blind filtering of noisy speech.

LAMSTAR was proposed in 1996 () and was further developed Graupe and Kordylewski from 1997–2002. A modified version, known as LAMSTAR 2, was developed by Schneider and Graupe in 2008.

The auto encoder idea is motivated by the concept of a "good" representation. For example, for a classifier, a good representation can be defined as one that yields a better-performing classifier.

An "encoder" is a deterministic mapping formula_101 that transforms an input vector x into hidden representation y, where formula_102, formula_103 is the weight matrix and b is an offset vector (bias). A "decoder" maps back the hidden representation y to the reconstructed input z via formula_104. The whole process of auto encoding is to compare this reconstructed input to the original and try to minimize the error to make the reconstructed value as close as possible to the original.

In "stacked denoising auto encoders", the partially corrupted output is cleaned (de-noised). This idea was introduced in 2010 by Vincent et al. with a specific approach to "good" representation, a "good representation" is one that can be obtained robustly from a corrupted input and that will be useful for recovering the corresponding clean input"." Implicit in this definition are the following ideas:
The algorithm starts by a stochastic mapping of formula_105 to formula_106 through formula_107, this is the corrupting step. Then the corrupted input formula_106 passes through a basic auto-encoder process and is mapped to a hidden representation formula_109. From this hidden representation, we can reconstruct formula_110. In the last stage, a minimization algorithm runs in order to have z as close as possible to uncorrupted input formula_105. The reconstruction error formula_112 might be either the cross-entropy loss with an affine-sigmoid decoder, or the squared error loss with an affine decoder.

In order to make a deep architecture, auto encoders stack. Once the encoding function formula_101 of the first denoising auto encoder is learned and used to uncorrupt the input (corrupted input), the second level can be trained.

Once the stacked auto encoder is trained, its output can be used as the input to a supervised learning algorithm such as support vector machine classifier or a multi-class logistic regression.

A deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Dong. It formulates the learning as a convex optimization problem with a closed-form solution, emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.

Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units, and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. The matrix of hidden units is formula_114. Modules are trained in order, so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class "y", and its estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem:
which has a closed-form solution.

Unlike other deep architectures, such as DBNs, the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward, as a batch-mode optimization problem. In purely discriminative tasks, DSNs perform better than conventional DBNs.

This architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics, and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer. TDSNs use covariance statistics in a bilinear mapping from each of two distinct sets of hidden units in the same layer to predictions, via a third-order tensor.

While parallelization and scalability are not considered seriously in conventional , all learning for s and s is done in batch mode, to allow parallelization. Parallelization allows scaling the design to larger (deeper) architectures and data sets.

The basic architecture is suitable for diverse tasks such as classification and regression.

The need for deep learning with real-valued inputs, as in Gaussian restricted Boltzmann machines, led to the "spike-and-slab" RBM ("ss"RBM), which models continuous-valued inputs with strictly binary latent variables. Similar to basic RBMs and its variants, a spike-and-slab RBM is a bipartite graph, while like GRBMs, the visible units (input) are real-valued. The difference is in the hidden layer, where each hidden unit has a binary spike variable and a real-valued slab variable. A spike is a discrete probability mass at zero, while a slab is a density over continuous domain; their mixture forms a prior.

An extension of ssRBM called µ-ssRBM provides extra modeling capacity using additional terms in the energy function. One of these terms enables the model to form a conditional distribution of the spike variables by marginalizing out the slab variables given an observation.

Compound hierarchical-deep models compose deep networks with non-parametric Bayesian models. Features can be learned using deep architectures such as DBNs, DBMs, deep auto encoders, convolutional variants, ssRBMs, deep coding networks, DBNs with sparse feature learning, RNNs, conditional DBNs, de-noising auto encoders. This provides a better representation, allowing faster learning and more accurate classification with high-dimensional data. However, these architectures are poor at learning novel classes with few examples, because all network units are involved in representing the input (a ) and must be adjusted together (high degree of freedom). Limiting the degree of freedom reduces the number of parameters to learn, facilitating learning of new classes from few examples. "Hierarchical Bayesian (HB)" models allow learning from few examples, for example for computer vision, statistics and cognitive science.

Compound HD architectures aim to integrate characteristics of both HB and deep networks. The compound HDP-DBM architecture is a "hierarchical Dirichlet process (HDP)" as a hierarchical model, incorporated with DBM architecture. It is a full generative model, generalized from abstract concepts flowing through the layers of the model, which is able to synthesize new examples in novel classes that look "reasonably" natural. All the levels are learned jointly by maximizing a joint log-probability score.

In a DBM with three hidden layers, the probability of a visible input is:
where formula_117 is the set of hidden units, and formula_118 are the model parameters, representing visible-hidden and hidden-hidden symmetric interaction terms.

A learned DBM model is an undirected model that defines the joint distribution formula_119. One way to express what has been learned is the conditional model formula_120 and a prior term formula_121.

Here formula_120 represents a conditional DBM model, which can be viewed as a two-layer DBM but with bias terms given by the states of formula_123:

A deep predictive coding network (DPCN) is a predictive coding scheme that uses top-down information to empirically adjust the priors needed for a bottom-up inference procedure by means of a deep, locally connected, generative model. This works by extracting sparse features from time-varying observations using a linear dynamical model. Then, a pooling strategy is used to learn invariant feature representations. These units compose to form a deep architecture and are trained by greedy layer-wise unsupervised learning. The layers constitute a kind of Markov chain such that the states at any layer depend only on the preceding and succeeding layers.

DPCNs predict the representation of the layer, by using a top-down approach using the information in upper layer and temporal dependencies from previous states.

DPCNs can be extended to form a convolutional network.

Integrating external memory with ANNs dates to early research in distributed representations and Kohonen's self-organizing maps. For example, in sparse distributed memory or hierarchical temporal memory, the patterns encoded by neural networks are used as addresses for content-addressable memory, with "neurons" essentially serving as address encoders and decoders. However, the early controllers of such memories were not differentiable.

Apart from long short-term memory (LSTM), other approaches also added differentiable memory to recurrent functions. For example:

Neural Turing machines couple LSTM networks to external memory resources, with which they can interact by attentional processes. The combined system is analogous to a Turing machine but is differentiable end-to-end, allowing it to be efficiently trained by gradient descent. Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying, sorting and associative recall from input and output examples.

Differentiable neural computers (DNC) are an NTM extension. They out-performed Neural turing machines, long short-term memory systems and memory networks on sequence-processing tasks.

Approaches that represent previous experiences directly and use a similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods. Deep learning is useful in semantic hashing where a deep graphical model the word-count vectors obtained from a large set of documents. Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document. Unlike sparse distributed memory that operates on 1000-bit addresses, semantic hashing works on 32 or 64-bit addresses found in a conventional computer architecture.

Memory networks are another extension to neural networks incorporating long-term memory. The long-term memory can be read and written to, with the goal of using it for prediction. These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base and the output is a textual response.

Deep neural networks can be potentially improved by deepening and parameter reduction, while maintaining trainability. While training extremely deep (e.g., 1 million layers) neural networks might not be practical, CPU-like architectures such as pointer networks and neural random-access machines overcome this limitation by using external random-access memory and other components that typically belong to a computer architecture such as registers, ALU and pointers. Such systems operate on probability distribution vectors stored in memory cells and registers. Thus, the model is fully differentiable and trains end-to-end. The key characteristic of these models is that their depth, the size of their short-term memory, and the number of parameters can be altered independently – unlike models like LSTM, whose number of parameters grows quadratically with memory size.

Encoder–decoder frameworks are based on neural networks that map highly structured input to highly structured output. The approach arose in the context of machine translation, where the input and output are written sentences in two natural languages. In that work, an LSTM RNN or CNN was used as an encoder to summarize a source sentence, and the summary was decoded using a conditional RNN language model to produce the translation. These systems share building blocks: gated RNNs and CNNs and trained attention mechanisms.

Multilayer kernel machines (MKM) are a way of learning highly nonlinear functions by iterative application of weakly nonlinear kernels. They use the kernel principal component analysis (KPCA), as a method for the unsupervised greedy layer-wise pre-training step of deep learning.

Layer formula_125 learns the representation of the previous layer formula_126, extracting the formula_127 principal component (PC) of the projection layer formula_126 output in the feature domain induced by the kernel. For the sake of dimensionality reduction of the updated representation in each layer, a supervised strategy selects the best informative features among features extracted by KPCA. The process is:
Some drawbacks accompany the KPCA method as the building cells of an MKM.

A more straightforward way to use kernel machines for deep learning was developed for spoken language understanding. The main idea is to use a kernel machine to approximate a shallow neural net with an infinite number of hidden units, then use stacking to splice the output of the kernel machine and the raw input in building the next, higher level of the kernel machine. The number of levels in the deep convex network is a hyper-parameter of the overall system, to be determined by cross validation.

Neural architecture search (NAS) uses machine learning to automate the design of ANNs. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset and use the results as feedback to teach the NAS network.

Using ANNs requires an understanding of their characteristics.
ANN capabilities fall within the following broad categories:


Because of their ability to reproduce and model nonlinear processes, ANNs have found many applications in a wide range of disciplines.

Application areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, game-playing and decision making (backgammon, chess, poker), pattern recognition (radar systems, face identification, signal classification, object recognition and more), sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g. automated trading systems), data mining, visualization, machine translation, social network filtering and e-mail spam filtering.

ANNs have been used to diagnose cancers, including lung cancer, prostate cancer, colorectal cancer and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.

ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters.

ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology, are just few examples of this kind.

Many types of models are used, defined at different levels of abstraction and modeling different aspects of neural systems. They range from models of the short-term behavior of individual neurons, models of how the dynamics of neural circuitry arise from interactions between individual neurons and finally to models of how behavior can arise from abstract neural modules that represent complete subsystems. These include models of the long-term, and short-term plasticity, of neural systems and their relations to learning and memory from the individual neuron to the system level.

The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.

A specific recurrent architecture with rational valued weights (as opposed to full precision real number-valued weights) has the full power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.

Models' "capacity" property roughly corresponds to their ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.

Models may not consistently converge on a single solution, firstly because many local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical. However, for CMAC neural network, a recursive least squares algorithm was introduced to train it, and this algorithm can be guaranteed to converge in one step.

Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the capacity of the network significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and optimally select hyperparameters to minimize the generalization error. The second is to use some form of "regularization". This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.
Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of the output of the network, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.

By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based neural network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is very useful in classification as it gives a certainty measure on classifications.

The softmax activation function is:

<section end="theory" />

A common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example and by grouping examples in so-called mini-batches. Improving the training efficiency and convergence capability has always been an ongoing research area for neural network. For example, by introducing a recursive least squares algorithm for CMAC neural network, the training process only takes one step to converge.

No neural network has solved computationally difficult problems such as the n-Queens problem, the travelling salesman problem, or the problem of factoring large integers.

A fundamental objection is that they do not reflect how real neurons function. Back propagation is a critical part of most artificial neural networks, although no such mechanism exists in biological neural networks. How information is coded by real neurons is not known. Sensor neurons fire action potentials more frequently with sensor activation and muscle cells pull more strongly when their associated motor neurons receive action potentials more frequently. Other than the case of relaying information from a sensor neuron to a motor neuron, almost nothing of the principles of how information is handled by biological neural networks is known.

The motivation behind ANNs is not necessarily to strictly replicate neural function, but to use biological neural networks as an inspiration. A central claim of ANNs is therefore that it embodies some new and powerful general principle for processing information. Unfortunately, these general principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. Alexander Dewdney commented that, as a result, artificial neural networks have a "something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything".

Biological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.

Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may compel a neural network designer to fill many millions of database rows for its connections which can consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons which must often be matched with enormous CPU processing power and time.

Schmidhuber notes that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of parallel GPUs can reduce training times from months to days.

Neuromorphic engineering addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.

Arguments against Dewdney's position are that neural networks have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.

Technology writer Roger Bridgman commented:

Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be "an opaque, unreadable table...valueless as a scientific resource".
In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.
Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs non-local learning and shallow vs deep architecture.

Advocates of hybrid models (combining neural networks and symbolic approaches), claim that such a mixture can better capture the mechanisms of the human mind.

Artificial neural networks have many variations. The simplest, static types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to change during the learning process. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be "supervised" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.



</doc>
<doc id="21525" url="https://en.wikipedia.org/wiki?curid=21525" title="Nutrition">
Nutrition

Nutrition is the science that interprets the interaction of nutrients and other substances in food in relation to maintenance, growth, reproduction, health and disease of an organism. It includes food intake, absorption, assimilation, biosynthesis, catabolism, and excretion.

The diet of an organism is what it eats, which is largely determined by the availability and palatability of foods. For humans, a healthy diet includes preparation of food and storage methods that preserve nutrients from oxidation, heat or leaching, and that reduce risk of foodborne illnesses.

In humans, an unhealthy diet can cause deficiency-related diseases such as blindness, anemia, scurvy, preterm birth, stillbirth and cretinism, or nutrient excess health-threatening conditions such as obesity and metabolic syndrome; and such common chronic systemic diseases as cardiovascular disease, diabetes, and osteoporosis. Undernutrition can lead to wasting in acute cases, and the stunting of marasmus in chronic cases of malnutrition.

The first recorded dietary advice, carved into a Babylonian stone tablet in about 2500 BC, cautioned those with pain inside to avoid eating onions for three days. Scurvy, later found to be a vitamin C deficiency, was first described in 1500 BC in the Ebers Papyrus.

According to Walter Gratzer, the study of nutrition probably began during the 6th century BC. In China, the concept of "qi" developed, a spirit or "wind" similar to what Western Europeans later called "pneuma". Food was classified into "hot" (for example, meats, blood, ginger, and hot spices) and "cold" (green vegetables) in China, India, Malaya, and Persia. "Humours" developed perhaps first in China alongside "qi". Ho the Physician concluded that diseases are caused by deficiencies of elements (Wu Xing: fire, water, earth, wood, and metal), and he classified diseases as well as prescribed diets. About the same time in Italy, Alcmaeon of Croton (a Greek) wrote of the importance of equilibrium between what goes in and what goes out, and warned that imbalance would result in disease marked by obesity or emaciation.

The first recorded nutritional experiment with human subjects is found in the Bible's Book of Daniel. Daniel and his friends were captured by the king of Babylon during an invasion of Israel. Selected as court servants, they were to share in the king's fine foods and wine. But they objected, preferring vegetables (pulses) and water in accordance with their Jewish dietary restrictions. The king's chief steward reluctantly agreed to a trial. Daniel and his friends received their diet for ten days and were then compared to the king's men. Appearing healthier, they were allowed to continue with their diet.

Around 475 BC, Anaxagoras stated that food is absorbed by the human body and, therefore, contains "homeomerics" (generative components), suggesting the existence of nutrients. Around 400 BC, Hippocrates, who recognized and was concerned with obesity, which may have been common in southern Europe at the time, said, "Let food be your medicine and medicine be your food." The works that are still attributed to him, "Corpus Hippocraticum", called for moderation and emphasized exercise.
Salt, pepper and other spices were prescribed for various ailments in various preparations for example mixed with vinegar. In the 2nd century BC, Cato the Elder believed that cabbage (or the urine of cabbage-eaters) could cure digestive diseases, ulcers, warts, and intoxication. Living about the turn of the millennium, Aulus Celsus, an ancient Roman doctor, believed in "strong" and "weak" foods (bread for example was strong, as were older animals and vegetables).

One mustn't overlook the doctrines of Galen: In use from his life in the 1st century AD until the 17th century, it was heresy to disagree with him for 1500 years. Galen was physician to gladiators in Pergamon, and in Rome, physician to Marcus Aurelius and the three emperors who succeeded him. Most of Galen's teachings were gathered and enhanced in the late 11th century by Benedictine monks at the School of Salerno in "Regimen sanitatis Salernitanum", which still had users in the 17th century. Galen believed in the bodily "humours" of Hippocrates, and he taught that "pneuma" is the source of life. Four elements (earth, air, fire and water) combine into "complexion", which combines into states (the four temperaments: sanguine, phlegmatic, choleric, and melancholic). The states are made up of pairs of attributes (hot and moist, cold and moist, hot and dry, and cold and dry), which are made of four humours: blood, phlegm, green (or yellow) bile, and black bile (the bodily form of the elements). Galen thought that for a person to have gout, kidney stones, or arthritis was scandalous, which Gratzer likens to Samuel Butler's "Erehwon" (1872) where sickness is a crime.

In the 1500s, Paracelsus was probably the first to criticize Galen publicly. Also in the 16th century, scientist and artist Leonardo da Vinci compared metabolism to a burning candle. Leonardo did not publish his works on this subject, but he was not afraid of thinking for himself and he definitely disagreed with Galen. Ultimately, 16th century works of Andreas Vesalius, sometimes called the father of modern human anatomy, overturned Galen's ideas. He was followed by piercing thought amalgamated with the era's mysticism and religion sometimes fueled by the mechanics of Newton and Galileo. Jan Baptist van Helmont, who discovered several gases such as carbon dioxide, performed the first quantitative experiment. Robert Boyle advanced chemistry. Sanctorius measured body weight. Physician Herman Boerhaave modeled the digestive process. Physiologist Albrecht von Haller worked out the difference between nerves and muscles.

Sometimes forgotten during his life, James Lind, a physician in the British navy, performed the first scientific nutrition experiment in 1747. Lind discovered that lime juice saved sailors that had been at sea for years from scurvy, a deadly and painful bleeding disorder. Between 1500 and 1800, an estimated two million sailors had died of scurvy. The discovery was ignored for forty years, after which British sailors became known as "limeys." The essential vitamin C within citrus fruits would not be identified by scientists until 1932.

Around 1770, Antoine Lavoisier discovered the details of metabolism, demonstrating that the oxidation of food is the source of body heat. Called the most fundamental chemical discovery of the 18th century, Lavoisier discovered the principle of conservation of mass. His ideas made the phlogiston theory of combustion obsolete.

In 1790, George Fordyce recognized calcium as necessary for the survival of fowl. In the early 19th century, the elements carbon, nitrogen, hydrogen, and oxygen were recognized as the primary components of food, and methods to measure their proportions were developed.

In 1816, François Magendie discovered that dogs fed only carbohydrates (sugar), fat (olive oil), and water died evidently of starvation, but dogs also fed protein survived, identifying protein as an essential dietary component. William Prout in 1827 was the first person to divide foods into carbohydrates, fat, and protein. During the 19th century, Jean-Baptiste Dumas and Justus von Liebig quarrelled over their shared belief that animals get their protein directly from plants (animal and plant protein are the same and that humans do not create organic compounds). With a reputation as the leading organic chemist of his day but with no credentials in animal physiology, Liebig grew rich making food extracts like beef bouillon and infant formula that were later found to be of questionable nutritious value. In the 1860s, Claude Bernard discovered that body fat can be synthesized from carbohydrate and protein, showing that the energy in blood glucose can be stored as fat or as glycogen.
In the early 1880s, Kanehiro Takaki observed that Japanese sailors (whose diets consisted almost entirely of white rice) developed beriberi (or endemic neuritis, a disease causing heart problems and paralysis), but British sailors and Japanese naval officers did not. Adding various types of vegetables and meats to the diets of Japanese sailors prevented the disease, (not because of the increased protein as Takaki supposed but because it introduced a few parts per million of thiamine to the diet, later understood as a cure).

In 1896, Eugen Baumann observed iodine in thyroid glands. In 1897, Christiaan Eijkman worked with natives of Java, who also suffered from beriberi. Eijkman observed that chickens fed the native diet of white rice developed the symptoms of beriberi but remained healthy when fed unprocessed brown rice with the outer bran intact. His assistant, Gerrit Grijns correctly identified and described the anti-beriberi substance in rice. Eijkman cured the natives by feeding them brown rice, discovering that food can cure disease. Over two decades later, nutritionists learned that the outer rice bran contains vitamin B1, also known as thiamine.

In the early 20th century, Carl von Voit and Max Rubner independently measured caloric energy expenditure in different species of animals, applying principles of physics in nutrition. In 1906, Edith G. Willcock and Frederick Hopkins showed that the amino acid tryptophan aids the well-being of mice but it did not assure their growth. In the middle of twelve years of attempts to isolate them, Hopkins said in a 1906 lecture that "unsuspected dietetic factors," other than calories, protein, and minerals, are needed to prevent deficiency diseases. In 1907, Stephen M. Babcock and Edwin B. Hart started the cow feeding, single-grain experiment, which took nearly four years to complete.
In 1912, Casimir Funk coined the term vitamin, a vital factor in the diet, from the words "vital" and "amine," because these unknown substances preventing scurvy, beriberi, and pellagra, were thought then to be derived from ammonia. The vitamins were studied in the first half of the 20th century.

In 1913, Elmer McCollum and Marguerite Davis discovered the first vitamin, fat-soluble vitamin A, then water-soluble vitamin B (in 1915; now known to be a complex of several water-soluble vitamins) and named vitamin C as the then-unknown substance preventing scurvy. Lafayette Mendel and Thomas Osborne also performed pioneering work on vitamins A and B. In 1919, Sir Edward Mellanby incorrectly identified rickets as a vitamin A deficiency because he could cure it in dogs with cod liver oil. In 1922, McCollum destroyed the vitamin A in cod liver oil, but found that it still cured rickets. Also in 1922, H.M. Evans and L.S. Bishop discover vitamin E as essential for rat pregnancy, originally calling it "food factor X" until 1925.

In 1925, Hart discovered that trace amounts of copper are necessary for iron absorption. In 1927, Adolf Otto Reinhold Windaus synthesized vitamin D, and was awarded the Nobel Prize in Chemistry in 1928. In 1928, Albert Szent-Györgyi isolated ascorbic acid, and in 1932 proved that it is vitamin C by preventing scurvy. In 1935, he synthesized it, and in 1937, he won a Nobel Prize for his efforts. Szent-Györgyi concurrently elucidated much of the citric acid cycle.

In the 1930s, William Cumming Rose identified essential amino acids, necessary protein components that the body cannot synthesize. In 1935, Underwood and Marston independently discovered the necessity of cobalt. In 1936, Eugene Floyd DuBois showed that work and school performance are related to caloric intake. In 1938, Erhard Fernholz discovered the chemical structure of vitamin E and then he tragically disappeared. It was synthesised the same year by Paul Karrer.

In 1940, rationing in the United Kingdom during and after World War II took place according to nutritional principles drawn up by Elsie Widdowson and others. In 1941, the first Recommended Dietary Allowances (RDAs) were established by the National Research Council.

In 1992, The U.S. Department of Agriculture introduced the Food Guide Pyramid. This replaced the Four Food Groups (1956-1992) and was superseded by the concept of MyPlate (2011–present).

The list of nutrients that people are known to require is, in the words of Marion Nestle, "almost certainly incomplete". As of 2014, nutrients are thought to be of two types: macro-nutrients which are needed in relatively large amounts, and micronutrients which are needed in smaller quantities. A type of carbohydrate, dietary fiber, i.e. non-digestible material such as cellulose, is required, for both mechanical and biochemical reasons, although the exact reasons remain unclear. Some nutrients can be stored - the fat-soluble vitamins - while others are required more or less continuously. Poor health can be caused by a lack of required nutrients, or for some vitamins and minerals, too much of a required nutrient.

The macronutrients are carbohydrates, fiber, fats, protein, and water.
The macronutrients (excluding fiber and water) provide structural material (amino acids from which proteins are built, and lipids from which cell membranes and some signaling molecules are built) and energy. Some of the structural material can be used to generate energy internally, and in either case it is measured in Joules or kilocalories (often called "Calories" and written with a capital "C" to distinguish them from little 'c' calories). Carbohydrates and proteins provide 17 kJ approximately (4 kcal) of energy per gram, while fats provide 37 kJ (9 kcal) per gram, though the net energy from either depends on such factors as absorption and digestive effort, which vary substantially from instance to instance. Vitamins, minerals, fiber, and water do not provide energy, but are required for other reasons.

Molecules of carbohydrates and fats consist of carbon, hydrogen, and oxygen atoms. Carbohydrates range from simple monosaccharides (glucose, fructose and galactose) to complex polysaccharides (starch). Fats are triglycerides, made of assorted fatty acid monomers bound to a glycerol backbone. Some fatty acids, but not all, are essential in the diet: they cannot be synthesized in the body. Protein molecules contain nitrogen atoms in addition to carbon, oxygen, and hydrogen. The fundamental components of protein are nitrogen-containing amino acids, some of which are essential in the sense that humans cannot make them internally. Some of the amino acids are convertible (with the expenditure of energy) to glucose and can be used for energy production, just as ordinary glucose, in a process known as gluconeogenesis. By breaking down existing protein, the carbon skeleton of the various amino acids can be metabolized to intermediates in cellular respiration; the remaining ammonia is discarded primarily as urea in urine.

Carbohydrates may be classified as monosaccharides, disaccharides, or polysaccharides depending on the number of monomer (sugar) units they contain. They constitute a large part of foods such as rice, noodles, bread, and other grain-based products, also potatoes , yams, beans, fruits, fruit juices and vegetables.
Monosaccharides, disaccharides, and polysaccharides contain one, two, and three or more sugar units, respectively. Polysaccharides are often referred to as "complex" carbohydrates because they are typically long, multiple branched chains of sugar units.

Traditionally, simple carbohydrates are believed to be absorbed quickly, and therefore to raise blood-glucose levels more rapidly than complex carbohydrates. This, however, is not accurate. Some simple carbohydrates (e.g., fructose) follow different metabolic pathways (e.g., fructolysis) that result in only a partial catabolism to glucose, while, in essence, many complex carbohydrates may be digested at the same rate as simple carbohydrates. The World Health Organization (WHO) recommends that added sugars should represent no more than 10% of total energy intake.

Dietary fiber is a carbohydrate that is incompletely absorbed in humans and in some animals. Like all carbohydrates, when it is metabolized it can produce four Calories (kilocalories) of energy per gram. However, in most circumstances it accounts for less than that because of its limited absorption and digestibility. Dietary fiber consists mainly of cellulose, a large carbohydrate polymer which is indigestible as humans do not have the required enzymes to disassemble it. There are two subcategories: soluble and insoluble fiber. Whole grains, fruits (especially plums, prunes, and figs), and vegetables are good sources of dietary fiber. There are many health benefits of a high-fiber diet. Dietary fiber helps reduce the chance of gastrointestinal problems such as constipation and diarrhea by increasing the weight and size of stool and softening it. Insoluble fiber, found in whole wheat flour, nuts and vegetables, especially stimulates peristalsis;– the rhythmic muscular contractions of the intestines, which move digest along the digestive tract. Soluble fiber, found in oats, peas, beans, and many fruits, dissolves in water in the intestinal tract to produce a gel that slows the movement of food through the intestines. This may help lower blood glucose levels because it can slow the absorption of sugar. Additionally, fiber, perhaps especially that from whole grains, is thought to possibly help lessen insulin spikes, and therefore reduce the risk of type 2 diabetes. The link between increased fiber consumption and a decreased risk of colorectal cancer is still uncertain.

A molecule of dietary fat typically consists of several fatty acids (containing long chains of carbon and hydrogen atoms), bonded to a glycerol. They are typically found as triglycerides (three fatty acids attached to one glycerol backbone). Fats may be classified as saturated or unsaturated depending on the detailed structure of the fatty acids involved. Saturated fats have all of the carbon atoms in their fatty acid chains bonded to hydrogen atoms, whereas unsaturated fats have some of these carbon atoms double-bonded, so their molecules have relatively fewer hydrogen atoms than a saturated fatty acid of the same length. Unsaturated fats may be further classified as monounsaturated (one double-bond) or polyunsaturated (many double-bonds). Furthermore, depending on the location of the double-bond in the fatty acid chain, unsaturated fatty acids are classified as omega-3 or omega-6 fatty acids. Trans fats are a type of unsaturated fat with "trans"-isomer bonds; these are rare in nature and in foods from natural sources; they are typically created in an industrial process called (partial) hydrogenation. There are nine kilocalories in each gram of fat. Fatty acids such as conjugated linoleic acid, catalpic acid, eleostearic acid and punicic acid, in addition to providing energy, represent potent immune modulatory molecules.

Saturated fats (typically from animal sources) have been a staple in many world cultures for millennia. Unsaturated fats (e. g., vegetable oil) are considered healthier, while trans fats are to be avoided. Saturated and some trans fats are typically solid at room temperature (such as butter or lard), while unsaturated fats are typically liquids (such as olive oil or flaxseed oil). Trans fats are very rare in nature, and have been shown to be highly detrimental to human health, but have properties useful in the food processing industry, such as rancidity resistance.

Most fatty acids are non-essential, meaning the body can produce them as needed, generally from other fatty acids and always by expending energy to do so. However, in humans, at least two fatty acids are essential and must be included in the diet. An appropriate balance of essential fatty acids—omega-3 and omega-6 fatty acids—seems also important for health, although definitive experimental demonstration has been elusive. Both of these "omega" long-chain polyunsaturated fatty acids are substrates for a class of eicosanoids known as prostaglandins, which have roles throughout the human body. They are hormones, in some respects. The omega-3 eicosapentaenoic acid (EPA), which can be made in the human body from the omega-3 essential fatty acid alpha-linolenic acid (ALA), or taken in through marine food sources, serves as a building block for series 3 prostaglandins (e.g., weakly inflammatory PGE3). The omega-6 dihomo-gamma-linolenic acid (DGLA) serves as a building block for series 1 prostaglandins (e.g. anti-inflammatory PGE1), whereas arachidonic acid (AA) serves as a building block for series 2 prostaglandins (e.g. pro-inflammatory PGE 2). Both DGLA and AA can be made from the omega-6 linoleic acid (LA) in the human body, or can be taken in directly through food. An appropriately balanced intake of omega-3 and omega-6 partly determines the relative production of different prostaglandins, which is one reason why a balance between omega-3 and omega-6 is believed important for cardiovascular health. In industrialized societies, people typically consume large amounts of processed vegetable oils, which have reduced amounts of the essential fatty acids along with too much of omega-6 fatty acids relative to omega-3 fatty acids.

The conversion rate of omega-6 DGLA to AA largely determines the production of the prostaglandins PGE1 and PGE2. Omega-3 EPA prevents AA from being released from membranes, thereby skewing prostaglandin balance away from pro-inflammatory PGE2 (made from AA) toward anti-inflammatory PGE1 (made from DGLA). Moreover, the conversion (desaturation) of DGLA to AA is controlled by the enzyme delta-5-desaturase, which in turn is controlled by hormones such as insulin (up-regulation) and glucagon (down-regulation). The amount and type of carbohydrates consumed, along with some types of amino acid, can influence processes involving insulin, glucagon, and other hormones; therefore, the ratio of omega-3 versus omega-6 has wide effects on general health, and specific effects on immune function and inflammation, and mitosis (i.e., cell division).

Proteins are structural materials in much of the animal body (e.g. muscles, skin, and hair). They also form the enzymes that control chemical reactions throughout the body. Each protein molecule is composed of amino acids, which are characterized by inclusion of nitrogen and sometimes sulphur (these components are responsible for the distinctive smell of burning protein, such as the keratin in hair). The body requires amino acids to produce new proteins (protein retention) and to replace damaged proteins (maintenance). As there is no protein or amino acid storage provision, amino acids must be present in the diet. Excess amino acids are discarded, typically in the urine. For all animals, some amino acids are "essential" (an animal cannot produce them internally) and some are "non-essential" (the animal can produce them from other nitrogen-containing compounds). About twenty amino acids are found in the human body, and about ten of these are essential and, therefore, must be included in the diet. A diet that contains adequate amounts of amino acids (especially those that are essential) is particularly important in some situations: during early development and maturation, pregnancy, lactation, or injury (a burn, for instance). A "complete" protein source contains all the essential amino acids; an "incomplete" protein source lacks one or more of the essential amino acids.

It is possible with protein combinations of two incomplete protein sources (e.g., rice and beans) to make a complete protein source, and characteristic combinations are the basis of distinct cultural cooking traditions. However, complementary sources of protein do not need to be eaten at the same meal to be used together by the body. Excess amino acids from protein can be converted into glucose and used for fuel through a process called gluconeogenesis.

Water is excreted from the body in multiple forms; including urine and feces, sweating, and by water vapour in the exhaled breath. Therefore, it is necessary to adequately rehydrate to replace lost fluids.

Early recommendations for the quantity of water required for maintenance of good health suggested that 6–8 glasses of water daily is the minimum to maintain proper hydration. However the notion that a person should consume eight glasses of water per day cannot be traced to a credible scientific source. The original water intake recommendation in 1945 by the Food and Nutrition Board of the National Research Council read: "An ordinary standard for diverse persons is 1 milliliter for each calorie of food. Most of this quantity is contained in prepared foods." More recent comparisons of well-known recommendations on fluid intake have revealed large discrepancies in the volumes of water we need to consume for good health. Therefore, to help standardize guidelines, recommendations for water consumption are included in two recent European Food Safety Authority (EFSA) documents (2010): (i) Food-based dietary guidelines and (ii) Dietary reference values for water or adequate daily intakes (ADI). These specifications were provided by calculating adequate intakes from measured intakes in populations of individuals with “desirable osmolarity values of urine and desirable water volumes per energy unit consumed.”

For healthful hydration, the current EFSA guidelines recommend total water intakes of 2.0 L/day for adult females and 2.5 L/day for adult males. These reference values include water from drinking water, other beverages, and from food. About 80% of our daily water requirement comes from the beverages we drink, with the remaining 20% coming from food. Water content varies depending on the type of food consumed, with fruit and vegetables containing more than cereals, for example. These values are estimated using country-specific food balance sheets published by the Food and Agriculture Organisation of the United Nations.

The EFSA panel also determined intakes for different populations. Recommended intake volumes in the elderly are the same as for adults as despite lower energy consumption, the water requirement of this group is increased due to a reduction in renal concentrating capacity. Pregnant and breastfeeding women require additional fluids to stay hydrated. The EFSA panel proposes that pregnant women should consume the same volume of water as non-pregnant women, plus an increase in proportion to the higher energy requirement, equal to 300 mL/day. To compensate for additional fluid output, breastfeeding women require an additional 700 mL/day above the recommended intake values for non-lactating women. Dehydration and over-hydration - too little and too much water, respectively - can have harmful consequences. Drinking too much water is one of the possible causes of hyponatremia, i.e., low serum sodium.

Pure ethanol provides 7 calories per gram. For distilled spirits, a standard serving in the United States is 1.5 fluid ounces, which at 40% ethanol (80 proof), would be 14 grams and 98 calories. Wine and beer contain a similar range of ethanol for servings of 5 ounces and 12 ounces, respectively, but these beverages also contain non-ethanol calories. A 5 ounce serving of wine contains 100 to 130 calories. A 12 ounce serving of beer contains 95 to 200 calories. According to the U.S. Department of Agriculture, based on NHANES 2013-2014 surveys, women ages 20 and up consume on average 6.8 grams/day and men consume on average 15.5 grams/day. Ignoring the non-alcohol contribution of those beverages, the average ethanol calorie contributions are 48 and 108 cal/day. Alcoholic beverages are considered empty calorie foods because other than calories, these contribute no essential nutrients.

The micronutrients are minerals, vitamins, and others.

Dietary minerals are inorganic chemical elements required by living organisms, other than the four elements carbon, hydrogen, nitrogen, and oxygen that are present in nearly all organic molecules. The term "mineral" is archaic, since the intent is to describe simply the less common elements in the diet. Some are heavier than the four just mentioned, including several metals, which often occur as ions in the body. Some dietitians recommend that these be supplied from foods in which they occur naturally, or at least as complex compounds, or sometimes even from natural inorganic sources (such as calcium carbonate from ground oyster shells). Some minerals are absorbed much more readily in the ionic forms found in such sources. On the other hand, minerals are often artificially added to the diet as supplements; the most famous is likely iodine in iodized salt which prevents goiter.

Many elements are essential in relative quantity; they are usually called "bulk minerals". Some are structural, but many play a role as electrolytes. Elements with recommended dietary allowance (RDA) greater than 150  mg/day are, in alphabetical order (with informal or folk-medicine perspectives in parentheses):

Many elements are required in trace amounts, usually because they play a catalytic role in enzymes. Some trace mineral elements (RDA < 200 mg/day) are, in alphabetical order:


Vitamins are essential nutrients, necessary in the diet for good health. (Vitamin D is an exception, as it can be synthesized in the skin in the presence of UVB radiation, and many animal species can synthesize vitamin C.) Vitamin deficiencies may result in disease conditions, including goitre, scurvy, osteoporosis, impaired immune system, disorders of cell metabolism, certain forms of cancer, symptoms of premature aging, and poor psychological health, among many others. Excess levels of some vitamins are also dangerous to health. The Food and Nutrition Board of the Institute of Medicine has established Tolerable Upper Intake Levels (ULs) for seven vitamins.

Phytochemicals such as polyphenols are compounds produced naturally in plants (phyto means "plant" in Greek). In general, the term is used to refer to compounds which do not appear to be nutritionally essential and yet may have positive impacts on health. To date, there is no conclusive evidence in humans that polyphenols or other non-nutrient compounds from plants have health benefit effects. Many of these substances from fruits, berries, nuts, spices, vegetables and whole grain foods are described as having antioxidant activity. While initial studies sought to reveal if nutrient antioxidant supplements might promote health, one meta-analysis concluded that supplementation with vitamins A and E and beta-carotene did not convey any benefits and may in fact increase risk of death. Vitamin C and selenium supplements did not impact mortality rate. Health effects of non-nutrient phytochemicals such as polyphenols were not assessed in this review.

Animal intestines contain a large population of gut flora. In humans, the four dominant phyla are Firmicutes, Bacteroidetes, Actinobacteria, and Proteobacteria. They are essential to digestion and are also affected by food that is consumed. Bacteria in the large intestine perform many important functions for humans, including breaking down and aiding in the absorption of fermentable fiber, stimulating cell growth, repressing the growth of harmful bacteria, training the immune system to respond only to pathogens, producing vitamin B, and defending against some infectious diseases. "Probiotics" refers to the idea of deliberately consuming live bacteria in an attempt to change the bacterial population in the large intestine, to the health benefit of the host human or animal. "Prebiotic (nutrition)" refers to the idea that consuming a bacterial energy source such as soluble fiber could support the population of health-beneficial bacteria in the large intestine. There is not yet a scientific consensus as to health benefits accruing from probiotics or prebiotics.

Carnivore and herbivore diets are contrasting, with basic nitrogen and carbon proportions vary for their particular foods. Many herbivores rely on bacterial fermentation to create digestible nutrients from indigestible plant cellulose, while obligate carnivores must eat animal meats to obtain certain vitamins or nutrients their bodies cannot otherwise synthesize.

Plant nutrition is the study of the chemical elements that are necessary for plant growth. There are several principles that apply to plant nutrition. Some elements are directly involved in plant metabolism. However, this principle does not account for the so-called beneficial elements, whose presence, while not required, has clear positive effects on plant growth.

A nutrient that is able to limit plant growth according to Liebig's law of the minimum is considered an essential plant nutrient if the plant cannot complete its full life cycle without it. There are 16 essential plant soil nutrients, besides the three major elemental nutrients carbon and oxygen that are obtained by photosynthetic plants from carbon dioxide in air, and hydrogen, which is obtained from water.

Plants uptake essential elements from the soil through their roots and from the air (consisting of mainly nitrogen and oxygen) through their leaves. Green plants obtain their carbohydrate supply from the carbon dioxide in the air by the process of photosynthesis. Carbon and oxygen are absorbed from the air, while other nutrients are absorbed from the soil. Nutrient uptake in the soil is achieved by cation exchange, wherein root hairs pump hydrogen ions (H) into the soil through proton pumps. These hydrogen ions displace cations attached to negatively charged soil particles so that the cations are available for uptake by the root. In the leaves, stomata open to take in carbon dioxide and expel oxygen. The carbon dioxide molecules are used as the carbon source in photosynthesis.

Although nitrogen is plentiful in the Earth's atmosphere, very few plants can use this directly. Most plants, therefore, require nitrogen compounds to be present in the soil in which they grow. This is made possible by the fact that largely inert atmospheric nitrogen is changed in a nitrogen fixation process to biologically usable forms in the soil by bacteria.

Plant nutrition is a difficult subject to understand completely, partially because of the variation between different plants and even between different species or individuals of a given clone. Elements present at low levels may cause deficiency symptoms, and toxicity is possible at levels that are too high. Furthermore, deficiency of one element may present as symptoms of toxicity from another element, and vice versa.

Canada's Food Guide is an example of a government-run nutrition program. Produced by Health Canada, the guide advises food quantities, provides education on balanced nutrition, and promotes physical activity in accordance with government-mandated nutrient needs. Like other nutrition programs around the world, Canada's Food Guide divides nutrition into four main food groups: vegetables and fruit, grain products, milk and alternatives, and meat and alternatives. It is interesting to note that, unlike its American counterpart, the Canadian guide references and provides alternative to meat and dairy, which can be attributed to the growing vegan and vegetarian movements.

In the US, nutritional standards and recommendations are established jointly by the US Department of Agriculture and US Department of Health and Human Services. Dietary and physical activity guidelines from the USDA are presented in the concept of MyPlate, which superseded the food pyramid, which replaced the Four Food Groups. The Senate committee currently responsible for oversight of the USDA is the "Agriculture, Nutrition and Forestry Committee". Committee hearings are often televised on C-SPAN. The U.S. Department of Health and Human Services provides a sample week-long menu that fulfills the nutritional recommendations of the government.

Governmental organisations have been working on nutrition literacy interventions in non-primary health care settings to address the nutrition information problem in the U.S. Some programs include:

The Family Nutrition Program (FNP) is a free nutrition education program serving low-income adults around the U.S. This program is funded by the Food Nutrition Service’s (FNS) branch of the United States Department of Agriculture (USDA) usually through a local state academic institution that runs the program. The FNP has developed a series of tools to help families participating in the Food Stamp Program stretch their food dollar and form healthful eating habits including nutrition education.

Expanded Food and Nutrition Education Program (ENFEP) is a unique program that currently operates in all 50 states and in American Samoa, Guam, Micronesia, Northern Marianas, Puerto Rico, and the Virgin Islands. It is designed to assist limited-resource audiences in acquiring the knowledge, skills, attitudes, and changed behavior necessary for nutritionally sound diets, and to contribute to their personal development and the improvement of the total family diet and nutritional well-being.

An example of a state initiative to promote nutrition literacy is Smart Bodies, a public-private partnership between the state’s largest university system and largest health insurer, Louisiana State Agricultural Center and Blue Cross and Blue Shield of Louisiana Foundation. Launched in 2005, this program promotes lifelong healthful eating patterns and physically active lifestyles for children and their families. It is an interactive educational program designed to help prevent childhood obesity through classroom activities that teach children healthful eating habits and physical exercise.

Nutrition is taught in schools in many countries. In England and Wales, the Personal and Social Education and Food Technology curricula include nutrition, stressing the importance of a balanced diet and teaching how to read nutrition labels on packaging. In many schools, a Nutrition class will fall within the Family and Consumer Science or Health departments. In some American schools, students are required to take a certain number of FCS or Health related classes. Nutrition is offered at many schools, and, if it is not a class of its own, nutrition is included in other FCS or Health classes such as: Life Skills, Independent Living, Single Survival, Freshmen Connection, Health etc. In many Nutrition classes, students learn about the food groups, the food pyramid, Daily Recommended Allowances, calories, vitamins, minerals, malnutrition, physical activity, healthful food choices, portion sizes, and how to live a healthy life.

A 1985, US National Research Council report entitled "Nutrition Education in US Medical Schools" concluded that nutrition education in medical schools was inadequate. Only 20% of the schools surveyed taught nutrition as a separate, required course. A 2006 survey found that this number had risen to 30%. Membership by physicians in leading professional nutrition societies such as the American Society for Nutrition has generally declined from the 1990s.

In the US, Registered dietitian nutritionists (RDs or RDNs) are health professionals qualified to provide safe, evidence-based dietary advice which includes a review of what is eaten, a thorough review of nutritional health, and a personalized nutritional treatment plan. They also provide preventive and therapeutic programs at work places, schools and similar institutions. Certified Clinical Nutritionists or CCNs, are trained health professionals who also offer dietary advice on the role of nutrition in chronic disease, including possible prevention or remediation by addressing nutritional deficiencies before resorting to drugs. Government regulation especially in terms of licensing, is currently less universal for the CCN than that of RD or RDN. Another advanced Nutrition Professional is a Certified Nutrition Specialist or CNS. These Board Certified Nutritionists typically specialize in obesity and chronic disease. In order to become board certified, potential CNS candidate must pass an examination, much like Registered Dieticians. This exam covers specific domains within the health sphere including; Clinical Intervention and Human Health.

The findings of the 2003 National Assessment of Adult Literacy (NAAL) provide a basis upon which to frame the nutrition literacy problem in the U.S. NAAL introduced the first ever measure of "the degree to which individuals have the capacity to obtain, process and understand basic health information and services needed to make appropriate health decisions" – an objective of Healthy People 2010 and of which nutrition literacy might be considered an important subset. On a scale of below basic, basic, intermediate and proficient, NAAL found 13 percent of adult Americans have proficient health literacy, 44% have intermediate literacy, 29 percent have basic literacy and 14 percent have below basic health literacy. The study found that health literacy increases with education and people living below the level of poverty have lower health literacy than those above it.

Another study examining the health and nutrition literacy status of residents of the lower Mississippi Delta found that 52 percent of participants had a high likelihood of limited literacy skills. While a precise comparison between the NAAL and Delta studies is difficult, primarily because of methodological differences, Zoellner et al. suggest that health literacy rates in the Mississippi Delta region are different from the U.S. general population and that they help establish the scope of the problem of health literacy among adults in the Delta region. For example, only 12 percent of study participants identified the My Pyramid graphic two years after it had been launched by the USDA. The study also found significant relationships between nutrition literacy and income level and nutrition literacy and educational attainment further delineating priorities for the region.

These statistics point to the complexities surrounding the lack of health/nutrition literacy and reveal the degree to which they are embedded in the social structure and interconnected with other problems. Among these problems are the lack of information about food choices, a lack of understanding of nutritional information and its application to individual circumstances, limited or difficult access to healthful foods, and a range of cultural influences and socioeconomic constraints such as low levels of education and high levels of poverty that decrease opportunities for healthful eating and living.

The links between low health literacy and poor health outcomes has been widely documented and there is evidence that some interventions to improve health literacy have produced successful results in the primary care setting. More must be done to further our understanding of nutrition literacy specific interventions in non-primary care settings in order to achieve better health outcomes.

Malnutrition refers to insufficient, excessive, or imbalanced consumption of nutrients by an organism. In developed countries, the diseases of malnutrition are most often associated with nutritional imbalances or excessive consumption. In developing countries, malnutrition is more likely to be caused by poor access to a range of nutritious foods or inadequate knowledge. In Mali the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) and the Aga Khan Foundation, trained women's groups to make "equinut", a healthy and nutritional version of the traditional recipe "di-dèguè" (comprising peanut paste, honey and millet or rice flour). The aim was to boost nutrition and livelihoods by producing a product that women could make and sell, and which would be accepted by the local community because of its local heritage.

Although under- and over-nutrition are often viewed as human problems, pet animals can be under- or overfed by their owners, domesticated animals can be undernourished for macro- and micro-nutrients, affecting growth and health, and wild animals can be undernourished to the point of starvation and death.

Nutritionism is the view that excessive reliance on food science and the study of nutrition can lead to poor nutrition and to ill health. It was originally credited to Gyorgy Scrinis, and was popularized by Michael Pollan. Since nutrients are invisible, policy makers rely on nutrition experts to advise on food choices. Because science has an incomplete understanding of how food affects the human body, Pollan argues, nutritionism can be blamed for many of the health problems relating to diet in the Western World today.

The U.S. Food and Nutrition Board sets Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for vitamins and minerals. EARs and RDAs are part of Dietary Reference Intakes. The DRI documents describe nutrient deficiency signs and symptoms.

The U.S. Food and Nutrition Board sets Tolerable Upper Intake Levels (known as ULs) for vitamins and minerals when evidence is sufficient. ULs are set a safe fraction below amounts shown to cause health problems. ULs are part of Dietary Reference Intakes. The European Food Safety Authority also reviews the same safety questions and set its own ULs.

When too much of one or more nutrients is present in the diet to the exclusion of the proper amount of other nutrients, the diet is said to be unbalanced. High calorie food ingredients such as vegetable oils, sugar and alcohol are referred to as "empty calories" because they displace from the diet foods that also contain protein, vitamins, minerals and fiber.

Research indicates that improving the awareness of nutritious meal choices and establishing long-term habits of healthy eating have a positive effect on cognitive and spatial memory capacity, with potential to increase a student's ability to process and retain academic information.

Some organizations have begun working with teachers, policymakers, and managed foodservice contractors to mandate improved nutritional content and increased nutritional resources in school cafeterias from primary to university level institutions. Health and nutrition have been proven to have close links with overall educational success. Currently, less than 10% of American college students report that they eat the recommended five servings of fruit and vegetables daily. Better nutrition has been shown to affect both cognitive and spatial memory performance; a study showed those with higher blood sugar levels performed better on certain memory tests. In another study, those who consumed yogurt performed better on thinking tasks when compared to those that consumed caffeine-free diet soda or confections. Nutritional deficiencies have been shown to have a negative effect on learning behavior in mice as far back as 1951.

There is limited research available that directly links a student's Grade Point Average (G.P.A.) to their overall nutritional health. Additional substantive data is needed to prove that overall intellectual health is closely linked to a person's diet, rather than just another correlation fallacy.

Nutritional supplement treatment may be appropriate for major depression, bipolar disorder, schizophrenia, and obsessive compulsive disorder, the four most common mental disorders in developed countries. Supplements that have been studied most for mood elevation and stabilization include eicosapentaenoic acid and docosahexaenoic acid (each of which an omega-3 fatty acid contained in fish oil but not in flaxseed oil), vitamin B12, folic acid, and inositol.

Cancer is now common in developing countries. According to a study by the International Agency for Research on Cancer, "In the developing world, cancers of the liver, stomach and esophagus were more common, often linked to consumption of carcinogenic preserved foods, such as smoked or salted food, and parasitic infections that attack organs." Developed countries "tended to have cancers linked to affluence or a 'Western lifestyle' — cancers of the colon, rectum, breast and prostate — that can be caused by obesity, lack of exercise, diet and age."

Several lines of evidence indicate lifestyle-induced hyperinsulinemia and reduced insulin function (i.e., insulin resistance) as a decisive factor in many disease states. For example, hyperinsulinemia and insulin resistance are strongly linked to chronic inflammation, which in turn is strongly linked to a variety of adverse developments such as arterial microtrauma and clot formation (i.e., heart disease) and exaggerated cell division (i.e., cancer). Hyperinsulinemia and insulin resistance (the so-called metabolic syndrome) are characterized by a combination of abdominal obesity, elevated blood sugar, elevated blood pressure, elevated blood triglycerides, and reduced HDL cholesterol. The negative effect of hyperinsulinemia on prostaglandin PGE1/PGE2 balance may be significant.

The state of obesity clearly contributes to insulin resistance, which in turn can cause type 2 diabetes. Virtually all obese and most type 2 diabetic individuals have marked insulin resistance. Although the association between overweight and insulin resistance is clear, the exact (likely multifarious) causes of insulin resistance remain less clear. It is important to note that it has been demonstrated that appropriate exercise, more regular food intake, and reducing glycemic load (see below) all can reverse insulin resistance in overweight individuals (and thereby lower their blood sugar level, in those with type 2 diabetes).

Obesity can unfavorably alter hormonal and metabolic status via resistance to the hormone leptin, and a vicious cycle may occur in which insulin/leptin resistance and obesity aggravate one another. The vicious cycle is putatively fueled by continuously high insulin/leptin stimulation and fat storage, as a result of high intake of strongly insulin/leptin stimulating foods and energy. Both insulin and leptin normally function as satiety signals to the hypothalamus in the brain; however, insulin/leptin resistance may reduce this signal and therefore allow continued overfeeding despite large body fat stores. In addition, reduced leptin signaling to the brain may reduce leptin's normal effect to maintain an appropriately high metabolic rate.

There is a debate about how and to what extent different dietary factors— such as intake of processed carbohydrates, total protein, fat, and carbohydrate intake, intake of saturated and trans fatty acids, and low intake of vitamins/minerals—contribute to the development of insulin and leptin resistance. In any case, analogous to the way modern man-made pollution may possess the potential to overwhelm the environment's ability to maintain homeostasis, the recent explosive introduction of high glycemic index and processed foods into the human diet may possess the potential to overwhelm the body's ability to maintain homeostasis and health (as evidenced by the metabolic syndrome epidemic).

Antinutrients are natural or synthetic compounds that interfere with the absorption of nutrients. Nutrition studies focus on antinutrients commonly found in food sources and beverages.

Sugar consumption in the United States

The relatively recent increased consumption of sugar has been linked to the rise of some afflictions such as diabetes, obesity, and more recently heart disease. Increased consumption of sugar has been tied to these three, among others. Obesity levels have more than doubled in the last 30 years among adults, going from 15% to 35% in the United States. Obesity and diet also happen to be high risk factors for diabetes. In the same time span that obesity doubled, diabetes numbers quadrupled in America. Increased weight, especially in the form of belly fat, and high sugar intake are also high risk factors for heart disease. Both sugar intake and fatty tissue increase the probability of elevated LDL cholesterol in the bloodstream. Elevated amounts of Low-density lipoprotein (LDL) cholesterol, is the primary factor in heart disease. In order to avoid all the dangers of sugar, moderate consumption is paramount.

Since the Industrial Revolution some two hundred years ago, the food processing industry has invented many technologies that both help keep foods fresh longer and alter the fresh state of food as they appear in nature. Cooling is the primary technology used to maintain freshness, whereas many more technologies have been invented to allow foods to last longer without becoming spoiled. These latter technologies include pasteurisation, autoclavation, drying, salting, and separation of various components, all of which appearing to alter the original nutritional contents of food. Pasteurisation and autoclavation (heating techniques) have no doubt improved the safety of many common foods, preventing epidemics of bacterial infection. But some of the (new) food processing technologies have downfalls as well.

Modern separation techniques such as milling, centrifugation, and pressing have enabled concentration of particular components of food, yielding flour, oils, juices, and so on, and even separate fatty acids, amino acids, vitamins, and minerals. Inevitably, such large-scale concentration changes the nutritional content of food, saving certain nutrients while removing others. Heating techniques may also reduce food's content of many heat-labile nutrients such as certain vitamins and phytochemicals, and possibly other yet-to-be-discovered substances. Because of reduced nutritional value, processed foods are often 'enriched' or 'fortified' with some of the most critical nutrients (usually certain vitamins) that were lost during processing. Nonetheless, processed foods tend to have an inferior nutritional profile compared to whole, fresh foods, regarding content of both sugar and high GI starches, potassium/sodium, vitamins, fiber, and of intact, unoxidized (essential) fatty acids. In addition,
processed foods often contain potentially harmful substances such as oxidized fats and trans fatty acids.

A dramatic example of the effect of food processing on a population's health is the history of epidemics of beri-beri in people subsisting on polished rice. Removing the outer layer of rice by polishing it removes with it the essential vitamin thiamine, causing beri-beri. Another example is the development of scurvy among infants in the late 19th century in the United States. It turned out that the vast majority of sufferers were being fed milk that had been heat-treated (as suggested by Pasteur) to control bacterial disease. Pasteurisation was effective against bacteria, but it destroyed the vitamin C.

As mentioned, lifestyle- and obesity-related diseases are becoming increasingly prevalent all around the world. There is little doubt that the increasingly widespread application of some modern food processing technologies has contributed to this development. The food processing industry is a major part of modern economy, and as such it is influential in political decisions (e.g., nutritional recommendations, agricultural subsidising). In any known profit-driven economy, health considerations are hardly a priority; effective production of cheap foods with a long shelf-life is more the trend. In general, whole, fresh foods have a relatively short shelf-life and are less profitable to produce and sell than are more processed foods. Thus, the consumer is left with the choice between more expensive, but nutritionally superior, whole, fresh foods, and cheap, usually nutritionally inferior, processed foods. Because processed foods are often cheaper, more convenient (in both purchasing, storage, and preparation), and more available, the consumption of nutritionally inferior foods has been increasing throughout the world along with many nutrition-related health complications.




</doc>
<doc id="21526" url="https://en.wikipedia.org/wiki?curid=21526" title="November 22">
November 22

In the ancient astronomy, it is the cusp day between Scorpio and Sagittarius. In some years it is Sagittarius, but others Scorpio.





</doc>
<doc id="21527" url="https://en.wikipedia.org/wiki?curid=21527" title="Number theory">
Number theory

Number theory (or arithmetic or higher arithmetic in older usage), is a branch of pure mathematics devoted primarily to the study of the integers. It is sometimes called "The Queen of Mathematics" because of its foundational place in the discipline. Number theorists study prime numbers as well as the properties of objects made out of integers (e.g., rational numbers) or defined as generalizations of the integers (e.g., algebraic integers).

Integers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory are often best understood through the study of analytical objects (e.g., the Riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers, e.g., as approximated by the latter (Diophantine approximation).

The older term for number theory is "arithmetic". By the early twentieth century, it had been superseded by "number theory". (The word "arithmetic" is used by the general public to mean "elementary calculations"; it has also acquired other meanings in mathematical logic, as in "Peano arithmetic", and computer science, as in "floating point arithmetic".) The use of the term "arithmetic" for "number theory" regained some ground in the second half of the 20th century, arguably in part due to French influence. In particular, "arithmetical" is preferred as an adjective to "number-theoretic".

The first historical find of an arithmetical nature is a fragment of a table: the broken clay tablet Plimpton 322 (Larsa, Mesopotamia, ca. 1800 BC) contains a list of "Pythagorean triples", i.e., integers formula_1 such that formula_2.
The triples are too many and too large to have been obtained by brute force. The heading over the first column reads: "The "takiltum" of the diagonal which has been subtracted such that the width..."
The table's layout suggests that it was constructed by means of what amounts, in modern language, to the identity

which is implicit in routine Old Babylonian exercises. If some other method was used, the triples were first constructed and then reordered by formula_4, presumably for actual use as a "table", i.e., with a view to applications.

It is not known what these applications may have been, or whether there could have been any; Babylonian astronomy, for example, truly came into its own only later. It has been suggested instead that the table was a source of numerical examples for school problems.

While Babylonian number theory—or what survives of Babylonian mathematics that can be called thus—consists of this single, striking fragment, Babylonian algebra (in the secondary-school sense of "algebra") was exceptionally well developed. Late Neoplatonic sources state that Pythagoras learned mathematics from the Babylonians. Much earlier sources state that Thales and Pythagoras traveled and studied in Egypt.

Euclid IX 21—34 is very probably Pythagorean; it is very simple material ("odd times even is even", "if an odd number measures [= divides] an even number, then it also measures [= divides] half of it"), but it is all that is needed to prove that formula_5
is irrational. Pythagorean mystics gave great importance to the odd and the even.
The discovery that formula_5 is irrational is credited to the early Pythagoreans (pre-Theodorus). By revealing (in modern terms) that numbers could be irrational, this discovery seems to have provoked the first foundational crisis in mathematical history; its proof or its divulgation are sometimes credited to Hippasus, who was expelled or split from the Pythagorean sect. This forced a distinction between "numbers" (integers and the rationals—the subjects of arithmetic), on the one hand, and "lengths" and "proportions" (which we would identify with real numbers, whether rational or not), on the other hand.

The Pythagorean tradition spoke also of so-called polygonal or figurate numbers. While square numbers, cubic numbers, etc., are seen now as more natural than triangular numbers, pentagonal numbers, etc., the study of the sums
of triangular and pentagonal numbers would prove fruitful in the early modern period (17th to early 19th century).

We know of no clearly arithmetical material in ancient Egyptian or Vedic sources, though there is some algebra in both. The Chinese remainder theorem appears as an exercise in "Sunzi Suanjing" (3rd, 4th or 5th century CE.) (There is one important step glossed over in Sunzi's solution: it is the problem that was later solved by Āryabhaṭa's Kuṭṭaka – see below.)

There is also some numerical mysticism in Chinese mathematics, but, unlike that of the Pythagoreans, it seems to have led nowhere. Like the Pythagoreans' perfect numbers, magic squares have passed from superstition into recreation.

Aside from a few fragments, the mathematics of Classical Greece is known to us either through the reports of contemporary non-mathematicians or through mathematical works from the early Hellenistic period. In the case of number theory, this means, by and large, "Plato" and "Euclid", respectively.

While Asian mathematics influenced Greek and Hellenistic learning, it seems to be the case that Greek mathematics is also an indigenous tradition.

Eusebius, PE X, chapter 4 mentions of Pythagoras:

"In fact the said Pythagoras, while busily studying the wisdom of each nation, visited Babylon, and Egypt, and all Persia, being instructed by the Magi and the priests: and in addition to these he is related to have studied under the Brahmans (these are Indian philosophers); and from some he gathered astrology, from others geometry, and arithmetic and music from others, and different things from different nations, and only from the wise men of Greece did he get nothing, wedded as they were to a poverty and dearth of wisdom: so on the contrary he himself became the author of instruction to the Greeks in the learning which he had procured from abroad."

Aristotle claimed that the philosophy of Plato closely followed the teachings of the Pythagoreans, and Cicero repeats this claim: "Platonem ferunt didicisse Pythagorea omnia" ("They say Plato learned all things Pythagorean").

Plato had a keen interest in mathematics, and distinguished clearly between arithmetic and calculation. (By "arithmetic" he meant, in part, theorising on number, rather than what "arithmetic" or "number theory" have come to mean.) It is through one of Plato's dialogues—namely, "Theaetetus"—that we know that Theodorus had proven that formula_7 are irrational. Theaetetus was, like Plato, a disciple of Theodorus's; he worked on distinguishing different kinds of incommensurables, and was thus arguably a pioneer in the study of number systems. (Book X of Euclid's Elements is described by Pappus as being largely based on Theaetetus's work.)

Euclid devoted part of his "Elements" to prime numbers and divisibility, topics that belong unambiguously to number theory and are basic to it (Books VII to IX of Euclid's Elements). In particular, he gave an algorithm for computing the greatest common divisor of two numbers (the Euclidean algorithm; "Elements", Prop. VII.2) and the first known proof of the infinitude of primes ("Elements", Prop. IX.20).

In 1773, Lessing published an epigram he had found in a manuscript during his work as a librarian; it claimed to be a letter sent by Archimedes to Eratosthenes. The epigram proposed what has become known as
Archimedes's cattle problem; its solution (absent from the manuscript) requires solving an indeterminate quadratic equation (which reduces to what would later be misnamed Pell's equation). As far as we know, such equations were first successfully treated by the Indian school. It is not known whether Archimedes himself had a method of solution.

Very little is known about Diophantus of Alexandria; he probably lived in the third century CE, that is, about five hundred years after Euclid. Six out of the thirteen books of Diophantus's "Arithmetica" survive in the original Greek; four more books survive in an Arabic translation. The "Arithmetica" is a collection of worked-out problems where the task is invariably to find rational solutions to a system of polynomial equations, usually of the form formula_8 or formula_9. Thus, nowadays, we speak of "Diophantine equations" when we speak of polynomial equations to which rational or integer solutions must be found.

One may say that Diophantus was studying rational points — i.e., points whose coordinates are rational — on curves and algebraic varieties; however, unlike the Greeks of the Classical period, who did what we would now call basic algebra in geometrical terms, Diophantus did what we would now call basic algebraic geometry in purely algebraic terms. In modern language, what Diophantus did was to find rational parametrizations of varieties; that is, given an equation of the form (say)
formula_10, his aim was to find (in essence) three rational functions formula_11 such that, for all values of formula_12 and formula_13, setting
formula_14 for formula_15 gives a solution to formula_16

Diophantus also studied the equations of some non-rational curves, for which no rational parametrisation is possible. He managed to find some rational points on these curves (elliptic curves, as it happens, in what seems to be their first known occurrence) by means of what amounts to a tangent construction: translated into coordinate geometry

While Diophantus was concerned largely with rational solutions, he assumed some results on integer numbers, in particular that every integer is the sum of four squares (though he never stated as much explicitly).

While Greek astronomy probably influenced Indian learning, to the point of introducing trigonometry, it seems to be the case that Indian mathematics is otherwise an indigenous tradition; in particular, there is no evidence that Euclid's Elements reached India before the 18th century.

Āryabhaṭa (476–550 CE) showed that pairs of simultaneous congruences formula_17, formula_18 could be solved by a method he called "kuṭṭaka", or "pulveriser"; this is a procedure close to (a generalisation of) the Euclidean algorithm, which was probably discovered independently in India. Āryabhaṭa seems to have had in mind applications to astronomical calculations.

Brahmagupta (628 CE) started the systematic study of indefinite quadratic equations—in particular, the misnamed Pell equation, in which Archimedes may have first been interested, and which did not start to be solved in the West until the time of Fermat and Euler. Later Sanskrit authors would follow, using Brahmagupta's technical terminology. A general procedure (the chakravala, or "cyclic method") for solving Pell's equation was finally found by Jayadeva (cited in the eleventh century; his work is otherwise lost); the earliest surviving exposition appears in Bhāskara II's Bīja-gaṇita (twelfth century).

Indian mathematics remained largely unknown in Europe until the late eighteenth century; Brahmagupta and Bhāskara's work was translated into English in 1817 by Henry Colebrooke.

In the early ninth century, the caliph Al-Ma'mun ordered translations of many Greek mathematical works and at least one Sanskrit work (the "Sindhind",
which may or may not be Brahmagupta's Brāhmasphuṭasiddhānta).
Diophantus's main work, the "Arithmetica", was translated into Arabic by Qusta ibn Luqa (820–912).
Part of the treatise "al-Fakhri" (by al-Karajī, 953 – ca. 1029) builds on it to some extent. According to Rashed Roshdi, Al-Karajī's contemporary Ibn al-Haytham knew what would later be called Wilson's theorem.

Other than a treatise on squares in arithmetic progression by Fibonacci — who lived and studied in north Africa and Constantinople during his formative years, ca. 1175–1200 — no number theory to speak of was done in western Europe during the Middle Ages. Matters started to change in Europe in the late Renaissance, thanks to a renewed study of the works of Greek antiquity. A catalyst was the textual emendation and translation into Latin of Diophantus's "Arithmetica" (Bachet, 1621, following a first attempt by Xylander, 1575).
Pierre de Fermat (1601–1665) never published his writings; in particular, his work on number theory is contained almost entirely in letters to mathematicians and in private marginal notes. He wrote down nearly no proofs in number theory; he had no models in the area. He did make repeated use of mathematical induction, introducing the method of infinite descent.

One of Fermat's first interests was perfect numbers (which appear in Euclid, "Elements" IX) and amicable numbers; this led him to work on integer divisors, which were from the beginning among the subjects of the
correspondence (1636 onwards) that put him in touch with the mathematical community of the day. He had already studied Bachet's edition of Diophantus carefully; by 1643, his interests had shifted largely to Diophantine problems and sums of squares (also treated by Diophantus).

Fermat's achievements in arithmetic include:

Fermat's claim ("Fermat's last theorem") to have shown there are no solutions to
formula_25 for all formula_26 appears only in his annotations on the margin of his copy of Diophantus.

The interest of Leonhard Euler (1707–1783) in number theory was first spurred in 1729, when a friend of his, the amateur Goldbach, pointed him towards some of Fermat's work on the subject. This has been called the "rebirth" of modern number theory, after Fermat's relative lack of success in getting his contemporaries' attention for the subject. Euler's work on number theory includes the following:


Joseph-Louis Lagrange (1736–1813) was the first to give full proofs of some of Fermat's and Euler's work and observations – for instance, the four-square theorem and the basic theory of the misnamed "Pell's equation" (for which an algorithmic solution was found by Fermat and his contemporaries, and also by Jayadeva and Bhaskara II before them.) He also studied quadratic forms in full generality (as opposed to formula_31) — defining their equivalence relation, showing how to put them in reduced form, etc.

Adrien-Marie Legendre (1752–1833) was the first to state the law of quadratic reciprocity. He also
conjectured what amounts to the prime number theorem and Dirichlet's theorem on arithmetic progressions. He gave a full treatment of the equation formula_32 and worked on quadratic forms along the lines later developed fully by Gauss. In his old age, he was the first to prove "Fermat's last theorem" for formula_33 (completing work by Peter Gustav Lejeune Dirichlet, and crediting both him and Sophie Germain).

In his "Disquisitiones Arithmeticae" (1798), Carl Friedrich Gauss (1777–1855) proved the law of quadratic reciprocity and developed the theory of quadratic forms (in particular, defining their composition). He also introduced some basic notation (congruences) and devoted a section to computational matters, including primality tests. The last section of the "Disquisitiones" established a link between roots of unity and number theory:
The theory of the division of the circle...which is treated in sec. 7 does not belong
by itself to arithmetic, but its principles can only be drawn from higher arithmetic.

In this way, Gauss arguably made a first foray towards both Évariste Galois's work and algebraic number theory.

Starting early in the nineteenth century, the following developments gradually took place:

Algebraic number theory may be said to start with the study of reciprocity and cyclotomy, but truly came into its own with the development of abstract algebra and early ideal theory and valuation theory; see below. A conventional starting point for analytic number theory is Dirichlet's theorem on arithmetic progressions (1837), whose proof introduced L-functions and involved some asymptotic analysis and a limiting process on a real variable. The first use of analytic ideas in number theory actually
goes back to Euler (1730s), who used formal power series and non-rigorous (or implicit) limiting arguments. The use of "complex" analysis in number theory comes later: the work of Bernhard Riemann (1859) on the zeta function is the canonical starting point; Jacobi's four-square theorem (1839), which predates it, belongs to an initially different strand that has by now taken a leading role in analytic number theory (modular forms).

The history of each subfield is briefly addressed in its own section below; see the main article of each subfield for fuller treatments. Many of the most interesting questions in each area remain open and are being actively worked on.

The term "elementary" generally denotes a method that does not use complex analysis. For example, the prime number theorem was first proven using complex analysis in 1896, but an elementary proof was found only in 1949 by Erdős and Selberg. The term is somewhat ambiguous: for example, proofs based on complex Tauberian theorems (e.g. Wiener–Ikehara) are often seen as quite enlightening but not elementary, in spite of using Fourier analysis, rather than complex analysis as such. Here as elsewhere, an "elementary" proof may be longer and more difficult for most readers than a non-elementary one.

Number theory has the reputation of being a field many of whose results can be stated to the layperson. At the same time, the proofs of these results are not particularly accessible, in part because the range of tools they use is, if anything, unusually broad within mathematics.

"Analytic number theory" may be defined
Some subjects generally considered to be part of analytic number theory, e.g., sieve theory, are better covered by the second rather than the first definition: some of sieve theory, for instance, uses little analysis, yet it does belong to analytic number theory.

The following are examples of problems in analytic number theory: the prime number theorem, the Goldbach conjecture (or the twin prime conjecture, or the Hardy–Littlewood conjectures), the Waring problem and the Riemann hypothesis. Some of the most important tools of analytic number theory are the circle method, sieve methods and L-functions (or, rather, the study of their properties). The theory of modular forms (and, more generally, automorphic forms) also occupies an increasingly central place in the toolbox of analytic number theory.

One may ask analytic questions about algebraic numbers, and use analytic means to answer such questions; it is thus that algebraic and analytic number theory intersect. For example, one may define prime ideals (generalizations of prime numbers in the field of algebraic numbers) and ask how many prime ideals there are up to a certain size. This question can be answered by means of an examination of Dedekind zeta functions, which are generalizations of the Riemann zeta function, a key analytic object at the roots of the subject. This is an example of a general procedure in analytic number theory: deriving information about the distribution of a sequence (here, prime ideals or prime numbers) from the analytic behavior of an appropriately constructed complex-valued function.

An "algebraic number" is any complex number that is a solution to some polynomial equation formula_34 with rational coefficients; for example, every solution formula_35 of formula_36 (say) is an algebraic number. Fields of algebraic numbers are also called "algebraic number fields", or shortly "number fields". Algebraic number theory studies algebraic number fields. Thus, analytic and algebraic number theory can and do overlap: the former is defined by its methods, the latter by its objects of study.

It could be argued that the simplest kind of number fields (viz., quadratic fields) were already studied by Gauss, as the discussion of quadratic forms in "Disquisitiones arithmeticae" can be restated in terms of ideals and
norms in quadratic fields. (A "quadratic field" consists of all
numbers of the form formula_37, where
formula_38 and formula_39 are rational numbers and formula_40
is a fixed rational number whose square root is not rational.)
For that matter, the 11th-century chakravala method amounts—in modern terms—to an algorithm for finding the units of a real quadratic number field. However, neither Bhāskara nor Gauss knew of number fields as such.

The grounds of the subject as we know it were set in the late nineteenth century, when "ideal numbers", the "theory of ideals" and "valuation theory" were developed; these are three complementary ways of dealing with the lack of unique factorisation in algebraic number fields. (For example, in the field generated by the rationals
and formula_41, the number formula_42 can be factorised both as formula_43 and
formula_44; all of formula_45, formula_46, formula_47 and
formula_48
are irreducible, and thus, in a naïve sense, analogous to primes among the integers.) The initial impetus for the development of ideal numbers (by Kummer) seems to have come from the study of higher reciprocity laws,i.e., generalisations of quadratic reciprocity.

Number fields are often studied as extensions of smaller number fields: a field "L" is said to be an "extension" of a field "K" if "L" contains "K".
Classifying the possible extensions of a given number field is a difficult and partially open problem. Abelian extensions—that is, extensions "L" of "K" such that the Galois group Gal("L"/"K") of "L" over "K" is an abelian group—are relatively well understood.
Their classification was the object of the programme of class field theory, which was initiated in the late 19th century (partly by Kronecker and Eisenstein) and carried out largely in 1900—1950.

An example of an active area of research in algebraic number theory is Iwasawa theory. The Langlands program, one of the main current large-scale research plans in mathematics, is sometimes described as an attempt to generalise class field theory to non-abelian extensions of number fields.

The central problem of "Diophantine geometry" is to determine when a Diophantine equation has solutions, and if it does, how many. The approach taken is to think of the solutions of an equation as a geometric object.

For example, an equation in two variables defines a curve in the plane. More generally, an equation, or system of equations, in two or more variables defines a curve, a surface or some other such object in "n"-dimensional space. In Diophantine geometry, one asks whether there are any "rational points" (points all of whose coordinates are rationals) or
"integral points" (points all of whose coordinates are integers) on the curve or surface. If there are any such points, the next step is to ask how many there are and how they are distributed. A basic question in this direction is: are there finitely
or infinitely many rational points on a given curve (or surface)? What about integer points?

An example here may be helpful. Consider the Pythagorean equation formula_49;
we would like to study its rational solutions, i.e., its solutions
formula_50 such that
"x" and "y" are both rational. This is the same as asking for all integer solutions
to formula_51; any solution to the latter equation gives
us a solution formula_52, formula_53 to the former. It is also the
same as asking for all points with rational coordinates on the curve
described by formula_54. (This curve happens to be a circle of radius 1 around the origin.)

The rephrasing of questions on equations in terms of points on curves turns out to be felicitous. The finiteness or not of the number of rational or integer points on an algebraic curve—that is, rational or integer solutions to an equation formula_55, where formula_56 is a polynomial in two variables—turns out to depend crucially on the "genus" of the curve. The "genus" can be defined as follows: allow the variables in formula_55 to be complex numbers; then formula_55 defines a 2-dimensional surface in (projective) 4-dimensional space (since two complex variables can be decomposed into four real variables, i.e., four dimensions). Count
the number of (doughnut) holes in the surface; call this number the "genus" of formula_55. Other geometrical notions turn out to be just as crucial.

There is also the closely linked area of Diophantine approximations: given a number formula_35, how well can it be approximated by rationals? (We are looking for approximations that are good relative to the amount of space that it takes to write the rational: call formula_61 (with formula_62) a good approximation to formula_35 if formula_64, where formula_65 is large.) This question is of special interest if formula_35 is an algebraic number. If formula_35 cannot be well approximated, then some equations do not have integer or rational solutions. Moreover, several concepts (especially that of height) turn out to be crucial both in Diophantine geometry and in the study of Diophantine approximations. This question is also of special interest in transcendental number theory: if a number can be better approximated than any algebraic number, then it is a transcendental number. It is by this argument that and e have been shown to be transcendental.

Diophantine geometry should not be confused with the geometry of numbers, which is a collection of graphical methods for answering certain questions in algebraic number theory. "Arithmetic geometry", on the other hand, is a contemporary term
for much the same domain as that covered by the term "Diophantine geometry". The term "arithmetic geometry" is arguably used
most often when one wishes to emphasise the connections to modern algebraic geometry (as in, for instance, Faltings's theorem) rather than to techniques in Diophantine approximations.

The areas below date as such from no earlier than the mid-twentieth century, even if they are based on older material. For example, as is explained below, the matter of algorithms in number theory is very old, in some sense older than the concept of proof; at the same time, the modern study of computability dates only from the 1930s and 1940s, and computational complexity theory from the 1970s.

Take a number at random between one and a million. How likely is it to be prime? This is just another way of asking how many primes there are between one and a million. Further: how many prime divisors will it have, on average? How many divisors will it have altogether, and with what likelihood? What is the probability that it will have many more or many fewer divisors or prime divisors than the average?

Much of probabilistic number theory can be seen as an important special case of the study of variables that are almost, but not quite, mutually independent. For example, the event that a random integer between one and a million be divisible by two and the event that it be divisible by three are almost independent, but not quite.

It is sometimes said that probabilistic combinatorics uses the fact that whatever happens with probability greater than formula_68 must happen sometimes; one may say with equal justice that many applications of probabilistic number theory hinge on the fact that whatever is unusual must be rare. If certain algebraic objects (say, rational or integer solutions to certain equations) can be shown to be in the tail of certain sensibly defined distributions, it follows that there must be few of them; this is a very concrete non-probabilistic statement following from a probabilistic one.

At times, a non-rigorous, probabilistic approach leads to a number of heuristic algorithms and open problems, notably Cramér's conjecture.

Let "A" be a set of "N" integers. Consider the set "A" + "A" = { "m" + "n" | "m", "n" ∈ "A" } consisting of all sums of two elements of "A". Is "A + A" much larger than "A"? Barely larger? If "A + A" is barely larger than "A", must "A" have plenty of arithmetic structure, for example, does "A" resemble an arithmetic progression?

If we begin from a fairly "thick" infinite set formula_69, does it contain many elements in arithmetic progression: formula_38,
formula_71, say? Should it be possible to write large integers as sums of elements of formula_69?

These questions are characteristic of "arithmetic combinatorics". This is a presently coalescing field; it subsumes "additive number theory" (which concerns itself with certain very specific sets formula_69 of arithmetic significance, such as the primes or the squares) and, arguably, some of the "geometry of numbers",
together with some rapidly developing new material. Its focus on issues of growth and distribution accounts in part for its developing links with ergodic theory, finite group theory, model theory, and other fields. The term "additive combinatorics" is also used; however, the sets formula_69 being studied need not be sets of integers, but rather subsets of non-commutative groups, for which the multiplication symbol, not the addition symbol, is traditionally used; they can also be subsets of rings, in which case the growth of formula_75 and formula_69·formula_69 may be
compared.

While the word "algorithm" goes back only to certain readers of al-Khwārizmī, careful descriptions of methods of solution are older than proofs: such methods (that is, algorithms) are as old as any recognisable mathematics—ancient Egyptian, Babylonian, Vedic, Chinese—whereas proofs appeared only with the Greeks of the classical period.
An interesting early case is that of what we now call the Euclidean algorithm. In its basic form (namely, as an algorithm for computing the greatest common divisor) it appears as Proposition 2 of Book VII in "Elements", together with a proof of correctness. However, in the form that is often used in number theory (namely, as an algorithm for finding integer solutions to an equation formula_78,
or, what is the same, for finding the quantities whose existence is assured by the Chinese remainder theorem) it first appears in the works of Āryabhaṭa (5th–6th century CE) as an algorithm called
"kuṭṭaka" ("pulveriser"), without a proof of correctness.

There are two main questions: "can we compute this?" and "can we compute it rapidly?". Anyone can test whether a number is prime or, if it is not, split it into prime factors; doing so rapidly is another matter. We now know fast algorithms for testing primality, but, in spite of much work (both theoretical and practical), no truly fast algorithm for factoring.

The difficulty of a computation can be useful: modern protocols for encrypting messages (e.g., RSA) depend on functions that are known to all, but whose inverses (a) are known only to a chosen few, and (b) would take one too long a time to figure out on one's own. For example, these functions can be such that their inverses can be computed only if certain large integers are factorized. While many difficult computational problems outside number theory are known, most working encryption protocols nowadays are based on the difficulty of a few number-theoretical problems.

On a different note — some things may not be computable at all; in fact, this can be proven in some instances. For instance, in 1970, it was proven, as a solution to Hilbert's 10th problem, that there is no Turing machine which can solve all Diophantine equations. In particular, this means that, given a computably enumerable set of axioms, there are Diophantine equations for which there is no proof, starting from the axioms, of whether the set of equations has or does not have integer solutions. (We would necessarily be speaking of Diophantine equations for which there are no integer solutions, since, given a Diophantine equation with at least one solution, the solution itself provides a proof of the fact that a solution exists. We cannot prove, of course, that a particular Diophantine equation is of this kind, since this would imply that it has no solutions.)

The number-theorist Leonard Dickson (1874–1954) said "Thank God that number theory is unsullied by any application". Such a view is no longer applicable to number theory. In 1974, Donald Knuth said "...virtually every theorem in elementary number theory arises in a natural, motivated way in connection with the problem of making computers do high-speed numerical calculations".
Elementary number theory is taught in discrete mathematics courses for computer scientists; on the other hand, number theory also has applications to the continuous in numerical analysis. As well as the well-known applications to cryptography, there are also applications to many other areas of mathematics.

The American Mathematical Society awards the "Cole Prize in Number Theory". Moreover number theory is one of the three mathematical subdisciplines rewarded by the "Fermat Prize".


 

Two of the most popular introductions to the subject are:

Hardy and Wright's book is a comprehensive classic, though its clarity sometimes suffers due to the authors' insistence on elementary methods (Apostol n.d.).
Vinogradov's main attraction consists in its set of problems, which quickly lead to Vinogradov's own research interests; the text itself is very basic and close to minimal. Other popular first introductions are:

Popular choices for a second textbook include:



</doc>
<doc id="21530" url="https://en.wikipedia.org/wiki?curid=21530" title="Nitroglycerin">
Nitroglycerin

Nitroglycerin (NG), also known as nitroglycerine, trinitroglycerin (TNG), trinitroglycerine, nitro, glyceryl trinitrate (GTN), or 1,2,3-trinitroxypropane, is a heavy, colorless, oily, explosive liquid most commonly produced by nitrating glycerol with white fuming nitric acid under conditions appropriate to the formation of the nitric acid ester. Chemically, the substance is an organic nitrate compound rather than a nitro compound, yet the traditional name is often retained. Invented in 1847, nitroglycerin has been used as an active ingredient in the manufacture of explosives, mostly dynamite, and as such it is employed in the construction, demolition, and mining industries. Since the 1880s, it has been used by the military as an active ingredient, and a gelatinizer for nitrocellulose, in some solid propellants, such as cordite and ballistite.

Nitroglycerin is a major component in double-based smokeless gunpowders used by reloaders. Combined with nitrocellulose, there are hundreds of powder combinations used by rifle, pistol, and shotgun reloaders.

In medicine, for over 130 years nitroglycerin has been used as a potent vasodilator (dilation of the vascular system) to treat heart conditions, such as angina pectoris and chronic heart failure. Though it was previously known that these beneficial effects are due to nitroglycerin being converted to nitric oxide, a potent venodilator, it was not until 2002 that the enzyme for this conversion was discovered to be mitochondrial aldehyde dehydrogenase. Nitroglycerin is available in sublingual tablets, sprays, and patches.

Nitroglycerin was the first practical explosive produced that was stronger than black powder. It was first synthesized by the Italian chemist Ascanio Sobrero in 1847, working under Théophile-Jules Pelouze at the University of Turin. Sobrero initially called his discovery "pyroglycerine" and warned vigorously against its use as an explosive.

Nitroglycerin was later adopted as a commercially useful explosive by Alfred Nobel, who experimented with safer ways to handle the dangerous compound after his younger brother, Emil Oskar Nobel, and several factory workers were killed in an explosion at the Nobels' armaments factory in 1864 in Heleneborg, Sweden.

One year later, Nobel founded Alfred Nobel & Company in Germany and built an isolated factory in the Krümmel hills of Geesthacht near Hamburg. This business exported a liquid combination of nitroglycerin and gunpowder called "Blasting Oil", but this was extremely unstable and difficult to handle, as evidenced in numerous catastrophes. The buildings of the Krümmel factory were destroyed twice.

In April 1866, three crates of nitroglycerin were shipped to California for the Central Pacific Railroad, which planned to experiment with it as a blasting explosive to expedite the construction of the Summit Tunnel through the Sierra Nevada Mountains. One of the crates exploded, destroying a Wells Fargo company office in San Francisco and killing 15 people. This led to a complete ban on the transportation of liquid nitroglycerin in California. The on-site manufacture of nitroglycerin was thus required for the remaining hard-rock drilling and blasting required for the completion of the First Transcontinental Railroad in North America.

Liquid nitroglycerin was widely banned elsewhere as well, and these legal restrictions led to Alfred Nobel and his company's developing dynamite in 1867. This was made by mixing nitroglycerin with diatomaceous earth (""kieselgur"" in German) found in the Krümmel hills. Similar mixtures, such as "dualine" (1867), "lithofracteur" (1869), and "gelignite" (1875), were formed by mixing nitroglycerin with other inert absorbents, and many combinations were tried by other companies in attempts to get around Nobel's tightly held patents for dynamite.

Dynamite mixtures containing nitrocellulose, which increases the viscosity of the mix, are commonly known as "gelatins".

Following the discovery that amyl nitrite helped alleviate chest pain, the physician William Murrell experimented with the use of nitroglycerin to alleviate angina pectoris and to reduce the blood pressure. He began treating his patients with small diluted doses of nitroglycerin in 1878, and this treatment was soon adopted into widespread use after Murrell published his results in the journal "The Lancet" in 1879. A few months before his death in 1896, Alfred Nobel was prescribed nitroglycerin for this heart condition, writing to a friend: "Isn't it the irony of fate that I have been prescribed nitro-glycerin, to be taken internally! They call it Trinitrin, so as not to scare the chemist and the public." The medical establishment also used the name "glyceryl trinitrate" for the same reason.

Large quantities of nitroglycerin were manufactured during World War I and World War II for use as military propellants and in military engineering work. During World War I, HM Factory, Gretna, the largest propellant factory in Britain, produced about 800 tonnes of cordite RDB per week. This amount required at least 336 tonnes of nitroglycerin per week (assuming no losses in production). The Royal Navy had its own factory at the Royal Navy Cordite Factory, Holton Heath in Dorset, England. A large cordite factory was also built in Canada during World War I. The Canadian Explosives Limited cordite factory at Nobel, Ontario, was designed to produce of cordite per month, requiring about 286 tonnes of nitroglycerin per month.

In its pure form, nitroglycerin is a contact explosive, with physical shock causing it to explode, and it degrades over time to even more unstable forms. This makes nitroglycerin highly dangerous to transport or use. In its undiluted form, it is one of the world's most powerful explosives, comparable to the more recently developed RDX and PETN.

Early in its history, it was discovered that liquid nitroglycerin can be "desensitized" by cooling it to about . At this temperature nitroglycerin freezes, contracting upon solidification. Thawing it out can be extremely sensitizing, especially if impurities are present or the warming is too rapid. It is possible to chemically "desensitize" nitroglycerin to a point where it can be considered approximately as "safe" as modern high explosives, such as by the addition of approximately 10% to 30% ethanol, acetone, or dinitrotoluene. (The percentage varies with the desensitizing agent used.) Desensitization requires extra effort to reconstitute the "pure" product. Failing this, it must be assumed that desensitized nitroglycerin is substantially more difficult to detonate, possibly rendering it useless as an explosive for practical application.

A serious problem in the use of nitroglycerin results from its high freezing point of . Solid nitroglycerin is much less sensitive to shock than the liquid, a feature that is common in explosives. In the past, nitroglycerin was often shipped in the frozen state, but this resulted in a high number of accidents during the thawing process just before its use. This disadvantage is overcome by using mixtures of nitroglycerin with other polynitrates. For example, a mixture of nitroglycerin and ethylene glycol dinitrate freezes at .

Nitroglycerin and any diluents can certainly deflagrate (burn). The explosive power of nitroglycerin derives from detonation: energy from the initial decomposition causes a strong pressure wave that detonates the surrounding fuel. This is a self-sustained shock wave that propagates through the explosive medium at 30 times the speed of sound as a near-instantaneous pressure-induced decomposition of the fuel into a white-hot gas. Detonation of nitroglycerin generates gases that would occupy more than 1,200 times the original volume at ordinary room temperature and pressure. The heat liberated raises the temperature to about . This is entirely different from deflagration, which depends solely upon available fuel regardless of pressure or shock. The decomposition results in much higher ratio of energy to gas moles released compared to other explosives, making it one of the hottest detonating high explosives.

Nitroglycerin can be produced by acid catalyzed nitration of glycerol (glycerin).

The industrial manufacturing process often reacts glycerol with a nearly 1:1 mixture of concentrated sulfuric acid and concentrated nitric acid. This can be produced by mixing white fuming nitric acid—a quite expensive pure nitric acid in which the oxides of nitrogen have been removed, as opposed to red fuming nitric acid, which contains nitrogen oxides—and concentrated sulfuric acid. More often, this mixture is attained by the cheaper method of mixing fuming sulfuric acid, also known as oleum—sulfuric acid containing excess sulfur trioxide—and azeotropic nitric acid (consisting of about 70% nitric acid, with the rest being water).

The sulfuric acid produces protonated nitric acid species, which are attacked by glycerol's nucleophilic oxygen atoms. The nitro group is thus added as an ester C−O−NO and water is produced. This is different from an electrophilic aromatic substitution reaction in which nitronium ions are the electrophile.

The addition of glycerol results in an exothermic reaction (i.e., heat is produced), as usual for mixed-acid nitrations. If the mixture becomes too hot, it results in a runaway reaction, a state of accelerated nitration accompanied by the destructive oxidation of organic materials by the hot nitric acid and the release of poisonous nitrogen dioxide gas at high risk of an explosion. Thus, the glycerin mixture is added slowly to the reaction vessel containing the mixed acid (not acid to glycerin). The nitrator is cooled with cold water or some other coolant mixture and maintained throughout the glycerin addition at about , much below which the esterification occurs too slowly to be useful. The nitrator vessel, often constructed of iron or lead and generally stirred with compressed air, has an emergency trap door at its base, which hangs over a large pool of very cold water and into which the whole reaction mixture (called the charge) can be dumped to prevent an explosion, a process referred to as drowning. If the temperature of the charge exceeds about (actual value varying by country) or brown fumes are seen in the nitrator's vent, then it is immediately drowned.

The main use of nitroglycerin, by tonnage, is in explosives such as dynamite and in propellants.

Nitroglycerin is an oily liquid that may explode when subjected to heat, shock or flame.

Alfred Nobel developed the use of nitroglycerin as a blasting explosive by mixing nitroglycerin with inert absorbents, particularly ""kieselguhr"", or diatomaceous earth. He named this explosive dynamite and patented it in 1867. It was supplied ready for use in the form of sticks, individually wrapped in greased waterproof paper. Dynamite and similar explosives were widely adopted for civil engineering tasks, such as in drilling highway and railroad tunnels, for mining, for clearing farmland of stumps, in quarrying, and in demolition work. Likewise, military engineers have used dynamite for construction and demolition work.

Nitroglycerin was also used as an ingredient in military propellants for use in firearms.

Nitroglycerin has been used in conjunction with hydraulic fracturing, a process used to recover oil and gas from shale formations. The technique involves displacing and detonating nitroglycerin in natural or hydraulically-induced fracture systems, or displacing and detonating nitroglycerin in hydraulically-induced fractures followed by wellbore shots using pelletized TNT.

Nitroglycerin has an advantage over some other high explosives, that on detonation it produces practically no visible smoke. Therefore, it is useful as an ingredient in the formulation of various kinds of smokeless powder.

Its sensitivity has limited the usefulness of nitroglycerin as a military explosive, and less sensitive explosives such as TNT, RDX, and HMX have largely replaced it in munitions. It remains important in military engineering, and combat engineers still use dynamite.

Alfred Nobel then developed ballistite, by combining nitroglycerin and guncotton. He patented it in 1887. Ballistite was adopted by a number of European governments, as a military propellant. Italy was the first to adopt it. The British Government and the Commonwealth governments adopted cordite instead, which had been developed by Sir Frederick Abel and Sir James Dewar of the United Kingdom in 1889. The original Cordite Mk I consisted of 58% nitroglycerin, 37% guncotton, and 5.0% petroleum jelly. Ballistite and cordite were both manufactured in the forms of "cords".

Smokeless powders were originally developed using nitrocellulose as the sole explosive ingredient. Therefore, they were known as "single-base" propellants. A range of smokeless powders that contain both nitrocellulose and nitroglycerin, known as "double-base" propellants, were also developed. Smokeless powders were originally supplied only for military use, but they were also soon developed for civilian use and were quickly adopted for sports. Some are known as sporting powders. "Triple-base" propellants contain nitrocellulose, nitroglycerin, and nitroguanidine, but are reserved mainly for extremely high-caliber ammunition rounds such as those used in tank cannons and naval artillery.

Blasting gelatin, also known as gelignite, was invented by Nobel in 1875, using nitroglycerin, wood pulp, and sodium or potassium nitrate. This was an early low-cost, flexible explosive.

Nitroglycerin belongs to a group of drugs called nitrates, which includes many other nitrates like isosorbide dinitrate (Isordil) and isosorbide mononitrate (Imdur, Ismo, Monoket). These agents all exert their effect by being converted to nitric oxide in the body by mitochondrial aldehyde dehydrogenase, and nitric oxide is a potent natural vasodilator.

In medicine, nitroglycerin is used as a medicine for angina pectoris, a painful symptom of ischemic heart disease caused by inadequate flow of blood and oxygen to the heart and as a potent antihypertensive agent. Nitroglycerin corrects the imbalance between the flow of oxygen and blood to the heart. At low doses, nitroglycerin will dilate veins more than arteries, thereby reducing preload (volume of blood in the heart after filling); this is thought to be its primary mechanism of action. By decreasing preload, the heart has less blood to pump which decreases oxygen requirement since the heart does not have to work as hard. Additionally, having a smaller preload reduces the ventricular transmural pressure (pressure exerted on the walls of the heart), which decreases the compression of heart arteries to allow more blood to flow through the heart. At higher doses, it also dilates arteries, thereby reducing afterload (decreasing the pressure against which the heart must pump). Improved myocardial oxygen demand vs oxygen delivery ratio leads to the following therapeutic effects during episodes of angina pectoris: subsiding of chest pain, decrease of blood pressure, increase of heart rate, and orthostatic hypotension. Patients experiencing angina when doing certain physical activities can often prevent symptoms by taking nitroglycerin 5 to 10 minutes before the activity. Overdoses may generate methemoglobinemia.

Nitroglycerin is available in tablets, ointment, solution for intravenous use, transdermal patches, or sprays administered sublingually. Some forms of nitroglycerin last much longer in the body than others. It has been shown that continuous exposure to nitrates can cause the body to stop responding normally to this medicine. Experts recommend that the patches be removed at night, allowing the body a few hours to restore its responsiveness to nitrates. Shorter-acting preparations of nitroglycerin can be used several times a day with less risk of developing tolerance. Nitroglycerin was first used by William Murrell to treat angina attacks in 1878, with the discovery published that same year.

Infrequent exposure to high doses of nitroglycerin can cause severe headaches known as "NG head" or "bang head". These headaches can be severe enough to incapacitate some people; however, humans develop a tolerance to and dependence on nitroglycerin after long-term exposure. Withdrawal can (rarely) be fatal; withdrawal symptoms include chest pain and heart problems and if unacceptable may be treated with re-exposure to nitroglycerin or other suitable organic nitrates.

For workers in nitroglycerin (NTG) manufacturing facilities, the effects of withdrawal sometimes include "Sunday heart attacks" in those experiencing regular nitroglycerin exposure in the workplace, leading to the development of tolerance for the venodilating effects. Over the weekend, the workers lose the tolerance and, when they are re-exposed on Monday, the drastic vasodilation produces a fast heart rate, dizziness, and a headache, this is referred to as "Monday Disease."

People can be exposed to nitroglycerin in the workplace by breathing it in, skin absorption, swallowing it, or eye contact. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for nitroglycerin exposure in the workplace as 0.2 ppm (2 mg/m) skin exposure over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.1 mg/m skin exposure over an 8-hour workday. At levels of 75 mg/m, nitroglycerin is immediately dangerous to life and health.



</doc>
<doc id="21533" url="https://en.wikipedia.org/wiki?curid=21533" title="Navy">
Navy

A navy or maritime force is the branch of a nation's armed forces principally designated for naval and amphibious warfare; namely, lake-borne, riverine, littoral, or ocean-borne combat operations and related functions. It includes anything conducted by surface ships, amphibious ships, submarines, and seaborne aviation, as well as ancillary support, communications, training, and other fields. The strategic offensive role of a navy is projection of force into areas beyond a country's shores (for example, to protect sea-lanes, ferry troops, or attack other navies, ports, or shore installations). The strategic defensive purpose of a navy is to frustrate seaborne projection-of-force by enemies. The strategic task of the navy also may incorporate nuclear deterrence by use of submarine-launched ballistic missiles. Naval operations can be broadly divided between riverine and littoral applications (brown-water navy), open-ocean applications (blue-water navy), and something in between (green-water navy), although these distinctions are more about strategic scope than tactical or operational division.

In most nations, the term "naval", as opposed to "navy", is interpreted as encompassing all maritime military forces, e.g., navy, naval infantry/marine corps, and coast guard forces.

First attested in English in the early 14th century, the word "navy" came via Old French "navie", "fleet of ships", from the Latin "navigium", "a vessel, a ship, bark, boat", from "navis", "ship". The word "naval" came from Latin "navalis", "pertaining to ship"; cf. Greek ("naus"), "ship", ("nautes"), "seaman, sailor". The earliest attested form of the word is in the Mycenaean Greek compound word , "na-u-do-mo" (*), "shipbuilders", written in Linear B syllabic script.

The word formerly denoted fleets of both commercial and military nature. In modern usage "navy" used alone always denotes a military fleet, although the term "merchant navy" for a commercial fleet still incorporates the non-military word sense. This overlap in word senses between commercial and military fleets grew out of the inherently dual-use nature of fleets; centuries ago, nationality was a trait that unified a fleet across both civilian and military uses. Although nationality of commercial vessels has little importance in peacetime trade other than for tax avoidance, it can have greater meaning during wartime, when supply chains become matters of patriotic attack and defense, and when in some cases private vessels are even temporarily converted to military vessels. The latter was especially important, and common, before 20th-century military technology existed, when merely adding artillery and naval infantry to any sailing vessel could render it fully as martial as any military-owned vessel. Such privateering has been rendered obsolete in blue-water strategy since modern missile and aircraft systems grew to leapfrog over artillery and infantry in many respects; but privateering nevertheless remains potentially relevant in littoral warfare of a limited and asymmetric nature.

Naval warfare developed when humans first fought from water-borne vessels. Prior to the introduction of the cannon and ships with sufficient capacity to carry the large guns, navy warfare primarily involved ramming and boarding actions. In the time of ancient Greece and the Roman Empire, naval warfare centered on long, narrow vessels powered by banks of oarsmen (such as triremes and quinqueremes) designed to ram and sink enemy vessels or come alongside the enemy vessel so its occupants could be attacked hand-to-hand. Naval warfare continued in this vein through the Middle Ages until the cannon became commonplace and capable of being reloaded quickly enough to be reused in the same battle. The Chola Dynasty of medieval India was known as one of the greatest naval powers of its time from 300 BC to 1279 AD. The Chola Navy, Chola kadarpadai comprised the naval forces of the Chola Empire along with several other Naval-arms of the country. The Chola navy played a vital role in the expansion of the Chola Tamil kingdom, including the conquest of the Sri Lanka islands, Kadaaram (Present day Burma), Sri Vijaya (present day Southeast Asia), the spread of Hinduism, Tamil architecture and Tamil culture to Southeast Asia and in curbing the piracy in Southeast Asia in 900 CE. In ancient China, large naval battles were known since the Qin Dynasty ("also see" Battle of Red Cliffs, 208), employing the war junk during the Han Dynasty. However, China's first official standing navy was not established until the Southern Song dynasty in the 12th century, a time when gunpowder was a revolutionary new application to warfare.

The mass and deck space required to carry a large number of cannon made oar-based propulsion impossible, and ships came to rely primarily on sails. Warships were designed to carry increasing numbers of cannon and naval tactics evolved to bring a ship's firepower to bear in a broadside, with ships-of-the-line arranged in a line of battle.

The development of large capacity, sail-powered ships carrying cannon led to a rapid expansion of European navies, especially the Spanish and Portuguese navies which dominated in the 16th and early 17th centuries, and helped propel the age of exploration and colonialism. The repulsion of the Spanish Armada (1588) by the English fleet revolutionized naval warfare by the success of a guns-only strategy and caused a major overhaul of the Spanish Navy, partly along English lines, which resulted in even greater dominance by the Spanish. From the beginning of the 17th century the Dutch cannibalized the Portuguese Empire in the East and, with the immense wealth gained, challenged Spanish hegemony at sea. From the 1620s, Dutch raiders seriously troubled Spanish shipping and, after a number of battles which went both ways, the Dutch Navy finally broke the long dominance of the Spanish Navy in the Battle of the Downs (1639).

England emerged as a major naval power in the mid-17th century in the first Anglo-Dutch war with a technical victory. Successive decisive Dutch victories in the second and third Anglo-Dutch Wars confirmed the Dutch mastery of the seas during the Dutch Golden Age, financed by the expansion of the Dutch Empire. The French Navy won some important victories near the end of the 17th century but a focus upon land forces led to the French Navy's relative neglect, which allowed the Royal Navy to emerge with an ever-growing advantage in size and quality, especially in tactics and experience, from 1695. Throughout the 18th century the Royal Navy gradually gained ascendancy over the French Navy, with victories in the War of Spanish Succession (1701–1714), inconclusive battles in the War of Austrian Succession (1740–1748), victories in the Seven Years' War (1754–1763), a partial reversal during the American War of Independence (1775–1783), and consolidation into uncontested supremacy during the 19th century from the Battle of Trafalgar in 1805. These conflicts saw the development and refinement of tactics which came to be called the line of battle.
The next stage in the evolution of naval warfare was the introduction of metal plating along the hull sides. The increased mass required steam-powered engines, resulting in an arms race between armor and weapon thickness and firepower. The first armored vessels, the French and British , made wooden vessels obsolete. Another significant improvement came with the invention of the rotating turrets, which allowed the guns to be aimed independently of ship movement. The battle between and during the American Civil War (1861–1865) is often cited as the beginning of this age of maritime conflict. The Russian Navy was considered the third strongest in the world on the eve of the Russo-Japanese War, which turned to be a catastrophe for the Russian military in general and the Russian Navy in particular. Although neither party lacked courage, the Russians were defeated by the Japanese in the Battle of Port Arthur, which was the first time in warfare that mines were used for offensive purposes. The warships of the Baltic Fleet sent to the Far East were lost in the Battle of Tsushima. A further step change in naval firepower occurred when the United Kingdom launched in 1906, but naval tactics still emphasized the line of battle.

The first practical military submarines were developed in the late 19th century and by the end of World War I had proven to be a powerful arm of naval warfare. During World War II, Nazi Germany's submarine fleet of U-boats almost starved the United Kingdom into submission and inflicted tremendous losses on U.S. coastal shipping. The , a sister ship of , was almost put out of action by miniature submarines known as X-Craft. The X-Craft severely damaged her and kept her in port for some months.

A major paradigm shift in naval warfare occurred with the introduction of the aircraft carrier. First at Taranto in 1940 and then at Pearl Harbor in 1941, the carrier demonstrated its ability to strike decisively at enemy ships out of sight and range of surface vessels. The Battle of Leyte Gulf (1944) was arguably the largest naval battle in history; it was also the last battle in which battleships played a significant role. By the end of World War II, the carrier had become the dominant force of naval warfare.

World War II also saw the United States become by far the largest Naval power in the world. In the late 20th and early 21st centuries, the United States Navy possessed over 70% of the world's total numbers and total tonnage of naval vessels of 1,000 tons or greater. Throughout the rest of the 20th century, the United States Navy would maintain a tonnage greater than that of the next 17 largest navies combined. During the Cold War, the Soviet Navy became a significant armed force, with large numbers of large, heavily armed ballistic missile submarines and extensive use of heavy, long-ranged antisurface missiles to counter the numerous United States carrier battle groups. Only three nations (United States, France, and Brazil) presently operate CATOBAR carriers of any size, while Russia, China and India operate sizeable STOBAR carriers (although all three are originally of Russian design). The United Kingdom is also currently constructing two carriers, which will be the largest STOVL vessels in service, and India is currently building two s (the second one with CATOBAR technology) and considering another. France is also looking at a new carrier, probably using a CATOBAR system and possibly based on the British "Queen Elizabeth" design.

A navy typically operates from one or more naval bases. The base is a port that is specialized in naval operations, and often includes housing, a munitions depot, docks for the vessels, and various repair facilities. During times of war temporary bases may be constructed in closer proximity to strategic locations, as it is advantageous in terms of patrols and station-keeping. Nations with historically strong naval forces have found it advantageous to obtain basing rights in other countries in areas of strategic interest.

Navy ships can operate independently or with a group, which may be a small squadron of comparable ships, or a larger naval fleet of various specialized ships. The commander of a fleet travels in the flagship, which is usually the most powerful vessel in the group. Prior to the invention of radio, commands from the flagship were communicated by means of flags. At night signal lamps could be used for a similar purpose. Later these were replaced by the radio transmitter, or the flashing light when radio silence was needed.

A "blue water navy" is designed to operate far from the coastal waters of its home nation. These are ships capable of maintaining station for long periods of time in deep ocean, and will have a long logistical tail for their support. Many are also nuclear powered to save having to refuel. By contrast a "brown water navy" operates in the coastal periphery and along inland waterways, where larger ocean-going naval vessels can not readily enter. Regional powers may maintain a "green water navy" as a means of localized force projection. Blue water fleets may require specialized vessels, such as minesweepers, when operating in the littoral regions along the coast.

A basic tradition is that all ships commissioned in a navy are referred to as ships rather than vessels, with the exception of destroyers and submarines, which are known as boats. The prefix on a ship's name indicates that it is a commissioned ship.

An important tradition on board naval vessels of some nations has been the ship's bell. This was historically used to mark the passage of time, as warning devices in heavy fog, and for alarms and ceremonies.

The ship's captain, and more senior officers are "piped" aboard the ship using a Boatswain's call.

In the United States, the First Navy Jack is a flag that has the words, "Don't Tread on Me" on the flag.

By English tradition, ships have been referred to as a "she". However, it was long considered bad luck to permit women to sail on board naval vessels. To do so would invite a terrible storm that would wreck the ship. The only women that were welcomed on board were figureheads mounted on the prow of the ship.

Firing a cannon salute partially disarms the ship, so firing a cannon for no combat reason showed respect and trust. As the tradition evolved, the number of cannon fired became an indication of the rank of the official being saluted.

Historically, navy ships were primarily intended for warfare. They were designed to withstand damage and to inflict the same, but only carried munitions and supplies for the voyage (rather than merchant cargo). Often, other ships which were not built specifically for warfare, such as the galleon or the armed merchant ships in World War II, did carry armaments. In more recent times, navy ships have become more specialized and have included supply ships, troop transports, repair ships, oil tankers and other logistics support ships as well as combat ships.

Modern navy combat ships are generally divided into seven main categories: aircraft carriers, cruisers, destroyers, frigates, corvettes, submarines, and amphibious assault ships. There are also support and auxiliary ships, including the oiler, minesweeper, patrol boat, hydrographic and oceanographic survey ship and tender. During the age of sail, the ship categories were divided into the ship of the line, frigate, and sloop-of-war.

Naval ship names are typically prefixed by an abbreviation indicating the national navy in which they serve. For a list of the prefixes used with ship names (HMS, USS, LÉ, etc.) see ship prefix.

Today ships are significantly faster than in former times, thanks to much improved propulsion systems. Also, the efficiency of the engines has improved, in terms of fuel, and of how many sailors it takes to operate them. In World War II, ships needed to refuel very often. However, today ships can go on very long journeys without refueling. Also, in World War II, the engine room needed about a dozen sailors to work the many engines, however, today, only about 4–5 are needed (depending on the class of the ship). Today, naval strike groups on longer missions are always followed by a range of support and replenishment ships supplying them with anything from fuel and munitions, to medical treatment and postal services. This allows strike groups and combat ships to remain at sea for several months at a time.

The term "boat" refers to small craft limited in their use by size and usually not capable of making lengthy independent voyages at sea. The old navy adage to differentiate between ships and boats is that boats are capable of being carried by ships. (Submarines by this rule are ships rather than boats, but are customarily referred to as boats reflecting their previous smaller size.)

Navies use many types of boat, ranging from dinghies to landing craft. They are powered by either diesel engines, out-board gasoline engines, or waterjets. Most boats are built of aluminum, fiberglass, or steel. Rigid-hulled inflatable boats are also used.

Patrol boats are used for patrols of coastal areas, lakes and large rivers.
Landing craft are designed to carry troops, vehicles, or cargo from ship to shore under combat conditions, to unload, to withdraw from the beach, and to return to the ship. They are rugged, with powerful engines, and usually armed. There are many types in today's navies including hovercraft. They will typically have a power-operated bow ramp, a cargo well and after structures that house engine rooms, pilot houses, and stowage compartments. These boats are sometimes carried by larger ships.

Special operations craft are high-speed craft used for insertion and extraction of special forces personnel and some may be transportable (and deployed) by air.

Boats used in non-combat roles include lifeboats, mail boats, line handling boats, buoy boats, aircraft rescue boats, torpedo retrievers, explosive ordnance disposal craft, utility boats, dive boats, targets, and work boats. Boats are also used for survey work, tending divers, and minesweeping operations. Boats for carrying cargo and personnel are sometimes known as launches, gigs, barges or shore party boats.

Naval forces are typically arranged into units based on the number of ships included, a single ship being the smallest operational unit. Ships may be combined into squadrons or flotillas, which may be formed into fleets. The largest unit size may be the whole Navy or Admiralty.

A task force can be assembled using ships from different fleets for an operational task.

Despite their acceptance in many areas of naval service, female sailors were not permitted to serve on board U.S. submarines until the U.S. Navy lifted the ban in April 2010. The major reasons historically cited by the U.S. Navy were the extended duty tours and close conditions which afford almost no privacy. The United Kingdom's Royal Navy has had similar restrictions. Australia, Canada, Norway, and Spain previously opened submarine service to women sailors.

A navy will typically have two sets of ranks, one for enlisted personnel and one for officers.

Typical ranks for commissioned officers include the following, in ascending order (Commonwealth ranks are listed first on each line; USA ranks are listed second in those instances where they differ from Commonwealth ranks):

"Flag officers" include any rank that includes the word "admiral" (or commodore in services other than the US Navy), and are generally in command of a battle group, strike group or similar flotilla of ships, rather than a single ship or aspect of a ship. However, commodores can also be temporary or honorary positions. For example, during World War II, a Navy captain was assigned duty as a convoy commodore, which meant that he was still a captain, but in charge of all the merchant vessels in the convoy.

The most senior rank employed by a navy will tend to vary depending on the size of the navy and whether it is wartime or peacetime, for example, few people have ever held the rank of Fleet Admiral in the U.S. Navy, the chief of the Royal Australian Navy holds the rank of Vice Admiral, and the chief of the Irish Naval Service holds the rank of Commodore.

Naval infantry, commonly known as marines, are a category of infantry that form part of a state’s naval forces and perform roles on land and at sea, including amphibious operations, as well as other, naval roles. They also perform other tasks, including land warfare, separate from naval operations.

During the era of the Roman empire, naval forces included marine legionaries for maritime boarding actions. These were troops primarily trained in land warfare, and did not need to be skilled at handling a ship. Much later during the age of sail, a component of marines served a similar role, being ship-borne soldiers who were used either during boarding actions, as sharp-shooters, or in raids along shorelines.

The Spanish "Infantería de Marina" was formed in 1537, making it the oldest, current marine force in the world. The British Royal Marines combine being both a ship-based force and also being specially trained in commando-style operations and tactics, operating in some cases separately from the rest of the Royal Navy. The Royal Marines also have their own special forces unit.

In the majority of countries, the marine force is an integral part of the navy. The United States Marine Corps is a separate armed service within the United States Department of the Navy, with its own leadership structure.

Naval aviation is the application of military air power by navies, whether from warships that embark aircraft, or land bases.

In World War I several navies used floatplanes and flying boats - mainly for scouting. By World War II, aircraft carriers could carry bomber aircraft capable of attacking naval and land targets, as well as fighter aircraft for defence. Since World War II helicopters have been embarked on smaller ships in roles such as anti-submarine warfare and transport. Some navies have also operated land-based aircraft in roles such as maritime patrol and training.

Naval aviation forces primarily perform naval roles at sea. However, they are also used in a variety of other roles.





</doc>
<doc id="21538" url="https://en.wikipedia.org/wiki?curid=21538" title="Normed vector space">
Normed vector space

In mathematics, a normed vector space is a vector space over the real or complex numbers, on which a norm is defined. A norm is the formalization and the generalization to real vector spaces of the intuitive notion of distance in the real world. A norm is a real-valued function defined on the vector space that has the following properties:


The generalization of these three properties to more abstract vector spaces leads to the notion of norm. A vector space on which a norm is defined is then called a normed space or normed vector space.
Normed vector spaces are central to the study of linear algebra and functional analysis.

A normed vector space is a pair formula_7 where formula_8 is a vector space and formula_9 a norm on formula_8.

A seminormed vector space is a pair formula_11 where formula_8 is a vector space and formula_13 a seminorm on formula_8.

We often omit formula_13 or formula_9 and just write formula_8 for a space if it is clear from the context what (semi) norm we are using.

In a more general sense, a vector norm can be taken to be any real-valued function that satisfies the three properties above.

A useful variation of the triangle inequality is

This also shows that a vector norm is a continuous function.

Note that property 2 depends on a choice of norm formula_19 on the field of scalars. When the scalar field is formula_20 (or more generally a subset of formula_21), this is usually taken to be the ordinary absolute value, but other choices are possible. For example, for a vector space over formula_22 one could take formula_19 to be the "p"-adic norm, which gives rise to a different class of normed vector spaces.

If ("V", ‖·‖) is a normed vector space, the norm ‖·‖ induces a metric (a notion of "distance") and therefore a topology on "V". This metric is defined in the natural way: the distance between two vectors u and v is given by ‖u−v‖. This topology is precisely the weakest topology which makes ‖·‖ continuous and which is compatible with the linear structure of "V" in the following sense:


Similarly, for any semi-normed vector space we can define the distance between two vectors u and v as ‖u−v‖. This turns the seminormed space into a pseudometric space (notice this is weaker than a metric) and allows the definition of notions such as continuity and convergence.
To put it more abstractly every semi-normed vector space is a topological vector space and thus carries a topological structure which is induced by the semi-norm.

Of special interest are complete normed spaces called Banach spaces. Every normed vector space "V" sits as a dense subspace inside a Banach space; this Banach space is essentially uniquely defined by "V" and is called the "completion" of "V".

All norms on a finite-dimensional vector space are equivalent from a topological viewpoint as they induce the same topology (although the resulting metric spaces need not be the same). And since any Euclidean space is complete, we can thus conclude that all finite-dimensional normed vector spaces are Banach spaces. A normed vector space "V" is locally compact if and only if the unit ball "B" = {"x" : ‖"x"‖ ≤ 1} is compact, which is the case if and only if "V" is finite-dimensional; this is a consequence of Riesz's lemma. (In fact, a more general result is true: a topological vector space is locally compact if and only if it is finite-dimensional.
The point here is that we don't assume the topology comes from a norm.)

The topology of a seminormed vector space has many nice properties. Given a neighbourhood system formula_24 around 0 we can construct all other neighbourhood systems as
with

Moreover there exists a neighbourhood basis for 0 consisting of absorbing and convex sets. As this property is very useful in functional analysis, generalizations of normed vector spaces with this property are studied under the name locally convex spaces.

The most important maps between two normed vector spaces are the continuous linear maps. Together with these maps, normed vector spaces form a category.

The norm is a continuous function on its vector space. All linear maps between finite dimensional vector spaces are also continuous.

An "isometry" between two normed vector spaces is a linear map "f" which preserves the norm (meaning ‖"f"(v)‖ = ‖v‖ for all vectors v). Isometries are always continuous and injective. A surjective isometry between the normed vector spaces "V" and "W" is called an "isometric isomorphism", and "V" and "W" are called "isometrically isomorphic". Isometrically isomorphic normed vector spaces are identical for all practical purposes.

When speaking of normed vector spaces, we augment the notion of dual space to take the norm into account. The dual "V" ' of a normed vector space "V" is the space of all "continuous" linear maps from "V" to the base field (the complexes or the reals) — such linear maps are called "functionals". The norm of a functional φ is defined as the supremum of |φ(v)| where v ranges over all unit vectors (i.e. vectors of norm 1) in "V". This turns "V" ' into a normed vector space. An important theorem about continuous linear functionals on normed vector spaces is the Hahn–Banach theorem.

The definition of many normed spaces (in particular, Banach spaces) involves a seminorm defined on a vector space and then the normed space is defined as the quotient space by the subspace of elements of seminorm zero. For instance, with the L spaces, the function defined by
is a seminorm on the vector space of all functions on which the Lebesgue integral on the right hand side is defined and finite. However, the seminorm is equal to zero for any function supported on a set of Lebesgue measure zero. These functions form a subspace which we "quotient out", making them equivalent to the zero function.

Given "n" seminormed spaces "X" with seminorms "q" we can define the product space as
with vector addition defined as
and scalar multiplication defined as

We define a new function "q"
for example as
which is a seminorm on "X". The function "q" is a norm if and only if all "q" are norms.

More generally, for each real "p"≥1 we have the seminorm:

For each p this defines the same topological space.

A straightforward argument involving elementary linear algebra shows that the only finite-dimensional seminormed spaces are those arising as the product space of a normed space and a space with trivial seminorm. Consequently, many of the more interesting examples and applications of seminormed spaces occur for infinite-dimensional vector spaces.




</doc>
<doc id="21541" url="https://en.wikipedia.org/wiki?curid=21541" title="Nicene Creed">
Nicene Creed

The Nicene Creed (Greek: or, , Latin: ) is a statement of belief widely used in Christian liturgy. It is called "Nicene" because it was originally adopted in the city of Nicaea (present day İznik, Turkey) by the First Council of Nicaea in 325. In 381, it was amended at the First Council of Constantinople, and the amended form is referred to as the Nicene or the Niceno-Constantinopolitan Creed.

The Oriental Orthodox and Assyrian churches use this profession of faith with the verbs in the original plural ("we believe") form, but the Eastern Orthodox and Catholic churches convert those verbs to the singular ("I believe"). The Anglican and many Protestant denominations generally use the singular form, but sometimes use the plural.

The Apostles' Creed is also used in the Latin West, but not in the Eastern liturgies. On Sundays and some other days, one or other of these two creeds is recited in the Roman Rite Mass after the homily. The Nicene Creed is also part of the profession of faith required of those undertaking important functions within the Catholic Church.

In the Byzantine Rite, the Nicene Creed is sung or recited at the Divine Liturgy, immediately preceding the Anaphora (Eucharistic Prayer), and is also recited daily at compline.

The purpose of a creed is to provide a doctrinal statement of correct belief or orthodoxy. The creeds of Christianity have been drawn up at times of conflict about doctrine: acceptance or rejection of a creed served to distinguish believers and deniers of a particular doctrine or set of doctrines. For that reason a creed was called in Greek a σύμβολον ("symbolon"), a word that originally meant half of a broken object which, when placed together with the other half, verified the bearer's identity. The Greek word passed through Latin "symbolum" into English "symbol", which only later took on the meaning of an outward sign of something.

The Nicene Creed was adopted in the face of the Arian controversy, whose leader, Arius, was a member of the clergy of Alexandria. "Arius objected to Alexander's (the bishop of the time) apparent carelessness in blurring the distinction of nature between the Father and the Son by his emphasis on eternal generation". Alexander accused Arius of denying the divinity of the Son and also of being too "Jewish" and "Greek" in his thought. Both Arius and Alexander rejected Gnosticism, Manichaeism and Sabellian formulae. The Nicene Creed was created as a result of the extensive adoption of the doctrine of Arius far outside Alexandria, in order to clarify the key tenets of the Christian faith.

The Nicene Creed of 325 explicitly affirms the co-essential divinity of the Son, applying to him the term "consubstantial". The 381 version speaks of the Holy Spirit as worshipped and glorified with the Father and the Son. The Athanasian Creed (not used in Eastern Christianity) describes in much greater detail the relationship between Father, Son and Holy Spirit. The Apostles' Creed makes no explicit statements about the divinity of the Son and the Holy Spirit, but, in the view of many who use it, the doctrine is implicit in it.

The original Nicene Creed was first adopted in 325 at the First Council of Nicaea. At that time, the text ended with the words "We believe in the Holy Spirit", after which various anathemas against Arian propositions were added.

F. J. A. Hort and Adolf Harnack argued that the Nicene creed was the local creed of Caesarea (an important center of Early Christianity) recited in the council by Eusebius of Caesarea. Their case relied largely on a very specific interpretation of Eusebius' own account of the Council's proceedings. More recent scholarship has not been convinced by their arguments. The large number of secondary divergences from the text of the creed quoted by Eusebius make it unlikely that it was used as a starting point by those who drafted the conciliar creed. Their initial text was probably a local creed from a Syro–Palestinian source into which they awkwardly inserted phrases to define the Nicene theology. The Eusebian Creed may thus have been either a second or one of many nominations for the Nicene Creed.

Soon after the Council of Nicaea, new formulae of faith were composed, most of them variations of the Nicene Symbol, to counter new phases of Arianism. The "Catholic Encyclopedia" identifies at least four before the Council of Sardica (341), where a new form was presented and inserted in the Acts of the Council, though it was not agreed on.

What is known as the "Niceno-Constantinopolitan Creed" or the "Nicene-Constantinopolitan Creed" received this name because of a belief that it was adopted at the Second Ecumenical Council held in Constantinople in 381 as a modification of the original Nicene Creed of 325. In that light, it also came to be very commonly known simply as the "Nicene Creed". It is the only authoritative "ecumenical" statement of the Christian faith accepted by the Roman Catholic, Eastern Orthodox, Oriental Orthodox, Anglican, and the major Protestant denominations. (The Apostles' and Athanasian creeds are not as widely accepted.)

It differs in a number of respects, both by addition and omission, from the creed adopted at the First Council of Nicaea. The most notable difference is the additional section "And [we believe] in the Holy Ghost, the Lord and Giver-of-Life, who proceedeth from the Father, who with the Father and the Son together is worshipped and glorified, who spake by the prophets. And [we believe] in one, holy, Catholic and Apostolic Church. We acknowledge one Baptism for the remission of sins, [and] we look for the resurrection of the dead and the life of the world to come. Amen."

Since the end of the 19th century, scholars have questioned the traditional explanation of the origin of this creed, which has been passed down in the name of the council, whose official acts have been lost over time. A local council of Constantinople in 382 and the third ecumenical council (Ephesus, 431) made no mention of it, with the latter affirming the 325 creed of Nicaea as a valid statement of the faith and using it to denounce Nestorianism. Though some scholarship claims that hints of the later creed's existence are discernible in some writings, no extant document gives its text or makes explicit mention of it earlier than the fourth ecumenical council at Chalcedon in 451. Many of the bishops of the 451 council themselves had never heard of it and initially greeted it skeptically, but it was then produced from the episcopal archives of Constantinople, and the council accepted it "not as supplying any omission but as an authentic interpretation of the faith of Nicaea". In spite of the questions raised, it is considered most likely that this creed was in fact adopted at the 381 second ecumenical council.

On the basis of evidence both internal and external to the text, it has been argued that this creed originated not as an editing of the original Creed proposed at Nicaea in 325, but as an independent creed (probably an older baptismal creed) modified to make it more like the Nicene Creed. Some scholars have argued that the creed may have been presented at Chalcedon as "a precedent for drawing up new creeds and definitions to supplement the Creed of Nicaea, as a way of getting round the ban on new creeds in Canon 7 of Ephesus". It is generally agreed that the Niceno-Constantinopolitan Creed is not simply an expansion of the Creed of Nicaea, and was probably based on another traditional creed independent of the one from Nicaea.

The third Ecumenical Council (Council of Ephesus of 431) reaffirmed the original 325 version of the Nicene Creed and declared that "it is unlawful for any man to bring forward, or to write, or to compose a different () faith as a rival to that established by the holy Fathers assembled with the Holy Ghost in Nicaea" (i.e., the 325 creed). The word is more accurately translated as used by the Council to mean "different", "contradictory", rather than "another". This statement has been interpreted as a prohibition against changing this creed or composing others, but not all accept this interpretation. This question is connected with the controversy whether a creed proclaimed by an Ecumenical Council is definitive in excluding not only excisions from its text but also additions to it.

In one respect, the Eastern Orthodox Church's received text of the Niceno-Constantinopolitan Creed differs from the earliest text, which is included in the acts of the Council of Chalcedon of 451: The Eastern Orthodox Church uses the singular forms of verbs such as "I believe", in place of the plural form ("we believe") used by the council. Byzantine Rite Eastern Catholic Churches use exactly the same form of the Creed, since the Catholic Church teaches that it is wrong to add "and the Son" to the Greek verb "ἐκπορευόμενον", though correct to add it to the Latin "qui procedit", which does not have precisely the same meaning. The form generally used in Western churches does add "and the Son" and also the phrase "God from God", which is found in the original 325 Creed.

The following table, which indicates by [square brackets] the portions of the 325 text that were omitted or moved in 381, and uses "italics" to indicate what phrases, absent in the 325 text, were added in 381, juxtaposes the earlier (AD 325) and later (AD 381) forms of this Creed in the English translation given in Philip Schaff's compilation "The Creeds of Christendom" (1877).

The differences between the actual wordings (in Greek) adopted in 325 and in 381 can be presented in a similar way, as follows:

In the late 6th century, some Latin-speaking churches added the words "and from the Son" ("Filioque") to the description of the procession of the Holy Spirit, in what many Eastern Orthodox Christians have at a later stage argued is a violation of Canon VII of the Third Ecumenical Council, since the words were not included in the text by either the Council of Nicaea or that of Constantinople. This was incorporated into the liturgical practice of Rome in 1014. "Filioque" eventually became one of the main causes for the East-West Schism in 1054, and the failures of the repeated union attempts.

The Vatican stated in 1995 that, while the words καὶ τοῦ Υἱοῦ ("and the Son") would indeed be heretical if used with the Greek verb ἐκπορεύομαι—which is one of the terms used by St. Gregory of Nazianzus and the one adopted by the Council of Constantinople—the word "Filioque" is not heretical when associated with the Latin verb "procedo" and the related word "processio." Whereas the verb ἐκπορεύομαι (from ἐκ, "out of" and πορεύομαι "to come or go") in Gregory and other Fathers necessarily means "to originate from a cause or principle," the Latin term "procedo" (from "pro", "forward;" and "cedo", "to go") has no such connotation and simply denotes the communication of the Divine Essence or Substance. In this sense, "processio" is similar in meaning to the Greek term προϊέναι, used by the Fathers from Alexandria (especially Cyril of Alexandria) as well as others. Partly due to the influence of the Latin translations of the New Testament (especially of John 15:26), the term ἐκπορευόμενον (the present participle of ἐκπορεύομαι) in the creed was translated into Latin as "procedentem". In time, the Latin version of the Creed came to be interpreted in the West in the light of the Western concept of "processio", which required the affirmation of the "Filioque" to avoid the heresy of Arianism.

The view that the Nicene Creed can serve as a touchstone of true Christian faith is reflected in the name "symbol of faith", which was given to it in Greek and Latin, when in those languages the word "symbol" meant a "token for identification (by comparison with a counterpart)", and which continues in use even in languages in which "symbol" no longer has that meaning.

In the Roman Rite Mass, the Latin text of the Niceno-Constantinopolitan Creed, with "Deum de Deo" (God from God) and "Filioque" (and from the Son), phrases absent in the original text, was previously the only form used for the "profession of faith". The Roman Missal now refers to it jointly with the Apostles' Creed as "the Symbol or Profession of Faith or Creed", describing the second as "the baptismal Symbol of the Roman Church, known as the Apostles' Creed".

The liturgies of the ancient Churches of Eastern Christianity (Eastern Orthodox Church, Oriental Orthodoxy, Church of the East and the Eastern Catholic Churches), use the Niceno-Constantinopolitan Creed, never the Western Apostles' Creed.
While in certain places where the Byzantine Rite is used, the choir or congregation sings the Creed at the Divine Liturgy, in many places the Creed is typically recited by the cantor, who in this capacity represents the whole congregation although many, and sometimes all, members of the congregation may join in rhythmic recitation. Where the latter is the practice, it is customary to invite, as a token of honor, any prominent lay member of the congregation who happens to be present, e.g., royalty, a visiting dignitary, the Mayor, etc., to recite the Creed in lieu of the cantor. This practice stems from the tradition that the prerogative to recite the Creed belonged to the Emperor, speaking for his populace.

Some evangelical and other Christians consider the Nicene Creed helpful and to a certain extent authoritative, but not infallibly so in view of their belief that only Scripture is truly authoritative. Non-Trinitarian groups, such as the Church of the New Jerusalem, The Church of Jesus Christ of Latter-day Saints and the Jehovah's Witnesses, explicitly reject some of the statements in the Nicene Creed.

There are several designations for the two forms of the Nicene creed, some with overlapping meanings:

In musical settings, particularly when sung in Latin, this Creed is usually referred to by its first word, "Credo".

This section is not meant to collect the texts of all liturgical versions of the Nicene Creed, and provides only three, the Greek, the Latin, and the Armenian, of special interest. Others are mentioned separately, but without the texts. All ancient liturgical versions, even the Greek, differ at least to some small extent from the text adopted by the First Councils of Nicaea and Constantinople. The Creed was originally written in Greek, owing to the location of the two councils.

But though the councils' texts have "Πιστεύομεν ... ὁμολογοῦμεν ... προσδοκοῦμεν" ("we" believe ... confess ... await), the Creed that the Churches of Byzantine tradition use in their liturgy has "Πιστεύω ... ὁμολογῶ ... προσδοκῶ" ("I" believe ... confess ... await), accentuating the personal nature of recitation of the Creed. The Latin text, as well as using the singular, has two additions: "Deum de Deo" (God from God) and "Filioque" (and from the Son). The Armenian text has many more additions, and is included as showing how that ancient church has chosen to recite the Creed with these numerous elaborations of its contents.

An English translation of the Armenian text is added; English translations of the Greek and Latin liturgical texts are given at English versions of the Nicene Creed in current use.

The Latin text adds "Deum de Deo" and "Filioque" to the Greek. On the latter see The Filioque Controversy above. Inevitably also, the overtones of the terms used, such as "" (pantokratora) and "omnipotentem" differ ("pantokratora" meaning Ruler of all; "omnipotentem" meaning omnipotent, Almighty). The implications of this for the interpretation of "" and "qui ... procedit" was the object of the study "The Greek and the Latin Traditions regarding the Procession of the Holy Spirit" published by the Pontifical Council for Promoting Christian Unity in 1996.

Again, the terms "" and "consubstantialem", translated as "of one being" or "consubstantial", have different overtones, being based respectively on Greek (stable being, immutable reality, substance, essence, true nature), and Latin "substantia" (that of which a thing consists, the being, essence, contents, material, substance).

"Credo", which in classical Latin is used with the accusative case of the thing held to be true (and with the dative of the person to whom credence is given), is here used three times with the preposition "in", a literal translation of the Greek "" (in unum Deum ..., in unum Dominum ..., in Spiritum Sanctum ...), and once in the classical preposition-less construction (unam, sanctam, catholicam et apostolicam Ecclesiam).

English translation of the Armenian version

The version in the Church Slavonic language, used by several Eastern Orthodox Churches is practically identical with the Greek liturgical version.

This version is used also by some Byzantine Rite Eastern Catholic Churches. Although the Union of Brest excluded addition of the "Filioque", this was sometimes added by Ruthenian Catholics, whose older liturgical books also show the phrase in brackets, and by Ukrainian Catholics. Writing in 1971, the Ruthenian Scholar Fr. Casimir Kucharek noted, "In Eastern Catholic Churches, the "Filioque" may be omitted except when scandal would ensue. Most of the Eastern Catholic Rites use it." However, in the decades that followed 1971 it has come to be used more rarely.

The versions used by Oriental Orthodoxy and the Church of the East differ from the Greek liturgical version in having "We believe", as in the original text, instead of "I believe".

The version found in the 1662 "Book of Common Prayer" is still commonly used by some English speakers, but more modern translations are now more common.

The International Consultation on English Texts published an English translation of the Nicene Creed, first in 1970 and then in successive revisions in 1971 and 1975. These texts were adopted by several churches. The Roman Catholic Church in the United States, which adopted the 1971 version in 1973, and the Catholic Church in other English-speaking countries, which in 1975 adopted the version published in that year, continued to use them until 2011 upon the introduction of the "Roman Missal third edition". The 1975 version was included in the 1979 Episcopal Church (United States) "Book of Common Prayer", though with one variation: in the line "For us men and for our salvation", it omitted the word "men".

For the text of the Nicene Creed published in 1988 by the English Language Liturgical Consultation, the successor body of the International Consultation on English Texts, see their website. For the text as recited in the Roman Rite of the Catholic Church, see the website of the Australian National Catholic Education Commission or Youcat, section 29.





</doc>
<doc id="21544" url="https://en.wikipedia.org/wiki?curid=21544" title="Nuclear fusion">
Nuclear fusion

In nuclear physics, nuclear fusion is a reaction in which two or more atomic nuclei come close enough to form one or more different atomic nuclei and subatomic particles (neutrons or protons). The difference in mass between the reactants and products is manifested as the release of large amounts of energy. This difference in mass arises due to the difference in atomic "binding energy" between the atomic nuclei before and after the reaction. Fusion is the process that powers active or "main sequence" stars, or other high magnitude stars.

A fusion process that produces a nucleus lighter than iron-56 or nickel-62 will generally yield a net energy release. These elements have the smallest mass per nucleon and the largest binding energy per nucleon, respectively. Fusion of light elements toward these releases energy (an exothermic process), while a fusion producing nuclei heavier than these elements will result in energy retained by the resulting nucleons, and the resulting reaction is endothermic. The opposite is true for the reverse process, nuclear fission. This means that the lighter elements, such as hydrogen and helium, are in general more fusible; while the heavier elements, such as uranium, thorium and plutonium, are more fissionable. The extreme astrophysical event of a supernova can produce enough energy to fuse nuclei into elements heavier than iron.

In 1920, Arthur Eddington suggested hydrogen-helium fusion could be the primary source of stellar energy. Quantum tunneling was discovered by Friedrich Hund in 1929, and shortly afterwards Robert Atkinson and Fritz Houtermans used the measured masses of light elements to show that large amounts of energy could be released by fusing small nuclei. Building on the early experiments in nuclear transmutation by Ernest Rutherford, laboratory fusion of hydrogen isotopes was accomplished by Mark Oliphant in 1932. In the remainder of that decade, the theory of the main cycle of nuclear fusion in stars were worked out by Hans Bethe. Research into fusion for military purposes began in the early 1940s as part of the Manhattan Project. Fusion was accomplished in 1951 with the Greenhouse Item nuclear test. Nuclear fusion on a large scale in an explosion was first carried out on November 1, 1952, in the Ivy Mike hydrogen bomb test.

Research into developing controlled thermonuclear fusion for civil purposes began in earnest in the 1940s, and it continues to this day.

The release of energy with the fusion of light elements is due to the interplay of two opposing forces: the nuclear force, which combines together protons and neutrons, and the Coulomb force, which causes protons to repel each other. Protons are positively charged and repel each other by the Coulomb force, but they can nonetheless stick together, demonstrating the existence of another, short-range, force referred to as nuclear attraction. Light nuclei (or nuclei smaller than iron and nickel) are sufficiently small and proton-poor allowing the nuclear force to overcome repulsion. This is because the nucleus is sufficiently small that all nucleons feel the short-range attractive force at least as strongly as they feel the infinite-range Coulomb repulsion. Building up nuclei from lighter nuclei by fusion releases the extra energy from the net attraction of particles. For larger nuclei, however, no energy is released, since the nuclear force is short-range and cannot continue to act across longer atomic length scales. Thus, energy is not released with the fusion of such nuclei; instead, energy is required as input for such processes.

Fusion powers stars and produces virtually all elements in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 606 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.7% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.

It takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially "fall" into each other and result is fusion and net energy produced. The fusion of lighter nuclei, which creates a heavier nucleus and often a free neutron or proton, generally releases more energy than it takes to force the nuclei together; this is an exothermic process that can produce self-sustaining reactions.

Energy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is —less than one-millionth of the released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. The complete conversion of one gram of matter would release 9×10 joules of energy. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though "individual" fission reactions are generally much more energetic than "individual" fusion ones, which are themselves millions of times more energetic than chemical reactions. Only direct conversion of mass into energy, such as that caused by the annihilatory collision of matter and antimatter, is more energetic per unit of mass than nuclear fusion.

Research into using fusion for the production of electricity has been pursued for over 60 years. Successful accomplishment of controlled fusion has been stymied by scientific and technological difficulties; nonetheless, important progress has been made. At present, controlled fusion reactions have been unable to produce break-even (self-sustaining) controlled fusion. The two most advanced approaches for it are magnetic confinement (toroid designs) and inertial confinement (laser designs).

Workable designs for a toroidal reactor that theoretically will deliver ten times more fusion energy than the amount needed to heat plasma to the required temperatures are in development (see ITER). The ITER facility is expected to finish its construction phase in 2019. It will start commissioning the reactor that same year and initiate plasma experiments in 2020, but is not expected to begin full deuterium-tritium fusion until 2027.

The US National Ignition Facility, which uses laser-driven inertial confinement fusion, was designed with a goal of break-even fusion; the first large-scale laser target experiments were performed in June 2009 and ignition experiments began in early 2011.

An important fusion process is the stellar nucleosynthesis that powers stars and the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounted for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei as a byproduct of the fusion process. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).

Around 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper "The Internal Constitution of the Stars". At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation "E = mc". This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered. Eddington's paper, based on knowledge at the time, reasoned that:

All of these speculations were proven correct in the following decades.

The primary source of solar energy, and similar size stars, is the fusion of hydrogen to form helium (the proton-proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to synthesize heavier elements. The heaviest elements are synthesized by fusion that occurs as a more massive star undergoes a violent supernova at the end of its life, a process known as supernova nucleosynthesis.

A substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.

When a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbours due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.

The electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from "all" the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.

The net result of the opposing electrostatic and strong nuclear forces is that the binding energy per nucleon generally increases with increasing size, up to the elements iron and nickel, and then decreases for heavier nuclei. Eventually, the binding energy becomes negative and very heavy nuclei (all with more than 208 nucleons, corresponding to a diameter of about 6 nucleons) are not stable. The four most tightly bound nuclei, in decreasing order of binding energy per nucleon, are , , , and . Even though the nickel isotope, , is more stable, the iron isotope is an order of magnitude more common. This is due to the fact that there is no easy way for stars to create through the alpha process.

An exception to this general trend is the helium-4 nucleus, whose binding energy is higher than that of lithium, the next heaviest element. This is because protons and neutrons are fermions, which according to the Pauli exclusion principle cannot exist in the same nucleus in exactly the same state. Each proton or neutron's energy state in a nucleus can accommodate both a spin up particle and a spin down particle. Helium-4 has an anomalously large binding energy because its nucleus consists of two protons and two neutrons, so all four of its nucleons can be in the ground state. Any additional nucleons would have to go into higher energy states. Indeed, the helium-4 nucleus is so tightly bound that it is commonly treated as a single particle in nuclear physics, namely, the alpha particle.

The situation is similar if two nuclei are brought together. As they approach each other, all the protons in one nucleus repel all the protons in the other. Not until the two nuclei actually come close enough for long enough so the strong nuclear force can take over (by way of tunneling) is the repulsive electrostatic force overcome. Consequently, even when the final energy state is lower, there is a large energy barrier that must first be overcome. It is called the Coulomb barrier.

The Coulomb barrier is smallest for isotopes of hydrogen, as their nuclei contain only a single positive charge. A diproton is not stable, so neutrons must also be involved, ideally in such a way that a helium nucleus, with its extremely tight binding, is one of the products.

Using deuterium-tritium fuel, the resulting energy barrier is about 0.1 MeV. In comparison, the energy needed to remove an electron from hydrogen is 13.6 eV, about 7500 times less energy. The (intermediate) result of the fusion is an unstable He nucleus, which immediately ejects a neutron with 14.1 MeV. The recoil energy of the remaining He nucleus is 3.5 MeV, so the total energy liberated is 17.6 MeV. This is many times more than what was needed to overcome the energy barrier.

The reaction cross section σ is a measure of the probability of a fusion reaction as a function of the relative velocity of the two reactant nuclei. If the reactants have a distribution of velocities, e.g. a thermal distribution, then it is useful to perform an average over the distributions of the product of cross section and velocity. This average is called the 'reactivity', denoted <σv>. The reaction rate (fusions per volume per time) is <σv> times the product of the reactant number densities:

If a species of nuclei is reacting with a nucleus like itself, such as the DD reaction, then the product formula_2 must be replaced by formula_3.

formula_4 increases from virtually zero at room temperatures up to meaningful magnitudes at temperatures of 10–100 keV. At these temperatures, well above typical ionization energies (13.6 eV in the hydrogen case), the fusion reactants exist in a plasma state.

The significance of formula_4 as a function of temperature in a device with a particular energy confinement time is found by considering the Lawson criterion. This is an extremely challenging barrier to overcome on Earth, which explains why fusion research has taken many years to reach the current advanced technical state.

If matter is sufficiently heated (hence being plasma), fusion reactions may occur due to collisions with extreme thermal kinetic energies of the particles. Thermonuclear weapons produce what amounts to an uncontrolled release of fusion energy. Controlled thermonuclear fusion energy has yet to be achieved.

Inertial confinement fusion (ICF) is a method aimed at releasing fusion energy by heating and compressing a fuel target, typically a pellet containing deuterium and tritium.

Inertial electrostatic confinement is a set of devices that use an electric field to heat ions to fusion conditions. The most well known is the fusor. Starting in 1999, a number of amateurs have been able to do amateur fusion using these homemade devices. Other IEC devices include: the Polywell, MIX POPS and Marble concepts.

If the energy to initiate the reaction comes from accelerating one of the nuclei, the process is called "beam-target" fusion; if both nuclei are accelerated, it is "beam-beam" fusion.

Accelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. Accelerating light ions is relatively easy, and can be done in an efficient manner—requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross sections. Therefore, the vast majority of ions expend their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.

Muon-catalyzed fusion is a fusion process that occurs at ordinary temperatures. It was studied in detail by Steven Jones in the early 1980s. Net energy production from this reaction has been unsuccessful because of the high energy required to create muons, their short 2.2 µs half-life, and the high chance that a muon will bind to the new alpha particle and thus stop catalyzing fusion.

Some other confinement principles have been investigated.

Antimatter-initialized fusion uses small amounts of antimatter to trigger a tiny fusion explosion. This has been studied primarily in the context of making nuclear pulse propulsion, and pure fusion bombs feasible. This is not near becoming a practical power source, due to the cost of manufacturing antimatter alone.

Pyroelectric fusion was reported in April 2005 by a team at UCLA. The scientists used a pyroelectric crystal heated from −34 to 7 °C (−29 to 45 °F), combined with a tungsten needle to produce an electric field of about 25 gigavolts per meter to ionize and accelerate deuterium nuclei into an erbium deuteride target. At the estimated energy levels, the D-D fusion reaction may occur, producing helium-3 and a 2.45 MeV neutron. Although it makes a useful neutron generator, the apparatus is not intended for power generation since it requires far more energy than it produces.

Hybrid nuclear fusion-fission (hybrid nuclear power) is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s, and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to the delays in the realization of pure fusion.
Project PACER, carried out at Los Alamos National Laboratory (LANL) in the mid-1970s, explored the possibility of a fusion power system that would involve exploding small hydrogen bombs (fusion bombs) inside an underground cavity. As an energy source, the system is the only fusion power system that could be demonstrated to work using existing technology. However it would also require a large, continuous supply of nuclear bombs, making the economics of such a system rather questionable.

At the temperatures and densities in stellar cores the rates of fusion reactions are notoriously slow. For example, at solar core temperature ("T" ≈ 15 MK) and density (160 g/cm), the energy release rate is only 276 μW/cm—about a quarter of the volumetric rate at which a resting human body generates heat. Thus, reproduction of stellar core conditions in a lab for nuclear fusion power production is completely impractical. Because nuclear reaction rates depend on density as well as temperature and most fusion schemes operate at relatively low densities, those methods are strongly dependent on higher temperatures. The fusion rate as a function of temperature (exp(−"E"/"kT")), leads to the need to achieve temperatures in terrestrial reactors 10–100 times higher temperatures than in stellar interiors: "T" ≈ 0.1–1.0×10 K.

In artificial fusion, the primary fuel is not constrained to be protons and higher temperatures can be used, so reactions with larger cross-sections are chosen. Another concern is the production of neutrons, which activate the reactor structure radiologically, but also have the advantages of allowing volumetric extraction of the fusion energy and tritium breeding. Reactions that release no neutrons are referred to as "aneutronic".

To be a useful energy source, a fusion reaction must satisfy several criteria. It must:


Few reactions meet these criteria. The following are those with the largest cross sections:

For reactions with two products, the energy is divided between them in inverse proportion to their masses, as shown. In most reactions with three products, the distribution of energy varies. For reactions that can result in more than one set of products, the branching ratios are given.

Some reaction candidates can be eliminated at once. The D-Li reaction has no advantage compared to p- because it is roughly as difficult to burn but produces substantially more neutrons through - side reactions. There is also a p- reaction, but the cross section is far too low, except possibly when "T" > 1 MeV, but at such high temperatures an endothermic, direct neutron-producing reaction also becomes very significant. Finally there is also a p- reaction, which is not only difficult to burn, but can be easily induced to split into two alpha particles and a neutron.

In addition to the fusion reactions, the following reactions with neutrons are important in order to "breed" tritium in "dry" fusion bombs and some proposed fusion reactors:

The latter of the two equations was unknown when the U.S. conducted the Castle Bravo fusion bomb test in 1954. Being just the second fusion bomb ever tested (and the first to use lithium), the designers of the Castle Bravo "Shrimp" had understood the usefulness of Lithium-6 in tritium production, but had failed to recognize that Lithium-7 fission would greatly increase the yield of the bomb. While Li-7 has a small neutron cross-section for low neutron energies, it has a higher cross section above 5 MeV. The 15 Mt yield was 150% greater than the predicted 6 Mt and caused unexpected exposure to fallout.

To evaluate the usefulness of these reactions, in addition to the reactants, the products, and the energy released, one needs to know something about the cross section. Any given fusion device has a maximum plasma pressure it can sustain, and an economical device would always operate near this maximum. Given this pressure, the largest fusion output is obtained when the temperature is chosen so that <σv>/T is a maximum. This is also the temperature at which the value of the triple product "nT"τ required for ignition is a minimum, since that required value is inversely proportional to <σv>/T (see Lawson criterion). (A plasma is "ignited" if the fusion reactions produce enough power to maintain the temperature without external heating.) This optimum temperature and the value of <σv>/T at that temperature is given for a few of these reactions in the following table.

Note that many of the reactions form chains. For instance, a reactor fueled with and creates some , which is then possible to use in the - reaction if the energies are "right". An elegant idea is to combine the reactions (8) and (9). The from reaction (8) can react with in reaction (9) before completely thermalizing. This produces an energetic proton, which in turn undergoes reaction (8) before thermalizing. Detailed analysis shows that this idea would not work well, but it is a good example of a case where the usual assumption of a Maxwellian plasma is not appropriate.

Any of the reactions above can in principle be the basis of fusion power production. In addition to the temperature and cross section discussed above, we must consider the total energy of the fusion products "E", the energy of the charged fusion products "E", and the atomic number "Z" of the non-hydrogenic reactant.

Specification of the - reaction entails some difficulties, though. To begin with, one must average over the two branches (2i) and (2ii). More difficult is to decide how to treat the and products. burns so well in a deuterium plasma that it is almost impossible to extract from the plasma. The - reaction is optimized at a much higher temperature, so the burnup at the optimum - temperature may be low. Therefore, it seems reasonable to assume the but not the gets burned up and adds its energy to the net reaction, which means the total reaction would be the sum of (2i), (2ii), and (1):

For calculating the power of a reactor (in which the reaction rate is determined by the D-D step), we count the - fusion energy "per D-D reaction" as "E" = (4.03 MeV + 17.6 MeV)×50% + (3.27 MeV)×50% = 12.5 MeV and the energy in charged particles as "E" = (4.03 MeV + 3.5 MeV)×50% + (0.82 MeV)×50% = 4.2 MeV. (Note: if the tritium ion reacts with a deuteron while it still has a large kinetic energy, then the kinetic energy of the helium-4 produced may be quite different from 3.5 MeV, so this calculation of energy in charged particles is only an approximation of the average.) The amount of energy per deuteron consumed is 2/5 of this, or 5.0 MeV (a specific energy of about 225 million MJ per kilogram of deuterium).

Another unique aspect of the - reaction is that there is only one reactant, which must be taken into account when calculating the reaction rate.

With this choice, we tabulate parameters for four of the most important reactions

The last column is the neutronicity of the reaction, the fraction of the fusion energy released as neutrons. This is an important indicator of the magnitude of the problems associated with neutrons like radiation damage, biological shielding, remote handling, and safety. For the first two reactions it is calculated as ("E"-"E")/"E". For the last two reactions, where this calculation would give zero, the values quoted are rough estimates based on side reactions that produce neutrons in a plasma in thermal equilibrium.

Of course, the reactants should also be mixed in the optimal proportions. This is the case when each reactant ion plus its associated electrons accounts for half the pressure. Assuming that the total pressure is fixed, this means that particle density of the non-hydrogenic ion is smaller than that of the hydrogenic ion by a factor 2/("Z"+1). Therefore, the rate for these reactions is reduced by the same factor, on top of any differences in the values of <σv>/T. On the other hand, because the - reaction has only one reactant, its rate is twice as high as when the fuel is divided between two different hydrogenic species, thus creating a more efficient reaction.

Thus there is a "penalty" of (2/(Z+1)) for non-hydrogenic fuels arising from the fact that they require more electrons, which take up pressure without participating in the fusion reaction. (It is usually a good assumption that the electron temperature will be nearly equal to the ion temperature. Some authors, however discuss the possibility that the electrons could be maintained substantially colder than the ions. In such a case, known as a "hot ion mode", the "penalty" would not apply.) There is at the same time a "bonus" of a factor 2 for - because each ion can react with any of the other ions, not just a fraction of them.

We can now compare these reactions in the following table.

The maximum value of <σv>/T is taken from a previous table. The "penalty/bonus" factor is that related to a non-hydrogenic reactant or a single-species reaction. The values in the column "inverse reactivity" are found by dividing 1.24 by the product of the second and third columns. It indicates the factor by which the other reactions occur more slowly than the - reaction under comparable conditions. The column "Lawson criterion" weights these results with "E" and gives an indication of how much more difficult it is to achieve ignition with these reactions, relative to the difficulty for the - reaction. The next-to-last column is labeled "power density" and weights the practical reactivity by "E". The final column indicates how much lower the fusion power density of the other reactions is compared to the - reaction and can be considered a measure of the economic potential.

The ions undergoing fusion in many systems will essentially never occur alone but will be mixed with electrons that in aggregate neutralize the ions' bulk electrical charge and form a plasma. The electrons will generally have a temperature comparable to or greater than that of the ions, so they will collide with the ions and emit x-ray radiation of 10–30 keV energy, a process known as Bremsstrahlung.

The huge size of the Sun and stars means that the x-rays produced in this process will not escape and will deposit their energy back into the plasma. They are said to be opaque to x-rays. But any terrestrial fusion reactor will be optically thin for x-rays of this energy range. X-rays are difficult to reflect but they are effectively absorbed (and converted into heat) in less than mm thickness of stainless steel (which is part of a reactor's shield). This means the bremsstrahlung process is carrying energy out of the plasma, cooling it.
The ratio of fusion power produced to x-ray radiation lost to walls is an important figure of merit. This ratio is generally maximized at a much higher temperature than that which maximizes the power density (see the previous subsection). The following table shows estimates of the optimum temperature and the power ratio at that temperature for several reactions:

The actual ratios of fusion to Bremsstrahlung power will likely be significantly lower for several reasons. For one, the calculation assumes that the energy of the fusion products is transmitted completely to the fuel ions, which then lose energy to the electrons by collisions, which in turn lose energy by Bremsstrahlung. However, because the fusion products move much faster than the fuel ions, they will give up a significant fraction of their energy directly to the electrons. Secondly, the ions in the plasma are assumed to be purely fuel ions. In practice, there will be a significant proportion of impurity ions, which will then lower the ratio. In particular, the fusion products themselves "must" remain in the plasma until they have given up their energy, and "will" remain some time after that in any proposed confinement scheme. Finally, all channels of energy loss other than Bremsstrahlung have been neglected. The last two factors are related. On theoretical and experimental grounds, particle and energy confinement seem to be closely related. In a confinement scheme that does a good job of retaining energy, fusion products will build up. If the fusion products are efficiently ejected, then energy confinement will be poor, too.

The temperatures maximizing the fusion power compared to the Bremsstrahlung are in every case higher than the temperature that maximizes the power density and minimizes the required value of the fusion triple product. This will not change the optimum operating point for - very much because the Bremsstrahlung fraction is low, but it will push the other fuels into regimes where the power density relative to - is even lower and the required confinement even more difficult to achieve. For - and -, Bremsstrahlung losses will be a serious, possibly prohibitive problem. For -, p- and p- the Bremsstrahlung losses appear to make a fusion reactor using these fuels with a quasineutral, isotropic plasma impossible. Some ways out of this dilemma are considered—and rejected—in "fundamental limitations on plasma fusion systems not in thermodynamic equilibrium". This limitation does not apply to non-neutral and anisotropic plasmas; however, these have their own challenges to contend with.

To determine the rate of fusion reactions, the value of most interest is the cross section, which describes the probability that particle with fuse by giving a characteristic area of interaction. In a classical picture, the cross section is the radius of the nuclei (on order of a femtometer) and can only occur when the particles have enough energy to overcome the Coulomb barrier and approach within this radius (Needing energy on the order of a MeV). This greatly overestimates the energy needed for fusion reactions to occur because it doesn't include quantum effects of the smeared radius as the DeBroglie wavelength as well as quantum tunnelling. An estimation of the fusion cross section is often broken into three pieces:

Where formula_7 is the geometric cross section, is the barrier transparency and is the reaction characteristics of the reaction. 

formula_7 is of the order of the square of the de-Broglie wavelength formula_9 where formula_10 is the reduced mass of the system and formula_11 is the center of mass energy of the system. 




</doc>
<doc id="21550" url="https://en.wikipedia.org/wiki?curid=21550" title="National Geographic Society">
National Geographic Society

The National Geographic Society (NGS), headquartered in Washington, D.C., United States, is one of the largest non-profit scientific and educational organizations in the world. Founded in 1888, its interests include geography, archaeology, and natural science, the promotion of environmental and historical conservation, and the study of world culture and history. The National Geographic Society’s logo is a yellow portrait frame—rectangular in shape—which appears on the margins surrounding the front covers of its magazines and as its television channel logo. In partnership with 21st Century Fox, the Society operates the magazine, TV channels, a website that features extra content, worldwide events, and other media operations.

The National Geographic Society was founded in 1888 "to increase and diffuse geographic knowledge." It is governed by a board of trustees, whose 21 members include distinguished educators, business executives, former government officials and conservationists. The organization sponsors and funds scientific research and exploration. National Geographic maintains a museum for the public in its Washington, D.C., headquarters.

It has helped to sponsor popular traveling exhibits, such as the early 2010s "King Tut" exhibit featuring artifacts from the tomb of the young Egyptian Pharaoh; "The Cultural Treasures of Afghanistan" which opened in May 2008 and traveled to other cities for 18 months; and an exhibition of China's Terracotta Warriors in its Washington headquarters in 2009–10. Its Education Foundation gives grants to education organizations and individuals to improve geography education. Its Committee for Research and Exploration has awarded more than 11,000 grants for scientific research and exploration.

National Geographic has retail stores in Washington, D.C., London, Sydney, and Panama. The locations outside of the United States are operated by Worldwide Retail Store S.L., a Spanish holding company.

The Society's media arm is National Geographic Partners, a joint venture between 21st Century Fox and the Society, which publishes a journal, "National Geographic" in English and nearly 40 local-language editions. It also publishes other magazines, books, school products, maps, and Web and film products in numerous languages and countries. National Geographic's various media properties reach more than 280 million people monthly.

The National Geographic Society began as a club for an elite group of academics and wealthy patrons interested in travel and exploration. On January 13, 1888, 33 explorers and scientists gathered at the Cosmos Club, a private club then located on Lafayette Square in Washington, D.C., to organize "a society for the increase and diffusion of geographical knowledge." After preparing a constitution and a plan of organization, the National Geographic Society was incorporated two weeks later on January 27. Gardiner Greene Hubbard became its first president and his son-in-law, Alexander Graham Bell, succeeded him in 1897.

In 1899, Bell's son-in-law Gilbert Hovey Grosvenor was named the first full-time editor of National Geographic magazine and served the organization for fifty-five years (until 1954), and members of the Grosvenor family have played important roles in the organization since. Bell and Gilbert Hovey Grosvenor devised the successful marketing notion of Society membership and the first major use of photographs to tell stories in magazines.

The current National Geographic Society president and CEO is Gary E. Knell. The chairman of the board of trustees is Jean Case. The editor-in-chief of National Geographic magazine is Susan Goldberg. Gilbert Melville Grosvenor, a former chairman of the Society board of trustees received the Presidential Medal of Freedom in 2005 for his leadership in geography education.

In 2004, the National Geographic Society headquarters in Washington, D.C., was one of the first buildings to receive a "Green" certification from Global Green USA. The National Geographic received the prestigious Prince of Asturias Award for Communication and Humanities in October 2006 in Oviedo, Spain.

In 2013 the society was investigated for possible violation of the Foreign Corrupt Practices Act relating to their close association with an Egyptian government official responsible for antiquities.

On September 9, 2015, the Society announced that it would re-organize its media properties and publications into a new company known as National Geographic Partners, which would be majority-owned by 21st Century Fox with a 73% stake. This new, for-profit corporation, would own "National Geographic" and other magazines, as well as its affiliated television networks—most of which were already owned in joint ventures with Fox. At the time of the deal’s announcement, James Murdoch, the CEO of 21st Century Fox, said in remarks to National Geographic that the pact created "an expanded canvas for the National Geographic brand to grow and reach customers in new ways, and to reach new customers."

On November 2, 2015, roughly two weeks before the closing of the expanded joint venture deal, National Geographic and 21st Century Fox announced that 9 percent of National Geographic's 2,000 employees, approximately 180 people, would be laid off, constituting the biggest staff reduction in the Society's history.

As reported by "The Guardian", a spokesman for National Geographic in a November 2, 2015, e-mail statement, briefly discussed the rationale for the staff reductions as part of the "... process of reorganizing in order to move forward strategically following the closing the National Geographic Partners deal, which is expected to occur in mid-November."

Additional specifics were provided to Photo District News by M. J. Jacobsen, National Geographic’s SVP of communications, similar to the contents of a formal announcement by the two companies. "The National Geographic Society and the National Geographic Channels are in the process of reorganizing in order to move forward strategically following the closing of the NG Partners deal (with Fox), which is expected to occur in mid-November," Jaobsen wrote. "Involuntary separations will represent about 9 percent of the overall workforce reduction, many in shared services and a voluntary separation offer has also been made to eligible employees," he added.

Specifics as to which departments would be affected were not immediately available but "The Washington Post" reported that the staff reduction appears to affect almost every department including the magazine and the National Geographic Channel.

On October 26, 2016, National Geographic announced a rebrand and the television network dropped "Channel" from its name.

On December 14, 2017, in a historic deal valued at over $52 billion, The Walt Disney Company announced it will buy the majority of 21st Century Fox. Disney would assume the controlling interest in the National Geographic partnership.

There were 33 original founders in 1888. Although Alexander Graham Bell is sometimes discussed as a founder, he was actually the second president, elected on January 7, 1898, and serving until 1903.


"The National Geographic Magazine", later shortened to "National Geographic", published its first issue in October 1888, nine months after the Society was founded, as the Society's official journal, a benefit for joining the tax-exempt National Geographic Society. Starting with the February 1910 (Vol XXI., No. 2) issue, the magazine began using its now famous trademarked yellow border around the edge of its covers.

There are 12 monthly issues of "National Geographic" per year. The magazine contains articles about geography, popular science, world history, culture, current events and photography of places and things all over the world and universe. "National Geographic" magazine is currently published in 40 local-language editions in many countries around the world. Combined English and other language circulation is around 6.8 million monthly, with some 60 million readers.

In addition to its flagship magazine, the Society publishes several other periodicals:


The Society also runs an online news outlet called "National Geographic Daily News".

Additionally, the Society publishes atlases, books, and maps. It previously published and co-published other magazines, including "National Geographic Adventure", "National Geographic Research" (a scientific journal), and others, and continues to publish special issues of various magazines.

National Geographic Films is a wholly owned taxable subsidiary of the National Geographic Society. Films it has produced include:

Programs by the National Geographic Society are also broadcast on television. National Geographic television specials and series have been aired on PBS and other networks in the United States and globally for many years. The "Geographic" series in the U.S. started on CBS in 1964, moved to ABC in 1973, shifted to PBS (produced by WQED, Pittsburgh) in 1975, shifted to NBC in 1995, and returned to PBS in 2000.

National Geographic Channel, launched in January 2001, is a joint venture of National Geographic and Fox Cable Networks. It has featured stories on numerous scientific figures such as Jacques Cousteau, Jane Goodall, and Louis Leakey that not only featured their work but as well helped make them world-famous and accessible to millions. Most of the specials were narrated by various actors, including Glenn Close, Linda Hunt, Stacy Keach, Richard Kiley, Burgess Meredith, Susan Sarandon, Alexander Scourby, Martin Sheen, and Peter Strauss. The specials' theme music, by Elmer Bernstein, was also adopted by the National Geographic Channel. The National Geographic Channel has begun to launch a number of sub-branded channels in international markets, such as Nat Geo Adventure, Nat Geo Music, and Nat Geo Wild.

The National Geographic Partners expanded joint venture with 21st Century Fox (to be controlled by the latter) will be concluded in mid-November 2015 with the new for-profit organization to own a 73% stake of the NG affiliated television networks (National Geographic-branded cable TV channels), most of which were already co-owned (with the percentage information not readily available), as per earlier joint ventures with Fox.

The Society operates the National Geographic Museum, located at 1145 17th Street, NW (17th and M), in Washington, D.C. The museum features changing exhibitions featuring the work of National Geographic explorers, photographers, and scientists. There are also changing exhibits related to natural history, culture, history or society. Permanent exhibits include artifacts like the camera Robert Peary used at the North Pole and pottery that Jacques Cousteau recovered from a shipwreck.

The Society has helped sponsor many expeditions and research projects over the years, including:

The Society supports many socially based projects including AINA, a Kabul-based organization dedicated to developing an independent Afghan media, which was founded by one of the Society's most famous photographers, Reza.

The Society also organizes the National Geographic Bee, an annual geographic contest for U.S. fourth- through eighth-graders. About 4 million students a year begin the geography competition locally, which culminates in a national competition of the winners of each state each May in Washington, D.C. Journalist Soledad O'Brien is the moderator of the Bee. She succeeded Alex Trebek, host of "Jeopardy!", who moderated the final round of the competition for 25 years, from its inception in 1989 to 2013. Every two years, the Society conducts an international geography competition of competing teams from all over the world. The most recent was held in St. Petersburg, Russia, in July 2013, and had representatives from 19 national teams. The team from the United States emerged as the winner, with teams from Canada and India in second and third place.

The Hubbard Medal is awarded by the National Geographic Society for distinction in exploration, discovery, and research. The medal is named for Gardiner Greene Hubbard, the first National Geographic Society president. The Hubbard Medal has been presented 35 times as of 2010, the most recent award going to Don Walsh.

The National Geographic Society also awards, rarely, the Alexander Graham Bell Medal, for exceptional contributions to geographic research. The award is named after Alexander Graham Bell, scientist, inventor and the second president of the NGS. Up to mid-2011, the medal has been twice presented:




</doc>
<doc id="21556" url="https://en.wikipedia.org/wiki?curid=21556" title="Norns">
Norns

The Norns (, plural: "") in Norse mythology are female beings who rule the destiny of gods and men. They roughly correspond to other controllers of humans' destiny, such as the Fates, elsewhere in European mythology.

In Snorri Sturluson's interpretation of the "Völuspá", Urðr (Wyrd), Verðandi and Skuld, the three most important of the Norns, come out from a hall standing at the Well of Urðr or Well of Fate. They draw water from the well and take sand that lies around it, which they pour over Yggdrasill so that its branches will not rot. These three Norns are described as powerful maiden giantesses (Jotuns) whose arrival from Jötunheimr ended the golden age of the gods. They may be the same as the maidens of Mögþrasir who are described in "Vafþrúðnismál" (see below).

Beside these three famous Norns, there are many others who appear at a person's birth in order to determine his or her future. In the pre-Christian Norse societies, Norns were thought to have visited newborn children. There were both malevolent and benevolent Norns: the former caused all the malevolent and tragic events in the world while the latter were kind and protective goddesses.

The origin of the name "norn" is uncertain, it may derive from a word meaning "to twine" and which would refer to their twining the thread of fate. Bek-Pedersen suggests that the word "norn" has relation to the Swedish dialect word "norna (nyrna)", a verb that means "secretly communicate". This relates to the perception of norns as shadowy, background figures who only really ever reveal their fateful secrets to men as their fates come to pass.

The name "Urðr" (Old English Wyrd, Weird) means "fate". It should be noted that "wyrd" and "urðr" are etymological cognates, which does not guarantee that "wyrd" and "urðr" share the same semantic quality of "fate" over time. Both "Urðr" and "Verðandi" are derived from the Old Norse verb "verða", "to be". It is commonly asserted that while "Urðr" derives from the past tense ("that which became or happened"), "Verðandi" derives from the present tense of "verða" ("that which is happening"). "Skuld" is derived from the Old Norse verb "skulu", "need/ought to be/shall be"; its meaning is "that which should become, or that needs to occur". Due to this, it has often been inferred that the three norns are in some way connected with the past, present and future respectively, but it has been disputed that their names really imply a temporal distinction and it has been emphasised that the words do not in themselves denote chronological periods in Old Norse.

There is no clear distinction between norns, fylgjas, hamingjas and valkyries, nor with the generic term dísir. Moreover, artistic license permitted such terms to be used for mortal women in Old Norse poetry. To quote Snorri Sturluson's "Skáldskaparmál" on the various names used for women:
These unclear distinctions among norns and other Germanic female deities are discussed in Bek-Pedersen's book "Norns in Old Norse Mythology."

There are a number of surviving Old Norse sources that relate to the norns. The most important sources are the Prose Edda and the Poetic Edda. The latter contains pagan poetry where the norns are frequently referred to, while the former contains, in addition to pagan poetry, retellings, descriptions and commentaries by the 12th and 13th century Icelandic chieftain and scholar Snorri Sturluson.

A skaldic reference to the norns appears in Hvini's poem in "Ynglingatal" 24 found in "Ynglingasaga" 47, where King Halfdan is put to rest by his men at Borró. This reference brings in the phrase ""norna dómr"" which means "judgment of the nornir". In most cases, when the norns pass judgment, it means death to those who have been judged - in this case, Halfdan. Along with being associated with being bringers of death, Bek-Pedersen suggests that this phrase brings in a quasi-legal aspect to the nature of the norns. This legal association is employed quite frequently within skaldic and eddic sources. This phrase can also be seen as a threat, as death is the final and inevitable decision that the norns can make with regard to human life.

The Poetic Edda is valuable in representing older material in poetry from which Snorri tapped information in the "Prose Edda". Like "Gylfaginning", the "Poetic Edda" mentions the existence of many lesser norns beside the three main norns. Moreover, it also agrees with "Gylfaginning" by telling that they were of several races and that the dwarven norns were the daughters of Dvalin. It also suggests that the three main norns were giantesses (female Jotuns).

"Fáfnismál" contains a discussion between the hero Sigurd and the dragon Fafnir who is dying from a mortal wound from Sigurd. The hero asks Fafnir of many things, among them the nature of the norns. Fafnir explains that they are many and from several races:

It appears from "Völuspá" and "Vafþrúðnismál" that the three main norns were not originally goddesses but giantesses (Jotuns), and that their arrival ended the early days of bliss for the gods, but that they come for the good of mankind.

"Völuspá" relates that three giantesses of huge might are reported to have arrived to the gods from Jotunheim:

"Vafþrúðnismál" probably refers to the norns when it talks of maiden giantesses who arrive to protect the people of earth as protective spirits (hamingjas):

The "Völuspá" contains the names of the three main Norns referring to them as maidens like "Vafþrúðnismál" probably does:

The norns visited each newly born child to allot his or her future, and in "Helgakviða Hundingsbana I", the hero Helgi Hundingsbane has just been born and norns arrive at the homestead:

In "Helgakviða Hundingsbana II", Helgi Hundingsbane blames the norns for the fact that he had to kill Sigrún's father Högni and brother Bragi in order to wed her:

Like Snorri Sturluson stated in "Gylfaginning", people's fate depended on the benevolence or the malevolence of particular norns. In "Reginsmál", the water dwelling dwarf Andvari blames his plight on an evil norn, presumably one of the daughters of Dvalin:

Another instance of Norns being blamed for an undesirable situation appears in "Sigurðarkviða hin skamma", where the valkyrie Brynhild blames malevolent norns for her long yearning for the embrace of Sigurd:
Brynhild's solution was to have Gunnarr and his brothers, the lords of the Burgundians, kill Sigurd and afterwards to commit suicide in order to join Sigurd in the afterlife. Her brother Atli (Attila the Hun) avenged her death by killing the lords of the Burgundians, but since he was married to their sister Guðrún, Atli would soon be killed by her. In "Guðrúnarkviða II", the Norns actively enter the series of events by informing Atli in a dream that his wife would kill him. The description of the dream begins with this stanza:
After having killed both her husband Atli and their sons, Guðrún blames the Norns for her misfortunes, as in "Guðrúnarhvöt", where Guðrún talks of trying to escaping the wrath of the norns by trying to kill herself:

"Guðrúnarhvöt" deals with how Guðrún incited her sons to avenge the cruel death of their sister Svanhild. In "Hamðismál", her sons' expedition to the Gothic king Ermanaric to exact vengeance is fateful. Knowing that he is about to die at the hands of the Goths, her son Sörli talks of the cruelty of the norns:

Since the norns were beings of ultimate power who were working in the dark, it should be no surprise that they could be referred to in charms, as they are by Sigrdrífa in "Sigrdrífumál":

In the part of Snorri Sturluson's "Prose Edda" which is called "Gylfaginning", Gylfi, the king of Sweden, has arrived at Valhalla calling himself Gangleri. There, he receives an education in Norse mythology from what is Odin in the shape of three men. They explain to Gylfi that there are three main norns, but also many others of various races, æsir, elves and dwarves:

The three main norns take water out of the well of Urd and water Yggdrasil:

Snorri furthermore informs the reader that the youngest norn, Skuld, is in effect also a valkyrie, taking part in the selection of warriors from the slain:

Some of the legendary sagas also contain references to the norns. The "Hervarar saga" contains a poem named "Hlöðskviða", where the Gothic king Angantýr defeats a Hunnish invasion led by his Hunnish half-brother Hlöðr. Knowing that his sister, the shieldmaiden Hervör, is one of the casualties, Angantýr looks at his dead brother and laments the cruelty of the norns:
In younger legendary sagas, such as "Norna-Gests þáttr" and "Hrólfs saga kraka", the norns appear to have been synonymous with völvas (witches, female shamans). In "Norna-Gests þáttr", where they arrive at the birth of the hero to shape his destiny, the norns are not described as weaving the web of fate, instead "Norna" appears to be interchangeable and possibly a synonym of "vala" (völva).

One of the last legendary sagas to be written down, the "Hrólfs saga kraka" talks of the norns simply as evil witches. When the evil half-elven princess Skuld assembles her army to attack Hrólfr Kraki, it contains in addition to undead warriors, elves and norns.

The belief in the norns as bringers of both gain and loss would last beyond Christianization, as testifies the runic inscription N 351 M from the Borgund stave church:

Three women carved on the right panel of Franks Casket, an Anglo-Saxon whalebone chest from the eighth century, have been identified by some scholars as being three norns.

A number of theories have been proposed regarding the norns.

The Germanic Matres and Matrones, female deities venerated in North-West Europe from the 1st to the 5th century AD depicted on votive objects and altars almost entirely in groups of three from the first to the fifth century AD have been proposed as connected with the later Germanic dísir, valkyries, and norns, potentially stemming from them.

Theories have been proposed that there is no foundation in Norse mythology for the notion that the three main norns should each be associated exclusively with the past, the present, and the future; rather, all three represent "destiny" as it is twined with the flow of time. Moreoever, theories have been proposed that the idea that there are three main norns may be due to a late influence from Greek and Roman mythology, where there are also spinning fate goddesses (Moirai and Parcae).

Viking death metal band Amon Amarth has an album titled "Fate of Norns". The band itself has many songs involving Norse mythology.

, written and composed by Tika･α, arranged by Naoko Etō and sang by Etsuko Yakushimaru Metro Orchestra, is the opening theme of the 2011 anime Mawaru Penguindrum (episodes 1–14), which has themes of constantly changing fate as its main plot.

Norns feature in the prologue of Richard Wagner's opera Götterdämmerung.

The 1990s Disney TV series "Gargoyles" features three sisters, referred to by the cast as the "Weird Sisters", that are implied to be Norns.

A norn was featured in the fourth season of . After his arrival in the Norselands, the norn tasks Hercules with averting Ragnarok. Since the Fates had already been featured in the series, only a single norn appears, who paints history in a book.

In the 2010 series "Lost Girl", there was a Norn who could be petitioned to change fate, for a price. Her price was always the one thing her petitioner values most, whether they realize it or not.

The Norns appear in Marvel Comics, usually in stories featuring the Norse inspired superhero Thor.

The Norns also appear in The Wicked + The Divine, published by Image Comics, written by Kieron Gillen with art by Jamie McKelvie and Matt Wilson. They are incarnated from a trio of journalists, led by Urðr, alongside Verðani and Skuld.

The main love interest of Oh My Goddess! is the Norn Verðandi, rendered as Belldandy. Her elder sister Urðr (rendered as Urd) and younger sister Skuld also show up, living with the protagonist Keiichi Morisato and their sister Belldandy. Aside from sticking loosely to the theme of Belldandy representing the present, Urd the past and Skuld the future, they are only loosely related to their mythic namesakes in this media.

The terminals that Yggdrasil from Digimon created for the New Digital World experiments consisting 3 layers are named Ulud, Versandi, and Skuld which are representing for past, present, and future. Ulud Urðr is a past plain which is a volcanic wasteland, inhabited by Dinosaur type and draconic Digimon. Versandi Verðandi is the "present" region which is a world of lush greenery and is home to beast, bird, plant and other nature Digimon. Skuld is the "future" region, a high-tech city where machine and insect Digimon inhabit.

The three Norns also appear as antagonists in Mythical Detective Loki Ragnarok, along with various other figures from Norse mythology, including Thor, Heimdallr, Freyr, Freyja, Fenrir, Jormungandr, and the eponymous Loki.

In the Calibur arc of Sword Art Online, Urðr appears to Kirito's party and gives them a quest to retrieve the sword Excalibur from Thrymheim before the last beast-type Evil God is killed, restoring Jötunheimr to its former glory. Upon successful completion of this quest, Urðr reappears to Kirito's party, along with her sisters Verdandi and Skuld. They thank them for completing the quest, and allow them to keep Excalibur. Other figures and elements from Norse mythology also appear in this arc, including Thrym, Freyja, Thor, and Mjölnir.

In the anime Norn9, the ship that the main characters live in is named Norn9, after the three Goddesses of Fate.





</doc>
<doc id="21557" url="https://en.wikipedia.org/wiki?curid=21557" title="Niflheim">
Niflheim

Niflheim (or Niflheimr) ("Mist Home", the "Abode of Mist" or "Mist World" , or probably "world of the darkness" according to the Oxford English Dictionary) is one of the Nine Worlds and is a location in Norse mythology which sometimes overlaps with the notions of Niflhel and Hel. The name "Niflheimr" only appears in two extant sources: "Gylfaginning" and the much-debated "Hrafnagaldr Óðins".

Niflheim was primarily a realm of primordial ice and cold, with the frozen rivers of Élivágar and the well of Hvergelmir, from which come all the rivers.

According to "Gylfaginning", Niflheim was the second of the two primordial realms to emenate out of Ginnungagap, the other one being Muspelheim, the realm of fire. Between these two realms of cold and heat, creation began when its waters mixed with the heat of Muspelheim to form a "creating steam". Later, it became the abode of Hel, a goddess daughter of Loki, and the afterlife for her subjects, those who did not die a heroic or notable death.

"Nifl" (whence the Icelandic "nifl") being cognate with the Anglo-Saxon "Nifol" ("dark"), Dutch "nevel" and German "Nebel" ("fog").

In "Gylfaginning" by Snorri Sturluson, Gylfi, the king of ancient Scandinavia, receives an education in Norse mythology from Odin in the guise of three men. Gylfi learns from Odin (as "Jafnhárr") that Niflheimr was the first world to be created after Muspelheim:

Odin (as "Þriði") further tells Gylfi that it was when the ice from Niflheimr met the flames from Muspelheimr that creation began and Ymir was formed:
In relation to the world tree Yggdrasill, "Jafnhárr" (Odin) tells Gylfi that Jötunheimr is located under the second root, where Ginnungagap ("Yawning Void") once was:
Gylfi is furthermore informed that when Loki had engendered Hel, she was cast into Niflheimr by Odin:

Hel thus became the mistress of the world of those dead in disease and old age. This is the only instance in which Niflheim and Hel are equated (the Poetic Edda mentions Hel but doesn't say anything about Niflheim).
However, there is some confusion in the different versions of the manuscript, with some of them saying Niflheim where others say Niflhel (the lowest level of Hel). Thus in the passage about the last destination of the "jötunn" who was killed by Thor after he had built Asgard:
In "Hrafnagaldr Óðins", there is a brief mention of Niflheimr as a location in the North, towards which the sun (Alfr's illuminator) chased the night as it rose:


</doc>
<doc id="21558" url="https://en.wikipedia.org/wiki?curid=21558" title="Nanna">
Nanna

Nanna may refer to:








</doc>
<doc id="21559" url="https://en.wikipedia.org/wiki?curid=21559" title="NASDAQ">
NASDAQ

The Nasdaq Stock Market (, also known simply as Nasdaq) is an American stock exchange. It is the second-largest exchange in the world by market capitalization, behind only the New York Stock Exchange located in the same city. The exchange platform is owned by Nasdaq, Inc., which also owns the Nasdaq Nordic (formerly known as OMX) and Nasdaq Baltic stock market network and several U.S. stock and options exchanges.

"Nasdaq" was initially an acronym for the National Association of Securities Dealers Automated Quotations. It was founded in 1971 by the National Association of Securities Dealers (NASD), which divested itself of Nasdaq in a series of sales in 2000 and 2001. The Nasdaq Stock Market is owned and operated by Nasdaq, Inc., the stock of which was listed on its own securities exchange on July 2, 2002, under the ticker symbol .
The Nasdaq Stock Market began trading on February 8, 1971. It was the world's first electronic stock market. At first, it was merely a “quotation system” and did not provide a way to perform electronic trades. The Nasdaq Stock Market helped lower the spread (the difference between the bid price and the ask price of the stock) but was unpopular among brokerages which made much of their money on the spread.

The Nasdaq Stock Market eventually assumed the majority of major trades that had been executed by the over-the-counter (OTC) system of trading, but there are still many securities traded in this fashion. As late as 1987, the Nasdaq exchange was still commonly referred to as "OTC" in media reports and also in the monthly Stock Guides (stock guides and procedures) issued by Standard & Poor's Corporation.

Over the years, the Nasdaq Stock Market became more of a stock market by adding trade and volume reporting and automated trading systems. It was also the first stock market in the United States to trade online, highlighting Nasdaq-traded companies and closing with the declaration that the Nasdaq Stock Market is "the stock market for the next hundred years". The Nasdaq Stock Market attracted new growth companies, including Microsoft, Apple, Cisco, Oracle and Dell, and it helped modernize the IPO.

Its main index is the NASDAQ Composite, which has been published since its inception. However, its exchange-traded fund tracks the large-cap NASDAQ-100 index, which was introduced in 1985 alongside the NASDAQ 100 Financial Index, which tracks the largest 100 companies in terms of market capitalization.

In 1992, the Nasdaq Stock Market joined with the London Stock Exchange to form the first intercontinental linkage of securities markets. The National Association of Securities Dealers spun off the Nasdaq Stock Market in 2000 to form a publicly traded company.

On March 10, 2000, the NASDAQ Composite peaked at 5,132.52, but fell to 3227 by April 17, and in the following 30 months fell 78% from its peak.

In 2006, the status of the Nasdaq Stock Market was changed from a stock market to a licensed national securities exchange.

In 2007, Nasdaq merged with OMX, a leading exchange operator in the Nordic countries, expanded its global footprint, and changed its name to the NASDAQ OMX Group.

To qualify for listing on the exchange, a company must be registered with the United States Securities and Exchange Commission (SEC), must have at least three market makers (financial firms that act as brokers or dealers for specific securities) and must meet minimum requirements for assets, capital, public shares, and shareholders.

In February 2011, in the wake of an announced merger of NYSE Euronext with Deutsche Börse, speculation developed that NASDAQ OMX and Intercontinental Exchange (ICE) could mount a counter-bid of their own for NYSE. NASDAQ OMX could be looking to acquire the American exchange's cash equities business, ICE the derivatives business. At the time, "NYSE Euronext’s market value was $9.75 billion. Nasdaq was valued at $5.78 billion, while ICE was valued at $9.45 billion." Late in the month, Nasdaq was reported to be considering asking either ICE or the Chicago Mercantile Exchange to join in what would probably have to be, if it proceeded, an $11–12 billion counterbid.

The European Association of Securities Dealers Automatic Quotation System (EASDAQ) was founded as a European equivalent to the Nasdaq Stock Market. It was purchased by NASDAQ in 2001 and became NASDAQ Europe. Operations were shut down, however, as a result of the burst of the dot-com bubble. In 2007, NASDAQ Europe was revived as Equiduct, and is currently operating under Börse Berlin.

On June 18, 2012, Nasdaq OMX became a founding member of the United Nations Sustainable Stock Exchanges initiative on the eve of the United Nations Conference on Sustainable Development (Rio+20).

In November 2016, Nasdaq Chief Operating Officer Adena Friedman was promoted to the role of CEO, becoming the first woman to run a major exchange in the U.S. In 2016, Nasdaq earned $272 million in listings-related revenues.

Nasdaq quotes are available at three levels:

The Nasdaq Stock Market sessions eastern time are:

4:00 am to 9:30 am premarket session

9:30 am to 4:00 pm normal trading session

4:00 pm to 8:00 pm postmarket session

The Nasdaq Stock Market has three different market tiers:

As of June 2015, the Nasdaq Stock Market had an average annualized growth rate of 9.24% since its opening in February 1971. Since the end of the recession in June 2009 however, it has increased by 18.29% per year.



</doc>
<doc id="21560" url="https://en.wikipedia.org/wiki?curid=21560" title="New York Stock Exchange">
New York Stock Exchange

The New York Stock Exchange (abbreviated as NYSE, and nicknamed "The Big Board"), is an American stock exchange located at 11 Wall Street, Lower Manhattan, New York City, New York. It is by far the world's largest stock exchange by market capitalization of its listed companies at US$21.3 trillion as of June 2017. The average daily trading value was approximately 169 billion in 2013. The NYSE trading floor is located at 11 Wall Street and is composed of 21 rooms used for the facilitation of trading. A fifth trading room, located at 30 Broad Street, was closed in February 2007. The main building and the 11 Wall Street building were designated National Historic Landmarks in 1978.

The NYSE is owned by Intercontinental Exchange, an American holding company that it also lists (). Previously, it was part of NYSE Euronext (NYX), which was formed by the NYSE's 2007 merger with Euronext.

The NYSE has been the subject of several lawsuits regarding fraud or breach of duty and in 2004 was sued by its former CEO for breach of contract and defamation.

The earliest recorded organization of securities trading in New York among brokers directly dealing with each other can be traced to the Buttonwood Agreement. Previously securities exchange had been intermediated by the auctioneers who also conducted more mundane auctions of commodities such as wheat and tobacco. On May 17, 1792 twenty four brokers signed the Buttonwood Agreement which set a floor commission rate charged to clients and bound the signers to give preference to the other signers in securities sales. The earliest securities traded were mostly governmental securities such as War Bonds from the Revolutionary War and First Bank of the United States stock, although Bank of New York stock was a non-governmental security traded in the early days. The Bank of North America along with the First Bank of the United States and the Bank of New York were the first shares traded on the New York Stock Exchange.

In 1817 the stockbrokers of New York operating under the Buttonwood Agreement instituted new reforms and reorganized. After sending a delegation to Philadelphia to observe the organization of their board of brokers, restrictions on manipulative trading were adopted as well as formal organs of governance. After re-forming as the New York Stock and Exchange Board the broker organization began renting out space exclusively for securities trading, which previously had been taking place at the Tontine Coffee House. Several locations were used between 1817 and 1865, when the present location was adopted.

The invention of the electrical telegraph consolidated markets, and New York's market rose to dominance over Philadelphia after weathering some market panics better than other alternatives. The Open Board of Stock Brokers was established in 1864 as a competitor to the NYSE. With 354 members, the Open Board of Stock Brokers rivaled the NYSE in membership (which had 533) "because it used a more modern, continuous trading system superior to the NYSE’s twice-daily call sessions." The Open Board of Stock Brokers merged with the NYSE in 1869. Robert Wright of "Bloomberg" writes that the merger increased the NYSE's members as well as trading volume, as "several dozen regional exchanges were also competing with the NYSE for customers. Buyers, sellers and dealers all wanted to complete transactions as quickly and cheaply as technologically possible and that meant finding the markets with the most trading, or the greatest liquidity in today’s parlance. Minimizing competition was essential to keep a large number of orders flowing, and the merger helped the NYSE to maintain its reputation for providing superior liquidity." The Civil War greatly stimulated speculative securities trading in New York. By 1869 membership had to be capped, and has been sporadically increased since. The latter half of the nineteenth century saw rapid growth in securities trading.

Securities trade in the latter nineteenth and early twentieth centuries was prone to panics and crashes. Government regulation of securities trading was eventually seen as necessary, with arguably the most dramatic changes occurring in the 1930s after a major stock market crash precipitated an economic depression.

The Stock Exchange Luncheon Club was situated on the seventh floor from 1898 until its closure in 2006.

The main building, located at 18 Broad Street, between the corners of Wall Street and Exchange Place, was designated a National Historic Landmark in 1978, as was the 11 Wall Street building.
The NYSE announced its plans to merge with Archipelago on April 21, 2005, in a deal intended to reorganize the NYSE as a publicly traded company. NYSE's governing board voted to merge with rival Archipelago on December 6, 2005, and became a for-profit, public company. It began trading under the name NYSE Group on March 8, 2006. A little over one year later, on April 4, 2007, the NYSE Group completed its merger with Euronext, the European combined stock market, thus forming NYSE Euronext, the first transatlantic stock exchange.

Wall Street is the leading US money center for international financial activities and the foremost US location for the conduct of wholesale financial services. "It comprises a matrix of wholesale financial sectors, financial markets, financial institutions, and financial industry firms" (Robert, 2002). The principal sectors are securities industry, commercial banking, asset management, and insurance.

Prior to the acquisition of NYSE Euronext by the ICE in 2013, Marsh Carter was the Chairman of the NYSE and the CEO was Duncan Niederauer. Presently, the chairman is Jeffrey Sprecher. In 2016, NYSE owner Intercontinental Exchange Inc. earned $419 million in listings-related revenues.

The exchange was closed shortly after the beginning of World War I (July 31, 1914), but it partially re-opened on November 28 of that year in order to help the war effort by trading bonds, and completely reopened for stock trading in mid-December.

On September 16, 1920, a bomb exploded on Wall Street outside the NYSE building, killing 33 people and injuring more than 400. The perpetrators were never found. The NYSE building and some buildings nearby, such as the JP Morgan building, still have marks on their façades caused by the bombing.

The Black Thursday crash of the Exchange on October 24, 1929, and the sell-off panic which started on Black Tuesday, October 29, are often blamed for precipitating the Great Depression. In an effort to try to restore investor confidence, the Exchange unveiled a fifteen-point program aimed to upgrade protection for the investing public on October 31, 1938.

On October 1, 1934, the exchange was registered as a national securities exchange with the U.S. Securities and Exchange Commission, with a president and a thirty-three-member board. On February 18, 1971, the non-profit corporation was formed, and the number of board members was reduced to twenty-five.

One of Abbie Hoffman's well-known publicity stunts took place in 1967, when he led members of the Yippie movement to the Exchange's gallery. The provocateurs hurled fistfuls of dollars toward the trading floor below. Some traders booed, and some laughed and waved. Three months later the stock exchange enclosed the gallery with bulletproof glass. Hoffman wrote a decade later, "We didn't call the press; at that time we really had no notion of anything called a media event."
On October 19, 1987, the Dow Jones Industrial Average (DJIA) dropped 508 points, a 22.6% loss in a single day, the second-biggest one-day drop the exchange had experienced. Black Monday was followed by Terrible Tuesday, a day in which the Exchange's systems did not perform well and some people had difficulty completing their trades.

Subsequently, there was another major drop for the Dow on October 13, 1989—the Mini-Crash of 1989. The crash was apparently caused by a reaction to a news story of a $6.75 billion leveraged buyout deal for UAL Corporation, the parent company of United Airlines, which broke down. When the UAL deal fell through, it helped trigger the collapse of the junk bond market causing the Dow to fall 190.58 points, or 6.91 percent.

Similarly, there was a panic in the financial world during the year of 1997; the Asian Financial Crisis. Like the fall of many foreign markets, the Dow suffered a 7.18% drop in value (554.26 points) on October 27, 1997, in what later became known as the 1997 Mini-Crash but from which the DJIA recovered quickly. This was the first time that the "circuit breaker" rule had operated.

On January 26, 2000, an altercation during filming of the music video for "Sleep Now in the Fire," which was directed by Michael Moore, caused the doors of the exchange to be closed and the band Rage Against the Machine to be escorted from the site by security after band members attempted to gain entry into the exchange.

In the aftermath of the September 11 attacks, the NYSE was closed for 4 trading sessions, resuming on Monday, September 17, one of the rare times the NYSE was closed for more than one session and only the third time since March 1933. On the first day, the NYSE suffered a 7.1% drop in value (684 points), after a week, it dropped by 14% (1370 points). An estimated of $1.4 trillion was lost within five days of trading. The NY Stock Exchange is only 5 blocks from Ground Zero.

On May 6, 2010, the Dow Jones Industrial Average posted its largest intraday percentage drop since the October 19, 1987, crash, with a 998-point loss later being called the 2010 Flash Crash (as the drop occurred in minutes before rebounding). The SEC and CFTC published a report on the event, although it did not come to a conclusion as to the cause. The regulators found no evidence that the fall was caused by erroneous ("fat finger") orders.

On October 29, 2012, the stock exchange was shut down for 2 days due to Hurricane Sandy.
The last time the stock exchange was closed due to weather for a full two days was on March 12 and 13 in 1888.

On May 1, 2014, the stock exchange was fined $4.5 million by the Securities and Exchange Commission to settle charges it violated market rules.

On August 14, 2014, Berkshire Hathaway's A Class shares, the highest priced shares on the NYSE, hit $200,000 a share for the first time.

On July 8, 2015, technical issues affected the stock exchange, halting trading at 11:32 am ET. The NYSE reassured stock traders that the outage was "not a result of a cyber breach," and the Department of Homeland Security confirmed that there was "no sign of malicious activity." Trading eventually resumed at 3:10 pm ET the same day.

On May 25, 2018, Stacey Cunningham, the NYSE’s chief operating officer, will become the Big Board’s 67th president, succeeding Thomas Farley. She will be the first female leader in the exchange's 226-year history.

The New York Stock Exchange is closed on New Years Day, Martin Luther King, Jr. Day, Washington's Birthday, Good Friday, Memorial Day, Fourth of July, Labor Day, Thanksgiving and Christmas. When those holidays occur on a weekend, the holiday is observed on the closest weekday. In addition, the Stock Exchange closes early on the day before Independence Day, the day after Thanksgiving, and the day before Christmas.

The New York Stock Exchange (sometimes referred to as "the Big Board") provides a means for buyers and sellers to trade shares of stock in companies registered for public trading. The NYSE is open for trading Monday through Friday from 9:30 am – 4:00 pm ET, with the exception of holidays declared by the Exchange in advance.

The NYSE trades in a continuous auction format, where traders can execute stock transactions on behalf of investors. They will gather around the appropriate post where a specialist broker, who is employed by a NYSE member firm (that is, he/she is not an employee of the New York Stock Exchange), acts as an auctioneer in an open outcry auction market environment to bring buyers and sellers together and to manage the actual auction. They do on occasion (approximately 10% of the time) facilitate the trades by committing their own capital and as a matter of course disseminate information to the crowd that helps to bring buyers and sellers together. The auction process moved toward automation in 1995 through the use of wireless hand held computers (HHC). The system enabled traders to receive and execute orders electronically via wireless transmission. On September 25, 1995, NYSE member Michael Einersen, who designed and developed this system, executed 1000 shares of IBM through this HHC ending a 203-year process of paper transactions and ushering in an era of automated trading.

As of January 24, 2007, all NYSE stocks can be traded via its electronic hybrid market (except for a small group of very high-priced stocks). Customers can now send orders for immediate electronic execution, or route orders to the floor for trade in the auction market. In the first three months of 2007, in excess of 82% of all order volume was delivered to the floor electronically. NYSE works with US regulators like the SEC and CFTC to coordinate risk management measures in the electronic trading environment through the implementation of mechanisms like circuit breakers and liquidity replenishment points.

Until 2005, the right to directly trade shares on the exchange was conferred upon owners of the 1366 "seats." The term comes from the fact that up until the 1870s NYSE members sat in chairs to trade. In 1868, the number of seats was fixed at 533, and this number was increased several times over the years. In 1953, the number of seats was set at 1,366. These seats were a sought-after commodity as they conferred the ability to directly trade stock on the NYSE, and seat holders were commonly referred to as members of the NYSE. The Barnes family is the only known lineage to have five generations of NYSE members: Winthrop H. Barnes (admitted 1894), Richard W.P. Barnes (admitted 1926), Richard S. Barnes (admitted 1951), Robert H. Barnes (admitted 1972), Derek J. Barnes (admitted 2003). Seat prices varied widely over the years, generally falling during recessions and rising during economic expansions. The most expensive inflation-adjusted seat was sold in 1929 for $625,000, which, today, would be over six million dollars. In recent times, seats have sold for as high as $4 million in the late 1990s and as low as $1 million in 2001. In 2005, seat prices shot up to $3.25 million as the exchange entered into an agreement to merge with Archipelago and became a for-profit, publicly traded company. Seat owners received $500,000 in cash per seat and 77,000 shares of the newly formed corporation. The NYSE now sells one-year licenses to trade directly on the exchange. Licenses for floor trading are available for $40,000 and a license for bond trading is available for as little as $1,000 as of 2010. Neither are resell-able, but may be transferable during a change of ownership of a corporation holding a trading license.

Following the Black Monday market crash in 1987, NYSE imposed trading curbs to reduce market volatility and massive panic sell-offs. Following the 2011 rule change, at the start of each trading day, the NYSE sets three circuit breaker levels at levels of 7% (Level 1), 13% (Level 2), and 20% (Level 3) of the average closing price of the S&P 500 for the preceding trading day. Level 1 and Level 2 declines result in a 15-minute trading halt unless they occur after 3:25 pm, when no trading halts apply. A Level 3 decline results in trading being suspended for the remainder of the day. (The biggest one-day decline in the S&P 500 since 1987 was the 9.0% drop on October 15, 2008.)

In the mid-1960s, the NYSE Composite Index (NYSE: NYA) was created, with a base value of 50 points equal to the 1965 yearly close. This was done to reflect the value of all stocks trading at the exchange instead of just the 30 stocks included in the Dow Jones Industrial Average. To raise the profile of the composite index, in 2003 the NYSE set its new base value of 5,000 points equal to the 2002 yearly close. Its close at the end of 2013 was 10,400.32.


In October 2008 NYSE Euronext completed acquisition of the American Stock Exchange (AMEX) for $260 million in stock.

On February 15, 2011, NYSE and Deutsche Börse announced their merger to form a new company, as yet unnamed, wherein Deutsche Börse shareholders will have 60% ownership of the new entity, and NYSE Euronext shareholders will have 40%.

On February 1, 2012, the European Commission blocked the merger of NYSE with Deutsche Börse, after commissioner Joaquin Almunia stated that the merger "would have led to a near-monopoly in European financial derivatives worldwide." Instead, Deutsche Börse and NYSE will have to sell either their Eurex derivatives or LIFFE shares in order to not create a monopoly. On February 2, 2012, NYSE Euronext and Deutsche Börse agreed to scrap the merger.

In April 2011, Intercontinental Exchange (ICE), an American futures exchange, and NASDAQ OMX Group had together made an unsolicited proposal to buy NYSE Euronext for approximately , a deal in which NASDAQ would have taken control of the stock exchanges. NYSE Euronext rejected this offer twice, but it was finally terminated after the United States Department of Justice indicated their intention to block the deal due to antitrust concerns.

In December 2012, ICE had proposed to buy NYSE Euronext in a stock swap with a valuation of $8 billion. NYSE Euronext shareholders would receive either $33.12 in cash, or $11.27 in cash and approximately a sixth of a share of ICE. The chairman and CEO of ICE, Jeffrey Sprecher, will retain those positions, but four members of the NYSE board of directors will be added to the ICE board.

The NYSE's opening and closing bells mark the beginning and the end of each trading day. The 'opening bell' is rung at 9:30 am ET to mark the start of the day's trading session. At 4 pm ET the 'closing bell' is rung and trading for the day stops. There are bells located in each of the four main sections of the NYSE that all ring at the same time once a button is pressed. There are three buttons that control the bells, located on the control panel behind the podium which overlooks the trading floor. The main bell, which is rung at the beginning and end of the trading day, is controlled by a green button. The second button, colored orange, activates a single-stroke bell that is used to signal a moment of silence. A third, red button controls a backup bell which is used in case the main bell fails to ring.

The signal to start and stop trading was not always a bell. The original signal was a gavel (which is still in use today along with the bell), but during the late 1800s, the NYSE decided to switch the gavel for a gong to signal the day's beginning and end. After the NYSE changed to its present location at 18 Broad Street in 1903, the gong was switched to the bell format that is currently being used.

A common sight today is the highly publicized events in which a celebrity or executive from a corporation stands behind the NYSE podium and pushes the button that signals the bells to ring. Due to the amount of coverage that the opening/closing bells receive, many companies coordinate new product launches and other marketing-related events to start on the same day as when the company's representatives ring the bell. It was only in 1995 that the NYSE began having special guests ring the bells on a regular basis; prior to that, ringing the bells was usually the responsibility of the exchange's floor managers.

Many of the people who ring the bell are business executives whose companies trade on the exchange. However, there have also been many famous people from outside the world of business that have rung the bell. Athletes such as Joe DiMaggio of the New York Yankees and Olympic swimming champion Michael Phelps, entertainers such as rapper Snoop Dogg, singer and actress Liza Minnelli and members of the band Kiss, and politicians such as Mayor of New York City Rudy Giuliani and President of South Africa Nelson Mandela have all had the honor of ringing the bell. Two United Nations Secretaries General have also rung the bell. On April 27, 2006, Secretary-General Kofi Annan rang the opening bell to launch the United Nations Principles for Responsible Investment. On July 24, 2013, Secretary-General Ban Ki-moon rang the closing bell to celebrate the NYSE joining the United Nations Sustainable Stock Exchanges Initiative.

In addition there have been many bell-ringers who are famous for heroic deeds, such as members of the New York police and fire departments following the events of 9/11, members of the United States Armed Forces serving overseas, and participants in various charitable organizations.

There have also been several fictional characters that have rung the bell, including Mickey Mouse, the Pink Panther, Mr. Potato Head, the Aflac Duck, and Darth Vader.


Notes
Bibliography



</doc>
<doc id="21561" url="https://en.wikipedia.org/wiki?curid=21561" title="Nanoengineering">
Nanoengineering

Nanoengineering is the practice of engineering on the nanoscale. It derives its name from the nanometre, a unit of measurement equalling one billionth of a meter.

Nanoengineering is largely a synonym for nanotechnology, but emphasizes the engineering rather than the pure science aspects of the field.

The first nanoengineering program was started at the University of Toronto within the Engineering Science program as one of the options of study in the final years. In 2003, the Lund Institute of Technology started a program in Nanoengineering. In 2004, the College of Nanoscale Science and Engineering at SUNY Polytechnic Institute was established on the campus of the University at Albany. In 2005, the University of Waterloo established a unique program which offers a full degree in Nanotechnology Engineering. Louisiana Tech University started the first program in the U.S. in 2005. In 2006 the University of Duisburg-Essen started a Bachelor and a Master program NanoEngineering. Unlike early NanoEngineering programs, the first Nanoengineering Department in the world, offering both undergraduate and graduate degrees, was established by the University of California, San Diego in 2007.
In 2009, the University of Toronto began offering all Options of study in Engineering Science as degrees, bringing the second nanoengineering degree to Canada. Rice University established in 2016 a Department of Materials Science and NanoEngineering (MSNE).
DTU Nanotech - the Department of Micro- and Nanotechnology - is a department at the Technical University of Denmark established in 1990.

In 2013, Wayne State University began offering a Nanoengineering Undergraduate Certificate Program, which is funded by a Nanoengineering Undergraduate Education (NUE) grant from the National Science Foundation. The primary goal is to offer specialized undergraduate training in nanotechnology. Other goals are: 1) to teach emerging technologies at the undergraduate level, 2) to train a new adaptive workforce, and 3) to retrain working engineers and professionals.



 


</doc>
<doc id="21562" url="https://en.wikipedia.org/wiki?curid=21562" title="NP (complexity)">
NP (complexity)

In computational complexity theory, NP (for nondeterministic polynomial time) is a complexity class used to describe certain types of decision problems. Informally, NP is the set of all decision problems for which the instances where the answer is "yes" have efficiently "verifiable" proofs. More precisely, these proofs have to be verifiable by deterministic computations that can be performed in polynomial time. 

Equivalently, the formal definition of NP is the set of decision problems "solvable" in polynomial time by a theoretical non-deterministic Turing machine. This second definition is the basis for the abbreviation NP, which stands for "nondeterministic, polynomial time." However, the "verifier-based definition" tends to be more intuitive and practical in common applications compared to the formal "machine definition". The two definitions are equivalent because the algorithm for the machine definition consists of two phases, the first of which consists of a guess about the solution, which is generated in a non-deterministic way, while the second phase consists of a deterministic algorithm that verifies or rejects the guess as a valid solution to the problem.

A method for solving a decision problem is given in the form of an algorithm. In the above definitions for NP, "polynomial time" refers to the increasing number of machine operations needed by an algorithm relative to the size of the problem. Polynomial time is therefore a measure of efficiency of an algorithm. Decision problems are commonly categorized into complexity classes (such as NP) based on the fastest known machine algorithms. As such, decision problems may change class if a faster algorithm is discovered.

The complexity class P is contained in NP, but NP contains many important problems, the hardest of which are called NP-complete problems, whose solutions are sufficient to deal with any other NP problem in polynomial time. The most important open question in complexity theory, the P versus NP (“P = NP?”) problem, asks whether polynomial time algorithms actually exist for solving NP-complete, and by corollary, all NP problems. It is widely believed that this is not the case.

The complexity class NP (which have efficiently verifiable proofs where the answer is "yes") is also related to the complexity class co-NP (which have efficiently verifiable proofs where the answer is "no"). Whether or not NP = co-NP is another outstanding question in complexity theory.

The complexity class NP can be defined in terms of NTIME as follows:

where formula_2 is the set of decision problems that can be solved by a non-deterministic Turing machine in formula_3 time.

Alternatively, NP can be defined using deterministic Turing machines as verifiers. A language "L" is in NP if and only if there exist polynomials "p" and "q", and a deterministic Turing machine "M", such that


Many natural computer science problems are covered by the class NP.
In particular, the decision versions of many interesting search problems and optimization problems are contained in NP.

In order to explain the verifier-based definition of NP, let us consider the subset sum problem:
Assume that we are given some integers, such as {−7, −3, −2, 5, 8}, and we wish to know whether some of these integers sum up to zero. In this example, the answer is "yes", since the subset of integers {−3, −2, 5} corresponds to the sum The task of deciding whether such a subset with sum zero exists is called the "subset sum problem".

To answer if some of the integers add to zero we can create an algorithm which obtains all the possible subsets. As the number of integers that we feed into the algorithm becomes larger, the number of subsets grows exponentially and so does the computation time. However, notice that, if we are given a particular subset (often called a certificate), we can easily check or "verify" whether the subset sum is zero, by just summing up the integers of the subset. So if the sum is indeed zero, that particular subset is the "proof" or witness for the fact that the answer is "yes". An algorithm that verifies whether a given subset has sum zero is called "verifier".

More generally, a problem is said to be in NP if there exists a verifier V for the problem.
Given any instance I of problem P, where the answer is "yes", there must exist a certificate (also called a witness) W such that, given the ordered pair (I,W) as input, V returns the answer "yes" in polynomial time. Furthermore, if the answer to I is "no", the verifier will return "no" with input (I,W) for all possible W. Note that V could return the answer "No" even if the answer to I is "yes", if W is not a valid witness. For example, in the subset sum problem, if there is a subset whose sum is zero, but we select W to be a subset whose sum is not zero, the verifier will return "No", while if there is no subset whose sum is zero, the verifier will return "no" regardless of the choice of W. The verifier needs only polynomial time (it just needs to check whether W is really a subset of I, and whether the sum of W is zero), so the subset sum problem is in NP.

The "no"-answer version of this problem is stated as: "given a finite set of integers, does every non-empty subset have a nonzero sum?". The verifier-based definition of NP does "not" require an easy-to-verify certificate for the "no"-answers. The class of problems with such certificates for the "no"-answers is called co-NP. In fact, it is an open question whether all problems in NP also have certificates for the "no"-answers and thus are in co-NP.

Equivalent to the verifier-based definition is the following characterization: NP is the set of decision problems solvable by a non-deterministic Turing machine that runs in polynomial time. (This means that there is an accepting computation path if a word is in the language – co-NP is defined dually with rejecting paths.) This definition is equivalent to the verifier-based definition because a non-deterministic Turing machine could solve an NP problem in polynomial time by non-deterministically selecting a certificate and running the verifier on the certificate. Similarly, if such a machine exists, then a polynomial time verifier can naturally be constructed from it.

This is an incomplete list of problems that are in NP.


NP is closed under:


It is not known whether NP is closed under complement (this question is the so-called "NP versus co-NP" question)

Because of the many important problems in this class, there have been extensive efforts to find polynomial-time algorithms for problems in NP. However, there remain a large number of problems in NP that defy such attempts, seeming to require super-polynomial time. Whether these problems are not decidable in polynomial time is one of the greatest open questions in computer science (see P versus NP ("P=NP") problem for an in-depth discussion).

An important notion in this context is the set of NP-complete decision problems, which is a subset of NP and might be informally described as the "hardest" problems in NP. If there is a polynomial-time algorithm for even "one" of them, then there is a polynomial-time algorithm for "all" the problems in NP. Because of this, and because dedicated research has failed to find a polynomial algorithm for any NP-complete problem, once a problem has been proven to be NP-complete this is widely regarded as a sign that a polynomial algorithm for this problem is unlikely to exist.

However, in practical uses, instead of spending computational resources looking for an optimal solution, a good enough (but potentially suboptimal) solution may often be found in polynomial time. Also, the real life applications of some problems are easier than their theoretical equivalents.

The two definitions of NP as the class of problems solvable by a nondeterministic Turing machine (TM) in polynomial time and the class of problems verifiable by a deterministic Turing machine in polynomial time are equivalent. The proof is described by many textbooks, for example Sipser's "Introduction to the Theory of Computation", section 7.3.

To show this, first suppose we have a deterministic verifier. A nondeterministic machine can simply nondeterministically run the verifier on all possible proof strings (this requires only polynomially many steps because it can nondeterministically choose the next character in the proof string in each step, and the length of the proof string must be polynomially bounded). If any proof is valid, some path will accept; if no proof is valid, the string is not in the language and it will reject.

Conversely, suppose we have a nondeterministic TM called A accepting a given language L. At each of its polynomially many steps, the machine's computation tree branches in at most a finite number of directions. There must be at least one accepting path, and the string describing this path is the proof supplied to the verifier. The verifier can then deterministically simulate A, following only the accepting path, and verifying that it accepts at the end. If A rejects the input, there is no accepting path, and the verifier will always reject.

NP contains all problems in P, since one can verify any instance of the problem by simply ignoring the proof and solving it. NP is contained in PSPACE—to show this, it suffices to construct a PSPACE machine that loops over all proof strings and feeds each one to a polynomial-time verifier. Since a polynomial-time machine can only read polynomially many bits, it cannot use more than polynomial space, nor can it read a proof string occupying more than polynomial space (so we do not have to consider proofs longer than this). NP is also contained in EXPTIME, since the same algorithm operates in exponential time.

co-NP contains those problems which have a simple proof for "no" instances, sometimes called counterexamples. For example, primality testing trivially lies in co-NP, since one can refute the primality of an integer by merely supplying a nontrivial factor. NP and co-NP together form the first level in the polynomial hierarchy, higher only than P.

NP is defined using only deterministic machines. If we permit the verifier to be probabilistic (this however, is not necessarily a BPP machine), we get the class MA solvable using an Arthur-Merlin protocol with no communication from Arthur to Merlin.

NP is a class of decision problems; the analogous class of function problems is FNP.

The only known strict inclusions came from the time hierarchy theorem and the space hierarchy theorem, and respectively they are formula_4 and formula_5.

In terms of descriptive complexity theory, NP corresponds precisely to the set of languages definable by existential second-order logic (Fagin's theorem).

NP can be seen as a very simple type of interactive proof system, where the prover comes up with the proof certificate and the verifier is a deterministic polynomial-time machine that checks it. It is complete because the right proof string will make it accept if there is one, and it is sound because the verifier cannot accept if there is no acceptable proof string.

A major result of complexity theory is that NP can be characterized as the problems solvable by probabilistically checkable proofs where the verifier uses O(log "n") random bits and examines only a constant number of bits of the proof string (the class PCP(log "n", 1)). More informally, this means that the NP verifier described above can be replaced with one that just "spot-checks" a few places in the proof string, and using a limited number of coin flips can determine the correct answer with high probability. This allows several results about the hardness of approximation algorithms to be proven.

The decision version of the travelling salesman problem is in NP. Given an input matrix of distances between "n" cities, the problem is to determine if there is a route visiting all cities with total distance less than "k".

A proof certificate can simply be a list of the cities. Then verification can clearly be done in polynomial time by a deterministic Turing machine. It simply adds the matrix entries corresponding to the paths between the cities.

A non-deterministic Turing machine can find such a route as follows:


One can think of each guess as "forking" a new copy of the Turing machine to follow each of the possible paths forward, and if at least one machine finds a route of distance less than "k", that machine accepts the input. (Equivalently, this can be thought of as a single Turing machine that always guesses correctly)

A binary search on the range of possible distances can convert the decision version of Traveling Salesman to the optimization version, by calling the decision version repeatedly (a polynomial number of times).





</doc>
<doc id="21565" url="https://en.wikipedia.org/wiki?curid=21565" title="November 5">
November 5





</doc>
<doc id="21566" url="https://en.wikipedia.org/wiki?curid=21566" title="Noam Chomsky">
Noam Chomsky

Avram Noam Chomsky (born December 7, 1928) is an American linguist, philosopher, cognitive scientist, historian, social critic and political activist. Sometimes described as "the father of modern linguistics", Chomsky is also a major figure in analytic philosophy and one of the founders of the field of cognitive science. He holds a joint appointment as Institute Professor Emeritus at the Massachusetts Institute of Technology (MIT) and laureate professor at the University of Arizona, and is the author of over 100 books on topics such as linguistics, war, politics, and mass media. Ideologically, he aligns with anarcho-syndicalism and libertarian socialism.
Born to middle-class Ashkenazi Jewish immigrants in Philadelphia, Chomsky developed an early interest in anarchism from alternative bookstores in New York City. He began studying at the University of Pennsylvania at age 16, taking courses in linguistics, mathematics, and philosophy. From 1951 to 1955, he was appointed to Harvard University's Society of Fellows. While at Harvard, he developed the theory of transformational grammar; for this, he was awarded his doctorate in 1955. Chomsky began teaching at MIT in 1957 and emerged as a significant figure in the field of linguistics for his landmark work "Syntactic Structures", which remodeled the scientific study of language. From 1958 to 1959, he was a National Science Foundation fellow at the Institute for Advanced Study. Chomsky is credited as the creator or co-creator of the universal grammar theory, the generative grammar theory, the Chomsky hierarchy, and the minimalist program. Chomsky also played a pivotal role in the decline of behaviorism, being particularly critical of the work of B. F. Skinner.
Chomsky vocally opposed U.S. involvement in the Vietnam War, believing the war to be an act of American imperialism. In 1967, Chomsky attracted widespread public attention for his anti-war essay entitled "The Responsibility of Intellectuals". Associated with the New Left, he was arrested multiple times for his activism and was placed on Nixon's Enemies List. While expanding his work in linguistics over subsequent decades, he also became involved in the Linguistics Wars. In collaboration with Edward S. Herman, Chomsky later co-wrote , which articulated the propaganda model of media criticism, and worked to expose the Indonesian occupation of East Timor. Additionally, his defense of unconditional freedom of speech—including free speech for Holocaust deniers—generated significant controversy in the Faurisson affair of the early 1980s. Following his retirement from active teaching, Chomsky has continued his vocal political activism by opposing the War on Terror and supporting the Occupy Movement.
One of the most cited scholars in history, Chomsky has influenced a broad array of academic fields. He is widely recognized as a paradigm shifter who helped spark a major revolution in the human sciences, contributing to the development of a new cognitivistic framework for the study of language and the mind. In addition to his continued scholarly research, he remains a leading critic of U.S. foreign policy, neoliberalism and contemporary state capitalism, the Israeli–Palestinian conflict, and mainstream news media. His ideas have proved highly significant within the anti-capitalist and anti-imperialist movements. Some of his critics have accused him of anti-Americanism.

Avram Noam Chomsky was born on December 7, 1928, in the East Oak Lane neighborhood of Philadelphia, Pennsylvania. His father was William "Zev" Chomsky, an Ashkenazi Jew originally from Ukraine who had fled to the United States in 1913. Having studied at Johns Hopkins University, William went on to become school principal of the Congregation Mikveh Israel religious school, and in 1924 was appointed to the faculty at Gratz College in Philadelphia. Chomsky's mother was the Belarusian-born Elsie Simonofsky (1904–1972), a teacher and activist whom William had met while working at Mikveh Israel.

Noam was the Chomsky family's first child. His younger brother, David Eli Chomsky, was born five years later. The brothers were close, although David was more easygoing while Noam could be very competitive. Chomsky and his brother were raised Jewish, being taught Hebrew and regularly discussing the political theories of Zionism; the family was particularly influenced by the Left Zionist writings of Ahad Ha'am. As a Jew, Chomsky faced anti-semitism as a child, particularly from the Irish and German communities living in Philadelphia.

Chomsky described his parents as "normal Roosevelt Democrats" who had a center-left position on the political spectrum; however, he was exposed to far-left politics through other members of the family, a number of whom were socialists involved in the International Ladies' Garment Workers' Union. He was substantially influenced by his uncle who owned a newspaper stand in New York City, where Jewish leftists came to debate the issues of the day. Whenever visiting his uncle, Chomsky frequented left-wing and anarchist bookstores in the city, voraciously reading political literature. He later described his discovery of anarchism as "a lucky accident", because it allowed him to become critical of other far-left ideologies, namely Stalinism and other forms of Marxism–Leninism.

Chomsky's primary education was at Oak Lane Country Day School, an independent Deweyite institution that focused on allowing its pupils to pursue their own interests in a non-competitive atmosphere. It was here, at the age of 10, that he wrote his first article, on the spread of fascism, following the fall of Barcelona to Francisco Franco's fascist army in the Spanish Civil War. At the age of 12, Chomsky moved on to secondary education at Central High School, where he joined various clubs and societies and excelled academically, but was troubled by the hierarchical and regimented method of teaching used there. During the same time period, Chomsky attended the Hebrew High School at Gratz College. From the age of 12 or 13, he identified more fully with anarchist politics.

In 1945, Chomsky, aged 16, embarked on a general program of study at the University of Pennsylvania, where he explored philosophy, logic, and languages and developed a primary interest in learning Arabic. Living at home, he funded his undergraduate degree by teaching Hebrew. However, he was frustrated with his experiences at the university, and considered dropping out and moving to a kibbutz in Mandatory Palestine. His intellectual curiosity was reawakened through conversations with the Russian-born linguist Zellig Harris, whom he first met in a political circle in 1947. Harris introduced Chomsky to the field of theoretical linguistics and convinced him to major in the subject. Chomsky's B.A. honors thesis was titled "Morphophonemics of Modern Hebrew", and involved his applying Harris's methods to the language. Chomsky revised this thesis for his M.A., which he received at Penn in 1951; it would subsequently be published as a book. He also developed his interest in philosophy while at university, in particular under the tutelage of his teacher Nelson Goodman.

From 1951 to 1955, Chomsky was named to the Society of Fellows at Harvard University, where he undertook research on what would become his doctoral dissertation. Having been encouraged by Goodman to apply, a significant factor in his decision to move to Harvard was that the philosopher W. V. Quine was based there. Both Quine and a visiting philosopher, J. L. Austin of the University of Oxford, would strongly influence Chomsky. In 1952, Chomsky published his first academic article, "Systems of Syntactic Analysis", which appeared not in a journal of linguistics, but in "The Journal of Symbolic Logic". Being highly critical of the established behaviorist currents in linguistics, in 1954 he presented his ideas at lectures given at the University of Chicago and Yale University. Although he had not been registered as a student at Pennsylvania for four years, in 1955 he submitted a thesis to them setting out his ideas on transformational grammar; he was awarded his Ph.D. on the basis of it, and it would be privately distributed among specialists on microfilm before being published in 1975 as part of "The Logical Structure of Linguistic Theory". Possession of this Ph.D. nullified his requirement to enter national service in the armed forces, which was otherwise due to begin in 1955. George Armitage Miller, a Professor at Harvard, read the Ph.D. and was impressed; together he and Chomsky published a number of technical papers in mathematical linguistics.

In 1947, Chomsky entered into a romantic relationship with Carol Doris Schatz, whom he had known since they were toddlers, and they married in 1949. After Chomsky was made a Fellow at Harvard, the couple moved to an apartment in the Allston area of Boston, remaining there until 1965, when they relocated to the suburb of Lexington. In 1953 the couple took up a Harvard travel grant in order to visit Europe, traveling from the United Kingdom through France and Switzerland and into Italy. On that same trip they also spent six weeks in Israel at Hashomer Hatzair's HaZore'a Kibbutz. Although enjoying himself, Chomsky was appalled by the Jewish nationalism and anti-Arab racism that he encountered in the country, as well as the pro-Stalinist trend that he thought pervaded the kibbutz's leftist community.

On visits to New York City, Chomsky continued to frequent the office of Yiddish anarchist journal "Freie Arbeiter Stimme", becoming enamored with the ideas of contributor Rudolf Rocker, whose work introduced him to the link between anarchism and classical liberalism. Other political thinkers whose work Chomsky read included the anarchist Diego Abad de Santillán, democratic socialists George Orwell, Bertrand Russell, and Dwight Macdonald, and works by Marxists Karl Liebknecht, Karl Korsch, and Rosa Luxemburg. His readings convinced him of the desirability of an anarcho-syndicalist society, and he became fascinated by the anarcho-syndicalist communes set up during the Spanish Civil War, which were documented in Orwell's "Homage to Catalonia" (1938). He avidly read leftist journal "politics", remarking that it "answered to and developed" his interest in anarchism, as well as the periodical "Living Marxism", published by council communist Paul Mattick. Although rejecting its Marxist basis, Chomsky was heavily influenced by council communism, voraciously reading articles in "Living Marxism" written by Antonie Pannekoek. He was also greatly interested in the Marlenite ideas of the Leninist League, an anti-Stalinist Marxist–Leninist group, sharing their views that the Second World War was orchestrated by Western capitalists and the Soviet Union's "state capitalists" to crush Europe's proletariat.

Chomsky had befriended two linguists at the Massachusetts Institute of Technology (MIT), Morris Halle and Roman Jakobson, the latter of whom secured him an assistant professor position at MIT in 1955. There Chomsky spent half his time on a mechanical translation project, and the other half teaching a course on linguistics and philosophy. Chomsky had been recruited to MIT by Jerome Wiesner, an influential scientist who, at this time, was also involved in getting the US's nuclear missile program established Having brought such missile research to MIT, Wiesner then became a nuclear strategy adviser to both Presidents Eisenhower and Kennedy, before returning to MIT to oversee research programmes at the Institute. However, despite its military involvement, Chomsky has described MIT as "a pretty free and open place, open to experimentation and without rigid requirements. It was just perfect for someone of my idiosyncratic interests and work." In 1957 MIT promoted him to the position of associate professor, and from 1957 to 1958 he was also employed by Columbia University as a visiting professor. That same year, Chomsky's first child, a daughter named Aviva, was born, and he published his first book on linguistics, "Syntactic Structures", a work that radically opposed the dominant Harris–Bloomfield trend in the field. The response to Chomsky's ideas ranged from indifference to hostility, and his work proved divisive and caused "significant upheaval" in the discipline. Linguist John Lyons later asserted that it "revolutionized the scientific study of language". From 1958 to 1959 Chomsky was a National Science Foundation fellow at the Institute for Advanced Study in Princeton, New Jersey.

In 1959 he published a review of B. F. Skinner's 1957 book "Verbal Behavior" in the journal "Language", in which he argued against Skinner's view of language as learned behavior. Opining that Skinner ignored the role of human creativity in linguistics, his review helped him to become an "established intellectual", and he proceeded to found MIT's Graduate Program in linguistics with Halle. In 1961 he was awarded academic tenure, being made a full professor in the Department of Modern Languages and Linguistics. He went on to be appointed plenary speaker at the Ninth International Congress of Linguists, held in 1962 in Cambridge, Massachusetts, which established him as the "de facto" spokesperson of American linguistics. He continued to publish his linguistic ideas throughout the decade, including in "Aspects of the Theory of Syntax" (1966), "Topics in the Theory of Generative Grammar" (1966), and "Cartesian Linguistics: A Chapter in the History of Rationalist Thought" (1966). Along with Halle, he also edited the "Studies in Language" series of books for Harper and Row, and extended the theory of generative grammar to phonology in "The Sound Pattern of English" (1968).

He continued to receive academic recognition and honors for his work, in 1966 visiting a variety of Californian institutions, first as the Linguistics Society of America Professor at the University of California, and then as the Beckman Professor at the University of California, Berkeley. His Beckman lectures would be assembled and published as "Language and Mind" in 1968. In this period, military scientists were also interested in Chomsky’s linguistics. As former Air Force Colonel, Anthony Debons, said: "much of the research conducted at MIT by Chomsky and his colleagues [has] direct application to the efforts undertaken by military scientists to develop … languages for computer operations in military command and control systems." Indeed, between 1963 and 1965, Chomsky was a consultant for a military sponsored project "to establish natural language as an operational language for command and control." One of Chomsky's students who also worked on this project, Barbara Partee, says that this research was justified to the military on the basis that "in the event of a nuclear war, the generals would be underground with some computers trying to manage things, and that it would probably be easier to teach computers to understand English than to teach the generals to program."

However, these scientists eventually found Chomsky’s theories unworkable for their computer systems. Other subsequent difficulties with the theories led to various debates between Chomsky and his critics that came to be known as the "Linguistics Wars", although they revolved largely around debating philosophical issues rather than linguistics proper.

Chomsky first involved himself in active political protest against U.S. involvement in the Vietnam War in 1962, speaking on the subject at small gatherings in churches and homes. However, it was not until 1967 that he publicly entered the debate on United States foreign policy. In February he published a widely read essay in "The New York Review of Books" entitled "The Responsibility of Intellectuals", in which he criticized the country's involvement in the conflict; the essay was based on an earlier talk that he had given to Harvard's Foundation for Jewish Campus Life. He expanded on his argument to produce his first political book, "American Power and the New Mandarins," which was published in 1969 and soon established him at the forefront of American dissent. His other political books of the time included "At War with Asia" (1971), "The Backroom Boys" (1973), "For Reasons of State" (1973), and "Peace in the Middle East?" (1975), published by Pantheon Books. Coming to be associated with the American New Left movement, he nevertheless thought little of prominent New Left intellectuals Herbert Marcuse and Erich Fromm, and preferred the company of activists to intellectuals. Although "The New York Review of Books" did publish contributions from Chomsky and other leftists from 1967 to 1973, when an editorial change put a stop to it, he was virtually ignored by the rest of the mainstream press throughout the late 1960s and early 1970s.

Along with his writings, Chomsky also became actively involved in left-wing activism. Refusing to pay half his taxes, he publicly supported students who refused the draft, and was arrested for being part of an anti-war teach-in outside the Pentagon. During this time, Chomsky, along with Mitchell Goodman, Denise Levertov, William Sloane Coffin, and Dwight Macdonald, also founded the anti-war collective RESIST. Although he questioned the objectives of the 1968 student protests, Chomsky gave many lectures to student activist groups; furthermore, he and his colleague Louis Kampf began running undergraduate courses on politics at MIT, independently of the conservative-dominated political science department.

During this period, MIT's various departments were researching helicopters, smart bombs and counterinsurgency techniques for the war in Vietnam and, as Chomsky says, "a good deal of [nuclear] missile guidance technology was developed right on the MIT campus". As Chomsky elaborates, "[MIT was] about 90% Pentagon funded at that time. And I personally was right in the middle of it. I was in a military lab ... the Research Laboratory for Electronics." By 1969, student activists were actively campaigning "to stop the war research" at MIT. Chomsky was sympathetic to the students but he also thought it best to keep such research on campus and he proposed that it should be restricted to what he called "systems of a purely defensive and deterrent character". MIT had six of its anti-war student activists sentenced to prison terms. Chomsky says MIT's students suffered things that "should not have happened." However, Chomsky has also claimed that MIT has "quite a good record on civil liberties". In 1970 Chomsky visited the Vietnamese city of Hanoi to give a lecture at the Hanoi University of Science and Technology; on this trip he also toured Laos to visit the refugee camps created by the war, and in 1973 he was among those leading a committee to commemorate the fiftieth anniversary of the War Resisters League.

As a result of his anti-war activism, Chomsky was ultimately arrested on multiple occasions, and U.S. President Richard Nixon included him on the master version of his Enemies List. He was aware of the potential repercussions of his civil disobedience, and his wife began studying for her own Ph.D. in linguistics in order to support the family in the event of Chomsky's imprisonment or loss of employment. However, MIT – despite being under some pressure to do so – refused to fire him due to his influential standing in the field of linguistics. His work in this area continued to gain international recognition; in 1967 he received honorary doctorates from both the University of London and the University of Chicago . In 1970, Loyola University and Swarthmore College also awarded him honorary D.H.L.'s, as did Bard College in 1971, Delhi University in 1972, and the University of Massachusetts in 1973.

In 1971 Chomsky gave the Bertrand Russell Memorial Lectures at the University of Cambridge, which were published as "Problems of Knowledge and Freedom" later that year. He also delivered the Whidden Lectures at McMaster University, the Huizinga Lecture at Leiden University in the Netherlands, the Woodbridge Lectures at Columbia University, and the Kant Lectures at Stanford University. In 1971 he partook in a televised debate with French philosopher Michel Foucault on Dutch television, entitled "Human Nature: Justice versus Power". Although largely agreeing with Foucault's ideas, he was critical of post-modernism and French philosophy generally, believing that post-modern leftist philosophers used obfuscating language which did little to aid the cause of the working-classes and lambasting France as having "a highly parochial and remarkably illiterate culture". Chomsky also continued to publish prolifically in linguistics, publishing "Studies on Semantics in Generative Grammar" (1972), an enlarged edition of "Language and Mind" (1972), and "Reflections on Language" (1975). In 1974 he became a corresponding fellow of the British Academy.

Throughout the late 1970s and 1980s, Chomsky's publications expanded and clarified his earlier work, addressing his critics and updating his grammatical theory. His public talks often generated considerable controversy, particularly when he criticized actions of the Israeli government and military, and his political views came under attack from right-wing and centrist figures, the most prominent of whom was Alan Dershowitz. Chomsky considered Dershowitz "a complete liar" and accused him of actively misrepresenting his position on issues. Furthermore, during the early 1970s he had begun collaborating with Edward S. Herman, who had also published critiques of the U.S. war in Vietnam. Together they authored "", a book which criticized U.S. military involvement in Southeast Asia and highlighted how mainstream media neglected to cover stories about these activities; the publisher Warner Modular initially accepted it, and it was published in 1973. However, Warner Modular's parent company, Warner Communications, disapproved of the book's contents and ordered all copies to be destroyed.

While mainstream publishing options proved elusive, Chomsky found support from Michael Albert's South End Press, an activist-oriented publishing company.
In 1979, Chomsky and Herman revised "Counter-Revolutionary Violence" and published it with South End Press as the two-volume "The Political Economy of Human Rights". In this they compared U.S. media reactions to the Cambodian genocide and the Indonesian occupation of East Timor. They argued that because Indonesia was a U.S. ally, U.S. media ignored the East Timorese situation while focusing on that in Cambodia, a U.S. enemy. Taking a particular interest in the situation in East Timor, Chomsky testified on the subject in front of the United Nations' Special Committee on Decolonization in both 1978 and 1979, and attended a conference on the occupation held in Lisbon in 1979. The following year, the Marxist academic, Steven Lukes authored an article for the "Times Higher Education Supplement" accusing Chomsky of betraying his anarchist ideals and acting as an apologist for Cambodian leader Pol Pot. Laura J. Summers and Robin Woodsworth Carlsen replied to the article, arguing that Lukes completely misunderstood Chomsky and Herman's work. The controversy damaged his reputation, and Chomsky maintains that his critics deliberately printed lies about him in order to defame him.

Although Chomsky had long publicly criticized Nazism and totalitarianism more generally, his commitment to freedom of speech led him to defend the right of French historian Robert Faurisson to advocate a position widely characterized as Holocaust denial. Without Chomsky's knowledge, his plea for the historian's freedom of speech was published as the preface to Faurisson's 1980 book "Mémoire en défense contre ceux qui m'accusent de falsifier l'histoire". Chomsky was widely condemned for defending Faurisson, and France's mainstream press accused Chomsky of being a Holocaust denier himself, refusing to publish his rebuttals to their accusations. Critiquing Chomsky's position, sociologist Werner Cohn later published an analysis of the affair titled "Partners in Hate: Noam Chomsky and the Holocaust Deniers". The Faurisson affair had a lasting, damaging effect on Chomsky's career, and Chomsky did not visit France, where the translation of his political writings was delayed until the 2000s, for almost thirty years following the debacle.

The election of Republican Party candidate Ronald Reagan to the U.S. Presidency in 1980 marked a period of increased military intervention in Central America. In 1985, during Nicaragua's Contra War – in which the U.S. supported the Contra militia against the Sandinista government – Chomsky travelled to Managua to meet with workers' organizations and refugees of the conflict, giving public lectures on politics and linguistics. Many of these lectures would be published in 1987 as "On Power and Ideology: The Managua Lectures". In 1983 he published "The Fateful Triangle", an examination of the Israel-Palestine conflict and the place of the U.S. within it, arguing that the U.S. had continually used the conflict for its own ends. In 1988, Chomsky then visited the Palestinian territories to witness the impact of Israeli military occupation.

In 1988, Chomsky and Herman published "Manufacturing Consent: The Political Economy of the Mass Media", in which they outlined their propaganda model for understanding the mainstream media; there they argued that even in countries without official censorship, the news provided was censored through four filters which had a great impact on what stories are reported and how they are presented. The book was adapted into a 1992 film, "", which was directed by Mark Achbar and Peter Wintonick. In 1989, Chomsky published "Necessary Illusions: Thought Control in Democratic Societies," in which he critiqued what he sees as the pseudo-democratic nature of Western capitalist states.

By the 1980s, a number of Chomsky's students had become leading linguistic specialists in their own right, expanding, revising, and expanding on Chomsky's ideas of generative grammar. By the end of the 1980s, Chomsky had established himself as a globally recognized figure.

In the 1990s, Chomsky embraced political activism to a greater degree than before. Retaining his commitment to the cause of East Timorese independence, in 1995 he visited Australia to talk on the issue at the behest of the East Timorese Relief Association and the National Council for East Timorese Resistance. The lectures that he gave on the subject would be published as "Powers and Prospects" in 1996. As a result of the international publicity generated by Chomsky, his biographer Wolfgang Sperlich opined that he did more to aid the cause of East Timorese independence than anyone but the investigative journalist John Pilger. After East Timor's independence from Indonesia was achieved in 1999, the Australian-led International Force for East Timor arrived as a peacekeeping force; Chomsky was critical of this, believing that it was designed to secure Australian access to East Timor's oil and gas reserves under the Timor Gap Treaty. Chomsky's book "Profit Over People: Neoliberalism and Global Order" was a Boston Globe and Voice Literary Supplement bestseller in 1999.

Chomsky retired from full-time teaching, although as an Emeritus he nevertheless continued to conduct research and seminars at MIT.

After the September 11 attacks in 2001, Chomsky was widely interviewed, with these interviews being collated and published by Seven Stories Press in October.
Chomsky argued that the ensuing War on Terror was not a new development, but rather a continuation of the same U.S. foreign policy and its concomitant rhetoric that had been pursued since at least the Reagan era of the 1980s. In 2003 he published "Hegemony or Survival", in which he articulated what he called the United States' "imperial grand strategy" and critiqued the Iraq War and other aspects of the 'War on Terror.'

Chomsky toured the world with increasing regularity during this period, giving talks on various subjects. In 2001 he gave the D.T. Lakdawala Memorial Lecture in New Delhi, India, and in 2003 visited Cuba at the invitation of the Latin American Association of Social Scientists. In 2002 Chomsky visited Turkey in order to attend the trial of a publisher who had been accused of treason for printing one of Chomsky's books; Chomsky insisted on being a co-defendant and amid international media attention the Security Courts dropped the prosecution on the first day. During that trip, Chomsky visited Kurdish areas of Turkey and spoke out in favour of the Kurds' human rights. A supporter of the World Social Forum, he attended their conferences in Brazil in both 2002 and 2003, also attending the Forum event in India.

His wife, Carol, died in December 2008.

Chomsky was drawn to the energy and activism of the Occupy movement, delivering talks at encampments and producing two works that chronicled its influence, first "Occupy" a pamphlet, in 2012, then, in 2013, "Occupy: Reflections on Class War, Rebellion and Solidarity". Both were published by Zuccotti Park Press. His analysis included a critique that attributed Occupy's growth as a response to a perceived abandonment of the interests of the white working class by the Democratic Party.

In March 2014, Chomsky joined the Nuclear Age Peace Foundation advisory council and is a senior fellow there.

In late 2015, Chomsky announced his support for Vermont U.S. senator Bernie Sanders in the upcoming 2016 United States presidential election.

In early 2016, Chomsky was publicly rebuked by President Recep Tayyip Erdoğan of Turkey after he signed an open letter condemning the Turkish leader for his anti-Kurdish repression and for holding double standards on terrorism. Chomsky accused Erdoğan of hypocrisy and added that the Turkish president supports al-Qaeda's Syrian affiliate, the al-Nusra Front. Chomsky also criticized the U.S.'s close ties with Saudi Arabia and U.S. involvement in Saudi Arabian-led intervention in Yemen, highlighting that Saudi has "one of the most grotesque human rights records in the world".

In 2016, the documentary "Requiem for the American Dream" was released, summarizing his views on capitalism and economic inequality through a "75-minute teach-in". "Requiem for the American Dream" was published as a book in 2017, and is a furthering of the ideas put forward in the 2016 documentary (Seven Stories Press).

In an interview with Al Jazeera, Chomsky called Donald Trump an "ignorant, thin-skinned megalomaniac" and a "greater evil" than Hillary Clinton. Asked about claims that Russia interfered in the U.S. presidential election through hacking, Chomsky said: "It’s possible, but it’s a kind of strange complaint in the United States. The U.S. has been interfering with, and undermining, elections all over the world for decades and is proud of it."

In August 2017, at age 88 and retired since 2002, Chomsky left MIT to join the linguistics department of University of Arizona in Tucson as part-time faculty, officially starting a few weeks later, and teaching in spring 2018. His salary is covered by philanthropy funds. Chomsky will maintain an office in Cambridge.

In July 2018, Chomsky said in an interview with Democracy Now! that "it's hard to think of a more brutal and sadistic policy" about the Trump administration family separation policy. In the same interview Chomsky also said that Alexandria Ocasio-Cortez's victory of the 2018 New York primary election "was a quite spectacular and significant event".

Within the field of linguistics, McGilvray credits Chomsky with inaugurating the "cognitive revolution". McGilvray also credits him with establishing the field as a formal, natural science, moving it away from the procedural form of structural linguistics that was dominant during the mid-20th century. As such, some have called him "the father of modern linguistics".

The basis to Chomsky's linguistic theory is rooted in biolinguistics, holding that the principles underlying the structure of language are biologically determined in the human mind and hence genetically transmitted. He therefore argues that all humans share the same underlying linguistic structure, irrespective of sociocultural differences. In adopting this position, Chomsky rejects the radical behaviorist psychology of B. F. Skinner which views the mind as a "tabula rasa" ("blank slate") and thus treats language as learned behavior. Accordingly, he argues that language is a unique evolutionary development of the human species and is unlike modes of communication used by any other animal species. Chomsky's nativist, internalist view of language is consistent with the philosophical school of "rationalism", and is contrasted with the anti-nativist, externalist view of language, which is consistent with the philosophical school of "empiricism".

Since the 1960s, Chomsky has maintained that syntactic knowledge is at least partially inborn, implying that children need only learn certain parochial features of their native languages. Chomsky based his argument on observations about human language acquisition, noting that there is an enormous gap between the linguistic stimuli to which children are exposed and the rich linguistic knowledge they attain (see: "poverty of the stimulus" argument). For example, although children are exposed to only a finite subset of the allowable syntactic variants within their first language, they somehow acquire the ability to understand and produce an infinite number of sentences, including ones that have never before been uttered. To explain this, Chomsky reasoned that the primary linguistic data (PLD) must be supplemented by an innate linguistic capacity. Furthermore, while a human baby and a kitten are both capable of inductive reasoning, if they are exposed to exactly the same linguistic data, the human will always acquire the ability to understand and produce language, while the kitten will never acquire either ability. Chomsky labeled whatever relevant capacity the human has that the cat lacks as the language acquisition device (LAD), and he suggested that one of the tasks for linguistics should be to determine what the LAD is and what constraints it imposes on the range of possible human languages. The universal features that would result from these constraints constitute "universal grammar".

Beginning with his "Syntactic Structures" (1957), a distillation of his "Logical Structure of Linguistic Theory" (1955), Chomsky challenges structural linguistics and introduces transformational grammar.

Chomsky's theory posits that language consists of both deep structures and surface structures. Surface structure 'faces out' and is represented by spoken utterances, while deep structure 'faces inward' and expresses the underlying relations between words and conceptual meaning. Transformational grammar is a generative grammar (which dictates that the syntax, or word order, of surface structures adheres to certain principles and parameters) that consists of a limited series of rules, expressed in mathematical notation, which transform deep structures into well-formed surface structures. The transformational grammar thus relates meaning and sound.

The Chomsky hierarchy, sometimes referred to as the Chomsky-Schützenberger hierarchy, is a containment hierarchy of classes of formal grammars. The hierarchy imposes a logical structure across different language classes and provides a basis for understanding the relationship between grammars (devices that enumerate the valid sentences within languages). In order of increasing expressive power it includes regular (or Type-3) grammars, context-free (or Type-2) grammars, context-sensitive (or Type-1) grammars, and recursively enumerable (or Type-0) grammars. Each class is a strict subset of the class above it, i.e., each successive class can generate a broader set of formal languages (infinite sets of strings composed from finite sets of symbols, or alphabets) than the one below. In addition to being important in linguistics, the Chomsky hierarchy is also relevant in theoretical computer science, especially in programming language theory, compiler construction, and automata theory.

Since the 1990s, much of Chomsky's research has focused on what he calls the Minimalist Program (MP), in which he departs from much of his past research and instead attempts to simplify language into a system that relates meaning and sound using the minimum possible faculties that could be expected, given certain external conditions that are imposed on us independently. Chomsky dispenses with concepts such as 'deep structure' and 'surface structure' and instead places emphasis on the plasticity of the brain's neural circuits, along with which comes an infinite number of concepts, or 'Logical Forms'. When exposed to linguistic data, the brain of a hearer-speaker then proceeds to associate sound and meaning, and the rules of grammar that we observe are in fact only the consequences, or side effects, of the way that language works. Thus, while much of Chomsky's prior research has focused on the rules of language, he now focuses on the mechanisms that the brain uses to "create" these rules.

Chomsky has built a "reputation as a political dissident". Chomsky's political views have changed little since his childhood, when he was influenced by the emphasis on political activism that was ingrained in Jewish working-class tradition. He usually identifies as an anarcho-syndicalist or a libertarian socialist. He views these positions not as precise political theories but as ideals that he thinks best meet the needs of humans: liberty, community, and freedom of association. Unlike some other socialists, such as those who accept Marxism, Chomsky believes that politics lies outside the remit of science; however, he still roots his ideas about an ideal society in empirical data and empirically justified theories.

In Chomsky's view, the truth about political realities is systematically distorted or suppressed through elite corporate interests, who use corporate media, advertising, and think tanks to promote their own propaganda. His work seeks to reveal such manipulations and the truth that they obscure. He believes that "common sense" is all that is required to break through the web of falsehood and see the truth, if it (common sense) is employed using both critical thinking skills and an awareness of the role that self-interest and self-deception plays both on oneself and on others. He believes that it is the moral responsibility of intellectuals to tell the truth about the world, but claims that few do so because they fear losing prestige and funding. He argues that, as such an intellectual, it is his duty to use his privilege, resources, and training to aid popular democracy movements in their struggles.

Although he had joined protest marches and organized activist groups, he identifies his primarily political outlet as being that of education, offering free lessons and lectures to encourage wider political consciousness. His political writings have covered a wide range of topics, although there are a number of core themes throughout much of his work. He is a member of the Industrial Workers of the World international union, and sits on the interim consultative committee of the International Organization for a Participatory Society.

Chomsky has been a prominent critic of U.S. imperialism. His published work has focused heavily on criticizing the actions of the United States, such as the U.S.-backed state terror campaign against left-wing dissidents across Latin America known as Operation Condor. Chomsky believes that the basic principle of the foreign policy of the United States is the establishment of "open societies" that are economically and politically controlled by the U.S. and where U.S.-based businesses can prosper. He argues that the U.S. seeks to suppress any movements within these countries that are not compliant with U.S. interests and ensure that U.S.-friendly governments are placed in power. When discussing current events, he emphasizes their place within a wider historical perspective.
He believes that official, sanctioned historical accounts of U.S. and British imperialism have consistently whitewashed these nations' actions in order to present them as having benevolent motives in either spreading democracy or, in older instances, spreading Christianity; criticizing these accounts, he seeks to correct them. Prominent examples that he regularly cites are the actions of the British Empire in India and Africa, and the actions of the U.S. in Vietnam, the Philippines, Latin America, and the Middle East.

Chomsky explains his decision to focus on criticizing the U.S. over other countries as being because, during his lifetime, the country has militarily and economically dominated the world, and because its liberal democratic electoral system allows for the citizenry to exert an influence on government policy. His hope is that, by spreading awareness of the negative impact that imperialism has on the populations affected by it, he can sway the population of the U.S. and other countries into opposing government policies that are imperialist in their nature. He urges people to criticize the motivations, decisions, and actions of their governments; to accept responsibility for one's own thoughts and actions; and to apply the same standards to others as one would apply to oneself.

He has been critical of U.S. involvement in the Israel–Palestine conflict, arguing that it has consistently blocked a peaceful settlement. Chomsky has long endorsed the left binationalist program, seeking to create a democratic state in the Levant that is home to both Jews and Arabs. However, acknowledging the realpolitik of the situation, Chomsky has also considered a two-state solution on the condition that both nation-states exist on equal terms. As a result of his criticisms of Israel, Chomsky was barred from entering Israel in 2010.

In his youth, Chomsky developed a dislike of capitalism and the selfish pursuit of material advancement. At the same time, he developed a disdain for the authoritarian attempts to establish a socialist society, as represented by the Marxist–Leninist policies of the Soviet Union. Rather than accepting the common view among American economists that a spectrum exists between total state ownership of the economy on the one hand and total private ownership on the other, he instead suggests that a spectrum should be understood between total democratic control of the economy on the one hand and total autocratic control (whether state or private) on the other. He argues that Western capitalist nations are not really democratic, because, in his view, a truly democratic society is one in which all persons have a say in public economic policy. He has stated his opposition to ruling elites, among them institutions like the IMF, World Bank, and GATT.

Chomsky highlights that, since the 1970s, the U.S. has become increasingly economically unequal as a result of the repeal of various financial regulations and the rescinding of the Bretton Woods financial control agreements. He characterizes the U.S. as a "de facto" one-party state, viewing both the Republican Party and Democratic Party as manifestations of a single "Business Party" controlled by corporate and financial interests. Chomsky highlights that, within Western capitalist liberal democracies, at least 80% of the population has no control over economic decisions, which are instead in the hands of a management class and ultimately controlled by a small, wealthy elite.

Noting that this economic system is firmly entrenched and difficult to overthrow, he believes that change is possible through the organized co-operation of large numbers of people who understand the problem and know how they want to re-organize the economy in a more equitable way. Although acknowledging that corporate domination of media and government stifle any significant change to this system, he sees reason for optimism, citing the historical examples of the social rejection of slavery as immoral, the advances in women's rights, and the forcing of government to justify invasions to illustrate how change is possible. He views violent revolution to overthrow a government as a last resort to be avoided if possible, citing the example of historical revolutions where the population's welfare has worsened as a result of the upheaval.

Chomsky deems libertarian socialist and anarcho-syndicalist ideas to be the inheritors of the classical liberal ideas of the Age of Enlightenment, arguing that his ideological position revolves around "nourishing the libertarian and creative character of the human being".
He envisions an anarcho-syndicalist future in which there is direct worker control of the means of production, with society governed by workers' councils, who would select representatives to meet together at general assemblies. In this, he believes that there will be no need for political parties. By controlling their productive life, he believes that individuals can gain job satisfaction, a sense of fulfillment, and purpose to their work. He argues that unpleasant and unpopular jobs could be fully automated, carried out by workers who are specially remunerated, or shared among everyone.

Chomsky's political writings have largely been focused on the two concepts of ideology and power, or the media and state policy. One of Chomsky's best-known works, "Manufacturing Consent", dissects the media's role in reinforcing and acquiescing to state policies across the political spectrum while marginalizing contrary perspectives. Chomsky asserts that this version of censorship, from government-guided "free market" forces, is more subtle and difficult to undermine than the equivalent propaganda system that was present in the Soviet Union. As he argues, the mainstream press is corporate owned and thus reflects corporate priorities and interests. Although acknowledging that many American journalists are dedicated and well-meaning, he argues that the choice of topics and issues featured in the mass media, the unquestioned premises on which that coverage rests, and the range of opinions that are expressed are all constrained to reinforce the state's ideology. He states that, although the mass media will criticize individual politicians and political parties, it will not undermine the wider state-corporate nexus of which it is a part. As evidence, he highlights that the U.S. mass media does not employ any socialist journalists or political commentators. He also points to examples of important news stories that have been ignored by U.S. mainstream media because reporting on them would reflect badly upon the U.S. state: For instance, it ignored the murder of Black Panther Fred Hampton with possible FBI involvement, the massacres perpetrated in Nicaragua by the U.S.-funded Contras, and the constant reporting on Israeli deaths while ignoring the far larger number of Palestinian deaths in the conflict between those two nations.
To remedy this situation, Chomsky calls for grassroots democratic control and involvement of the media.

Chomsky considers most conspiracy theories to be fruitless, distracting substitutes to thinking about policy formation in an institutional framework, where individual manipulation is secondary to broader social imperatives. He does not dismiss conspiracy theories outright, but he does consider them unproductive to challenging power in a substantial way.
In response to the labeling of his own thoughts as "conspiracy theory", Chomsky has replied that it is very rational for the media to manipulate information in order to sell it, like any other business. He asks whether General Motors would be accused of conspiracy if they deliberately selected what they would use or discard to sell their product.

Chomsky has also been active in a number of philosophical fields, including the philosophy of mind, the philosophy of language, and the philosophy of science. In these fields he has been highly critical of many other philosophers, in particular those operating within the field of cognitive science.

Chomsky endeavors to keep his family life, linguistic scholarship, and political activism strictly separate from one another, calling himself "scrupulous at keeping my politics out of the classroom". An intensely private person, he is uninterested in appearances and the fame that his work has brought him. McGilvray suggested that Chomsky was never motivated by a desire for fame, but that he was impelled to tell what he perceived as the truth and a desire to aid others in doing so. He also has little interest in modern art and music. He reads four or five newspapers daily; in the U.S., he subscribes to "The Boston Globe", "The New York Times", "The Wall Street Journal", "Financial Times", and "The Christian Science Monitor". He acknowledges that his income and the financial security that it accords him means that he lives a privileged life compared to the majority of the world's population. He characterizes himself as a "worker", albeit one who uses his intellect as his employable skill.

Despite having been raised Jewish, Chomsky is currently non-religious, although he has expressed approval of forms of religion such as liberation theology. He is known for his "dry, laconic wit", and for the use of irony in his writings, and has attracted controversy for labeling established political and academic figures with terms such as "corrupt", "fascist", and "fraudulent". Chomsky's colleague Steven Pinker has said that he "portrays people who disagree with him as stupid or evil, using withering scorn in his rhetoric", and that this contributes to the extreme reactions that he generates from his critics. Chomsky avoids attending academic conferences, including left-oriented ones such as the Socialist Scholars Conference, preferring to speak to activist groups or hold university seminars for mass audiences.

Chomsky was married to Carol Doris Schatz (Chomsky) from 1949 until her death in 2008. They had three children together: Aviva (b. 1957), Diane (b. 1960), and Harry (b. 1967). In 2014, Chomsky married Valeria Wasserman.

Chomsky's legacy is as both a "leader in the field" of linguistics and "a figure of enlightenment and inspiration" for political dissenters. Despite his academic success, his political viewpoints and activism have resulted in him being distrusted by the mainstream media apparatus, and he is regarded as being "on the outer margin of acceptability."

Linguist John Lyons remarked that within a few decades of publication, Chomskyan linguistics had become "the most dynamic and influential" school of thought in the field. By the 1970s, his work had also come to exert a considerable influence on philosophy, while a poll conducted by Minnesota State University found "Syntactic Structures" to be the single most important work in the field of cognitive science. In addition, his work in automata theory and the Chomsky hierarchy has become well known in computer science, and he is much cited within the field of computational linguistics.

Chomsky's work contributed substantially to the decline of behaviorist psychology; in addition, some arguments in evolutionary psychology are derived from his research results. Nim Chimpsky, a chimpanzee who was the subject of a study in animal language acquisition at Columbia University, was named after Chomsky in reference to his view of language acquisition as a uniquely human ability.

The 1984 Nobel Prize laureate in Medicine and Physiology, Niels Kaj Jerne, used Chomsky's generative model to explain the human immune system, equating "components of a generative grammar ... with various features of protein structures". The title of Jerne's Stockholm Nobel Lecture was "The Generative Grammar of the Immune System". His theory of generative grammar has also carried over into music theory and analysis.
An MIT press release found that Chomsky was cited within the Arts and Humanities Citation Index more often than any other living scholar from 1980 to 1992.

Despite their respect for his intellectual contribution, a number of linguists and philosophers have been very critical of Chomsky's approach to language. These critics include Christina Behme, Cedric Boeckx, Margaret Boden, Rudolph Botha, Vyvyan Evans, Nicholas Evans, Daniel Everett, Adele Goldberg, Anna Kinsella, Chris Knight, Stephen Levinson, Bruce Nevin, Geoffrey K. Pullum, Barbara Scholtz, Pieter Seuren and Michael Tomasello.

Chomsky's approach to academic freedom has led him to give support to MIT academics whose actions he deplores. In 1969, when Chomsky heard that Walt Rostow, a major architect of the Vietnam war, wanted to return to work at MIT, Chomsky threatened "to protest publicly" if Rostow was "denied a position at MIT". Then, in 1989, when Pentagon adviser John Deutch wanted to be the President of MIT, Chomsky supported his candidacy. Later, when Deutch became head of the CIA, "The New York Times" quoted Chomsky as saying, "He has more honesty and integrity than anyone I've ever met... If somebody's got to be running the C.I.A., I'm glad it's him."

Chomsky biographer Wolfgang B. Sperlich characterizes the linguist and activist as "one of the most notable contemporary champions of the people", while journalist John Pilger described him as a "genuine people's hero; an inspiration for struggles all over the world for that basic decency known as freedom. To a lot of people in the margins – activists and movements – he's unfailingly supportive." Arundhati Roy called him "one of the greatest, most radical public thinkers of our time", and Edward Said thought him to be "one of the most significant challengers of unjust power and delusions". Fred Halliday stated that by the start of the 21st century, Chomsky had become a "guru" for the world's anti-capitalist and anti-imperialist movements. The propaganda model of media criticism that he and Herman developed has been widely accepted in radical media critiques and adopted to some level in mainstream criticism of the media, also exerting a significant influence on the growth of alternative media, including radio, publishers, and the Internet, which in turn have helped to disseminate his work.

However, Sperlich notes that Chomsky has been vilified by corporate interests, particularly in the mainstream press. University departments devoted to history and political science rarely include Chomsky's work on their syllabuses for undergraduate reading. Critics have argued that despite publishing widely on social and political issues, Chomsky has no expertise in these areas; to this he has responded that such issues are not as complex as many social scientists claim and that almost everyone is able to comprehend them, regardless of whether they have been academically trained to do so or not.

His far-reaching criticisms of U.S. foreign policy and the legitimacy of U.S. power have raised controversy. A document obtained pursuant to a Freedom of Information Act (FOIA) request from the U.S. government revealed that the Central Intelligence Agency (CIA) monitored Chomsky's activities and for years denied doing so. The CIA also destroyed its files on Chomsky at some point in time, possibly in violation of federal law. He has often received undercover police protection at MIT and when speaking on the Middle East, although he has refused uniformed police protection. German newspaper "Der Spiegel" described him as "the Ayatollah of anti-American hatred", while conservative commentator David Horowitz termed him "the most devious, the most dishonest and ... the most treacherous intellect in America", one whose work was infused with an "anti-American dementia" and which evidences Chomsky's "pathological hatred of his own country". Writing in "Commentary" magazine, the journalist Jonathan Kay described Chomsky as "a hard-boiled anti-American monomaniac who simply refuses to believe anything that any American leader says".

His criticism of Israel has led to him being accused of being a traitor to the Jewish people and an anti-Semite. Criticizing Chomsky's defense of the right of individuals to engage in Holocaust denial on the grounds that freedom of speech must be extended to all viewpoints, Werner Cohn accused Chomsky of being "the most important patron" of the Neo-Nazi movement, while the Anti-Defamation League (ADL) accused him of being a Holocaust denier himself. The ADL have been accused of monitoring Chomsky's activities, and have characterized him as a "dupe of intellectual pride so overweening that he is incapable of making distinctions between totalitarian and democratic societies, between oppressors and victims". In turn, Chomsky has claimed that the ADL is dominated by "Stalinist types" who oppose democracy in Israel. Alan Dershowitz considered Chomsky to be a "false prophet of the left", while Chomsky has accused Dershowitz of being on "a crazed jihad, dedicating much of his life to trying to destroy my reputation".

According to McGilvray, many of Chomsky's critics "do not bother quoting his work or quote out of context, distort, and create straw men that cannot be supported by Chomsky's text".

In Spring 2017, Chomsky taught a short-term politics course at the University of Arizona.

In 1970, Chomsky was named one of the "makers of the twentieth century" by the London "Times". In early 1969, he delivered the John Locke Lectures at Oxford University; in January 1971, the Bertrand Russell Memorial Lecture at the University of Cambridge; in 1972, the Nehru Memorial Lecture in New Delhi; in 1975, the Whidden Lectures at McMaster University; in 1977, the Huizinga Lecture in Leiden; in 1978, the Woodbridge Lectures at Columbia University; in 1979, the Kant Lectures at Stanford University; in 1988, the Massey Lectures at the University of Toronto; in 1997, The Davie Memorial Lecture on Academic Freedom in Cape Town; in 2011, the Rickman Godlee Lecture at University College, London; and many others.

Chomsky has received honorary degrees from many colleges and universities around the world, including from the following:

In the United States, he is a member of the American Academy of Arts and Sciences, the National Academy of Sciences, the Linguistic Society of America, the American Philosophical Association, and the American Association for the Advancement of Science. Abroad, he is a member of the Utrecht Society of Arts and Sciences, the Deutsche Akademie der Naturforscher Leopoldina, a corresponding fellow of the British Academy, an honorary member of the British Psychological Society, and a foreign member of the Department of Social Sciences of the Serbian Academy of Sciences and Arts. In addition, he is a recipient of a 1971 Guggenheim Fellowship, the 1984 American Psychological Association Award for Distinguished Contributions to Psychology, 1988 the Kyoto Prize in Basic Sciences, the 1996 Helmholtz Medal, the 1999 Benjamin Franklin Medal in Computer and Cognitive Science, and the Dorothy Eldridge Peacemaker Award. He is also a two-time winner of the Gustavus Myers Center Award, receiving the honor in both 1986 and 1988, and the NCTE George Orwell Award for Distinguished Contribution to Honesty and Clarity in Public Language, receiving the honor in both 1987 and 1989. He has also received the Rabindranath Tagore Centenary Award from The Asiatic Society.

In 2004 Chomsky received the Carl-von-Ossietzky Prize from the city of Oldenburg, Germany, to acknowledge his body of work as a political analyst and media critic. In 2005, Chomsky received an honorary fellowship from the Literary and Historical Society. In February 2008, he received the President's Medal from the Literary and Debating Society of the National University of Ireland, Galway. Since 2009, he has been an honorary member of International Association of Professional Translators and Interpreters (IAPTI).

In 2010, Chomsky received the Erich Fromm Prize in Stuttgart, Germany. In April 2010, Chomsky became the third scholar to receive the University of Wisconsin's A.E. Havens Center's Award for Lifetime Contribution to Critical Scholarship.
Chomsky has an Erdős number of four.

Chomsky was voted the world's leading public intellectual in The 2005 Global Intellectuals Poll jointly conducted by American magazine "Foreign Policy" and British magazine "Prospect". In a list compiled by the magazine "New Statesman" in 2006, he was voted seventh in the list of "Heroes of our time."

Actor Viggo Mortensen and avant-garde guitarist Buckethead dedicated their 2003 album "Pandemoniumfromamerica" to Chomsky. On January 22, 2010, a special honorary concert for Chomsky was given at Kresge Auditorium at MIT. The concert, attended by Chomsky and dozens of his family and friends, featured music composed by Edward Manukyan and speeches by Chomsky's colleagues, including David Pesetsky of MIT and Gennaro Chierchia, head of the linguistics department at Harvard University.

In May 2007, Jamia Millia Islamia, a prestigious Indian university, named one of its complexes after Noam Chomsky.

In June 2011, Chomsky was awarded the Sydney Peace Prize, which cited his "unfailing courage, critical analysis of power and promotion of human rights." Also in 2011, Chomsky was inducted into IEEE Intelligent Systems' AI's Hall of Fame for "significant contributions to the field of AI and intelligent systems."

In 2013, a newly described species of bee was named after him: "Megachile chomskyi".

In 2014, he was awarded the Neil and Saras Smith Medal for Linguistics by the British Academy: this medal is awarded "for lifetime achievement in the scholarly study of linguistics".

In 2016, he was awarded the Int'l Courage of Conscience Award by the Peace Abbey: this award was bestowed at MIT "for his unrelenting critique of U.S. foreign policy, capitalism and the globalization of systems and structures of profit and greed".

In 2017 he was one of three recipients awarded the Seán MacBride Peace Prize "for his tireless commitment to peace, his strong critiques to U.S. foreign policy, and his anti-imperialism".





</doc>
<doc id="21571" url="https://en.wikipedia.org/wiki?curid=21571" title="Nial">
Nial

Nial (from "Nested Interactive Array Language") is a high-level array programming language developed from about 1981 by Mike Jenkins of Queen's University, Kingston, Ontario, Canada. Jenkins co-created the Jenkins–Traub algorithm.

Nial combines a functional programming notation for arrays based on an array theory developed by Trenchard More with structured programming concepts for numeric, character and symbolic data.

It is most often used for prototyping and artificial intelligence.

In 1982, Jenkins formed a company (Nial Systems Ltd) to market the language and the Q'Nial implementation of Nial. As of 2014, the company website supports an Open Source project for the Q'Nial software with the binary and source available for download. Its license is derived from Artistic License 1.0, the only differences being the preamble, the definition of "Copyright Holder" (which is changed from "whoever is named in the copyright or copyrights for the package" to "NIAL Systems Limited"), and an instance of "whoever" (which is changed to "whomever").

Nial uses a generalized and expressive Array Theory in its Version 4, but sacrificed some of the generality of functional model, and modified the Array Theory in the Version 6. Only Version 6 is available now.

Nial defines all its datatypes as nested rectangular arrays. ints, booleans, chars etc. are considered as a solitary array or an array containing a single member. Arrays themselves can contain other arrays to form arbitrarily deep structures. Nial also provides Records. They are defined as non-homogenous array structure.

Functions in Nial are called Operations. From Nial manual: "An operation is a functional object that is given an argument array and returns a result array. The process of executing an operation by giving it an argument value is called an operation call or an operation application."

Nial like other APL-derived languages allows the unification of binary operators and operations. Thus the below notations have the same meaning.
Note: codice_1 is same as codice_2

 2 + 3 

 + [2,3]

 + 2 3

 + (2 3)

Nial also uses transformers which are higher order functions. They use the argument operation to construct a new modified operation.

An atlas in Nial is an operation made up of an array of component operations. When an atlas is applied to a value, each element of the atlas is applied in turn to the value to provide an end result. This is used to provide point free (without-variables) style of definitions. It is also used by the transformers. In the below examples 'inner [+,*]' the list '[+,*]' is an atlas.

codice_3

Arrays can also be literal
codice_4

Shape gives the array dimensions and reshape can be used to reshape the dimensions.
codice_5

Definitions are of the form '<name> is <expression>'

 fact is recur [ 0 =, 1 first, pass, product, -1 +]

 rev is reshape [ shape, across [pass, pass, converse append ] ]

Contrast with [[APL programming language|APL]]

 Checking the divisibility of A by B

Defining is_prime filter
Count generates an array [1..N] and pass is N (identity operation).
eachright applies is_divisible(pass,element) in each element of count-generated array. 
Thus this transforms the count-generated array into an array where numbers that can divide N are replaced by '1' and others by '0'. Hence if the number N is prime, sum [transformed array] must be 2 (itself and 1).

Now all that remains is to generate another array using count N, and filter all that are not prime.

link joins together its argument arrays
<br>sublist [A,B] returns a list of items of B chosen according to the list of booleans given in A, selecting those items of B where the corresponding item of A is true.
In a Fork [A,B,C] X the first A is a predicate, and if A(X) is true, then B(X) is returned else C(X) is returned.
Pass is an identity operation for arrays.

Using it.

[[Category:Array programming languages]]

</doc>
<doc id="21572" url="https://en.wikipedia.org/wiki?curid=21572" title="Nag Hammadi">
Nag Hammadi

Nag Hammadi ( ; "") is a city in Upper Egypt.
It is located on the west bank of the Nile in the Qena Governorate, about 80 kilometres north-west of Luxor. It had a population of close to 43,000 as of 2007. 

The town of Nag Hammadi is named for its founder, Mahmoud Pasha Hammadi, a member of the Hammadi family in Sohag, Egypt. Mahmoud Pasha Hammadi was a major landholder in Sohag, and known for his strong opposition to the British occupation of 1882.

Nag Hammadi is about 5 km west of ancient Chenoboskion ()
The "Nag Hammadi library", an important collection of 2nd-century Gnostic texts, was found at 
Jabal al-Ṭārif near Nag Hammadi was the site in 1945.

The city was the site of the Nag Hammadi massacre in January 2010, wherein eight Coptic Christians were shot dead by three men. In total, nineteen Coptic Christians were attacked.

Sugar and aluminium are produced in Nag Hammadi. Egyptalum is the largest aluminium producer in the Middle East. Wood particleboard is manufactured from sugar cane bagasse. 


</doc>
<doc id="21573" url="https://en.wikipedia.org/wiki?curid=21573" title="Niels Henrik Abel">
Niels Henrik Abel

Niels Henrik Abel (; ; 5 August 1802 – 6 April 1829) was a Norwegian mathematician who made pioneering contributions in a variety of fields. His most famous single result is the first complete proof demonstrating the impossibility of solving the general quintic equation in radicals. This question was one of the outstanding open problems of his day, and had been unresolved for over 350 years. He was also an innovator in the field of elliptic functions, discoverer of Abelian functions. Through the great works from Abel's hand he was known to the world's mathematicians; he made his discoveries while living in poverty and died at the age of 26 from tuberculosis.

Most of his work was done in six or seven years of his working life. Regarding Abel, the French mathematician Charles Hermite said: "Abel has left mathematicians enough to keep them busy for five hundred years." Another French mathematician, Adrien-Marie Legendre, said: ""quelle tête celle du jeune Norvégien!"" ("what a head the young Norwegian has!").

The Abel Prize in mathematics, originally proposed in 1899 to complement the Nobel Prizes, is named in his honour.

Niels Henrik Abel was born in Nedstrand, Norway, as the second child of the pastor Søren Georg Abel and Anne Marie Simonsen. When Niels Henrik Abel was born, the family was living at a rectory on Finnøy. Much suggests that Niels Henrik was born in the neighboring parish, as his parents were guests of the bailiff in Nedstrand in July / August of his year of birth.

Niels Henrik Abel's father, Søren Georg Abel, had a degree in theology and philosophy and served as pastor at Finnøy. Søren's father, Niels's grandfather, Hans Mathias Abel, was also a pastor, at Gjerstad Church near the town of Risør. Søren had spent his childhood at Gjerstad, and had also served as chaplain there; and after his father's death in 1804, Søren was appointed pastor at Gjerstad and the family moved there. The Abel family originated in Schleswig and came to Norway in the 17th century.
Anne Marie Simonsen was from Risør; her father, Niels Henrik Saxild Simonsen, was a tradesman and merchant ship-owner, and said to be the richest person in Risør. Anne Marie had grown up with two stepmothers, in relatively luxurious surroundings. At Gjerstad rectory, she enjoyed arranging balls and social gatherings. Much suggests she was early on an alcoholic and took little interest in the upbringing of the children.
Niels Henrik and his brothers were given their schooling by their father, with handwritten books to read. An addition table in a book of mathematics reads: 1+0=0.

With Norwegian independence and the first election held in Norway, in 1814, Søren Abel was elected as a representative to the Storting. Meetings of the Storting were held until 1866 in the main hall of the Cathedral School in Christiania (now known as Oslo). Almost certainly, this is how he came into contact with the school, and he decided that his eldest son, Hans Mathias, should start there the following year. However, when the time for his departure approached, Hans was so saddened and depressed over having to leave home that his father did not dare send him away. He decided to send Niels instead.

In 1815, Niels Abel entered the Cathedral School at the age of 13. His elder brother Hans joined him there a year later. They shared rooms and had classes together. Hans got better grades than Niels; however, a new mathematics teacher, Bernt Michael Holmboe, was appointed in 1818. He gave the students mathematical tasks to do at home. He saw Niels Henrik's talent in mathematics, and encouraged him to study the subject to an advanced level. He even gave Niels private lessons after school.

In 1818, Søren Abel had a public theological argument with the theologian Stener Johannes Stenersen regarding his catechism from 1806. The argument was well covered in press. Søren was given the nickname "Abel Treating" "(Norwegian: "Abel Spandabel")". Niels' reaction to the quarrel was said to have been "excessive gaiety". At the same time, Søren also almost faced impeachment after insulting Carsten Anker, the host of the Norwegian Constituent Assembly; and in September 1818 he returned to Gjerstad with his political career in ruins. He began drinking heavily and died only two years later, in 1820, aged 48.

Bernt Michael Holmboe supported Niels Henrik Abel with a scholarship to remain at the school and raised money from his friends to enable him to study at the Royal Frederick University.

When Abel entered the university in 1821, he was already the most knowledgeable mathematician in Norway. Holmboe had nothing more he could teach him and Abel had studied all the latest mathematical literature in the university library. During that time, Abel started working on the quintic equation in radicals. Mathematicians had been looking for a solution to this problem for over 250 years. In 1821, Abel thought he had found the solution. The two professors of mathematics in Christiania, Søren Rasmussen and Christopher Hansteen, found no errors in Abel's formulas, and sent the work on to the leading mathematician in the Nordic countries, Carl Ferdinand Degen in Copenhagen. He too found no faults but still doubted that the solution, which so many outstanding mathematicians had sought for so long, could really have been found by an unknown student in far-off Christiania. Degen noted, however, Abel's unusually sharp mind, and believed that such a talented young man should not waste his abilities on such a "sterile object" as the fifth degree equation, but rather on elliptic functions and transcendence; for then, wrote Degen, he would "discover Magellanian thoroughfares to large portions of a vast analytical ocean". Degen asked Abel to give a numerical example of his method. While trying to provide an example, Abel found a mistake in his paper. This led to a discovery in 1823 that a solution to a fifth- or higher-degree equation was impossible.

Abel graduated in 1822. His performance was exceptionally high in mathematics and average in other matters.

After he graduated, professors from university supported Abel financially, and Professor Christopher Hansteen let him live in a room in the attic of his home. Abel would later view Ms. Hansteen as his second mother. While living here, Abel helped his younger brother, Peder Abel, through examen artium. He also helped his sister Elisabeth to find work in the town.

In early 1823, Niels Abel published his first article in "Magazin for Naturvidenskaberne", Norway's first scientific journal, which had been co-founded by Professor Hansteen. Abel published several articles, but the journal soon realized that this was not material for the common reader. In 1823, Abel also wrote a paper in French. It was "a general representation of the possibility to integrate all differential formulas" ("Norwegian: en alminnelig Fremstilling af Muligheten at integrere alle mulige Differential-Formler)". He applied for funds at the university to publish it. However the work was lost, while being reviewed, never to be found thereafter.
In mid-1823, Professor Rasmussen gave Abel a gift of 100 speciedaler so he could travel to Copenhagen and visit Ferdinand Degen and other mathematicians there. While in Copenhagen, Abel did some work on Fermat's Last Theorem. Abel's uncle, Peder Mandrup Tuxen, lived at the naval base in Christianshavn, Copenhagen, and at a ball there Niels Abel met Christine Kemp, his future fiancée. In 1824, Christine moved to Son, Norway to work as a governess and the couple got engaged over Christmas.

After returning from Copenhagen, Abel applied for a government scholarship in order to visit top mathematicians in Germany and France, but he was instead granted 200 speciedaler yearly for two years, to stay in Christiania and study German and French. In the next two years, he was promised a scholarship of 600 speciedaler yearly and he would then be permitted to travel abroad. While studying these languages, Abel published his first notable work in 1824, "Mémoire sur les équations algébriques où on démontre l'impossibilité de la résolution de l'équation générale du cinquième degré" (Memoir on algebraic equations, in which the impossibility of solving the general equation of the fifth degree is proven). For, in 1823, Abel had at last proved the impossibility of solving the quintic equation in radicals (now referred to as the Abel–Ruffini theorem). However, this paper was in an abstruse and difficult form, in part because he had restricted himself to only six pages, in order to save money on printing. A more detailed proof was published in 1826 in the first volume of "Crelle's Journal".

In 1825, Abel wrote a personal letter to King Carl Johan of Norway/Sweden requesting permission to travel abroad. He was granted this permission, and in September 1825 he left Christiania together with four friends from university (Christian P.B Boeck, Balthazar M. Keilhau, Nicolay B. Møller and Otto Tank). These four friends of Abel were traveling to Berlin and to the Alps to study geology. Abel wanted to follow them to Copenhagen and from there make his way to Göttingen. The terms for his scholarship stipulated that he was to visit Gauss in Göttingen and then continue to Paris. However, when he got as far as Copenhagen he changed his plans. He wanted to follow his friends to Berlin instead, intending to visit Göttingen and Paris afterwards.

On the way, he visited the astronomer Heinrich Christian Schumacher in Altona, now a district of Hamburg. He then spent four months in Berlin, where he became well acquainted with August Leopold Crelle, who was then about to publish his mathematical journal, Journal für die reine und angewandte Mathematik. This project was warmly encouraged by Abel, who contributed much to the success of the venture. Abel contributed seven articles to it in its first year.

From Berlin Abel also followed his friends to the Alps. He went to Leipzig and Freiberg to visit Georg Amadeus Carl Friedrich Naumann and his brother the mathematician August Naumann. In Freiberg Abel did research in the theory of functions, particularly, elliptic, hyperelliptic, and a new class now known as abelian functions.

From Freiberg they went on to Dresden, Prague, Vienna, Trieste, Venice, Verona, Bolzano, Innsbruck, Luzern and Basel. From July 1826 Abel traveled on his own from Basel to Paris. Abel had sent most of his work to Berlin to be published in Crelle's Journal, but he had saved what he regarded as his most important work for the French Academy of Sciences, a theorem on addition of algebraic differentials. With the help of a painter, Johan Gørbitz, he found an apartment in Paris and continued his work on the theorem. He finished in October 1826, and submitted it to the academy. It was to be reviewed by Augustin-Louis Cauchy. Abel's work was scarcely known in Paris, and his modesty restrained him from proclaiming his research. The theorem was put aside and forgotten until his death.

Abel's limited finances finally compelled him to abandon his tour in January 1827. He returned to Berlin, and was offered a position as editor of Crelle's Journal, but opted out. By May 1827 he was back in Norway. His tour abroad was viewed as a failure. He had not visited Gauss in Göttingen and he had not published anything in Paris. His scholarship was therefore not renewed and he had to take up a private loan in Norges Bank of 200 spesidaler. He never repaid this loan. He also started tutoring. He continued to send most of his work to Crelle's Journal. But in mid-1828 he published, in rivalry with Carl Jacobi, an important work on elliptic functions in Astronomische Nachrichten in Altona.

While in Paris, Abel contracted tuberculosis. At Christmas 1828, he traveled by sled to Froland to visit his fiancée. He became seriously ill on the journey; and, although a temporary improvement allowed the couple to enjoy the holiday together, he died relatively soon after on 6 April 1829, just two days before a letter arrived from August Crelle. Crelle had been searching for a new job for Abel in Berlin and had actually managed to have him appointed as a Professor at the University of Berlin. Crelle wrote to Abel to tell him, but the good news came too late.

Abel showed that there is no general algebraic solution for the roots of a quintic equation, or any general polynomial equation of degree greater than four, in terms of explicit algebraic operations. To do this, he invented (independently of Galois) a branch of mathematics known as group theory, which is invaluable not only in many areas of mathematics, but for much of physics as well. Abel sent a paper on the unsolvability of the quintic equation to Carl Friedrich Gauss, who proceeded to discard without a glance what he believed to be the worthless work of a crank.

As a 16-year-old, Abel gave a rigorous proof of the binomial theorem valid for all numbers, extending Euler's result which had held only for rationals. Abel wrote a fundamental work on the theory of elliptic integrals, containing the foundations of the theory of elliptic functions.
While travelling to Paris he published a paper revealing the double periodicity of elliptic functions, which Adrien-Marie Legendre later described to Augustin-Louis Cauchy as "a monument more lasting than bronze" (borrowing a famous sentence by the Roman poet Horatius). The paper was, however, misplaced by Cauchy.

While abroad Abel had sent most of his work to Berlin to be published in the Crelle's Journal, but he had saved what he regarded as his most important work for the French Academy of Sciences, a theorem on addition of algebraic differentials. The theorem was put aside and forgotten until his death.
While in Freiberg Abel did research in the theory of functions, particularly, elliptic, hyperelliptic, and a new class now known as abelian functions.

In 1823 Abel wrote a paper titled "a general representation of the possibility to integrate all differential formulas" ("Norwegian: en alminnelig Fremstilling af Muligheten at integrere alle mulige Differential-Formler)". He applied for funds at the university to publish it. However the work was lost, while being reviewed, never to be found thereafter.

Abel said famously of Carl Friedrich Gauss's writing style, "He is like the fox, who effaces his tracks in the sand with his tail."

Under Abel's guidance, the prevailing obscurities of analysis began to be cleared, new fields were entered upon and the study of functions so advanced as to provide mathematicians with numerous ramifications along which progress could be made. His works, the greater part of which originally appeared in "Crelle's Journal", were edited by Bernt Michael Holmboe and published in 1839 by the Norwegian government, and a more complete edition by Ludwig Sylow and Sophus Lie was published in 1881. The adjective "abelian", derived from his name, has become so commonplace in mathematical writing that it is conventionally spelled with a lower-case initial "a" (e.g., abelian group, abelian category, and abelian variety).

On 6 April 1929, four Norwegian stamps were issued for the centenary of Abel's death. His portrait appears on the 500-kroner banknote (version V) issued during 1978–1985. On 5 June 2002, four Norwegian stamps were issued in honour of Abel two months before the bicentenary of his birth. There is also a 20-kroner coin issued by Norway in his honour. A statue of Abel stands in Oslo, and crater Abel on the Moon was named after him. In 2002, the Abel Prize was established in his memory.

Mathematician Felix Klein wrote about Abel:





</doc>
<doc id="21574" url="https://en.wikipedia.org/wiki?curid=21574" title="November 19">
November 19






</doc>
<doc id="21575" url="https://en.wikipedia.org/wiki?curid=21575" title="November 20">
November 20





</doc>
<doc id="21576" url="https://en.wikipedia.org/wiki?curid=21576" title="November 21">
November 21





</doc>
<doc id="21577" url="https://en.wikipedia.org/wiki?curid=21577" title="November 30">
November 30





</doc>
<doc id="21578" url="https://en.wikipedia.org/wiki?curid=21578" title="November 29">
November 29





</doc>
<doc id="21579" url="https://en.wikipedia.org/wiki?curid=21579" title="November 28">
November 28





</doc>
<doc id="21580" url="https://en.wikipedia.org/wiki?curid=21580" title="November 25">
November 25





</doc>
<doc id="21581" url="https://en.wikipedia.org/wiki?curid=21581" title="November 26">
November 26





</doc>
<doc id="21582" url="https://en.wikipedia.org/wiki?curid=21582" title="Nicolaus von Amsdorf">
Nicolaus von Amsdorf

Nicolaus von Amsdorf (German: Nikolaus von Amsdorf, 3 December 1483 – 14 May 1565) was a German Lutheran theologian and an early Protestant reformer. As bishop of Naumburg (1542–1546), he became the first Lutheran bishop in the Holy Roman Empire.

He was born in Torgau, on the Elbe.

He was educated at Leipzig, and then at Wittenberg, where he was one of the first who matriculated (1502) in the recently founded university. He soon obtained various academic honours, and became professor of theology in 1511.

Like Andreas Karlstadt, he was at first a leading exponent of the older type of scholastic theology, but under the influence of Luther abandoned his Aristotelian positions for a theology based on the Augustinian doctrine of grace. Throughout his life he remained one of Luther's most determined supporters; he was with him at the Leipzig conference (1519), and the Diet of Worms (1521); and was privy to the secret of his Wartburg seclusion. He assisted the first efforts of the Reformation at Magdeburg (1524), at Goslar (1531) and at Einbeck (1534); took an active part in the debates at Schmalkalden (1537), where he defended the use of the sacrament by the unbelieving; and (1539) spoke out strongly against the bigamy of the Landgrave of Hesse.

After the death of Philip of the Palatinate, bishop of Naumburg-Zeitz, he was installed there on 20 January 1542, though in opposition to the chapter, by the Prince-elector of Saxony and Luther. His position was a painful one, and he longed to get back to Magdeburg, but was persuaded by Luther to stay. After Luther's death (1546) and the Battle of Mühlberg (1547) he had to yield to his rival, Julius von Pflug, and retire to the protection of the young duke of Weimar. Here he took part in founding Jena University (1558); opposed the "Augsburg Interim" (1548); superintended the publication of the Jena edition of Luther's works; and debated on the freedom of the will, original sin, and, more noticeably, on the Christian value of good works, in regard to which he held that they were not only useless, but prejudicial in the matter of man's salvation. He urged the separation of the High Lutheran party from Melanchthon (1557), got the Saxon dukes to oppose the Frankfurt Recess (1558) and continued to fight for the purity of Lutheran doctrine.

He died at Eisenach in 1565, and was buried in the church of St. Georg there, where his effigy shows a well-knit frame and sharp-cut features.

He was a man of strong will, of great aptitude for controversy, and considerable learning, and thus exercised a decided influence on the Reformation. Many letters and other short productions of his pen are extant in manuscript, especially five thick volumes of Amsdorfiana, in the Weimar library. They are a valuable source for our knowledge of Luther. A small sect, which adopted his opinion on good works, was called after him; but it is now of mere historical interest.




</doc>
<doc id="21583" url="https://en.wikipedia.org/wiki?curid=21583" title="Nationality">
Nationality

Nationality is a legal relationship between an individual person and a state. Nationality affords the state jurisdiction over the person and affords the person the protection of the state. What these rights and duties are varies from state to state.

By custom and international conventions, it is the right of each state to determine who its nationals are. Such determinations are part of nationality law. In some cases, determinations of nationality are also governed by public international law—for example, by treaties on statelessness and the European Convention on Nationality.

Nationality differs technically and legally from citizenship, which is a different legal relationship between a person and a country. The noun "national" can include both citizens and non-citizens. The most common distinguishing feature of citizenship is that citizens have the right to participate in the political life of the state, such as by voting or standing for election. However, in most modern countries all nationals are citizens of the state, and full citizens are always nationals of the state.

In older texts, the word "nationality" rather than "ethnicity", often used to refer to an ethnic group (a group of people who share a common ethnic identity, language, culture, descent, history, and so forth). This older meaning of "nationality" is not defined by political borders or passport ownership and includes nations that lack an independent state (such as the Arameans, Scots, Welsh, English, Basques, Catalans, Kurds, Kabyles, Baloch, Berbers, Bosniaks, Kashmiris,
Palestinians, Sindhi, Tamils, Hmong, Inuit, Copts, Māori, Sikhs, Wakhi and Székelys).

Individuals may also be considered nationals of groups with autonomous status that have ceded some power to a larger government.

In international law, nationality is the status or relationship that gives a nation the right to protect a person from other nations. Diplomatic and consular protection are dependent upon this relationship between the person and the state. A person's status as being the national of a country is used to resolve the conflict of laws.

Nationality is also the status that allows a nation to grant rights to the subject and to impose obligations upon the subject. In most cases, no rights or obligations are automatically attached to this status, although the status is a necessary precondition for any rights and obligations created by the state.

Within the broad limits imposed by few treaties and international law, states may freely define who their nationals are and are not. However, since the "Nottebohm" case, other states are only required to respect their claim to protect an alleged national if the nationality is based on a true social bond. In the case of dual nationality, states may determine the most effective nationality for a person, to determine which state's laws are most relevant. There are also limits on removing a person's status as a national. Article 15 of the Universal Declaration of Human Rights states that "Everyone has the right to a nationality," and "No one shall be arbitrarily deprived of his nationality nor denied the right to change his nationality."

Nationals normally have the right to enter or return to the country they belong to. Passports are issued to nationals of a state, rather than only to citizens, because the passport is the travel document used to enter the country. However, nationals may not have the right of abode (the right to live permanently) in the countries that grant them passports.

Conceptually, citizenship is focused on the internal political life of the state and nationality is a matter of international dealings.

In the modern era, the concept of full citizenship encompasses not only active political rights, but full civil rights and social rights. Nationality is a necessary but not sufficient condition to exercise full political rights within a state or other polity. Nationality is required for full citizenship, and some people have no nationality in international law. A person who is denied full citizenship or nationality is commonly called a stateless person.

Historically, the most significant difference between a national and a citizen is that the citizen has the right to vote for elected officials, and to be elected. This distinction between full citizenship and other, lesser relationships goes back to antiquity. Until the 19th and 20th centuries, it was typical for only a small percentage of people who belonged to a city or state to be full citizens. In the past, most people were excluded from citizenship on the basis of gender, socioeconomic class, ethnicity, religion, and other factors. However, they held a legal relationship with their government akin to the modern concept of nationality.

United States nationality law defines some persons born in U.S. outlying possessions as U.S. nationals but not citizens. British nationality law defines six classes of British national, among which "British citizen" is one class (having the right of abode in the United Kingdom, along with some "British subjects"). Similarly, in the Republic of China, commonly known as Taiwan, the status of national without household registration applies to people who have Republic of China nationality, but do not have an automatic entitlement to enter or reside in the Taiwan Area, and do not qualify for civic rights and duties there. Under the nationality laws of Mexico, Colombia, and some other Latin American countries, nationals do not become citizens until they turn 18.

Nationality is sometimes used simply as an alternative word for ethnicity or national origin, just as some people assume that citizenship and nationality are identical. In some countries, the cognate word for "nationality" in local language may be understood as a synonym of ethnicity or as an identifier of cultural and family-based self-determination, rather than on relations with a state or current government. For example, some Kurds say that they have Kurdish nationality, even though there is no Kurdish sovereign state at this time in history.
In the context of former Soviet Union and former Socialist Federal Republic of Yugoslavia, "nationality" is often used as translation of the Russian "nacional'nost' " and Serbo-Croatian "narodnost", which were the terms used in those countries for ethnic groups and local affiliations within the member states of the federation. In the Soviet Union, more than 100 such groups were formally recognized. Membership in these groups was identified on Soviet internal passports, and recorded in censuses in both the USSR and Yugoslavia. In the early years of the Soviet Union's existence, ethnicity was usually determined by the person's native language, and sometimes through religion or cultural factors, such as clothing. Children born after the revolution were categorized according to their parents' recorded ethnicities. Many of these ethnic groups are still recognized by modern Russia and other countries.

Similarly, the term "nationalities of China" refers to ethnic and cultural groups in China. Spain is one nation, made up of nationalities, which are not politically recognized as nations (state), but can be considered smaller nations within the Spanish nation. Spanish law recognises the autonomous communities of Andalusia, Aragon, Balearic Islands, Canary Islands, Catalonia, Valencia, Galicia and the Basque Country as "nationalities" ("nacionalidades").

National identity is a person's subjective sense of belonging to one state or to one nation. A person may be a national of a state, in the sense of being its citizen, without subjectively or emotionally feeling a part of that state, for example many migrants in Europe often identify with their ancestral and/or religious background rather than with the state of which they are citizens. Conversely, a person may feel that he belongs to one state without having any legal relationship to it. For example, children who were brought to the U.S. illegally when quite young and grow up there with little contact with their native country and its culture often have a national identity of feeling American, despite legally being nationals of a different country.

Dual nationality is when a single person has a formal relationship with two separate, sovereign states. This might occur, for example, if a person's parents are nationals of separate countries, and the mother's country claims all offspring of the mother's as their own nationals, but the father's country claims all offspring of the father's.

Nationality, with its historical origins in allegiance to a sovereign monarch, was seen originally as a permanent, inherent, unchangeable condition, and later, when a change of allegiance was permitted, as a strictly exclusive relationship, so that becoming a national of one state required rejecting the previous state.

Dual nationality was considered a problem that caused conflict between states and sometimes imposed mutually exclusive requirements on affected people, such as simultaneously serving in two countries' military forces. Through the middle of the 20th century, many international agreements were focused on reducing the possibility of dual nationality. Since then, many accords recognizing and regulating dual nationality have been formed.

Statelessness is the condition in which an individual has no formal or protective relationship with any state. This might occur, for example, if a person's parents are nationals of separate countries, and the mother's country rejects all offspring of mothers married to foreign fathers, but the father's country rejects all offspring born to foreign mothers. Although this person may have an emotional national identity, he or she may not legally be the national of any state.

Another stateless situation arises when a person holds a travel document (passport) which recognizes the bearer as having the nationality of a "state" which is not internationally recognized, has no entry in the International Organization for Standardization's country list, is not a member of the United Nations, etc. In the current era, persons native to Taiwan who hold Republic of China passports are one example. 
This list includes parents' ability to confer nationality on their children or to spouses. 




</doc>
<doc id="21585" url="https://en.wikipedia.org/wiki?curid=21585" title="Nereus">
Nereus

In Greek mythology, Nereus (; ) was the eldest son of Pontus (the Sea) and Gaia (the Earth), who with Doris fathered the Nereids and Nerites, with whom Nereus lived in the Aegean Sea.

R. S. P. Beekes suggests a Pre-Greek origin.

In the "Iliad" the Old Man of the Sea is the father of Nereids, though Nereus is not directly named. He was never more manifestly the Old Man of the Sea than when he was described, like Proteus, as a shapeshifter with the power of prophecy, who would aid heroes such as Heracles who managed to catch him even as he changed shapes. Nereus and Proteus (the "first") seem to be two manifestations of the god of the sea who was supplanted by Poseidon when Zeus overthrew Cronus.

The earliest poet to link Nereus with the labours of Heracles was Pherekydes, according to a "scholion" on Apollonius of Rhodes.

During the course of the 5th century BC, Nereus was gradually replaced by Triton, who does not appear in Homer, in the imagery of the struggle between Heracles and the sea-god who had to be restrained in order to deliver his information that was employed by the vase-painters, independent of any literary testimony.

In a late appearance, according to a fragmentary papyrus, Alexander the Great paused at the Syrian seashore before the climacteric battle of Issus (333 BC), and resorted to prayers, "calling on Thetis, Nereus and the Nereids, nymphs of the sea, and invoking Poseidon the sea-god, for whom he ordered a four-horse chariot to be cast into the waves."

Nereus was known for his truthfulness and virtue:

The Attic vase-painters showed the draped torso of Nereus issuing from a long coiling scaly fishlike tail. Bearded Nereus generally wields a staff of authority. He was also shown in scenes depicting the flight of the Nereides as Peleus wrestled their sister Thetis.

In Aelian's natural history, written in the early third century CE, Nereus was also the father of a watery consort of Aphrodite named Nerites who was transformed into "a shellfish with a spiral shell, small in size but of surpassing beauty."

Nereus was father to Thetis, one of the Nereids, who in turn was mother to the great Greek hero Achilles, and Amphitrite, who married Poseidon.





</doc>
<doc id="21586" url="https://en.wikipedia.org/wiki?curid=21586" title="Nereid">
Nereid

In Greek mythology, the Nereids ( ; "Nereides", sg. "Nereis") are sea nymphs (female spirits of sea waters), the 50 daughters of Nereus and Doris, sisters to Nerites. They often accompany Poseidon, the god of the sea, and can be friendly and helpful to sailors, like the Argonauts in their search for the Golden Fleece.

Nereids are particularly associated with the Aegean Sea, where they dwelt with their father Nereus in the depths within a golden palace. The most notable of them are Thetis, wife of Peleus and mother of Achilles; Amphitrite, wife of Poseidon; and Galatea, the vain love interest of the Cyclops Polyphemus.They symbolized everything that is beautiful and kind about the sea. Their melodious voices sang as they danced around their father. They are represented as very beautiful girls, crowned with branches of red coral and dressed in white silk robes trimmed with gold, but who went barefoot. They were part of Poseidon's entourage and carried his trident.

In Homer's "Iliad" XVIII, when Thetis cries out in sympathy for the grief of Achilles for the slain Patroclus, her sisters appear. The Nereid Opis is mentioned in Virgil's "Aeneid". She is called by the goddess Diana to avenge the death of the Amazon-like female warrior Camilla. Diana gives Opis magical weapons for revenge on Camilla's killer, the Etruscan Arruns. Opis sees and laments Camilla's death and shoots Arruns in revenge as directed by Diana.

In modern Greek folklore, the term "nereid" (, "neráida") has come to be used for all nymphs, fairies, or mermaids, not merely nymphs of the sea.

Nereid, a moon of the planet Neptune, is named after the Nereids.

"This list is correlated from four sources: Homer's "Iliad", Hesiod's "Theogony", the "Bibliotheca", and Hyginus. Because of this, the total number of names goes beyond fifty."



</doc>
<doc id="21587" url="https://en.wikipedia.org/wiki?curid=21587" title="Nemesis (disambiguation)">
Nemesis (disambiguation)

Nemesis is a Greek mythological spirit of divine retribution against those who succumb to hubris. Nemesis may also refer to:















</doc>
<doc id="21588" url="https://en.wikipedia.org/wiki?curid=21588" title="Nereid (moon)">
Nereid (moon)

Nereid is the third-largest moon of Neptune. It has a highly eccentric orbit. It was the second moon of Neptune to be discovered, by Gerard Kuiper in 1949.

Nereid was discovered on 1 May 1949 by Gerard P. Kuiper on photographic plates taken with the 82-inch telescope at the McDonald Observatory. He proposed the name in the report of his discovery. It is named after the Nereids, sea-nymphs of Greek mythology and attendants of the god Neptune. It was the second and last moon of Neptune to be discovered before the arrival of Voyager 2 (not counting a single observation of an occultation by Larissa in 1981).

Nereid orbits Neptune in the prograde direction at an average distance of , but its high eccentricity of 0.7507 takes it as close as and as far as .

The unusual orbit suggests that it may be either a captured asteroid or Kuiper belt object, or that it was an inner moon in the past and was perturbed during the capture of Neptune's largest moon Triton.

In 1991, a rotation period of Nereid of about 13.6 hours was determined by an analysis of its light curve. In 2003, another rotation period of about was measured. However, this determination was later disputed, and other researchers for a time failed to detect any periodic modulation in Nereid's light curve from ground-based observations. In 2016, a clear rotation period of 11.594 ± 0.017 hours was determined based on observations with the Kepler space telescope.

Nereid is third-largest of Neptune's satellites, and has an average radius of about . It is rather large for an irregular satellite. The shape of Nereid is unknown.

Since 1987 some photometric observations of Nereid have detected large (by ~1 of magnitude) variations of its brightness, which can happen over years and months, but sometimes even over a few days. They persist even after a correction for distance and phase effects. On the other hand, not all astronomers who have observed Nereid have noticed such variations. This means that they may be quite chaotic. To date there is no credible explanation of the variations, but, if they exist, they are likely related to the rotation of Nereid. Nereid's rotation could be either in the state of forced precession or even chaotic rotation (like Hyperion) due to its highly elliptical orbit.

In 2016, extended observations with the Kepler space telescope showed only low-amplitude variations (0.033 magnitudes). Thermal modeling based on infrared observations from the Spitzer and Herschel space telescopes suggest that Nereid is only moderately elongated which disfavors forced precession of the rotation. The thermal model also indicates that the surface roughness of Nereid is very high, likely similar to the Saturnian moon Hyperion.

Spectrally, Nereid appears neutral in colour and water ice has been detected on its surface. Its spectrum appears to be intermediate between Uranus's moons Titania and Umbriel, which suggests that Nereid's surface is composed of a mixture of water ice and some spectrally neutral material. The spectrum is markedly different from minor planets of the outer solar system, centaurs Pholus, Chiron and Chariklo, suggesting that Nereid formed around Neptune rather than being a captured body.

Halimede, which has similar colours, may be a fragment of Nereid that was broken off during a collision.

The only spacecraft to visit Nereid was "Voyager 2", which passed it at a distance of between 20 April and 19 August 1989. "Voyager 2" obtained 83 images with observation accuracies of to . Prior to "Voyager 2"'s arrival, observations of Nereid had been limited to ground-based observations that could only establish its intrinsic brightness and orbital elements. Although the images obtained by "Voyager 2" do not have a high enough resolution to allow surface features to be distinguished, "Voyager 2" was able to measure the size of Nereid and found that it was grey in colour and had a higher albedo than Neptune's other small satellites.

In the Larry Niven book "Ringworld", Nereid is described as having been leased by the outsiders "half a millennium ago". The protagonist, Louis Wu, speculates that the outsiders evolved on a gas giant moon similar to Nereid.



</doc>
<doc id="21592" url="https://en.wikipedia.org/wiki?curid=21592" title="Netball">
Netball

Netball is a ball sport played by two teams of seven players. Its development, derived from early versions of basketball, began in England in the 1890s. By 1960, international playing rules had been standardised for the game, and the International Federation of Netball and Women's Basketball (later renamed the International Netball Federation (INF)) was formed. As of 2011, the INF comprises more than 60 national teams organized into five global regions.

Games are played on a rectangular court with raised goal rings at each end. Each team attempts to score goals by passing a ball down the court and shooting it through its goal ring. Players are assigned specific positions, which define their roles within the team and restrict their movement to certain areas of the court. During general play, a player with the ball can hold on to it for only three seconds before shooting for a goal or passing to another player. The winning team is the one that scores the most goals. Netball games are 60 minutes long. Variations have been developed to increase the game's pace and appeal to a wider audience.

Netball is most popular in Commonwealth nations, specifically in schools, and is predominantly played by women. According to the INF, netball is played by more than 20 million people in more than 80 countries. Major domestic leagues in the sport include the Netball Superleague in Great Britain, Suncorp Super Netball in Australia and the ANZ Premiership in New Zealand. Four major competitions take place internationally: the quadrennial World Netball Championships, the Commonwealth Games, and the yearly Quad Series and Fast5 Series. In 1995, netball became an International Olympic Committee recognised sport, but it has not been played at the Olympics.

Netball emerged from early versions of basketball and evolved into its own sport as the number of women participating in sports increased. Basketball was invented in 1891 by James Naismith in the United States. The game was initially played indoors between two teams of nine players, using an association football that was thrown into closed-end peach baskets. Naismith's game spread quickly across the United States and variations of the rules soon emerged. Physical education instructor Senda Berenson developed modified rules for women in 1892; these eventually gave rise to women's basketball. Around this time separate intercollegiate rules were developed for men and women. The various basketball rules converged into a universal set in the United States.

Martina Bergman-Österberg introduced a version of basketball in 1893 to her female students at the Physical Training College in Hampstead, London. The rules of the game were modified at the college over several years: the game moved outdoors and was played on grass; the baskets were replaced by rings that had nets; and in 1897 and 1899, rules from women's basketball in the United States were incorporated. Madame Österberg's new sport acquired the name "net ball". The first codified rules of netball were published in 1901 by the Ling Association, later the Physical Education Association of the United Kingdom. From England, netball spread to other countries in the British Empire. Variations of the rules and even names for the sport arose in different areas: "women's (outdoor) basketball" arrived in Australia around 1900 and in New Zealand from 1906, while "netball" was being played in Jamaican schools by 1909.

From the start, it was considered socially appropriate for women to play netball; netball's restricted movement appealed to contemporary notions of women's participation in sports, and the sport was distinct from potential rival male sports. Netball became a popular women's sport in countries where it was introduced and spread rapidly through school systems. School leagues and domestic competitions emerged during the first half of the 20th century, and in 1924 the first national governing body was established in New Zealand. International competition was initially hampered by a lack of funds and varying rules in different countries. Australia hosted New Zealand in the first international game of netball in Melbourne on 20 August 1938; Australia won 40–11. Efforts began in 1957 to standardise netball rules globally: by 1960 international playing rules had been standardised, and the International Federation of Netball and Women's Basketball, later the International Netball Federation (INF), was formed to administer the sport worldwide.

Representatives from England, Australia, New Zealand, South Africa, and the West Indies were part of a 1960 meeting in Sri Lanka that standardised the rules for the game. The game spread to other African countries in the 1970s. South Africa was prohibited from competing internationally from 1969 to 1994 due to apartheid. In the United States, Netball's popularity also increased during the 1970s, particularly in the New York area, and the United States of America Netball Association was created in 1992. The game also became popular in the Pacific Island nations of the Cook Islands, Fiji and Samoa during the 1970s. Netball Singapore was created in 1962, and the Malaysian Netball Association was created in 1978.

In Australia, the term "women's basketball" was used to refer to both netball and basketball. During the 1950s and 1960s, a movement arose to change the Australian name of the game from "women's basketball" to "netball" in order to avoid confusion between the two sports. The Australian Basketball Union offered to pay the costs involved to alter the name, but the netball organisation rejected the change. In 1970, the Council of the All Australia Netball Association officially changed the name to "netball" in Australia.

In 1963, the first international tournament was held in Eastbourne, England. Originally called the World Tournament, it later became known as the World Netball Championships. Following the first tournament, one of the organisers, Miss R. Harris, declared,
The World Netball Championships have been held every four years since, most recently in 2015. The World Youth Netball Championships started in Canberra in 1988, and have been held roughly every four years since. In 1995, the International Olympic Committee designated netball as an Olympic recognised sport. Three years later it debuted at the 1998 Commonwealth Games in Kuala Lumpur. Other international competitions also emerged in the late 20th century, including the Nations Cup and the Asian Netball Championship.

As of 2006, the IFNA recognises only women's netball. Men's netball teams exist in some areas but attract less attention from sponsors and spectators. Men's netball started to become popular in Australia during the 1980s, and the first men's championship was held in 1985. In 2004, New Zealand and Fiji sent teams to compete in the Australian Mixed and Men's National Championships. By 2006, mixed netball teams in Australia had as many male participants as rugby union. Other countries with men's national teams include Canada, Fiji, Jamaica, Kenya, Pakistan and the United Arab Emirates. Unlike women's netball at elite and national levels, men's and mixed gender teams are largely self-funded.

An all-transgender netball team from Indonesia competed at the 1994 Gay Games in New York City. The team had been the Indonesian national champions. At the 2000 Gay Games VI in Sydney, netball and volleyball were the two sports with the highest rates of transgender athletes participating. There were eight teams of indigenous players, with seven identifying as transgender. They came from places like Palm Island in northern Queensland, Samoa, Tonga and Papua New Guinea. Teams with transgender players were allowed to participate in several divisions including men's, mixed and transgender; they were not allowed to compete against the cisgender women's teams.

The objective of a game is to score more goals than the opposition. Goals are scored when a team member positioned in the attacking shooting circle shoots the ball through the goal ring. The goal rings are in diameter and sit atop -high goal posts that have no backboards. A -radius semi-circular "shooting circle" is an area at each end of the court. The goal posts are located within the shooting circle. Each team defends one shooting circle and attacks the other. The netball court is long, wide, and divided lengthwise into thirds. The ball is usually made of leather or rubber, measures in circumference, and weighs . A normal game consists of four 15-minute quarters and can be played outdoors or in a covered stadium.

Each team is allowed seven players on the court. Each player is assigned a specific position, which limits their movement to a certain area of the court. A "bib" worn by each player contains a two-letter abbreviation indicating this position. Only two positions are permitted in the attacking shooting circle, and can therefore shoot for a goal. Similarly, only two positions are permitted in the defensive shooting circle; they try to prevent the opposition from shooting goals. Other players are restricted to two thirds of the court, with the exception of the Centre, who may move anywhere on the court except for a shooting circle.
At the beginning of every quarter and after a goal has been scored, play starts with a player in the centre position passing the ball from the centre of the court. These "centre passes" alternate between the teams, regardless of which team scored the last goal. When the umpire blows the whistle to restart play, four players from each team can move into the centre third to receive the pass. The centre pass must be caught or touched in the centre third. The ball is then moved up and down the court through passing and must be touched by a player in each adjacent third of the court. Players can hold the ball for only three seconds at any time. It must be released before the foot they were standing on when they caught it touches the ground again. Contact between players is only permitted if it does not impede an opponent or the general play. When defending a pass or shot players must be at least away from the player with the ball. If illegal contact is made, the player who contacted cannot participate in play until the player taking the penalty has passed or shot the ball. If the ball is held in two hands and either dropped or a shot at goal is missed, the same player cannot be the first to touch it unless it first rebounds off the goal.

Indoor netball is a variation of netball, played exclusively indoors, in which the playing court is often surrounded on each side and overhead by a net. The net prevents the ball from leaving the court, permitting faster play by reducing playing stoppages.

Different forms of indoor netball exist. In a seven-per-side version called "action netball", seven players per team play with rules similar to netball. However, a game is split into 15-minute halves with a three-minute break in between. This version is played in Australia, New Zealand, South Africa and England.

A six-per-side version of the sport is also played in New Zealand. Two Centres per team can play in the whole court except the shooting circles; the remaining attacking and defending players are each restricted to one half of the court, including the shooting circles. The attacking and Centre players may shoot from outside the shooting circle for a two-point goal.

A five-per-side game is also common in indoor netball. Players can move throughout the court, with the exception of the shooting circles, which are restricted to certain attacking or defending players.

Fast5 (originally called Fastnet) is a variation on the rules of netball designed to make games faster and more television-friendly. The World Netball Series promotes it to raise the sport's profile and attract more spectators and greater sponsorship. The game is much shorter, with each quarter lasting only six minutes and only a two-minute break between quarters. The coaches can give instructions from the sideline during play, and unlimited substitutions are allowed. Like six-per-side indoor netball, attacking players may shoot two-point goals from outside the shooting circle. Each team can separately nominate one "power play" quarter, in which each goal scored by that team is worth double points and the centre pass is taken by the team that conceded the goal.

Netball has been adapted in several ways to meet children's needs. The rules for children are similar to those for adults, but various aspects of the game (such as the length of each quarter, goal height, and ball size) are modified.

Fun Net is a version of netball developed by Netball Australia for five- to seven-year-olds. It aims to improve basic netball skills using games and activities. The Fun Net program runs for 8–16 weeks. There are no winners or losers. The goal posts are high, and a smaller ball is used.

Netball Australia also runs a modified game called Netta aimed at 8- to 11-year-olds. The goal height and ball size are the same as for adults, but players rotate positions during the game, permitting each player to play each position. Netta was created to develop passing and catching skills. Its rules permit six seconds between catching and passing the ball, instead of the three seconds permitted in the adult game. Most players under 11 play this version at netball clubs.

A version called High Five Netball is promoted by the All England Netball Association. It is aimed at 9- to 11-year-old girls and includes only five positions. The players swap positions during the game. When a player is not on the court, she is expected to help the game in some other way, such as being the timekeeper or scorekeeper. High Five Netball has four six-minute quarters.

The recognised international governing body of netball is the International Federation of Netball Associations (IFNA), based in Manchester, England. Founded in 1960, the organisation was initially called the International Federation of Netball and Women's Basketball. The IFNA is responsible for compiling world rankings for national teams, maintaining the rules for netball and organising several major international competitions.

As of July 2012, the IFNA has 49 full and 24 associate national members in five regions. Each region has an IFNA regional federation.
The IFNA is affiliated with the General Association of International Sports Federations, the International World Games Association and the Association of IOC Recognised International Sports Federations. It is also a signatory to the World Anti-Doping Code.

Netball is a popular participant sport in countries of the Commonwealth of Nations. Non-Commonwealth entities with full IFNA membership include Switzerland, Taiwan, Thailand, Argentina, Bermuda, the Cayman Islands and the United States, along with former Commonwealth members Zimbabwe, Ireland and Hong Kong. According to the IFNA, over 20 million people play netball in more than 80 countries. International tournaments are held among countries in each of the five IFNA regions, either annually or every four years. School leagues and national club competitions have been organised in England, Australia, New Zealand and Jamaica since the early twentieth century. Franchise-based netball leagues did not emerge until the late 1990s. These competitions sought to increase the profile of the sport in their respective countries. Despite widespread local interest, participation was largely amateur.

Netball was first included in the 1998 Commonwealth Games and has been a fixture ever since; it is currently one of the "core" sports that must be contested at each edition of the Games.

The major international tournament in Africa is organised by the Confederation of Southern African Netball Associations, which invites teams from Botswana, Namibia, Zambia, Malawi, South Africa, Lesotho, Swaziland, Zimbabwe and the Seychelles to take part. The tournament is hosted by a country within the region; senior and under 21 teams compete. The tournament has served as a qualifier for the World Championships. South Africa launched a new domestic competition in 2011 called Netball Grand Series. It features eight regional teams from South Africa and is aimed at increasing the amount of playing time for players. It runs for 17 weeks and replaces the National Netball League, which was played over only two weeks. According to Proteas captain Elsje Jordaan, it was hoped that the competition would create an opportunity for players to become professional.
The American Federation of Netball Associations (AFNA) hosts two tournaments each year: the Caribbean Netball Association (CNA) Under 16 Championship and the AFNA Senior Championship. The CNA championship involves two divisions of teams from the Caribbean islands. In 2010 five teams competed in two rounds of round robin matches in the Championship Division, while four teams competed in the Developmental Division. Jamaica, which has lost only once in the tournament, decided not to play the 2011 tournament. The AFNA Senior Championship includes Canada and the US along with the Caribbean nations. The tournament serves as a qualifier for the World Championship. Jamaica, with its high ranking, does not have to qualify; this leaves two spots to the other teams in the tournament.

The Asian Netball Championship is held every four years. The seventh Asian games were held in 2009 and featured Singapore, Thailand, Maldives, Taiwan, Malaysia, Sri Lanka, Hong Kong, India and Pakistan. There is also an Asian Youth Netball Championship for girls under 21 years of age, the seventh of which was held in 2010.

The major netball competition in Europe is the Netball Superleague, which features nine teams from England, Wales and Scotland. The league was created in 2005. Matches are broadcast on Sky Sports.

Netball has been featured at the Pacific Games, a multi-sport event with participation from 22 countries from around the South Pacific. The event is held every four years and has 12 required sports; the host country chooses the other four. Netball is not a required sport and has missed selection, particularly when former French or American territories host the games.

The ANZ Championship was a Trans-Tasman competition held between 2008 and 2016 that was broadcast on television in both New Zealand and Australia. It was contested among ten teams from Australia and New Zealand. It began in April 2008, succeeding Australia's Commonwealth Bank Trophy and New Zealand's National Bank Cup as the pre-eminent netball league in those countries. The competition was held annually between April and July, consisting of 69 matches played over 17 weeks. The ANZ Championship saw netball become a semi-professional sport in both countries, with increased media coverage and player salaries. The competition was replaced by new leagues in 2017, the Suncorp Super Netball (Australia) and ANZ Premiership (New Zealand).

There are four major international netball competitions; the Netball World Cup, Netball at the Commonwealth Games, Netball Quad Series and Fast5 Netball World Series.

Netball's important competition is the World Netball Championships (also known as the Netball World Cup), held every four years. It was first held in 1963 at the Chelsea College of Physical Education at Eastbourne, England, with eleven nations competing. Since its inception the competition has been dominated primarily by the Australian and New Zealand teams, which hold ten and four titles, respectively. Trinidad and Tobago is the only other team to win a championship title. That title, won in 1979, was shared with New Zealand and Australia; all three teams finished with equal points at the end of the round robin, and there were no finals.

The Fast5 Series is a competition among the top six national netball teams, as ranked by the INF World Rankings. It is organised by the INF in conjunction with the national governing bodies of the six competing nations, UK Sport, and the host city's local council. The All England Netball Association covers air travel, accommodation, food and local travel expenses for all teams, while the respective netball governing bodies cover player allowances. It is held over three days, with each team playing each other once during the first two days in a round-robin format. The four highest-scoring teams advance to the semi-finals; the winners face each other in the Grand Final. The competition features modified fastnet rules and has been likened to Twenty20 cricket and rugby sevens. A new format featuring shorter matches with modified rules was designed to make the game more appealing to spectators and television audiences. The World Netball Series was held annually in England from 2009 to 2011.

Netball gained Olympic recognition in 1995 after 20 years of lobbying. Although it has never been played at the Summer Olympics, politicians and administrators have been campaigning to have it included in the near future. Its absence from the Olympics has been seen by the netball community as a hindrance in the global growth of the game by limiting access to media attention and funding sources. Some funding sources became available with recognition in 1995, including the International Olympic Committee, national Olympic committees, national sport organisations, and state and federal governments.

One study found that over 14 weeks of play about 5% of people develop an injury. The most common injury is of the ankle (usually lateral ligament ankle strain and less often an ankle fracture). Knee injuries were less common and included anterior cruciate ligament injuries. The main cause of these injuries is believed to be due to incorrect landing. One study found not warm-up as a risk factor. Hypermobility, have a range of motion beyond normal limits, has been associated with injuries in one small study. Higher grade players, in both senior and junior competitions, are more susceptible to injuries than lower grade players, due to the high intensity and rapid pace of the game.

In October 2005, Australian captain Liz Ellis, developed an ACL in a match against New Zealand. This injury ruled her out of the chance to play at the 2006 Melbourne Commonwealth games. In October 2014, Casey Kopua ruptured the patella tendon in her left knee and had knee surgery to repair the tendon that would result in her missing up to 6 months of netball.





</doc>
<doc id="21594" url="https://en.wikipedia.org/wiki?curid=21594" title="Njörðr">
Njörðr

In Norse mythology, Njörðr is a god among the Vanir. Njörðr, father of the deities Freyr and Freyja by his unnamed sister, was in an ill-fated marriage with the goddess Skaði, lives in Nóatún and is associated with the sea, seafaring, wind, fishing, wealth, and crop fertility.

Njörðr is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, the "Prose Edda", written in the 13th century by Snorri Sturluson, in euhemerized form as a beloved mythological early king of Sweden in "Heimskringla", also written by Snorri Sturluson in the 13th century, as one of three gods invoked in the 14th century "Hauksbók" ring oath, and in numerous Scandinavian place names. Veneration of Njörðr survived into 18th or 19th century Norwegian folk practice, where the god is recorded as Njor and thanked for a bountiful catch of fish.

Njörðr has been the subject of an amount of scholarly discourse and theory, often connecting him with the figure of the much earlier attested Germanic goddess Nerthus, the hero Hadingus, and theorizing on his formerly more prominent place in Norse paganism due to the appearance of his name in numerous place names. "Njörðr" is sometimes modernly anglicized as Njord, Njoerd, or Njorth.

The name "Njörðr" corresponds to that of the older Germanic fertility goddess "Nerthus", and both derive from the Proto-Germanic "*Nerþuz". The original meaning of the name is contested, but it may be related to the Irish word "nert" which means "force" and "power". It has been suggested that the change of sex from the female "Nerthus" to the male "Njörðr" is due to the fact that feminine nouns with u-stems disappeared early in Germanic language while the masculine nouns with u-stems prevailed. However, other scholars hold the change to be based not on grammatical gender but on the evolution of religious beliefs; that *Nerþuz and Njörðr appear as different genders because they are to be considered separate beings. The name "Njörðr" may be related to the name of the Norse goddess Njörun.

Njörðr's name appears in various place names in Scandinavia, such as "Nærdhæwi" (now Nalavi, Närke), "Njærdhavi" (now Mjärdevi, Linköping; both using the religious term vé), "Nærdhælunda" (now Närlunda, Helsingborg), "Nierdhatunum" (now Närtuna, Uppland) in Sweden, Njarðvík in southwest Iceland, Njarðarlög and Njarðey (now Nærøy) in Norway. Njörðr's name appears in a word for sponge; "Njarðarvöttr" (Old Norse "Njörðr's glove"). Additionally, in Old Icelandic translations of Classical mythology the Roman god Saturn's name is glossed as "Njörðr."

Njörðr is described as a future survivor of Ragnarök in stanza 39 of the poem "Vafþrúðnismál". In the poem, the god Odin, disguised as "Gagnráðr" faces off with the wise jötunn Vafþrúðnir in a battle of wits. While Odin states that Vafþrúðnir knows all the fates of the gods, Odin asks Vafþrúðnir "from where Njörðr came to the sons of the Æsir," that Njörðr rules over quite a lot of temples and hörgrs (a type of Germanic altar), and further adds that Njörðr was not raised among the Æsir. In response, Vafþrúðnir says:

In stanza 16 of the poem "Grímnismál", Njörðr is described as having a hall in Nóatún made for himself. The stanza describes Njörðr as a "prince of men," that he is "lacking in malice," and that he "rules over the "high-timbered temple." In stanza 43, the creation of the god Freyr's ship Skíðblaðnir is recounted, and Freyr is cited as the son of Njörðr. In the prose introduction to the poem "Skírnismál", Freyr is mentioned as the son of Njörðr, and stanza 2 cites the goddess Skaði as the mother of Freyr. Further in the poem, Njörðr is again mentioned as the father of Freyr in stanzas 38, 39, and 41.

In the late flyting poem "Lokasenna", an exchange between Njörðr and Loki occurs in stanzas 33, 34, 35, and 36. After Loki has an exchange with the goddess Freyja, in stanza 33 Njörðr states:

Loki responds in the stanza 34, stating that "from here you were sent east as hostage to the gods" (a reference to the Æsir-Vanir War) and that "the daughters of Hymir used you as a pisspot, and pissed in your mouth." In stanza 35, Njörðr responds that:

Loki tells Njörðr to "stop" and "keep some moderation," and that he "won't keep it a secret any longer" that Njörðr's son Freyr was produced with his unnamed sister, "though you'd expect him to be worse than he is." The god Tyr then interjects and the flyting continues in turn.

Njörðr is referenced in stanza 22 of the poem "Þrymskviða", where he is referred to as the father of the goddess Freyja. In the poem, the jötunn Þrymr mistakenly thinks that he will be receiving the goddess Freyja as his bride, and while telling his fellow jötunn to spread straw on the benches in preparation for the arrival of Freyja, he refers to her as the daughter of Njörðr of Nóatún. Towards the end of the poem "Sólarljóð", Njörðr is cited as having nine daughters. Two of the names of these daughters are given; the eldest Ráðveig and the youngest Kreppvör.

Njörðr is also mentioned in the "Prose Edda" books "Gylfaginning" and "Skáldskaparmál".

In the "Prose Edda", Njörðr is introduced in chapter 23 of the book "Gylfaginning". In this chapter, Njörðr is described by the enthroned figure of High as living in the heavens at Nóatún, but also as ruling over the movement of the winds, having the ability to calm both sea and fire, and that he is to be invoked in seafaring and fishing. High continues that Njörðr is very wealthy and prosperous, and that he can also grant wealth in land and valuables to those who request his aid. Njörðr originates from Vanaheimr and is devoid of Æsir stock, and he is described as having been traded with Hœnir in hostage exchange with between the Æsir and Vanir.

High further states that Njörðr's wife is Skaði, that she is the daughter of the jötunn Þjazi, and recounts a tale involving the two. High recalls that Skaði wanted to live in the home once owned by her father called Þrymheimr ("Thunder Home"). However, Njörðr wanted to live nearer to the sea. Subsequently, the two made an agreement that they would spend nine nights in Þrymheimr and then next three nights in Nóatún (or nine winters in Þrymheimr and another nine in Nóatún according to the "Codex Regius" manuscript). However, when Njörðr returned from the mountains to Nóatún, he says:

Skaði then responds:

High states that afterward Skaði went back up to the mountains to Þrymheimr and recites a stanza where Skaði skis around, hunts animals with a bow, and lives in her fathers old house. Chapter 24 begins, which describes Njörðr as the father of two beautiful and powerful children: Freyr and Freyja. In chapter 37, after Freyr has spotted the beautiful jötunn Gerðr, he becomes overcome with sorrow, and refuses to sleep, drink, or talk. Njörðr then sends for Skírnir to find out who he seems to be so angry at, and, not looking forward to being treated roughly, Skírnir reluctantly goes to Freyr.

Njörðr is introduced in "Skáldskaparmál" within a list of 12 Æsir attending a banquet held for Ægir. Further in "Skáldskaparmál", the skaldic god Bragi recounds the death of Skaði's father Þjazi by the Æsir. As one of the three acts of reparation performed by the Æsir for Þjazi's death, Skaði was allowed by the Æsir to choose a husband from amongst them, but given the stipulation that she may not see any part of them but their feet when making the selection. Expecting to choose the god Baldr by the beauty of the feet she selects, Skaði instead finds that she has picked Njörðr.

In chapter 6, a list of kennings is provided for Njörðr: "God of chariots," "Descendant of Vanir," "a Van," father of Freyr and Freyja, and "the giving god." This is followed by an excerpt from a composition by the 11th century skald Þórðr Sjáreksson, explained as containing a reference to Skaði leaving Njörðr:

Chapter 7 follows and provides various kennings for Freyr, including referring to him as the son of Njörðr. This is followed by an excerpt from a work by the 10th century skald Egill Skallagrímsson that references Njörðr (here anglicized as "Niord"):

In chapter 20, "daughter of Njörðr" is given as a kenning for Freyja. In chapter 33, Njörðr is cited among the gods attending a banquet held by Ægir. In chapter 37, Freyja is again referred to as Njörðr's daughter in a verse by the 12th century skald Einarr Skúlason. In chapter 75, Njörðr is included in a list of the Æsir. Additionally, "Njörðr" is used in kennings for "warrior" or "warriors" various times in "Skáldskaparmál".

Njörðr appears in or is mentioned in three Kings' sagas collected in "Heimskringla"; "Ynglinga saga", the "Saga of Hákon the Good" and the "Saga of Harald Graycloak". In chapter 4 of "Ynglinga saga", Njörðr is introduced in connection with the Æsir-Vanir War. When the two sides became tired of war, they came to a peace agreement and exchanged hostages. For their part, the Vanir send to the Æsir their most "outstanding men"; Njörðr, described as wealthy, and Freyr, described as his son, in exchange for the Æsir's Hœnir. Additionally, the Æsir send Mímir in exchange for the wise Kvasir.

Further into chapter 4, Odin appoints Njörðr and Freyr as priests of sacrificial offerings, and they became gods among the Æsir. Freyja is introduced as a daughter of Njörðr, and as the priestess at the sacrifices. In the saga, Njörðr is described as having once wed his unnamed sister while he was still among the Vanir, and the couple produced their children Freyr and Freyja from this union, though this custom was forbidden among the Æsir.

Chapter 5 relates that Odin gave all of his temple priests dwelling places and good estates, in Njörðr's case being Nóatún. Chapter 8 states that Njörðr married a woman named Skaði, though she would not have intercourse with him. Skaði then marries Odin, and the two had numerous sons.

In chapter 9, Odin dies and Njörðr takes over as ruler of the Swedes, and he continues the sacrifices. The Swedes recognize him as their king, and pay him tribute. Njörðr's rule is marked with peace and many great crops, so much so that the Swedes believed that Njörðr held power over the crops and over the prosperity of mankind. During his rule, most of the Æsir die, their bodies are burned, and sacrifices are made by men to them. Njörðr has himself "marked for" Odin and he dies in his bed. Njörðr's body is burnt by the Swedes, and they weep heavily at his tomb. After Njörðr's reign, his son Freyr replaces him, and he is greatly loved and "blessed by good seasons like his father."

In chapter 14 of "Saga of Hákon the Good" a description of the pagan Germanic custom of Yule is given. Part of the description includes a series of toasts. The toasts begin with Odin's toasts, described as for victory and power for the king, followed by Njörðr and Freyr's toast, intended for good harvests and peace. Following this, a beaker is drank for the king, and then a toast is given for departed kin. Chapter 28 quotes verse where the kenning "Njörðr-of-roller-horses" is used for "sailor". In the "Saga of Harald Graycloak", a stanza is given of a poem entitled "Vellekla" ("Lack of Gold") by the 10th century Icelandic skald Einarr skálaglamm that mentions Njörðr in a kenning for "warrior."

In chapter 80 of the 13th century Icelandic saga "Egils saga", Egill Skallagrímsson composes a poem in praise of Arinbjörn ("Arinbjarnarkviða"). In stanza 17, Egill writes that all others watch in marvel how Arinbjörn gives out wealth, as he has been so endowed by the gods Freyr and Njörðr.

Veneration of Njörðr survived into 18th or 19th century Norwegian folk practice, as recorded in a tale collected by Halldar O. Opedal from an informant in Odda, Hordaland, Norway. The informant comments on a family tradition in which the god is thanked for a bountiful catch of fish: 

Scholar Georges Dumézil further cites various tales of "havmennesker" (Norwegian "sea people") who govern over sea weather, wealth, or, in some incidents, give magic boats are likely connected to Njörðr.

Njörðr is often identified with the goddess Nerthus, whose reverence by various Germanic tribes is described by Roman historian Tacitus in his 1st CE century work "Germania". The connection between the two is due to the linguistic relationship between "Njörðr" and the reconstructed "*Nerþuz", "Nerthus" being the feminine, Latinized form of what "Njörðr" would have looked like around 1 CE. This has led to theories about the relation of the two, including that Njörðr may have once been a hermaphroditic god or, generally considered more likely, that the name may indicate an otherwise unattested divine brother and sister pair such as Freyr and Freyja. Consequently, Nerthus has been identified with Njörðr's unnamed sister with whom he had Freyja and Freyr, which is mentioned in "Lokasenna".

In Saami mythology, Bieka-Galles (or Biega-, Biegga-Galles, depending on dialect; "The Old Man of the Winds") is a deity who rules over rain and wind, and is the subject of boat and wooden shovel (or, rather, oar) offerings. Due to similarities in between descriptions of Njörðr in "Gylfaginning" and descriptions of Bieka-Galles in 18th century missionary reports, Axel Olrik identified this deity as the result of influence from the seafaring North Germanic peoples on the landbound Saami.

Parallels have been pointed out between Njörðr and the figure of Hadingus, attested in book I of Saxo Grammaticus' 13th century work "Gesta Danorum". Some of these similarities include that, in parallel to Skaði and Njörðr in "Skáldskaparmál", Hadingus is chosen by his wife Regnhild after selecting him from other men at a banquet by his lower legs, and, in parallel to Skaði and Njörðr in "Gylfaginning", Hadingus complains in verse of his displeasure at his life away from the sea and how he is disturbed by the howls of wolves, while his wife Regnhild complains of life at the shore and states her annoyance at the screeching sea birds. Georges Dumézil theorized that in the tale Hadingus passes through all three functions of his trifunctional hypothesis, before ending as an Odinic hero, paralleling Njörðr's passing from the Vanir to the Æsir in the Æsir-Vanir War.

In stanza 8 of the poem "Fjölsvinnsmál", Svafrþorinn is stated as the father of Menglöð by an unnamed mother, who the hero Svipdagr seeks. Menglöð has often been theorized as the goddess Freyja, and according to this theory, Svafrþorinn would therefore be Njörðr. The theory is complicated by the etymology of the name "Svafrþorinn" ("þorinn" meaning "brave" and "svafr" means "gossip") (or possibly connects to "sofa" "sleep"), which Rudolf Simek says makes little sense when attempting to connect it to Njörðr.

Njörðr has been the subject of an amount of artistic depictions. Depictions include "Freyr und Gerda; Skade und Niurd" (drawing, 1883) by K. Ehrenberg, "Njörðr" (1893) by Carl Frederick von Saltza, "Skadi" (1901) by E. Doepler d. J., and "Njörd's desire of the Sea" (1908) by W. G. Collingwood.

Njörðr is one of the incarnated gods in the New Zealand comedy/drama "The Almighty Johnsons". The part of "Johan Johnson/Njörðr" is played by Stuart Devenie.

 


</doc>
<doc id="21597" url="https://en.wikipedia.org/wiki?curid=21597" title="Neutral">
Neutral

Neutral or neutrality may refer to:











</doc>
<doc id="21601" url="https://en.wikipedia.org/wiki?curid=21601" title="Niger–Congo languages">
Niger–Congo languages

The Niger–Congo languages constitute one of the world's major language families and Africa's largest in terms of geographical area, number of speakers and number of distinct languages. It is generally considered to be the world's largest language family in terms of distinct languages, ahead of Austronesian, although this is complicated by the ambiguity about what constitutes a distinct language; the number of named Niger–Congo languages listed by "Ethnologue" is 1,540. It is the third largest language family in the world by number of native speakers, comprising around 700 million people as of 2015. Within Niger–Congo, the Bantu languages alone account for 350 million people (2015), or half the total Niger–Congo speaking population.

One of the characteristics common to most Niger–Congo languages (the Atlantic–Congo languages) is the use of a noun class system. The most widely spoken Niger–Congo languages by number of native speakers are Yoruba, Igbo, Fula and Shona. The most widely spoken by number of speakers is Swahili.

While the ultimate genetic unity of Niger–Congo is widely accepted (aside from Dogon, Mande and a few other languages), the internal cladistic structure of Niger–Congo is not well established. Its primary branches are Dogon, Mande, Ijo, Katla, Rashad and Atlantic–Congo.

The language family most likely originated in or near the area where these languages were spoken prior to Bantu expansion (i.e. West Africa or Central Africa). Its expansion may have been associated with the expansion of Sahel agriculture in the African Neolithic period, following the desiccation of the Sahara in c. 3500 BCE.

According to Roger Blench (2004), all specialists in Niger–Congo languages believe the languages to have a common origin, rather than merely constituting a typological classification, for reasons including their shared noun-class system, shared verbal extensions and shared basic lexicon. Similar classifications to Niger–Congo have been made ever since Diedrich Westermann in 1922. Joseph Greenberg continued that tradition, making it the starting point for modern linguistic classification in Africa, with some of his most notable publications going to press starting in the 1960s. However, there has been active debate for many decades over the appropriate subclassifications of the languages in this language family, which is a key tool used in localising a language's place of origin. No definitive "Proto-Niger–Congo" lexicon or grammar has been developed for the language family as a whole.

An important unresolved issue in determining the time and place where the Niger–Congo languages originated and their range prior to recorded history is this language family's relationship to the Kordofanian languages, spoken now spoken in the Nuba mountains of Sudan, which is not contiguous with the remainder of the Niger–Congo-language-speaking region and is at the northeasternmost extent of the current Niger–Congo linguistic region. The current prevailing linguistic view is that Kordofanian languages are part of the Niger–Congo language family and that these may be the first of the many languages still spoken in that region to have been spoken in the region. The evidence is insufficient to determine if this outlier group of Niger–Congo language speakers represent a prehistoric range of a Niger–Congo linguistic region that has since contracted as other languages have intruded, or if instead, this represents a group of Niger–Congo language speakers who migrated to the area at some point in prehistory where they were an isolated linguistic community from the beginning. 

There is more agreement regarding the place of origin of Benue–Congo, the largest subfamily of the group. Within Benue–Congo, the place of origin of the Bantu languages as well as time at which it started to expand is known with great specificity. Blench (2004), relying particularly on prior work by Kay Williamson and P. De Wolf, argued that Benue–Congo probably originated at the confluence of the Benue and Niger Rivers in central Nigeria. These estimates of the place of origin of the Benue-Congo language family do not fix a date for the start of that expansion, other than that it must have been sufficiently prior to the Bantu expansion to allow for the diversification of the languages within this language family that includes Bantu.

The classification of the relatively divergent family of the Ubangian languages, centred in the Central African Republic, as part of the Niger–Congo language family is disputed. Ubangian was grouped with Niger–Congo by Greenberg (1963), and later authorities concurred, but it was questioned by Dimmendaal (2008). 

The Bantu expansion, beginning around 1000 BC, swept across much of Central and Southern Africa, leading to the extinction of most of the indigenous Pygmy and Bushmen (Khoisan) populations there.

The following is an overview of the language groups usually included in Niger–Congo. The genetic relationship of some branches is not universally accepted, and the cladistic connection between those who are accepted as related may also be unclear.

The core phylum of the Niger–Congo group are the Atlantic–Congo languages. The non-Atlantic–Congo languages within Niger–Congo are grouped as Dogon, Mande, Ijo (sometimes with Defaka as Ijoid), Katla and Rashad.

Atlantic–Congo combines the Atlantic languages, which do not form one branch, and Volta–Congo. It comprises more than 80% of the Niger–Congo speaking population, or close to 600 million people (2015).

The proposed Savannas group combines Adamawa, Ubangian and Gur. Outside of the Savannas group, Volta–Congo comprises Kru, Kwa (or "West Kwa"), Volta–Niger (also "East Kwa" or "West Benue–Congo") and Benue–Congo (or "East Benue–Congo"). Volta–Niger includes the two largest languages of Nigeria, Yoruba and Igbo. Benue–Congo includes the Southern Bantoid group, which is dominated by the Bantu languages, which account for 350 million people (2015), or half the total Niger–Congo speaking population.

The strict genetic unity of any of these subgroups may themselves be under dispute. For example, Roger Blench (2012) argued that Adamawa, Ubangian, Kwa, Bantoid, and Bantu are not coherent groups.

"Glottolog" (2013) does not accept that the Kordofanian branches (Lafofa, Talodi and Heiban) or the difficult-to-classify Laal language have been demonstrated to be Atlantic–Congo languages. It otherwise accepts the family but not its inclusion within a broader Niger–Congo.

The Atlantic–Congo group is characterised the noun class systems of its languages. Atlantic–Congo largely corresponds to Mukarovsky's "Western Nigritic" phylum.


The polyphyletic Atlantic group accounts for about 35 million speakers as of 2016, mostly accounted for by Fula and Wolof speakers. Atlantic is not considered to constitute a valid group.








The Niger–Congo languages outside of the Atlantic–Congo supergroup are centred in the upper Senegal and Niger river basins, south and west of Timbuktu (Mande, Dogon), with a significant exclave at the Niger Delta (Ijoid). They account for a total population of about 100 million (2015), mostly Mandé and Ijaw.

The Kordofanian languages of Sudan may or may not be grouped with Atlantic–Congo. They are spoken in south-central Sudan, around the Nuba Mountains. "Kordofanian" is a geographic grouping, not a genetic one, named for the Kordofan region. These are minor languages, spoken by a total of about 100,000 people according to 1980s estimates. The genetic relationship of this group of twenty or so languages with Niger–Congo is disputed. Some linguists associate it with the Niger–Congo family, others consider them as forming a separate "Niger–Kordofanian" language family and yet others do not accept Niger–Kordofanian as a single group. 

The endangered or extinct Laal, Mpre and Jalaa languages are often assigned to Niger–Congo.

Niger–Congo as it is known today was only gradually recognized as a linguistic unit. In early classifications of the languages of Africa, one of the principal criteria used to distinguish different groupings was the languages' use of prefixes to classify nouns, or the lack thereof. A major advance came with the work of Sigismund Wilhelm Koelle, who in his 1854 "Polyglotta Africana" attempted a careful classification, the groupings of which in quite a number of cases correspond to modern groupings. An early sketch of the extent of Niger–Congo as one language family can be found in Koelle's observation, echoed in Bleek (1856), that the Atlantic languages used prefixes just like many Southern African languages. Subsequent work of Bleek, and some decades later the comparative work of Meinhof, solidly established Bantu as a linguistic unit.

In many cases, wider classifications employed a blend of typological and racial criteria. Thus, Friedrich Müller, in his ambitious classification (1876–88), separated the 'Negro' and Bantu languages. Likewise, the Africanist Karl Richard Lepsius considered Bantu to be of African origin, and many 'Mixed Negro languages' as products of an encounter between Bantu and intruding Asiatic languages.

In this period a relation between Bantu and languages with Bantu-like (but less complete) noun class systems began to emerge. Some authors saw the latter as languages which had not yet completely evolved to full Bantu status, whereas others regarded them as languages which had partly lost original features still found in Bantu. The Bantuist Meinhof made a major distinction between Bantu and a 'Semi-Bantu' group which according to him was originally of the unrelated Sudanic stock.

Westermann, a pupil of Meinhof, set out to establish the internal classification of the then Sudanic languages. In a 1911 work he established a basic division between 'East' and 'West'. A historical reconstruction of West Sudanic was published in 1927, and in his 1935 'Charakter und Einteilung der Sudansprachen' he conclusively established the relationship between Bantu and West Sudanic.

Joseph Greenberg took Westermann's work as a starting-point for his own classification. In a series of articles published between 1949 and 1954, he argued that Westermann's 'West Sudanic' and Bantu formed a single genetic family, which he named Niger–Congo; that Bantu constituted a subgroup of the Benue–Congo branch; that Adamawa–Eastern, previously not considered to be related, was another member of this family; and that Fula belonged to the West Atlantic languages. Just before these articles were collected in final book form ("The Languages of Africa") in 1963, he amended his classification by adding Kordofanian as a branch co-ordinate with Niger–Congo as a whole; consequently, he renamed the family "Congo–Kordofanian", later "Niger–Kordofanian". Greenberg's work on African languages, though initially greeted with scepticism, became the prevailing view among scholars. 

Bennet and Sterk (1977) presented an internal reclassification based on lexicostatistics that laid the foundation for the regrouping in Bendor-Samuel (1989). Kordofanian was presented as one of several primary branches rather than being coordinate to the family as a whole, prompting re-introduction of the term "Niger–Congo", which is in current use among linguists. Many classifications continue to place Kordofanian as the most distant branch, but mainly due to negative evidence (fewer lexical correspondences), rather than positive evidence that the other languages form a valid genealogical group. Likewise, Mande is often assumed to be the second-most distant branch based on its lack of the noun-class system prototypical of the Niger–Congo family. Other branches lacking any trace of the noun-class system are Dogon and Ijaw, whereas the Talodi branch of Kordofanian does have cognate noun classes, suggesting that Kordofanian is also not a unitary group.

"Glottolog" (2013) accepts the core with noun-class systems, the Atlantic–Congo languages, apart from the recent inclusion of some of the Kordofanian groups, but not Niger–Congo as a whole. They list the following as separate families:

Oxford Handbooks Online (2016) has indicated that the continuing reassessment of Niger-Congo's "internal structure is due largely to the preliminary nature of Greenberg’s classification, explicitly based as it was on a methodology that doesn’t produce proofs for genetic affiliations between languages but rather aims at identifying “likely candidates.”...The ongoing descriptive and documentary work on individual languages and their varieties, greatly expanding our knowledge on formerly little-known linguistic regions, is helping to identify clusters and units that allow for the application of the historical-comparative method. Only the reconstruction of lower-level units, instead of “big picture” contributions based on mass comparison, can help to verify (or disprove) our present concept of Niger-Congo as a genetic grouping consisting of Benue-Congo plus Volta-Niger, Kwa, Adamawa plus Gur, Kru, the so-called Kordofanian languages, and probably the language groups traditionally classified as Atlantic."

The coherence of Niger-Congo as a language phylum is supported by Grollemund, et al. (2016), using computational phylogenetic methods. The East/West Volta-Congo division, West/East Benue-Congo division, and North/South Bantoid division are not supported, whereas a Bantoid group consisting of Ekoid, Bendi, Dakoid, Jukunoid, Tivoid, Mambiloid, Beboid, Mamfe, Tikar, Grassfields, and Bantu is supported.

The Automated Similarity Judgment Program (ASJP) also groups many Niger-Congo branches together.

Over the years, several linguists have suggested a link between Niger–Congo and Nilo-Saharan, probably starting with Westermann's comparative work on the 'Sudanic' family in which 'Eastern Sudanic' (now classified as Nilo-Saharan) and 'Western Sudanic' (now classified as Niger–Congo) were united. Gregersen (1972) proposed that Niger–Congo and Nilo-Saharan be united into a larger phylum, which he termed "Kongo–Saharan". His evidence was mainly based on the uncertainty in the classification of Songhay, morphological resemblances, and lexical similarities. A more recent proponent was Roger Blench (1995), who puts forward phonological, morphological and lexical evidence for uniting Niger–Congo and Nilo-Saharan in a "Niger–Saharan" phylum, with special affinity between Niger–Congo and Central Sudanic. However, fifteen years later his views had changed, with Blench (2011) proposing instead that the noun-classifier system of Central Sudanic, commonly reflected in a tripartite general–singulative–plurative number system, triggered the development or elaboration of the noun-class system of the Atlantic–Congo languages, with tripartite number marking surviving in the Plateau and Gur languages of Niger–Congo, and the lexical similarities being due to loans.

Niger–Congo languages have a clear preference for open syllables of the type CV (Consonant Vowel). The typical word structure of Proto-Niger-Congo is thought to have been CVCV, a structure still attested in, for example, Bantu, Mande and Ijoid – in many other branches this structure has been reduced through phonological change. Verbs are composed of a root followed by one or more extensional suffixes. Nouns consist of a root originally preceded by a noun class prefix of (C)V- shape which is often eroded by phonological change.

Reconstructions of the consonant set of several branches of Niger–Congo (Stewart for proto-Volta–Congo, Mukarovsky for his proto-West-Nigritic, roughly corresponding to Atlantic–Congo) have posited independently a regular phonological contrast between two classes of consonants. Pending more clarity as to the precise nature of this contrast it is commonly characterized as a contrast between fortis and lenis consonants. Five places of articulation are postulated for the consonant inventory of proto-Niger–Congo: labial, alveolar, palatal, velar, and labial-velar.

Many Niger–Congo languages' vowel harmony is based on the [ATR] (advanced tongue root) feature. In this type of vowel harmony, the position of the root of the tongue in regards to backness is the phonetic basis for the distinction between two harmonizing sets of vowels. In its fullest form, this type involves two classes, each of five vowels:

The roots are then divided into [+ATR] and [−ATR] categories. This feature is lexically assigned to the roots because there is no determiner within a normal root that causes the [ATR] value.

There are two types of [ATR] vowel harmony controllers in Niger–Congo. The first controller is the root. When a root contains a [+ATR] or [−ATR] vowel, then that value is applied to the rest of the word, which involves crossing morpheme boundaries. For example, suffixes in Wolof assimilate to the [ATR] value of the root to which they attach. Some examples of these suffixes that alternate depending on the root are:

Furthermore, the directionality of assimilation in [ATR] root-controlled vowel harmony need not be specified. The root features [+ATR] and [−ATR] spread left and/or right as needed, so that no vowel would lack a specification and be ill-formed.

Unlike in the root-controlled harmony system, where the two [ATR] values behave symmetrically, a large number of Niger–Congo languages exhibit a pattern where the [+ATR] value is more active or dominant than the [−ATR] value. This results in the second vowel harmony controller being the [+ATR] value. If there is even one vowel that is [+ATR] in the whole word, then the rest of the vowels harmonize with that feature. However, if there is no vowel that is [+ATR], the vowels appear in their underlying form. This form of vowel harmony control is best exhibited in West African languages. For example, in Nawuri, the diminutive suffix /-bi/ will cause the underlying [−ATR] vowels in a word to become phonetically [+ATR].

There are two types of vowels which affect the harmony process. These are known as neutral or opaque vowels. Neutral vowels do not harmonize to the [ATR] value of the word, and instead maintain their own [ATR] value. The vowels that follow them, however, will receive the [ATR] value of the root. Opaque vowels maintain their own [ATR] value as well, but they affect the harmony process behind them. All of the vowels following an opaque vowel will harmonize with the [ATR] value of the opaque vowel instead of the [ATR] vowel of the root.

The vowel inventory listed above is a ten-vowel language. This is a language in which all of the vowels of the language participate in the harmony system, producing five harmonic pairs. Vowel inventories of this type are still found in some branches of Niger-Congo, for example in the Ghana Togo Mountain languages. However, this is the rarer inventory as oftentimes there are one or more vowels that are not part of a harmonic pair. This has resulted in seven-and nine-vowel systems being the more popular systems. The majority of languages with [ATR] controlled vowel harmony have either seven- or nine-vowel phonemes, with the most common non-participatory vowel being /a/. It has been asserted that this is because vowel quality differences in the mid-central region where /ə/, the counterpart of /a/, is found, are difficult to perceive. Another possible reason for the non-participatory status of /a/ is that there is articulatory difficulty in advancing the tongue root when the tongue body is low in order to produce a low [+ATR] vowel. Therefore, the vowel inventory for nine-vowel languages is generally:

And seven-vowel languages have one of two inventories:

Note that in the nine-vowel language, the missing vowel is, in fact, [ə], [a]'s counterpart, as would be expected.

The fact that ten vowels have been reconstructed for proto-Atlantic, proto-Ijoid and possibly proto-Volta–Congo has led to the hypothesis that the original vowel inventory of Niger–Congo was a full ten-vowel system. On the other hand, Stewart, in recent comparative work, reconstructs a seven-vowel system for his proto-Potou-Akanic-Bantu.

Several scholars have documented a contrast between oral and nasal vowels in Niger–Congo. In his reconstruction of proto-Volta–Congo, Steward (1976) postulates that nasal consonants have originated under the influence of nasal vowels; this hypothesis is supported by the fact that there are several Niger–Congo languages that have been analysed as lacking nasal consonants altogether. Languages like this have nasal vowels accompanied with complementary distribution between oral and nasal consonants before oral and nasal vowels. Subsequent loss of the nasal/oral contrast in vowels may result in nasal consonants becoming part of the phoneme inventory. In all cases reported to date, the bilabial /m/ is the first nasal consonant to be phonologized. Niger–Congo thus invalidates two common assumptions about nasals: that all languages have at least one primary nasal consonant, and that if a language has only one primary nasal consonant it is /n/.

Niger–Congo languages commonly show fewer nasalized than oral vowels. Kasem, a language with a ten-vowel system employing ATR vowel harmony, has seven nasalized vowels. Similarly, Yoruba has seven oral vowels and only five nasal ones. However, the recently discovered language of Zialo has nasal equivalent for each of its seven vowels.

The large majority of present-day Niger–Congo languages are tonal. A typical Niger–Congo tone system involves two or three contrastive level tones. Four level systems are less widespread, and five level systems are rare. Only a few Niger–Congo languages are non-tonal; Swahili is perhaps the best known, but within the Atlantic branch some others are found. Proto-Niger–Congo is thought to have been a tone language with two contrastive levels. Synchronic and comparative-historical studies of tone systems show that such a basic system can easily develop more tonal contrasts under the influence of depressor consonants or through the introduction of a downstep. Languages which have more tonal levels tend to use tone more for lexical and less for grammatical contrasts.

Niger–Congo languages are known for their system of noun classification, traces of which can be found in every branch of the family but Mande, Ijoid, Dogon, and the Katla and Rashad branches of Kordofanian. These noun-classification systems are somewhat analogous to grammatical gender in other languages, but there are often a fairly large number of classes (often 10 or more), and the classes may be male human/female human/animate/inanimate, or even completely gender-unrelated categories such as places, plants, abstracts, and groups of objects. For example, in Bantu, the Swahili language is called "Kiswahili," while the Swahili people are "Waswahili." Likewise, in Ubangian, the Zande language is called "Pazande," while the Zande people are called "Azande."

In the Bantu languages, where noun classification is particularly elaborate, it typically appears as prefixes, with verbs and adjectives marked according to the class of the noun they refer to. For example, in Swahili, "watu wazuri wataenda" is 'good "(zuri)" people "(tu)" will go "(ta-enda)"'.

The same Atlantic–Congo languages which have noun classes also have a set of verb applicatives and other verbal extensions, such as the reciprocal suffix "-na" (Swahili "penda" 'to love', "pendana" 'to love each other'; also applicative "pendea" 'to love for' and causative "pendeza" 'to please').

A subject–verb–object word order is quite widespread among today's Niger–Congo languages, but SOV is found in branches as divergent as Mande, Ijoid and Dogon. As a result, there has been quite some debate as to the basic word order of Niger–Congo.

Whereas Claudi (1993) argues for SVO on the basis of existing SVO > SOV grammaticalization paths, Gensler (1997) points out that the notion of 'basic word order' is problematic as it excludes structures with, for example, auxiliaries.
However, the structure SC-OC-VbStem (Subject concord, Object concord, Verb stem) found in the "verbal complex" of the SVO Bantu languages suggests an earlier SOV pattern (where the subject and object were at least represented by pronouns).

Noun phrases in most Niger–Congo languages are characteristically "noun-initial", with adjectives, numerals, demonstratives and genitives all coming after the noun. The major exceptions are found in the western areas where verb-final word order predominates and genitives precede nouns, though other modifiers still come afterwards. Degree words almost always follow adjectives, and except in verb-final languages adpositions are prepositional.

The verb-final languages of the Mende region have two quite unusual word order characteristics. Although verbs follow their direct objects, oblique adpositional phrases (like "in the house", "with timber") typically come after the verb, creating a SOVX word order. Also noteworthy in these languages is the prevalence of internally headed and correlative relative clauses, in both of which the head occurs "inside" the relative clause rather than the main clause.




</doc>
<doc id="21606" url="https://en.wikipedia.org/wiki?curid=21606" title="Napo River">
Napo River

The Napo River () is a tributary to the Amazon River that rises in Ecuador on the flanks of the east Andean volcanoes of Antisana, Sinchulawa and Cotopaxi.

The total length is . The river drains an area of . The mean annual discharge is per second.

Before it reaches the plains it receives a great number of small streams from impenetrable, saturated and much broken mountainous districts, where the dense and varied vegetation seems to fight for every piece of ground. This river is one of Ecuador's Physical Features. From the north it is joined by the Coca River, having its sources in the gorges of Cayambe volcano on the equator, and also a powerful river, the Aguarico having its headwaters between Cayambe and the Colombia frontier.

From the west, it receives a secondary tributary, the Curaray, from the Andean slopes, between Cotopaxi and the Tungurahua volcano. From its Coca branch to the mouth of the Curaray the Napo is full of snags and shelving sandbanks and throws out numerous canoes among jungle-tangled islands, which in the wet season are flooded, giving the river an immense width. From the Coca to the Amazon it runs through a forested plain where not a hill is visible from the river - its uniformly level banks being only interrupted by swamps and lagoons.

From the Amazon the Napo is navigable for river craft up to its Curaray branch, a distance of about , and perhaps a bit further; thence, by painful canoe navigation, its upper waters may be ascended as far as Santa Rosa, the usual point of embarkation for any venturesome traveller who descends from the Quito tableland. The Coca river may be penetrated as far up as its middle course, where it is jammed between two mountain walls, in a deep canyon, along which it dashes over high falls and numerous reefs. This is the stream made famous by the expedition of Gonzalo Pizarro.



</doc>
<doc id="21607" url="https://en.wikipedia.org/wiki?curid=21607" title="Nanay River">
Nanay River

The Nanay River is a river in northern Peru. It is a tributary of the Amazon River, merging into this river at the city of Iquitos. The lower part of the Nanay flows to the north and west of the city, while the Itaya River flows to the south and east. Other nearby settlements on the Nanay River include the villages of Santo Tomás, Padre Cocha, and Santa Clara. During periods when the river is low, the many beaches along the Nanay are popular destinations. The Nanay belongs entirely to the lowlands, and is very crooked, has a slow current and divides into many "canos" and strings of lagoons which flood the flat, low areas of country on either side. It is simply the drainage ditch of districts which are extensively overflowed in the rainy season. Captain Archibald Butt USN, ascended it , to near its source. A part of the Nanay River flows through the Allpahuayo-Mishana National Reserve.

The Nanay is a blackwater river and it has a high fish species richness, including several that are well-known from the aquarium industry. Some of these, notably green discus, are the result of accidental introductions that happened in the 1970s.


</doc>
<doc id="21609" url="https://en.wikipedia.org/wiki?curid=21609" title="Nine-ball">
Nine-ball

Nine-ball (sometimes written 9-ball) is a contemporary form of pool (pocket billiards), with historical beginnings rooted in the United States and traceable to the 1920s. The game may be played in social and recreational settings by any number of players (generally one-on-one) and subject to whatever rules are agreed upon beforehand, or in league and tournament settings in which the number of players and the rules are set by the sponsors. During much of its history, nine-ball has been known as a "money game" in both professional and recreational settings, but has since become established as a legitimate alternative to eight ball, straight pool and other major competition games.

In recent decades, nine-ball has become the dominant tournament game in professional pool, in the World Pool-Billiard Association, Women's Professional Billiard Association and United States Professional Poolplayers Association. Matches proceed quickly, suitable for the time constraints of television coverage, and the fast-paced games tend to keep the audience engaged.

The game is played on a pocket billiards table with six pockets and with ten balls. The , which is usually a solid shade of white (but may be spotted in some tournaments), is struck to hit the lowest numbered ball on the table (often referred to as the ); each of these balls is distinctly colored and numbered 1 through 9. The object of the game is to legally pocket the 9-ball.

In nine-ball, except when a push-out has been invoked, a legal shot consists of striking the cue ball into the lowest numbered object ball on the table and subsequently either pocketing an object ball, or driving any ball (including the cue ball) to any rail, otherwise the shot is a . Additional conditions apply for the break shot (see below). Object balls do not have to be pocketed in numerical order; Any ball may be pocketed at any time during the game, so long as the lowest-numbered ball is contacted first by the cue ball. Nine-ball is not a game. The 9-ball itself can be legally pocketed for a win at any turn in the game, intentionally or by chance, including the break shot. Conversely, a player could potentially pocket all of the object balls numbered one through eight during the course of the game and lose after the other player pockets only the nine-ball.

Players alternate s at the table, meaning play continues by one player until he or she misses, commits a foul, or pockets the 9 ball for the win. The penalty for a foul is that the player's inning ends and the opponent comes to the table with , able to place the cue ball anywhere on the table prior to shooting.

Nine-ball is a relatively fast-paced game and is rarely played by the rack. Instead, players normally play a match (or ) to a set number of games, often five, seven or nine. The first player to win that set number of games wins the match.

The object balls are placed in a diamond-shaped configuration, with the 1 ball positioned at the front (toward the position of the breaking player) on the , and the 9 ball placed in the center. The physical rack used to position the balls is typically triangle-shaped, usually wood or plastic, and capable of holding all fifteen object balls, although diamond-shaped racks that hold only nine balls are sometimes used. The placement of the remaining balls is generally considered to be random. However, in some tournaments, the ball being to the lesser player must be one of the two balls placed behind the 1 ball at the apex of the rack. An imaginary line drawn through the one-ball and back apex of the diamond should be parallel to the long rails of the table (perpendicular to the short rails). The placement of balls is expected to be precise, especially in league and tournament play; if any ball in the rack does not touch each adjacent ball, or if the rack is not "straight", or if the 1 ball is not resting precisely on the , the player assigned the break may demand a re-rack. "(See also "European alterations", below, for a recently devised "template-trained" racking system.)"

One person is chosen to shoot first, by the rack. Usually this is determined by flipping a coin, or by , especially in professional tournaments in the case of the latter, or it may be ruled by the authority in charge, the sponsor or the players themselves that the winner or loser of the previous game will always shoot first in the next rack. As with most pocket billiard games, the base of the cue ball must be behind the for the break shot. If the player who breaks fails to make a legal break, the opponent can either demand a re-rack and become the breaker, or continue to play as if it had been an ordinary foul, depending upon the rules of the event. If the breaker pockets a ball and commits no foul, it remains the breaker's turn. If the breaker pockets the 9 ball on the break (without fouling), this is an instant win. "(See also "European alterations", below, for recent moves to change the breaking rules.)"

After the break (regardless of its result), before the second shot of the game, the player at the table may call a "." A push-out can be called by the breaking player if he legally pocketed a ball on the break, or the non-breaking player if no ball was pocketed on the break. Calling a push-out for the shot after the break allows the player taking the shot to legally hit the cue ball in almost any fashion with no foul, with the exception that the cue ball must stay on the table and illegal shots such as double-hitting the cue ball or a "" would still be called a foul. Playing a push-out shot ends the player's inning and play passes to the opponent. The main purpose of the push-out shot is to alleviate an unlucky lie after the break, where it is difficult to make a legal shot. Unlike any other shot of the game, for a push-out shot, the cue ball is not required to contact any object ball and if an object ball is contacted, it is not required to be the lowest numbered ball. If the nine-ball is pocketed on a push-out shot it is ; however, any other pocketed object ball remains pocketed and is not spotted. A push-out should be called so that the opponent or referee hears the call, and it is customary for the opponent or referee to confirm that he heard the push-out call, so that there is no controversy surrounding the shot. After a push-out shot was called and played, the incoming player has the choice of accepting the table as it lies, or forcing the pushing-out player to take the next shot of the game (always the third shot of the game). Only one push-out is allowed per game, and it "must" be immediately after the break. "(See also "The rise of 'Texas express' rules", below, for the historical multi-push-out rule variation.)" If the pushing-out player has a particular type of shot he feels comfortable with, such as a jump shot, or two-rail bank shot, it may be strategical to leave that type of shot after the push-out. The ideal push-out shot leaves a lie that the opponent believes likely to be makeable, and will accept, but will fail to actually make, giving control of the table back to the pusher-out, and which the pusher-out is confident to make if the shot is passed back to him. Thus nine-ball players aim for a push-out that has about a 50/50 chance of being accepted or returned.

Winning a game occurs any time a player hits the lowest numbered ball first and pockets the 9-ball without committing a foul. When only the 9-ball is on the table, this is straightforward and obvious; however, when other balls remain on the table, any number of events can result in victory so long as the aforementioned requirements are met. For example, if the player is on the 5-ball, and hits it in such a way that the 5-ball then hits the 9-ball and pockets it, that would be a legal victory as this is a legal shot. Loss of game can occur if three successive fouls are committed and the fouling player is warned audibly or visually after the 2nd foul during the third inning.

In most rule systems, including those of the World Pool-Billiard Association and its national affiliates like the Billiard Congress of America, if a player fouls and pockets the 9 ball, or knocks the 9 ball off the table, the 9 ball is placed on the foot spot, and the incoming player receives .

The general rules the game is played under are fairly consistent and usually do not stray too far from the format set forth in the Billiard Congress of America (BCA) BCA World Standardized Rules for Nine Ball, which have merged with those of the World Pool-Billiard Association (WPA), to form the World Standardised Rules, although amateur league play may be governed by similar but slightly different rules promulgated by the American Poolplayers Association (APA) and other organizations.

For much of its history nine-ball rules allowed participants to "" multiple times during a game "(see "The push-out", above, for the modern push-out rules)", meaning any player could call a "push-out", and then hit the cue ball to any area on the table without being penalized by normal rules, such as failure to contact the lowest-numbered ball on the table. However, once a push-out was called and executed, the incoming player had the right to shoot or give the inning back to the opponent. If the player shooting the resulting shot fouled, the other player would have ball-in-hand; hence this manner of play was called the "two-foul" version. "One-foul" became popular in the 1970s, as play turned more aggressive for the early televised matches. This newer version of nine-ball awarded ball-in-hand on any cue ball foul. A now-standard rule variant, which started to sweep the sport of nine-ball in the mid-1980s, restricted the push-out option to once per game and only to the inning immediately following the break. This change profoundly affected the way the game was played. By about 1990 this new push-out rule had become ubiquitous and it and any additional rules appended to it were collectively referred to as "" rules, so called because of the supposed US state of origin and the speeding up of the game. Today, Texas express push-out rules dominate the way nine-ball is played and is the variant incorporated into the official rules maintained by the WPA and its affiliates like the BCA.

As of the 2000s, the rules have been somewhat in flux in certain contexts, especially in Europe. The European Pocket Billiard Federation (EPBF), BCA's WPA-affiliate counterpart in Europe, has done away with standardized racking techniques, and instead relies upon s in the cloth to position the balls, with no physical ball rack required; these indentations are carefully created using a "", such that the divots are slightly closer together than they would be expected to be, thus creating ball-on-ball pressure as the balls settle "partially" into the divot pattern, into which they cannot quite fit. This results in an especially tight rack, without any known possibility of cheating by carefully manipulating the ball positions while racking. This innovative racking technique was invented and patented as the Rack-M-Rite Racking Template by US professional player David Smith and his partner Dale Craig; it was first used in professional events on the Billiard Channel Tour in 2000 by tournament director David Vandenburgh. It later became the official rack of the EPBF Euro-Tour.

Another Euro-Tour innovation is a new requirement that the break shot be taken from a "", not unlike break shot zone used in snooker and blackball, consisting of the middle 50% of the . This change defeats the common break-from-the-side-rail technique for pocketing the 9 ball on the break and winning the game instantly. While 9 ball breaks are still possible, they are much more difficult under the new rule. This requirement was recently added to the Europe vs. US all-star team event, the Mosconi Cup, but has not otherwise been seen much by North Americans.

Yet a third EPBF change, used on the Euro-Tour for several years, is the "" rule, a stringent requirement that in order for a break shot to be legal, at least three object balls must either be pocketed or come up-table and "cross the ". Failure to do so constitutes a loss-of-turn (but "not" ) foul – even if two object balls are pocketed, a potential major windfall for the non-breaking player under these rules. More stringently yet, the requirements are independent – if a ball crosses the head string and is then pocketed, it counts as a pocketed ball but "not" a head string-crossing ball. This alteration (from WPA's requirement that "one" object ball be pocketed or "four driven to cushions") requires a powerful break shot, and was instituted to thwart a different form of break manipulation, the recently developed "nine-ball soft break", in which a languid break performed correctly, and given a tight rack (such as that produced by EPBF template-trained racking), is almost guaranteed to pocket a in a , perhaps even both wing balls, meanwhile the remaining balls stay mostly or entirely on the foot end of the table, giving the breaker an easy of short shots. By effectively banning the soft break, wins "on a silver platter" are much less likely. One problem with this "three above the line" break requirement is that very careful attention must be paid to whether or not particular balls cross the head string, such that even professional referees have had to resort to video playback, as happened several times at the Mosconi Cup, when this rule, too, was introduced in 2007 by the MC's organizers, Matchroom Sport, in an effort to make the event more competitive and interesting to audiences, and more even (the US has mostly dominated the annual event since its inception, and they did in fact lose the 2007 match).

Another Mosconi Cup rule change in 2007 called for racking such that the 9 ball rather than the 1 ball is on the (i.e. the racker rolls the balls forward farther; the balls remain in the same position in the rack), which further thwarts pocketing a wing ball easily.

While the modern folk game of three-ball bears no resemblance to nine-ball, the earliest-known version of three-ball was essentially nine-ball played with only three balls, racked in a triangle, in which the 3 ball was the . It is a quick game, and (due to the comparatively very high possibility of pocketing the 3 ball on the break) one with a more significant luck component than nine-ball and most other pool games.

Six-ball is essentially identical to nine-ball but with three fewer balls, and racked in a three-row triangle, with the 6 ball (or more often the 15 ball; "see below") as the "", placed in the center of the back row. According to Rudolph "Minnesota Fats" Wanderone, the game arose in early 20th century billiard halls that charged by the rack instead of by the hour, as nine-ball players had already paid for the 10 through 15 balls and did not want to waste them. This explanation of the game's origin may be particularly plausible because six-ball remains popular today as a diversion or practice round among nine-ball-playing players, using coin-operated tables that deliver a full set of fifteen balls.

Seven-ball is a similar game, the primary differences being there are only seven object balls, racked in a hexagon, and the game is won by pocketing the 7 ball. Seven-ball is ed with the 1 ball at the apex on the and the 7 ball (the ) in the center of the hexagon. This game is not particularly common, and is primarily known because of ESPN's "Sudden Death Seven-ball" which aired in the early 2000s. Though hardly necessary, specialized equipment for the game can be purchased, including a unique black-striped seven ball and a hexagonal rack.

Ten-ball is a more stringent variant of the game, using ten balls (racked in a triangle with the 10 ball, the in this case, in the center), in which all pocketed balls must be and in which the money ball "cannot" be pocketed on the break for an instant win. Due to its more challenging nature, and the fact that there is no publicly known technique for reliably pocketing specific object balls on the break shot, there have been suggestions among the professional circuit that ten-ball should replace nine-ball as the pro game of choice, especially since the rise of the nine-ball soft break, which is still legal in most international and non-European competition. Regardless of the future of the nine-ball versus ten-ball debate, there are already hotly contested professional ten-ball tournaments.

9-Ball Kiss (also carom nine) is played with the usual nine-ball rack, but breaking with the 1 ball, with the cue ball placed at the head of the rack (in the usual place of the 1 ball). As in regular 9-ball, play progresses from the lowest-numbered ball on the table; however a legal shot is made by shooting the object ball rather than the cue ball. The object ball must make first contact with the cue ball to count as a legal shot, the goal being to carom the object ball into a pocket (a kiss-shot) or into another ball. Once a legal shot has been performed, any ball then sunk counts for that player; the winner is the player to first pocket the 9-ball after a legal shot.

A gambling version of nine-ball played with group of people. The game is played like regular 9 ball with a player order. Heckling the shooters is allowed but no touching may occur. Money balls are the three, six, and nine ball. Values of the money balls are 1-1-2 where each player pays out to the person who sunk a money ball. If the nine ball is sunk, the payout is the nine ball value and what ever money balls are left on the table. If a money ball is pocketed and the cue ball is scratched, that player must pay out that value to each of his opponents. Winner of the game has break on the next game, player before winner racks the next game.




</doc>
<doc id="21611" url="https://en.wikipedia.org/wiki?curid=21611" title="New World Order">
New World Order

New World Order, new world order or The New World Order may refer to:






</doc>
<doc id="21615" url="https://en.wikipedia.org/wiki?curid=21615" title="Nostradamus">
Nostradamus

Michel de Nostredame (depending on the source, 14 or 21 December 1503 – 2 July 1566), usually Latinised as Nostradamus, was a French physician and reputed seer, who is best known for his book "Les Propheties", a collection of 942 poetic quatrains allegedly predicting future events. The book was first published in 1555 and has rarely been out of print since his death.

Nostradamus's family was originally Jewish, but had converted to Catholicism before he was born. He studied at the University of Avignon, but was forced to leave after just over a year when the university closed due to an outbreak of the plague. He worked as an apothecary for several years before entering the University of Montpellier, hoping to earn a doctorate, but was almost immediately expelled after his work as an apothecary (a manual trade forbidden by university statutes) was discovered. He first married in 1531, but his wife and two children were killed in 1534 during another plague outbreak. He fought alongside doctors against the plague before remarrying to Anne Ponsarde, who bore him six children. He wrote an almanac for 1550 and, as a result of its success, continued writing them for future years as he began working as an astrologer for various wealthy patrons. Catherine de' Medici became one of his foremost supporters. His "Les Propheties", published in 1555, relied heavily on historical and literary precedent and initially received mixed reception. He suffered from severe gout towards the end of his life, which eventually developed in edema. He died on 2 July 1566. Many popular authors have retold apocryphal legends about his life.

In the years since the publication of his "Les Propheties", Nostradamus has attracted a large number of supporters, who, along with much of the popular press, credit him with having accurately predicted many major world events. Most academic sources reject the notion that Nostradamus had any genuine supernatural prophetic abilities and maintain that the associations made between world events and Nostradamus's quatrains are the result of misinterpretations or mistranslations (sometimes deliberate). These academics argue that Nostradamus's predictions are characteristically vague, meaning they could be applied to virtually anything, and are useless for determining whether their author had any real prophetic powers. They also point out that English translations of his quatrains are almost always of extremely poor quality, based on later manuscripts, produced by authors with little knowledge of sixteenth-century French, and often deliberately mistranslated to make the prophecies fit whatever events the translator believed they were supposed to have predicted.

Nostradamus was born on either 14 or 21 December 1503 in Saint-Rémy-de-Provence, Provence, France, where his claimed birthplace still exists, and baptized Michel. He was one of at least nine children of notary Jaume (or Jacques) de Nostredame and Reynière, granddaughter of Pierre de Saint-Rémy who worked as a physician in Saint-Rémy. Jaume's family had originally been Jewish, but his father, Cresquas, a grain and money dealer based in Avignon, had converted to Catholicism around 1459-60, taking the Christian name "Pierre" and the surname "Nostredame" (Our Lady), the saint on whose day his conversion was solemnised. The earliest ancestor who can be identified on the paternal side is Astruge of Carcassonne, who died about 1420. Michel's known siblings included Delphine, Jean (c. 1507–77), Pierre, Hector, Louis, Bertrand, Jean II (born 1522) and Antoine (born 1523).
Little else is known about his childhood, although there is a persistent tradition that he was educated by his maternal great-grandfather Jean de St. Rémy — a tradition which is somewhat undermined by the fact that the latter disappears from the historical record after 1504, when the child was only one year old.

At the age of 15 Nostradamus entered the University of Avignon to study for his baccalaureate. After little more than a year (when he would have studied the regular trivium of grammar, rhetoric and logic rather than the later quadrivium of geometry, arithmetic, music, and astronomy/astrology), he was forced to leave Avignon when the university closed its doors during an outbreak of the plague. After leaving Avignon, Nostradamus, by his own account, traveled the countryside for eight years from 1521 researching herbal remedies. In 1529, after some years as an apothecary, he entered the University of Montpellier to study for a doctorate in medicine. He was expelled shortly afterwards by the student "procurator", Guillaume Rondelet, when it was discovered that he had been an apothecary, a "manual trade" expressly banned by the university statutes, and had been slandering doctors. The expulsion document, "BIU Montpellier, Register S 2 folio 87", still exists in the faculty library. However, some of his publishers and correspondents would later call him "Doctor". After his expulsion, Nostradamus continued working, presumably still as an apothecary, and became famous for creating a "rose pill" that purportedly protected against the plague.

In 1531 Nostradamus was invited by Jules-César Scaliger, a leading Renaissance scholar, to come to Agen. There he married a woman of uncertain name (possibly Henriette d'Encausse), who bore him two children. In 1534 his wife and children died, presumably from the plague. After their deaths, he continued to travel, passing through France and possibly Italy.

On his return in 1545, he assisted the prominent physician Louis Serre in his fight against a major plague outbreak in Marseille, and then tackled further outbreaks of disease on his own in Salon-de-Provence and in the regional capital, Aix-en-Provence. Finally, in 1547, he settled in Salon-de-Provence in the house which exists today, where he married a rich widow named Anne Ponsarde, with whom he had six children—three daughters and three sons. Between 1556 and 1567 he and his wife acquired a one-thirteenth share in a huge canal project, organised by Adam de Craponne, to create the Canal de Craponne to irrigate the largely waterless Salon-de-Provence and the nearby Désert de la Crau from the river Durance.

After another visit to Italy, Nostradamus began to move away from medicine and toward the "occult," although evidence suggests that he remained a Roman Catholic and was opposed to the Protestant Reformation. But it seems he could have dabbled in horoscopes, necromancy, scrying, and good luck charms such as the hawthorn rod. Following popular trends, he wrote an almanac for 1550, for the first time in print Latinising his name to Nostradamus. He was so encouraged by the almanac's success that he decided to write one or more annually. Taken together, they are known to have contained at least 6,338 prophecies, as well as at least eleven annual calendars, all of them starting on 1 January and not, as is sometimes supposed, in March. It was mainly in response to the almanacs that the nobility and other prominent persons from far away soon started asking for horoscopes and "psychic" advice from him, though he generally expected his clients to supply the birth charts on which these would be based, rather than calculating them himself as a professional astrologer would have done. When obliged to attempt this himself on the basis of the published tables of the day, he frequently made errors and failed to adjust the figures for his clients' place or time of birth.

He then began his project of writing a book of one thousand mainly French quatrains, which constitute the largely undated prophecies for which he is most famous today. Feeling vulnerable to opposition on religious grounds, however, he devised a method of obscuring his meaning by using "Virgilianised" syntax, word games and a mixture of other languages such as Greek, Italian, Latin, and Provençal. For technical reasons connected with their publication in three installments (the publisher of the third and last installment seems to have been unwilling to start it in the middle of a "Century," or book of 100 verses), the last fifty-eight quatrains of the seventh "Century" have not survived in any extant edition.

The quatrains, published in a book titled "Les Propheties" (The Prophecies), received a mixed reaction when they were published. Some people thought Nostradamus was a servant of evil, a fake, or insane, while many of the elite evidently thought otherwise. Catherine de' Medici, wife of King Henry II of France, was one of Nostradamus's greatest admirers. After reading his almanacs for 1555, which hinted at unnamed threats to the royal family, she summoned him to Paris to explain them and to draw up horoscopes for her children. At the time, he feared that he would be beheaded, but by the time of his death in 1566, Queen Catherine had made him Counselor and Physician-in-Ordinary to her son, the young King Charles IX of France.

Some accounts of Nostradamus's life state that he was afraid of being persecuted for heresy by the Inquisition, but neither prophecy nor astrology fell in this bracket, and he would have been in danger only if he had practised magic to support them. In 1538 he came into conflict with the Church in Agen after an Inquisitor visited the area looking for Anti-Catholic views. His brief imprisonment at Marignane in late 1561 was solely because he had violated a recent royal decree by publishing his 1562 almanac without the prior permission of a bishop.

By 1566, Nostradamus's gout, which had plagued him painfully for many years and made movement very difficult, turned into edema, or dropsy. In late June he summoned his lawyer to draw up an extensive will bequeathing his property plus 3,444 crowns (around $300,000 US today), minus a few debts, to his wife pending her remarriage, in trust for her sons pending their twenty-fifth birthdays and her daughters pending their marriages. This was followed by a much shorter codicil. On the evening of 1 July, he is alleged to have told his secretary Jean de Chavigny, "You will not find me alive at sunrise." The next morning he was reportedly found dead, lying on the floor next to his bed and a bench (Presage 141 [originally 152] "for November 1567", as posthumously edited by Chavigny to fit what happened). He was buried in the local Franciscan chapel in Salon (part of it now incorporated into the restaurant "La Brocherie") but re-interred during the French Revolution in the Collégiale Saint-Laurent, where his tomb remains to this day.

In "The Prophecies" Nostradamus compiled his collection of major, long-term predictions. The first installment was published in 1555 and contained 353 quatrains. The third edition, with three hundred new quatrains, was reportedly printed in 1558, but now survives as only part of the omnibus edition that was published after his death in 1568. This version contains one unrhymed and 941 rhymed quatrains, grouped into nine sets of 100 and one of 42, called "Centuries".

Given printing practices at the time (which included type-setting from dictation), no two editions turned out to be identical, and it is relatively rare to find even two copies that are exactly the same. Certainly there is no warrant for assuming—as would-be "code-breakers" are prone to do—that either the spellings or the punctuation of any edition are Nostradamus's originals.

The "Almanacs", by far the most popular of his works, were published annually from 1550 until his death. He often published two or three in a year, entitled either "Almanachs" (detailed predictions), "Prognostications" or "Presages" (more generalised predictions).

Nostradamus was not only a diviner, but a professional healer. It is known that he wrote at least two books on medical science. One was an extremely free translation (or rather a paraphrase) of "The Protreptic" of Galen ("Paraphrase de C. GALIEN, sus l'Exhortation de Menodote aux estudes des bonnes Artz, mesmement Medicine"), and in his so-called "Traité des fardemens" (basically a medical cookbook containing, once again, materials borrowed mainly from others), he included a description of the methods he used to treat the plague, including bloodletting, none of which apparently worked. The same book also describes the preparation of cosmetics.

A manuscript normally known as the "Orus Apollo" also exists in the Lyon municipal library, where upwards of 2,000 original documents relating to Nostradamus are stored under the aegis of Michel Chomarat. It is a purported translation of an ancient Greek work on Egyptian hieroglyphs based on later Latin versions, all of them unfortunately ignorant of the true meanings of the ancient Egyptian script, which was not correctly deciphered until Champollion in the 19th century.

Since his death, only the "Prophecies" have continued to be popular, but in this case they have been quite extraordinarily so. Over two hundred editions of them have appeared in that time, together with over 2,000 commentaries. Their persistence in popular culture seems to be partly because their vagueness and lack of dating make it easy to quote them selectively after every major dramatic event and retrospectively claim them as "hits".

Nostradamus claimed to base his published predictions on judicial astrology—the astrological 'judgment', or assessment, of the 'quality' (and thus potential) of events such as births, weddings, coronations etc.—but was heavily criticised by professional astrologers of the day such as Laurens Videl for incompetence and for assuming that "comparative horoscopy" (the comparison of future planetary configurations with those accompanying known past events) could actually predict what would happen in the future.

Research suggests that much of his prophetic work paraphrases collections of ancient end-of-the-world prophecies (mainly Bible-based), supplemented with references to historical events and anthologies of omen reports, and then projects those into the future in part with the aid of comparative horoscopy. Hence the many predictions involving ancient figures such as Sulla, Gaius Marius, Nero, and others, as well as his descriptions of "battles in the clouds" and "frogs falling from the sky." Astrology itself is mentioned only twice in Nostradamus's "Preface" and 41 times in the "Centuries" themselves, but more frequently in his dedicatory "Letter to King Henry II". In the last quatrain of his sixth "century" he specifically attacks astrologers.

His historical sources include easily identifiable passages from Livy, Suetonius' "The Twelve Caesars", Plutarch and other classical historians, as well as from medieval chroniclers such as Geoffrey of Villehardouin and Jean Froissart. Many of his astrological references are taken almost word for word from Richard Roussat's "" of 1549–50.

One of his major prophetic sources was evidently the "Mirabilis Liber" of 1522, which contained a range of prophecies by Pseudo-Methodius, the Tiburtine Sibyl, Joachim of Fiore, Savonarola and others (his "Preface" contains 24 biblical quotations, all but two in the order used by Savonarola). This book had enjoyed considerable success in the 1520s, when it went through half a dozen editions, but did not sustain its influence, perhaps owing to its mostly Latin text, Gothic script and many difficult abbreviations. Nostradamus was one of the first to re-paraphrase these prophecies in French, which may explain why they are credited to him. Modern views of plagiarism did not apply in the 16th century; authors frequently copied and paraphrased passages without acknowledgement, especially from the classics. The latest research suggests that he may in fact have used bibliomancy for this—randomly selecting a book of history or prophecy and taking his cue from whatever page it happened to fall open at.

Further material was gleaned from the "De honesta disciplina" of 1504 by Petrus Crinitus, which included extracts from Michael Psellos's "De daemonibus", and the "De Mysteriis Aegyptiorum" (Concerning the mysteries of Egypt…), a book on Chaldean and Assyrian magic by Iamblichus, a 4th-century Neo-Platonist. Latin versions of both had recently been published in Lyon, and extracts from both are paraphrased (in the second case almost literally) in his first two verses, the first of which is appended to this article. While it is true that Nostradamus claimed in 1555 to have burned all of the occult works in his library, no one can say exactly what books were destroyed in this fire.

Only in the 17th century did people start to notice his reliance on earlier, mainly classical sources.

Nostradamus's reliance on historical precedent is reflected in the fact that he explicitly rejected the label "prophet" (i.e. a person having prophetic powers of his own) on several occasions:

Although, my son, I have used the word "prophet", I would not attribute to myself a title of such lofty sublimity—"Preface to César", 1555
Not that I would attribute to myself either the name or the role of a prophet—"Preface to César", 1555
[S]ome of [the prophets] predicted great and marvelous things to come: [though] for me, I in no way attribute to myself such a title here.—"Letter to King Henry II", 1558
Not that I am foolish enough to claim to be a prophet.—Open letter to Privy Councillor (later Chancellor) Birague, 15 June 1566

Given this reliance on literary sources, it is unlikely that Nostradamus used any particular methods for entering a trance state, other than contemplation, meditation and incubation. His sole description of this process is contained in "letter 41" of his collected Latin correspondence. The popular legend that he attempted the ancient methods of flame gazing, water gazing or both simultaneously is based on a naive reading of his first two verses, which merely liken his efforts to those of the Delphic and Branchidic oracles. The first of these is reproduced at the bottom of this article and the second can be seen by visiting the relevant facsimile site (see External Links). In his dedication to King Henri II, Nostradamus describes "emptying my soul, mind and heart of all care, worry and unease through mental calm and tranquility", but his frequent references to the "bronze tripod" of the Delphic rite are usually preceded by the words "as though" (compare, once again, External References to the original texts).

Most of the quatrains deal with disasters, such as plagues, earthquakes, wars, floods, invasions, murders, droughts, and battles—all undated and based on foreshadowings by the "Mirabilis Liber". Some quatrains cover these disasters in overall terms; others concern a single person or small group of people. Some cover a single town, others several towns in several countries. A major, underlying theme is an impending invasion of Europe by Muslim forces from farther east and south headed by the expected Antichrist, directly reflecting the then-current Ottoman invasions and the earlier Saracen equivalents, as well as the prior expectations of the "Mirabilis Liber". All of this is presented in the context of the supposedly imminent end of the world—even though this is not in fact mentioned—a conviction that sparked numerous collections of end-time prophecies at the time, including an unpublished collection by Christopher Columbus. Views on Nostradamus have varied widely throughout history. Academic views such as those of Jacques Halbronn regard Nostradamus's "Prophecies" as antedated forgeries written by later hands with a political axe to grind.

Many of Nostradamus's supporters believe that his prophecies are genuine. Owing to the subjective nature of these interpretations, however, no two of them completely agree on exactly what Nostradamus predicted, whether for the past or for the future. Many supporters, however, do agree, for example, that he predicted the Great Fire of London, the French Revolution, and the rises of Napoleon and Adolf Hitler, both world wars, and the nuclear destruction of Hiroshima and Nagasaki. Popular authors frequently claim that he predicted whatever major event had just happened at the time of each book's publication, such as the Apollo moon landings in 1969, the Space Shuttle "Challenger" disaster in 1986, the death of Diana, Princess of Wales in 1997, and the September 11 attacks on the World Trade Center in 2001. This 'movable feast' aspect appears to be characteristic of the genre.

Possibly the first of these books to become popular in English was Henry C. Roberts' "The Complete Prophecies of Nostradamus" of 1947, reprinted at least seven times during the next forty years, which contained both transcriptions and translations, with brief commentaries. This was followed in 1961 (reprinted in 1982) by Edgar Leoni's "Nostradamus and His Prophecies". After that came Erika Cheetham's "The Prophecies of Nostradamus", incorporating a reprint of the posthumous 1568 edition, which was reprinted, revised and republished several times from 1973 onwards, latterly as "The Final Prophecies of Nostradamus". This served as the basis for the documentary "The Man Who Saw Tomorrow" and both did indeed mention possible generalised future attacks on New York (via nuclear weapons), though not specifically on the World Trade Center or on any particular date.

A two-part translation of Jean-Charles de Fontbrune's "Nostradamus: historien et prophète" was published in 1980, and John Hogue has published a number of books on Nostradamus from about 1987, including "Nostradamus and the Millennium: Predictions of the Future", "Nostradamus: The Complete Prophecies" (1999) and "Nostradamus: A Life and Myth" (2003). In 1992 one commentator who claimed to be able to contact Nostradamus under hypnosis even had him "interpreting" his own verse X.6 (a prediction specifically about floods in southern France around the city of Nîmes and people taking refuge in its "collosse", or Colosseum, a Roman amphitheatre now known as the "Arènes") as a prediction of an undated "attack on the Pentagon", despite the historical seer's clear statement in his dedicatory letter to King Henri II that his prophecies were about Europe, North Africa and part of Asia Minor.

With the exception of Roberts, these books and their many popular imitators were almost unanimous not merely about Nostradamus's powers of prophecy but also in inventing intriguing aspects of his purported biography: that he had been a descendant of the Israelite tribe of Issachar; he had been educated by his grandfathers, who had both been physicians to the court of Good King René of Provence; he had attended Montpellier University in 1525 to gain his first degree; after returning there in 1529, he had successfully taken his medical doctorate; he had gone on to lecture in the Medical Faculty there, until his views became too unpopular; he had supported the heliocentric view of the universe; he had travelled to the Habsburg Netherlands, where he had composed prophecies at the abbey of Orval; in the course of his travels, he had performed a variety of prodigies, including identifying future Pope, Sixtus V, who was then only a seminary monk. He is credited with having successfully cured the Plague at Aix-en-Provence and elsewhere; he had engaged in scrying, using either a magic mirror or a bowl of water; he had been joined by his secretary Chavigny at Easter 1554; having published the first installment of his "Propheties", he had been summoned by Queen Catherine de' Medici to Paris in 1556 to discuss with her his prophecy at quatrain I.35 that her husband King Henri II would be killed in a duel; he had examined the royal children at Blois; he had bequeathed to his son a "lost book" of his own prophetic paintings; he had been buried standing up; and he had been found, when dug up at the French Revolution, to be wearing a medallion bearing the exact date of his disinterment. This was first recorded by Samuel Pepys "as early as 1667", long before the French Revolution. Pepys records in his celebrated diary a legend that, before his death, Nostradamus made the townsfolk swear that his grave would never be disturbed; but that 60 years later his body was exhumed, whereupon a brass plaque was found on his chest correctly stating the date and time when his grave would be opened and cursing the exhumers.

In 2000, Li Hongzhi claimed that the 1999 prophecy at X.72 was a prediction of the Chinese Falun Gong persecution which began in July of 1999, leading to an increased interest in Nostradamus among Falun Gong members.

From the 1980s onwards, however, an academic reaction set in, especially in France. The publication in 1983 of Nostradamus's private correspondence and, during succeeding years, of the original editions of 1555 and 1557 discovered by Chomarat and Benazra, together with the unearthing of much original archival material revealed that much that was claimed about Nostradamus did not fit the documented facts. The academics revealed that "not one" of the claims just listed was backed up by any known contemporary documentary evidence. Most of them had evidently been based on unsourced rumours relayed as fact by much later commentators, such as Jaubert (1656), Guynaud (1693) and Bareste (1840), on modern misunderstandings of the 16th-century French texts, or on pure invention. Even the often-advanced suggestion that quatrain I.35 had successfully prophesied King Henri II's death did not actually appear in print for the first time until 1614, 55 years after the event.

Skeptics such as James Randi suggest that his reputation as a prophet is largely manufactured by modern-day supporters who fit his words to events that have either already occurred or are so imminent as to be inevitable, a process sometimes known as "retroactive clairvoyance" (postdiction). No Nostradamus quatrain is known to have been interpreted as predicting a specific event before it occurred, other than in vague, general terms that could equally apply to any number of other events. This even applies to quatrains that contain specific dates, such as III.77, which predicts "in 1727, in October, the king of Persia [shall be] captured by those of Egypt"—a prophecy that has, as ever, been interpreted retrospectively in the light of later events, in this case as though it presaged the known "peace treaty" between the Ottoman Empire and Persia of that year; Egypt was also an important Ottoman territory at this time. Similarly, Nostradamus's notorious '1999' prophecy at X.72 (see Nostradamus in popular culture) describes no event that commentators have succeeded in identifying either before or since, other than by dint of twisting the words to fit whichever of the many contradictory happenings they are keen to claim as 'hits'. Moreover, no quatrain suggests, as is often claimed by books and films on the alleged Mayan Prophecy, that the world would end in December 2012. In his preface to the "Prophecies", Nostradamus himself stated that his prophecies extend 'from now to the year 3797'—an extraordinary date which, given that the preface was written in 1555, may have more than a little to do with the fact that 2242 (3797 − 1555) had recently been proposed by his major astrological source Richard Roussat as a possible date for the end of the world.

Additionally, scholars have pointed out that almost all English translations of Nostradamus's quatrains are of extremely poor quality, seem to display little or no knowledge of 16th-century French, are tendentious, and are sometimes intentionally altered in order to make them fit whatever events the translator believed they were supposed to refer (or vice versa). None of them were based on the original editions: Roberts had based his writings on that of 1672, Cheetham and Hogue on the posthumous edition of 1568. Even Leoni accepted on page 115 that he had never seen an original edition, and on earlier pages he indicated that much of his biographical material was unsourced.

None of this research and criticism was originally known to most of the English-language commentators, by dint of the dates when they were writing and, to some extent, the language in which it was written. Hogue was in a position to take advantage of it, but it was only in 2003 that he accepted that some of his earlier biographical material had in fact been apocryphal. Meanwhile, some of the more recent sources listed (Lemesurier, Gruber, Wilson) have been particularly scathing about later attempts by some lesser-known authors and Internet enthusiasts to extract alleged hidden meanings from the texts, whether with the aid of anagrams, numerical codes, graphs or otherwise.

The prophecies retold and expanded by Nostradamus figured largely in popular culture in the 20th and 21st centuries. As well as being the subject of hundreds of books (both fiction and nonfiction), Nostradamus's life has been depicted in several films and videos, and his life and writings continue to be a subject of media interest.

There have also been several well-known Internet hoaxes, where quatrains in the style of Nostradamus have been circulated by e-mail as the real thing. The best-known examples concern the collapse of the World Trade Center in the 11 September attacks.

With the arrival of the year 2012, Nostradamus's prophecies started to be co-opted (especially by the History Channel) as evidence suggesting that the end of the world was imminent, notwithstanding the fact that his book never mentions the end of the world, let alone the year 2012.





</doc>
<doc id="21619" url="https://en.wikipedia.org/wiki?curid=21619" title="List of multi-level marketing companies">
List of multi-level marketing companies

This is a list of companies which use multi-level marketing (also known as network marketing, direct selling, referral marketing, and pyramid selling) for most of their sales.



</doc>
<doc id="21620" url="https://en.wikipedia.org/wiki?curid=21620" title="Noah Webster">
Noah Webster

Noah Webster Jr. (October 16, 1758 – May 28, 1843) was an American lexicographer, textbook pioneer, English-language spelling reformer, political writer, editor, and prolific author. He has been called the "Father of American Scholarship and Education". His blue-backed speller books taught five generations of American children how to spell and read. Webster's name has become synonymous with "dictionary" in the United States, especially the modern Merriam-Webster dictionary that was first published in 1828 as "An American Dictionary of the English Language".

Born in West Hartford, Connecticut, Webster graduated from Yale College in 1778. He passed the bar examination after studying law under Oliver Ellsworth and others, but was unable to find work as a lawyer. He found some financial success by opening a private school and writing a series of educational books, including the "Blue-Backed Speller." A strong supporter of the American Revolution and the ratification of the United States Constitution, Webster hoped his educational works would provide an intellectual foundation for American nationalism.

In 1793, Alexander Hamilton recruited Webster to move to New York City and become an editor for a Federalist Party newspaper. He became a prolific author, publishing newspaper articles, political essays, and textbooks. He returned to Connecticut in 1798 and served in the Connecticut House of Representatives. Webster founded the Connecticut Society for the Abolition of Slavery in 1791 but later became somewhat disillusioned with the abolitionist movement.

In 1806, Webster published his first dictionary, "A Compendious Dictionary of the English Language". The following year, he started working on an expanded and comprehensive dictionary, finally publishing it in 1828. He was very influential in popularizing certain spellings in the United States. He was also influential in establishing the Copyright Act of 1831, the first major statutory revision of U.S. copyright law. While working on a second volume of his dictionary, Webster died in 1843, and the rights to the dictionary were acquired by George and Charles Merriam.

Webster was born in the Western Division of Hartford (which became West Hartford, Connecticut) to an established family. His father Noah Sr. (1722–1813) was a descendant of Connecticut Governor John Webster; his mother Mercy (Steele) Webster (1727–1794) was a descendant of Governor William Bradford of Plymouth Colony. His father was primarily a farmer, though he was also deacon of the local Congregational church, captain of the town's militia, and a founder of a local book society (a precursor to the public library). After American independence, he was appointed a justice of the peace.

Webster's father never attended college, but he was intellectually curious and prized education. Webster's mother spent long hours teaching her children spelling, mathematics, and music. At age six, Webster began attending a dilapidated one-room primary school built by West Hartford's Ecclesiastical Society. Years later, he described the teachers as the "dregs of humanity" and complained that the instruction was mainly in religion. Webster's experiences there motivated him to improve the educational experience of future generations.

At age fourteen, his church pastor began tutoring him in Latin and Greek to prepare him for entering Yale College. Webster enrolled at Yale just before his 16th birthday, studying during his senior year with Ezra Stiles, Yale's president. His four years at Yale overlapped the American Revolutionary War and, because of food shortages and threatened British invasions, many of his classes had to be held in other towns. Webster served in the Connecticut Militia. His father had mortgaged the farm to send Webster to Yale, but he was now on his own and had nothing more to do with his family.

Webster lacked career plans after graduating from Yale in 1778, later writing that a liberal arts education "disqualifies a man for business". He taught school briefly in Glastonbury, but the working conditions were harsh and the pay low. He quit to study law. While studying law under future U.S. Supreme Court Chief Justice Oliver Ellsworth, Webster also taught full-time in Hartford—which was grueling, and ultimately impossible to continue. He quit his legal studies for a year and lapsed into a depression; he then found another practicing attorney to tutor him, and completed his studies and passed the bar examination in 1781. As the Revolutionary War was still going on, he could not find work as a lawyer. He received a master's degree from Yale by giving an oral dissertation to the Yale graduating class. Later that year, he opened a small private school in western Connecticut that was a success. Nevertheless, he soon closed it and left town, probably because of a failed romance. Turning to literary work as a way to overcome his losses and channel his ambitions, he began writing a series of well-received articles for a prominent New England newspaper justifying and praising the American Revolution and arguing that the separation from Britain was permanent. He then founded a private school catering to wealthy parents in Goshen, New York and, by 1785, he had written his speller, a grammar book and a reader for elementary schools. Proceeds from continuing sales of the popular blue-backed speller enabled Webster to spend many years working on his famous dictionary.

Webster was by nature a revolutionary, seeking American independence from the cultural thralldom to Britain. To replace it, he sought to create a utopian America, cleansed of luxury and ostentation and the champion of freedom. By 1781, Webster had an expansive view of the new nation. American nationalism was superior to Europe because American values were superior, he claimed.

Webster dedicated his "Speller" and "Dictionary" to providing an intellectual foundation for American nationalism. From 1787 to 1789, Webster was an outspoken supporter of the new Constitution. In October 1787, he wrote a pamphlet entitled "An Examination into the Leading Principles of the Federal Constitution Proposed by the Late Convention Held at Philadelphia," published under the pen name "A Citizen of America." The pamphlet was influential, particularly outside New York State.

In terms of political theory, he de-emphasized virtue (a core value of republicanism) and emphasized widespread ownership of property (a key element of Federalism). He was one of the few Americans who paid much attention to French theorist Jean-Jacques Rousseau. It was not Rousseau's politics but his ideas on pedagogy in "Emile" (1762) that influenced Webster in adjusting his "Speller" to the stages of a child's development.

Webster married well and had joined the elite in Hartford but did not have much money. In 1793, Alexander Hamilton lent him $1,500 to move to New York City to edit the leading Federalist Party newspaper. In December, he founded New York's first daily newspaper "American Minerva" (later known as the "Commercial Advertiser"), which he edited for four years, writing the equivalent of 20 volumes of articles and editorials. He also published the semi-weekly publication "The Herald, A Gazette for the country" (later known as "The New York Spectator").

As a Federalist spokesman, he defended the administrations of George Washington and John Adams, especially their policy of neutrality between Britain and France, and he especially criticized the excesses of the French Revolution and its Reign of Terror. When French ambassador Citizen Genêt set up a network of pro-Jacobin "Democratic-Republican Societies" that entered American politics and attacked President Washington, he condemned them. He later defended Jay's Treaty between the United States and Britain. As a result, he was repeatedly denounced by the Jeffersonian Republicans as "a pusillanimous, half-begotten, self-dubbed patriot," "an incurable lunatic," and "a deceitful newsmonger ... Pedagogue and Quack." 

Webster was elected a Fellow of the American Academy of Arts and Sciences in 1799.

For decades, he was one of the most prolific authors in the new nation, publishing textbooks, political essays, a report on infectious diseases, and newspaper articles for his Federalist party. He wrote so much that a modern bibliography of his published works required 655 pages. He moved back to New Haven in 1798; he was elected as a Federalist to the Connecticut House of Representatives in 1800 and 1802–1807.

The Copyright Act of 1831 was the first major statutory revision of U.S. copyright law, a result of intensive lobbying by Noah Webster and his agents in Congress. Webster also played a critical role lobbying individual states throughout the country during the 1780s to pass the first American copyright laws, which were expected to have distinct nationalistic implications for the infant nation.

As a teacher, he had come to dislike American elementary schools. They could be overcrowded, with up to seventy children of all ages crammed into one-room schoolhouses. They had poor, underpaid staff, no desks, and unsatisfactory textbooks that came from England. Webster thought that Americans should learn from American books, so he began writing the three volume compendium "A Grammatical Institute of the English Language". The work consisted of a speller (published in 1783), a grammar (published in 1784), and a reader (published in 1785). His goal was to provide a uniquely American approach to training children. His most important improvement, he claimed, was to rescue "our native tongue" from "the clamour of pedantry" that surrounded English grammar and pronunciation. He complained that the English language had been corrupted by the British aristocracy, which set its own standard for proper spelling and pronunciation. Webster rejected the notion that the study of Greek and Latin must precede the study of English grammar. The appropriate standard for the American language, argued Webster, was "the same republican principles as American civil and ecclesiastical constitutions." This meant that the people-at-large must control the language; popular sovereignty in government must be accompanied by popular usage in language.

The "Speller" was arranged so that it could be easily taught to students, and it progressed by age. From his own experiences as a teacher, Webster thought that the "Speller" should be simple and gave an orderly presentation of words and the rules of spelling and pronunciation. He believed that students learned most readily when he broke a complex problem into its component parts and had each pupil master one part before moving to the next. Ellis argues that Webster anticipated some of the insights currently associated with Jean Piaget's theory of cognitive development. Webster said that children pass through distinctive learning phases in which they master increasingly complex or abstract tasks. Therefore, teachers must not try to teach a three-year-old how to read; they could not do it until age five. He organized his speller accordingly, beginning with the alphabet and moving systematically through the different sounds of vowels and consonants, then syllables, then simple words, then more complex words, then sentences.

The speller was originally titled "The First Part of the Grammatical Institute of the English Language". Over the course of 385 editions in his lifetime, the title was changed in 1786 to "The American Spelling Book", and again in 1829 to "The Elementary Spelling Book". Most people called it the "Blue-Backed Speller" because of its blue cover and, for the next one hundred years, Webster's book taught children how to read, spell, and pronounce words. It was the most popular American book of its time; by 1837, it had sold 15 million copies, and some 60 million by 1890—reaching the majority of young students in the nation's first century. Its royalty of a half-cent per copy was enough to sustain Webster in his other endeavors. It also helped create the popular contests known as spelling bees.

As time went on, Webster changed the spellings in the book to more phonetic ones. Most of them already existed as alternative spellings. He chose spellings such as "defense", "color", and "traveler", and changed the "re" to "er" in words such as "center". He also changed "tongue" to the older spelling "tung", but this did not catch on.

Part three of his "Grammatical Institute" (1785) was a reader designed to uplift the mind and "diffuse the principles of virtue and patriotism."

"In the choice of pieces," he explained, "I have not been inattentive to the political interests of America. Several of those masterly addresses of Congress, written at the commencement of the late Revolution, contain such noble, just, and independent sentiments of liberty and patriotism, that I cannot help wishing to transfuse them into the breasts of the rising generation."

Students received the usual quota of Plutarch, Shakespeare, Swift, and Addison, as well as such Americans as Joel Barlow's "Vision of Columbus", Timothy Dwight's "Conquest of Canaan", and John Trumbull's poem "M'Fingal." He included excerpts from Tom Paine's "The Crisis" and an essay by Thomas Day calling for the abolition of slavery in accord with the Declaration of Independence.

Webster's "Speller" was entirely secular by design. It ended with two pages of important dates in American history, beginning with Columbus's discovery of America in 1492 and ending with the battle of Yorktown in 1781. There was no mention of God, the Bible, or sacred events. "Let sacred things be appropriated for sacred purposes," wrote Webster. As Ellis explains, "Webster began to construct a secular catechism to the nation-state. Here was the first appearance of 'civics' in American schoolbooks. In this sense, Webster's speller becoming what was to be the secular successor to "The New England Primer" with its explicitly biblical injunctions." Later in life, Webster became intensely religious and added religious themes. However, after 1840, Webster's books lost market share to the "McGuffey Eclectic Readers" of William Holmes McGuffey, which sold over 120 million copies.

Vincent P. Bynack (1984) examines Webster in relation to his commitment to the idea of a unified American national culture that would stave off the decline of republican virtues and solidarity. Webster acquired his perspective on language from such theorists as Maupertuis, Michaelis, and Herder. There he found the belief that a nation's linguistic forms and the thoughts correlated with them shaped individuals' behavior. Thus, the etymological clarification and reform of American English promised to improve citizens' manners and thereby preserve republican purity and social stability. This presupposition animated Webster's "Speller" and "Grammar".

In 1806, Webster published his first dictionary, . In 1807 Webster began compiling an expanded and fully comprehensive dictionary, "An American Dictionary of the English Language;" it took twenty-six years to complete. To evaluate the etymology of words, Webster learned twenty-eight languages, including Old English (Anglo-Saxon), Gothic, German, Greek, Latin, Italian, Spanish, French, Dutch, Welsh, Russian, Hebrew, Aramaic, Persian, Arabic, and Sanskrit. Webster hoped to standardize American speech, since Americans in different parts of the country used different languages. They also spelled, pronounced, and used English words differently.

Webster completed his dictionary during his year abroad in January 1825 in a boarding house in Cambridge, England. His book contained seventy thousand words, of which twelve thousand had never appeared in a published dictionary before. As a spelling reformer, Webster preferred spellings that matched pronunciation better. In "A Companion to the American Revolution" (2008), John Algeo notes: "It is often assumed that characteristically American spellings were invented by Noah Webster. He was very influential in popularizing certain spellings in America, but he did not originate them. Rather […] he chose already existing options such as "center, color" and "check" on such grounds as simplicity, analogy or etymology." He also added American words, like "skunk" and "squash", that did not appear in British dictionaries. At the age of seventy, Webster published his dictionary in 1828, registering the copyright on April 14.

Though it now has an honored place in the history of American English, Webster's first dictionary only sold 2,500 copies. He was forced to mortgage his home to develop a second edition, and his life from then on was plagued with debt.

In 1840, the second edition was published in two volumes. On May 28, 1843, a few days after he had completed revising an appendix to the second edition, and with much of his efforts with the dictionary still unrecognized, Noah Webster died. The rights to his dictionary were acquired by George and Charles Merriam in 1843 from Webster's estate and all contemporary Merriam-Webster dictionaries trace their lineage to that of Webster, although many others have adopted his name, attempting to share in the prestige.

Lepore (2008) demonstrates Webster's paradoxical ideas about language and politics and shows why Webster's endeavors were at first so poorly received. Culturally conservative Federalists denounced the work as radical—too inclusive in its lexicon and even bordering on vulgar. Meanwhile, Webster's old foes the Republicans attacked the man, labeling him mad for such an undertaking.

Scholars have long seen Webster's 1844 dictionary to be an important resource for reading poet Emily Dickinson's life and work; she once commented that the "Lexicon" was her "only companion" for years. One biographer said, "The dictionary was no mere reference book to her; she read it as a priest his breviary—over and over, page by page, with utter absorption."

Nathan Austin has explored the intersection of lexicographical and poetic practices in American literature, and attempts to map out a "lexical poetics" using Webster's definitions as his base. Poets mined his dictionaries, often drawing upon the lexicography in order to express word play. Austin explicates key definitions from both the "Compendious" (1806) and "American" (1828) dictionaries, and finds a range of themes such as the politics of "American" versus "British" English and issues of national identity and independent culture. Austin argues that Webster's dictionaries helped redefine Americanism in an era of highly flexible cultural identity. Webster himself saw the dictionaries as a nationalizing device to separate America from Britain, calling his project a "federal language", with competing forces towards regularity on the one hand and innovation on the other. Austin suggests that the contradictions of Webster's lexicography were part of a larger play between liberty and order within American intellectual discourse, with some pulled toward Europe and the past, and others pulled toward America and the new future.

In 1850 Blackie and Son in Glasgow published the first general dictionary of English that relied heavily upon pictorial illustrations integrated with the text. Its "The Imperial Dictionary, English, Technological, and Scientific, Adapted to the Present State of Literature, Science, and Art; On the Basis of Webster's English Dictionary" used Webster's for most of their text, adding some additional technical words that went with illustrations of machinery.

Webster in early life was something of a freethinker, but in 1808 he became a convert to Calvinistic orthodoxy, and thereafter became a devout Congregationalist who preached the need to Christianize the nation. Webster grew increasingly authoritarian and elitist, fighting against the prevailing grain of Jacksonian Democracy. Webster viewed language as a tool to control unruly thoughts. His "American Dictionary" emphasized the virtues of social control over human passions and individualism, submission to authority, and fear of God; they were necessary for the maintenance of the American social order. As he grew older, Webster's attitudes changed from those of an optimistic revolutionary in the 1780s to those of a pessimistic critic of man and society by the 1820s.

His 1828 "American Dictionary" contained the greatest number of Biblical definitions given in any reference volume. Webster considered education "useless without the Bible." Webster released his own edition of the Bible in 1833, called the Common Version. He used the King James Version (KJV) as a base and consulted the Hebrew and Greek along with various other versions and commentaries. Webster molded the KJV to correct grammar, replaced words that were no longer used, and did away with words and phrases that could be seen as offensive.

In 1834, he published "Value of the Bible and Excellence of the Christian Religion", an apologetic book in defense of the Bible and Christianity itself.

Webster helped found the Connecticut Society for the Abolition of Slavery in 1791, but by the 1830s rejected the new tone among abolitionists that emphasized Americans who tolerated slavery were themselves sinners. In 1837, Webster warned his daughter Eliza about her fervent support of the abolitionist cause. Webster wrote, "slavery is a great sin and a general calamity—but it is not "our" sin, though it may prove to be a terrible calamity to us in the north. But we cannot legally interfere with the South on this subject." He added, "To come north to preach and thus disturb "our" peace, when we can legally do nothing to effect this object, is, in my view, highly criminal and the preachers of abolitionism deserve the penitentiary."

Noah Webster married Rebecca Greenleaf (1766–1847) on October 26, 1789, New Haven, Connecticut. They had eight children:

He moved to Amherst, Massachusetts in 1812, where he helped to found Amherst College. In 1822 the family moved back to New Haven, where Webster was awarded an honorary degree from Yale the following year. He is buried in New Haven's Grove Street Cemetery.







</doc>
<doc id="21626" url="https://en.wikipedia.org/wiki?curid=21626" title="Near-Earth object">
Near-Earth object

A near-Earth object (NEO) is any small Solar System body whose orbit can bring it into proximity with Earth. By definition, a Solar System body is a NEO if its closest approach to the Sun (perihelion) is less than 1.3 astronomical units (AU). If a NEO's orbit crosses the Earth's and the object is larger than across, it is considered a potentially hazardous object (PHO). Most known PHOs and NEOs are asteroids.

There are more than seventeen thousand near-Earth asteroids (NEAs), more than one hundred short-period near-Earth comets (NECs), and a number of solar-orbiting spacecraft and meteoroids large enough to be tracked in space before striking the Earth. It is now widely accepted that collisions in the past have had a significant role in shaping the geological and biological history of the Earth. NEOs have become of increased interest since the 1980s because of greater awareness of the potential danger some of the asteroids or comets pose. When impacting the Earth, asteroids as small as 20 m cause sufficiently strong shock waves and heat to damage the local environment and populations. Larger asteroids penetrate the atmosphere to the surface of the Earth producing craters and tsunamis if water bodies are hit. It is possible to deflect asteroids and methods of mitigation are being researched.

Based on the orbit calculations of NEOs, the risk of future impacts is assessed on two scales, the Torino scale and the more complex Palermo scale, both of which rate a risk of any significance with values above 0. Some NEOs have had positive initial Torino or Palermo scale ratings after their discovery, but , more precise calculations based on subsequent observations led to a reduction of the rating to or below 0 in all cases.

Since 1998, the United States, the European Union, and other nations are scanning for NEOs in an effort called Spaceguard. The initial aim of cataloging at least 90% of NEOs that are at least 1 kilometer (km) wide—about 0.62 miles (mi), which would cause a global catastrophe in case on an impact on Earth—had been met by 2011. In later years, the survey effort has been expanded to objects as small as about across, which still have potential for large-scale, though not global, damage.

Due to their Earth-like orbits and low surface gravity, NEOs are easy targets for spacecraft. , five near-Earth comets and three near-Earth asteroids have been visited by spacecraft, and probes are en route to two more NEAs. Plans to mine NEAs commercially have been drafted by private companies.

Near-Earth objects (NEOs) are small Solar System bodies with orbits around the Sun that lie partly between 0.983 and 1.3 astronomical units (AU; Sun–Earth distance) away from the Sun. Thus, NEOs aren't objects that are currently near the Earth, but objects that can potentially approach the Earth at a relatively close distance.

When a NEO is detected, like all other small Solar System bodies, it is submitted to the International Astronomical Union's (IAU's) Minor Planet Center (MPC) for cataloging. MPC maintains separate lists of confirmed NEOs and potential NEOs. Some NEOs orbits intersect that of Earth's so they pose a collision danger. These are considered potentially hazardous objects (PHOs). For the asteroids among PHOs, the potentially hazardous asteroids (PHAs), MPC maintains a separate list. NEOs are also catalogued by two separate units of the Jet Propulsion Laboratory (JPL) of the National Aeronautics and Space Administration (NASA): the Center for Near Earth Object Studies (CNEOS) and the Solar System Dynamics Group.

PHAs are currently defined based on parameters that measure the object's potential to make threatening close approaches to the Earth. Mostly objects with an Earth minimum orbit intersection distance (MOID) of 0.05 AU or less and an absolute magnitude of 22.0 or brighter (a rough indicator of large size) are considered PHAs. Objects that cannot approach closer to the Earth (i.e. MOID) than , or are smaller than about in diameter (i.e. H = 22.0 with assumed albedo of 14%), are not considered PHAs. NASA's catalog for near-Earth objects also includes the approach distances of asteroids and comets measured in lunar distances.

The first near-Earth objects to be observed by humans were comets. Their extraterrestrial nature was recognised and confirmed only after Tycho Brahe tried to measure the distance of a comet in 1577; and the periodicity of comets was recognised in 1705, when Edmond Halley first published his orbit calculations for the returning object now known as Halley's Comet. The 1758–1759 return of Halley's Comet was the first comet appearance predicted in advance.

The first near-Earth asteroid to be discovered was 433 Eros in 1898. The asteroid was subject to several observation campaigns, because it allowed for a precise determination of the distance of the Earth from the Sun.

In 1937, asteroid 69230 Hermes was discovered when it passed the Earth at twice the distance of the Moon. Hermes was considered a threat because it was lost after its discovery, thus its orbit and potential for collision with Earth were not known precisely. Hermes was re-discovered in 2003, and is now not considered an immediate threat.

On June 14, 1968, the 1.4 km diameter asteroid 1566 Icarus passed Earth at a distance of , or 16 times the distance of the Moon. During this approach, Icarus became the first minor planet to be observed using radar, with measurements obtained at the Haystack Observatory and the Goldstone Tracking Station. This was the first close approach predicted years in advance (Icarus had been discovered in 1949), and also earned significant public attention, due to alarmist news reports. A year before the approach, MIT students launched Project Icarus, devising a plan to deflect the asteroid with rockets in case it was found to be on a collision course with Earth. Project Icarus received wide media coverage, and inspired the 1979 disaster movie "Meteor", in which the US and the USSR join forces to blow up an Earth-bound fragment of an asteroid hit by a comet.

On March 23, 1989, the diameter Apollo asteroid 4581 Asclepius (1989 FC) missed the Earth by . If the asteroid had impacted it would have created the largest explosion in recorded history, equivalent to 20,000 megatons of TNT. It attracted widespread attention because it was discovered only after the closest approach.

In March 1998, early orbit calculations for recently discovered asteroid showed a potential 2028 close approach from the Earth, well within the orbit of the Moon, but with a large error margin allowing for a direct hit. Further data allowed a revision of the 2028 approach distance to , with no chance of collision. By that time, inaccurate reports of a potential impact caused a media storm.
From the late 1990s, a typical frame of reference for looking at NEOs has been through the scientific concept of risk. In this frame, the risk that any near-Earth object poses is typically seen through a lens that is a function of both the culture and the technology of human society. Through history, humans have associated NEOs with changing risks, based on religious, philosophical or scientific views, as well as humanity's technological or economical capability to deal with such risks. Thus, NEOs have been seen as omens of natural disasters or wars, harmless spectacle in an unchanging universe, the source of era-changing cataclysms or potentially poisonous fumes (during Earth's passage through the tail of Halley's Comet in 1910), and finally as the causes of crater-forming impacts that can even cause extinction.

For near-Earth comets, the potential of catastrophic impacts was recognised as soon as the first orbit calculations provided an understanding of their orbits: in 1694, Edmond Halley presented a theory that Noah's flood in the Bible was caused by a comet impact. Human perception of near-Earth asteroids as benign objects of fascination or killer objects with high risk to human society has ebbed and flowed in the short period of human history that NEAs have been scientifically observed. The modern awareness of the threat of impacts that create craters much bigger than the impacting bodies and have indirect effects on an even wider area arose in the scientific community in the 1980s, after the confirmation of a theory that the Cretaceous–Paleogene extinction event (in which dinosaurs died out) 65 million years ago was caused by a large asteroid impact.

The awareness of the wider public of the impact risk rose after the observation of the impact of the fragments of Comet Shoemaker–Levy 9 into Jupiter in July 1994. In 1998, the movies "Deep Impact" and "Armageddon" popularised the notion that near-Earth objects could cause catastrophic impacts. Also at this time, a conspiracy theory arose about the supposed 2003 impact of the fictitious planet Nibiru, which persisted on the internet as the predicted impact date was moved to 2012 and then 2017.

There are two schemes for the scientific classification of impact hazards from NEOs:

On both scales, risks of any concern are indicated by values above zero.

The annual background frequency of impacts used in the Palermo scale is the estimated long-term average frequency of impacts, as a function of impact energy. The Palermo scale uses one particular estimate for this function, but the exact function is known with great uncertainty, and other estimates have been published at the time of the scale's creation.

NASA maintains an automated system to evaluate the threat from known NEOs over the next 100 years, which generates the continuously updated Sentry Risk Table. All or nearly all of the objects are highly likely to eventually drop off the list as more observations come in, reducing the uncertainties and enabling more accurate orbital predictions.

In March 2002, became the first asteroid with a positive rating on the Torino Scale, with an approximately one in 9,300 chance on a potential impact in 2049. After more observations, the estimated risk was reduced to zero, and the asteroid was removed from the Sentry Risk Table in April 2002. It is now known that in the next two centuries, will pass closest to the Earth at a safe distance of on August 31, 2080.
Asteroid was lost after its 1950 discovery, since its observations over just 17 days were insufficient to determine its orbit, and then rediscovered on December 31, 2000. It has a diameter of about a kilometer (0.6 miles). It was also observed by radar during its 2001 close approach, allowing for much more precise orbit calculations. Although this asteroid will not strike for at least 800 years and thus has no Torino scale rating, it was added to the Sentry list in April 2002 because it was the first object with a Palermo scale value greater than zero. The then-calculated 1 in 300 maximum chance of impact and +0.17 Palermo scale value was roughly 50% greater than the background risk of impact by all similarly large objects until 2880. Uncertainties in the orbit calculations were further reduced using radar observations in 2012, and decreased the odds of an impact. Taking all radar and optical observations up until 2015 into account, the probability of impact is, , assessed at 1 in 8,300. The corresponding Palermo scale value of −1.42 is still the highest for all objects on the Sentry List Table. , only one more object () has a Palermo scale value above −2 for a single impact date.

On December 24, 2004, asteroid 99942 Apophis (at the time known by its provisional designation ) was assigned a 4 on the Torino scale, the highest rating ever achieved, as there was a 2.7% chance of Earth impact on April 13, 2029. By December 28, 2004, additional observations had produced a smaller uncertainty zone which no longer included the Earth during the 2029 approach. The 2029 risk of impact consequently dropped to zero, but later potential impact solutions were still rated 1 on the Torino scale. The 2036 risk was lowered to a Torino rating of 0 in August 2006. , calculations show Apophis has no chance of impacting Earth before 2060.

In February 2006, has been assigned a Torino Scale rating of 2 due to a close encounter predicted for May 4, 2102. After more precise calculations, the rating was lowered to 1 in May 2006 and 0 in October 2006, and the asteroid was removed from the Sentry Risk Table entirely in February 2008.

, is listed with the highest chance of impacting Earth, at 1 in 20 on September 5, 2095. At only across, the asteroid does not pose a serious threat: the possible 2095 impact rates only −3.32 on the Palermo Scale.

The first astronomical program dedicated to the discovery of near-Earth asteroids was the Palomar Planet-Crossing Asteroid Survey, started in 1973 by astronomers Eugene Shoemaker and Eleanor Helin. The link to impact hazard, the need for dedicated survey telescopes and options to head off an eventual impact were first discussed at a 1981 interdisciplinary conference in Snowmass, Colorado. Plans for a more comprehensive survey, named the Spaceguard Survey, were developed by NASA from 1992, under a mandate from the United States Congress. To promote the survey on an international level, the International Astronomical Union (IAU) organised a workshop at Vulcano, Italy in 1995, and set up the Spaceguard Foundation also in Italy a year later. In 1998, the United States Congress gave NASA a mandate to detect of 90% of near-earth asteroids over diameter (that threaten global devastation) by 2008.

Several surveys have undertaken "Spaceguard" activities (an umbrella term), including Lincoln Near-Earth Asteroid Research (LINEAR), Spacewatch, Near-Earth Asteroid Tracking (NEAT), Lowell Observatory Near-Earth-Object Search (LONEOS), Catalina Sky Survey (CSS), Campo Imperatore Near-Earth Object Survey (CINEOS), Japanese Spaceguard Association, Asiago-DLR Asteroid Survey (ADAS) and Near-Earth Object WISE (NEOWISE). As a result, the ratio of the known and the estimated total number of near-Earth asteroids larger than 1 km in diameter rose from about 20% in 1998 to 65% in 2004, 80% in 2006, and 93% in 2011. The original Spaceguard goal has thus been met. , 893 NEAs larger than 1 km have been discovered, or 97% of an estimated total of about 920.

In 2005, the original USA Spaceguard mandate was extended by the George E. Brown, Jr. Near-Earth Object Survey Act, which calls for NASA to detect 90% of NEOs with diameters of or greater, by 2020. In January 2016, NASA announced the creation of the Planetary Defense Coordination Office (PDCO) to track NEOs larger than about in diameter and coordinate an effective threat response and mitigation effort.

In May 2016, the precision of asteroid diameter estimates arising from the Wide-field Infrared Survey Explorer and NEOWISE missions was questioned by technologist Nathan Myhrvold, Although the early original criticism did not pass peer review and faced criticism for its methodology itself, a more recent peer-reviewed study was subsequently published.

Survey programs aim to identify threats years in advance, giving humanity time to prepare a space mission to avert the threat.

The ATLAS project, by contrast, aims to find impacting asteroids shortly before impact, much too late for deflection maneuvers but still in time to evacuate and otherwise prepare the affected Earth region. Another project, the Zwicky Transient Facility (ZTF), which surveys for objects that change their brightness rapidly, also detects asteroids passing close to Earth.

Scientists involved in NEO research have also considered options for actively averting the threat from an object found to be on a collision course with Earth. All viable methods aim to deflect rather than destroy the threatening NEO, because the fragments would still cause widespread destruction. Deflection, which means a change in the object's orbit months to years prior to the predicted impact, also requires orders of magnitude less energy.

Near-Earth objects are classified as meteoroids, asteroids, or comets depending on size and composition. Asteroids can also be members of an asteroid family, and comets create meteoroid streams that can generate meteor showers.

, according to statistics maintained by CNEOS, 18,445 NEOs have been discovered: 107 near-Earth comets and 18,338 near-Earth asteroids. There are 1,914 NEOs that are classified as potentially hazardous asteroids (PHAs).

, there are 847 NEAs on the Sentry impact risk page at the NASA website. A significant number of these NEAs are equal to or smaller than 50 meters in diameter and none of the listed objects are placed even in the "green zone" (Torino Scale 1), meaning that none warrant the attention of general public.

These are objects in a near-Earth orbit without the tail or coma of a comet. , 18,136 near-Earth asteroids are known.

NEAs survive in their orbits for just a few million years. They are eventually eliminated by planetary perturbations, causing ejection from the Solar System or a collision with the Sun or a planet. With orbital lifetimes short compared to the age of the Solar System, new asteroids must be constantly moved into near-Earth orbits to explain the observed asteroids. The accepted origin of these asteroids is that main-belt asteroids are moved into the inner Solar System through orbital resonances with Jupiter. The interaction with Jupiter through the resonance perturbs the asteroid's orbit and it comes into the inner Solar System. The asteroid belt has gaps, known as Kirkwood gaps, where these resonances occur as the asteroids in these resonances have been moved onto other orbits. New asteroids migrate into these resonances, due to the Yarkovsky effect that provides a continuing supply of near-Earth asteroids. Compared to the entire mass of the asteroid belt, the mass loss necessary to sustain the NEA population is relatively small; totalling less than 6% over the past 3.5 billion years. The composition of near-Earth asteroids is comparable to that of asteroids from the asteroid belt, reflecting a variety of asteroid spectral types.

A small number of NEAs are extinct comets that have lost their volatile surface materials, although having a faint or intermittent comet-like tail does not necessarily result in a classification as a near-Earth comet, making the boundaries somewhat fuzzy. The rest of the near-Earth asteroids are driven out of the asteroid belt by gravitational interactions with Jupiter.

Many asteroids have natural satellites (minor-planet moons). , 66 NEAs were known to have at least one moon, including three known to have two moons. The asteroid 3122 Florence, one of the largest PHAs with a diameter of , has two moons measuring across, which were discovered by radar imaging during the asteroid's 2017 approach to Earth.

The size of most asteroids can only be estimated on the basis of their brightness and a representative asteroid albedo or surface reflectivity, which is commonly assumed to be 14%. Using this method, an absolute magnitude of 17.75 roughly corresponds to a diameter of and an absolute magnitude of 22.0 corresponds to a diameter of . These size estimates have significant uncertainties for individual asteroids, since asteroid albedos can range as low as 0.05 and as high as 0.3. As of May 2018 and using this crude measure, 889 NEAs listed by CNEOS, including 155 PHAs, measure at least 1 km in diameter, and 8,148 known NEAs are larger than 140 m in diameter. A small fraction of these asteroids has much more accurate direct diameter measurements, from radar observations, from images of the asteroid surface, or from stellar occultations. 

The smallest known near-Earth asteroid is with an absolute magnitude of 33.2, corresponding to a diameter of about . The largest such object is 1036 Ganymed, with an absolute magnitude of 9.45 and a diameter of about .

In 2000, NASA significantly reduced its estimate of the number of existing near-Earth asteroids over one kilometer in diameter from 1,000–2,000 to 500–1,000. Shortly thereafter, an alternative estimate based on the LINEAR survey provided a number of . In 2011, on the basis of NEOWISE observations, the estimated number of one-kilometer NEAs was narrowed to (of which 93% have been discovered), while the number of NEAs larger than 140 meters across was estimated at . The NEOWISE estimate differed from other estimates in assuming a lower average asteroid albedo, which produces larger estimated diameters for the same asteroid brightness. This resulted in 911 then known asteroids at least 1 km across, as opposed to the 830 then listed by CNEOS. In 2017, using an improved statistical method, two studies reduced the estimated number of NEAs brighter than absolute magnitude 17.75 (approximately over one kilometer in diameter) to . The estimated number of asteroids over 140 m across rose to , double the WISE estimate.

The number of asteroids brighter than , which corresponds to about in diameter, is estimated at about —of which about 1.3 percent have been discovered by February 2016; while the number of asteroids brighter than (larger than ) is estimated at about million—of which about 0.003 percent have been discovered by February 2016.

Near-Earth asteroids are divided into groups based on their semi-major axis (a), perihelion distance (q), and aphelion distance (Q):

Atiras and Amors do not cross the Earth's orbit and are not immediate impact threats, but their orbits may change to become Earth-crossing orbits in the future.

, 17 Atiras, 1,342 Atens, 9,964 Apollos and 6,813 Amors have been discovered and cataloged.

NEAs on a co-orbital configuration have the same orbital period as the Earth. All co-orbital asteroids have special orbits that are relatively stable and, paradoxically, can prevent them from getting close to Earth:

In 1961, the IAU defined meteoroids as a class of solid interplanetary objects distinct from asteroids by their considerably smaller size. This definition was useful at the time because, with the exception of the Tunguska event, all historically observed meteors were produced by objects significantly smaller than the smallest asteroids observable by telescopes. As the distinction began to blur with the discovery of ever smaller asteroids and a greater variety of observed NEO impacts, revised definitions with size limits have been proposed from the 1990s. In April 2017, the IAU adopted a revised definition that generally limits meteoroids to a size between 30 µm and 1 m in diameter, but permits the use of the term for any object of any size that caused a meteor, thus leaving the distinction between asteroid and meteoroid blurred.

Near-Earth comets (NECs) are objects in a near-Earth orbit with a tail or coma. Comet nuclei are typically less dense than asteroids but they pass Earth at higher relative speeds, thus the impact energy of comet nucleus is slightly larger than that of a similar-sized asteroid. NECs may pose an additional hazard due to fragmentation: the meteoroid streams which produce meteor showers may include large inactive fragments, effectively NEAs. Although no impact of a comet in Earth's history has been conclusively confirmed, the Tunguska event may have been caused by a fragment of Comet Encke.

Comets are commonly divided between short-period and long-period comets. Short-period comets, with an orbital period of less than 200 years, originated in the Kuiper belt, beyond the orbit of Neptune; while long-period comets originate in the Oort Cloud, in the outer reaches of the Solar System. The orbital period distinction is of importance in the evaluation of the risk from near-Earth comets because short-period NECs are likely to have been observed during multiple apparitions and thus their orbits can be determined with some precision, while long-period NECs can be assumed to have been seen for the first and last time when they appeared during the Age of Science, thus their approaches cannot be predicted well in advance. Since the threat from long-period NECs is estimated to be at most 1% of the threat from NEAs, and long-period comets are very faint and thus difficult to detect at large distances from the Sun, Spaceguard efforts consistently focused on asteroids and short-period comets. CNEOS even restricts its definition of NECs to short-period comets—, 107 such objects have been discovered.

, only 20 comets have been observed to pass within of Earth, including 10 which are or have been short-period comets. Two of these comets, Halley's Comet and 73P/Schwassmann–Wachmann, have been observed during multiple close approaches. The closest observed approach was 0.0151 AU (5.88 LD) for Lexell's Comet on July 1, 1770. After an orbit change due to a close approach of Jupiter in 1779, this object is no longer a NEC. The closest approach ever observed for a current short-period NEC is 0.0229 AU (8.92 LD) for Comet Tempel–Tuttle in 1366. This comet is the parent body of the Leonid meteor shower, which also produced the Great Meteor Storm of 1833. Orbital calculations show that P/1999 J6 (SOHO), a faint sungrazing comet and confirmed short-period NEC observed only during its close approaches to the Sun, passed Earth undetected at a distance of 0.0121 AU (4.70 LD) on June 12, 1999.

Comet 109P/Swift–Tuttle, which is also the source of the Perseid meteor shower which hits Earth every year in August, has a roughly 130-year orbit which passes close to the Earth. After the comet's 1992 return, when only the two previous returns in 1862 and 1737 have been identified, orbital calculations showed that the comet would pass very close to Earth during its next return in 2126, with an impact within the range of uncertainty. By 1993, even earlier returns (back to at least 188 AD) have been identified, and the new orbital calculation eliminated the impact risk, predicting the comet to pass Earth in 2126 at a distance of 24 million kilometers. In 3044, the comet is expected to pass Earth at less than 1.6 million kilometers.

Defunct space probes and final stages of rockets can end up in near-Earth orbits around the Sun, and be re-discovered by NEO surveys when they return to Earth's vicinity.

In September 2002, astronomers found an object designated J002E3. The object was on a temporary satellite orbit around Earth, leaving for a solar orbit in June 2003. Calculations showed that it was also on a solar orbit before 2002, but was close to Earth in 1971. J002E3 was identified as the third stage of the Saturn V rocket that carried Apollo 12 to the Moon. In 2006, two more apparent temporary satellites were discovered which were suspected of being artificial. One of them was eventually confirmed as an asteroid and classified as the temporary satellite . The other, 6Q0B44E, was confirmed as an artificial object, but its identity is unknown. Another confirmed artificial temporary satellite with unidentified origin is .

In some cases, active space probes on solar orbits have been observed by NEO surveys and erroneously catalogued as asteroids before identification. During its 2007 flyby of Earth on its route to a comet, ESA's space probe "Rosetta" was detected unidentified and classified as asteroid , with an alert issued due to its close approach. The designation was similarly removed from asteroid catalogues when the observed object was identified with "Gaia", ESA's space observatory for astrometry.

When a near-Earth object impacts Earth, objects up to a few tens of metres across ordinarily explode in the upper atmosphere, with most or all of the solids vaporized, while larger objects hit the water surface, forming waves, or the solid surface, forming impact craters.

The frequency of impacts of objects of various sizes is estimated on the basis of orbit simulations of NEO populations, the frequency of impact craters on the Earth and the Moon, and the frequency of close encounters. The study of impact craters indicates that impact frequency has been more or less steady for the past 3.5 billion years, which can be explained by a steady replenishment of the NEO population from the asteroid main belt. One impact model based on widely accepted NEO population models estimates the average time between the impact of two stony asteroids with a diameter of at least at about one year; for asteroids across (which impacts with as much energy as the atomic bomb dropped on Hiroshima, approximately 15 kilotonnes of TNT) at five years, for asteroids across (an impact energy of 10 megatons, comparable to the Tunguska event in 1908) at 1,300 years, for asteroids across at half a million years, and for asteroids across at 18 million years. Some other models estimate similar impact frequencies, while others calculate higher frequencies. For Tunguska-sized (10-megaton) impacts, there are estimates of one event every 2,000–3,000 years and one event every 300 years.

The second-largest observed impact after the Tunguska meteor was a 1.1-megaton air blast near the Prince Edward Islands off the coast of South Africa in 1963, which was detected only by infrasound sensors. The third-largest but most-observed impact was the Chelyabinsk meteor of February 15, 2013. A asteroid exploded with the equivalent blast yield of 400–500 kilotons. The calculated orbit of the pre-impact asteroid is similar to that of Apollo asteroid 2011 EO40, making the latter the meteor's possible parent body.

On October 7, 2008, 19 hours after it has been first observed, asteroid blew up above the Nubian Desert in Sudan. It was the first time that an asteroid was observed and its impact had been predicted prior to its entry into the atmosphere as a meteor. 10.7 kg of meteorites were recovered after the impact.

On January 2, 2014, just 21 hours after it was the first asteroid to be discovered in 2014, blew up in Earth's atmosphere above the Atlantic Ocean. Far from any land, the meteor explosion was only observed by three infrasound detectors of the Comprehensive Nuclear-Test-Ban Treaty Organization. This impact was the second to be predicted in advance.

Observed impacts aren't restricted to the surface and atmosphere of Earth. Dust-sized NEOs have impacted man-made spacecraft, including NASA's Long Duration Exposure Facility, which collected interplanetary dust in low Earth orbit for six years from 1984. Impacts on the Moon can be observed as flashes of light with a typical duration of a fraction of a second. The first lunar impacts were recorded during the 1999 Leonid storm. Subsequently, several continuous monitoring programs were launched. , the largest observed lunar impact occurred on September 11, 2013, lasted 8 seconds, and was likely caused by an object in diameter.

Each year, several mostly small NEOs pass Earth closer than the distance of the Moon.

On August 10, 1972, a meteor that became known as 1972 Great Daylight Fireball was witnessed by many people; it moved north over the Rocky Mountains from the U.S. Southwest to Canada. It was an Earth-grazing meteoroid that passed within of the Earth's surface, and was filmed by a tourist at the Grand Teton National Park in Wyoming with an 8-millimeter color movie camera.

On October 13, 1990, Earth-grazing meteoroid EN131090 was observed above Czechoslovakia and Poland, moving at along a trajectory from south to north. The closest approach to the Earth was above the surface. It was captured by two all-sky cameras of the European Fireball Network, which for the first time enabled geometric calculations of the orbit of such a body.

On March 18, 2004, LINEAR announced that a asteroid, 2004 FH, would pass the Earth that day at only , about one-tenth the distance to the Moon, and the closest miss ever noticed until then. They estimated that similar-sized asteroids come as close about every two years.

On March 31, 2004, two weeks after 2004 FH, set a new record for closest recorded approach above the atmosphere, passing Earth's surface only away (about one Earth radius or one-sixtieth of the distance to the Moon). Because it was very small (6 meters/20 feet), FU was detected only hours before its closest approach. If it had collided with Earth, it probably would have disintegrated harmlessly in the atmosphere.

On February 4, 2011, an asteroid designated , estimated at in diameter, passed within of the Earth, setting a new record for closest approach without impact, which still stands .

On November 8, 2011, asteroid , relatively large at about in diameter, passed within (0.85 lunar distances) of Earth.

On February 15, 2013, the asteroid 367943 Duende () passed approximately above the surface of Earth, closer than satellites in geosynchronous orbit. The asteroid was not visible to the unaided eye. This was the first close passage of an object discovered during a previous passage, and was thus the first to be predicted well in advance.

Some NEOs are of special interest because they can be physically explored with lower mission velocity than is necessary for even the Moon, due to their combination of low velocity with respect to Earth and weak gravity. They may present interesting scientific opportunities both for direct geochemical and astronomical investigation, and as potentially economical sources of extraterrestrial materials for human exploitation. This makes them an attractive target for exploration.

The IAU held a minor planets workshop in Tucson, Arizona, in March 1971. At that point, launching a spacecraft to asteroids was considered premature; the workshop only inspired the first astronomical survey specifically aiming for NEAs. Missions to asteroids were considered again during a workshop at the University of Chicago held by NASA’s Office of Space Science in January 1978. Of all of the near-Earth asteroids (NEA) that had been discovered by mid-1977, it was estimated that spacecraft could rendezvous with and return from only about 1 in 10 using less propulsive energy than is necessary to reach Mars. It was recognised that due to the low surface gravity of all NEAs, moving around on the surface of a NEA would cost very little energy, and thus space probes could gather multiple samples. Overall, it was estimated that about one percent of all NEAs might provide opportunities for human-crewed missions, or no more than about ten NEAs known at the time. A five-fold increase in the NEA discovery rate was deemed necessary to make a manned mission within ten years worthwhile.

The first near-Earth asteroid to be visited by a spacecraft was asteroid 433 Eros when NASA's "Near Earth Asteroid Rendezvous" ("NEAR") probe orbited it from February 2001, landing on the asteroid surface in February 2002. A second near-Earth asteroid, the long peanut-shaped 25143 Itokawa, was visited in September 2005 by JAXA's "Hayabusa" mission, which succeeded in taking material samples back to Earth. A third near-Earth asteroid, the long elongated 4179 Toutatis, was explored by CNSA's "Chang'e 2" spacecraft during a flyby in December 2012.

The Apollo asteroid 162173 Ryugu is the target of JAXA's "Hayabusa 2" mission. The space probe was launched in December 2014, is expected to arrive at the asteroid in June 2018, and to return a sample to Earth in December 2020. The Apollo asteroid 101955 Bennu, which, , has the second-highest cumulative Palermo scale rating (−1.71 for several close encounters between 2175 and 2199), is the target of NASA's "OSIRIS-REx" probe. The New Frontiers program mission was launched in September 2016. On its two-year journey to Bennu, the probe is searching for Earth's Trojan asteroids, will rendezvous with Bennu in August 2018, and will return samples from the asteroid in September 2023.

In April 2012, the company Planetary Resources announced its plans to mine asteroids commercially. In a first phase, the company reviewed data and selected potential targets among NEAs. In a second phase, space probes would be sent to the selected NEAs; mining spacecraft would be sent in a third phase. Planetary Resources launched two testbed satellites in April 2015 and January 2018, and the first prospecting satellite for the second phase is planned for a 2020 launch.

The first near-Earth comet visited by a space probe was 21P/Giacobini–Zinner in 1985, when the NASA/ESA probe "International Cometary Explorer" ("ICE") passed through its coma. In March 1986, ICE, along with Soviet probes "Vega 1" and "Vega 2", ISAS probes "Sakigake" and "Suisei" and ESA probe "Giotto" flew by the nucleus of Halley's Comet. In 1992, "Giotto" also visited another NEC, 26P/Grigg–Skjellerup.

In November 2010, the NASA probe "Deep Impact" flew by the near-Earth comet 103P/Hartley. Earlier, in July 2005, this probe flew by the non-near-Earth comet Tempel 1, hitting it with a large copper mass.

In August 2014, ESA probe "Rosetta" began orbiting near-Earth comet 67P/Churyumov–Gerasimenko, while its lander "Philae" landed on its surface in November 2014. After the end of its mission, Rosetta was crashed into the comet′s surface in 2016.




</doc>
<doc id="21627" url="https://en.wikipedia.org/wiki?curid=21627" title="Nation state">
Nation state

A nation state (or nation-state), in the most specific sense, is a country where a distinct cultural or ethnic group (a "nation" or "people") inhabits a territory and has formed a state (often a sovereign state) that it predominantly governs. It is a more precise concept than "country", since a country need not have a predominant ethnic group.

A nation, in the sense of a common ethnicity, may include a diaspora or refugees who live outside the nation-state; some nations of this sense do not have a state where that ethnicity predominates. In a more general sense, a nation-state is simply a large, politically sovereign country or administrative territory. A nation-state may be contrasted with:

This article mainly discusses the more specific definition of a nation-state, as a typically sovereign country dominated by a particular ethnicity.

The relationship between a nation (in the ethnic sense) and a state can be complex. The presence of a state can encourage ethnogenesis, and a group with a pre-existing ethnic identity can influence the drawing of territorial boundaries or to argue for political legitimacy.

This definition of a "nation-state" is not universally accepted. "All attempts to develop terminological consensus around "nation" resulted in failure", concludes academic Valery Tishkov.

Walker Connor discusses the impressions surrounding the characters of "nation", "(sovereign) state", "nation state", and "nationalism". Connor, who gave the term "ethnonationalism" wide currency, also discusses the tendency to confuse nation and state and the treatment of all states as if nation states. In "Globalization and Belonging", Sheila L. Crouche discusses "The Definitional Dilemma".

The origins and early history of nation states are disputed. A major theoretical question is: "Which came first, the nation or the nation state?" Scholars such as Steven Weber, David Woodward, and Jeremy Black have advanced the hypothesis that the nation state did not arise out of political ingenuity or an unknown undetermined source, nor was it an accident of history or political invention; but is an inadvertent byproduct of 15th-century intellectual discoveries in political economy, capitalism, mercantilism, political geography, and geography combined together with cartography and advances in map-making technologies. It was with these intellectual discoveries and technological advances that the nation state arose. For others, the nation existed first, then nationalist movements arose for sovereignty, and the nation state was created to meet that demand. Some "modernization theories" of nationalism see it as a product of government policies to unify and modernize an already existing state. Most theories see the nation state as a 19th-century European phenomenon, facilitated by developments such as state-mandated education, mass literacy and mass media. However, historians also note the early emergence of a relatively unified state and identity in Portugal and the Dutch Republic.

In France, Eric Hobsbawm argues, the French state preceded the formation of the French people. Hobsbawm considers that the state made the French nation, not French nationalism, which emerged at the end of the 19th century, the time of the Dreyfus Affair. At the time of the 1789 French Revolution, only half of the French people spoke some French, and 12–13% spoke the version of it that was to be found in literature and in educational facilities, according to Hobsbawm.

During the Italian unification, the number of people speaking the Italian language was even lower. The French state promoted the replacement of various regional dialects and languages by a centralised French language. The introduction of conscription and the Third Republic's 1880s laws on public instruction, facilitated the creation of a national identity, under this theory.

Some nation states, such as Germany and Italy, came into existence at least partly as a result of political campaigns by nationalists, during the 19th century. In both cases, the territory was previously divided among other states, some of them very small. The sense of common identity was at first a cultural movement, such as in the "Völkisch movement" in German-speaking states, which rapidly acquired a political significance. In these cases, the nationalist sentiment and the nationalist movement clearly precede the unification of the German and Italian nation states.

Historians Hans Kohn, Liah Greenfeld, Philip White and others have classified nations such as Germany or Italy, where cultural unification preceded state unification, as "ethnic nations" or "ethnic nationalities". However, "state-driven" national unifications, such as in France, England or China, are more likely to flourish in multiethnic societies, producing a traditional national heritage of "civic nations", or "territory-based nationalities". Some authors deconstruct the distinction between ethnic nationalism and civic nationalism because of the ambiguity of the concepts. They argue that the paradigmatic case of Ernest Renan is an idealisation and it should be interpreted within the German tradition and not in opposition to it. For example, they argue that the arguments used by Renan at the conference "What is a nation?" are not consistent with his thinking. This alleged civic conception of the nation would be determined only by the case of the loss gives Alsace and Lorraine in the Franco-Prussian War.

The idea of a nation state was and is associated with the rise of the modern system of states, often called the "Westphalian system" in reference to the Treaty of Westphalia (1648). The balance of power, which characterized that system, depended on its effectiveness upon clearly defined, centrally controlled, independent entities, whether empires or nation states, which recognize each other's sovereignty and territory. The Westphalian system did not create the nation state, but the nation state meets the criteria for its component states (by assuming that there is no disputed territory).

The nation state received a philosophical underpinning in the era of Romanticism, at first as the "natural" expression of the individual peoples (romantic nationalism: see Johann Gottlieb Fichte's conception of the "Volk", later opposed by Ernest Renan). The increasing emphasis during the 19th century on the ethnic and racial origins of the nation, led to a redefinition of the nation state in these terms. Racism, which in Boulainvilliers's theories was inherently antipatriotic and antinationalist, joined itself with colonialist imperialism and "continental imperialism", most notably in pan-Germanic and pan-Slavic movements.

The relation between racism and ethnic nationalism reached its height in the 20th century fascism and Nazism. The specific combination of "nation" ("people") and "state" expressed in such terms as the "Völkische Staat" and implemented in laws such as the 1935 Nuremberg laws made fascist states such as early Nazi Germany qualitatively different from non-fascist nation states. Minorities were not considered part of the people ("Volk"), and were consequently denied to have an authentic or legitimate role in such a state. In Germany, neither Jews nor the Roma were considered part of the people and were specifically targeted for persecution. German nationality law defined "German" on the basis of German ancestry, excluding "all" non-Germans from the people.

In recent years, a nation state's claim to absolute sovereignty within its borders has been much criticized. A global political system based on international agreements and supra-national blocs characterized the post-war era. Non-state actors, such as international corporations and non-governmental organizations, are widely seen as eroding the economic and political power of nation states, potentially leading to their eventual disappearance.

In Europe, during the 18th century, the classic non-national states were the "multiethnic" empires, the Austrian Empire, Kingdom of France, Kingdom of Hungary, the Russian Empire, the Ottoman Empire, the British Empire and smaller nations at what would now be called sub-state level. The multi-ethnic empire was an absolute monarchy ruled by a king, emperor or sultan. The population belonged to many ethnic groups, and they spoke many languages. The empire was dominated by one ethnic group, and their language was usually the language of public administration. The ruling dynasty was usually, but not always, from that group.

This type of state is not specifically European: such empires existed on all continents, except Australia and Antarctica. Some of the smaller European states were not so ethnically diverse, but were also dynastic states, ruled by a royal house. Their territory could expand by royal intermarriage or merge with another state when the dynasty merged. In some parts of Europe, notably Germany, very small territorial units existed. They were recognised by their neighbours as independent, and had their own government and laws. Some were ruled by princes or other hereditary rulers, some were governed by bishops or abbots. Because they were so small, however, they had no separate language or culture: the inhabitants shared the language of the surrounding region.

In some cases these states were simply overthrown by nationalist uprisings in the 19th century. Liberal ideas of free trade played a role in German unification, which was preceded by a customs union, the Zollverein. However, the Austro-Prussian War, and the German alliances in the Franco-Prussian War, were decisive in the unification. The Austro-Hungarian Empire and the Ottoman Empire broke up after the First World War, and the Russian Empire became the Soviet Union after the Russian Civil War.

A few of the smaller states survived: the independent principalities of Liechtenstein, Andorra, Monaco, and the republic of San Marino. (Vatican City is a special case. All of the larger Papal States save the Vatican itself were occupied and absorbed by Italy by 1870. The resulting Roman Question was resolved with the rise of the modern state under the 1929 Lateran treaties between Italy and the Holy See.)

"Legitimate states that govern effectively and dynamic industrial economies are widely regarded today as the defining characteristics of a modern nation-state."

Nation states have their own characteristics, differing from those of the pre-national states. For a start, they have a different attitude to their territory when compared with dynastic monarchies: it is semisacred and nontransferable. No nation would swap territory with other states simply, for example, because the king's daughter married. They have a different type of border, in principle defined only by the area of settlement of the national group, although many nation states also sought natural borders (rivers, mountain ranges). They are constantly changing in population size and power because of the limited restrictions of their borders.

The most noticeable characteristic is the degree to which nation states use the state as an instrument of national unity, in economic, social and cultural life.

The nation state promoted economic unity, by abolishing internal customs and tolls. In Germany, that process, the creation of the Zollverein, preceded formal national unity. Nation states typically have a policy to create and maintain a national transportation infrastructure, facilitating trade and travel. In 19th-century Europe, the expansion of the rail transport networks was at first largely a matter for private railway companies, but gradually came under control of the national governments. The French rail network, with its main lines radiating from Paris to all corners of France, is often seen as a reflection of the centralised French nation state, which directed its construction. Nation states continue to build, for instance, specifically national motorway networks. Specifically transnational infrastructure programmes, such as the Trans-European Networks, are a recent innovation.

The nation states typically had a more centralised and uniform public administration than its imperial predecessors: they were smaller, and the population less diverse. (The internal diversity of the Ottoman Empire, for instance, was very great.) After the 19th-century triumph of the nation state in Europe, regional identity was subordinate to national identity, in regions such as Alsace-Lorraine, Catalonia, Brittany and Corsica. In many cases, the regional administration was also subordinated to central (national) government. This process was partially reversed from the 1970s onward, with the introduction of various forms of regional autonomy, in formerly centralised states such as France.

The most obvious impact of the nation state, as compared to its non-national predecessors, is the creation of a uniform national culture, through state policy. The model of the nation state implies that its population constitutes a nation, united by a common descent, a common language and many forms of shared culture. When the implied unity was absent, the nation state often tried to create it. It promoted a uniform national language, through language policy. The creation of national systems of compulsory primary education and a relatively uniform curriculum in secondary schools, was the most effective instrument in the spread of the national languages. The schools also taught the national history, often in a propagandistic and mythologised version, and (especially during conflicts) some nation states still teach this kind of history.

Language and cultural policy was sometimes negative, aimed at the suppression of non-national elements. Language prohibitions were sometimes used to accelerate the adoption of national languages and the decline of minority languages (see examples: Anglicisation, Czechization, Francisation, Italianization, Germanisation, Magyarisation, Polonisation, Russification, Serbization, Slovakisation).

In some cases, these policies triggered bitter conflicts and further ethnic separatism. But where it worked, the cultural uniformity and homogeneity of the population increased. Conversely, the cultural divergence at the border became sharper: in theory, a uniform French identity extends from the Atlantic coast to the Rhine, and on the other bank of the Rhine, a uniform German identity begins. To enforce that model, both sides have divergent language policy and educational systems.

In some cases, the geographic boundaries of an ethnic population and a political state largely coincide. In these cases, there is little immigration or emigration, few members of ethnic minorities, and few members of the "home" ethnicity living in other countries.

Examples of nation states where ethnic groups make up more than 85% of the population include the following:

The notion of a unifying "national identity" also extends to countries that host multiple ethnic or language groups, such as India. For example, Switzerland is constitutionally a confederation of cantons, and has four official languages, but it has also a "Swiss" national identity, a national history and a classic national hero, Wilhelm Tell.

Innumerable conflicts have arisen where political boundaries did not correspond with ethnic or cultural boundaries.

After World War II in the Josip Broz Tito era, nationalism was appealed to for uniting South Slav peoples. Later in the 20th century, after the break-up of the Soviet Union, leaders appealed to ancient ethnic feuds or tensions that ignited conflict between the Serbs, Croats and Slovenes, as well Bosnians, Montenegrins and Macedonians, eventually breaking up the long collaboration of peoples. Ethnic cleansing was carried out in the Balkans, resulting in the destruction of the formerly socialist republic and producing the civil wars in Bosnia and Herzegovina in 1992–95, resulting in mass population displacements and segregation that radically altered what was once a highly diverse and intermixed ethnic makeup of the region. These conflicts were largely about creating a new political framework of states, each of which would be ethnically and politically homogeneous. Serbians, Croatians and Bosnians insisted they were ethnically distinct although many communities had a long history of intermarriage. Presently Slovenia (89% Slovene), Croatia (90.4% Croat) and Serbia (83% Serb) could be classified as nation states per se, whereas Macedonia (66% Macedonian), Montenegro (42% Montenegrin) and Bosnia and Herzegovina (50.1% Bosniak) are multinational states.

Belgium is a classic example of a state that is not a nation state. The state was formed by secession from the United Kingdom of the Netherlands in 1830, whose neutrality and integrity was protected by the Treaty of London 1839; thus it served as a buffer state after the Napoleonic Wars between the European powers France, Prussia (after 1871 the German Empire) and the United Kingdom until World War I, when its neutrality was breached by the Germans. Currently, Belgium is divided between the Flemings in the north and the French-speaking or the German-speaking population in the south. The Flemish population in the north speaks Dutch, the Walloon population in the south speaks French or German. The Brussels population speaks French or Dutch.

The Flemish identity is also cultural, and there is a strong separatist movement espoused by the political parties, the right-wing Vlaams Belang and the Nieuw-Vlaamse Alliantie. The Francophone Walloon identity of Belgium is linguistically distinct and regionalist. There is also unitary Belgian nationalism, several versions of a Greater Netherlands ideal, and a German-speaking community of Belgium annexed from Germany in 1920, and re-annexed by Germany in 1940–1944. However these ideologies are all very marginal and politically insignificant during elections.

China covers a large geographic area and uses the concept of "Zhonghua minzu" or Chinese nationality, in the sense of ethnic groups, but it also officially recognizes the majority Han ethnic group which accounts for over 90% of the population, and no fewer than 55 ethnic national minorities.

According to Philip G. Roeder, Moldova is an example of a Soviet era "segment-state" (Moldavian SSR), where the "nation-state project of the segment-state trumped the nation-state project of prior statehood. In Moldova, despite strong agitation from university faculty and students for reunification with Romania, the nation-state project forged within the Moldavian SSR trumped the project for a return to the interwar nation-state project of Greater Romania." See Controversy over linguistic and ethnic identity in Moldova for further details.

The United Kingdom is an unusual example of a nation state, due to its claimed "countries within a country" status. The United Kingdom, which is formed by the union of England, Scotland, Wales and Northern Ireland, is a unitary state formed initially by the merger of two independent kingdoms, the Kingdom of England (which already included Wales) and the Kingdom of Scotland, but the Treaty of Union (1707) that set out the agreed terms has ensured the continuation of distinct features of each state, including separate legal systems and separate national churches.

In 2003, the British Government described the United Kingdom as "countries within a country". While the Office for National Statistics and others describe the United Kingdom as a "nation state", others, including a then Prime Minister, describe it as a "multinational state", and the term Home Nations is used to describe the four national teams that represent the four nations of the United Kingdom (England, Northern Ireland, Scotland, Wales). Some refer to it as a "Union State".

There has been academic debate over whether the United Kingdom can be legally dissolved as it is normally recognized internationally as a single nation state. English law jurist A.V. Dicey from an English legal perspective wrote that the question is based on whether the legislation giving rise to the union (the Union with Scotland Act), one of the two pieces of legislation which created the state, can be repealed. Dicey claimed because the Law of England does not acknowledge the word "unconstitutional", as a matter of English law it can be repealed. He also stated any tampering with the Acts of Union 1707 would be political madness.

A similar unusual example is the Kingdom of the Netherlands. As of 10 October 2010, the Kingdom of the Netherlands consists of four countries:

Each is expressly designated as a "land" in Dutch law by the Charter for the Kingdom of the Netherlands. Unlike the German "Länder" and the Austrian "Bundesländer", "landen" is consistently translated as "countries" by the Dutch government.

Israel was founded as a Jewish state in 1948. Its "Basic Laws" describe it as both a Jewish and a democratic state. According to the Israel Central Bureau of Statistics, 75.7% of Israel's population are Jews. Arabs, who make up 20.4% of the population, are the largest ethnic minority in Israel. Israel also has very small communities of Armenians, Circassians, Assyrians, Samaritans, and persons of some Jewish heritage. There are also some non-Jewish spouses of Israeli Jews. However, these communities are very small, and usually number only in the hundreds or thousands.

Pakistan, even being an ethnically diverse country and officially a federation, is regarded as a nation state due to its ideological basis on which it was given independence from British India as a separate nation rather than as part of a unified India. Different ethnic groups in Pakistan are strongly bonded by their common Muslim identity, common cultural and social values, common historical heritage, a national Lingua franca (Urdu) and joint political, strategic and economic interests.

The most obvious deviation from the ideal of "one nation, one state" is the presence of minorities, especially ethnic minorities, which are clearly not members of the majority nation. An ethnic nationalist definition of a nation is necessarily exclusive: ethnic nations typically do not have open membership. In most cases, there is a clear idea that surrounding nations are different, and that includes members of those nations who live on the "wrong side" of the border. Historical examples of groups who have been specifically singled out as "outsiders" are the Roma and Jews in Europe.

Negative responses to minorities within the nation state have ranged from cultural assimilation enforced by the state, to expulsion, persecution, violence, and extermination. The assimilation policies are usually enforced by the state, but violence against minorities is not always state initiated: it can occur in the form of mob violence such as lynching or pogroms. Nation states are responsible for some of the worst historical examples of violence against minorities not considered part of the nation.

However, many nation states accept specific minorities as being part of the nation, and the term "national minority" is often used in this sense. The Sorbs in Germany are an example: for centuries they have lived in German-speaking states, surrounded by a much larger ethnic German population, and they have no other historical territory. They are now generally considered to be part of the German nation and are accepted as such by the Federal Republic of Germany, which constitutionally guarantees their cultural rights. Of the thousands of ethnic and cultural minorities in nation states across the world, only a few have this level of acceptance and protection.

Multiculturalism is an official policy in many states, establishing the ideal of peaceful existence among multiple ethnic, cultural, and linguistic groups. Many nations have laws protecting minority rights.

When national boundaries that do not match ethnic boundaries are drawn, such as in the Balkans and Central Asia, ethnic tension, massacres and even genocide, sometimes has occurred historically (see Serbian genocide, Bosnian genocide and 2010 South Kyrgyzstan ethnic clashes).

Ideally, the border of a nation state extends far enough to include all the members of the nation, and all of the national homeland. Again, in practice some of them always live on the 'wrong side' of the border. Part of the national homeland may be there too, and it may be governed by the 'wrong' nation. The response to the non-inclusion of territory and population may take the form of irredentism: demands to annex "unredeemed" territory and incorporate it into the nation state.

Irredentist claims are usually based on the fact that an identifiable part of the national group lives across the border. However, they can include claims to territory where no members of that nation live at present, because they lived there in the past, the national language is spoken in that region, the national culture has influenced it, geographical unity with the existing territory, or a wide variety of other reasons. Past grievances are usually involved and can cause revanchism.

It is sometimes difficult to distinguish irredentism from pan-nationalism, since both claim that all members of an ethnic and cultural nation belong in one specific state. Pan-nationalism is less likely to specify the nation ethnically. For instance, variants of Pan-Germanism have different ideas about what constituted Greater Germany, including the confusing term "Grossdeutschland", which, in fact, implied the inclusion of huge Slavic minorities from the Austro-Hungarian Empire.

Typically, irredentist demands are at first made by members of non-state nationalist movements. When they are adopted by a state, they typically result in tensions, and actual attempts at annexation are always considered a "casus belli", a cause for war. In many cases, such claims result in long-term hostile relations between neighbouring states. Irredentist movements typically circulate maps of the claimed national territory, the "greater" nation state. That territory, which is often much larger than the existing state, plays a central role in their propaganda.

Irredentism should not be confused with claims to overseas colonies, which are not generally considered part of the national homeland. Some French overseas colonies would be an exception: French rule in Algeria unsuccessfully treated the colony as a "département" of France.

It has been speculated by both proponents of globalization and various science fiction writers that the concept of a nation state may disappear with the ever-increasing interconnectedness of the world. Such ideas are sometimes expressed around concepts of a world government. Another possibility is a societal collapse and move into communal anarchy or zero world government, in which nation states no longer exist and government is done on the local level based on a global ethic of human rights.

This falls in line with the concept of internationalism, which states that sovereignty is an outdated concept and a barrier to achieving peace and harmony in the world.

Globalization especially has helped to bring about the discussion about the disappearance of nation states, as global trade and the rise of the concepts of a 'global citizen' and a common identity have helped to reduce differences and 'distances' between individual nation states, especially with regards to the internet.

The theory of the clash of civilizations lies in direct contrast to cosmopolitan theories about an ever more-connected world that no longer requires nation states. According to political scientist Samuel P. Huntington, people's cultural and religious identities will be the primary source of conflict in the post–Cold War world.

The theory was originally formulated in a 1992 lecture at the American Enterprise Institute, which was then developed in a 1993 "Foreign Affairs" article titled "The Clash of Civilizations?", in response to Francis Fukuyama's 1992 book, "The End of History and the Last Man". Huntington later expanded his thesis in a 1996 book "The Clash of Civilizations and the Remaking of World Order".

Huntington began his thinking by surveying the diverse theories about the nature of global politics in the post–Cold War period. Some theorists and writers argued that human rights, liberal democracy and capitalist free market economics had become the only remaining ideological alternative for nations in the post–Cold War world. Specifically, Francis Fukuyama, in "The End of History and the Last Man", argued that the world had reached a Hegelian "end of history".

Huntington believed that while the age of ideology had ended, the world had reverted only to a normal state of affairs characterized by cultural conflict. In his thesis, he argued that the primary axis of conflict in the future will be along cultural and religious lines.

As an extension, he posits that the concept of different civilizations, as the highest rank of cultural identity, will become increasingly useful in analyzing the potential for conflict.

In the 1993 "Foreign Affairs" article, Huntington writes:

Sandra Joireman suggests that Huntington may be characterised as a neo-primordialist, as, while he sees people as having strong ties to their ethnicity, he does not believe that these ties have always existed.

Historians often look to the past to find the origins of a particular nation state. Indeed, they often put so much emphasis on the importance of the nation state in modern times, that they distort the history of earlier periods in order to emphasize the question of origins. Lansing and English argue that much of the medieval history of Europe was structured to follow the historical winners—especially the nation states that emerged around Paris and London. Important developments that did not directly lead to a nation state get neglected, they argue:





</doc>
<doc id="21628" url="https://en.wikipedia.org/wiki?curid=21628" title="Nicolas-Louis de Lacaille">
Nicolas-Louis de Lacaille

Abbé Nicolas-Louis de Lacaille, formerly sometimes spelled de la Caille, (; 15 March 1713 – 21 March 1762) was a French astronomer.

Born at Rumigny (in present-day Ardennes), he attended school in Mantes-sur-Seine (now Mantes-la-Jolie). Afterwards, he studied rhetoric and philosophy at the and then theology at the Collège de Navarre. He was left destitute in 1731 by the death of his father, who had held a post in the household of the duchess of Vendôme. However, he was supported in his studies by the Duc de Bourbon, his father's former patron.

After he graduated, he did not accept ordination as a priest but took deacon's orders, becoming an Abbé. He concentrated thereafter on science, and, through the patronage of Jacques Cassini, obtained employment, first in surveying the coast from Nantes to Bayonne, then, in 1739, in remeasuring the French arc of the meridian, for which he is honored with a pyramid at Juvisy-sur-Orge. The success of this difficult operation, which occupied two years, and achieved the correction of the anomalous result published by Jacques Cassini in 1718, was mainly due to Lacaille's industry and skill. He was rewarded by admission to the Royal Academy of Sciences and appointment as Professor of mathematics in the Mazarin college of the University of Paris, where he constructed a small observatory fitted for his own use. He was the author of a number of influential textbooks and a firm advocate of Newtonian gravitational theory. Among his students were Antoine Lavoisier and Jean Sylvain Bailly, both of whom were guillotined during the Revolution.

His desire to determine the distances of the planets trigonometrically, using the longest possible baseline, led him to propose, in 1750, an expedition to the Cape of Good Hope. This was officially sanctioned by Roland-Michel Barrin de La Galissonière. There, he constructed an observatory on the shore of Table Bay with the support of the Dutch Governor Ryk Tulbagh. The primary result of his two-year stay was observed nearly 10,000 southern stars, the production of which required observing every night for over a year. In the course of his survey he took note of 42 nebulous objects. He also achieved his aim of determining the lunar and solar parallaxes (Mars serving as an intermediary). This work required near-simultaneous observations from Europe which were carried out by Jérôme Lalande.

His southern catalogue, called "Coelum Australe Stelliferum", was published posthumously in 1763. He found it necessary to introduce 14 new constellations which have since become standard. One of these was Mons Mensae, the only constellation named after a terrestrial feature (the Table Mountain).

While at the Cape, Lacaille determined the radius of the earth in the southern hemisphere. He set out a baseline in the Swartland plain north of present-day Darling. Using triangulation he then measured a 137 km arc of meridian between Cape Town and Aurora, determining the latitudes of the end points by means of astronomical observations. There is a memorial to his work at a location near Aurora, pictured here. His result suggested that the earth was more flattened towards the south pole than towards the north. George Everest, of the Indian Survey, while recuperating from an illness at the Cape nearly seventy years later, suggested that Lacaille's latitude observations had been affected by the gravitational attraction of Table Mountain at the southern end and by the Piketberg Mountain at the northern. In 1838, Thomas Maclear, who was Astronomer Royal at the Cape, repeated the measurements over a longer baseline and ultimately confirmed Everest's conjecture.Maclear's Beacon was erected on the Table Mountain in Cape Town to help with the verification.

During his voyage to the southern hemisphere as a passenger on the vessel "Le Glorieux", captained by the noted hydrographer Jean-Baptiste d'Après de Mannevillette, Lacaille became conscious of the difficulties in determining positions at sea. On his return to Paris he prepared the first set of tables of the Moon's position that was accurate enough to use for determining time and longitude by the method of 'Lunars' (Lunar distances) using the orbital theory of Clairaut. Lacaille was in fact an indefatigable calculator. Apart from constructing astronomical ephemerides and mathematical tables, he calculated a table of eclipses for 1800 years. Lalande said of him that, during a comparatively short life, he had made more observations and calculations than all the astronomers of his time put together. The quality of his work rivalled its quantity, while the disinterestedness and rectitude of his moral character earned him universal respect.

On his return to Paris in 1754, following a diversion to Mauritius, Lacaille was distressed to find himself an object of public attention. He resumed his work at the Mazarin College.

In 1757 he published his "Astronomiae Fundamenta Novissimus", containing a list of about 400 bright stars with positions corrected for aberration and nutation. He carried out calculations on comet orbits and was responsible for giving Halley's Comet its name. His last public lecture, given on 14 September 1761 at the Royal Academy of Sciences, summarised the improvements to astronomy that had occurred during his lifetime, to which he had made no small contribution.
His death, probably caused in part by over-work, occurred in 1762. He was buried in the vaults of the Mazarin College, now the Institut de France in Paris.

In 1754, he was elected a foreign member of the Royal Swedish Academy of Sciences. He was also an honorary member of the academies of Saint Petersburg and Berlin, the Royal Society of London and the Royal Society of Göttingen, and the Institute of Bologna. 

The crater "La Caille" on the Moon is named after him. Asteroid 9135 Lacaille (AKA 7609 P-L and 1994 EK6), discovered on 17 October 1960 by Cornelis Johannes van Houten, Ingrid van Houten-Groeneveld and Tom Gehrels at Palomar Observatory, was also named after him.

In honor of his contribution to the study of the southern hemisphere sky, a 60-cm telescope at Reunion Island will be named the Lacaille Telescope.





</doc>
