<doc id="7463" url="https://en.wikipedia.org/wiki?curid=7463" title="Cold fusion">
Cold fusion

Cold fusion is a hypothesized type of nuclear reaction that would occur at, or near, room temperature. This is compared with the "hot" fusion which takes place naturally within stars, under immense pressure and at temperatures of millions of degrees, and distinguished from muon-catalyzed fusion. There is currently no accepted theoretical model that would allow cold fusion to occur.

In 1989 Martin Fleischmann (then one of the world's leading electrochemists) and Stanley Pons reported that their apparatus had produced anomalous heat ("excess heat") of a magnitude they asserted would defy explanation except in terms of nuclear processes. They further reported measuring small amounts of nuclear reaction byproducts, including neutrons and tritium. The small tabletop experiment involved electrolysis of heavy water on the surface of a palladium (Pd) electrode. The reported results received wide media attention, and raised hopes of a cheap and abundant source of energy.

Many scientists tried to replicate the experiment with the few details available. Hopes faded due to the large number of negative replications, the withdrawal of many reported positive replications, the discovery of flaws and sources of experimental error in the original experiment, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989, most scientists considered cold fusion claims dead, and cold fusion subsequently gained a reputation as pathological science. In 1989 the United States Department of Energy (DOE) concluded that the reported results of excess heat did not present convincing evidence of a useful source of energy and decided against allocating funding specifically for cold fusion. A second DOE review in 2004, which looked at new research, reached similar conclusions and did not result in DOE funding of cold fusion.

A small community of researchers continues to investigate cold fusion, now often preferring the designation low-energy nuclear reactions (LENR) or condensed matter nuclear science (CMNS). Since cold fusion articles are rarely published in peer-reviewed mainstream scientific journals, they do not attract the level of scrutiny expected for mainstream scientific publications.
Nuclear fusion is normally understood to occur at temperatures in the tens of millions of degrees. Since the 1920s, there has been speculation that nuclear fusion might be possible at much lower temperatures by catalytically fusing hydrogen absorbed in a metal catalyst. In 1989, a claim by Stanley Pons and Martin Fleischmann (then one of the world's leading electrochemists) that such cold fusion had been observed caused a brief media sensation before the majority of scientists criticized their claim as incorrect after many found they could not replicate the excess heat. Since the initial announcement, cold fusion research has continued by a small community of researchers who believe that such reactions happen and hope to gain wider recognition for their experimental evidence.

The ability of palladium to absorb hydrogen was recognized as early as the nineteenth century by Thomas Graham. In the late 1920s, two Austrian born scientists, Friedrich Paneth and Kurt Peters, originally reported the transformation of hydrogen into helium by nuclear catalysis when hydrogen was absorbed by finely divided palladium at room temperature. However, the authors later retracted that report, saying that the helium they measured was due to background from the air.

In 1927 Swedish scientist John Tandberg reported that he had fused hydrogen into helium in an electrolytic cell with palladium electrodes. On the basis of his work, he applied for a Swedish patent for "a method to produce helium and useful reaction energy". Due to Paneth and Peters's retraction and his inability to explain the physical process, his patent application was denied. After deuterium was discovered in 1932, Tandberg continued his experiments with heavy water. The final experiments made by Tandberg with heavy water were similar to the original experiment by Fleischmann and Pons. Fleischmann and Pons were not aware of Tandberg's work.

The term "cold fusion" was used as early as 1956 in a "New York Times" article about Luis Alvarez's work on muon-catalyzed fusion. Paul Palmer and then Steven Jones of Brigham Young University used the term "cold fusion" in 1986 in an investigation of "geo-fusion", the possible existence of fusion involving hydrogen isotopes in a planetary core. In his original paper on this subject with Clinton Van Siclen, submitted in 1985, Jones had coined the term "piezonuclear fusion".

The most famous cold fusion claims were made by Stanley Pons and Martin Fleischmann in 1989. After a brief period of interest by the wider scientific community, their reports were called into question by nuclear physicists. Pons and Fleischmann never retracted their claims, but moved their research program to France after the controversy erupted.

Martin Fleischmann of the University of Southampton and Stanley Pons of the University of Utah hypothesized that the high compression ratio and mobility of deuterium that could be achieved within palladium metal using electrolysis might result in nuclear fusion. To investigate, they conducted electrolysis experiments using a palladium cathode and heavy water within a calorimeter, an insulated vessel designed to measure process heat. Current was applied continuously for many weeks, with the heavy water being renewed at intervals. Some deuterium was thought to be accumulating within the cathode, but most was allowed to bubble out of the cell, joining oxygen produced at the anode. For most of the time, the power input to the cell was equal to the calculated power leaving the cell within measurement accuracy, and the cell temperature was stable at around 30 °C. But then, at some point (in some of the experiments), the temperature rose suddenly to about 50 °C without changes in the input power. These high temperature phases would last for two days or more and would repeat several times in any given experiment once they had occurred. The calculated power leaving the cell was significantly higher than the input power during these high temperature phases. Eventually the high temperature phases would no longer occur within a particular cell.

In 1988 Fleischmann and Pons applied to the United States Department of Energy for funding towards a larger series of experiments. Up to this point they had been funding their experiments using a small device built with $100,000 out-of-pocket. The grant proposal was turned over for peer review, and one of the reviewers was Steven Jones of Brigham Young University. Jones had worked for some time on muon-catalyzed fusion, a known method of inducing nuclear fusion without high temperatures, and had written an article on the topic entitled "Cold nuclear fusion" that had been published in "Scientific American" in July 1987. Fleischmann and Pons and co-workers met with Jones and co-workers on occasion in Utah to share research and techniques. During this time, Fleischmann and Pons described their experiments as generating considerable "excess energy", in the sense that it could not be explained by chemical reactions alone. They felt that such a discovery could bear significant commercial value and would be entitled to patent protection. Jones, however, was measuring neutron flux, which was not of commercial interest. To avoid future problems, the teams appeared to agree to simultaneously publish their results, though their accounts of their 6 March meeting differ.

In mid-March 1989, both research teams were ready to publish their findings, and Fleischmann and Jones had agreed to meet at an airport on 24 March to send their papers to "Nature" via FedEx. Fleischmann and Pons, however, pressured by the University of Utah, which wanted to establish priority on the discovery, broke their apparent agreement, submitting their paper to the "Journal of Electroanalytical Chemistry" on 11 March, and disclosing their work via a press release and press conference on 23 March. Jones, upset, faxed in his paper to "Nature" after the press conference.

Fleischmann and Pons' announcement drew wide media attention. But the 1986 discovery of high-temperature superconductivity had made the scientific community more open to revelations of unexpected scientific results that could have huge economic repercussions and that could be replicated reliably even if they had not been predicted by established theories. Many scientists were also reminded of the Mössbauer effect, a process involving nuclear transitions in a solid. Its discovery 30 years earlier had also been unexpected, though it was quickly replicated and explained within the existing physics framework.

The announcement of a new purported clean source of energy came at a crucial time: adults still remembered the 1973 oil crisis and the problems caused by oil dependence, anthropogenic global warming was starting to become notorious, the anti-nuclear movement was labeling nuclear power plants as dangerous and getting them closed, people had in mind the consequences of strip mining, acid rain, the greenhouse effect and the Exxon Valdez oil spill, which happened the day after the announcement. In the press conference, Chase N. Peterson, Fleischmann and Pons, backed by the solidity of their scientific credentials, repeatedly assured the journalists that cold fusion would solve environmental problems, and would provide a limitless inexhaustible source of clean energy, using only seawater as fuel. They said the results had been confirmed dozens of times and they had no doubts about them. In the accompanying press release Fleischmann was quoted saying: "What we have done is to open the door of a new research area, our indications are that the discovery will be relatively easy to make into a usable technology for generating heat and power, but continued work is needed, first, to further understand the science and secondly, to determine its value to energy economics."

Although the experimental protocol had not been published, physicists in several countries attempted, and failed, to replicate the excess heat phenomenon. The first paper submitted to "Nature" reproducing excess heat, although it passed peer-review, was rejected because most similar experiments were negative and there were no theories that could explain a positive result; this paper was later accepted for publication by the journal "Fusion Technology". Nathan Lewis, professor of chemistry at the California Institute of Technology, led one of the most ambitious validation efforts, trying many variations on the experiment without success, while CERN physicist Douglas R. O. Morrison said that "essentially all" attempts in Western Europe had failed. Even those reporting success had difficulty reproducing Fleischmann and Pons' results. On 10 April 1989, a group at Texas A&M University published results of excess heat and later that day a group at the Georgia Institute of Technology announced neutron production—the strongest replication announced up to that point due to the detection of neutrons and the reputation of the lab. On 12 April Pons was acclaimed at an ACS meeting. But Georgia Tech retracted their announcement on 13 April, explaining that their neutron detectors gave false positives when exposed to heat. Another attempt at independent replication, headed by Robert Huggins at Stanford University, which also reported early success with a light water control, became the only scientific support for cold fusion in 26 April US Congress hearings. But when he finally presented his results he reported an excess heat of only one degree celsius, a result that could be explained by chemical differences between heavy and light water in the presence of lithium. He had not tried to measure any radiation and his research was derided by scientists who saw it later. For the next six weeks, competing claims, counterclaims, and suggested explanations kept what was referred to as "cold fusion" or "fusion confusion" in the news.

In April 1989, Fleischmann and Pons published a "preliminary note" in the "Journal of Electroanalytical Chemistry". This paper notably showed a gamma peak without its corresponding Compton edge, which indicated they had made a mistake in claiming evidence of fusion byproducts. Fleischmann and Pons replied to this critique, but the only thing left clear was that no gamma ray had been registered and that Fleischmann refused to recognize any mistakes in the data. A much longer paper published a year later went into details of calorimetry but did not include any nuclear measurements.

Nevertheless, Fleischmann and Pons and a number of other researchers who found positive results remained convinced of their findings. The University of Utah asked Congress to provide $25 million to pursue the research, and Pons was scheduled to meet with representatives of President Bush in early May.

On 30 April 1989 cold fusion was declared dead by the "New York Times". The "Times" called it a circus the same day, and the "Boston Herald" attacked cold fusion the following day.

On 1 May 1989 the American Physical Society held a session on cold fusion in Baltimore, including many reports of experiments that failed to produce evidence of cold fusion. At the end of the session, eight of the nine leading speakers stated that they considered the initial Fleischmann and Pons claim dead, with the ninth, Johann Rafelski, abstaining. Steven E. Koonin of Caltech called the Utah report a result of ""the incompetence and delusion of Pons and Fleischmann,"" which was met with a standing ovation. Douglas R. O. Morrison, a physicist representing CERN, was the first to call the episode an example of pathological science.

On 4 May, due to all this new criticism, the meetings with various representatives from Washington were cancelled.

From 8 May only the A&M tritium results kept cold fusion afloat.

In July and November 1989, "Nature" published papers critical of cold fusion claims. Negative results were also published in several other scientific journals including "Science", "Physical Review Letters", and "Physical Review C" (nuclear physics).

In August 1989, in spite of this trend, the state of Utah invested $4.5 million to create the National Cold Fusion Institute.

The United States Department of Energy organized a special panel to review cold fusion theory and research. The panel issued its report in November 1989, concluding that results as of that date did not present convincing evidence that useful sources of energy would result from the phenomena attributed to cold fusion. The panel noted the large number of failures to replicate excess heat and the greater inconsistency of reports of nuclear reaction byproducts expected by established conjecture. Nuclear fusion of the type postulated would be inconsistent with current understanding and, if verified, would require established conjecture, perhaps even theory itself, to be extended in an unexpected way. The panel was against special funding for cold fusion research, but supported modest funding of "focused experiments within the general funding system." Cold fusion supporters continued to argue that the evidence for excess heat was strong, and in September 1990 the National Cold Fusion Institute listed 92 groups of researchers from 10 different countries that had reported corroborating evidence of excess heat, but they refused to provide any evidence of their own arguing that it could endanger their patents. However, no further DOE nor NSF funding resulted from the panel's recommendation. By this point, however, academic consensus had moved decidedly toward labeling cold fusion as a kind of "pathological science".

In March 1990 Michael H. Salamon, a physicist from the University of Utah, and nine co-authors reported negative results. University faculty were then "stunned" when a lawyer representing Pons and Fleischmann demanded the Salamon paper be retracted under threat of a lawsuit. The lawyer later apologized; Fleischmann defended the threat as a legitimate reaction to alleged bias displayed by cold-fusion critics.

In early May 1990 one of the two A&M researchers, Kevin Wolf, acknowledged the possibility of spiking, but said that the most likely explanation was tritium contamination in the palladium electrodes or simply contamination due to sloppy work. In June 1990 an article in "Science" by science writer Gary Taubes destroyed the public credibility of the A&M tritium results when it accused its group leader John Bockris and one of his graduate students of spiking the cells with tritium. In October 1990 Wolf finally said that the results were explained by tritium contamination in the rods. An A&M cold fusion review panel found that the tritium evidence was not convincing and that, while they couldn't rule out spiking, contamination and measurements problems were more likely explanations, and Bockris never got support from his faculty to resume his research.

On 30 June 1991 the National Cold Fusion Institute closed after it ran out of funds; it found no excess heat, and its reports of tritium production were met with indifference.

On 1 January 1991 Pons left the University of Utah and went to Europe. In 1992, Pons and Fleischman resumed research with Toyota Motor Corporation's IMRA lab in France. Fleischmann left for England in 1995, and the contract with Pons was not renewed in 1998 after spending $40 million with no tangible results. The IMRA laboratory stopped cold fusion research in 1998 after spending £12 million. Pons has made no public declarations since, and only Fleischmann continued giving talks and publishing papers.

Mostly in the 1990s, several books were published that were critical of cold fusion research methods and the conduct of cold fusion researchers. Over the years, several books have appeared that defended them. Around 1998, the University of Utah had already dropped its research after spending over $1 million, and in the summer of 1997, Japan cut off research and closed its own lab after spending $20 million.

A 1991 review by a cold fusion proponent had calculated "about 600 scientists" were still conducting research. After 1991, cold fusion research only continued in relative obscurity, conducted by groups that had increasing difficulty securing public funding and keeping programs open. These small but committed groups of cold fusion researchers have continued to conduct experiments using Fleischmann and Pons electrolysis set-ups in spite of the rejection by the mainstream community. "The Boston Globe" estimated in 2004 that there were only 100 to 200 researchers working in the field, most suffering damage to their reputation and career. Since the main controversy over Pons and Fleischmann had ended, cold fusion research has been funded by private and small governmental scientific investment funds in the United States, Italy, Japan, and India.

Cold fusion research continues today in a few specific venues, but the wider scientific community has generally marginalized the research being done and researchers have had difficulty publishing in mainstream journals. The remaining researchers often term their field Low Energy Nuclear Reactions (LENR), Chemically Assisted Nuclear Reactions (CANR), Lattice Assisted Nuclear Reactions (LANR), Condensed Matter Nuclear Science (CMNS) or Lattice Enabled Nuclear Reactions; one of the reasons being to avoid the negative connotations associated with "cold fusion". The new names avoid making bold implications, like implying that fusion is actually occurring.

The researchers who continue acknowledge that the flaws in the original announcement are the main cause of the subject's marginalization, and they complain of a chronic lack of funding and no possibilities of getting their work published in the highest impact journals. University researchers are often unwilling to investigate cold fusion because they would be ridiculed by their colleagues and their professional careers would be at risk. In 1994, David Goodstein, a professor of physics at Caltech, advocated for increased attention from mainstream researchers and described cold fusion as:

United States Navy researchers at the Space and Naval Warfare Systems Center (SPAWAR) in San Diego have been studying cold fusion since 1989. In 2002 they released a two-volume report, "Thermal and nuclear aspects of the Pd/DO system," with a plea for funding. This and other published papers prompted a 2004 Department of Energy (DOE) review.
In August 2003, the U.S. Secretary of Energy, Spencer Abraham, ordered the DOE to organize a second review of the field. This was thanks to an April 2003 letter sent by MIT's Peter L. Hagelstein, and the publication of many new papers, including the Italian ENEA and other researchers in the 2003 International Cold Fusion Conference, and a two-volume book by U.S. SPAWAR in 2002. Cold fusion researchers were asked to present a review document of all the evidence since the 1989 review. The report was released in 2004. The reviewers were "split approximately evenly" on whether the experiments had produced energy in the form of heat, but "most reviewers, even those who accepted the evidence for excess power production, 'stated that the effects are not repeatable, the magnitude of the effect has not increased in over a decade of work, and that many of the reported experiments were not well documented.'" In summary, reviewers found that cold fusion evidence was still not convincing 15 years later, and they didn't recommend a federal research program. They only recommended that agencies consider funding individual well-thought studies in specific areas where research "could be helpful in resolving some of the controversies in the field". They summarized its conclusions thus:

Cold fusion researchers placed a "rosier spin" on the report, noting that they were finally being treated like normal scientists, and that the report had increased interest in the field and caused "a huge upswing in interest in funding cold fusion research." However, in a 2009 BBC article on an American Chemical Society's meeting on cold fusion, particle physicist Frank Close was quoted stating that the problems that plagued the original cold fusion announcement were still happening: results from studies are still not being independently verified and inexplicable phenomena encountered are being labelled as "cold fusion" even if they are not, in order to attract the attention of journalists.

In February 2012, millionaire Sidney Kimmel, convinced that cold fusion was worth investing in by a 19 April 2009 interview with physicist Robert Duncan on the US news-show "60 Minutes", made a grant of $5.5 million to the University of Missouri to establish the Sidney Kimmel Institute for Nuclear Renaissance (SKINR). The grant was intended to support research into the interactions of hydrogen with palladium, nickel or platinum under extreme conditions. In March 2013 Graham K. Hubler, a nuclear physicist who worked for the Naval Research Laboratory for 40 years, was named director. One of the SKINR projects is to replicate a 1991 experiment in which a professor associated with the project, Mark Prelas says bursts of millions of neutrons a second were recorded, which was stopped because "his research account had been frozen". He claims that the new experiment has already seen "neutron emissions at similar levels to the 1991 observation".

In May 2016, the United States House Committee on Armed Services, in its report on the 2017 National Defense Authorization Act, directed the Secretary of Defense to "provide a briefing on the military utility of recent U.S. industrial base LENR advancements to the House Committee on Armed Services by September 22, 2016."

Since the Fleischmann and Pons announcement, the Italian National agency for new technologies, energy and sustainable economic development (ENEA) has funded Franco Scaramuzzi's research into whether excess heat can be measured from metals loaded with deuterium gas. Such research is distributed across ENEA departments, CNR laboratories, INFN, universities and industrial laboratories in Italy, where the group continues to try to achieve reliable reproducibility (i.e. getting the phenomenon to happen in every cell, and inside a certain frame of time). In 2006–2007, the ENEA started a research program which claimed to have found excess power of up to 500 percent, and in 2009, ENEA hosted the 15th cold fusion conference.

Between 1992 and 1997, Japan's Ministry of International Trade and Industry sponsored a "New Hydrogen Energy (NHE)" program of US$20 million to research cold fusion. Announcing the end of the program in 1997, the director and one-time proponent of cold fusion research Hideo Ikegami stated "We couldn't achieve what was first claimed in terms of cold fusion. (...) We can't find any reason to propose more money for the coming year or for the future." In 1999 the Japan C-F Research Society was established to promote the independent research into cold fusion that continued in Japan. The society holds annual meetings. Perhaps the most famous Japanese cold fusion researcher is Yoshiaki Arata, from Osaka University, who claimed in a demonstration to produce excess heat when deuterium gas was introduced into a cell containing a mixture of palladium and zirconium oxide, a claim supported by fellow Japanese researcher Akira Kitamura of Kobe University and McKubre at SRI.

In the 1990s India stopped its research in cold fusion at the Bhabha Atomic Research Centre because of the lack of consensus among mainstream scientists and the US denunciation of the research. Yet, in 2008, the National Institute of Advanced Studies recommended that the Indian government revive this research. Projects were commenced at the Chennai's Indian Institute of Technology, the Bhabha Atomic Research Centre and the Indira Gandhi Centre for Atomic Research. However, there is still skepticism among scientists and, for all practical purposes, research has stalled since the 1990s. A special section in the Indian multidisciplinary journal "Current Science" published 33 cold fusion papers in 2015 by major cold fusion researchers including several Indian researchers.

A cold fusion experiment usually includes:

Electrolysis cells can be either open cell or closed cell. In open cell systems, the electrolysis products, which are gaseous, are allowed to leave the cell. In closed cell experiments, the products are captured, for example by catalytically recombining the products in a separate part of the experimental system. These experiments generally strive for a steady state condition, with the electrolyte being replaced periodically. There are also "heat-after-death" experiments, where the evolution of heat is monitored after the electric current is turned off.

The most basic setup of a cold fusion cell consists of two electrodes submerged in a solution containing palladium and heavy water. The electrodes are then connected to a power source to transmit electricity from one electrode to the other through the solution. Even when anomalous heat is reported, it can take weeks for it to begin to appear—this is known as the "loading time," the time required to saturate the palladium electrode with hydrogen (see "Loading ratio" section).

The Fleischmann and Pons early findings regarding helium, neutron radiation and tritium were never replicated satisfactorily, and its levels were too low for the claimed heat production and inconsistent with each other. Neutron radiation has been reported in cold fusion experiments at very low levels using different kinds of detectors, but levels were too low, close to background, and found too infrequently to provide useful information about possible nuclear processes.

An excess heat observation is based on an energy balance. Various sources of energy input and output are continuously measured. Under normal conditions, the energy input can be matched to the energy output to within experimental error. In experiments such as those run by Fleischmann and Pons, an electrolysis cell operating steadily at one temperature transitions to operating at a higher temperature with no increase in applied current. If the higher temperatures were real, and not an experimental artifact, the energy balance would show an unaccounted term. In the Fleischmann and Pons experiments, the rate of inferred excess heat generation was in the range of 10–20% of total input, though this could not be reliably replicated by most researchers. Researcher Nathan Lewis discovered that the excess heat in Fleischmann and Pons's original paper was not measured, but estimated from measurements that didn't have any excess heat.

Unable to produce excess heat or neutrons, and with positive experiments being plagued by errors and giving disparate results, most researchers declared that heat production was not a real effect and ceased working on the experiments. In 1993, after their original report, Fleischmann reported "heat-after-death" experiments—where excess heat was measured after the electric current supplied to the electrolytic cell was turned off. This type of report has also become part of subsequent cold fusion claims.

Known instances of nuclear reactions, aside from producing energy, also produce nucleons and particles on readily observable ballistic trajectories. In support of their claim that nuclear reactions took place in their electrolytic cells, Fleischmann and Pons reported a neutron flux of 4,000 neutrons per second, as well as detection of tritium. The classical branching ratio for previously known fusion reactions that produce tritium would predict, with 1 watt of power, the production of 10 neutrons per second, levels that would have been fatal to the researchers. In 2009, Mosier-Boss et al. reported what they called the first scientific report of highly energetic neutrons, using CR-39 plastic radiation detectors, but the claims cannot be validated without a quantitative analysis of neutrons.

Several medium and heavy elements like calcium, titanium, chromium, manganese, iron, cobalt, copper and zinc have been reported as detected by several researchers, like Tadahiko Mizuno or George Miley. The report presented to the United States Department of Energy (DOE) in 2004 indicated that deuterium-loaded foils could be used to detect fusion reaction products and, although the reviewers found the evidence presented to them as inconclusive, they indicated that those experiments did not use state-of-the-art techniques.

In response to doubts about the lack of nuclear products, cold fusion researchers have tried to capture and measure nuclear products correlated with excess heat. Considerable attention has been given to measuring He production. However, the reported levels are very near to background, so contamination by trace amounts of helium normally present in the air cannot be ruled out. In the report presented to the DOE in 2004, the reviewers' opinion was divided on the evidence for He; with the most negative reviews concluding that although the amounts detected were above background levels, they were very close to them and therefore could be caused by contamination from air.

One of the main criticisms of cold fusion was that deuteron-deuteron fusion into helium was expected to result in the production of gamma rays—which were not observed and were not observed in subsequent cold fusion experiments. Cold fusion researchers have since claimed to find X-rays, helium, neutrons and nuclear transmutations. Some researchers also claim to have found them using only light water and nickel cathodes. The 2004 DOE panel expressed concerns about the poor quality of the theoretical framework cold fusion proponents presented to account for the lack of gamma rays.

Researchers in the field do not agree on a theory for cold fusion. One proposal considers that hydrogen and its isotopes can be absorbed in certain solids, including palladium hydride, at high densities. This creates a high partial pressure, reducing the average separation of hydrogen isotopes. However, the reduction in separation is not enough by a factor of ten to create the fusion rates claimed in the original experiment. It was also proposed that a higher density of hydrogen inside the palladium and a lower potential barrier could raise the possibility of fusion at lower temperatures than expected from a simple application of Coulomb's law. Electron screening of the positive hydrogen nuclei by the negative electrons in the palladium lattice was suggested to the 2004 DOE commission, but the panel found the theoretical explanations not convincing and inconsistent with current physics theories.

Criticism of cold fusion claims generally take one of two forms: either pointing out the theoretical implausibility that fusion reactions have occurred in electrolysis set-ups or criticizing the excess heat measurements as being spurious, erroneous, or due to poor methodology or controls. There are a couple of reasons why known fusion reactions are an unlikely explanation for the excess heat and associated cold fusion claims.

Because nuclei are all positively charged, they strongly repel one another. Normally, in the absence of a catalyst such as a muon, very high kinetic energies are required to overcome this charged repulsion. Extrapolating from known fusion rates, the rate for uncatalyzed fusion at room-temperature energy would be 50 orders of magnitude lower than needed to account for the reported excess heat. In muon-catalyzed fusion there are more fusions because the presence of the muon causes deuterium nuclei to be 207 times closer than in ordinary deuterium gas. But deuterium nuclei inside a palladium lattice are further apart than in deuterium gas, and there should be fewer fusion reactions, not more.

Paneth and Peters in the 1920s already knew that palladium can absorb up to 900 times its own volume of hydrogen gas, storing it at several thousands of times the atmospheric pressure. This led them to believe that they could increase the nuclear fusion rate by simply loading palladium rods with hydrogen gas. Tandberg then tried the same experiment but used electrolysis to make palladium absorb more deuterium and force the deuterium further together inside the rods, thus anticipating the main elements of Fleischmann and Pons' experiment. They all hoped that pairs of hydrogen nuclei would fuse together to form helium, which at the time was needed in Germany to fill zeppelins, but no evidence of helium or of increased fusion rate was ever found.

This was also the belief of geologist Palmer, who convinced Steven Jones that the helium-3 occurring naturally in Earth perhaps came from fusion involving hydrogen isotopes inside catalysts like nickel and palladium. This led their team in 1986 to independently make the same experimental setup as Fleischmann and Pons (a palladium cathode submerged in heavy water, absorbing deuterium via electrolysis). Fleischmann and Pons had much the same belief, but they calculated the pressure to be of 10 atmospheres, when cold fusion experiments only achieve a loading ratio of one to one, which only has between 10,000 and 20,000 atmospheres. John R. Huizenga says they had misinterpreted the Nernst equation, leading them to believe that there was enough pressure to bring deuterons so close to each other that there would be spontaneous fusions.

Conventional deuteron fusion is a two-step process, in which an unstable high energy intermediary is formed:
Experiments have observed only three decay pathways for this excited-state nucleus, with the branching ratio showing the probability that any given intermediate follows a particular pathway. The products formed via these decay pathways are:
Only about one in one million of the intermediaries decay along the third pathway, making its products comparatively rare when compared to the other paths. This result is consistent with the predictions of the Bohr model. If one watt (1 eV = 1.602 x 10 joule) of nuclear power were produced from deuteron fusion consistent with known branching ratios, the resulting neutron and tritium (H) production would be easily measured. Some researchers reported detecting He but without the expected neutron or tritium production; such a result would require branching ratios strongly favouring the third pathway, with the actual rates of the first two pathways lower by at least five orders of magnitude than observations from other experiments, directly contradicting both theoretically predicted and observed branching probabilities. Those reports of He production did not include detection of gamma rays, which would require the third pathway to have been changed somehow so that gamma rays are no longer emitted.

The known rate of the decay process together with the inter-atomic spacing in a metallic crystal makes heat transfer of the 24 MeV excess energy into the host metal lattice prior to the intermediary's decay inexplicable in terms of conventional understandings of momentum and energy transfer, and even then there would be measurable levels of radiation. Also, experiments indicate that the ratios of deuterium fusion remain constant at different energies. In general, pressure and chemical environment only cause small changes to fusion ratios. An early explanation invoked the Oppenheimer–Phillips process at low energies, but its magnitude was too small to explain the altered ratios.

Cold fusion setups utilize an input power source (to ostensibly provide activation energy), a platinum group electrode, a deuterium or hydrogen source, a calorimeter, and, at times, detectors to look for byproducts such as helium or neutrons. Critics have variously taken issue with each of these aspects and have asserted that there has not yet been a consistent reproduction of claimed cold fusion results in either energy output or byproducts. Some cold fusion researchers who claim that they can consistently measure an excess heat effect have argued that the apparent lack of reproducibility might be attributable to a lack of quality control in the electrode metal or the amount of hydrogen or deuterium loaded in the system. Critics have further taken issue with what they describe as mistakes or errors of interpretation that cold fusion researchers have made in calorimetry analyses and energy budgets.

In 1989, after Fleischmann and Pons had made their claims, many research groups tried to reproduce the Fleischmann-Pons experiment, without success. A few other research groups, however, reported successful reproductions of cold fusion during this time. In July 1989, an Indian group from the Bhabha Atomic Research Centre (P. K. Iyengar and M. Srinivasan) and in October 1989, John Bockris' group from Texas A&M University reported on the creation of tritium. In December 1990, professor Richard Oriani of the University of Minnesota reported excess heat.

Groups that did report successes found that some of their cells were producing the effect, while other cells that were built exactly the same and used the same materials were not producing the effect. Researchers that continued to work on the topic have claimed that over the years many successful replications have been made, but still have problems getting reliable replications. Reproducibility is one of the main principles of the scientific method, and its lack led most physicists to believe that the few positive reports could be attributed to experimental error. The DOE 2004 report said among its conclusions and recommendations:

Cold fusion researchers (McKubre since 1994, ENEA in 2011) have speculated that a cell that is loaded with a deuterium/palladium ratio lower than 100% (or 1:1) will not produce excess heat. Since most of the negative replications from 1989–1990 did not report their ratios, this has been proposed as an explanation for failed replications. This loading ratio is hard to obtain, and some batches of palladium never reach it because the pressure causes cracks in the palladium, allowing the deuterium to escape. Fleischmann and Pons never disclosed the deuterium/palladium ratio achieved in their cells, there are no longer any batches of the palladium used by Fleischmann and Pons (because the supplier uses now a different manufacturing process), and researchers still have problems finding batches of palladium that achieve heat production reliably.

Some research groups initially reported that they had replicated the Fleischmann and Pons results but later retracted their reports and offered an alternative explanation for their original positive results. A group at Georgia Tech found problems with their neutron detector, and Texas A&M discovered bad wiring in their thermometers. These retractions, combined with negative results from some famous laboratories, led most scientists to conclude, as early as 1989, that no positive result should be attributed to cold fusion.

The calculation of excess heat in electrochemical cells involves certain assumptions. Errors in these assumptions have been offered as non-nuclear explanations for excess heat.

One assumption made by Fleischmann and Pons is that the efficiency of electrolysis is nearly 100%, meaning nearly all the electricity applied to the cell resulted in electrolysis of water, with negligible resistive heating and substantially all the electrolysis product leaving the cell unchanged. This assumption gives the amount of energy expended converting liquid DO into gaseous D and O. The efficiency of electrolysis is less than one if hydrogen and oxygen recombine to a significant extent within the calorimeter. Several researchers have described potential mechanisms by which this process could occur and thereby account for excess heat in electrolysis experiments.

Another assumption is that heat loss from the calorimeter maintains the same relationship with measured temperature as found when calibrating the calorimeter. This assumption ceases to be accurate if the temperature distribution within the cell becomes significantly altered from the condition under which calibration measurements were made. This can happen, for example, if fluid circulation within the cell becomes significantly altered. Recombination of hydrogen and oxygen within the calorimeter would also alter the heat distribution and invalidate the calibration.

The ISI identified cold fusion as the scientific topic with the largest number of published papers in 1989, of all scientific disciplines. The Nobel Laureate Julian Schwinger declared himself a supporter of cold fusion in the fall of 1989, after much of the response to the initial reports had turned negative. He tried to publish his theoretical paper "Cold Fusion: A Hypothesis" in "Physical Review Letters", but the peer reviewers rejected it so harshly that he felt deeply insulted, and he resigned from the American Physical Society (publisher of "PRL") in protest.

The number of papers sharply declined after 1990 because of two simultaneous phenomena: scientists abandoning the field and journal editors declining to review new papers, and cold fusion fell off the ISI charts. Researchers who got negative results abandoned the field, while others kept publishing. A 1993 paper in "Physics Letters A" was the last paper published by Fleischmann, and "one of the last reports [by Fleischmann] to be formally challenged on technical grounds by a cold fusion skeptic".

The "Journal of Fusion Technology" (FT) established a permanent feature in 1990 for cold fusion papers, publishing over a dozen papers per year and giving a mainstream outlet for cold fusion researchers. When editor-in-chief George H. Miley retired in 2001, the journal stopped accepting new cold fusion papers. This has been cited as an example of the importance of sympathetic influential individuals to the publication of cold fusion papers in certain journals.

The decline of publications in cold fusion has been described as a "failed information epidemic". The sudden surge of supporters until roughly 50% of scientists support the theory, followed by a decline until there is only a very small number of supporters, has been described as a characteristic of pathological science. The lack of a shared set of unifying concepts and techniques has prevented the creation of a dense network of collaboration in the field; researchers perform efforts in their own and in disparate directions, making the transition to "normal" science more difficult.

Cold fusion reports continued to be published in a small cluster of specialized journals like "Journal of Electroanalytical Chemistry" and "Il Nuovo Cimento". Some papers also appeared in "Journal of Physical Chemistry", "Physics Letters A", "International Journal of Hydrogen Energy", and a number of Japanese and Russian journals of physics, chemistry, and engineering. Since 2005, "Naturwissenschaften" has published cold fusion papers; in 2009, the journal named a cold fusion researcher to its editorial board. In 2015 the Indian multidisciplinary journal "Current Science" published a special section devoted entirely to cold fusion related papers.

In the 1990s, the groups that continued to research cold fusion and their supporters established (non-peer-reviewed) periodicals such as "Fusion Facts", "Cold Fusion Magazine", "Infinite Energy Magazine" and "New Energy Times" to cover developments in cold fusion and other fringe claims in energy production that were ignored in other venues. The internet has also become a major means of communication and self-publication for CF researchers.

Cold fusion researchers were for many years unable to get papers accepted at scientific meetings, prompting the creation of their own conferences. The first International Conference on Cold Fusion (ICCF) was held in 1990, and has met every 12 to 18 months since. Attendees at some of the early conferences were described as offering no criticism to papers and presentations for fear of giving ammunition to external critics; thus allowing the proliferation of crackpots and hampering the conduct of serious science. Critics and skeptics stopped attending these conferences, with the notable exception of Douglas Morrison, who died in 2001. With the founding in 2004 of the International Society for Condensed Matter Nuclear Science (ISCMNS), the conference was renamed the International Conference on Condensed Matter Nuclear Science (the reasons are explained in the subsequent research section), but reverted to the old name in 2008. Cold fusion research is often referenced by proponents as "low-energy nuclear reactions", or LENR, but according to sociologist Bart Simon the "cold fusion" label continues to serve a social function in creating a collective identity for the field.

Since 2006, the American Physical Society (APS) has included cold fusion sessions at their semiannual meetings, clarifying that this does not imply a softening of skepticism. Since 2007, the American Chemical Society (ACS) meetings also include "invited symposium(s)" on cold fusion. An ACS program chair said that without a proper forum the matter would never be discussed and, "with the world facing an energy crisis, it is worth exploring all possibilities."

On 22–25 March 2009, the American Chemical Society meeting included a four-day symposium in conjunction with the 20th anniversary of the announcement of cold fusion. Researchers working at the U.S. Navy's Space and Naval Warfare Systems Center (SPAWAR) reported detection of energetic neutrons using a heavy water electrolysis set-up and a CR-39 detector, a result previously published in "Naturwissenschaften". The authors claim that these neutrons are indicative of nuclear reactions; without quantitative analysis of the number, energy, and timing of the neutrons and exclusion of other potential sources, this interpretation is unlikely to find acceptance by the wider scientific community.

Although details have not surfaced, it appears that the University of Utah forced the 23 March 1989 Fleischmann and Pons announcement to establish priority over the discovery and its patents before the joint publication with Jones. The Massachusetts Institute of Technology (MIT) announced on 12 April 1989 that it had applied for its own patents based on theoretical work of one of its researchers, Peter L. Hagelstein, who had been sending papers to journals from the 5 to 12 April. On 2 December 1993 the University of Utah licensed all its cold fusion patents to ENECO, a new company created to profit from cold fusion discoveries, and in March 1998 it said that it would no longer defend its patents.

The U.S. Patent and Trademark Office (USPTO) now rejects patents claiming cold fusion. Esther Kepplinger, the deputy commissioner of patents in 2004, said that this was done using the same argument as with perpetual motion machines: that they do not work. Patent applications are required to show that the invention is "useful", and this utility is dependent on the invention's ability to function. In general USPTO rejections on the sole grounds of the invention's being "inoperative" are rare, since such rejections need to demonstrate "proof of total incapacity", and cases where those rejections are upheld in a Federal Court are even rarer: nevertheless, in 2000, a rejection of a cold fusion patent was appealed in a Federal Court and it was upheld, in part on the grounds that the inventor was unable to establish the utility of the invention.

A U.S. patent might still be granted when given a different name to disassociate it from cold fusion, though this strategy has had little success in the US: the same claims that need to be patented can identify it with cold fusion, and most of these patents cannot avoid mentioning Fleischmann and Pons' research due to legal constraints, thus alerting the patent reviewer that it is a cold-fusion-related patent. David Voss said in 1999 that some patents that closely resemble cold fusion processes, and that use materials used in cold fusion, have been granted by the USPTO. The inventor of three such patents had his applications initially rejected when they were reviewed by experts in nuclear science; but then he rewrote the patents to focus more in the electrochemical parts so they would be reviewed instead by experts in electrochemistry, who approved them. When asked about the resemblance to cold fusion, the patent holder said that it used nuclear processes involving "new nuclear physics" unrelated to cold fusion. Melvin Miles was granted in 2004 a patent for a cold fusion device, and in 2007 he described his efforts to remove all instances of "cold fusion" from the patent description to avoid having it rejected outright.

At least one patent related to cold fusion has been granted by the European Patent Office.

A patent only legally prevents others from using or benefiting from one's invention. However, the general public perceives a patent as a stamp of approval, and a holder of three cold fusion patents said the patents were very valuable and had helped in getting investments.

In "Undead Science", sociologist Bart Simon gives some examples of cold fusion in popular culture, saying that some scientists use cold fusion as a synonym for outrageous claims made with no supporting proof, and courses of ethics in science give it as an example of pathological science. It has appeared as a joke in "Murphy Brown" and "The Simpsons". It was adopted as a software product name Adobe ColdFusion and a brand of protein bars (Cold Fusion Foods). It has also appeared in advertising as a synonym for impossible science, for example a 1995 advertisement for Pepsi Max.

The plot of "The Saint", a 1997 action-adventure film, parallels the story of Fleischmann and Pons, although with a different ending. The film might have affected the public perception of cold fusion, pushing it further into the science fiction realm.

"Final Exam", the 16th episode of season 4 of "The Outer Limits", depicts a student named Todtman who has invented a cold fusion weapon, and attempts to use it as a tool for revenge on people who have wronged him over the years. Despite the secret being lost with his death at the end of the episode, it is implied that another student elsewhere is on a similar track, and may well repeat Todtman's efforts.

In the "DC's Legends of Tomorrow" episode "No Country for Old Dads," Ray Palmer theorizes that cold fusion could repair the shattered Fire Totem, if it wasn't only theoretical. Damien Dahrk reveals that he assassinated a scientist in 1962 East Germany that developed a formula for cold fusion. Ray and Dahrk's daughter Nora time travel from 2018 to 1962 in an attempt to rescue the scientist from the younger version of Dahrk and/or retrieve the formula.



</doc>
<doc id="7466" url="https://en.wikipedia.org/wiki?curid=7466" title="Coal tar">
Coal tar

Coal tar is a thick dark liquid which is a by-product of the production of coke and coal gas from coal. It has both medical and industrial uses. It may be applied to the affected area to treat psoriasis and seborrheic dermatitis (dandruff). It may be used in combination with ultraviolet light therapy. Industrially it is a railway tie preservative and used in the surfacing of roads.
Side effects include skin irritation, sun sensitivity, allergic reactions, and skin discoloration. It is unclear if use during pregnancy is safe for the baby and use during breastfeeding is not typically recommended. The exact mechanism of action is unknown. It is a complex mixture of phenols, polycyclic aromatic hydrocarbons (PAHs), and heterocyclic compounds. It demonstrates antifungal, anti-inflammatory, anti-itch, and antiparasitic properties.
Coal tar was discovered around 1665 and used for medical purposes as early as the 1800s. It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. Coal tar is available as a generic medication and over the counter. In the United Kingdom 125 ml of 5% shampoo costs the NHS about £1.89. In the United States a month of treatment costs less than $25 USD. Coal-tar was one of the key starting materials for the early pharmaceutical industry.

Coal tar may be used in two forms: crude coal tar () or a coal tar solution () also known as liquor carbonis detergens (LCD).

Coal tar is used in medicated shampoo, soap and ointment, as a treatment for dandruff and psoriasis, and to kill and repel head lice. When used as a medication in the U.S., coal tar preparations are considered over-the-counter drug pharmaceuticals and are subject to regulation by the FDA. Named brands include Denorex, Balnetar, Psoriasin, Tegrin, T/Gel, and Neutar. When used in the extemporaneous preparation of topical medications, it is supplied in the form of coal tar topical solution USP, which consists of a 20% w/v solution of coal tar in alcohol, with an additional 5% w/v of polysorbate 80 USP; this must then be diluted in an ointment base such as petrolatum.

Pine tar has historically also been used for this purpose. Though it is frequently cited online as having been banned as a medical product by the FDA due to a "lack of evidence having been submitted for proof of effectiveness", pine tar is included in the Code of Federal Regulations, subchapter D: Drugs for Human Use, as an OTC treatment for "Dandruff/seborrheic dermatitis/psoriasis".

Various phenolic coal tar derivatives have analgesic (pain-killer) properties. These included acetanilide, phenacetin, and paracetamol (acetaminophen). Paracetamol is the only coal-tar derived analgesic still in use today, but industrial phenol is now usually synthesized from crude oil rather than coal tar.

Coal tar was a component of the first sealed roads. In its original development by Edgar Purnell Hooley, tarmac was tar covered with granite chips. Later the filler used was industrial slag. Today, petroleum derived binders and sealers are more commonly used. These sealers are used to extend the life and reduce maintenance cost associated with asphalt pavements, primarily in asphalt road paving, car parks and walkways.

Coal tar is incorporated into some parking-lot sealcoat products used to protect the structural integrity of the underlying pavement. Sealcoat products that are coal-tar based typically contain 20 to 35 percent coal-tar pitch. Research shows it is used in United States states from Alaska to Florida, but several areas have banned its use in sealcoat products,

Being flammable, coal tar is sometimes used for heating or to fire boilers. Like most heavy oils, it must be heated before it will flow easily.

A large part of the binder used in the graphite industry for making "green blocks" is coke oven volatiles (COV), a considerable portion of which is coal tar. During the baking process of the green blocks as a part of commercial graphite production, most of the coal tar binders are vaporised and are generally burned in an incinerator to prevent release into the atmosphere, as COV and coal tar can be injurious to health.

Coal tar is also used to manufacture paints, synthetic dyes (notably tartrazine/Yellow #5), and photographic materials.

In the coal gas era, there were many companies in Britain whose business was to distill coal tar to separate the higher-value fractions, such as naphtha, creosote and pitch. A great many industrial chemicals were first isolated from coal tar during this time. These companies included:


According to the National Psoriasis Foundation, coal tar is a valuable, safe and inexpensive treatment option for millions of people with psoriasis and other scalp or skin conditions. According to the FDA, coal tar concentrations between 0.5% and 5% are considered safe and effective for psoriasis.

Evidence is inconclusive whether the coal tar in the concentrations seen in non-prescription treatments causes cancer, because there is insufficient data to make a judgment. While coal tar consistently causes cancer in animal studies, short-term treatments of humans have shown no significant increase in rates of cancer. It's possible that the skin can repair itself after short-term exposure to PAHs, but not after long-term exposure.

Coal tar was one of the first chemical substances proven to cause cancer from occupational exposure, during research in 1775 on the cause of chimney sweeps' carcinoma. Modern studies have shown that working with coal tar pitch, such as during the paving of roads or when working on roofs, increases the risk of cancer.

Coal tar contains many polycyclic aromatic hydrocarbons, and it is believed that their metabolites bind to DNA, damaging it. Long-term skin exposure to these compounds can produce "tar warts", which can progress to squamous cell carcinoma.

The International Agency for Research on Cancer lists coal tars as Group 1 carcinogens, meaning they directly cause cancer. Both the U.S. Department of Health and Human Services and the state of California list coal tars as known human carcinogens.

Coal tar causes increased sensitivity to sunlight, so skin treated with topical coal tar preparations should be protected from sunlight.

The residue from the distillation of high-temperature coal tar, primarily a complex mixture of three or more membered condensed ring aromatic hydrocarbons, was listed on 28 October 2008 as a substance of very high concern by the European Chemicals Agency.

It is a keratolytic agent, which reduces the growth rate of skin cells and softens the skin's keratin.

Coal tar is produced through thermal destruction (pyrolysis) of coal. Its composition varies with the process and type of coal used – lignite, bituminous or anthracite.

Coal tar contains approximately 10,000 chemicals, of which only about 50% have been identified. Components include polycyclic aromatic hydrocarbons (4-rings: chrysene, fluoranthene, pyrene, triphenylene, naphthacene, benzanthracene, 5-rings: picene, benzo[a]pyrene, benzo[e]pyrene, benzofluoranthenes, perylene, 6-rings: dibenzopyrenes, dibenzofluoranthenes, benzoperylenes, 7-rings: coronene), as well as methylated and polymethylated derivatives, mono- and polyhydroxylated derivatives, and heterocyclic compounds. Others include benzene, toluene, xylenes, cumenes, coumarone, indene, benzofuran, naphthalene and methyl-naphthalenes, acenaphthene, fluorene, phenol, cresols, pyridine, picolines, phenanthracene, carbazole, quinolines, fluoranthene. Many of these constituents are known carcinogens.

Exposure to coal tar pitch volatiles can occur in the workplace by breathing, skin contact, or eye contact. The Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit) to 0.2 mg/m benzene-soluble fraction over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.1 mg/m cyclohexane-extractable fraction over an 8-hour workday. At levels of 80 mg/m, coal tar pitch volatiles are immediately dangerous to life and health.




</doc>
<doc id="7467" url="https://en.wikipedia.org/wiki?curid=7467" title="Cobbler">
Cobbler

Cobbler(s) may refer to:








</doc>
<doc id="7471" url="https://en.wikipedia.org/wiki?curid=7471" title="Catherine of Siena">
Catherine of Siena

Saint Catherine of Siena (25 March 1347 in Siena – 29 April 1380 in Rome), was a tertiary of the Dominican Order and a Scholastic philosopher and theologian who had a great influence on the Catholic Church. She is declared a saint and a doctor of the Church.

Born in Siena, she grew up there and wanted very soon to devote herself to God, against the will of her parents. She joined the Sisters of the Penance of St. Dominic and made her vows. She made herself known very quickly by being marked by mystical phenomena such as stigmata and mystical marriage.

She accompanied the chaplain of the Dominicans to the pope in Avignon, as ambassador of Florence, then at war against the pope. Her influence with Pope Gregory XI played a role in his decision to leave Avignon for Rome. She was then sent by him to negotiate peace with Florence. After Gregory XI's death and peace concluded, she returned to Siena. She dictated to secretaries her set of spiritual treatises "The Dialogue of Divine Providence".

The Great Schism of the West led Catherine of Siena to go to Rome with the pope. She sent numerous letters to princes and cardinals to promote obedience to Pope Urban VI and defend what she calls the "vessel of the Church." She died on 29 April 1380, exhausted by her penances. Urban VI celebrated her funeral and burial in the Basilica of Santa Maria sopra Minerva in Rome.

The devotion around Catherine of Siena developed rapidly after her death. She was canonized in 1461, declared patron saint of Rome in 1866, and of Italy in 1939. First woman declared "doctor of the Church" on 4 October 1970 by Pope Paul VI with Teresa of Ávila, she was proclaimed patron saint of Europe in 1999 by Pope John Paul II. She is also the patron saint of journalists, media, and all communication professions, because of her epistolary work for the papacy.

Catherine of Siena is one of the outstanding figures of medieval Catholicism, by the strong influence she has had in the history of the papacy. She is behind the return of the Pope from Avignon to Rome, and then carried out many missions entrusted by the pope, something quite rare for a simple nun in the Middle Ages. 

Her writings—and especially "The Dialogue", her major work which includes a set of treatises she would have dictated during ecstasies—mark theological thought. She is one of the most influential writers in Catholicism, to the point that she is one of only four women to be declared a doctor of the Church. This recognition by the Church consecrates the importance of her writings.

Since 18 June 1939, Catherine of Siena has been one of the two patron saints of Italy, together with Francis of Assisi. On 4 October 1970, she was proclaimed a Doctor of the Church by Pope Paul VI, and on 1 October 1999, Pope John Paul II named her as one of the six patron saints of Europe, together with Benedict of Nursia, Saints Cyril and Methodius, Bridget of Sweden and Edith Stein.

Caterina di Giacomo di Benincasa was born on 25 March 1347 in Black Death-ravaged Siena, Italy, to Lapa Piagenti, the daughter of a local poet, and Giacomo di Benincasa, a cloth dyer who ran his enterprise with the help of his sons. The house where Catherine grew up is still in existence. Lapa was about forty years old when she gave premature birth to twin daughters Catherine and Giovanna. She had already borne 22 children, but half of them had died. Giovanna was handed over to a wet-nurse and died soon after. Catherine was nursed by her mother and developed into a healthy child. She was two years old when Lapa had her 25th child, another daughter named Giovanna. As a child Catherine was so merry that the family gave her the pet name of "Euphrosyne", which is Greek for "joy" and the name of an early Christian saint.

Catherine is said by her confessor and biographer Raymond of Capua O.P.'s "Life" to have had her first vision of Christ when she was five or six years old: She and a brother were on the way home from visiting a married sister when she is said to have experienced a vision of Christ seated in glory with the Apostles Peter, Paul, and John. Raymond continues that at age seven, Catherine vowed to give her whole life to God.

When Catherine was sixteen, her older sister Bonaventura died in childbirth; already anguished by this, Catherine soon learned that her parents wanted her to marry Bonaventura's widower. She was absolutely opposed and started a strict fast. She had learned this from Bonaventura, whose husband had been far from considerate but his wife had changed his attitude by refusing to eat until he showed better manners. Besides fasting, Catherine further disappointed her mother by cutting off her long hair as a protest against being overly encouraged to improve her appearance to attract a husband.
Catherine would later advise Raymond of Capua to do during times of trouble what she did now as a teenager: "Build a cell inside your mind, from which you can never flee." In this inner cell she made her father into a representation of Christ, her mother into the Blessed Virgin Mary, and her brothers into the apostles. Serving them humbly became an opportunity for spiritual growth. Catherine resisted the accepted course of marriage and motherhood on the one hand, or a nun's veil on the other. She chose to live an active and prayerful life outside a convent's walls following the model of the Dominicans. Eventually her father gave up and permitted her to live as she pleased.

A vision of Saint Dominic gave strength to Catherine, but her wish to join his Order was no comfort to Lapa, who took her daughter with her to the baths in Bagno Vignoni to improve her health. Catherine fell seriously ill with a violent rash, fever and pain, which conveniently made her mother accept her wish to join the "Mantellate", the local association of Dominican tertiaries. Lapa went to the Sisters of the Order and persuaded them to take in her daughter. Within days, Catherine seemed entirely restored, rose from bed and donned the black and white habit of the Third Order of Saint Dominic. Catherine received the habit of a Dominican tertiary from the friars of the order after vigorous protests from the tertiaries themselves, who up to that point had been only widows. As a tertiary, she lived outside the convent, at home with her family like before. The Mantellate taught Catherine how to read, and she lived in almost total silence and solitude in the family home.

Her custom of giving away clothing and food without asking anyone's permission cost her family significantly, but she requested nothing for herself. By staying in their midst, she could live out her rejection of them more strongly. She did not want their food, referring to the table laid for her in Heaven with her real family. 
According to Raymond of Capua, at the age of twenty-one (c. 1368), Catherine experienced what she described in her letters as a "Mystical Marriage" with Jesus, later a popular subject in art as the "Mystic marriage of Saint Catherine". Caroline Walker Bynum explains one surprising and controversial aspect of this marriage that occurs both in artistic representations of the event and in some early accounts of her life: "Underlining the extent to which the marriage was a fusion with Christ's physicality [...] Catherine received, not the ring of gold and jewels that her biographer reports in his bowdlerized version, but the ring of Christ's foreskin." Catherine herself mentions the foreskin-as-wedding ring motif in one of her letters (#221), equating the wedding ring of a virgin with a foreskin; she typically claimed that her own wedding ring to Christ was simply invisible. Raymond of Capua also records that she was told by Christ to leave her withdrawn life and enter the public life of the world. Catherine rejoined her family and began helping the ill and the poor, where she took care of them in hospitals or homes. Her early pious activities in Siena attracted a group of followers, women and men, who gathered around her.

As social and political tensions mounted in Siena, Catherine found herself drawn to intervene in wider politics. She made her first journey to Florence in 1374, probably to be interviewed by the Dominican authorities at the General Chapter held in Florence in May 1374, though this is controverted (if she was interviewed, then the absence of later evidence suggests she was deemed sufficiently orthodox). It seems that at this time she acquired Raymond of Capua as her confessor and spiritual director.

After this visit, she began travelling with her followers throughout northern and central Italy advocating reform of the clergy and advising people that repentance and renewal could be done through "the total love for God." In Pisa, in 1375, she used what influence she had to sway that city and Lucca away from alliance with the anti-papal league whose force was gaining momentum and strength. She also lent her enthusiasm towards promoting the launch of a new crusade. It was in Pisa in 1375 that, according to Raymond of Capua's biography, she received the stigmata (visible, at Catherine's request, only to herself).

Physical travel was not the only way in which Catherine made her views known. From 1375 onwards, she began dictating letters to scribes. These letters were intended to reach men and women of her circle, increasingly widening her audience to include figures in authority as she begged for peace between the republics and principalities of Italy and for the return of the Papacy from Avignon to Rome. She carried on a long correspondence with Pope Gregory XI, asking him to reform the clergy and the administration of the Papal States.

Towards the end of 1375, she returned to Siena, to assist a young political prisoner, Niccolò di Tuldo, at his execution. In June 1376 Catherine went to Avignon as ambassador of the Republic of Florence to make peace with the Papal States (on 31 March 1376 Gregory XI had placed Florence under interdict). She was unsuccessful and was disowned by the Florentine leaders, who sent ambassadors to negotiate on their own terms as soon as Catherine's work had paved the way for them. Catherine sent an appropriately scorching letter back to Florence in response. While in Avignon, Catherine also tried to convince Pope Gregory XI, the last Avignon Pope, to return to Rome. Gregory did indeed return his administration to Rome in January 1377; to what extent this was due to Catherine's influence is a topic of much modern debate.

Catherine returned to Siena and spent the early months of 1377 founding a women's monastery of strict observance outside the city in the old fortress of Belcaro. She spent the rest of 1377 at Rocca d'Orcia, about twenty miles from Siena, on a local mission of peace-making and preaching. During this period, in autumn 1377, she had the experience which led to the writing of her "Dialogue" and learned to write, although she still seems to have chiefly relied upon her secretaries for her correspondence.

Late in 1377 or early in 1378 Catherine again travelled to Florence, at the order of Gregory XI, to seek peace between Florence and Rome. Following Gregory's death in March 1378 riots, the revolts of the Ciompi, broke out in Florence on 18 June, and in the ensuing violence she was nearly assassinated. Eventually, in July 1378, peace was agreed between Florence and Rome; Catherine returned quietly to Florence.

In late November 1378, with the outbreak of the Western Schism, the new Pope, Urban VI, summoned her to Rome. She stayed at Pope Urban VI's court and tried to convince nobles and cardinals of his legitimacy, both meeting with individuals at court and writing letters to persuade others.

For many years she had accustomed herself to a rigorous abstinence. She received the Holy Eucharist almost daily. This extreme fasting appeared unhealthy in the eyes of the clergy and her own sisterhood. Her confessor, Blessed Raymond, ordered her to eat properly. But Catherine claimed that she was unable to, describing her inability to eat as an "infermità" (illness). From the beginning of 1380, Catherine could neither eat nor swallow water. On February 26 she lost the use of her legs.

Catherine died in Rome, on 29 April 1380, at the age of thirty-three, having eight days earlier suffered a massive stroke which paralyzed her from the waist down. Her last words were, "Father, into Your Hands I commend my soul and my spirit."

There is some internal evidence of Catherine's personality, teaching and work in her nearly four hundred letters, her "Dialogue", and her prayers.

Much detail about her life has also, however, been drawn from the various sources written shortly after her death in order to promote her cult and canonisation. Though much of this material is heavily hagiographic, it has been an important source for historians seeking to reconstruct Catherine's life. Various sources are particularly important, especially the works of Raymond of Capua, who was Catherine's spiritual director and close friend from 1374 until her death, and himself became Master General of the Order in 1380. Raymond began writing what is known as the "Legenda Major", his "Life" of Catherine, in 1384, and completed it in 1395.

Another important work written after Catherine's death was "Libellus de Supplemento" ("Little Supplement Book"), written between 1412 and 1418 by Tommaso d'Antonio Nacci da Siena (commonly called Thomas of Siena, or Tommaso Caffarini): the work is an expansion of Raymond's "Legenda Major" making heavy use of the notes of Catherine's first confessor, Tommaso della Fonte (notes that do not survive anywhere else). Caffarini later published a more compact account of Catherine's life, entitled the "Legenda Minor".

From 1411 onwards, Caffarini also co-ordinated the compiling of the "Processus" of Venice, the set of documents submitted as part of the process of canonisation of Catherine, which provides testimony from nearly all of Catherine's disciples. There is also an anonymous piece entitled "Miracoli della Beata Caterina" ("Miracle of Blessed Catherine"), written by an anonymous Florentine. A few other relevant pieces survive.

Three genres of work by Catherine survive:

Catherine's theology can be described as mystical, and was employed towards practical ends for her own spiritual life or those of others. She used the language of medieval scholastic philosophy to elaborate her experiential mysticism. Interested mainly with achieving an incorporeal union with God, Catherine practiced extreme fasting and asceticism, eventually to the extent of living solely off the Eucharist every day. For Catherine, this practice was the means to realize fully her love of Christ in her mystical experience, with a large proportion of her ecstatic visions relating to the consumption or rejection of food during her life. She viewed Christ as a "bridge" between the soul and God and transmitted that idea, along with her other teachings, in her book The Dialogue. The Dialogue is highly systematic and explanatory in its presentation of her mystical ideas; however, these ideas themselves are not so much based in reason or logic as they are based in her ecstatic mystical experience.

She was buried in the (Roman) cemetery of Santa Maria sopra Minerva which lies near the Pantheon. After miracles were reported to take place at her grave, Raymond moved her inside the Basilica of Santa Maria sopra Minerva, where she lies to this day.

Her head however, was parted from her body and inserted in a gilt bust of bronze. This bust was later taken to Siena, and carried through that city in a procession to the Dominican church. Behind the bust walked Lapa, Catherine's mother, who lived until she was 89 years old. By then she had seen the end of the wealth and the happiness of her family, and followed most of her children and several of her grandchildren to the grave. She helped Raymond of Capua write his biography of her daughter, and said, "I think God has laid my soul athwart in my body, so that it can't get out." The incorrupt head and thumb were entombed in the Basilica of San Domenico at Siena, where they remain.

Pope Pius II, himself from Siena, canonized Catherine on 29 June 1461.

On 4 October 1970, Pope Paul VI named Catherine a Doctor of the Church; this title was almost simultaneously given to Saint Teresa of Ávila (27 September 1970), making them the first women to receive this honour.

Initially however, her feast day was not included in the General Roman Calendar. When it was added in 1597, it was put on the day of her death, April 29; however, because this conflicted with the feast of Saint Peter of Verona which also fell on the 29th of April, Catherine's feast day was moved in 1628 to the new date of April 30. In the 1969 revision of the calendar, it was decided to leave the celebration of the feast of St Peter of Verona to local calendars, because he was not as well known worldwide, and Catherine's feast was restored to April 29.

In his decree of 13 April 1866, Pope Pius IX declared Catherine of Siena to be a co-patroness of Rome. On 18 June 1939 Pope Pius XII named her a joint patron saint of Italy along with Saint Francis of Assisi.

On 1 October 1999, Pope John Paul II made her one of Europe's patron saints, along with Edith Stein and Bridget of Sweden. She is also the patroness of the historically Catholic American woman's fraternity, Theta Phi Alpha.

The people of Siena wished to have Catherine's body. A story is told of a miracle whereby they were partially successful: knowing that they could not smuggle her whole body out of Rome, they decided to take only her head which they placed in a bag. When stopped by the Roman guards, they prayed to Catherine to help them, confident that she would rather have her body (or at least part thereof) in Siena. When they opened the bag to show the guards, it appeared no longer to hold her head but to be full of rose petals. Once back at Siena, as they reopened the bag her head was visible once more. Due to this story, Catherine is often seen holding a lily.

Catherine ranks high among the mystics and spiritual writers of the Church. She remains a greatly respected figure for her spiritual writings, and political boldness to "speak truth to power"—it being exceptional for a woman, in her time period, to have had such influence in politics and on world history.

The main churches in honor of Catherine of Siena are:





English translations of The "Dialogue" include:

The Letters are translated into English as:

The Prayers are translated into English as:

Raymond of Capua's "Life" was translated into English in 1493 and 1609, and in Modern English is translated as:




</doc>
<doc id="7472" url="https://en.wikipedia.org/wiki?curid=7472" title="Charles Lyell">
Charles Lyell

Sir Charles Lyell, 1st Baronet, (14 November 1797 – 22 February 1875) was a Scottish geologist who popularised the revolutionary work of James Hutton. He is best known as the author of "Principles of Geology", which presented uniformitarianism–the idea that the Earth was shaped by the same scientific processes still in operation today–to the broad general public. "Principles of Geology" also challenged theories popularised by Georges Cuvier, which were the most accepted and circulated ideas about geology in Europe at the time.

His scientific contributions included an explanation of earthquakes, the theory of gradual "backed up-building" of volcanoes, and in stratigraphy the division of the Tertiary period into the Pliocene, Miocene, and Eocene. He also coined the currently-used names for geological eras, Palaeozoic, Mesozoic and Cenozoic. He incorrectly conjectured that icebergs may be the emphasis behind the transport of glacial erratics, and that silty loess deposits might have settled out of flood waters.

Lyell, following deistic traditions, favoured an indefinitely long age for the earth, despite geological evidence suggesting an old but finite age. He was a close friend of Charles Darwin, and contributed significantly to Darwin's thinking on the processes involved in evolution. He helped to arrange the simultaneous publication in 1858 of papers by Darwin and Alfred Russel Wallace on natural selection, despite his personal religious qualms about the theory. He later published evidence from geology of the time man had existed on Earth.

Lyell was born into a wealthy family, on 14 November 1797, at the family's estate house, Kinnordy House, near Kirriemuir in Forfarshire. He was the eldest of ten children. Lyell's father, also named Charles Lyell, was noted as a translator and scholar of Dante. Also an accomplished botanist, it was he who first exposed his son to the study of nature. Lyell's grandfather, also Charles Lyell, had made the family fortune supplying the Royal Navy at Montrose, enabling him to buy Kinnordy House.
The family seat is located in Strathmore, near the Highland Boundary Fault. Round the house, in the strath, is good farmland, but within a short distance to the north-west, on the other side of the fault, are the Grampian Mountains in the Highlands. His family's second country home was in a completely different geological and ecological area: he spent much of his childhood at Bartley Lodge in the New Forest, in Hampshire in southern England.

Lyell entered Exeter College, Oxford, in 1816, and attended William Buckland's lectures. He with a graduated BA Hons. second class degree in classics, in December 1819, and gained his M.A. 1821.
After graduation he took up law as a profession, entering Lincoln's Inn in 1820. He completed a circuit through rural England, where he could observe geological phenomena. In 1821 he attended Robert Jameson's lectures in Edinburgh, and visited Gideon Mantell at Lewes, in Sussex. In 1823 he was elected joint secretary of the Geological Society. As his eyesight began to deteriorate, he turned to geology as a full-time profession. His first paper, "On a recent formation of freshwater limestone in Forfarshire", was presented in 1822. By 1827, he had abandoned law and embarked on a geological career that would result in fame and the general acceptance of uniformitarianism, a working out of the ideas proposed by James Hutton a few decades earlier.
In 1832, Lyell married Mary Horner in Bonn, daughter of Leonard Horner (1785–1864), also associated with the Geological Society of London. The new couple spent their honeymoon in Switzerland and Italy on a geological tour of the area.

During the 1840s, Lyell travelled to the United States and Canada, and wrote two popular travel-and-geology books: "Travels in North America" (1845) and "A Second Visit to the United States" (1849). After the Great Chicago Fire, Lyell was one of the first to donate books to help found the Chicago Public Library. In 1866, he was elected a foreign member of the Royal Swedish Academy of Sciences.

Lyell's wife died in 1873, and two years later (in 1875) Lyell himself died as he was revising the twelfth edition of "Principles". He is buried in Westminster Abbey. Lyell was knighted (Kt) in 1848, and later, in 1864, made a baronet (Bt), which is an hereditary honour. He was awarded the Copley Medal of the Royal Society in 1858 and the Wollaston Medal of the Geological Society in 1866. Mount Lyell, the highest peak in Yosemite National Park, is named after him; the crater Lyell on the Moon and a crater on Mars were named in his honour; Mount Lyell in western Tasmania, Australia, located in a profitable mining area, bears Lyell's name; and the Lyell Range in north-west Western Australia is named after him as well. In Southwest Nelson in the South Island of New Zealand, the Lyell Range, Lyell River and the gold mining town of Lyell (now only a camping site) were all named after Lyell. The jawless fish "Cephalaspis lyelli", from the Old Red Sandstone of southern Scotland, was named by Louis Agassiz in honour of Lyell.

Lyell had private means, and earned further income as an author. He came from a prosperous family, worked briefly as a lawyer in the 1820s, and held the post of Professor of Geology at King's College London in the 1830s. From 1830 onward his books provided both income and fame. Each of his three major books was a work continually in progress. All three went through multiple editions during his lifetime, although many of his friends (such as Darwin) thought the first edition of the "Principles" was the best written. Lyell used each edition to incorporate additional material, rearrange existing material, and revisit old conclusions in light of new evidence.

"Principles of Geology", Lyell's first book, was also his most famous, most influential, and most important. First published in three volumes in 1830–33, it established Lyell's credentials as an important geological theorist and propounded the doctrine of uniformitarianism. It was a work of synthesis, backed by his own personal observations on his travels.

The central argument in "Principles" was that "the present is the key to the past" – a concept of the Scottish Enlightenment which David Hume had stated as "all inferences from experience suppose ... that the future will resemble the past", and James Hutton had described when he wrote in 1788 that "from what has actually been, we have data for concluding with regard to that which is to happen thereafter." Geological remains from the distant past can, and should, be explained by reference to geological processes now in operation and thus directly observable. Lyell's interpretation of geological change as the steady accumulation of minute changes over enormously long spans of time was a powerful influence on the young Charles Darwin. Lyell asked Robert FitzRoy, captain of HMS "Beagle", to search for erratic boulders on the survey voyage of the "Beagle", and just before it set out FitzRoy gave Darwin Volume 1 of the first edition of Lyell's "Principles". When the "Beagle" made its first stop ashore at St Jago in the Cape Verde islands, Darwin found rock formations which seen "through Lyell's eyes" gave him a revolutionary insight into the geological history of the island, an insight he applied throughout his travels.

While in South America Darwin received Volume 2 which considered the ideas of Lamarck in some detail. Lyell rejected Lamarck's idea of organic evolution, proposing instead "Centres of Creation" to explain diversity and territory of species. However, as discussed below, many of his letters show he was fairly open to the idea of evolution. In geology Darwin was very much Lyell's disciple, and brought back observations and his own original theorising, including ideas about the formation of atolls, which supported Lyell's uniformitarianism. On the return of the "Beagle" (October 1836) Lyell invited Darwin to dinner and from then on they were close friends. Although Darwin discussed evolutionary ideas with him from 1842, Lyell continued to reject evolution in each of the first nine editions of the "Principles". He encouraged Darwin to publish, and following the 1859 publication of "On the Origin of Species", Lyell finally offered a tepid endorsement of evolution in the tenth edition of "Principles".

"Elements of Geology" began as the fourth volume of the third edition of "Principles": Lyell intended the book to act as a suitable field guide for students of geology. The systematic, factual description of geological formations of different ages contained in "Principles" grew so unwieldy, however, that Lyell split it off as the "Elements" in 1838. The book went through six editions, eventually growing to two volumes and ceasing to be the inexpensive, portable handbook that Lyell had originally envisioned. Late in his career, therefore, Lyell produced a condensed version titled "Student's Elements of Geology" that fulfilled the original purpose.

"Geological Evidences of the Antiquity of Man" brought together Lyell's views on three key themes from the geology of the Quaternary Period of Earth history: glaciers, evolution, and the age of the human race. First published in 1863, it went through three editions that year, with a fourth and final edition appearing in 1873. The book was widely regarded as a disappointment because of Lyell's equivocal treatment of evolution. Lyell, a devout Christian, had great difficulty reconciling his beliefs with natural selection.

Lyell's geological interests ranged from volcanoes and geological dynamics through stratigraphy, palaeontology, and glaciology to topics that would now be classified as prehistoric archaeology and paleoanthropology. He is best known, however, for his role in popularising the doctrine of uniformitarianism. He played a critical role in advancing the study of loess.

From 1830 to 1833 his multi-volume "Principles of Geology" was published. The work's subtitle was "An attempt to explain the former changes of the Earth's surface by reference to causes now in operation", and this explains Lyell's impact on science. He drew his explanations from field studies conducted directly before he went to work on the founding geology text. He was, along with the earlier John Playfair, the major advocate of James Hutton's idea of uniformitarianism, that the earth was shaped entirely by slow-moving forces still in operation today, acting over a very long period of time. This was in contrast to catastrophism, an idea of abrupt geological changes, which had been adapted in England to support belief in Noah's flood. Describing the importance of uniformitarianism on contemporary geology, Lyell wrote,

Never was there a doctrine more calculated to foster indolence, and to blunt the keen edge of curiosity, than this assumption of the discordance between the former and the existing causes of change... The student was taught to despond from the first. Geology, it was affirmed, could never arise to the rank of an exact science... [With catastrophism] we see the ancient spirit of speculation revived, and a desire manifestly shown to cut, rather than patiently untie, the Gordian Knot.

Lyell saw himself as "the spiritual saviour of geology, freeing the science from the old dispensation of Moses." The two terms, "uniformitarianism" and "catastrophism", were both coined by William Whewell; in 1866 R. Grove suggested the simpler term "continuity" for Lyell's view, but the old terms persisted. In various revised editions (12 in all, through 1872), "Principles of Geology" was the most influential geological work in the middle of the 19th century, and did much to put geology on a modern footing. For his efforts he was knighted in 1848, then made a baronet in 1864.

Lyell noted the "economic advantages" that geological surveys could provide, citing their felicity in mineral-rich countries and provinces. Modern surveys, like the British Geological Survey (founded in 1835), and the US Geological Survey (founded in 1879), map and exhibit the natural resources within the country. So, in endorsing surveys, as well as advancing the study of geology, Lyell helped to forward the business of modern extractive industries, such as the coal and oil industry.

Before the work of Lyell, phenomena such as earthquakes were understood by the destruction that they brought. One of the contributions that Lyell made in "Principles" was to explain the cause of earthquakes. Lyell, in contrast focused on recent earthquakes (150 yrs), evidenced by surface irregularities such as faults, fissures, stratigraphic displacements and depressions.

Lyell's work on volcanoes focused largely on Vesuvius and Etna, both of which he had earlier studied. His conclusions supported gradual building of volcanoes, so-called "backed up-building", as opposed to the upheaval argument supported by other geologists.

Lyell's most important specific work was in the field of stratigraphy. From May 1828, until February 1829, he travelled with Roderick Impey Murchison (1792–1871) to the south of France (Auvergne volcanic district) and to Italy. In these areas he concluded that the recent strata (rock layers) could be categorised according to the number and proportion of marine shells encased within. Based on this he proposed dividing the Tertiary period into three parts, which he named the Pliocene, Miocene, and Eocene.

In "Principles of Geology" (first edition, vol. 3, Ch. 2, 1833) Lyell proposed that icebergs could be the means of transport for erratics. During periods of global warming, ice breaks off the poles and floats across submerged continents, carrying debris with it, he conjectured. When the iceberg melts, it rains down sediments upon the land. Because this theory could account for the presence of diluvium, the word "drift" became the preferred term for the loose, unsorted material, today called "till". Furthermore, Lyell believed that the accumulation of fine angular particles covering much of the world (today called loess) was a deposit settled from mountain flood water. Today some of Lyell's mechanisms for geological processes have been disproven, though many have stood the test of time. His observational methods and general analytical framework remain in use today as foundational principles in geology.

Lyell initially accepted the conventional view of other men of science, that the fossil record indicated a directional geohistory in which species went extinct. Around 1826, when he was on circuit, he read Lamarck's "Zoological Philosophy" and on 2 March 1827 wrote to Mantell, expressing admiration, but cautioning that he read it "rather as I hear an advocate on the wrong side, to know what can be made of the case in good hands".:
He struggled with the implications for human dignity, and later in 1827 wrote private notes on Lamarck's ideas. Lyell reconciled transmutation of species with natural theology by suggesting that it would be as much a "remarkable manifestation of creative Power" as creating each species separately. He countered Lamarck's views by rejecting continued cooling of the Earth in favour of "a fluctuating cycle", a long-term steady-state geohistory as proposed by James Hutton. The fragmentary fossil record already showed "a high class of fishes, close to reptiles" in the Carboniferous period which he called "the first Zoological era", and quadrupeds could also have existed then. In November 1827, after William Broderip found a Middle Jurassic fossil of the early mammal "Didelphis", Lyell told his father that "There was everything but man even as far back as the Oolite." Lyell inaccurately portrayed Lamarckism as a response to the fossil record, and said it was falsified by a lack of progress. He said in the second volume of "Principles" that the occurrence of this one fossil of the higher mammallia "in these ancient strata, is as fatal to the theory of successive development, as if several hundreds had been discovered."

In the first edition of "Principles", the first volume briefly set out Lyell's concept of a steady state with no real progression of fossils, in which humanity had appeared recently, with unique intellectual and moral qualities but no great physical distinction from animals. The second volume dismissed Lamarck's claims of animal forms arising from habits, continuous spontaneous generation of new life, and man having evolved from lower forms. Lyell explicitly rejected Lamark's concept of transmutation of species, drawing on Cuvier's arguments, and concluded that species had been created with stable attributes. He discussed the geographical distribution of plants and animals, and proposed that every species of plant or animal was descended from a pair or individual, originated in response to differing external conditions. Species would regularly go extinct, in a "struggle for existence" between hybrids, or a "war one with another" due to population pressure. He was vague about how replacement species formed, portraying this as an infrequent occurrence which could rarely be observed.

The leading naturalist Sir John Herschel wrote from Cape Town on 20 February 1836, thanking Lyell for sending a copy of "Principles" and praising the book as opening a way for bold speculation on "that mystery of mysteries, the replacement of extinct species by others" – by analogy with other intermediate causes, "the origination of fresh species, could it ever come under our cognizance, would be found to be a natural in contradistinction to a miraculous process". Lyell replied: "In regard to the origination of new species, I am very glad to find that you think it probable that it may be carried on through the intervention of intermediate causes. I left this rather to be inferred, not thinking it worth while to offend a certain class of persons by embodying in words what would only be a speculation." 
Whewell subsequently questioned this topic, and in March 1837 Lyell told him:

As a result of his letters and, no doubt, personal conversations, Huxley and Haeckel were convinced that, at the time he wrote "Principles", he believed new species had arisen by natural methods. Sedgwick wrote worried letters to him about this.

By the time Darwin returned from the "Beagle" survey expedition in 1836, he had begun to doubt Lyell's ideas about the permanence of species. He continued to be a close personal friend, and Lyell was one of the first scientists to support "On the Origin of Species", though he did not subscribe to all its contents. Lyell was also a friend of Darwin's closest colleagues, Hooker and Huxley, but unlike them he struggled to square his religious beliefs with evolution. This inner struggle has been much commented on. He had particular difficulty in believing in natural selection as the main motive force in evolution.

Lyell and Hooker were instrumental in arranging the peaceful co-publication of the theory of natural selection by Darwin and Alfred Russel Wallace in 1858: each had arrived at the theory independently. Lyell's data on stratigraphy were important because Darwin thought that populations of an organism changed slowly, requiring "geological time".

Although Lyell did not publicly accept evolution (descent with modification) at the time of writing the "Principles", after the Darwin–Wallace papers and the "Origin" Lyell wrote in his notebook:

Lyell's acceptance of natural selection, Darwin's proposed mechanism for evolution, was equivocal, and came in the tenth edition of "Principles". "The Antiquity of Man" (published in early February 1863, just before Huxley's "Man's place in nature") drew these comments from Darwin to Huxley:
Quite strong remarks: no doubt Darwin resented Lyell's repeated suggestion that he owed a lot to Lamarck, whom he (Darwin) had always specifically rejected. Darwin's daughter Henrietta (Etty) wrote to her father: "Is it fair that Lyell always calls your theory a modification of Lamarck's?" 

In other respects "Antiquity" was a success. It sold well, and it "shattered the tacit agreement that mankind should be the sole preserve of theologians and historians". But when Lyell wrote that it remained a profound mystery how the huge gulf between man and beast could be bridged, Darwin wrote "Oh!" in the margin of his copy.

Places named after Lyell:













</doc>
<doc id="7473" url="https://en.wikipedia.org/wiki?curid=7473" title="Chelsea F.C.">
Chelsea F.C.

Chelsea Football Club is a professional football club in London, England, that competes in the Premier League. Founded in 1905, the club's home ground since then has been Stamford Bridge.

Chelsea won the First Division title in 1955, followed by various cup competitions between 1965 and 1971. The past two decades have seen sustained success, with the club winning 23 trophies since 1997. In total, the club has won 28 major trophies; six titles, eight FA Cups, five League Cups and four FA Community Shields, one UEFA Champions League, two UEFA Cup Winners' Cups, one UEFA Europa League and one UEFA Super Cup.

Chelsea's regular kit colours are royal blue shirts and shorts with white socks. The club's crest has been changed several times in attempts to re-brand the club and modernise its image. The current crest, featuring a ceremonial lion rampant regardant holding a staff, is a modification of the one introduced in the early 1950s. The club have the sixth-highest average all-time attendance in English football, and for the 2017–18 season at 41,280.<ref name="14/15 attendances"></ref> Since 2003, Chelsea have been owned by Russian billionaire Roman Abramovich. In 2018, they were ranked by "Forbes" magazine as the seventh most valuable football club in the world, at £1.54 billion ($2.06 billion) and in the 2016–17 season it was the eighth highest-earning football club in the world, earned €428 million.

In 1904, Gus Mears acquired the Stamford Bridge athletics stadium with the aim of turning it into a football ground. An offer to lease it to nearby Fulham was turned down, so Mears opted to found his own club to use the stadium. As there was already a team named Fulham in the borough, the name of the adjacent borough of Chelsea was chosen for the new club; names like "Kensington FC", "Stamford Bridge FC" and "London FC" were also considered. Chelsea were founded on 10 March 1905 at The Rising Sun pub (now The Butcher's Hook), opposite the present-day main entrance to the ground on Fulham Road, and were elected to the Football League shortly afterwards.

The club won promotion to the First Division in their second season, and yo-yoed between the First and Second Divisions in their early years. They reached the 1915 FA Cup Final, where they lost to Sheffield United at Old Trafford, and finished third in the First Division in 1920, the club's best league campaign to that point. Chelsea attracted large crowds and had a reputation for signing big-name players, but success continued to elude the club in the inter-war years.

Former Arsenal and England centre-forward Ted Drake became manager in 1952 and proceeded to modernise the club. He removed the club's Chelsea pensioner crest, improved the youth set-up and training regime, rebuilt the side with shrewd signings from the lower divisions and amateur leagues, and led Chelsea to their first major trophy success – the League championship – in 1954–55. The following season saw UEFA create the European Champions' Cup, but after objections from The Football League and the FA Chelsea were persuaded to withdraw from the competition before it started. Chelsea failed to build on this success, and spent the remainder of the 1950s in mid-table. Drake was dismissed in 1961 and replaced by player-coach Tommy Docherty.

Docherty built a new team around the group of talented young players emerging from the club's youth set-up and Chelsea challenged for honours throughout the 1960s, enduring several near-misses. They were on course for a treble of League, FA Cup and League Cup going into the final stages of the 1964–65 season, winning the League Cup but faltering late on in the other two. In three seasons the side were beaten in three major semi-finals and were FA Cup runners-up. Under Docherty's successor, Dave Sexton, Chelsea won the FA Cup in 1970, beating Leeds United 2–1 in a final replay. Chelsea took their first European honour, a UEFA Cup Winners' Cup triumph, the following year, with another replayed win, this time over Real Madrid in Athens.

The late 1970s through to the '80s was a turbulent period for Chelsea. An ambitious redevelopment of Stamford Bridge threatened the financial stability of the club, star players were sold and the team were relegated. Further problems were caused by a notorious hooligan element among the support, which was to plague the club throughout the decade. In 1982, Chelsea were, at the nadir of their fortunes, acquired by Ken Bates for the nominal sum of £1, although by now the Stamford Bridge freehold had been sold to property developers, meaning the club faced losing their home. On the pitch, the team had fared little better, coming close to relegation to the Third Division for the first time, but in 1983 manager John Neal put together an impressive new team for minimal outlay. Chelsea won the Second Division title in 1983–84 and established themselves in the top division, before being relegated again in 1988. The club bounced back immediately by winning the Second Division championship in 1988–89.

After a long-running legal battle, Bates reunited the stadium freehold with the club in 1992 by doing a deal with the banks of the property developers, who had been bankrupted by a market crash. Chelsea's form in the new Premier League was unconvincing, although they did reach the 1994 FA Cup Final with Glenn Hoddle. It was not until the appointment of Ruud Gullit as player-manager in 1996 that their fortunes changed. He added several top international players to the side, as the club won the FA Cup in 1997 and established themselves as one of England's top sides again. Gullit was replaced by Gianluca Vialli, who led the team to victory in the League Cup Final, the UEFA Cup Winners' Cup Final and the UEFA Super Cup in 1998, the FA Cup in 2000 and their first appearance in the UEFA Champions League. Vialli was sacked in favour of Claudio Ranieri, who guided Chelsea to the 2002 FA Cup Final and Champions League qualification in 2002–03.

In June 2003, Bates sold Chelsea to Russian billionaire Roman Abramovich for £140 million. Over £100 million was spent on new players, but Ranieri was unable to deliver any trophies, and was replaced by José Mourinho. Under Mourinho, Chelsea became the fifth English team to win back-to-back league championships since the Second World War (2004–05 and 2005–06), in addition to winning an FA Cup (2007) and two League Cups (2005 and 2007). After a poor start to the 2007-2008 season, Mourinho was replaced by Avram Grant, who led the club to their first UEFA Champions League final, which they lost on penalties to Manchester United.

Luiz Felipe Scolari took over from Grant, but was sacked after 7 months following poor results. Guus Hiddink then took over the club on an interim basis while continuing to manage the Russian national football team. Hiddink guided Chelsea to another FA Cup success, after which he left the club to return full time to the Russian managerial position. In 2009–10, his successor Carlo Ancelotti led them to their first Premier League and FA Cup Double", the team becoming the first English top-flight club to score 100 league goals in a season since 1963. In 2012, caretaker manager Roberto Di Matteo led Chelsea to their seventh FA Cup, and their first UEFA Champions League title, beating Bayern Munich 4–3 on penalties, the first London club to win the trophy. In 2013, interim manager Rafael Benítez guided Chelsea to win the UEFA Europa League against Benfica, becoming the first club to hold two major European titles simultaneously and one of five clubs, and the first British club followed by Manchester United, to have won all three of UEFA's major club competitions. In the summer of 2013, Mourinho returned as manager, leading Chelsea to League Cup success in March 2015, and their fifth league title two months later. Mourinho was sacked after four months of the following season, with the club having lost 9 of their first 16 games and sitting only one point above the relegation zone. Two years later, under new coach Antonio Conte, Chelsea won its sixth English title.

Chelsea have only had one home ground, Stamford Bridge, where they have played since the team's foundation. It was officially opened on 28 April 1877 and for the first 28 years of its existence it was used almost exclusively by the London Athletic Club as an arena for athletics meetings and not at all for football. In 1904 the ground was acquired by businessman Gus Mears and his brother Joseph, who had also purchased nearby land (formerly a large market garden) with the aim of staging football matches on the now 12.5 acre (51,000 m²) site. Stamford Bridge was designed for the Mears family by the noted football architect Archibald Leitch, who had also designed Ibrox, Craven Cottage and Hampden Park. Most football clubs were founded first, and then sought grounds in which to play, but Chelsea were founded for Stamford Bridge.

Starting with an open bowl-like design and one covered terrace, Stamford Bridge had an original capacity of around 100,000. The early 1930s saw the construction of a terrace on the southern part of the ground with a roof that covered around one fifth of the stand. It eventually became known as the "Shed End", the home of Chelsea's most loyal and vocal supporters, particularly during the 1960s, 70s and 80s. The exact origins of the name are unclear, but the fact that the roof looked like a corrugated iron shed roof played a part.

In the early 1970s, the club's owners announced a modernisation of Stamford Bridge with plans for a state-of-the-art 50,000 all-seater stadium. Work began on the East Stand in 1972 but the project was beset with problems and was never completed; the cost brought the club close to bankruptcy, culminating in the freehold being sold to property developers. Following a long legal battle, it was not until the mid-1990s that Chelsea's future at the stadium was secured and renovation work resumed. The north, west and southern parts of the ground were converted into all-seater stands and moved closer to the pitch, a process completed by 2001.

When Stamford Bridge was redeveloped in the Bates era many additional features were added to the complex including two hotels, apartments, bars, restaurants, the Chelsea Megastore, and an interactive visitor attraction called Chelsea World of Sport. The intention was that these facilities would provide extra revenue to support the football side of the business, but they were less successful than hoped and before the Abramovich takeover in 2003 the debt taken on to finance them was a major burden on the club. Soon after the takeover a decision was taken to drop the "Chelsea Village" brand and refocus on Chelsea as a football club. However, the stadium is sometimes still referred to as part of ""Chelsea Village"" or ""The Village"".

The Stamford Bridge freehold, the pitch, the turnstiles and Chelsea's naming rights are now owned by Chelsea Pitch Owners, a non-profit organisation in which fans are the shareholders. The CPO was created to ensure the stadium could never again be sold to developers. As a condition for using the Chelsea FC name, the club has to play its first team matches at Stamford Bridge, which means that if the club moves to a new stadium, they may have to change their name.
Chelsea's training ground is located in Cobham, Surrey. Chelsea moved to Cobham in 2004. Their previous training ground in Harlington was taken over by QPR in 2005. The new training facilities in Cobham were completed in 2007.

Stamford Bridge has been used for a variety of other sporting events since 1905. It hosted the FA Cup Final from 1920 to 1922, has held ten FA Cup Semi-finals (most recently in 1978), ten FA Charity Shield matches (the last in 1970), and three England international matches, the last in 1932; it was also the venue for an unofficial "Victory International" in 1946. The 2013 UEFA Women's Champions League Final was played at Stamford Bridge. In October 1905 it hosted a rugby union match between the All Blacks and Middlesex, and in 1914 hosted a baseball match between the touring New York Giants and the Chicago White Sox. It was the venue for a boxing match between world flyweight champion Jimmy Wilde and Joe Conn in 1918. The running track was used for dirt track racing between 1928 and 1932, greyhound racing from 1933 to 1968, and Midget car racing in 1948. In 1980, Stamford Bridge hosted the first international floodlit cricket match in the UK, between Essex and the West Indies. It was also the home stadium of the London Monarchs American Football team for the 1997 season.
The current club ownership have stated that a larger stadium is necessary in order for Chelsea to stay competitive with rival clubs who have significantly larger stadia, such as Arsenal and Manchester United. Owing to its location next to a main road and two railway lines, fans can only enter the ground via the Fulham Road exits, which places constraints on expansion due to health and safety regulations. The club have consistently affirmed their desire to keep Chelsea at their current home, but have nonetheless been linked with a move to various nearby sites, including the Earls Court Exhibition Centre, Battersea Power Station and the Chelsea Barracks. In October 2011, a proposal from the club to buy back the freehold to the land on which Stamford Bridge sits was voted down by Chelsea Pitch Owners shareholders. In May 2012, the club made a formal bid to purchase Battersea Power Station, with a view to developing the site into a new stadium, but lost out to a Malaysian consortium. The club subsequently announced plans to redevelop Stamford Bridge into a 60,000-seater stadium.
On 11 January 2017 it was announced that the stadium was given the go ahead from Hammersmith and Fulham council for the new 60,000 stadium to be built.
However, on 31 May 2018, the club released a statement via their website stating that "Chelsea Football Club announces today that it has put its new stadium project on hold. No further pre-construction design and planning work will occur." The statement went on to elaborate that "The decision was made due to the current unfavourable investment climate."

Chelsea have had four main crests, which all underwent minor variations. The first, adopted when the club was founded, was the image of a Chelsea pensioner, the army veterans who reside at the nearby Royal Hospital Chelsea. This contributed to the club's original "pensioner" nickname, and remained for the next half-century, though it never appeared on the shirts. When Ted Drake became Chelsea manager in 1952, he began to modernise the club. Believing the Chelsea pensioner crest to be old-fashioned, he insisted that it be replaced. A stop-gap badge which comprised the initials C.F.C. was adopted for a year. In 1953, the club crest was changed to an upright blue lion looking backwards and holding a staff. It was based on elements in the coat of arms of the Metropolitan Borough of Chelsea with the "lion rampant regardant" taken from the arms of then club president Viscount Chelsea and the staff from the Abbots of Westminster, former Lords of the Manor of Chelsea. It also featured three red roses, to represent England, and two footballs. This was the first Chelsea crest to appear on the shirts, in the early 1960s.

In 1986, with Ken Bates now owner of the club, Chelsea's crest was changed again as part of another attempt to modernise and because the old rampant lion badge could not be trademarked. The new badge featured a more naturalistic non-heraldic lion, in white and not blue, standing over the C.F.C. initials. This lasted for the next 19 years, with some modifications such as the use of different colours, including red from 1987 to 1995, and yellow from 1995 until 1999, before the white returned. With the new ownership of Roman Abramovich, and the club's centenary approaching, combined with demands from fans for the popular 1950s badge to be restored, it was decided that the crest should be changed again in 2005. The new crest was officially adopted for the start of the 2005–06 season and marked a return to the older design, used from 1953 to 1986, featuring a blue heraldic lion holding a staff. For the centenary season this was accompanied by the words '100 YEARS' and 'CENTENARY 2005–2006' on the top and bottom of the crest respectively.

Chelsea have always worn blue shirts, although they originally used the paler eton blue, which was taken from the racing colours of then club president, Earl Cadogan, and was worn with white shorts and dark blue or black socks. The light blue shirts were replaced by a royal blue version in around 1912. In the 1960s Chelsea manager Tommy Docherty changed the kit again, switching to blue shorts (which have remained ever since) and white socks, believing it made the club's colours more modern and distinctive, since no other major side used that combination; this kit was first worn during the 1964–65 season. Since then Chelsea have always worn white socks with their home kit apart from a short spell from 1985 to 1992, when blue socks were reintroduced.

Chelsea's away colours are usually all yellow or all white with blue trim. More recently, the club have had a number of black or dark blue away kits which alternate every year. As with most teams, they have also had some more unusual ones. At Docherty's behest, in the 1966 FA Cup semi-final they wore blue and black stripes, based on Inter Milan's kit. In the mid-1970s, the away strip was a red, white and green kit inspired by the Hungarian national side of the 1950s. Other memorable away kits include an all jade strip worn from 1986–89, red and white diamonds from 1990–92, graphite and tangerine from 1994–96, and luminous yellow from 2007–08. The graphite and tangerine strip often appears in lists of the worst football kits ever.

Chelsea are among the most widely supported football clubs in the world. They have the sixth highest average all-time attendance in English football and regularly attract over 40,000 fans to Stamford Bridge; they were the seventh best-supported Premier League team in the 2013–14 season, with an average gate of 41,572. Chelsea's traditional fanbase comes from all over the Greater London area including working-class parts such as Hammersmith and Battersea, wealthier areas like Chelsea and Kensington, and from the home counties. There are also numerous official supporters clubs in the United Kingdom and all over the world. Between 2007 and 2012, Chelsea were ranked fourth worldwide in annual replica kit sales, with an average of 910,000. Chelsea's official Twitter account has 9.8 million followers as of September 2017.

At matches, Chelsea fans sing chants such as "Carefree" (to the tune of "Lord of the Dance", whose lyrics were probably written by supporter Mick Greenaway), "Ten Men Went to Mow", "We All Follow the Chelsea" (to the tune of "Land of Hope and Glory"), "Zigga Zagga", and the celebratory "Celery", with the latter often resulting in fans ritually throwing celery. The vegetable was banned inside Stamford Bridge after an incident involving Arsenal midfielder Cesc Fàbregas at the 2007 League Cup Final.

During the 1970s and 1980s in particular, Chelsea supporters were associated with football hooliganism. The club's "football firm", originally known as the Chelsea Shed Boys, and subsequently as the Chelsea Headhunters, were nationally notorious for football violence, alongside hooligan firms from other clubs such as West Ham United's Inter City Firm and Millwall's Bushwackers, before, during and after matches. The increase of hooligan incidents in the 1980s led chairman Ken Bates to propose erecting an electric fence to deter them from invading the pitch, a proposal that the Greater London Council rejected.

Since the 1990s, there has been a marked decline in crowd trouble at matches, as a result of stricter policing, CCTV in grounds and the advent of all-seater stadia. In 2007, the club launched the 'Back to the Shed' campaign to improve the atmosphere at home matches, with notable success. According to Home Office statistics, 126 Chelsea fans were arrested for football-related offences during the 2009–10 season, the third highest in the division, and 27 banning orders were issued, the fifth-highest in the division.

Chelsea have long-standing rivalries with North London clubs Arsenal and Tottenham Hotspur. A strong rivalry with Leeds United dates back to several heated and controversial matches in the 1960s and 1970s, particularly the 1970 FA Cup Final. More recently a rivalry with Liverpool has grown following repeated clashes in cup competitions. Chelsea's fellow West London sides Brentford, Fulham and Queens Park Rangers are generally not considered major rivals, as matches have only taken place intermittently due to the clubs often being in separate divisions. 

A 2004 survey by Planetfootball.com found that Chelsea fans consider their main rivalries to be with (in descending order): Arsenal, Tottenham Hotspur and Manchester United. In the same survey, fans of Arsenal, Fulham, Leeds United, QPR, Tottenham, and West Ham United named Chelsea as one of their three main rivals. In a 2008 poll conducted by the Football Fans Census, Chelsea fans named Liverpool, Arsenal and Manchester United as their most disliked clubs. In the same survey, "Chelsea" was the top answer to the question "Which other English club do you dislike the most?"

A 2012 survey, conducted among 1200 supporters of the top four league divisions across the country, found that many clubs’ main rivals had changed since 2003 and reported that Chelsea fans consider Tottenham to be their main rival, above Arsenal and Manchester United.

Chelsea's highest appearance-maker is ex-captain Ron Harris, who played in 795 competitive games for the club between 1961 and 1980. The record for a Chelsea goalkeeper is held by Harris's contemporary, Peter Bonetti, who made 729 appearances (1959–79). With 103 caps (101 while at the club), Frank Lampard of England is Chelsea's most capped international player.

Frank Lampard is Chelsea's all-time top goalscorer, with 211 goals in 648 games (2001–2014); he passed Bobby Tambling's longstanding record of 202 in May 2013. Seven other players have also scored over 100 goals for Chelsea: George Hilsdon (1906–12), George Mills (1929–39), Roy Bentley (1948–56), Jimmy Greaves (1957–61), Peter Osgood (1964–74 and 1978–79), Kerry Dixon (1983–92) and Didier Drogba (2004–12 and 2014–2015). Greaves holds the record for the most goals scored in one season (43 in 1960–61).

Chelsea's biggest winning scoreline in a competitive match is 13–0, achieved against Jeunesse Hautcharage in the Cup Winners' Cup in 1971. The club's biggest top-flight win was an 8–0 victory against Wigan Athletic in 2010, which was matched in 2012 against Aston Villa. Chelsea's biggest loss was an 8–1 reverse against Wolverhampton Wanderers in 1953. Officially, Chelsea's highest home attendance is 82,905 for a First Division match against Arsenal on 12 October 1935. However, an estimated crowd of over 100,000 attended a friendly match against Soviet team Dynamo Moscow on 13 November 1945. The modernisation of Stamford Bridge during the 1990s and the introduction of all-seater stands mean that neither record will be broken for the foreseeable future. The current legal capacity of Stamford Bridge is 41,663. Every starting player in Chelsea's 57 games of the 2013–14 season was a full international – a new club record.
Chelsea hold the English record for the fewest goals conceded during a league season (15), the highest number of clean sheets overall in a Premier League season (25) (both set during the 2004–05 season), and the most consecutive clean sheets from the start of a league season (6, set during the 2005–06 season). The club's 21–0 aggregate victory over Jeunesse Hautcharage in the UEFA Cup Winners' Cup in 1971 remains a record in European competition. Chelsea hold the record for the longest streak of unbeaten matches at home in the English top flight, which lasted 86 matches from 20 March 2004 to 26 October 2008. They secured the record on 12 August 2007, beating the previous record of 63 matches unbeaten set by Liverpool between 1978 and 1980. Chelsea's streak of eleven consecutive away league wins, set between 5 April 2008 and 6 December 2008, is also a record for the English top flight. Their £50 million purchase of Fernando Torres from Liverpool in January 2011 was the record transfer fee paid by a British club until Ángel Di María signed for Manchester United in August 2014 for £59.7 million.

Chelsea, along with Arsenal, were the first club to play with shirt numbers, on 25 August 1928 in their match against Swansea Town. They were the first English side to travel by aeroplane to a domestic away match, when they visited Newcastle United on 19 April 1957, and the first First Division side to play a match on a Sunday, when they faced Stoke City on 27 January 1974. On 26 December 1999, Chelsea became the first British side to field an entirely foreign starting line-up (no British or Irish players) in a Premier League match against Southampton.

In May 2007, Chelsea were the first team to win the FA Cup at the new Wembley Stadium, having also been the last to win it at the old Wembley. They were the first English club to be ranked No. 1 under UEFA's five-year coefficient system in the 21st century. They were the first team in Premier League history, and the first team in the English top flight since their great rivals Tottenham Hotspur in 1962-63, to score at least 100 goals in a single season, reaching the milestone on the final day of the 2009–10 season. Chelsea are the only London club to win the UEFA Champions League, after beating Bayern Munich in the 2012 final. Upon winning the 2012–13 UEFA Europa League, Chelsea became the first English club to win all four European trophies and the only club to hold the Champions League and the Europa League at the same time.

Chelsea Football Club were founded by Gus Mears in 1905. After his death in 1912, his descendants continued to own the club until 1982, when Ken Bates bought the club from Mears' great-nephew Brian Mears for £1. Bates bought a controlling stake in the club and floated Chelsea on the AIM stock exchange in March 1996. In July 2003, Roman Abramovich purchased just over 50% of Chelsea Village plc's share capital, including Bates' 29.5% stake, for £30 million and over the following weeks bought out most of the remaining 12,000 shareholders at 35 pence per share, completing a £140 million takeover. Other shareholders at the time of the takeover included the Matthew Harding estate (21%), BSkyB (9.9%) and various anonymous offshore trusts. After passing the 90% share threshold, Abramovich took the club back into private hands, delisting it from the AIM on 22 August 2003. He also took on responsibility for the club's debt of £80 million, quickly paying most of it.

Thereafter, Abramovich changed the ownership name to Chelsea FC plc, whose ultimate parent company is Fordstam Limited, which is controlled by him. Chelsea are additionally funded by Abramovich via interest free soft loans channelled through his holding company Fordstam Limited. The loans stood at £709 million in December 2009, when they were all converted to equity by Abramovich, leaving the club themselves debt free, although the debt remains with Fordstam. Since 2008 the club have had no external debt.

Chelsea did not turn a profit in the first nine years of Abramovich's ownership, and made record losses of £140m in June 2005. In November 2012, Chelsea announced a profit of £1.4 million for the year ending 30 June 2012, the first time the club had made a profit under Abramovich's ownership. This was followed by a loss in 2013 and then their highest ever profit of £18.4 million for the year to June 2014.

Chelsea have been described as a global brand; a 2012 report by Brand Finance ranked Chelsea fifth among football brands and valued the club's brand value at US$398 million – an increase of 27% from the previous year, also valuing them at US$10 million more than the sixth best brand, London rivals Arsenal – and gave the brand a strength rating of AA (very strong). In 2016, "Forbes" magazine ranked Chelsea the seventh most valuable football club in the world, at £1.15 billion ($1.66 billion). As of 2016, Chelsea are ranked eighth in the Deloitte Football Money League with an annual commercial revenue of £322.59 million.

Chelsea's kit has been manufactured by Nike since July 2017. Previously, the kit was manufactured by Adidas, which was originally contracted to supply the club's kit from 2006 to 2018. The partnership was extended in October 2010 in a deal worth £160 million over eight years. This deal was again extended in June 2013 in a deal worth £300 million over another ten years. In May 2016, Adidas announced that by mutual agreement, the kit sponsorship would end six years early on 30 June 2017. Chelsea had to pay £40m in compensation to Adidas. In October 2016, Nike was announced as the new kit sponsor, in a deal worth £900m over 15 years, until 2032. Previously, the kit was manufactured by Umbro (1975–81), Le Coq Sportif (1981–86), The Chelsea Collection (1986–87), Umbro (1987–2006), and Adidas (2006–2017).

Chelsea's first shirt sponsor was Gulf Air, agreed during the 1983–84 season. The club were then sponsored by Grange Farms, Bai Lin Tea and Simod before a long-term deal was signed with Commodore International in 1989; Amiga, an offshoot of Commodore, also appeared on the shirts. Chelsea were subsequently sponsored by Coors beer (1994–97), Autoglass (1997–2001), Emirates (2001–05), Samsung Mobile (2005–08) and Samsung (2008–15). Chelsea's current shirt sponsor is the Yokohama Rubber Company. Worth £40 million per year, the deal is second in English football to Chevrolet's £50 million-per-year sponsorship of Manchester United.

Following the door-opening of sleeve sponsor in the English League, Chelsea had Alliance Tyres as its first sleeve sponsor in the 2017–18 season. For the 2018–19 season, they have Hyundai Motor Company as the new sleeve sponsor.

The club has a variety of other sponsors, which include Carabao, Delta Air Lines, Beats by Dre, Singha, EA Sports, Rexona, Hublot, Ericsson, William Hill, Levy Restaurants, Wipro, Grand Royal Whisky, Bangkok Bank, Guangzhou R&F, Mobinil, IndusInd Bank, and Ole777.

In 1930, Chelsea featured in one of the earliest football films, "The Great Game". One-time Chelsea centre forward, Jack Cock, who by then was playing for Millwall, was the star of the film and several scenes were shot at Stamford Bridge, including on the pitch, the boardroom, and the dressing rooms. It included guest appearances by then-Chelsea players Andrew Wilson, George Mills, and Sam Millington. Owing to the notoriety of the Chelsea Headhunters, a football firm associated with the club, Chelsea have also featured in films about football hooliganism, including 2004's "The Football Factory". Chelsea also appear in the Hindi film "Jhoom Barabar Jhoom". In April 2011, Montenegrin comedy series "Nijesmo mi od juče" made an episode in which Chelsea play against FK Sutjeska Nikšić for qualification of the UEFA Champions League.

Up until the 1950s, the club had a long-running association with the music halls; their underachievement often provided material for comedians such as George Robey. It culminated in comedian Norman Long's release of a comic song in 1933, ironically titled "On the Day That Chelsea Went and Won the Cup", the lyrics of which describe a series of bizarre and improbable occurrences on the hypothetical day when Chelsea finally won a trophy. In Alfred Hitchcock's 1935 film "The 39 Steps", Mr Memory claims that Chelsea last won the Cup in 63 BC, "in the presence of the Emperor Nero." Scenes in a 1980 episode of "Minder" were filmed during a real match at Stamford Bridge between Chelsea and Preston North End with Terry McCann (played by Dennis Waterman) standing on the terraces.

The song "Blue is the Colour" was released as a single in the build-up to the 1972 League Cup Final, with all members of Chelsea's first team squad singing; it reached number five in the UK Singles Chart. The song has since been adopted as an anthem by a number of other sports teams around the world, including the Vancouver Whitecaps (as "White is the Colour") and the Saskatchewan Roughriders (as "Green is the Colour"). In the build-up to the 1997 FA Cup Final, the song "Blue Day", performed by Suggs and members of the Chelsea squad, reached number 22 in the UK charts. Bryan Adams, a fan of Chelsea, dedicated the song "We're Gonna Win" from the album "18 Til I Die" to the club.
Chelsea also operate a women's football team, Chelsea Football Club Women, formerly known as Chelsea Ladies. They have been affiliated to the men's team since 2004 and are part of the club's Community Development programme. They play their home games at Kingsmeadow, the home ground of the EFL League One club AFC Wimbledon. The club were promoted to the Premier Division for the first time in 2005 as Southern Division champions and won the Surrey County Cup nine times between 2003 and 2013. In 2010 Chelsea Ladies were one of the eight founder members of the FA Women's Super League. In 2015, Chelsea Ladies won the FA Women's Cup for the first time, beating Notts County Ladies at Wembley Stadium, and a month later clinched their first FA WSL title to complete a league and cup double. John Terry, former captain of the Chelsea men's team, is the President of Chelsea Women.

"For recent transfers, see 2018–19 Chelsea F.C. season."

"For further information: Chelsea F.C. Under-23s and Academy"

Source: Chelsea F.C.

The following managers won at least one trophy when in charge of Chelsea:

!Position
!Staff

Chelsea FC plc is the company which owns Chelsea Football Club. The ultimate parent company of Chelsea FC plc is Fordstam Limited and the ultimate controlling party of Fordstam Limited is Roman Abramovich.

On 22 October 2014, Chelsea announced that Ron Gourlay, after ten successful years at the club including five as Chief Executive, is leaving Chelsea to pursue new business opportunities. On 27 October 2014, Chelsea announced that Christian Purslow is joining the club to run global commercial activities and the club do not expect to announce any other senior appointments in the near future having chairman Bruce Buck and Director Marina Granovskaia assumed the executive responsibilities. Guy Laurence was appointed as the club's Chief Executive on 11 January 2018, filling the vacancy following the departure of Michael Emenalo.

Chelsea Ltd.

Chelsea F.C. plc Board

Executive Board

Life President

Vice-Presidents

Club Ambassadors

Source: Chelsea F.C.

Upon winning the 2012–13 UEFA Europa League, Chelsea became the fourth club in history to have won the "European Treble" of European Cup/UEFA Champions League, European Cup Winners' Cup/UEFA Cup Winners' Cup, and UEFA Cup/UEFA Europa League after Juventus, Ajax and Bayern Munich. Chelsea are the first English club to have won all three major UEFA trophies.





Source: Chelsea F.C.






</doc>
<doc id="7475" url="https://en.wikipedia.org/wiki?curid=7475" title="CANDU reactor">
CANDU reactor

The CANDU, for Canada Deuterium Uranium, is a Canadian pressurized heavy-water reactor design used to generate electric power. The acronym refers to its deuterium oxide (heavy water) moderator and its use of (originally, natural) uranium fuel. CANDU reactors were first developed in the late 1950s and 1960s by a partnership between Atomic Energy of Canada Limited (AECL), the Hydro-Electric Power Commission of Ontario, Canadian General Electric, and other companies.

There have been two major types of CANDU reactors, the original design of around 500 MW that was intended to be used in multi-reactor installations in large plants, and the rationalized CANDU 6 in the 600 MW class that is designed to be used in single stand-alone units or in small multi-unit plants. CANDU 6 units were built in Quebec and New Brunswick, as well as Pakistan, Argentina, South Korea, Romania, and China. A single example of a non-CANDU 6 design was sold to India. The multi-unit design was used only in Ontario, Canada, and grew in size and power as more units were installed in the province, reaching ~880 MW in the units installed at the Darlington Nuclear Generating Station. An effort to rationalize the larger units in a fashion similar to CANDU 6 led to the CANDU 9.

By the early 2000s, sales prospects for the original CANDU designs were dwindling due to the introduction of newer designs from other companies. AECL responded by cancelling CANDU 9 development and moving to the Advanced CANDU reactor (ACR) design. ACR failed to find any buyers; its last potential sale was for an expansion at Darlington, but this was cancelled in 2009. In October 2011, the Canadian Federal Government licensed the CANDU design to Candu Energy (a wholly owned subsidiary of SNC-Lavalin), which also acquired the former reactor development and marketing division of AECL at that time. Candu Energy offers support services for existing sites and is completing formerly stalled installations in Romania and Argentina through a partnership with China National Nuclear Corporation. SNC Lavalin, the successor to AECL, is pursuing new Candu 6 reactor sales in Argentina (Atucha 3), as well as China and Britain. Sales effort for the ACR reactor has ended.

The basic operation of the CANDU design is similar to other nuclear reactors. Fission reactions in the reactor core heat pressurized water in a "primary cooling loop". A heat exchanger, also known as a steam generator, transfers the heat to a "secondary cooling loop", which powers a steam turbine with an electric generator attached to it (for a typical Rankine thermodynamic cycle). The exhaust steam from the turbines is then cooled, condensed and returned as feedwater to the steam generator. The final cooling often uses cooling water from a nearby source, such as a lake, river, or ocean. Newer CANDU plants, such as the Darlington Nuclear Generating Station near Toronto, Ontario, use a diffuser to spread the warm outlet water over a larger volume and limit the effects on the environment. 

Where the CANDU design differs is details of the fissile core and the primary cooling loop. Natural uranium consists of a mix of mostly uranium-238 with small amounts of uranium-235 and trace amounts of other isotopes. Fission in these elements releases high-energy neutrons, which can cause other U-235 atoms in the fuel to undergo fission as well. This process is much more effective when the neutron energies are much lower than what the reactions release naturally. Most reactors use some form of neutron moderator to lower the energy of the neutrons, or "thermalize" them, which makes the reaction more efficient. The energy lost by the neutrons heats the moderator and is extracted for power.

Most commercial reactor designs use normal water as the moderator. Water absorbs some of the neutrons, enough that it is not possible to keep the reaction going in natural uranium. CANDU replaces this "light" water with heavy water.
Heavy water’s extra neutron decreases its ability to absorb excess neutrons, resulting in a better neutron economy. This allows CANDU to run on unenriched natural uranium, or uranium mixed with a wide variety of other materials such as plutonium and thorium. This was a major goal of the CANDU design; by operating on natural uranium the cost of enrichment is removed. This also presents an advantage in nuclear proliferation terms, as there is no need for enrichment facilities, which might also be used for weapons.

In conventional light-water reactor (LWR) designs, the entire fissile core is placed in a large pressure vessel. The amount of heat that can be removed by a unit of a coolant is a function of the temperature; by pressurizing the core, the water can be heated to much greater temperatures before boiling, thereby removing more heat and allowing the core to be smaller and more efficient. Building a pressure vessel of the required size is a challenge. At the time of CANDU's design, Canada lacked the heavy industry to cast and machine the pressure vessels of this size.

In CANDU the fuel bundles are instead contained in much smaller metal tubes about 10 cm diameter. The tubes are then contained in a larger vessel containing additional heavy water acting purely as a moderator. This vessel, known as a "calandria", is not pressurized and remains at much lower temperatures, making it much easier to fabricate. In order to prevent the heat from the pressure tubes leaking into the surrounding moderator, each fuel tube is enclosed in a second tube. Carbon dioxide gas in the gap between the two tubes acts as an insulator. The moderator tank also acts as a large heat sink that provides an additional safety feature.

In a conventional design with a pressurized core, refuelling the system requires the core to shut down and the pressure vessel to be opened. Due to the arrangement used in CANDU, only the single tube being refuelled needs to be depressurized. This allows the CANDU system to be continually refuelled without shutting down, another major design goal. In modern systems, two robotic machines hook up to the reactor faces and open the end caps of a pressure tube. One machine pushes in the new fuel, whereby the depleted fuel is pushed out and collected at the other end. A significant operational advantage of online refuelling is that a failed or leaking fuel bundle can be removed from the core once it has been located, thus reducing the radiation levels in the primary cooling loop.

Each fuel bundle is a cylinder assembled from alloy tubes containing ceramic pellets of fuel. In older designs, the assembly had 28 or 37 half-meter-long fuel tubes with 12 such assemblies lying end-to-end in a pressure tube. The newer CANFLEX bundle has 43 tubes, with two pellet sizes (so the power rating can be increased without melting the hottest pellets). It is about in diameter, long and weighs about and replaces the 37-tube bundle. To allow the neutrons to flow freely between the bundles, the tubes and bundles are made of neutron-transparent zircaloy (zirconium + 2.5% wt niobium).

Natural uranium is a mix of isotopes, mainly uranium-238, with 0.72% fissile uranium-235 by weight. A reactor aims for a steady rate of fission over time, where the neutrons released by fission cause an equal number of fissions in other fissile atoms. This balance is referred to as "criticality". The neutrons released in these reactions are fairly energetic and don't readily react with (get "captured" by) the surrounding fissile material. In order to improve this rate, they must have their energy "moderated", ideally to the same energy as the fuel atoms themselves. As these neutrons are in thermal equilibrium with the fuel, they are referred to as "thermal neutrons".

During moderation it helps to separate the neutrons and uranium, since U has a large affinity for intermediate-energy neutrons ("resonance" absorption), but is only easily fissioned by the few energetic neutrons above ≈1.5–2 MeV. Since most of the fuel is usually U, most reactor designs are based on thin fuel rods separated by moderator, allowing the neutrons to travel in the moderator before entering the fuel again. More neutrons are released than are needed to maintain the chain reaction; when uranium-238 absorbs just the excess, plutonium is created, which helps to make up for the depletion of uranium-235. Eventually the build-up of fission products that are even more neutron-absorbing than U slows the reaction and calls for refuelling.

Light water makes an excellent moderator, the light hydrogen atoms are very close in mass to a neutron and can absorb a lot of energy in a single collision (like a collision of two billiard balls). Light hydrogen is also fairly effective at absorbing neutrons, and there will be too few left over to react with the small amount of U in natural uranium, preventing criticality. In order to allow criticality, the fuel must be "enriched", increasing the amount of U to a usable level. In light-water reactors, the fuel is typically enriched to between 2% and 5% U (the leftover fraction with less U is called depleted uranium). Enrichment facilities are expensive to build and operate. They are also a proliferation concern, as they can be used to enrich the U much further, up to weapons-grade material (90% or more U). This can be remedied if the fuel is supplied and reprocessed by an internationally approved supplier.

The main advantage of heavy-water moderator over light water is the reduced absorption of the neutrons that sustain the chain reaction, allowing a lower concentration of active atoms (to the point of using unenriched natural uranium fuel). Deuterium ("heavy hydrogen") already has the extra neutron that light hydrogen would absorb, reducing the tendency to capture neutrons. Deuterium has twice the mass of a single neutron (vs light hydrogen, which has about the same mass); the mismatch means that more collisions are needed to moderate the neutrons, requiring a larger thickness of moderator between the fuel rods. This increases the size of the reactor core and the leakage of neutrons. It is also the practical reason for the calandria design, otherwise, a very large pressure vessel would be needed. The low U density in natural uranium also implies that less of the fuel will be consumed before the fission rate drops too low to sustain criticality, because the ratio of U to fission products + U is lower. In CANDU most of the moderator is at lower temperatures than in other designs, reducing the spread of speeds and the overall speed of the moderator particles. This means that most of the neutrons will end up at a lower energy and be more likely to cause fission, so CANDU not only "burns" natural uranium, but it does so more effectively as well. Overall, CANDU reactors use 30–40% less mined uranium than light-water reactors per unit of electricity produced. This is a major advantage of the heavy-water design; it not only requires less fuel, but as the fuel does not have to be enriched, it is much less expensive as well.

A further unique feature of heavy-water moderation is the greater stability of the chain reaction. This is due to the relatively low binding energy of the deuterium nucleus (2.2 MeV), leading to some energetic neutrons and especially gamma rays breaking the deuterium nuclei apart to produce extra neutrons. Both gammas produced directly by fission and by the decay of fission fragments have enough energy, and the half-lives of the fission fragments range from seconds to hours or even years. The slow response of these gamma-generated neutrons delays the response of the reactor and gives the operators extra time in case of an emergency. Since gamma rays travel for meters through water, an increased rate of chain reaction in one part of the reactor will produce a response from the rest of the reactor, allowing various negative feedbacks to stabilize the reaction.

On the other hand, the fission neutrons are thoroughly slowed down before they reach another fuel rod, meaning that it takes neutrons a longer time to get from one part of the reactor to the other. Thus if the chain reaction accelerates in one section of the reactor, the change will propagate itself only slowly to the rest of the core, giving time to respond in an emergency. The independence of the neutrons' energies from the nuclear fuel used is what allows such fuel flexibility in a CANDU reactor, since every fuel bundle will experience the same environment and affect its neighbors in the same way, whether the fissile material is uranium-235, uranium-233 or plutonium.

Canada developed the heavy-water-moderated design in the post–World War II era to explore nuclear energy while lacking access to enrichment facilities. War-era enrichment systems were extremely expensive to build and operate, whereas the heavy water solution allowed the use of natural uranium in the experimental ZEEP reactor. A much less expensive enrichment system was developed, but the United States classified work on the cheaper gas centrifuge process. The CANDU was therefore designed to use natural uranium.

The CANDU includes a number of active and passive safety features in its design. Some of these are a side effect of the physical layout of the system.

CANDU designs have a positive void coefficient, as well as a small power coefficient, normally considered bad in reactor design. This implies that steam generated in the coolant will "increase" the reaction rate, which in turn would generate more steam. This is one of the many reasons for the cooler mass of moderator in the calandria, as even a serious steam incident in the core would not have a major impact on the overall moderation cycle. Only if the moderator itself starts to boil, would there be any significant effect, and the large thermal mass ensures that this will occur slowly. The deliberately "sluggish" response of the fission process in CANDU allows controllers more time to diagnose and deal with problems.

The fuel channels can only maintain criticality if they are mechanically sound. If the temperature of the fuel bundles increases to the point where they are mechanically unstable, their horizontal layout means that they will bend under gravity, shifting the layout of the bundles and reducing the efficiency of the reactions. Because the original fuel arrangement is optimal for a chain reaction, and the natural uranium fuel has little excess reactivity, any significant deformation will stop the inter-fuel pellet fission reaction. This will not stop heat production from fission product decay, which would continue to supply a considerable heat output. If this process further weakens the fuel bundles, they will eventually bend far enough to touch the calandria tube, allowing heat to be efficiently transferred into the moderator tank. The moderator vessel has a considerable thermal capability on its own and is normally kept relatively cool.

Heat generated by fission products would initially be at about 7% of full reactor power, which requires significant cooling. The CANDU designs have several emergency cooling systems, as well as having limited self-pumping capability through thermal means (the steam generator is well above the reactor). Even in the event of a catastrophic accident and core meltdown, it is important to remember that the fuel is not critical in light water. This means that cooling the core with water from nearby sources will not add to the reactivity of the fuel mass.

Normally the rate of fission is controlled by light-water compartments called liquid zone controllers, which absorb excess neutrons, and by adjuster rods, which can be raised or lowered in the core to control the neutron flux. These are used for normal operation, allowing the controllers to adjust reactivity across the fuel mass, as different portions would normally burn at different rates depending on their position. The adjuster rods can also be used to slow or stop criticality. Because these rods are inserted into the low-pressure calandria, not the high-pressure fuel tubes, they would not be "ejected" by steam, a design issue for many pressurized-water reactors.

There are two independent, fast-acting safety shutdown systems as well. Shutoff rods are held above the reactor by electromagnets and drop under gravity into the core to quickly end criticality. This system works even in the event of a complete power failure, as the electromagnets only hold the rods out of the reactor when power is available. A secondary system injects a high-pressure gadolinium nitrate neutron absorber solution into the calandria.

A heavy-water design can sustain a chain reaction with a lower concentration of fissile atoms than light-water reactors, allowing it to use some alternative fuels; for example, "recovered uranium" (RU) from used LWR fuel. CANDU was designed for natural uranium with only 0.7% U-235, so RU with 0.9% U-235 is a rich fuel. This extracts a further 30–40% energy from the uranium. The DUPIC ("Direct Use of spent PWR fuel In CANDU") process under development can recycle it even without reprocessing. The fuel is sintered in air (oxidized), then in hydrogen (reduced) to break it into a powder, which is then formed into CANDU fuel pellets.
CANDU can also breed fuel from the more abundant thorium. This is being investigated by India to take advantage of its natural thorium reserves.

Even better than LWRs, CANDU can utilize a mix of uranium and plutonium oxides (MOX fuel), the plutonium either from dismantled nuclear weapons or reprocessed reactor fuel. The mix of isotopes in reprocessed plutonium is not attractive for weapons, but can be used as fuel (instead of being simply nuclear waste), while burning weapons-grade plutonium eliminates a proliferation hazard. If the aim is explicitly to burn plutonium or other actinides from spent fuel, then special inert-matrix fuels are proposed to do this more efficiently than MOX. Since they contain no uranium, these fuels do not breed any extra plutonium.

The neutron economy of heavy-water moderation and precise control of on-line refueling allow CANDU to use a great range of fuels other than enriched uranium, e.g., natural uranium, reprocessed uranium, thorium, plutonium, and used LWR fuel. Given the expense of enrichment, this can make fuel much cheaper. There is an initial investment into the tonnes of 99.75% pure heavy water to fill the core and heat-transfer system. In the case of the Darlington plant, costs released as part of a freedom of information act request put the overnight cost of the plant (four reactors totalling 3,512 MW net capacity) at $5.117 billion CAD (about $4.2 billion USD at early-1990s exchange rates). Total capital costs including interest were $14.319 billion CAD (about $11.9 billion USD) with the heavy water accounting for $1.528 billion, or 11%, of this.

Since heavy water is less efficient at slowing neutrons, CANDU needs a larger moderator-to-fuel ratio and a larger core for the same power output. Although a calandria-based core is cheaper to build, its size increases the cost for standard features like the containment building. Generally nuclear plant construction and operations are ≈65% of overall lifetime cost; for CANDU, costs are dominated by construction even more. Fueling CANDU is cheaper than other reactors, costing only ≈10% of the total, so the overall price per kWh electricity is comparable. The next-generation Advanced CANDU Reactor (ACR) mitigates these disadvantages by having light-water coolant and using a more compact core with less moderator.

When first introduced, CANDUs offered much better capacity factor (ratio of power generated to what would be generated by running at full power, 100% of the time) than LWRs of a similar generation. The light-water designs spent, on average, about half the time being refueled or maintained. Since the 1980s, dramatic improvements in LWR outage management have narrowed the gap, with several units achieving capacity factors ~90% and higher, with an overall fleet performance of 92% in 2010. The latest-generation CANDU 6 reactors have an 88–90% CF, but overall performance is dominated by the older Canadian units with CFs on the order of 80%. Refurbished units have demonstrated poor performance to date, on the order of 65%.

Some CANDU plants suffered from cost overruns during construction, often from external factors such as government action. For instance, a number of imposed construction delays led to roughly a doubling of the cost of the Darlington Nuclear Generating Station near Toronto, Ontario. Technical problems and redesigns added about another billion to the resulting $14.4 billion price. In contrast, in 2002 two CANDU 6 reactors at Qinshan in China were completed on-schedule and on-budget, an achievement attributed to tight control over scope and schedule.

In terms of safeguards against nuclear weapons proliferation, CANDUs meet a similar level of international certification as other reactors. The plutonium for India's first nuclear detonation, Operation Smiling Buddha in 1974, was produced in a CIRUS reactor supplied by Canada and partially paid for by the Canadian government using heavy water supplied by the United States. In addition to its two PHWR reactors, India has some safeguarded pressurised heavy-water reactors (PHWRs) based on the CANDU design, and two safeguarded light-water reactors supplied by the US. Plutonium has been extracted from the spent fuel from all of these reactors; India mainly relies on an Indian designed and built military reactor called Dhruva. The design is believed to be derived from the CIRUS reactor, with the Dhruva being scaled-up for more efficient plutonium production. It is this reactor which is thought to have produced the plutonium for India's more recent (1998) Operation Shakti nuclear tests.

Although heavy water is relatively immune to neutron capture, a small amount of the deuterium turns into tritium in this way. Tritium + deuterium mix undergoes nuclear fusion more easily than any other substance. Tritium can be used in both the "fusion boost" of a boosted fission weapon and the main fusion process of an H-bomb. In an H-bomb, it is usually created "in situ" by neutron irradiation of lithium-6.

Tritium is extracted from some CANDU plants in Canada, mainly to improve safety in case of heavy-water leakage. The gas is stockpiled and used in a variety of commercial products, notably "powerless" lighting systems and medical devices. In 1985 what was then Ontario Hydro sparked controversy in Ontario due to its plans to sell tritium to the U.S. The plan, by law, involved sales to non-military applications only, but some speculated that the exports could have freed American tritium for the U.S. nuclear weapons program. Future demands appear to outstrip production, in particular the demands of future generations of experimental fusion reactors like ITER. Currently between 1.5 and 2.1 kg of tritium are recovered yearly at the Darlington separation facility, of which a minor fraction is sold.

The 1998 Operation Shakti test series in India included one bomb of about 45 kt yield that India has publicly claimed was a hydrogen bomb. An offhand comment in the BARC publication "Heavy Water — Properties, Production and Analysis" appears to suggest that the tritium was extracted from the heavy water in the CANDU and PHWR reactors in commercial operation. "Janes Intelligence Review" quotes the Chairman of the Indian Atomic Energy Commission as admitting to the tritium extraction plant, but refusing to comment on its use. India is also capable of creating tritium more efficiently by irradiation of lithium-6 in reactors.

Tritium is a radioactive form of hydrogen (H-3), with a half-life of 12.3 years. 
It is produced in small amounts in nature (about 4 kg per year globally) by cosmic ray interactions in the upper atmosphere.
Tritium is considered a weak radionuclide because of its low-energy radioactive emissions (beta particle energy up to 18.6 keV). 
The beta particles travel 6 mm in air and only penetrate skin up to 6 micrometers. The biological half-life of inhaled, ingested, or absorbed tritium is 10–12 days.

Tritium is generated in the fuel of all reactors; CANDU reactors generate tritium also in their coolant and moderator, due to neutron capture in heavy hydrogen. 
Some of this tritium escapes into containment and is generally recovered; a small percentage (about 1%) escapes containment and is considered a routine radioactive emission (also higher than from an LWR of comparable size). Responsible operation of a CANDU plant therefore includes monitoring tritium in the surrounding environment (and publishing the results).

In some CANDU reactors the tritium is periodically extracted. Typical emissions from CANDU plants in Canada are less than 1% of the national regulatory limit, which is based on International Commission on Radiological Protection (ICRP) guidelines (for example, the maximal permitted drinking-water concentration for tritium in Canada, 7,000 Bq/L, corresponds to 1/10 of the ICRP's dose limit for members of the public). Tritium emissions from other CANDU plants are similarly low.

In general, there is significant public controversy about radioactive emissions from nuclear power plants, and for CANDU plants one of the main concerns is tritium. In 2007 Greenpeace published a critique of tritium emissions from Canadian nuclear power plants by Ian Fairlie. This report was criticized by Richard Osborne.

The CANDU development effort has gone through four major stages over time. The first systems were experimental and prototype machines of limited power. These were replaced by a second generation of machines of 500 to 600 MW (the CANDU 6), a series of larger machines of 900 MW, and finally developing into the CANDU 9 and current ACR-1000 effort.

The first heavy-water-moderated design in Canada was the ZEEP, which started operation just after the end of World War II. ZEEP was joined by several other experimental machines, including the NRX in 1947 and NRU in 1957. These efforts led to the first CANDU-type reactor, the Nuclear Power Demonstration (NPD), in Rolphton, Ontario. It was intended as a proof-of-concept and rated for only 22 MW, a very low power for a commercial power reactor. NPD produced the first nuclear-generated electricity in Canada and ran successfully from 1962 to 1987.

The second CANDU was the Douglas Point reactor, a more powerful version rated at roughly 200 MW and located near Kincardine, Ontario. It went into service in 1968 and ran until 1984. Uniquely among CANDU stations, Douglas Point had an oil-filled window with a view of the east reactor face, even when the reactor was operating. Douglas Point was originally planned to be a two-unit station, but the second unit was cancelled because of the success of the larger 515 MW units at Pickering.
Gentilly-1, in Bécancour, Quebec near Trois-Rivières, Quebec, was also an experimental version of CANDU, using a boiling light-water coolant and vertical pressure tubes, but was not considered successful and closed after seven years of fitful operation. Gentilly-2, a CANDU-6 reactor, has been operating since 1983. Following statements from the in-coming Parti Québécois government in September 2012 that Gentilly would close, the operator, Hydro-Québec, has decided to cancel a previously announced refurbishment of the plant and announced its shutdown at the end of 2012, citing economic reasons for the decision. The company will then undertake a 50-year decommissioning process estimated to cost $1.8 billion.

In parallel with the classic CANDU design, experimental variants were being developed. WR-1, located at the AECL's Whiteshell Laboratories in Pinawa, Manitoba, used vertical pressure tubes and organic oil as the primary coolant. The oil used has a higher boiling point than water, allowing the reactor to operate at higher temperatures and lower pressures than a conventional reactor. WR-1's outlet temperature was about 490 °C compared to the CANDU 6's nominal 310 °C, which means that , resulting in a smaller and less expensive core. The higher temperatures also result in more efficient conversion to steam, and ultimately, electricity. WR-1 operated successfully for many years and promised a significantly higher efficiency than water-cooled versions.

The successes at NPD and Douglas Point led to the decision to construct the first multi-unit station in Pickering, Ontario. Pickering A, consisting of Units 1 to 4, went into service in 1971. Pickering B with units 5 to 8 came online in 1983, giving a full-station capacity of 4,120 MW. The station is very close to the city of Toronto, in order to reduce transmission costs.

A series of improvements to the basic Pickering design led to the CANDU 6 design, which first went into operation in the early 1980s. CANDU 6 was essentially a version of the Pickering power plant that was redesigned to be able to be built in single-reactor units. CANDU 6 was used in several installations outside Ontario, including the Gentilly-2 in Quebec, and Point Lepreau Nuclear Generating Station in New Brunswick. CANDU 6 forms the majority of foreign CANDU systems, including the designs exported to Argentina, Romania, China and South Korea. Only India operates a CANDU system that is not based on the CANDU 6 design.

The economics of nuclear power plants generally scale well with size. This improvement at larger sizes is offset by the sudden appearance of large quantities of power on the grid, which leads to a lowering of electricity prices through supply and demand effects. Predictions in the late 1960s suggested that growth in electricity demand would overwhelm these downward pricing pressures, leading most designers to introduce plants in the 1000 MW range.

Pickering A was quickly followed by such an upscaling effort for the Bruce Nuclear Generating Station, constructed in stages between 1970 and 1987. It is the largest nuclear facility in North America and second largest in the world (after Kashiwazaki-Kariwa in Japan), with eight reactors at around 800 MW each, in total 6,232 MW (net) and 7,276 MW (gross). Another, smaller, upscaling led to the Darlington Nuclear Generating Station design, similar to the Bruce plant, but delivering about 880 MW per reactor.

As was the case for the development of the Pickering design into the CANDU 6, the Bruce design was also developed into the similar CANDU 9. Like the CANDU 6, the CANDU 9 is essentially a repackaging of the Bruce design, so that it can be built as a single-reactor unit. No CANDU 9 reactors have been built.

Through the 1980s and 90s the nuclear power market suffered a major crash, with few new plants being constructed in North America or Europe. Design work continued throughout, and new design concepts were introduced that dramatically improved safety, capital costs, economics and overall performance. These generation III+ and generation IV machines became a topic of considerable interest in the early 2000s, as it appeared that a nuclear renaissance was underway and large numbers of new reactors would be built over the next decade.

AECL had been working on a design known as the ACR-700, using elements of the latest versions of the CANDU 6 and CANDU 9, with a design power of 700 MW. During the nuclear renaissance, the upscaling seen in the earlier years re-expressed itself, and the ACR-700 was developed into the 1200 MW ACR-1000. ACR-1000 is the next-generation (officially, "generation III+") CANDU technology, which makes some significant modifications to the existing CANDU design.

The main change, and the most radical among the CANDU generations, is the use of pressurized light water as the coolant. This significantly reduces the cost of implementing the primary cooling loop, which no longer has to be filled with expensive heavy water. The ACR-1000 uses about 1/3rd the heavy water needed in earlier-generation designs. It also eliminates tritium production in the coolant loop, the major source of tritium leaks in operational CANDU designs. The redesign also allows a slightly negative void reactivity, a major design goal of all Gen III+ machines.

The design also requires the use of slightly enriched uranium, enriched by about 1 or 2%. The main reason for this is to increase the burn-up ratio, allowing bundles to remain in the reactor longer, so that only a third as much spent fuel is produced. This also has effects on operational costs and timetables, as the refuelling frequency is reduced. As is the case with earlier CANDU designs, the ACR-1000 also offers online refuelling.

Outside of the reactor, the ACR-1000 has a number of design changes that are expected to dramatically lower capital and operational costs. Primary among these changes is the design lifetime of 60 years, which dramatically lowers the price of the electricity generated over the lifetime of the plant. The design also has an expected capacity factor of 90%. Higher-pressure steam generators and turbines improve efficiency downstream of the reactor.

Many of the operational design changes were also applied to the existing CANDU 6 to produce the Enhanced CANDU 6. Also known as CANDU 6e or EC 6, this was an evolutionary upgrade of the CANDU 6 design with a gross output of 740 MW per unit. The reactors are designed with a lifetime of over 50 years, with a mid-life program to replace some of the key components e.g. the fuel channels. The projected average annual capacity factor is more than 90%. Improvements to construction techniques (including modular, open-top assembly) decrease construction costs. The CANDU 6e is designed to operate at power settings as low as 50%, allowing them to adjust to load demand much better than the previous designs.

By most measures, the CANDU is "the Ontario reactor". The system was developed almost entirely in Ontario, and only two experimental designs were built in other provinces. Of the 29 commercial CANDU reactors built, 22 are in Ontario. Of these 22, a number of reactors have been removed from service. Two new CANDU reactors have been proposed for Darlington with Canadian government help with financing, but these plans ended in 2009 due to high costs.

AECL has heavily marketed CANDU within Canada, but has found a limited reception. To date, only two non-experimental reactors have been built in other provinces, one each in Quebec and New Brunswick, other provinces have concentrated on hydro and coal-fired plants. Several Canadian provinces have developed large amounts of hydro power. Alberta and Saskatchewan do not have extensive hydro resources, and use mainly fossil fuels to generate electric power.

Interest has been expressed in Western Canada, where CANDU reactors are being considered as heat and electricity sources for the energy-intensive oil sands extraction process, which currently uses natural gas. Energy Alberta Corporation announced 27 August 2007 that they had applied for a licence to build a new nuclear plant at Lac Cardinal (30 km west of the town of Peace River, Alberta), with two ACR-1000 reactors going online in 2017 producing 2.2 gigawatts (electric). A 2007 parliamentary review suggested placing the development efforts on hold.

The company was later purchased by Bruce Power, who proposed expanding the plant to four units of a total 4.4 gigawatts. These plans were upset and Bruce later withdrew its application for the Lac Cardinal, proposing instead a new site about 60 km away. The plans are currently moribund after a wide consultation with the public demonstrated that while about of the population were open to reactors, were opposed.

During the 1970s, the international nuclear sales market was extremely competitive, with many national nuclear companies being supported by their governments' foreign embassies. In addition, the pace of construction in the United States had meant that cost overruns and delayed completion was generally over, and subsequent reactors would be cheaper. Canada, a relatively new player on the international market, had numerous disadvantages in these efforts. The CANDU was deliberately designed to reduce the need for very large machined parts, making it suitable for construction by countries without a major industrial base. Sales efforts have had their most success in countries that could not locally build designs from other firms.

In the late 1970s, AECL noted that each reactor sale would employ 3,600 Canadians and result in $300 million in balance-of-payments income. These sales efforts were aimed primarily at countries being run by dictatorships or similar, a fact that led to serious concerns in parliament. These efforts also led to a scandal when it was discovered millions of dollars had been given to foreign sales agents, with little or no record of who they were, or what they did to earn the money. This led to a Royal Canadian Mounted Police investigation after questions were raised about sales efforts in Argentina, and new regulations on full disclosure of fees for future sales.

CANDU's first success was the sale of early CANDU designs to India. In 1963, an agreement was signed for export of a 200 MWe power reactor based on the Douglas Point reactor. The success of the deal led to the 1966 sale of a second reactor of the same design. The first reactor, then known as RAPP-1 for "Rajasthan Atomic Power Project", began operation in 1972. A serious problem with cracking of the reactor's end shield led to the reactor being shut down for long periods, and the reactor was finally downrated to 100 MW. Construction of the RAPP-2 reactor was still underway when India detonated its first atomic bomb in 1974, leading to Canada ending nuclear dealings with the country. Part of the sales agreement was a technology transfer process. When Canada withdrew from development, India continued construction of CANDU-like plants across the country. By 2010, CANDU-based reactors were operational at the following sites: Kaiga (3), Kakrapar (2), Madras (2), Narora (2), Rajasthan (6), and Tarapur (2).

In Pakistan, the Karachi Nuclear Power Plant with a gross capacity of 137 MW was built between 1966 and 1971.

In 1972, AECL submitted a design based on the Pickering plant to Argentina's Comision Nacional de Energia Atomica process, in partnership with the Italian company Italimpianti. High inflation during construction led to massive losses, and efforts to re-negotiate the deal were interrupted by the March 1976 coup led by General Videla. The Embalse Nuclear Power Station began commercial operation in January 1984. There have been ongoing negotiations to open more CANDU 6 reactors in the country, including a 2007 deal between Canada, China and Argentina, but to date no firm plans have been announced.

A licensing agreement with Romania was signed in 1977, selling the CANDU 6 design for $5 million per reactor for the first four reactors, and then $2 million each for the next twelve. In addition, Canadian companies would supply a varying amount of equipment for the reactors, about $100 million of the first reactor's $800 million price tag, and then falling over time. In 1980, Nicolae Ceaușescu asked for a modification to provide goods instead of cash, in exchange the amount of Canadian content was increased and a second reactor would be built with Canadian help. Economic troubles in the country worsened throughout the construction phase. The first reactor of the Cernavodă Nuclear Power Plant only came online in April 1996, a decade after its December 1985 predicted startup. Further loans were arranged for completion of the second reactor, which went online in November 2007.

In January 1975, a deal was announced for a single CANDU 6 reactor to be built in South Korea, now known as the Wolsong-1 Power Reactor. Construction started in 1977 and commercial operation began in April 1983. In December 1990 a further deal was announced for three additional units at the same site, which began operation in the period 1997–1999. South Korea also negotiated development and technology transfer deals with Westinghouse for their advanced System-80 reactor design, and all future development is based on locally built versions of this reactor.

In June 1998, construction started on a CANDU 6 reactor in Qinshan China Qinshan Nuclear Power Plant, as Phase III (units 4 and 5) of the planned 11 unit facility. Commercial operation began in December 2002 and July 2003, respectively. These are the first heavy water reactors in China. Qinshan is the first CANDU-6 project to use open-top reactor building construction, and the first project where commercial operation began earlier than the projected date.

CANDU Energy is continuing marketing efforts in China. In addition, China and Argentina have agreed a contract to build a 700 MWe Candu-6 derived reactor. Construction is planned to start in 2018 at Atucha.

The cost of electricity from any power plant can be calculated by roughly the same selection of factors: capital costs for construction or the payments on loans made to secure that capital, the cost of fuel on a per-watt-hour basis, and fixed and variable maintenance fees. In the case of nuclear power, one normally includes two additional costs, the cost of permanent waste disposal, and the cost of decommissioning the plant when its useful lifetime is over. Generally, the capital costs dominate the price of nuclear power, as the amount of power produced is so large that it overwhelms the cost of fuel and maintenance. The World Nuclear Association calculates that the cost of fuel, including all processing, accounts for less than one cent (0.01 USD) per kWh.

Information on economic performance on CANDU is somewhat lopsided; the majority of reactors are in Ontario, which is also the "most public" among the major CANDU operators, so their performance dominates the available information. Based on Ontario's record, the economic performance of the CANDU system is quite poor. Although much attention has been focussed on the problems with the Darlington plant, every CANDU design in Ontario went over budget by at least 25%, and average over 150% higher than estimated. Darlington was the worst, at 350% over budget, but this project was stopped in-progress thereby incurring additional interest charges during a period of high interest rates, which is a special situation that was not expected to repeat itself.

In the 1980s, the pressure tubes in the Pickering A reactors were replaced ahead of design life due to unexpected deterioration caused by hydrogen embrittlement. Extensive inspection and maintenance has avoided this problem in later reactors.

All the Pickering A and Bruce A reactors were shut down in 1999 in order to focus on restoring operational performance in the later generations at Pickering, Bruce, and Darlington. Before restarting the Pickering A reactors, OPG undertook a limited refurbishment program. The original cost and time estimates based on inadequate project scope development were greatly below the actual time and cost and it was determined that Pickering units 2 and 3 would not be restarted for commercial reasons. Despite this refurbishment, the reactors have not performed well since the restart.

These overruns were repeated at Bruce, with Units 3 and 4 running 90% over budget. Similar overruns were experienced at Point Lepreau, and Gentilly-2 plant was shut down on December 28, 2012.

Based on the projected capital costs, and the low cost of fuel and in-service maintenance, in 1994 power from CANDU was predicted to be well under 5 cents/kWh. In 1998, Ontario Hydro calculated that the cost of generation from CANDU was 7.7 cents/kWh, whereas hydropower was only 1.1 cents/kWh, and their coal-fired plants were 4.3 cents/kWh. As Ontario Hydro received a regulated price averaging 6.3 cents/kWh for power in this period, the revenues from the other forms of generation were being used to fund the operating losses of the nuclear plants. The debt left over from the nuclear construction could not be included in the rate base until the reactors were declared in service, thereby exacerbating the total capital cost of construction with unpaid interest, at that time around $15 billion, and another $3.5 billion in debts throughout the system was held by a separate entity and repaid through a standing charge on electricity bills.

In 1999, Ontario Hydro was broken up and its generation facilities re-formed into Ontario Power Generation (OPG). In order to make the successor companies more attractive for private investors, $19.4 billion in "stranded debt" was placed in the control of the Ontario Electricity Financial Corporation. This debt is slowly paid down through a variety of sources, including a 0.7-cent/kWh tariff on all power, all income taxes paid by all operating companies, and all dividends paid by the OPG and Hydro One. Even with these sources of income, the amount of debt has grown on several occasions, and in 2010 stood at almost $15 billion. This is in spite of total payments on the order of $19 billion, ostensibly enough to have paid off the debt entirely if interest repayment requirements are ignored.

Darlington is currently in the process of considering a major re-build of several units, as it too is reaching its design mid-life time. The budget is currently estimated to be between $8.5 and $14 billion, and produce power at 6 to 8 cents/kWh. This prediction is based on three assumptions that appear to have never been met in operation: that the rebuild will be completed on-budget, that the system will operate at an average capacity utilization of 82%, and that the Ontario taxpayer will pay 100% of any cost overruns. Although Darlington Units 1, 3 and 4 have operated with an average lifetime annual capacity factor of 85% and Unit 2 with a capacity factor of 78%, refurbished units at Pickering and Bruce have lifetime capacity factors between 59 and 69%. This includes periods of several years while the units were shut down for the retubing and refurbishing. In 2009, Bruce A units 3 and 4 had capacity factors of 80.5% and 76.7% respectively, in a year when they had a major Vacuum Building outage.

Today there are 31 CANDU reactors in use around the world, and 13 "CANDU-derivatives" in India, developed from the CANDU design. After India detonated a nuclear bomb in 1974, Canada stopped nuclear dealings with India. The breakdown is:




</doc>
<doc id="7477" url="https://en.wikipedia.org/wiki?curid=7477" title="Cuitláhuac">
Cuitláhuac

Cuitláhuac (, ) (c. 1476 – 1520) or Cuitláhuac (in Spanish orthography; , , honorific form Cuitlahuatzin) was the 10th "tlatoani" (ruler) of the Aztec city of Tenochtitlan for 80 days during the year Two Flint (1520). He is credited with leading the Mexica resistance to the Spanish invasion, following the death of his kinsman Moctezuma II.

Cuitláhuac was the eleventh son of the ruler Axayacatl and an older brother of Moctezuma II, the late ruler of Tenochtitlan, who died during the Spanish occupation of the city. His mother's father, also called Cuitlahuac, had been ruler of Iztapalapa, and the younger Cuitláhuac also ruled there initially. Cuitláhuac was an experienced warrior and an adviser to Moctezuma, warning him not to allow the Spaniards to enter Tenochtitlan. Hernán Cortés imprisoned both Moctezuma and Cuitláhuac. Following the massacre of Aztec elites when Cortés was away from Tenochtitlan, the Mexica besieged the Spanish and their indigenous allies. Cuitláhuac was released on the pretense to reopen the market to get food to the invaders. Moctezuma was killed under disputed circumstances, and Cuitláhuac was elected "tlatoani" following the flight of the Spaniards and their allies form Tenochtitlan on June 30, 1520. Some sources claim he was serving in that role even before Moctezuma's death.

Cuitláhuac was ritually married to Moctezuma's eldest daughter, a ten- or eleven-year-old girl, who later was called Isabel Moctezuma.
Cuitláhuac ruled just 80 days, perhaps dying from smallpox that had been introduced to the New World by the Europeans. The early sources do not explicitly say from what he succumbed. Immediately after Cuitláhuac's death, Cuauhtémoc was made the next "tlatoani".

The modern Mexican municipality of Cuitláhuac, Veracruz and the Mexico City Metro station Metro Cuitláhuac are named in honor of Cuitláhuac. The asteroid 2275 Cuitláhuac is also named after this ruler.

There is an Avenue in Mexico City Called Cuitláhuac (Eje 3 Norte) that runs from Avenue Insurgentes to Avenue Mexico-Tacuba and that is part of an inner ring; also many streets in other towns and villages in Mexico are so called.


 


</doc>
<doc id="7478" url="https://en.wikipedia.org/wiki?curid=7478" title="Cuauhtémoc">
Cuauhtémoc

Cuauhtémoc (, also known as Cuauhtemotzin, Guatimozin or Guatemoc; c. 1495) was the Aztec ruler ("tlatoani") of Tenochtitlan from 1520 to 1521, making him the last Aztec Emperor. The name Cuauhtemōc means "one who has descended like an eagle", and is commonly rendered in English as "Descending Eagle", as in the moment when an eagle folds its wings and plummets down to strike its prey. This is a name that implies aggressiveness and determination.

Cuauhtémoc took power in 1520 as successor of Cuitláhuac and was a cousin of the late emperor Moctezuma II. His young wife, who was later known as Isabel Moctezuma, was one of Moctezuma's daughters. He ascended to the throne when he was around 25 years old, while Tenochtitlan was being besieged by the Spanish and devastated by an epidemic of smallpox brought to the New World by the invaders. After the killings in the Great Temple, there were probably few Aztec captains available to take the position.

Cuauhtemoc's date of birth is unknown, as he does not enter the historical record until he became emperor. He was the eldest legitimate son of Emperor Ahuitzotl and may well have attended the last New Fire ceremony, marking the beginning of a new 52-year cycle in the Aztec calendar. Like the rest of Cuauhtemoc's early biography, that is inferred from knowledge of his age, and the likely events and life path of someone of his rank. Following education in the calmecac, the school for elite boys, and then his military service, he was named ruler of Tlatelolco, with the title "cuauhtlatoani" ("eagle ruler") in 1515. To have reached this position of rulership, Cuauhtemoc had to be a male of high birth and a warrior who had captured enemies for sacrifice.

When Cuauhtemoc was elected tlatoani in 1520, Tenochtitlan had already been rocked by the invasion of the Spanish and their indigenous allies, the death of Moctezuma II, and the death of Moctezuma's brother Cuitlahuac, who succeeded him as ruler, but died of smallpox shortly afterwards. In keeping with traditional practice, the most able candidate among the high noblemen was chosen by vote of the highest noblemen, Cuauhtemoc assumed the rulership. Although under Cuitlahuac Tenochtitlan began mounting a defense against the invaders, it was increasingly isolated militarily and largely faced the crisis alone, as the numbers of Spanish allies increased with the desertion of many polities previously under its control.
Cuauhtémoc called for reinforcements from the countryside to aid the defense of Tenochtitlán, after eighty days of warfare against the Spanish. Of all the Nahuas, only Tlatelolcas remained loyal, and the surviving Tenochcas looked for refuge in Tlatelolco, where even women took part in the battle. Cuauhtémoc was captured on August 13, 1521, while fleeing Tenochtitlán by crossing Lake Texcoco with his wife, family, and friends.

He surrendered to Hernán Cortés along with the surviving "pipiltin" (nobles) and, according to Spanish sources, he asked Cortés to take his knife and "strike me dead immediately". According to the same Spanish accounts, Cortés refused the offer and treated his foe magnanimously. "You have defended your capital like a brave warrior," he declared. "A Spaniard knows how to respect valor, even in an enemy."

At Cuauhtémoc's request, Cortés also allowed the defeated Mexica to depart the city unmolested. Subsequently, however, when the booty found did not measure up to the Spaniards' expectations, Cuauhtémoc was subjected to "torture by fire", whereby the soles of his bare feet were slowly broiled over red-hot coals, in an unsuccessful attempt to discover its whereabouts. On the statue to Cuauhtemoc, on the Paseo de la Reforma in Mexico City, there is a bas relief showing the Spaniards' torture of the emperor. Eventually, some gold was recovered but far less than Cortés and his men expected.

Cuauhtémoc continued to hold his position under the Spanish, keeping the title of tlatoani, but he was no longer the sovereign ruler. He ordered the construction of a renaissance-style two-storied stone palace in Tlatelolco, in which he settled after the destruction of Mexico City; the building survived and was known as the Tecpan or palace.

In 1525, Cortés took Cuauhtémoc and several other indigenous nobles on his expedition to Honduras, as he feared that Cuauhtémoc could have led an insurrection in his absence. While the expedition was stopped in the Chontal Maya capital of Itzamkanac, known as Acalan in Nahuatl, Cortés had Cuauhtémoc executed for allegedly conspiring to kill him and the other Spaniards.
There are a number of discrepancies in the various accounts of the event. According to Cortés himself, on 27 February 1525, he learned from a citizen of Tenochtitlan, Mexicalcingo, that Cuauhtémoc, Coanacoch (the ruler of Texcoco), and Tetlepanquetzal, the ruler of Tlacopan, were plotting his death. Cortés interrogated them until each confessed and then had Cuauhtémoc, Tetlepanquetzal, and another lord, Tlacatlec, hanged. Cortés wrote that the other lords would be too frightened to plot against him again, as they believed he had uncovered the plan through magic powers. Cortés's account is supported by the historian Francisco López de Gómara.

According to Bernal Díaz del Castillo, a conquistador serving under Cortés who recorded his experiences in his book "The True History of the Conquest of New Spain", the supposed plot was revealed by two men, named Tapia and Juan Velásquez. Díaz portrays the executions as unjust and based on no evidence, and he admits to having liked Cuauhtémoc personally. He also records Cuauhtémoc giving the following speech to Cortés through his interpreter Malinche:

Díaz wrote that afterwards, Cortés suffered from insomnia because of guilt and badly injured himself while he was wandering at night.

Fernando de Alva Cortés Ixtlilxóchitl, a mestizo historian and descendant of Coanacoch, wrote an account of the executions in the 17th century partly based on Texcocan oral tradition. According to Ixtlilxóchitl, the three lords were joking cheerfully with one another because of a rumor that Cortés had decided to return the expedition to Mexico, when Cortés asked a spy to tell him what they were talking about. The spy reported honestly, but Cortés invented the plot himself. Cuauhtémoc, Coanacoch, and Tetlepanquetzal were hanged as well as eight others. However, Cortés cut down Coanacoch, the last to be hanged, after his brother began rallying his warriors. Coanacoch did not have long to enjoy his reprieve, as Ixtlilxóchitl wrote that he died a few days later.

Tlacotzin, Cuauhtémoc's "cihuacoatl", was appointed his successor as "tlatoani". He died the next year before he could return to Tenochtitlan.

The modern-day town of Ixcateopan in the state of Guerrero is home to an ossuary purportedly containing Cuauhtémoc's remains. Archeologist Eulalia Guzmán, a "passionate indigenista", excavated the bones in 1949, which were discovered shortly after bones of Cortés, found in Mexico City, had been authenticated by the Instituto Nacional de Antropología e Historia (INAH). Initially, Mexican scholars congratulated Guzmán, but after a similar examination by scholars at INAH, their authenticity as Cuauhtemoc's was rejected, as the bones in the ossuary belonged to several different persons, several of them seemingly women. The finding caused a public uproar. A panel assembled by Guzmán gave support to the initial contention. The Secretariat of Public Education (SEP) had another panel examine the bones, which gave support to INAH's original finding, but did not report on the finding publicly. A scholarly study of the controversy was published in 2011 and argued that the available data suggests that the grave is an elaborate hoax prepared by a local of Ichcateopan as a way of generating publicity, and that subsequently supported by Mexican nationalists such as Guzman who wished to use the find for political purposes.

Cuauhtemoc is the embodiment of indigenist nationalism in Mexico, being the only Aztec emperor who survived the conquest by the Spanish Empire (and their native allies). He is honored by a monument on the Paseo de la Reforma, his face has appeared on Mexican banknotes, and he is celebrated in paintings, music, and popular culture.

Many places in Mexico are named in honour of Cuauhtémoc. These include Ciudad Cuauhtémoc in Chihuahua and the Cuauhtémoc borough of the Mexican Federal District, as well as Ciudad Cuauhtémoc, in the state of Veracruz.

There is a Cuauhtémoc station on Line 1 of the Mexico City metro as well as one for Moctezuma, but none for Hernán Cortés. There is also a metro station in Monterrey named after him.

Cuauhtémoc is also one of the few non-Spanish given names for Mexican boys that is perennially popular.
Cuauhtémoc Cárdenas Solórzano, a prominent Mexican politician, is named after him. In the Aztec campaign of the PC game "", the player plays as Cuauhtémoc, despite the name "Montezuma" for the campaign itself, and Cuauhtémoc narrates the openings and closings to each scenario. In the next installment to the series, "", Cuauhtémoc was the leader of Aztecs. The Mexican football player Cuauhtémoc Blanco was also named after him.

In the 1996 Rage Against The Machine single "People of the Sun", lyricist Zack De La Rocha rhymes "When the fifth sun sets get back reclaimed, The spirit of Cuauhtémoc alive and untamed".

Cuauhtémoc, in the name Guatemoc, is portrayed sympathetically in the adventure novel "Montezuma's Daughter", by H. Rider Haggard. First appearing in Chapter XIV, he becomes friends with the protagonist after they save each other's lives. His coronation, torture, and death are described in the novel.


 


</doc>
<doc id="7480" url="https://en.wikipedia.org/wiki?curid=7480" title="Cross section (physics)">
Cross section (physics)

When two particles interact, their mutual cross section is the area transverse to their relative motion within which they must meet in order to scatter from each other. If the particles are hard inelastic spheres that interact only upon contact, their scattering cross section is related to their geometric size. If the particles interact through some action-at-a-distance force, such as electromagnetism or gravity, their scattering cross section is generally larger than their geometric size. When a cross section is specified as a function of some final-state variable, such as particle angle or energy, it is called a differential cross section. When a cross section is integrated over all scattering angles (and possibly other variables), it is called a total cross section. Cross sections are typically denoted (sigma) and measured in units of area.

Scattering cross sections may be defined in nuclear, atomic, and particle physics for collisions of accelerated beams of one type of particle with targets (either stationary or moving) of a second type of particle. The probability for any given reaction to occur is in proportion to its cross section. Thus, specifying the cross section for a given reaction is a proxy for stating the probability that a given scattering process will occur.

The measured reaction rate of a given process depends strongly on experimental variables such as the density of the target material, the intensity of the beam, the detection efficiency of the apparatus, or the angle setting of the detection apparatus. However, these quantities can be factored away, allowing measurement of the underlying two-particle collisional cross section.

Differential and total scattering cross sections are among the most important measurable quantities in nuclear, atomic, and particle physics.

In a gas of finite-sized particles there are collisions among particles that depend on their cross-sectional size. The average distance that a particle travels between collisions depends on the density of gas particles. These quantities are related by

where

If the particles in the gas can be treated as hard spheres of radius that interact by direct contact, as illustrated in Figure 1, then the effective cross section for the collision of a pair is

If the particles in the gas interact by a force with a larger range than their physical size, then the cross section is a larger effective area that may depend on a variety of variables such as the energy of the particles.

Cross sections can be computed for atomic collisions but also are used in the subatomic realm. For example, in nuclear physics a "gas" of low-energy neutrons collides with nuclei in a reactor or other nuclear device, with a cross section that is energy-dependent and hence also with well-defined mean free path between collisions.

If a beam of particles enters a thin layer of material of thickness , the flux of the beam will decrease by according to

where is the total cross section of "all" events, including scattering, absorption, or transformation to another species. The number density of scattering centers is designated by . Solving this equation exhibits the exponential attenuation of the beam intensity:

where is the initial flux, and is the total thickness of the material. For light, this is called the Beer–Lambert law.

Consider a classical measurement where a single particle is scattered off a single stationary target particle. Conventionally, a Spherical coordinate system is used, with the target placed at the origin and the axis of this coordinate system aligned with the incident beam. The angle is the scattering angle, measured between the incident beam and the scattered beam, and the is the azimuthal angle.

The impact parameter is the perpendicular offset of the trajectory of the incoming particle, and the outgoing particle emerges at an angle . For a given interaction (Coulombic, magnetic, gravitational, contact, etc.), the impact parameter and the scattering angle have a definite one-to-one functional dependence on each other. Generally the impact parameter can neither be controlled nor measured from event to event and is assumed to take all possible values when averaging over many scattering events. The differential size of the cross section is the area element in the plane of the impact parameter, i.e. . The differential angular range of the scattered particle at angle is the solid angle element . The differential cross section is the quotient of these quantities, .

It is a function of the scattering angle (and therefore also the impact parameter), plus other observables such as the momentum of the incoming particle. The differential cross section is always taken to be positive, even though larger impact parameters generally produce less deflection. In cylindrically symmetric situations (about the beam axis), the azimuthal angle is not changed by the scattering process, and the differential cross section can be written as

In situations where the scattering process is not azimuthally symmetric, such as when the beam or target particles possess magnetic moments oriented perpendicular to the beam axis, the differential cross section must also be expressed as a function of the azimuthal angle.

For scattering of particles of incident flux off a stationary target consisting of many particles, the differential cross section at an angle is related to the flux of scattered particle detection in particles per unit time by

Here is the finite angular size of the detector (SI unit: sr), is the number density of the target particles (SI units: m), and is the thickness of the stationary target (SI units: m). This formula assumes that the target is thin enough that each beam particle will interact with at most one target particle.

The total cross section may be recovered by integrating the differential cross section over the full solid angle ( steradians):

It is common to omit the “differential” qualifier when the type of cross section can be inferred from context. In this case, may be referred to as the "integral cross section" or "total cross section". The latter term may be confusing in contexts where multiple events are involved, since “total” can also refer to the sum of cross sections over all events.

The differential cross section is extremely useful quantity in many fields of physics, as measuring it can reveal a great amount of information about the internal structure of the target particles. For example, the differential cross section of Rutherford scattering provided strong evidence for the existence of the atomic nucleus.

Instead of the solid angle, the momentum transfer may be used as the independent variable of differential cross sections.

Differential cross sections in inelastic scattering contain resonance peaks that indicate the creation of metastable states and contain information about their energy and lifetime.

In the time-independent formalism of quantum scattering, the initial wave function (before scattering) is taken to be a plane wave with definite momentum :

where and are the "relative" coordinates between the projectile and the target. The arrow indicates that this only describes the "asymptotic behavior" of the wave function when the projectile and target are too far apart for the interaction to have any effect.

After the scattering takes place, it is expected that the wave function takes on the following asymptotic form:

where is some function of the angular coordinates known as the scattering amplitude. This general form is valid for any short-ranged, energy-conserving interaction. It is not true for long-ranged interactions, so there are additional complications when dealing with electromagnetic interactions.

The full wave function of the system behaves asymptotically as the sum

The differential cross section is related to the scattering amplitude:

This has the simple interpretation as the probability density for finding the scattered projectile at a given angle.

A cross section is therefore a measure of the effective surface area seen by the impinging particles, and as such is expressed in units of area. The cross section of two particles (i.e. observed when the two particles are colliding with each other) is a measure of the interaction event between the two particles. The cross section is proportional to the probability that an interaction will occur; for example in a simple scattering experiment the number of particles scattered per unit of time (current of scattered particles ) depends only on the number of incident particles per unit of time (current of incident particles ), the characteristics of target (for example the number of particles per unit of surface ), and the type of interaction. For we have

If the reduced masses and momenta of the colliding system are , and , before and after the collision respectively, the differential cross section is given by

where the on-shell matrix is defined by

in terms of the S-matrix. Here is the Dirac delta function. The computation of the S-matrix is the main goal of the scattering theory.

Although the SI unit of total cross sections is m, smaller units are usually used in practice.

In nuclear and particle physics, the conventional unit is the barn b, where 1 b = 10 m = 100 fm. Smaller prefixed units such as mb and μb are also widely used. Correspondingly, the differential cross section can be measured in units such as mb/sr.

When the scattered radiation is visible light, it is conventional to measure the path length in centimetres. To avoid the need for conversion factors, the scattering cross section is expressed in cm, and the number concentration in cm. The measurement of the scattering of visible light is known as nephelometry, and is effective for particles of 2–50 µm in diameter: as such, it is widely used in meteorology and in the measurement of atmospheric pollution.

The scattering of X-rays can also be described in terms of scattering cross sections, in which case the square ångström is a convenient unit: 1 Å = 10 m = = 10 b.

For light, as in other settings, the scattering cross section is generally different from the geometrical cross section of a particle, and it depends upon the wavelength of light and the permittivity, shape and size of the particle. The total amount of scattering in a sparse medium is proportional to the product of the scattering cross section and the number of particles present.

In terms of area, the "total cross section" () is the sum of the cross sections due to absorption, scattering and luminescence. The sum of the absorption and scattering cross sections is sometimes referred to as the extinction cross section.
The total cross section is related to the absorbance of the light intensity through the Beer–Lambert law, which says that absorbance is proportional to particle concentration:
where is the absorbance at a given wavelength , is the particle concentration as a number density, and is the path length. The absorbance of the radiation is the logarithm (decadic or, more usually, natural) of the reciprocal of the transmittance :

In the context of scattering light on extended bodies, the scattering cross section, , describes the likelihood of light being scattered by a macroscopic particle. In general, the scattering cross section is different from the geometrical cross section of a particle, as it depends upon the wavelength of light and the permittivity in addition to the shape and size of the particle. The total amount of scattering in a sparse medium is determined by the product of the scattering cross section and the number of particles present. In terms of area, the "total cross section" () is the sum of the cross sections due to absorption, scattering and luminescence:

The total cross section is related to the absorbance of the light intensity through the Beer–Lambert law, which says that absorbance is proportional to concentration: , where is the absorbance at a given wavelength , is the concentration as a number density, and is the path length. The extinction or absorbance of the radiation is the logarithm (decadic or, more usually, natural) of the reciprocal of the transmittance :

There is no simple relationship between the scattering cross section and the physical size of the particles, as the scattering cross section depends on the wavelength of radiation used. This can be seen when driving in foggy weather: the droplets of water (which form the fog) scatter red light less than they scatter the shorter wavelengths present in white light, and the red rear fog light can be distinguished more clearly than the white headlights of an approaching vehicle. That is to say that the scattering cross section of the water droplets is smaller for red light than for light of shorter wavelengths, even though the physical size of the particles is the same.

The scattering cross section is related to the meteorological range :

The quantity is sometimes denoted , the scattering coefficient per unit length.

The elastic collision of two hard spheres is an instructive example that demonstrates the sense of calling this quantity a cross section. and are respectively the radii of the scattering center and scattered sphere.
The total cross section is

So in this case the total scattering cross section is equal to the area of the circle (with radius ) within which the center of mass of the incoming sphere has to arrive for it to be deflected, and outside which it passes by the stationary scattering center.

Another example illustrates the details of the calculation of a simple light scattering model obtained by a reduction of the dimension. For simplicity, we will consider the scattering of a beam of light on a plane treated as a uniform density of parallel rays and within the framework of geometrical optics from a circle with radius with a perfectly reflecting boundary. Its three-dimensional equivalent is therefore the more difficult problem of a laser or flashlight light scattering from the mirror sphere, for example, from the mechanical bearing ball. The unit of cross section in one dimension is the unit of length, for example 1 m. Let be the angle between the light ray and the radius joining the reflection point of the light ray with the center point of the circle mirror. Then the increase of the length element perpendicular to the light beam is expressed by this angle as
the reflection angle of this ray with respect to the incoming ray is then , and the scattering angle is
The energy or the number of photons reflected from the light beam with the intensity or density of photons on the length is 
The differential cross section is therefore ()
As it is seen from the behaviour of the sine function, this quantity has the maximum for the backward scattering (; the light is reflected perpendicularly and returns), and the zero minimum for the scattering from the edge of the circle directly forward (). It confirms the intuitive expectations that the mirror circle acts like a diverging lens, and a thin beam is more diluted the closer it is from the edge defined with respect to the incoming direction. The total cross section can be obtained by summing (integrating) the differential section of the entire range of angles:
so it is equal as much as the circular mirror is totally screening the two-dimensional space for the beam of light. In three dimensions for the mirror ball with the radius it is therefore equal .

We can now use the result from the Example 2 to calculate the differential cross section for the light scattering from the perfectly reflecting sphere in three dimensions. Let us denote now the radius of the sphere as . Let us parametrize the plane perpendicular to the incoming light beam by the cylindrical coordinates and . In any plane of the incoming and the reflected ray we can write now from the previous example:
while the impact area element is 
Using the relation for the solid angle in the spherical coordinates:
and the trigonometric identity
we obtain
while the total cross section as we expected is
As one can see, it also agrees with the result from the Example 1 if the photon is assumed to be a rigid sphere of zero radius.

Notes

Sources




</doc>
<doc id="7482" url="https://en.wikipedia.org/wiki?curid=7482" title="Christian mythology">
Christian mythology

Christian mythology is the body of myths associated with Christianity. The term encompasses a broad variety of stories and legends. Various authors have used it to refer to the mythological and allegorical elements found in the Bible, such as the story of the Leviathan. The term has been applied to myths and legends from the Middle Ages, such as the story of Saint George and the Dragon, the stories of King Arthur and his Knights of the Round Table, and the legends of the "Parsival". Multiple commentators have classified John Milton's epic poem "Paradise Lost" as a work of "Christian mythology". The term has also been applied to modern stories revolving around Christian themes and motifs, such as the writings of C. S. Lewis, J. R. R. Tolkien, Madeleine L'Engle, and George MacDonald.

Mythological themes and elements occur throughout Christian literature, including recurring myths such as ascending to a mountain, the "axis mundi", myths of combat, descent into the Underworld, accounts of a dying-and-rising god, flood stories, stories about the founding of a tribe or city, and myths about great heroes (or saints) of the past, paradises, and self-sacrifice.

In ancient Greek, "muthos", from which the English word "myth" derives, meant "story" or "narrative." Early Christians contrasted their sacred stories with "myths", by which they meant false and pagan stories.

Several modern Christian writers, such as C.S. Lewis, have described elements of Christianity, particularly the story of Christ, as "myth" which is also "true" ("true myth"). Others object to associating Christianity with "myth" for a variety of reasons: the association of the term "myth" with polytheism, the use of the term "myth" to indicate falsehood or non-historicity, and the lack of an agreed-upon definition of "myth".

As examples of Biblical myths, Every cites the creation account in Genesis 1 and 2 and the story of Eve's temptation. Many Christians believe parts of the Bible to be symbolic or metaphorical (such as the Creation in Genesis).

Mythic patterns such as the primordial struggle between good and evil appear in passages throughout the Hebrew Bible, including passages that describe historical events. A distinctive characteristic of the Hebrew Bible is the reinterpretation of myth on the basis of history, as in the Book of Daniel, a record of the experience of the Jews of the Second Temple period under foreign rule, presented as a prophecy of future events and expressed in terms of "mythic structures" with "the Hellenistic kingdom figured as a terrifying monster that cannot but recall [the Near Eastern pagan myth of] the dragon of chaos".

Mircea Eliade argues that the imagery used in some parts of the Hebrew Bible reflects a "transfiguration of history into myth". For example, Eliade says, the portrayal of Nebuchadnezzar as a dragon in Jeremiah 51:34 is a case in which the Hebrews "interpreted contemporary events by means of the very ancient cosmogonico-heroic myth" of a battle between a hero and a dragon.

According to scholars including Neil Forsyth and John L. McKenzie, the Old Testament incorporates stories, or fragments of stories, from extra-biblical mythology. According to the "New American Bible", a Catholic Bible translation produced by the Confraternity of Christian Doctrine, the story of the Nephilim in Genesis 6:1-4 "is apparently a fragment of an old legend that had borrowed much from ancient mythology", and the "sons of God" mentioned in that passage are "celestial beings of mythology". The "New American Bible" also says that Psalm 93 alludes to "an ancient myth" in which God battles a personified Sea. Some scholars have identified the biblical creature Leviathan as a monster from Canaanite mythology. According to Howard Schwartz, "the myth of the fall of Lucifer" existed in fragmentary form in Isaiah 14:12 and other ancient Jewish literature; Schwartz claims that the myth originated from "the ancient Canaanite myth of Athtar, who attempted to rule the throne of Ba'al, but was forced to descend and rule the underworld instead".

Some scholars have argued that the calm, orderly, monotheistic creation story in Genesis 1 can be interpreted as a reaction against the creation myths of other Near Eastern cultures. In connection with this interpretation, David and Margaret Leeming describe Genesis 1 as a "demythologized myth", and John L. McKenzie asserts that the writer of Genesis 1 has "excised the mythical elements" from his creation story.

Perhaps the most famous topic in the Bible that could possibly be connected with mythical origins is the topic of Heaven (or the sky) as the place where God (or angels, or the saints) resides, with stories such as the ascension of Elijah (who disappeared in the sky), war of man with an angel, flying angels. Even in the New Testament Saint Paul is said to "have visited the third heaven", and Jesus was portrayed in several books as going to return from Heaven on a cloud, in the same way he ascended thereto. The official text repeated by the attendees during Roman Catholic mass (the Apostles' Creed) contains the words "He ascended into Heaven, and is Seated at the Right Hand of God, The Father. From thence He will come again to judge the living and the dead". Medieval cosmology adapted its view of the Cosmos to conform with these scriptures, in the concept of celestial spheres (later attacked, amongst others, by Giordano Bruno). Some famous opponents of religion, including John Lennon and Stephen Hawking, mentioned this in their public works.

According to a number of scholars, the Christ story contains mythical themes such as descent to the underworld, the heroic monomyth, and the "dying god" (see section below on "mythical themes and types").

Some scholars have argued that the Book of Revelation incorporates imagery from ancient mythology. According to the "New American Bible", the image in Revelation 12:1-6 of a pregnant woman in the sky, threatened by a dragon, "corresponds to a widespread myth throughout the ancient world that a goddess pregnant with a savior was pursued by a horrible monster; by miraculous intervention, she bore a son who then killed the monster". Bernard McGinn suggests that the image of the two Beasts in Revelation stems from a "mythological background" involving the figures of Leviathan and Behemoth.

The Pastoral Epistles contain denunciations of "myths" ("muthoi"). This may indicate that Rabbinic or Gnostic mythology was popular among the early Christians to whom the epistles were written and that the epistles' author was attempting to resist that mythology.

The Sibylline oracles contain predictions that the dead Roman Emperor Nero, infamous for his persecutions, would return one day as an Antichrist-like figure. According to Bernard McGinn, these parts of the oracles were probably written by a Christian and incorporated "mythological language" in describing Nero's return.

According to Mircea Eliade, the Middle Ages witnessed "an upwelling of mythical thought" in which each social group had its own "mythological traditions". Often a profession had its own "origin myth" which established models for members of the profession to imitate; for example, the knights tried to imitate Lancelot or Parsifal. The medieval trouveres developed a "mythology of woman and Love" which incorporated Christian elements but, in some cases, ran contrary to official church teaching.

George Every includes a discussion of medieval legends in his book "Christian Mythology". Some medieval legends elaborated upon the lives of Christian figures such as Christ, the Virgin Mary, and the saints. For example, a number of legends describe miraculous events surrounding Mary's birth and her marriage to Joseph.

In many cases, medieval mythology appears to have inherited elements from myths of pagan gods and heroes. According to Every, one example may be "the myth of St. George" and other stories about saints battling dragons, which were "modelled no doubt in many cases on older representations of the creator and preserver of the world in combat with chaos". Eliade notes that some "mythological traditions" of medieval knights, namely the Arthurian cycle and the Grail theme, combine a veneer of Christianity with traditions regarding the Celtic Otherworld. According to Lorena Laura Stookey, "many scholars" see a link between stories in "Irish-Celtic mythology" about journeys to the Otherworld in search of a cauldron of rejuvenation and medieval accounts of the quest for the Holy Grail.

According to Eliade, "eschatological myths" became prominent during the Middle Ages during "certain historical movements". These eschatological myths appeared "in the Crusades, in the movements of a Tanchelm and an Eudes de l'Etoile, in the elevation of Fredrick II to the rank of Messiah, and in many other collective messianic, utopian, and prerevolutionary phenomena". One significant eschatological myth, introduced by Gioacchino da Fiore's theology of history, was the "myth of an imminent third age that will renew and complete history" in a "reign of the Holy Spirit"; this "Gioacchinian myth" influenced a number of messianic movements that arose in the late Middle Ages.

During the Renaissance, there arose a critical attitude that sharply distinguished between apostolic tradition and what George Every calls "subsidiary mythology"—popular legends surrounding saints, relics, the cross, etc.—suppressing the latter.

The works of Renaissance writers often included and expanded upon Christian and non-Christian stories such as those of creation and the Fall. Rita Oleyar describes these writers as "on the whole, reverent and faithful to the primal myths, but filled with their own insights into the nature of God, man, and the universe". An example is John Milton's "Paradise Lost", an "epic elaboration of the Judeo-Christian mythology" and also a "veritable encyclopedia of myths from the Greek and Roman tradition".

According to Cynthia Stewart, during the Reformation, the Protestant reformers used "the founding myths of Christianity" to critique the church of their time.

Every argues that "the disparagement of myth in our own civilization" stems partly from objections to perceived idolatry, objections which intensified in the Reformation, both among Protestants and among Catholics reacting against the classical mythology revived during the Renaissance.

The philosophes of the Enlightenment used criticism of myth as a vehicle for veiled criticisms of the Bible and the church. According to Bruce Lincoln, the philosophes "made irrationality the hallmark of myth and constituted philosophy—rather than the Christian "kerygma"—as the antidote for mythic discourse. By implication, Christianity could appear as a more recent, powerful, and dangerous instance of irrational myth".

Some commentators have categorized a number of modern fantasy works as "Christian myth" or "Christian mythopoeia". Examples include the fiction of C.S. Lewis, Madeleine L'Engle, J.R.R. Tolkien, and George MacDonald.

In "The Eternal Adam and the New World Garden", written in 1968, David W. Noble argued that the Adam figure had been "the central myth in the American novel since 1830". As examples, he cites the works of Cooper, Hawthorne, Melville, Twain, Hemingway, and Faulkner.

According to Lorena Laura Stookey, many myths feature sacred mountains as "the sites of revelations": "In myth, the ascent of the holy mountain is a spiritual journey, promising purification, insight, wisdom, or knowledge of the sacred". As examples of this theme, Stookey includes the revelation of the Ten Commandments on Mount Sinai, Christ's ascent of a mountain to deliver his Sermon on the Mount, and Christ's ascension into Heaven from the Mount of Olives.

Many mythologies involve a "world center", which is often the sacred place of creation; this center often takes the form of a tree, mountain, or other upright object, which serves as an "axis mundi" or axle of the world. A number of scholars have connected the Christian story of the crucifixion at Golgotha with this theme of a cosmic center. In his "Creation Myths of the World", David Leeming argues that, in the Christian story of the crucifixion, the cross serves as "the "axis mundi", the center of a new creation".

According to a tradition preserved in Eastern Christian folklore, Golgotha was the summit of the cosmic mountain at the center of the world and the location where Adam had been both created and buried. According to this tradition, when Christ is crucified, his blood falls on Adam's skull, buried at the foot of the cross, and redeems him. George Every discusses the connection between the cosmic center and Golgotha in his book "Christian Mythology", noting that the image of Adam's skull beneath the cross appears in many medieval representations of the crucifixion.

In "Creation Myths of the World", Leeming suggests that the Garden of Eden may also be considered a world center.

Many Near Eastern religions include a story about a battle between a divine being and a dragon or other monster representing chaos—a theme found, for example, in the "Enuma Elish". A number of scholars call this story the "combat myth". A number of scholars have argued that the ancient Israelites incorporated the combat myth into their religious imagery, such as the figures of Leviathan and Rahab, the Song of the Sea, Isaiah 51:9-10's description of God's deliverance of his people from Babylon, and the portrayals of enemies such as Pharaoh and Nebuchadnezzar. The idea of Satan as God's opponent may have developed under the influence of the combat myth. Scholars have also suggested that the Book of Revelation uses combat myth imagery in its descriptions of cosmic conflict.

According to Christian tradition, Christ descended to hell after his death, in order to free the souls there; this event is known as the Harrowing of Hell. This story is narrated in the Gospel of Nicodemus and may be the meaning behind 1 Peter 3:18-22. According to David Leeming, writing in "The Oxford Companion to World Mythology", the harrowing of hell is an example of the motif of the hero's descent to the underworld, which is common in many mythologies.

Many myths, particularly from the Near East, feature a god who dies and is resurrected; this figure is sometimes called the "dying god". An important study of this figure is James George Frazer's "The Golden Bough", which traces the dying god theme through a large number of myths. The dying god is often associated with fertility. A number of scholars, including Frazer, have suggested that the Christ story is an example of the "dying god" theme. In the article "Dying god" in "The Oxford Companion to World Mythology", David Leeming notes that Christ can be seen as bringing fertility, though of a spiritual as opposed to physical kind.

In his 2006 homily for Corpus Christi, Pope Benedict XVI noted the similarity between the Christian story of the resurrection and pagan myths of dead and resurrected gods: "In these myths, the soul of the human person, in a certain way, reached out toward that God made man, who, humiliated unto death on a cross, in this way opened the door of life to all of us."

Many cultures have myths about a flood that cleanses the world in preparation for rebirth. Such stories appear on every inhabited continent on earth. An example is the biblical story of Noah. In "The Oxford Companion to World Mythology", David Leeming notes that, in the Bible story, as in other flood myths, the flood marks a new beginning and a second chance for creation and humanity.

According to Sandra Frankiel, the records of "Jesus' life and death, his acts and words" provide the "founding myths" of Christianity. Frankiel claims that these founding myths are "structurally equivalent" to the creation myths in other religions, because they are "the pivot around which the religion turns to and which it returns", establishing the "meaning" of the religion and the "essential Christian practices and attitudes". Tom Cain uses the expression "founding myths" more broadly, to encompass such stories as those of the War in Heaven and the fall of man; according to Cain, "the disastrous consequences of disobedience" is a pervasive theme in Christian founding myths.

In his influential 1909 work "Der Mythus von der Geburt des Helden" (The Myth of the Birth of the Hero), Otto Rank argued that the births of many mythical heroes follow a common pattern. Rank includes the story of Christ's birth as a representative example of this pattern.

According to Mircea Eliade, one pervasive mythical theme associates heroes with the slaying of dragons, a theme which Eliade traces back to "the very ancient cosmogonico-heroic myth" of a battle between a divine hero and a dragon. He cites the Christian legend of Saint George as an example of this theme. An example from the later Middle Ages comes from Dieudonné de Gozon, third Grand Master of the Knights of Rhodes, famous for slaying the dragon of Malpasso. Eliade writes:
"Legend, as was natural, bestowed upon him the attributes of St. George, famed for his victorious fight with the monster. […] In other words, by the simple fact that he was regarded as a hero, de Gozon was identified with a category, an archetype, which […] equipped him with a mythical biography from which it was "impossible" to omit combat with a reptilian monster."
In the "Oxford Companion to World Mythology" David Leeming lists Moses, Jesus, and King Arthur as examples of the "heroic monomyth", calling the Christ story "a particularly complete example of the heroic monomyth". Leeming regards resurrection as a common part of the heroic monomyth, in which the resurrected heroes often become sources of "material or spiritual food for their people"; in this connection, Leeming notes that Christians regard Jesus as the "bread of life".

In terms of values, Leeming contrasts "the myth of Jesus" with the myths of other "Christian heroes such as St. George, Roland, el Cid, and even King Arthur"; the later hero myths, Leeming argues, reflect the survival of pre-Christian heroic values—"values of military dominance and cultural differentiation and hegemony"—more than the values expressed in the Christ story.

Many religious and mythological systems contain myths about a paradise. Many of these myths involve the loss of a paradise that existed at the beginning of the world. Some scholars have seen in the story of the Garden of Eden an instance of this general motif.

Sacrifice is an element in many religious traditions and often represented in myths. In "The Oxford Companion to World Mythology", David Leeming lists the story of Abraham and Isaac and the story of Christ's death as examples of this theme. Wendy Doniger describes the gospel accounts as a "meta-myth" in which Jesus realizes that he is part of a "new myth [...] of a man who is sacrificed in hate" but "sees the inner myth, the old myth of origins and acceptance, the myth of a god who sacrifices himself in love".

According to Mircea Eliade, many traditional societies have a cyclic sense of time, periodically reenacting mythical events. Through this reenactment, these societies achieve an "eternal return" to the mythical age. According to Eliade, Christianity retains a sense of cyclical time, through the ritual commemoration of Christ's life and the imitation of Christ's actions; Eliade calls this sense of cyclical time a "mythical aspect" of Christianity.

However, Judeo-Christian thought also makes an "innovation of the first importance", Eliade says, because it embraces the notion of linear, historical time; in Christianity, "time is no longer [only] the circular Time of the Eternal Return; it has become linear and irreversible Time". Summarizing Eliade's statements on this subject, Eric Rust writes, "A new religious structure became available. In the Judaeo-Christian religions—Judaism, Christianity, Islam—history is taken seriously, and linear time is accepted. [...] The Christian myth gives such time a beginning in creation, a center in the Christ-event, and an end in the final consummation."

Heinrich Zimmer also notes Christianity's emphasis on linear time; he attributes this emphasis specifically to the influence of Saint Augustine's theory of history. Zimmer does not explicitly describe the cyclical conception of time as itself "mythical" per se, although he notes that this conception "underl[ies] Hindu mythology".

Neil Forsyth writes that "what distinguishes both Jewish and Christian religious systems [...] is that they elevate to the sacred status of myth narratives that are situated in historical time".

According to Carl Mitcham, "the Christian mythology of progress toward transcendent salvation" created the conditions for modern ideas of scientific and technological progress. Hayden White describes "the myth of Progress" as the "secular, Enlightenment counterpart" of "Christian myth". Reinhold Niebuhr described the modern idea of ethical and scientific progress as "really a rationalized version of the Christian myth of salvation".

According to Mircea Eliade, the medieval "Gioacchinian myth [...] of universal renovation in a more or less imminent future" has influenced a number of modern theories of history, such as those of Lessing (who explicitly compares his views to those of medieval "enthusiasts"), Fichte, Hegel, and Schelling; and has also influenced a number of Russian writers.

Calling Marxism "a truly messianic Judaeo-Christian ideology", Eliade writes that Marxism "takes up and carries on one of the great eschatological myths of the Middle Eastern and Mediterranean world, namely: the redemptive part to be played by the Just (the 'elect', the 'anointed', the 'innocent', the 'missioners', in our own days the proletariat), whose sufferings are invoked to change the ontological status of the world".

In his article "The Christian Mythology of Socialism", Will Herberg argues that socialism inherits the structure of its ideology from the influence of Christian mythology upon western thought.

In "The Oxford Companion to World Mythology", David Leeming claims that Judeo-Christian messianic ideas have influenced 20th-century totalitarian systems, citing the state ideology of the Soviet Union as an example.

According to Hugh S. Pyper, the biblical "founding myths of the Exodus and the exile, read as stories in which a nation is forged by maintaining its ideological and racial purity in the face of an oppressive great power", entered "the rhetoric of nationalism throughout European history", especially in Protestant countries and smaller nations.



</doc>
<doc id="7484" url="https://en.wikipedia.org/wiki?curid=7484" title="Company (disambiguation)">
Company (disambiguation)

A company is a group of more than one persons to carry out an enterprise and so a form of business organization.

Company may also refer to:





</doc>
<doc id="7485" url="https://en.wikipedia.org/wiki?curid=7485" title="Corporation">
Corporation

A corporation is a company or group of people or an organisation authorized to act as a single entity (legally a person) and recognized as such in law. Early incorporated entities were established by charter (i.e. by an "ad hoc" act granted by a monarch or passed by a parliament or legislature). Most jurisdictions now allow the creation of new corporations through registration. Corporations enjoy limited liability for their investors, which can lead to losses being externalized from investors to the government or general public.

Corporations come in many different types but are usually divided by the law of the jurisdiction where they are chartered into two kinds: by whether they can issue stock or not, or by whether they are formed to make a profit or not.

Where local law distinguishes corporations by the ability to issue stock, corporations allowed to do so are referred to as "stock corporations", ownership of the corporation is through stock, and owners of stock are referred to as "stockholders" or "shareholders". Corporations not allowed to issue stock are referred to as "non-stock" corporations; those who are considered the owners of a non-stock corporation are persons (or other entities) who have obtained membership in the corporation and are referred to as a "member" of the corporation.

Corporations chartered in regions where they are distinguished by whether they are allowed to be for profit or not are referred to as "for profit" and "not-for-profit" corporations, respectively.

There is some overlap between stock/non-stock and for-profit/not-for-profit in that not-for-profit corporations are always non-stock as well. A for-profit corporation is almost always a stock corporation, but some for-profit corporations may choose to be non-stock. To simplify the explanation, whenever "Stockholder" or "shareholder" is used in the rest of this article to refer to a stock corporation, it is presumed to mean the same as "member" for a non-profit corporation or for a profit, non-stock corporation.

Registered corporations have legal personality and their shares are owned by shareholders whose liability is generally limited to their investment. Shareholders do not typically actively manage a corporation; shareholders instead elect or appoint a board of directors to control the corporation in a fiduciary capacity. In most circumstances, a shareholder may also serve as a director or officer of a corporation.

In American English, the word "corporation" is most often used to describe large business corporations. In British English and in the Commonwealth countries, the term "company" is more widely used to describe the same sort of entity while the word "corporation" encompasses all incorporated entities. In American English, the word "company" can include entities such as partnerships that would not be referred to as companies in British English as they are not a separate legal entity.

Late in the 19th century, a new form of company having the limited liability protections of a corporation, and the more favorable tax treatment of either a sole proprietorship or partnership was developed. While not a corporation, this new type of entity became very attractive as an alternative for corporations not needing to issue stock. In Germany, the organization was referred to as "Gesellschaft mit beschränkter Haftung" or "GmbH". In the last quarter of the 20th Century this new form of non-corporate organization became available in the United States and other countries, and was known as the "limited liability company" or "LLC". Since the GmbH and LLC forms of organization are technically not corporations (even though they have many of the same features), they will not be discussed in this article.

The word "corporation" derives from "corpus", the Latin word for body, or a "body of people". By the time of Justinian (reigned 527–565), Roman law recognized a range of corporate entities under the names "universitas", "corpus" or "collegium". These included the state itself (the "Populus Romanus"), municipalities, and such private associations as sponsors of a religious cult, burial clubs, political groups, and guilds of craftsmen or traders. Such bodies commonly had the right to own property and make contracts, to receive gifts and legacies, to sue and be sued, and, in general, to perform legal acts through representatives. Private associations were granted designated privileges and liberties by the emperor.

Entities which carried on business and were the subjects of legal rights were found in ancient Rome, and the Maurya Empire in ancient India. In medieval Europe, churches became incorporated, as did local governments, such as the Pope and the City of London Corporation. The point was that the incorporation would survive longer than the lives of any particular member, existing in perpetuity. The alleged oldest commercial corporation in the world, the Stora Kopparberg mining community in Falun, Sweden, obtained a charter from King Magnus Eriksson in 1347.

In medieval times, traders would do business through common law constructs, such as partnerships. Whenever people acted together with a view to profit, the law deemed that a partnership arose. Early guilds and livery companies were also often involved in the regulation of competition between traders.

The progenitors of the modern corporation were the chartered companies, such as the Dutch East India Company (VOC) and the Hudson's Bay Company, which were created to lead the colonial ventures of European nations in the 17th century. Acting under a charter sanctioned by the Dutch government, the Dutch East India Company defeated Portuguese forces and established itself in the Moluccan Islands in order to profit from the European demand for spices. Investors in the VOC had issued paper certificates as proof of share ownership, and were able to trade their shares on the original Amsterdam Stock Exchange. Shareholders were also explicitly granted limited liability in the company's royal charter.

In England, the government created corporations under a royal charter or an Act of Parliament with the grant of a monopoly over a specified territory. The best-known example, established in 1600, was the East India Company of London. Queen Elizabeth I granted it the exclusive right to trade with all countries to the east of the Cape of Good Hope. Some corporations at this time would act on the government's behalf, bringing in revenue from its exploits abroad. Subsequently, the Company became increasingly integrated with English and later British military and colonial policy, just as most corporations were essentially dependent on the Royal Navy's ability to control trade routes.

Labeled by both contemporaries and historians as "the grandest society of merchants in the universe", the English East India Company would come to symbolize the dazzlingly rich potential of the corporation, as well as new methods of business that could be both brutal and exploitative. On 31 December 1600, Queen Elizabeth I granted the company a 15-year monopoly on trade to and from the East Indies and Africa. By 1711, shareholders in the East India Company were earning a return on their investment of almost 150 per cent. Subsequent stock offerings demonstrated just how lucrative the Company had become. Its first stock offering in 1713–1716 raised £418,000, its second in 1717–1722 raised £1.6 million.
A similar chartered company, the South Sea Company, was established in 1711 to trade in the Spanish South American colonies, but met with less success. The South Sea Company's monopoly rights were supposedly backed by the Treaty of Utrecht, signed in 1713 as a settlement following the War of the Spanish Succession, which gave Great Britain an "asiento" to trade in the region for thirty years. In fact the Spanish remained hostile and let only one ship a year enter. Unaware of the problems, investors in Britain, enticed by extravagant promises of profit from company promoters bought thousands of shares. By 1717, the South Sea Company was so wealthy (still having done no real business) that it assumed the public debt of the British government. This accelerated the inflation of the share price further, as did the Bubble Act 1720, which (possibly with the motive of protecting the South Sea Company from competition) prohibited the establishment of any companies without a Royal Charter. The share price rose so rapidly that people began buying shares merely in order to sell them at a higher price, which in turn led to higher share prices. This was the first speculative bubble the country had seen, but by the end of 1720, the bubble had "burst", and the share price sank from £1000 to under £100. As bankruptcies and recriminations ricocheted through government and high society, the mood against corporations, and errant directors was bitter.

In the late 18th century, Stewart Kyd, the author of the first treatise on corporate law in English, defined a corporation as:

Due to the late 18th century abandonment of mercantilist economic theory and the rise of classical liberalism and laissez-faire economic theory due to a revolution in economics led by Adam Smith and other economists, corporations transitioned from being government or guild affiliated entities to being public and private economic entities free of governmental directions.

Adam Smith wrote in his 1776 work "The Wealth of Nations" that mass corporate activity could not match private entrepreneurship, because people in charge of others' money would not exercise as much care as they would with their own.

The British Bubble Act 1720's prohibition on establishing companies remained in force until its repeal in 1825. By this point, the Industrial Revolution had gathered pace, pressing for legal change to facilitate business activity. The repeal was the beginning of a gradual lifting on restrictions, though business ventures (such as those chronicled by Charles Dickens in "Martin Chuzzlewit") under primitive companies legislation were often scams. Without cohesive regulation, proverbial operations like the "Anglo-Bengalee Disinterested Loan and Life Assurance Company" were undercapitalised ventures promising no hope of success except for richly paid promoters.

The process of incorporation was possible only through a royal charter or a private act and was limited, owing to Parliament's jealous protection of the privileges and advantages thereby granted. As a result, many businesses came to be operated as unincorporated associations with possibly thousands of members. Any consequent litigation had to be carried out in the joint names of all the members and was almost impossibly cumbersome. Though Parliament would sometimes grant a private act to allow an individual to represent the whole in legal proceedings, this was a narrow and necessarily costly expedient, allowed only to established companies.

Then, in 1843, William Gladstone became the chairman of a Parliamentary Committee on Joint Stock Companies, which led to the Joint Stock Companies Act 1844, regarded as the first modern piece of company law. The Act created the Registrar of Joint Stock Companies, empowered to register companies by a two-stage process. The first, provisional, stage cost £5 and did not confer corporate status, which arose after completing the second stage for another £5. For the first time in history, it was possible for ordinary people through a simple registration procedure to incorporate. The advantage of establishing a company as a separate legal person was mainly administrative, as a unified entity under which the rights and duties of all investors and managers could be channeled.

However, there was still no limited liability and company members could still be held responsible for unlimited losses by the company. The next, crucial development, then, was the Limited Liability Act 1855, passed at the behest of the then Vice President of the Board of Trade, Mr. Robert Lowe. This allowed investors to limit their liability in the event of business failure to the amount they invested in the company – shareholders were still liable directly to creditors, but just for the unpaid portion of their shares. (The principle that shareholders are liable to the corporation had been introduced in the Joint Stock Companies Act 1844).

The 1855 Act allowed limited liability to companies of more than 25 members (shareholders). Insurance companies were excluded from the act, though it was standard practice for insurance contracts to exclude action against individual members. Limited liability for insurance companies was allowed by the Companies Act 1862.

This prompted the English periodical "The Economist" to write in 1855 that "never, perhaps, was a change so vehemently and generally demanded, of which the importance was so much overrated. " The major error of this judgment was recognised by the same magazine more than 70 years later, when it claimed that, "[t]he economic historian of the future. . . may be inclined to assign to the nameless inventor of the principle of limited liability, as applied to trading corporations, a place of honour with Watt and Stephenson, and other pioneers of the Industrial Revolution. "

These two features – a simple registration procedure and limited liability – were subsequently codified into the landmark 1856 Joint Stock Companies Act. This was subsequently consolidated with a number of other statutes in the Companies Act 1862, which remained in force for the rest of the century, up to and including the time of the decision in "Salomon v A Salomon & Co Ltd".

The legislation shortly gave way to a railway boom, and from then, the numbers of companies formed soared. In the later nineteenth century, depression took hold, and just as company numbers had boomed, many began to implode and fall into insolvency. Much strong academic, legislative and judicial opinion was opposed to the notion that businessmen could escape accountability for their role in the failing businesses.

In 1892, Germany introduced the Gesellschaft mit beschränkter Haftung with a separate legal personality and limited liability even if all the shares of the company were held by only one person. This inspired other countries to introduce corporations of this kind.

The last significant development in the history of companies was the 1897 decision of the House of Lords in "Salomon v. Salomon & Co." where the House of Lords confirmed the separate legal personality of the company, and that the liabilities of the company were separate and distinct from those of its owners.

In the United States, forming a corporation usually required an act of legislation until the late 19th century. Many private firms, such as Carnegie's steel company and Rockefeller's Standard Oil, avoided the corporate model for this reason (as a trust). State governments began to adopt more permissive corporate laws from the early 19th century, although these were all restrictive in design, often with the intention of preventing corporations for gaining too much wealth and power.

New Jersey was the first state to adopt an "enabling" corporate law, with the goal of attracting more business to the state, in 1896. In 1899, Delaware followed New Jersey's lead with the enactment of an enabling corporate statute, but Delaware only became the leading corporate state after the enabling provisions of the 1896 New Jersey corporate law were repealed in 1913.

The end of the 19th century saw the emergence of holding companies and corporate mergers creating larger corporations with dispersed shareholders. Countries began enacting anti-trust laws to prevent anti-competitive practices and corporations were granted more legal rights and protections.
The 20th century saw a proliferation of laws allowing for the creation of corporations by registration across the world, which helped to drive economic booms in many countries before and after World War I. Another major post World War I shift was toward the development of conglomerates, in which large corporations purchased smaller corporations to expand their industrial base.

Starting in the 1980s, many countries with large state-owned corporations moved toward privatization, the selling of publicly owned (or 'nationalised') services and enterprises to corporations. Deregulation (reducing the regulation of corporate activity) often accompanied privatization as part of a laissez-faire policy.

Under perfect competition, there would be no corporations; individuals would trade among themselves. Since the conditions for perfect competition cannot be met, however, market failures arise. Some of these can be overcome by contracts, but in many cases the transactions costs will simply be so high that it is not worth doing business or the necessary agreements would be unenforcable, leading to missing markets. Corporations allow individuals to overcome certain market failures and create markets that would otherwise be missing. 

According to Coase in "The nature of the firm" "the distinguishing mark of the firm is the supersession of the price mechanism... Outside the firm, price movements direct production, which is coordinated through a series of exchange transactions on the market. Within a firm, these market transactions are eliminated and in place of the complicated market structure with exchange transactions is substituted the entrepreneur-coordinator, who directs production."

One of the conditions of perfect competition is no returns to scale. In the real world, however, such returns exist, and corporations are one way to allow the economy to capture the efficiences of scaling up.

A corporation is, at least in theory, owned and controlled by its members. In a joint-stock company the members are known as shareholders and each of their shares in the ownership, control, and profits of the corporation is determined by the portion of shares in the company that they own. Thus a person who owns a quarter of the shares of a joint-stock company owns a quarter of the company, is entitled to a quarter of the profit (or at least a quarter of the profit given to shareholders as dividends) and has a quarter of the votes capable of being cast at general meetings.

In another kind of corporation, the legal document which established the corporation or which contains its current rules will determine who the corporation's members are. Who a member is depends on what kind of corporation is involved. In a worker cooperative, the members are people who work for the cooperative. In a credit union, the members are people who have accounts with the credit union.

The day-to-day activities of a corporation are typically controlled by individuals appointed by the members. In some cases, this will be a single individual but more commonly corporations are controlled by a committee or by committees. Broadly speaking, there are two kinds of committee structure.

Historically, corporations were created by a charter granted by government. Today, corporations are usually registered with the state, province, or national government and regulated by the laws enacted by that government. Registration is the main prerequisite to the corporation's assumption of limited liability. The law sometimes requires the corporation to designate its principal address, as well as a registered agent (a person or company designated to receive legal service of process). It may also be required to designate an agent or other legal representative of the corporation.

Generally, a corporation files articles of incorporation with the government, laying out the general nature of the corporation, the amount of stock it is authorized to issue, and the names and addresses of directors. Once the articles are approved, the corporation's directors meet to create bylaws that govern the internal functions of the corporation, such as meeting procedures and officer positions.

The law of the jurisdiction in which a corporation operates will regulate most of its internal activities, as well as its finances. If a corporation operates outside its home state, it is often required to register with other governments as a foreign corporation, and is almost always subject to laws of its host state pertaining to employment, crimes, contracts, civil actions, and the like.

Corporations generally have a distinct name. Historically, some corporations were named after their membership: for instance, "The President and Fellows of Harvard College". Nowadays, corporations in most jurisdictions have a distinct name that does not need to make reference to their membership. In Canada, this possibility is taken to its logical extreme: many smaller Canadian corporations have no names at all, merely numbers based on a registration number (for example, "12345678 Ontario Limited"), which is assigned by the provincial or territorial government where the corporation incorporates.

In most countries, corporate names include a term or an abbreviation that denotes the corporate status of the entity (for example, "Incorporated" or "Inc." in the United States) or the limited liability of its members (for example, "Limited" or "Ltd."). These terms vary by jurisdiction and language. In some jurisdictions, they are mandatory, and in others they are not. Their use puts everybody on constructive notice that they are dealing with an entity whose liability is limited: one can only collect from whatever assets the entity still controls when one obtains a judgment against it.

Some jurisdictions do not allow the use of the word "company" alone to denote corporate status, since the word "company" may refer to a partnership or some other form of collective ownership (in the United States it can be used by a sole proprietorship but this is not generally the case elsewhere).

Despite not being individual human beings, corporations, as far as US law is concerned, are legal persons, and have many of the same rights and responsibilities as natural persons do. For example, a corporation can own property, and can sue or be sued. Corporations can exercise human rights against real individuals and the state, and they can themselves be responsible for human rights violations. Corporations can be "dissolved" either by statutory operation, order of court, or voluntary action on the part of shareholders. Insolvency may result in a form of corporate failure, when creditors force the liquidation and dissolution of the corporation under court order, but it most often results in a restructuring of corporate holdings. Corporations can even be convicted of criminal offenses, such as fraud and manslaughter. However, corporations are not considered living entities in the way that humans are.




</doc>
<doc id="7487" url="https://en.wikipedia.org/wiki?curid=7487" title="Fairchild Channel F">
Fairchild Channel F

The Fairchild Channel F is a home video game console released by Fairchild Semiconductor in November 1976 across North America at the retail price of $169.95. It was also released in Japan in October the following year. It has the distinction of being the first programmable ROM cartridge–based video game console, and the first console to use a microprocessor. It was launched as the Video Entertainment System, or VES, but when Atari released its VCS the next year, Fairchild renamed its machine. By 1977, the Fairchild Channel F had sold 250,000 units, trailing behind sales of the VCS.

The Channel F electronics were designed by Jerry Lawson using the Fairchild F8 CPU, the first public outing of this processor. The F8 was very complex compared to the typical integrated circuits of the day, and had more inputs and outputs than other contemporary chips. Because chip packaging was not available with enough pins, the F8 was instead fabricated as a pair of chips that had to be used together to form a complete CPU.

Lawson worked with Nick Talesfore and Ron Smith. As manager of Industrial Design, Talesfore was responsible for the design of the hand controllers, console, and video game cartridges. Smith was responsible for the mechanical engineering of the video cartridges and controllers. All worked for Wilf Corigan, head of Fairchild Semiconductor, a division of Fairchild Camera & Instrument.

One feature unique to this console is the 'hold' button, which allow the player to freeze the game, change the time or change the speed of the game. The functions printed on the console is how they work in the built-in games and also some of the original games, all buttons (except reset) are controlled by the programming and can be used for anything the programmer decides. The hold function is not universal (like the hardwired reset). In the original unit, sound is played through an internal speaker, rather than the TV set. However, the System II passed sound to the television through the RF modulator.

The controllers are a joystick without a base; the main body is a large hand grip with a triangular "cap" on top, the top being the portion that actually moved for eight-way directional control. It could be used as both a joystick and paddle (twist), and not only could it be pushed down to operate as a fire button it could be pulled up as well. The model 1 unit contained a small compartment for storing the controllers when moving it. The System II featured detachable controllers and had two holders at the back to wind the cable around and to store the controller in. Zircon later offered a special control which featured an action button on the front of the joystick. It was marketed by Zircon as "Channel F Jet-Stick" in a letter sent out to registered owners before Christmas 1982.

Despite the failure of the Channel F, the joystick's design was so popular—"Creative Computing" called it "outstanding"— that Zircon also released an Atari joystick port-compatible version, the Video Command Joystick, first released without the extra fire button. Before that, only the downwards plunge motion was connected and acted as the fire button; the pull-up and twist actions weren't connected to anything.

Twenty-seven cartridges, termed 'Videocarts', were officially released to consumers in the United States during the ownership of Fairchild and Zircon, the first twenty-one of which were released by Fairchild. Several of these cartridges were capable of playing more than one game and were typically priced at $19.95. The Videocarts were yellow and approximately the size and overall texture of an 8 track cartridge. They usually featured colorful label artwork. The earlier artwork was created by nationally known artist Tom Kamifuji and art directed by Nick Talesfore. The console contained two built-in games, Tennis and Hockey, which were both advanced "Pong" clones. In Hockey the reflecting bar could be changed to diagonals by twisting the controller, and could move all over the playing field. Tennis was much like the original Pong.

A sales brochure from 1978 listed 'Keyboard Videocarts' for sale. The three shown were "K-1 Casino Poker", "K-2 Space Odyssey", and "K-3 Pro-Football". These were intended to use the Keyboard accessory. All further brochures, released after Zircon took over Fairchild, never listed this accessory nor anything called a Keyboard Videocart.

There was one additional cartridge released numbered Videocart-51 and simply titled 'Demo 1'. This Videocart was shown in a single sales brochure released shortly after Zircon acquired the company. It was never listed for sale after this single brochure which was used in the winter of 1979.

Names according to the cartridge, not the packaging.
Carts listed (as mentioned above) but never released:

Official carts that also exist:

German SABA also released a few compatible carts different from the original carts, translation in Videocart 1 Tic-Tac-Toe to German words, Videocart 3 released with different abbreviations (German), Videocart 18 changed graphics and German word list and the SABA 20, a chess game released only by SABA.

A homebrew clone of "Pac-Man" for the Channel F was released in 2009.

Ken Uston reviewed 32 games in his book "Ken Uston's Guide to Buying and Beating the Home Video Games" in 1982, and rated some of the Channel F's titles highly; of these, "Alien Invasion" and "Video Whizball" were considered by Uston to be "the finest adult cartridges currently available for the Fairchild Channel F System." The games on a whole, however, rated last on his survey of over 200 games for the Atari, Intellivision, Astrocade and Odyssey consoles, and contemporary games were rated "Average" with future Channel F games rated "below average". Uston rated almost one half of the Channel F games as "high in interest" and called that "an impressive proportion" and further noted that "Some of the Channel F cartridges are timeless; no matter what technological developments occur, they will continue to be of interest." His overall conclusion was that the games "serve a limited, but useful, purpose" and that the "strength of the Channel F offering is in its excellent educational line for children."

In 1983, after Zircon announced its discontinuation of the Channel F, "Video Games" reviewed the console. Calling it "the system nobody knows", the magazine described its graphics and sounds as "somewhat primitive by today's standards". It described "Space War" as perhaps "the most antiquated game of its type still on the market", and rated the 25 games for the console with an average grade of three ("not too good") on a scale from one to ten. The magazine stated, however, that Fairchild "managed to create some fascinating games, even by today's standards", calling "Casino Royale" ("Video Blackjack") "the best card game, from blackjack to bridge, made for "any" TV-game system". It also favorably reviewed "Dodge-It" ("simple but great"), "Robot War" ("Berzerk without guns"), and "Whizball" ("thoroughly original ... hockey "with" guns"), but concluded that only those interested in nostalgia, video game collecting, or card games would purchase the Channel F in 1983.

Original Channel F technical specifications:

Some time in 1979, Zircon International bought the rights to the Channel F and released the re-designed console as the Channel F System II to compete with Atari's VCS. This re-designed System II was completed by Nick Talesfore at Fairchild. He was the same industrial designer who designed the original game console. Only six new games were released after the debut of the second system before its demise, several of which were developed at Fairchild before they sold it off.

The major changes were in design, with the controllers removable from the base unit instead of being wired directly into it, the controller storage was moved to the rear of the unit, and the sound was now mixed into the RF TV signal so the unit no longer needed a speaker. Electronics was also simplified with custom logic chips instead of standard logic resulting in a much smaller circuit board. This version also featured a simpler and more modern-looking case design. However, by this time the market was in the midst of the first video game crash, and Fairchild eventually threw in the towel and left the market. A number of licensed versions were released in Europe, including the Luxor Video Entertainment System in Scandinavia (Sweden), Adman Grandstand in the UK, and the Saba Videoplay, Nordmende Teleplay and ITT Tele-Match Processor, from Germany and also Dumont Videoplay and Barco Challenger from the Barco/Dumont company in Italy and Belgium.




</doc>
<doc id="7489" url="https://en.wikipedia.org/wiki?curid=7489" title="Collation">
Collation

Collation is the assembly of written information into a standard order. Many systems of collation are based on numerical order or alphabetical order, or extensions and combinations thereof. Collation is a fundamental element of most office filing systems, library catalogs, and reference books.

Collation differs from "classification" in that classification is concerned with arranging information into logical categories, while collation is concerned with the ordering of items of information, usually based on the form of their identifiers. Formally speaking, a collation method typically defines a total order on a set of possible identifiers, called sort keys, which consequently produces a total preorder on the set of items of information (items with the same identifier are not placed in any defined order).

A collation algorithm such as the Unicode collation algorithm defines an order through the process of comparing two given character strings and deciding which should come before the other. When an order has been defined in this way, a "sorting algorithm" can be used to put a list of any number of items into that order.

The main advantage of collation is that it makes it fast and easy for a user to find an element in the list, or to confirm that it is absent from the list. In automatic systems this can be done using a binary search algorithm or interpolation search; manual searching may be performed using a roughly similar procedure, though this will often be done unconsciously. Other advantages are that one can easily find the first or last elements on the list (most likely to be useful in the case of numerically sorted data), or elements in a given range (useful again in the case of numerical data, and also with alphabetically ordered data when one may be sure of only the first few letters of the sought item or items).

Strings representing numbers may be sorted based on the values of the numbers that they represent. For example, "−4", "2.5", "10", "89", "30,000". Note that pure application of this method may provide only a partial ordering on the strings, since different strings can represent the same number (as with "2" and "2.0" or, when scientific notation is used, "2e3" and "2000").

A similar approach may be taken with strings representing dates or other items that can be ordered chronologically or in some other natural fashion.

Alphabetical order is the basis for many systems of collation where items of information are identified by strings consisting principally of letters from an alphabet. The ordering of the strings relies on the existence of a standard ordering for the letters of the alphabet in question. (The system is not limited to alphabets in the strict technical sense; languages that use a syllabary or abugida, for example Cherokee, can use the same ordering principle provided there is a set ordering for the symbols used.)

To decide which of two strings comes first in alphabetical order, initially their first letters are compared. The string whose first letter appears earlier in the alphabet comes first in alphabetical order. If the first letters are the same, then the second letters are compared, and so on, until the order is decided. (If one string runs out of letters to compare, then it is deemed to come first; for example, "cart" comes before "carthorse".) The result of arranging a set of strings in alphabetical order is that words with the same first letter are grouped together, and within such a group words with the same first two letters are grouped together, and so on.

Capital letters are typically treated as equivalent to their corresponding lowercase letters. (For alternative treatments in computerized systems, see Automated collation, below.)

Certain limitations, complications, and special conventions may apply when alphabetical order is used:

In several languages the rules have changed over time, and so older dictionaries may use a different order than modern ones. Furthermore, collation may depend on use. For example, German dictionaries and telephone directories use different approaches.

Another form of collation is radical-and-stroke sorting, used for non-alphabetic writing systems such as the hanzi of Chinese and the kanji of Japanese, whose thousands of symbols defy ordering by convention. In this system, common components of characters are identified; these are called radicals in Chinese and logographic systems derived from Chinese. Characters are then grouped by their primary radical, then ordered by number of pen strokes within radicals. When there is no obvious radical or more than one radical, convention governs which is used for collation. For example, the Chinese character 妈 (meaning "mother") is sorted as a six-stroke character under the three-stroke primary radical 女.

The radical-and-stroke system is cumbersome compared to an alphabetical system in which there are a few characters, all unambiguous. The choice of which components of a logograph comprise separate radicals and which radical is primary is not clear-cut. As a result, logographic languages often supplement radical-and-stroke ordering with alphabetic sorting of a phonetic conversion of the logographs. For example, the kanji word "Tōkyō" (東京) can be sorted as if it were spelled out in the Japanese characters of the hiragana syllabary as "to-u-ki--u" (とうきょう), using the conventional sorting order for these characters.

In addition, in Greater China, surname stroke ordering is a convention in some official documents where people's names are listed without hierarchy.

The radical-and-stroke system, or some similar pattern-matching and stroke-counting method, was traditionally the only practical method for constructing dictionaries that someone could use to look up a logograph whose pronunciation was unknown. With the advent of computers, dictionary programs are now available that allow one to handwrite a character using a mouse or stylus.

When information is stored in digital systems, collation may become an automated process. It is then necessary to implement an appropriate collation algorithm that allows the information to be sorted in a satisfactory manner for the application in question. Often the aim will be to achieve an alphabetical or numerical ordering that follows the standard criteria as described in the preceding sections. However, not all of these criteria are easy to automate.

The simplest kind of automated collation is based on the numerical codes of the symbols in a character set, such as ASCII coding (or any of its supersets such as Unicode), with the symbols being ordered in increasing numerical order of their codes, and this ordering being extended to strings in accordance with the basic principles of alphabetical ordering (mathematically speaking, lexicographical ordering). So a computer program might treat the characters "a", "b", "C", "d", and "$" as being ordered "$", "C", "a", "b", "d" (the corresponding ASCII codes are "$" = 36, "a" = 97, "b" = 98, "C" = 67, and "d" = 100). Therefore, strings beginning with "C", "M", or "Z" would be sorted before strings with lower-case "a", "b", etc. This is sometimes called "ASCIIbetical order". This deviates from the standard alphabetical order, particularly due to the ordering of capital letters before all lower-case ones (and possibly the treatment of spaces and other non-letter characters). It is therefore often applied with certain alterations, the most obvious being case conversion (often to uppercase, for historical reasons) before comparison of ASCII values.

In many collation algorithms, the comparison is based not on the numerical codes of the characters, but with reference to the collating sequence – a sequence in which the characters are assumed to come for the purpose of collation – as well as other ordering rules appropriate to the given application. This can serve to apply the correct conventions used for alphabetical ordering in the language in question, dealing properly with differently cased letters, modified letters, digraphs, particular abbreviations, and so on, as mentioned above under Alphabetical order, and in detail in the Alphabetical order article. Such algorithms are potentially quite complex, possibly requiring several passes through the text.

Problems are nonetheless still common when the algorithm has to encompass more than one language. For example, in German dictionaries the word "ökonomisch" comes between "offenbar" and "olfaktorisch", while Turkish dictionaries treat "o" and "ö" as different letters, placing "oyun" before "öbür".

A standard algorithm for collating any collection of strings composed of any standard Unicode symbols is the Unicode Collation Algorithm. This can be adapted to use the appropriate collation sequence for a given language by tailoring its default collation table. Several such tailorings are collected in Common Locale Data Repository.

In some applications, the strings by which items are collated may differ from the identifiers that are displayed. For example, "The Shining" might be sorted as "Shining, The" (see Alphabetical order above), but it may still be desired to display it as "The Shining". In this case two sets of strings can be stored, one for display purposes, and another for collation purposes. Strings used for collation in this way are called "sort keys".

Sometimes, it is desired to order text with embedded numbers using proper numerical order. For example, "Figure 7b" goes before "Figure 11a", even though '7' comes after '1' in Unicode. This can be extended to Roman numerals. This behavior is not particularly difficult to produce as long as only integers are to be sorted, although it can slow down sorting significantly. For example, Microsoft Windows does this when sorting file names.

Sorting decimals properly is a bit more difficult, because different locales use different symbols for a decimal point, and sometimes the same character used as a decimal point is also used as a separator, for example "Section 3.2.5". There is no universal answer for how to sort such strings; any rules are application dependent.

Ascending order of numbers differs from alphabetical order, e.g. 11 comes alphabetically before 2. This can be fixed with leading zeros: 02 comes alphabetically before 11. See e.g. ISO 8601.

Also −13 comes alphabetically after −12 although it is less. With negative numbers, to make ascending order correspond with alphabetical sorting, more drastic measures are needed such as adding a constant to all numbers to make them all positive.

In some contexts, numbers and letters are used not so much as a basis for establishing an ordering, but as a means of labeling items that are already ordered. For example, pages, sections, chapters, and the like, as well as the items of lists, are frequently "numbered" in this way. Labeling series that may be used include ordinary Arabic numerals (1, 2, 3, ...), Roman numerals (I, II, III, ... or i, ii, iii, ...), or letters (A, B, C, ... or a, b, c, ...). (An alternative method for indicating list items, without numbering them, is to use a bulleted list.)

When letters of an alphabet are used for this purpose of enumeration, there are certain language-specific conventions as to which letters are used. For example, the Russian letters Ъ and Ь (which in writing are only used for modifying the preceding consonant), and usually also Ы, Й, and Ё, are usually omitted. Also in many languages that use extended Latin script, the modified letters are often not used in enumeration.




</doc>
<doc id="7490" url="https://en.wikipedia.org/wiki?curid=7490" title="Civil Rights Act">
Civil Rights Act

Civil Rights Act may refer to several acts of the United States Congress, including:




</doc>
<doc id="7491" url="https://en.wikipedia.org/wiki?curid=7491" title="Cola">
Cola

Cola is a sweetened, carbonated soft drink, made from ingredients that contain caffeine from the kola nut and non-cocaine derivatives from coca leaves, flavored with vanilla and other ingredients. With the primary exception of Coca-Cola, most colas now use other flavoring (and caffeinating) ingredients than kola nuts and coca leaves with a similar taste. Cola became popular worldwide after pharmacist John Pemberton invented Coca-Cola in 1886. His non-alcoholic recipe was inspired by the coca wine of pharmacist Angelo Mariani, created in 1863.

Most modern colas usually contain caramel color, caffeine, and sweeteners such as sugar or high-fructose corn syrup. They now come in numerous different brands. Among them, the most popular are Coca-Cola and Pepsi-Cola. These two cola companies have been competing since the 1890s, but their rivalry has intensified since the 1980s.

The primary modern flavoring ingredients in a cola drink are sugar, citrus oils (from oranges, limes, or lemon fruit peel), cinnamon, vanilla, and an acidic flavorant. Manufacturers of cola drinks add trace ingredients to create distinctively different tastes for each brand. Trace flavorings may include nutmeg and a wide variety of ingredients, but the base flavorings that most people identify with a cola taste remain vanilla and cinnamon. Acidity is often provided by phosphoric acid, sometimes accompanied by citric or other isolated acids. Coca-Cola's recipe is maintained as a corporate trade secret.

A variety of different sweeteners may be added to cola, often partly dependent on local agricultural policy. High-fructose corn syrup (HFCS) is predominantly used in the United States and Canada due to the lower cost of government-subsidized corn. In Europe, however, HFCS is subject to production quotas designed to encourage the production of sugar; sugar is thus typically used to sweeten sodas. In addition, stevia or an artificial sweetener may be used; "sugar-free" or "diet" colas typically contain artificial sweeteners only.

Cola can be manufactured with sugar as in Mexican Coca-Cola. Kosher for Passover Coca-Cola sold in the U.S. around the Jewish holiday also uses sucrose rather than HFCS and is also highly sought after by people who prefer the original taste. In addition, PepsiCo has recently been marketing versions of its Pepsi and Mountain Dew sodas that are sweetened with sugar instead of HFCS. These are marketed under the name "Throwback" and became permanent products.

In the 1940s, Coca-Cola produced White Coke at the request of Marshal of the Soviet Union Georgy Zhukov.

Clear colas were again produced during the Clear Craze of the early 1990s. Brands included Crystal Pepsi, Tab Clear, and 7 Up Ice Cola. Crystal Pepsi has been repeatedly reintroduced in the 2010s.

In Denmark, a popular clear cola was made by the Cooperative FDB in 1976. It was especially known for being the "Hippie Cola" because of the focus of the harmful effects the colour additive could have on children and the boycott of multinational brands. It was inspired by a campaign on harmful additives in Denmark by the Environmental-Organisation NOAH, an independent Danish division of Friends of the Earth. This was followed up with a variety of sodas without artificial colouring. Today many organic colas are available in Denmark, but, for nostalgic reasons, the Cola has still maintained its popularity to a certain degree.

A 2007 study found that consumption of colas, both those with natural sweetening and those with artificial sweetening, was associated with increased risk of chronic kidney disease. The phosphoric acid used in colas was thought to be a possible cause.

Studies indicate "soda and sweetened drinks are the main source of calories in [the] American diet", so most nutritionists advise that Coca-Cola and other soft drinks can be harmful if consumed excessively, particularly to young children whose soft drink consumption competes with, rather than complements, a balanced diet. Studies have shown that regular soft drink users have a lower intake of calcium, magnesium, ascorbic acid, riboflavin, and vitamin A.

The drink has also aroused criticism for its use of caffeine, which can cause physical dependence (caffeine addiction). A link has been shown between long-term regular cola intake and osteoporosis in older women (but not men). This was thought to be due to the presence of phosphoric acid, and the risk was found to be the same for caffeinated and noncaffeinated colas, as well as the same for diet and sugared colas.

Many soft drinks are sweetened mostly or entirely with high-fructose corn syrup, rather than sugar. Some nutritionists caution against consumption of corn syrup because it may aggravate obesity and type-2 diabetes more than cane sugar.









</doc>
<doc id="7492" url="https://en.wikipedia.org/wiki?curid=7492" title="Capability Maturity Model">
Capability Maturity Model

The Capability Maturity Model (CMM) is a development model created after a study of data collected from organizations that contracted with the U.S. Department of Defense, who funded the research. The term "maturity" relates to the degree of formality and optimization of processes, from "ad hoc" practices, to formally defined steps, to managed result metrics, to active optimization of the processes.

The model's aim is to improve existing software development processes, but it can also be applied to other processes.

The Capability Maturity Model was originally developed as a tool for objectively assessing the ability of government contractors' "processes" to implement a contracted software project. The model is based on the process maturity framework first described in "IEEE Software" and, later, in the 1989 book "Managing the Software Process" by Watts Humphrey. It was later published in a report in 1993 and as a book by the same authors in 1995.

Though the model comes from the field of software development, it is also used as a model to aid in business processes generally, and has also been used extensively worldwide in government offices, commerce, and industry.

In the 1960s, the use of computers grew more widespread, more flexible and less costly. Organizations began to adopt computerized information systems, and the demand for software development grew significantly. Many processes for software development were in their infancy, with few standard or "best practice" approaches defined.

As a result, the growth was accompanied by growing pains: project failure was common, the field of computer science was still in its early years, and the ambitions for project scale and complexity exceeded the market capability to deliver adequate products within a planned budget. Individuals such as Edward Yourdon, Larry Constantine, Gerald Weinberg, Tom DeMarco, and David Parnas began to publish articles and books with research results in an attempt to professionalize the software-development processes.

In the 1980s, several US military projects involving software subcontractors ran over-budget and were completed far later than planned, if at all. In an effort to determine why this was occurring, the United States Air Force funded a study at the SEI.

The first application of a staged maturity model to IT was not by CMU/SEI, but rather by Richard L. Nolan, who, in 1973 published the stages of growth model for IT organizations.

Watts Humphrey began developing his process maturity concepts during the later stages of his 27-year career at IBM.

Active development of the model by the US Department of Defense Software Engineering Institute (SEI) began in 1986 when Humphrey joined the Software Engineering Institute located at Carnegie Mellon University in Pittsburgh, Pennsylvania after retiring from IBM. At the request of the U.S. Air Force he began formalizing his Process Maturity Framework to aid the U.S. Department of Defense in evaluating the capability of software contractors as part of awarding contracts.

The result of the Air Force study was a model for the military to use as an objective evaluation of software subcontractors' process capability maturity. Humphrey based this framework on the earlier Quality Management Maturity Grid developed by Philip B. Crosby in his book "Quality is Free". Humphrey's approach differed because of his unique insight that organizations mature their processes in stages based on solving process problems in a specific order. Humphrey based his approach on the staged evolution of a system of software development practices within an organization, rather than measuring the maturity of each separate development process independently. The CMM has thus been used by different organizations as a general and powerful tool for understanding and then improving general business process performance.

Watts Humphrey's Capability Maturity Model (CMM) was published in 1988 and as a book in 1989, in "Managing the Software Process".

Organizations were originally assessed using a process maturity questionnaire and a Software Capability Evaluation method devised by Humphrey and his colleagues at the Software Engineering Institute.

The full representation of the Capability Maturity Model as a set of defined process areas and practices at each of the five maturity levels was initiated in 1991, with Version 1.1 being completed in January 1993. The CMM was published as a book in 1995 by its primary authors, Mark C. Paulk, Charles V. Weber, Bill Curtis, and Mary Beth Chrissis.
United States of America
New York, USA.

The CMM model's application in software development has sometimes been problematic. Applying multiple models that are not integrated within and across an organization could be costly in training, appraisals, and improvement activities. The Capability Maturity Model Integration (CMMI) project was formed to sort out the problem of using multiple models for software development processes, thus the CMMI model has superseded the CMM model, though the CMM model continues to be a general theoretical process capability model used in the public domain.

The CMM was originally intended as a tool to evaluate the ability of government contractors to perform a contracted software project. Though it comes from the area of software development, it can be, has been, and continues to be widely applied as a general model of the maturity of "process" (e.g., IT service management processes) in IS/IT (and other) organizations.

A maturity model can be viewed as a set of structured levels that describe how well the behaviors, practices and processes of an organization can reliably and sustainably produce required outcomes.

A maturity model can be used as a benchmark for comparison and as an aid to understanding - for example, for comparative assessment of different organizations where there is something in common that can be used as a basis for comparison. In the case of the CMM, for example, the basis for comparison would be the organizations' software development processes.

The model involves five aspects:

There are five levels defined along the continuum of the model and, according to the SEI: "Predictability, effectiveness, and control of an organization's software processes are believed to improve as the organization moves up these five levels. While not rigorous, the empirical evidence to date supports this belief".

Within each of these maturity levels are Key Process Areas which characterise that level, and for each such area there are five factors: goals, commitment, ability, measurement, and verification. These are not necessarily unique to CMM, representing — as they do — the stages that organizations must go through on the way to becoming mature.

The model provides a theoretical continuum along which process maturity can be developed incrementally from one level to the next. Skipping levels is not allowed/feasible.






At maturity level 5, processes are concerned with addressing statistical "common causes" of process variation and changing the process (for example, to shift the mean of the process performance) to improve process performance. This would be done at the same time as maintaining the likelihood of achieving the established quantitative process-improvement objectives. There are only a few companies in the world that have attained this level 5..

The model was originally intended to evaluate the ability of government contractors to perform a software project. It has been used for and may be suited to that purpose, but critics pointed out that process maturity according to the CMM was not necessarily mandatory for successful software development.

The software process framework documented is intended to guide those wishing to assess an organization's or project's consistency with the Key Process Areas. For each maturity level there are five checklist types:




</doc>
<doc id="7499" url="https://en.wikipedia.org/wiki?curid=7499" title="RDX">
RDX

RDX is the organic compound with the formula (ONNCH). It is a white solid without smell or taste, widely used as an explosive. Chemically, it is classified as a nitramide, chemically similar to HMX. A more energetic explosive than TNT, it was used widely in World War II.

It is often used in mixtures with other explosives and plasticizers or phlegmatizers (desensitizers). RDX is stable in storage and is considered one of the most energetic and brisant of the military high explosives.

RDX is also known, but less commonly, as cyclonite, hexogen (particularly in Russian, French, German and German-influenced languages), T4, and, chemically, as cyclotrimethylenetrinitramine. In the 1930s, the Royal Arsenal, Woolwich, started investigating cyclonite to use against German U-boats that were being built with thicker hulls. The goal was having an explosive more energetic than TNT. For security reasons, Britain termed cyclonite as "Research Department Explosive" (R.D.X.). The term "RDX" appeared in the United States in 1946. The first public reference in the United Kingdom to the name "RDX", or "R.D.X.", to use the official title, appeared in 1948; its authors were the managing chemist, ROF Bridgwater, the chemical research and development department, Woolwich, and the director of Royal Ordnance Factories, Explosives; again, it was referred to as simply RDX.

RDX was widely used during World War II, often in explosive mixtures with TNT such as Torpex, Composition B, Cyclotols, and H6. RDX was used in one of the first plastic explosives. The bouncing bomb depth charges used in the "Dambusters Raid" each contained of Torpex. RDX is believed to have been used in many bomb plots, including terrorist plots.

RDX is the base for a number of common military explosives:

Outside military applications, RDX is also used in controlled demolition to raze structures. The demolition of the Jamestown Bridge in the U.S. state of Rhode Island was one instance where RDX shaped charges were used to remove the span.

RDX is classified by chemists as a hexahydro-1,3,5-triazine derivative. It is obtained by treating hexamine with white fuming nitric acid.

This nitrolysis reaction also produces dinitromethane, ammonium nitrate, and water as byproducts. The overall reaction is:

RDX was used by both sides in World War II. The U.S. produced about 15,000 long tons per month during WWII and Germany about 7,000 long tons per month. RDX had the major advantages of possessing greater explosive force than TNT, used in World War I, and requiring no additional raw materials for its manufacture.

RDX was reported in 1898 by Georg Friedrich Henning, who obtained a German patent (patent No. 104280) for its manufacture by nitrolysis of hexamine (hexamethylenetetramine) with concentrated nitric acid. In this patent, the medical properties of RDX were mentioned; however, three further German patents obtained by Henning in 1916 proposed its use in smokeless propellants. The German military started investigating its use in 1920, referring to it as hexogen. Research and development findings were not published further until Edmund von Herz, described as an Austrian and later a German citizen, obtained a British patent in 1921 and a United States patent in 1922. Both patent claims were initiated in Austria; and described the manufacture of RDX by nitrating hexamethylenetetramine. The British patent claims included the manufacture of RDX by nitration, its use with or without other explosives, its use as a bursting charge and as an initiator. The U.S. patent claim was for the use of a hollow explosive device containing RDX and a detonator cap containing RDX. In the 1930s, Germany developed improved production methods.

During World War II, Germany used the code names W Salt, SH Salt, K-method, the E-method, and the KA-method. These names represented the identities of the developers of the various chemical routes to RDX. The W-method was developed by Wolfram in 1934 and gave RDX the code name "W-Salz". It used sulfamic acid, formaldehyde, and nitric acid. SH-Salz (SH salt) was from Schnurr, who developed a batch-process in 1937–38 based on nitrolysis of hexamine. The K-method, from Knöffler, involved addition of ammonium nitrate to the hexamine/nitric acid process. The E-method, developed by Ebele, proved to be identical to the Ross and Schiessler process described below. The KA-method, also developed by Knöffler, turned out to be identical to the Bachmann process described below.

The explosive shells fired by the MK 108 cannon and the warhead of the R4M rocket, both used in Luftwaffe fighter aircraft as offensive armament, both used hexogen as their explosive base.

In the United Kingdom (UK), RDX was manufactured from 1933 by the research department in a pilot plant at the Royal Arsenal in Woolwich, London, a larger pilot plant being built at the RGPF Waltham Abbey just outside London in 1939. In 1939 a twin-unit industrial-scale plant was designed to be installed at a new site, ROF Bridgwater, away from London and production of RDX started at Bridgwater on one unit in August 1941. The ROF Bridgwater plant brought in ammonia and methanol as raw materials: the methanol was converted to formaldehyde and some of the ammonia converted to nitric acid, which was concentrated for RDX production. The rest of the ammonia was reacted with formaldehyde to produce hexamine. The hexamine plant was supplied by Imperial Chemical Industries. It incorporated some features based on data obtained from the United States (U.S.). RDX was produced by continually adding hexamine and concentrated nitric acid to a cooled mixture of hexamine and nitric acid in the nitrator. The RDX was purified and processed for its intended use; recovery and reuse of some methanol and nitric acid also was carried out. The hexamine-nitration and RDX purification plants were duplicated (i.e. twin-unit) to provide some insurance against loss of production due to fire, explosion, or air attack.

The United Kingdom and British Empire were fighting without allies against Nazi Germany until the middle of 1941 and had to be self-sufficient. At that time (1941), the UK had the capacity to produce (160,000 lb) of RDX per week; both Canada, an allied country and self-governing dominion within the British Empire, and the U.S. were looked upon to supply ammunition and explosives, including RDX. By 1942 the Royal Air Force's annual requirement was forecast to be of RDX, much of which came from North America (Canada and the U.S.).

A different method of production to the Woolwich process was found and used in Canada, possibly at the McGill University department of chemistry. This was based on reacting paraformaldehyde and ammonium nitrate in acetic anhydride. A UK patent application was made by Robert Walter Schiessler (Pennsylvania State University) and James Hamilton Ross (McGill, Canada) in May 1942; the UK patent was issued in December 1947. Gilman states that the same method of production had been independently discovered by Ebele in Germany prior to Schiessler and Ross, but that this was not known by the Allies. Urbański provides details of five methods of production, and he refers to this method as the (German) E-method.

At the beginning of the 1940s, the major U.S. explosive manufacturers, E. I. du Pont de Nemours & Company and Hercules, had several decades of experience of manufacturing trinitrotoluene (TNT) and had no wish to experiment with new explosives. U.S. Army Ordnance held the same viewpoint and wanted to continue using TNT. RDX had been tested by Picatinny Arsenal in 1929, and it was regarded as too expensive and too sensitive. The Navy proposed to continue using ammonium picrate. In contrast, the National Defense Research Committee (NDRC), who had visited The Royal Arsenal, Woolwich, thought new explosives were necessary. James B. Conant, chairman of Division B, wished to involve academic research into this area. Conant therefore set up an experimental explosives research laboratory at the Bureau of Mines, Bruceton, Pennsylvania, using Office of Scientific Research and Development (OSRD) funding.

In 1941, the UK's Tizard Mission visited the U.S. Army and Navy departments and part of the information handed over included details of the "Woolwich" method of manufacture of RDX and its stabilisation by mixing it with beeswax. The UK was asking that the U.S. and Canada, combined, supply (440,000 lb) of RDX per day. A decision was taken by William H. P. Blandy, chief of the Bureau of Ordnance, to adopt RDX for use in mines and torpedoes. Given the immediate need for RDX, the U.S. Army Ordnance, at Blandy's request, built a plant that just copied the equipment and process used at Woolwich. The result was the Wabash River Ordnance Works run by E. I. du Pont de Nemours & Company. At that time, this works had the largest nitric acid plant in the world. The Woolwich process was expensive; it needed of strong nitric acid for every pound of RDX.

By early 1941, the NDRC was researching new processes. The Woolwich or direct nitration process has at least two serious disadvantages: (1) it used large amounts of nitric acid and (2) at least one-half of the formaldehyde is lost. One mole of hexamethylenetetramine could produce at most one mole of RDX. At least three laboratories with no previous explosive experience were tasked to develop better production methods for RDX; they were based at Cornell, Michigan, and Pennsylvania State universities. Werner Emmanuel Bachmann, from Michigan, successfully developed the "combination process" by combining the Canadian process with direct nitration. The combination process required large quantities of acetic anhydride instead of nitric acid in the old British "Woolwich process". Ideally, the combination process could produce two moles of RDX from each mole of hexamethylenetetramine.

The vast production of RDX could not continue to rely on the use of natural beeswax to desensitize the RDX. A substitute stabilizer based on petroleum was developed at the Bruceton Explosives Research Laboratory.

The NDRC tasked three companies to develop pilot plants. They were the Western Cartridge Company, E. I. du Pont de Nemours & Company, and Tennessee Eastman Company, part of Eastman Kodak. At the Eastman Chemical Company (TEC), a leading manufacturer of acetic anhydride, Werner Emmanuel Bachmann developed a continuous-flow process for RDX. RDX was crucial to the war effort and the current batch-production process was too slow. In February 1942, TEC began producing small amounts of RDX at its Wexler Bend pilot plant, which led to the U.S. government authorizing TEC to design and build Holston Ordnance Works (H.O.W.) in June 1942. By April 1943, RDX was being manufactured there. At the end of 1944, the Holston plant and the Wabash River Ordnance Works, which used the Woolwich process, were producing (50 million pounds) of Composition B per month.

The U.S. Bachmann process for RDX was found to be richer in HMX than the United Kingdom's RDX. This later led to a RDX plant using the Bachmann process being set up at ROF Bridgwater in 1955 to produce both RDX and HMX.

The United Kingdom's intention in World War II was to use "desensitised" RDX. In the original Woolwich process, RDX was phlegmatized with beeswax, but later paraffin wax was used, based on the work carried out at Bruceton. In the event the UK was unable to obtain sufficient RDX to meet its needs, some of the shortfall was met by substituting amatol, a mixture of ammonium nitrate and TNT.

Karl Dönitz was reputed to have claimed that "an aircraft can no more kill a U-boat than a crow can kill a mole". Nonetheless, by May 1942 Wellington bombers began to deploy depth charges containing Torpex, a mixture of RDX, TNT, and aluminium, which had up to 50 percent more destructive power than TNT-filled depth charges. Considerable quantities of the RDX–TNT mixture were produced at the Holston Ordnance Works, with Tennessee Eastman developing an automated mixing and cooling process based around the use of stainless steel conveyor belts.

A semtex bomb was used in the Pan Am Flight 103 (known also as the Lockerbie) bombing in 1988. The 1993 Bombay bombings used RDX placed into several vehicles as bombs. RDX was the main component used for the 2006 Mumbai train bombings and the Jaipur bombings in 2008. It also is believed to be the explosive used in the 1999 Russian apartment bombings, 2004 Russian aircraft bombings, and 2010 Moscow Metro bombings.

Ahmed Ressam, the al-Qaeda Millennium Bomber, used a small quantity of RDX as one of the components in the explosives that he prepared to bomb Los Angeles International Airport on New Year's Eve 1999-2000; the combined explosives could have produced a blast forty times greater than that of a devastating car bomb.

In July 2012, the Kenyan government arrested two Iranian nationals and charged them with illegal possession of of RDX. According to the Kenyan Police, the Iranians planned to use the RDX for "attacks on Israeli, US, UK and Saudi Arabian targets".

RDX was used in the assassination of Lebanese Prime Minister Rafic Hariri on February 14, 2005.

RDX has a high nitrogen content and a high O:C ratio, both of which indicate its explosive potential for formation of N and CO.

RDX undergoes a deflagration to detonation transition (DDT) in confinement and certain circumstances.

The velocity of detonation of RDX at a density of 1.76 g/cm is 8750 m/s.

It starts to decompose at approximately 170 °C and melts at 204 °C. At room temperature, it is very stable. It burns rather than explodes. It detonates only with a detonator, being unaffected even by small arms fire. This property makes it a useful military explosive. It is less sensitive than pentaerythritol tetranitrate (PETN). Under normal conditions, RDX has a Figure of Insensitivity of exactly 80 (RDX defines the reference point).

RDX sublimates in vacuum, which limits some applications.

RDX, when exploded in air, has about 1.5 times the explosive energy of TNT per unit weight and about 2.0 times per unit volume.

RDX is insoluble in water, with solubility 0.05975 g/L at temperature of 25°C.

The substance's toxicity has been studied for many years. RDX has caused convulsions (seizures) in military field personnel ingesting it, and in munition workers inhaling its dust during manufacture. At least one fatality was attributed to RDX toxicity in a European munitions manufacturing plant.

During the Vietnam War, at least 40 American soldiers were hospitalized with composition C-4 (which is 91% RDX) intoxication from December 1968 to December 1969. C-4 was frequently used by soldiers as a fuel to heat food, and the food was generally mixed by the same knife that was used to cut C-4 into small pieces prior to burning. Soldiers were exposed to C-4 either due to inhaling the fumes, or due to ingestion, made possible by many small particles adhering to the knife having been deposited into the cooked food. The symptom complex involved nausea, vomiting, generalized seizures, and prolonged postictal confusion and amnesia; which indicated toxic encephalopathy.

Oral toxicity of RDX depends on its physical form; in rats, the LD50 was found to be 100 mg/kg for finely powdered RDX, and 300 mg/kg for coarse, granular RDX. A case has been reported of a human child hospitalized in status epilepticus following the ingestion of 84.82 mg/kg dose of RDX (or 1.23 g for the patient's body weight of 14.5 kg) in the "plastic explosive" form.

The substance has low to moderate toxicity with a possible human carcinogen classification. Further research is ongoing, however, and this classification may be revised by the United States Environmental Protection Agency (EPA). Remediating RDX-contaminated water supplies has proven to be successful. It is known to be a kidney toxin in humans and highly toxic to earthworms and plants, and thus army testing ranges where RDX was used heavily may need to undergo environmental remediation. Concerns have been raised by research published in late 2017 indicating that the issue has been not been addressed correctly by U.S. officials.

RDX has limited civilian use as a rat poison.

RDX is degraded by the organisms in sewage sludge as well as the fungus "Phanaerocheate chrysosporium". Both wild and transgenic plants can phytoremediate explosives from soil and water.

FOX-7 is considered to be approximately a 1-to-1 replacement for RDX in almost all applications.



</doc>
<doc id="7500" url="https://en.wikipedia.org/wiki?curid=7500" title="Celebes (disambiguation)">
Celebes (disambiguation)

Celebes may refer to:



</doc>
<doc id="7502" url="https://en.wikipedia.org/wiki?curid=7502" title="Christianity and Judaism">
Christianity and Judaism

Christianity is rooted in Second Temple Judaism, but the two religions diverged in the first centuries of the Christian Era. Christianity emphasizes correct belief (or orthodoxy), focusing on the New Covenant as mediated through Jesus Christ, as recorded in the New Testament. Judaism places emphasis on correct conduct (or orthopraxy), focusing on the Mosaic covenant, as recorded in the Torah and Talmud.

Christians believe in individual salvation from sin through receiving Jesus Christ as their God and savior through faith. Jews believe in individual and collective participation in an eternal dialogue with God through tradition, rituals, prayers and ethical actions. Christianity generally believes in a Triune God, one person of whom became human. Judaism emphasizes the Oneness of God and rejects the Christian concept of God in human form.

Judaism's purpose is to carry out what it holds to be the only covenant between God and the Jewish people. The Torah (lit. "teaching"), both written and oral, tell the story of this covenant, and provides Jews with the terms of the covenant. The Oral Torah is the primary guide for Jews to abide by these terms, as expressed in tractate Gittin 60b, "the Holy One, Blessed be He, did not make His covenant with Israel except by virtue of the Oral Law" to help them learn how to live a holy life, and to bring holiness, peace and love into the world and into every part of life, so that life may be elevated to a high level of kedushah, originally through study and practice of the Torah, and since the destruction of the Second Temple, through prayer as expressed in tractate Sotah 49a "Since the destruction of the Temple, every day is more cursed than the preceding one; and the existence of the world is assured only by the kedusha...and the words spoken after the study of Torah."

Since the adoption of the Amidah, the acknowledgement of God through the declaration from Isaiah 6:3 "Kadosh [holy], kadosh, kadosh, is HaShem, Master of Legions; the whole world is filled with His glory". as a replacement for the study of Torah, which is a daily obligation for a Jew, and sanctifies God in itself. This continuous maintenance of relationship between the individual Jew and God through either study, or prayer repeated three times daily, is the confirmation of the original covenant. This allows the Jewish people as a community to strive and fulfill the prophecy "I, the Lord, have called you in righteousness, and will hold your hand and keep you. And I will establish you as a covenant of the people, for a light unto the nations." () (i.e., a role model) over the course of history, and a part of the divine intent of bringing about an age of peace and sanctity where ideally a faithful life and good deeds should be ends in themselves, not means. See also Jewish principles of faith.

According to Christian theologian Alister McGrath, the Jewish Christians affirmed every aspect of then contemporary Second Temple Judaism with the addition of the belief that Jesus was the messiah, with Isaiah 49:6, "an explicit parallel to 42:6" quoted by Paul the Apostle in Acts 13:47 and reinterpreted by Justin Martyr. According to Christian writers, most notably Paul, the Bible teaches that people are, in their current state, sinful, and the New Testament reveals that Jesus is both the Son of man and the Son of God, united in the hypostatic union, God the Son, God made incarnate; that Jesus' death by crucifixion was a sacrifice to atone for all of humanity's sins, and that acceptance of Jesus as Savior and Lord saves one from Divine Judgment, giving Eternal life. Jesus is the mediator of the New Covenant. His famous Sermon on the Mount is considered by some Christian scholars to be the proclamation of the New Covenant ethics, in contrast to the Mosaic Covenant of Moses from Mount Sinai.

The subject of the Tanakh, or Hebrew Bible, is the history of the Children of Israel, especially in terms of their relationship with God. Thus, Judaism has also been characterized as a culture or as a civilization. In his work "Judaism as a Civilization", the founder of the Reconstructionist Judaism, Mordecai Kaplan defines Judaism as an evolving religious civilization. One crucial sign of this is that one need not believe, or even do, anything to be Jewish; the historic definition of 'Jewishness' requires only that one be born of a Jewish mother, or that one convert to Judaism in accord with Jewish law. (Today, Reform and Reconstructionist Jews also include those born of Jewish fathers and gentile mothers if the children are raised as Jews.)

To many religious Jews, Jewish ethnicity is closely tied to their relationship with God, and thus has a strong theological component. This relationship is encapsulated in the notion that Jews are a chosen people. For strictly observant Jews, being "chosen" fundamentally means that it was God's wish that a group of people would exist in a covenant, and would be bound to obey a certain set of laws as a duty of their covenant, and that the Children of Israel "chose" to enter into this covenant with God. They view their divine purpose as being ideally a "light upon the nations" and a "holy people" (i.e., a people who live their lives fully in accordance with Divine will as an example to others), not "the one path to God". For Jews, salvation comes from God, freely given, and observance of the Law is one way of responding to God's grace.

Jews hold that other nations and peoples are not required (nor expected) to obey the Law of Moses. The only laws Judaism believes are automatically binding (in order to be assured of a place in the world to come) on non-Jews are the Seven Laws of Noah. Thus, as an ethnic religion, Judaism holds that others may have their own, different, paths to God (or holiness, or "salvation"), as long as they are consistent with the Seven Laws of Noah.

While ethnicity and culture play a large part in Jewish identity, they are not the only way Jews define themselves as Jews. There are secular Jews, who do use ethnicity and culture as their defining criteria and there are religious Jews, who do not. Rather, religious Jews define their Jewishness within the context of their Judaism. In this context, a religious convert could "feel" more Jewish than a secular ethnic Jew. While Kaplan defines Judaism as a civilization, there are many who would not agree, citing millennia of religious tradition and observance as more than simple civilization. Most observant Jews would say that Judaism is a love story.

Judaism and Christianity share the belief that there is One, True God, who is the only one worthy to be worshipped. Judaism sees this One, True God as a singular, ineffable, undefinable being. Phrases such as "Ground of All Being", "Unfolding Reality" and "Creator and Sustainer of Life" capture only portions of who God is to Jews. While God does not change, our perception of God does, and so, Jews are open to new experiences of God's presence. Christianity, with a few exceptions, sees the One, True God as having triune personhood: God the Father, God the Son (Jesus) and God the Holy Spirit. God is the same yesterday, today and tomorrow, so Christians generally look to the Scriptures (both Hebrew and Christian) for an understanding of who God is.

Christianity is characterized by its claim to universality, which marks a significant break from current Jewish identity and thought, but has its roots in Hellenistic Judaism. Christians believe that Jesus represents the fulfillment of God's promise to Abraham and the nation of Israel, that Israel would be a blessing to all nations. Most Christians believe that the Law was "fulfilled" by Jesus and has become unnecessary to "faith life". Although Christians generally believe their religion to be very inclusive (since not only Jews but all gentiles can be Christian), Jews see Christianity as highly exclusive, because some denominations view non-Christians (such as Jews and Pagans) as having an incomplete or imperfect relationship with God, and therefore excluded from grace, salvation, heaven, or eternal life. For some Christians, it is the stated or "confessed" belief in Jesus as Savior that makes God's grace available to an individual, and salvation can come no other way (Solus Christus in Protestantism, Extra Ecclesiam nulla salus in Catholicism, see dual covenant theology for a traditional view). In Catholicism, Orthodoxy, Anglicanism and 'mainline' Protestantism (Lutherans, Methodists et cetera), sanctifying grace is ordinarily received via the Sacraments. However, God can also work outside the Sacraments. Also see "Invincible Ignorance" as understood in Catholic theology.

This crucial difference between the two religions has other implications. For example, while in a conversion to Judaism a convert must accept basic Jewish principles of faith, and renounce all other religions, the process is more like a form of adoption, or changing national citizenship (i.e. becoming a formal member of the people, or tribe), with the convert becoming a "child of Abraham and Sarah". For many reasons, some historical and some religious, Judaism does not encourage its members to convert others and in fact would require the initiative from the person who would like to convert. In contrast, most Christian denominations actively seek converts, following the Great Commission, and conversion to Christianity is generally a declaration of faith (although some denominations view it specifically as adoption into a community of Christ, and orthodox Christian tradition views it as being a literal joining together of the members of Christ's body).

Both Christianity and Judaism have been affected by the diverse cultures of their respective members. For example, what Jews from Eastern Europe and from North Africa consider "Jewish food" has more in common with the cuisines of non-Jewish Eastern Europeans and North Africans than with each other, although for religious Jews all food-preparation must conform to the same laws of Kashrut. According to non-Orthodox Jews and critical historians, Jewish law too has been affected by surrounding cultures (for example, some scholars argue that the establishment of absolute monotheism in Judaism was a reaction against the dualism of Zoroastrianism that Jews encountered when living under Persian rule; Jews rejected polygamy during the Middle Ages, influenced by their Christian neighbors). According to Orthodox Jews too there are variations in Jewish custom from one part of the world to another. It was for this reason that Joseph Karo's Shulchan Aruch did not become established as the authoritative code of Jewish law until after Moshe Isserlis added his commentary, which documented variations in local custom.

The Hebrew Bible is composed of three parts; the Torah (Instruction, the Septuagint translated the Hebrew to "nomos" or "Law"), the Nevi'im (Prophets) and the Ketuvim (Writings). Collectively, these are known as the Tanakh. According to Rabbinic Judaism the Torah was revealed by God to Moses; within it, Jews find 613 Mitzvot (commandments).

Rabbinic tradition asserts that God revealed two Torahs to Moses, one that was written down, and one that was transmitted orally. Whereas the written Torah has a fixed form, the Oral Torah is a living tradition that includes not only specific supplements to the written Torah (for instance, what is the proper manner of "shechita" and what is meant by "Frontlets" in the Shema), but also procedures for understanding and talking about the written Torah (thus, the Oral Torah revealed at Sinai includes debates among rabbis who lived long after Moses). The Oral Law elaborations of narratives in the Bible and stories about the rabbis are referred to as "aggadah". It also includes elaboration of the 613 commandments in the form of laws referred to as "halakha". Elements of the Oral Torah were committed to writing and edited by Judah HaNasi in the Mishnah in 200 CE; much more of the Oral Torah were committed to writing in the Babylonian and Jerusalem Talmuds, which were edited around 600 CE and 450 CE, respectively. The Talmuds are notable for the way they combine law and lore, for their explication of the midrashic method of interpreting tests, and for their accounts of debates among rabbis, which preserve divergent and conflicting interpretations of the Bible and legal rulings.

Since the transcription of the Talmud, notable rabbis have compiled law codes that are generally held in high regard: the Mishneh Torah, the Tur, and the Shulchan Aruch. The latter, which was based on earlier codes and supplemented by the commentary by Moshe Isserles that notes other practices and customs practiced by Jews in different communities, especially among Ashkenazim, is generally held to be authoritative by Orthodox Jews. The Zohar, which was written in the 13th century, is generally held as the most important esoteric treatise of the Jews.

All contemporary Jewish movements consider the Tanakh, and the Oral Torah in the form of the Mishnah and Talmuds as sacred, although movements are divided as to claims concerning their divine revelation, and also their authority. For Jews, the Torah—written and oral—is the primary guide to the relationship between God and man, a living document that has unfolded and will continue to unfold whole new insights over the generations and millennia. A saying that captures this goes, "Turn it [the Torah's words] over and over again, for everything is in it."

Christians accept the Written Torah and other books of the Hebrew Bible (alternatively called Old Testament) as Scripture, although they generally give readings from the Koine Greek Septuagint translation instead of the Biblical Hebrew/Biblical Aramaic Masoretic Text. Two notable examples are:

Instead of the traditional Jewish order and names for the books, Christians organize and name the books closer to that found in the Septuagint. Some Christian denominations (such as Anglican, Roman Catholic, and Eastern Orthodox), include a number of books that are not in the Hebrew Bible (the biblical apocrypha or deuterocanonical books or Anagignoskomena, see Development of the Old Testament canon) in their biblical canon that are not in today's Jewish canon, although they were included in the Septuagint. Christians reject the Jewish Oral Torah, which was still in oral, and therefore unwritten, form in the time of Jesus.

Christians believe that God has established a New Covenant with people through Jesus, as recorded in the Gospels, Acts of the Apostles, Epistles, and other books collectively called the New Testament (the word "testament" attributed to Tertullian is commonly interchanged with the word "covenant"). For some Christians, such as Roman Catholics and Orthodox Christians, this New Covenant includes authoritative sacred traditions and canon law. Others, especially Protestants, reject the authority of such traditions and instead hold to the principle of "sola scriptura", which accepts only the Bible itself as the final rule of faith and practice. Anglicans do not believe in "sola scriptura". For them scripture is the longest leg of a 3-legged stool: scripture, tradition and reason. Scripture cannot stand on its since it must be interpreted in the light of the Church's patristic teaching and ecumenical creeds. Additionally, some denominations include the "oral teachings of Jesus to the Apostles", which they believe have been handed down to this day by apostolic succession.

Christians refer to the biblical books about Jesus as the New Testament, and to the canon of Hebrew books as the Old Testament. Judaism does not accept the retronymic labeling of its sacred texts as the "Old Testament", and some Jews refer to the New Testament as the Christian Testament or Christian Bible. Judaism rejects all claims that the Christian New Covenant supersedes, abrogates, fulfills, or is the unfolding or consummation of the covenant expressed in the Written and Oral Torahs. Therefore, just as Christianity does not accept that Mosaic law has any authority over Christians, Judaism does not accept that the New Testament has any religious authority over Jews.

Many Jews view Christians as having quite an ambivalent view of the Torah, or Mosaic law: on one hand Christians speak of it as God's absolute word, but on the other, they apply its commandments with a certain selectivity. Some Jews contend that Christians cite commandments from the Old Testament to support one point of view but then ignore other commandments of a similar class and of equal weight. Examples of this are certain commandments that God states explicitly be a "lasting covenant" (NIV ). Some translate the Hebrew as a "perpetual covenant" (). Likewise, some Christians contend that Jews cite some commandments from the Torah to support one view, but then ignore other commandments of a similar class and of equal weight.

Christians explain that such selectivity is based on rulings made by early Jewish Christians in the Book of Acts, at the Council of Jerusalem, that, while believing gentiles did not need to fully convert to Judaism, they should follow some aspects of Torah like avoiding idolatry and fornication and blood, including, according to some interpretations, homosexuality. This view is also reflected by modern Judaism, in that Righteous gentiles needn't convert to Judaism and need to observe only the Noahide Laws, which also contain prohibitions against idolatry and fornication and blood.

Some Christians agree that Jews who accept Jesus should still observe all of Torah, see for example Dual-covenant theology, based on warnings by Jesus to Jews not to use him as an excuse to disregard it, and they support efforts of those such as Messianic Jews (Messianic Judaism is considered by most Christians and Jews to be a form of Christianity) to do that, but some Protestant forms of Christianity oppose all observance to the Mosaic law, even by Jews, which Luther criticised as Antinomianism.

A minority view in Christianity, known as Christian Torah-submission, holds that the Mosaic law as it is written is binding on all followers of God under the New Covenant, even for gentiles, because it views God’s commands as "everlasting" (, ; , ; ) and "good" (; ; ).

Traditionally, both Judaism and Christianity believe in the God of Abraham, Isaac and Jacob, for Jews the God of the Tanakh, for Christians the God of the Old Testament, the creator of the universe. Judaism and major sects of Christianity reject the view that God is entirely immanent (although some see this as the concept of the Holy Ghost) and within the world as a physical presence, (although trinitarian Christians believe in the incarnation of God). Both religions reject the view that God is entirely transcendent, and thus separate from the world, as the pre-Christian Greek Unknown God. Both religions reject atheism on one hand and polytheism on the other.

Both religions agree that God shares both transcendent and immanent qualities. How these religions resolve this issue is where the religions differ. Christianity posits that God exists as a Trinity; in this view God exists as three distinct persons who share a single divine essence, or substance. In those three there is one, and in that one there are three; the one God is indivisible, while the three persons are distinct and unconfused, God the Father, God the Son, and God the Holy Spirit. It teaches that God became especially immanent in physical form through the Incarnation of God the Son who was born as Jesus of Nazareth, who is believed to be at once fully God and fully human. There are denominations self-describing as Christian who question one or more of these doctrines, however, see Nontrinitarianism. By contrast, Judaism sees God as a single entity, and views trinitarianism as both incomprehensible and a violation of the Bible's teaching that God is one. It rejects the notion that Jesus or any other object or living being could be 'God', that God could have a literal 'son' in physical form or is divisible in any way, or that God could be made to be joined to the material world in such fashion. Although Judaism provides Jews with a word to label God's transcendence ("Ein Sof", without end) and immanence ("Shekhinah", in-dwelling), these are merely human words to describe two ways of experiencing God; God is one and indivisible.

A minority Jewish view, which appears in some codes of Jewish law, is that while Christian worship is polytheistic (due to the multiplicity of the Trinity), it is permissible for them to swear in God's name, since they are referring to the one God. This theology is referred to in Hebrew as Shituf (literally "partnership" or "association"). Although worship of a trinity is considered to be not different from any other form of idolatry for Jews, it may be an acceptable belief for non-Jews (according to the ruling of some Rabbinic authorities).

Judaism teaches that the purpose of the Torah is to teach us how to act correctly. God's existence is a given in Judaism, and not something that most authorities see as a matter of required belief. Although some authorities see the Torah as commanding Jews to believe in God, Jews see belief in God as a necessary, but not sufficient, condition for a Jewish life. The quintessential verbal expression of Judaism is the Shema Yisrael, the statement that the God of the Bible is their God, and that this God is unique and one. The quintessential physical expression of Judaism is behaving in accordance with the 613 Mitzvot (the commandments specified in the Torah), and thus live one's life in God's ways.

Thus fundamentally in Judaism, one is enjoined to bring holiness into life (with the guidance of God's laws), rather than removing oneself from life to be holy.

Much of Christianity also teaches that God wants people to perform good works, but all branches hold that good works alone will not lead to salvation, which is called Legalism, the exception being dual-covenant theology. Some Christian denominations hold that salvation depends upon transformational faith in Jesus, which expresses itself in good works as a testament (or witness) to ones faith for others to see (primarily Eastern Orthodox Christianity and Roman Catholicism), while others (including most Protestants) hold that faith alone is necessary for salvation. Some argue that the difference is not as great as it seems, because it really hinges on the definition of "faith" used. The first group generally uses the term "faith" to mean "intellectual and heartfelt assent and submission". Such a faith will not be salvific until a person has allowed it to effect a life transforming conversion (turning towards God) in their being (see Ontotheology). The Christians that hold to "salvation by faith alone" (also called by its Latin name "sola fide") define faith as being implicitly ontological—mere intellectual assent is not termed "faith" by these groups. Faith, then, is life-transforming by definition.

In both religions, offenses against the will of God are called sin. These sins can be thoughts, words, or deeds.

Catholicism categorizes sins into various groups. A wounding of the relationship with God is often called venial sin; a complete rupture of the relationship with God is often called mortal sin. Without salvation from sin (see below), a person's separation from God is permanent, causing such a person to enter Hell in the afterlife. Both the Catholic Church and the Orthodox Church define sin more or less as a "macula", a spiritual stain or uncleanliness that constitutes damage to man's image and likeness of God.

Hebrew has several words for sin, each with its own specific meaning. The word "pesha", or "trespass", means a sin done out of rebelliousness. The word "aveira" means "transgression". And the word "avone", or "iniquity", means a sin done out of moral failing. The word most commonly translated simply as "sin", "het", literally means "to go astray". Just as Jewish law, "halakha" provides the proper "way" (or path) to live, sin involves straying from that path. Judaism teaches that humans are born with free will, and morally neutral, with both a "yetzer hatov", (literally, "the good inclination", in some views, a tendency towards goodness, in others, a tendency towards having a productive life and a tendency to be concerned with others) and a "yetzer hara", (literally "the evil inclination", in some views, a tendency towards evil, and in others, a tendency towards base or animal behavior and a tendency to be selfish). In Judaism all human beings are believed to have free will and can choose the path in life that they will take. It does not teach that choosing good is impossible—only at times more difficult. There is almost always a "way back" if a person wills it. (Although texts mention certain categories for whom the way back will be exceedingly hard, such as the slanderer, the habitual gossip, and the malicious person)

The rabbis recognize a positive value to the "yetzer hara": one tradition identifies it with the observation on the last day of creation that God's accomplishment was "very good" (God's work on the preceding days was just described as "good") and explain that without the yetzer ha'ra there would be no marriage, children, commerce or other fruits of human labor; the implication is that yetzer ha'tov and yetzer ha'ra are best understood not as moral categories of good and evil but as selfless versus selfish orientations, either of which used rightly can serve God's will.

In contrast to the Jewish view of being morally balanced, Original Sin refers to the idea that the sin of Adam and Eve's disobedience (sin "at the origin") has passed on a spiritual heritage, so to speak. Christians teach that human beings inherit a corrupted or damaged human nature in which the tendency to do bad is greater than it would have been otherwise, so much so that human nature would not be capable now of participating in the afterlife with God. This is not a matter of being "guilty" of anything; each person is only personally guilty of their own actual sins. However, this understanding of original sin is what lies behind the Christian emphasis on the need for spiritual salvation from a spiritual Saviour, who can forgive and set aside sin even though humans are not inherently pure and worthy of such salvation. Paul the Apostle in Romans and I Corinthians placed special emphasis on this doctrine, and stressed that belief in Jesus would allow Christians to overcome death and attain salvation in the hereafter.

Roman Catholics, Eastern Orthodox Christians, and some Protestants teach the Sacrament of Baptism is the means by which each person's damaged human nature is healed and Sanctifying Grace (capacity to enjoy and participate in the spiritual life of God) is restored. This is referred to as "being born of water and the Spirit", following the terminology in the Gospel of St. John. Most Protestants believe this salvific grace comes about at the moment of personal decision to follow Jesus, and that baptism is a symbol of the grace already received.

As in English, the Hebrew word for "love", ahavah אהבה, is used to describe intimate or romantic feelings or relationships, such as the love between parent and child in Genesis 22:2; 25: 28; 37:3; the love between close friends in I Samuel 18:2, 20:17; or the love between a young man and young woman in Song of Songs. Christians will often use the Septuagint to make distinctions between the types of love: "philia" for brotherly, "eros" for romantic and "agape" for self-sacrificing love.

Like many Jewish scholars and theologians, literary critic Harold Bloom understands Judaism as fundamentally a religion of love. But he argues that one can understand the Hebrew conception of love only by looking at one of the core commandments of Judaism, Leviticus 19:18, "Love your neighbor as yourself", also called the second Great Commandment. Talmudic sages Hillel and Rabbi Akiva commented that this is a major element of the Jewish religion. Also, this commandment is arguably at the center of the Jewish faith. As the third book of the Torah, Leviticus is literally the central book. Historically, Jews have considered it of central importance: traditionally, children began their study of the Torah with Leviticus, and the midrashic literature on Leviticus is among the longest and most detailed of midrashic literature (see Bamberger 1981: 737). Bernard Bamberger considers Leviticus 19, beginning with God's commandment in verse 3—"You shall be holy, for I the Lord your God, am holy"—to be "the climactic chapter of the book, the one most often read and quoted" (1981:889). Leviticus 19:18 is itself the climax of this chapter.

The only statements in the Tanakh about the status of a fetus state that killing an unborn infant does not have the same status as killing a born human being, and mandates a much lesser penalty (Exodus 21: 22–25) (although this interpretation is disputed, the passage could refer to an injury to a woman that causes a premature, live birth).

The Talmud states that the fetus is not yet a full human being until it has been born (either the head or the body is mostly outside of the woman), therefore killing a fetus is not murder, and abortion—in restricted circumstances—has always been legal under Jewish law. Rashi, the great 12th century commentator on the Bible and Talmud, states clearly of the fetus "lav nefesh hu": "it is not a person." The Talmud contains the expression "ubar yerech imo"—the fetus is as the thigh of its mother,' i.e., the fetus is deemed to be part and parcel of the pregnant woman's body." The Babylonian Talmud Yevamot 69b states that: "the embryo is considered to be mere water until the fortieth day." Afterwards, it is considered subhuman until it is born. Christians who agree with these views may refer to this idea as abortion before the quickening of the fetus.

Judaism unilaterally supports, in fact mandates, abortion if doctors believe that it is necessary to save the life of the woman. Many rabbinic authorities allow abortions on the grounds of gross genetic imperfections of the fetus. They also allow abortion if the woman were suicidal because of such defects. However, Judaism holds that abortion is impermissible for family planning or convenience reasons. Each case must be decided individually, however, and the decision should lie with the pregnant woman, the man who impregnated her and their Rabbi.

Jews and Christians accept as valid and binding many of the same moral principles taught in the Torah. There is a great deal of overlap between the ethical systems of these two faiths. Nonetheless, there are some highly significant doctrinal differences.

Judaism has many teachings about peace and compromise, and its teachings make physical violence the last possible option. Nonetheless, the Talmud teaches that "If someone comes with the intention to murder you, then one is obligated to kill in self-defense [rather than be killed]". The clear implication is that to bare one's throat would be tantamount to suicide (which Jewish law forbids) and it would also be considered helping a murderer kill someone and thus would "place an obstacle in front of a blind man" (i.e., makes it easier for another person to falter in their ways). The tension between the laws dealing with peace, and the obligation to self-defense, has led to a set of Jewish teachings that have been described as tactical-pacifism. This is the avoidance of force and violence whenever possible, but the use of force when necessary to save the lives of one's self and one's people.

Although killing oneself is forbidden under normal Jewish law as being a denial of God's goodness in the world, under extreme circumstances when there has seemed no choice but to either be killed or forced to betray their religion, Jews have committed suicide or mass suicide (see Masada, First French persecution of the Jews, and York Castle for examples). As a grim reminder of those times, there is even a prayer in the Jewish liturgy for "when the knife is at the throat", for those dying "to sanctify God's Name". (See: "Martyrdom"). These acts have received mixed responses by Jewish authorities. Where some Jews regard them as examples of heroic martyrdom, but others saying that while Jews should always be willing to face martyrdom if necessary, it was wrong for them to take their own lives.

Because Judaism focuses on this life, many questions to do with survival and conflict (such as the classic moral dilemma of two people in a desert with only enough water for one to survive) were analysed in great depth by the rabbis within the Talmud, in the attempt to understand the principles a godly person should draw upon in such a circumstance.

The Sermon on the Mount records that Jesus taught that if someone comes to harm you, then one must turn the other cheek. This has led four Protestant Christian denominations to develop a theology of pacifism, the avoidance of force and violence at all times. They are known historically as the "peace churches", and have incorporated Christ's teachings on nonviolence into their theology so as to apply it to participation in the use of violent force; those denominations are the Quakers, Mennonites, Amish, and the Church of the Brethren. Many other churches have people who hold to the doctrine without making it a part of their doctrines, or who apply it to individuals but not to governments, see also Evangelical counsels. The vast majority of Christian nations and groups have not adopted this theology, nor have they followed it in practice. See also But to bring a sword.

Although the Hebrew Bible has many references to capital punishment, the Jewish sages used their authority to make it nearly impossible for a Jewish court to impose a death sentence. Even when such a sentence might have been imposed, the Cities of Refuge and other sanctuaries, were at hand for those unintentionally guilty of capital offences. It was said in the Talmud about the death penalty in Judaism, that if a court killed more than one person in seventy years, it was a barbarous (or "bloody") court and should be condemned as such.

Christianity usually reserved the death penalty for heresy, the denial of the orthodox view of God's view, and witchcraft or similar non-Christian practices. For example, in Spain, unrepentant Jews were exiled, and it was only those crypto-Jews who had accepted baptism under pressure but retained Jewish customs in private, who were punished in this way. It is presently acknowledged by most of Christianity that these uses of capital punishment were deeply immoral.

Orthodox Jews, unlike most Christians, still practice a restrictive diet that has many rules. Most Christians believe that the kosher food laws have been superseded, for example citing what Jesus taught in Mark 7: what you eat doesn't make you unclean but what comes out of a man's heart makes him unclean—although Roman Catholicism and Eastern Orthodoxy have their own set of dietary observances. Eastern Orthodoxy, in particular has very elaborate and strict rules of fasting, and continues to observe the Council of Jerusalem's apostolic decree of Act 15.

Some Christian denominations observe some biblical food laws, for example the practice of Ital in Rastifarianism. Jehovah's Witnesses do not eat blood products and are known for their refusal to accept blood transfusions based on not "eating blood".

Judaism does not see human beings as inherently flawed or sinful and needful of being saved from it, but rather capable with a free will of being righteous, and unlike Christianity does not closely associate ideas of "salvation" with a New Covenant delivered by a Jewish messiah, although in Judaism Jewish people will have a renewed national commitment of observing God's commandments under the New Covenant, and the Jewish Messiah will also be ruling at a time of global peace and acceptance of God by all people.

Judaism holds instead that proper living is accomplished through good works and heartfelt prayer, as well as a strong faith in God. Judaism also teaches that gentiles can receive a share in "the world to come". This is codified in the Mishna Avot 4:29, the Babylonian Talmud in tractates Avodah Zarah 10b, and Ketubot 111b, and in Maimonides's 12th century law code, the "Mishneh Torah", in "Hilkhot Melachim" (Laws of Kings) 8.11.

The Protestant view is that every human is a sinner, and being saved by God's grace, not simply by the merit of one's own actions, pardons a damnatory sentence to Hell.

In Judaism, one must go "to those he has harmed" in order to be entitled to forgiveness. This means that in Judaism a person cannot obtain forgiveness from God for wrongs the person has done to other people. This also means that, unless the victim forgave the perpetrator before he died, murder is unforgivable in Judaism, and they will answer to God for it, though the victims' family and friends can forgive the murderer for the grief they caused them.

Thus the "reward" for forgiving others is not God's forgiveness for wrongs done to others, but rather help "in obtaining forgiveness from the other person."

Sir Jonathan Sacks, Chief Rabbi of the United Hebrew Congregations of the Commonwealth, summarized: "it is not that God forgives, while human beings do not. To the contrary, we believe that just as only God can forgive sins against God, so only human beings can forgive sins against human beings."

In Christianity, forgiveness by God is promised to the repentant even though the wronged party has not forgiven the offender: "If we confess our sins, he is faithful and just and will forgive us our sins and purify us from all unrighteousness." (1 John 1:9 ) Jesus, however, requires his disciples to forgive others if they want to be forgiven themselves. Matthew 6:14,15, which follows the Lord's Prayer, says "For if you forgive men when they sin against you, your heavenly Father will also forgive you. But if you do not forgive men their sins, your Father will not forgive your sins." Forgiveness is not an option to a Christian, rather one must forgive to be a Christian.

Both Christianity and Judaism believe in some form of judgment. Most Christians (the exception is Full Preterism) believe in the future Second Coming of Jesus, which includes the Resurrection of the Dead and the Last Judgment. Those who have accepted Jesus as their personal saviour will be saved and live in God's presence in the Kingdom of Heaven, those who have not accepted Jesus as their saviour, will be cast into the Lake of Fire (eternal torment, finite torment, or simply annihilated), see for example The Sheep and the Goats.

In Jewish liturgy there is significant prayer and talk of a "book of life" that one is written into, indicating that God judges each person each year even after death. This annual judgment process begins on Rosh Hashanah and ends with Yom Kippur. Additionally, God sits daily in judgment concerning a person's daily activities. Upon the anticipated arrival of the Messiah, God will judge the nations for their persecution of Israel during the exile. Later, God will also judge the Jews over their observance of the Torah.

There is little Jewish literature on heaven or hell as actual places, and there are few references to the afterlife in the Hebrew Bible. One is the ghostly apparition of Samuel, called up by the Witch of Endor at King Saul's command. Another is a mention by the Prophet Daniel of those who sleep in the earth rising to either everlasting life or everlasting abhorrence.

Early Hebrew views were more concerned with the fate of the nation of Israel as a whole, rather than with individual immortality. A stronger belief in an afterlife for each person developed during the Second Temple period but was contested by various Jewish sects. Pharisees believed that in death, people rest in their graves until they are physically resurrected with the coming of the Messiah, and within that resurrected body the soul would exist eternally. Maimonides also included the concept of resurrection in his Thirteen Principles of Faith.

Judaism's view is summed up by a biblical observation about the Torah: in the beginning God clothes the naked (Adam), and at the end God buries the dead (Moses). The Children of Israel mourned for 40 days, then got on with their lives.

In Judaism, Heaven is sometimes described as a place where God debates Talmudic law with the angels, and where Jews spend eternity studying the Written and Oral Torah. Jews do not believe in "Hell" as a place of eternal torment. Gehenna is a place or condition of purgatory where Jews spend up to twelve months purifying to get into heaven, depending on how sinful they have been, although some suggest that certain types of sinners can never be purified enough to go to heaven and rather than facing eternal torment, simply cease to exist. Therefore, some violations like suicide would be punished by separation from the community, such as not being buried in a Jewish cemetery (in practice, rabbis often rule suicides to be mentally incompetent and thus not responsible for their actions). Judaism also does not have a notion of hell as a place ruled by Satan since God's dominion is total and Satan is only one of God's angels.

Catholics also believe in a purgatory for those who are going to heaven, but Christians in general believe that Hell is a fiery place of torment that never ceases, called the Lake of Fire. A small minority believe this is not permanent, and that those who go there will eventually either be saved or cease to exist. Heaven for Christians is depicted in various ways. As the Kingdom of God described in the New Testament and particularly the Book of Revelation, Heaven is a new or restored earth, a World to Come, free of sin and death, with a New Jerusalem led by God, Jesus, and the most righteous of believers starting with 144,000 Israelites from every tribe, and all others who received salvation living peacefully and making pilgrimages to give glory to the city.

In Christianity, promises of Heaven and Hell as rewards and punishments are often used to motivate good and bad behavior, as threats of disaster were used by prophets like Jeremiah to motivate the Israelites. Modern Judaism generally rejects this form of motivation, instead teaching to do the right thing because it's the right thing to do. As Maimonides wrote:

"A man should not say: I shall carry out the precepts of the Torah and study her wisdom in order to receive all the blessings written therein or in order to merit the life of the World to Come and I shall keep away from the sins forbidden by the Torah in order to be spared the curses mentioned in the Torah or in order not to be cut off from the life of the World to Come. It is not proper to serve God in this fashion. For one who serves thus serves out of fear. Such a way is not that of the prophets and sages. Only the ignorant, and the women and children serve God in this way. These are trained to serve out of fear until they obtain sufficient knowledge to serve out of love. One who serves God out of love studies the Torah and practices the precepts and walks in the way of wisdom for no ulterior motive at all, neither out of fear of evil nor in order to acquire the good, but follows the truth because it is true and the good will follow the merit of attaining to it. It is the stage of Abraham our father whom the Holy One, blessed be God, called "My friend" (Isaiah 41:8 – "ohavi" = the one who loves me) because he served out of love alone. It is regarding this stage that the Holy One, Blessed be God, commanded us through Moses, as it is said: "You shall love the Lord your God" (Deuteronomy 6:5). When man loves God with a love that is fitting he automatically carries out all the precepts of love.

Jews believe that a descendant of King David will one day appear to restore the Kingdom of Israel and usher in an era of peace, prosperity, and spiritual understanding for Israel and all the nations of the world. Jews refer to this person as Moshiach or "anointed one", translated as messiah in English. The traditional Jewish understanding of the messiah is that he is fully human and born of human parents without any supernatural element. The messiah is expected to have a relationship with God similar to that of the prophets of the Tanakh. In his commentary on the Talmud, Maimonides (Rabbi Moshe ben Maimon) wrote:

He adds:

He also clarified the nature of the Messiah:

The Christian view of Jesus as Messiah goes beyond such claims and is the fulfillment and union of three anointed offices; a prophet like Moses who delivers God's commands and covenant and frees people from bondage, a High Priest in the order of Melchizedek overshadowing the Levite priesthood and a king like King David ruling over Jews, and like God ruling over the whole world and coming from the line of David.

For Christians, Jesus is also fully human and fully divine as the Word of God who sacrifices himself so that humans can receive salvation. Jesus sits in Heaven at the Right Hand of God and will judge humanity in the end times when he returns to earth.

Christian readings of the Hebrew Bible find many references to Jesus. This can take the form of specific prophesy, and in other cases of foreshadowing by types or forerunners. Traditionally, most Christian readings of the Bible maintained that almost every prophecy was actually about the coming of Jesus, and that the entire Old Testament of the Bible is a prophecy about the coming of Jesus.

Catholicism teaches "Extra Ecclesiam Nulla Salus" ("Outside the Church there is no salvation"), which some, like Fr. Leonard Feeney, interpreted as limiting salvation to Catholics only. At the same time, it does not deny the possibility that those not visibly members of the Church may attain salvation as well. In recent times, its teaching has been most notably expressed in the Vatican II council documents "Unitatis Redintegratio" (1964), "Lumen gentium" (1964), "Nostra aetate" (1965), an encyclical issued by Pope John Paul II: "Ut unum sint" (1995), and in a document issued by the Congregation for the Doctrine of the Faith, "Dominus Iesus" in 2000. The latter document has been criticised for claiming that non-Christians are in a "gravely deficient situation" as compared to Catholics, but also adds that "for those who are not formally and visibly members of the Church, salvation in Christ is accessible by virtue of a grace which, while having a mysterious relationship to the Church, does not make them formally part of the Church, but enlightens them in a way which is accommodated to their spiritual and material situation."

Pope John Paul II on October 2, 2000 emphasized that this document did not say that non-Christians were actively denied salvation: "...this confession does not deny salvation to non-Christians, but points to its ultimate source in Christ, in whom man and God are united". On December 6 the Pope issued a statement to further emphasize that the Church continued to support its traditional stance that salvation was available to believers of other faiths: "The gospel teaches us that those who live in accordance with the Beatitudes—the poor in spirit, the pure of heart, those who bear lovingly the sufferings of life—will enter God's kingdom." He further added, "All who seek God with a sincere heart, including those who do not know Christ and his church, contribute under the influence of Grace to the building of this Kingdom." On August 13, 2002 American Catholic bishops issued a joint statement with leaders of Reform and Conservative Judaism, called "Reflections on Covenant and Mission", which affirmed that Christians should not target Jews for conversion. The document stated: "Jews already dwell in a saving covenant with God" and "Jews are also called by God to prepare the world for God's Kingdom." However, many Christian denominations still believe it is their duty to reach out to "unbelieving" Jews.

In December 2015, the Vatican released a 10,000-word document that, among other things, stated that Jews do not need to be converted to find salvation, and that Catholics should work with Jews to fight antisemitism.

Eastern Orthodox Christianity emphasizes a continuing life of repentance or "metanoia", which includes an increasing improvement in thought, belief and action. Regarding the salvation of Jews, Muslims, and other non-Christians, the Orthodox have traditionally taught that there is no salvation outside the church. Orthodoxy recognizes that other religions may contain truth, to the extent that they are in agreement with Christianity.

Many Orthodox theologians believe that all people will have an opportunity to embrace union with God, including Jesus, after their death, and so become part of the Church at that time. God is thought to be good, just, and merciful; it would not seem just to condemn someone because they never heard the Gospel message, or were taught a distorted version of the Gospel by heretics. Therefore, the reasoning goes, they must at some point have an opportunity to make a genuine informed decision. Ultimately, those who persist in rejecting God condemn themselves, by cutting themselves off from the ultimate source of all Life, and from the God who is Love embodied. Jews, Muslims, and members of other faiths, then, are expected to convert to Christianity in the afterlife.

The Church of Jesus Christ of Latter-day Saints (which is not part of Eastern Orthodoxy) also holds this belief, and performs baptism for the dead, in which people are baptized in behalf of their ancestors who, it is believed, are given the opportunity to accept the ordinance.

Judaism is not a proselytizing religion. Orthodox Judaism deliberately makes it very difficult to convert and become a Jew, and requires a significant and full-time effort in living, study, righteousness, and conduct over several years. The final decision is by no means a foregone conclusion. A person cannot become Jewish by marrying a Jew, or by joining a synagogue, nor by any degree of involvement in the community or religion, but only by explicitly undertaking intense, formal, and supervised work over years aimed towards that goal. Some less strict versions of Judaism have made this process somewhat easier but it is still far from common.

In the past, Judaism was more evangelistic, but this was often more akin just to "greater openness to converts" rather than active soliciting of conversions. Since Jews believe that one need not be a Jew to approach God, there is no religious pressure to convert non-Jews to their faith.

The Chabad-Lubavitch branch of Hasidic Judaism has been an exception to this non-proselytizing standard, since in recent decades it has been actively promoting Noahide Laws for gentiles as an alternative to Christianity.

By contrast, Christianity is an explicitly evangelistic religion. Christians are commanded by Jesus to "Therefore go and make disciples of all nations". Historically, evangelism has on rare occasions led to forced conversion under threat of death or mass expulsion.

Many Jews view Jesus as one in a long list of failed Jewish claimants to be the Messiah, none of whom fulfilled the tests of a prophet specified in the Law of Moses. Others see Jesus as a teacher who worked with the gentiles and ascribe the messianic claims that Jews find objectionable to his later followers. Because much physical and spiritual violence was done to Jews in the name of Jesus and his followers, and because evangelism is still an active aspect of many church's activities, many Jews are uncomfortable with discussing Jesus and treat him as a non-person. In answering the question "What do Jews think of Jesus", philosopher Milton Steinberg claims, for Jews, Jesus cannot be accepted as anything more than a teacher. "In only a few respects did Jesus deviate from the Tradition," Steinberg concludes, "and in all of them, Jews believe, he blundered."

Judaism does not believe that God requires the sacrifice of any human. This is emphasized in Jewish traditions concerning the story of the Akedah, the binding of Isaac. In the Jewish explanation, this is a story in the Torah whereby God wanted to test Abraham's faith and willingness, and Isaac was never going to be actually sacrificed. Thus, Judaism rejects the notion that anyone can or should die for anyone else's sin. Judaism is more focused on the practicalities of understanding how one may live a sacred life in the world according to God's will, rather than a hope of a future one. Judaism does not believe in the Christian concept of hell but does have a punishment stage in the afterlife (i.e. Gehenna, the New Testament word translated as hell) as well as a Heaven (Gan Eden), but the religion does not intend it as a focus.

Judaism views the worship of Jesus as inherently polytheistic, and rejects the Christian attempts to explain the Trinity as a complex monotheism. Christian festivals have no religious significance in Judaism and are not celebrated, but some secular Jews in the West treat Christmas as a secular holiday.

Christians believe that Christianity is the fulfillment and successor of Judaism, retaining much of its doctrine and many of its practices including monotheism, the belief in a Messiah, and certain forms of worship like prayer and reading from religious texts. Christians believe that Judaism requires blood sacrifice to atone for sins, and believe that Judaism has abandoned this since the destruction of the Second Temple. Most Christians consider the Mosaic Law to have been a necessary intermediate stage, but that once the crucifixion of Jesus occurred, adherence to civil and ceremonial Law was superseded by the New Covenant.

Some Christians adhere to New Covenant theology, which states that with the arrival of his New Covenant, Jews have ceased being blessed under his Mosaic covenant. This position has been softened or disputed by other Christians, where Jews are recognized to have a special status under the Abrahamic covenant. New Covenant theology is thus in contrast to Dual-covenant theology.

Some Christians who view the Jewish people as close to God seek to understand and incorporate elements of Jewish understanding or perspective into their beliefs as a means to respect their "parent" religion of Judaism, or to more fully seek out and return to their Christian roots. Christians embracing aspects of Judaism are sometimes criticized as Biblical Judaizers by Christians when they pressure gentile Christians to observe Mosaic teachings rejected by most modern Christians.

Some scholars have found evidence of continuous interactions between Jewish-Christian and rabbinic movements from the mid- to late second century CE to the fourth century CE.

In addition to each having varied views on the other as a religion, there has also been a long and often painful history of conflict, persecution and at times, reconciliation, between the two religions, which have influenced their mutual views of their relationship over time.

Since the time of the Middle Ages, the Catholic Church upheld "Constitution pro Judæis" (Formal Statement on the Jews), which stated 

Persecution, forcible conversion, and forcible displacement of Jews (i.e. hate crimes) occurred for many centuries, with occasional gestures to reconciliation from time to time. Pogroms were common throughout Christian Europe, including organized violence, restrictive land ownership and professional lives, forcible relocation and ghettoization, mandatory dress codes, and at times humiliating actions and torture. All had major effects on Jewish cultures. There have also been non-coercive outreach and missionary efforts such as the Church of England's Ministry Among Jewish People, founded in 1809.

For Martin Buber, Judaism and Christianity were variations on the same theme of messianism. Buber made this theme the basis of a famous definition of the tension between Judaism and Christianity:

Pre-messianically, our destinies are divided. Now to the Christian, the Jew is the incomprehensibly obdurate man who declines to see what has happened; and to the Jew, the Christian is the incomprehensibly daring man who affirms in an unredeemed world that its redemption has been accomplished. This is a gulf which no human power can bridge.
The Nazi Party was known for its persecution of Christian Churches; many of them, such as the Protestant Confessing Church and the Catholic Church, as well as Quakers and Jehovah's Witnesses, aided and rescued Jews who were being targeted by the antireligious régime.

Following the Holocaust, attempts have been made to construct a new Jewish-Christian relationship of mutual respect for differences, through the inauguration of the interfaith body the Council of Christians and Jews in 1942 and International Council of Christians and Jews. The Seelisberg Conference in 1947 established 10 points relating to the sources of Christian antisemitism. The ICCJ's "Twelve points of Berlin" sixty years later aim to reflect a recommitment to interreligious dialogue between the two communities.

Pope John Paul II and the Catholic Church have "upheld the Church's acceptance of the continuing and permanent election of the Jewish people" as well as a reaffirmation of the covenant between God and the Jews. In December 2015, the Vatican released a 10,000-word document that, among other things, stated that Catholics should work with Jews to fight antisemitism.

On December 3, 2015, the Center for Jewish-Christian Understanding and Cooperation (CJCUC) spearheaded a petition of orthodox rabbis from around the world calling for increased partnership between Jews and Christians.
The unprecedented Orthodox Rabbinic Statement on Christianity, entitled ""To Do the Will of Our Father in Heaven: Toward a Partnership between Jews and Christians"", was initially signed by over 25 prominent Orthodox rabbis in Israel, United States and Europe and now has over 60 signatories.

On August 31st 2017, representatives of the Conference of European Rabbis, the Rabbinical Council of America, and the Commission of the Chief Rabbinate of Israel issued and presented the Holy See with a statement entitled "Between Jerusalem and Rome". The document pays particular tribute to the Second Vatican Council’s Declaration "Nostra Aetate", whose fourth chapter represents the “Magna Charta” of the Holy See's dialogue with the Jewish world. The Statement "Between Jerusalem and Rome" does not hide the theological differences that exist between the two faith traditions while all the same it expresses a firm resolve to collaborate more closely, now and in the future.




</doc>
<doc id="7504" url="https://en.wikipedia.org/wiki?curid=7504" title="Cesare Borgia">
Cesare Borgia

Cesare Borgia (; Catalan: ; , ; 13 September 1475 – 12 March 1507), Duke of Valentinois, was an Italian "condottiero", nobleman, politician, and cardinal with Aragonese origin, whose fight for power was a major inspiration for "The Prince" by Machiavelli. He was the son of Pope Alexander VI (r. 1492–1503, born Rodrigo Borgia) and his long-term mistress Vannozza dei Cattanei. He was the brother of Lucrezia Borgia; Giovanni Borgia (Juan), Duke of Gandia; and Gioffre Borgia (Jofré in Valencian), Prince of Squillace. He was half-brother to Don Pedro Luis de Borja (1460–88) and Girolama de Borja, children of unknown mothers.

After initially entering the church and becoming a cardinal on his father's election to the Papacy, he became the first person to resign a cardinalcy after the death of his brother in 1498. His father set him up as a prince with territory carved from the Papal States, but after his father's death he was unable to retain power for long. According to Machiavelli this was not due to a lack of foresight, but rather, his own illness.

Like many aspects of Cesare Borgia's life, the date of his birth is a subject of dispute. He was born in Rome—in either 1475 or 1476—the illegitimate son of Cardinal Roderic Llançol i de Borja, usually known as "Rodrigo Borgia", later Pope Alexander VI, and his mistress Vannozza dei Cattanei, about whom information is sparse. The Borgia family originally came from the Kingdom of Valencia, and rose to prominence during the mid-15th century; Cesare's great-uncle Alphonso Borgia (1378–1458), bishop of Valencia, was elected Pope Callixtus III in 1455. Cesare's father, Pope Alexander VI, was the first pope who openly recognized his children born out of wedlock.

Stefano Infessura writes that Cardinal Borgia falsely claimed Cesare to be the legitimate son of another man—Domenico d'Arignano, the nominal husband of Vannozza dei Cattanei. More likely, Pope Sixtus IV granted Cesare a release from the necessity of proving his birth in a papal bull of 1 October 1480.

Cesare was initially groomed for a career in the Church. Following school in Perugia and Pisa, Cesare studied law at the "Studium Urbis" (nowadays Sapienza University of Rome). He was made Bishop of Pamplona at the age of 15 and archbishop of Valencia at 17. In 1493, he had also been appointed bishop of both Castres and Elne. In 1494, he also received the title of abbot of the abbey of Saint-Michel-de-Cuxa. Along with his father's elevation to Pope, Cesare was made Cardinal at the age of 18.

Alexander VI staked the hopes of the Borgia family on Cesare's brother Giovanni, who was made captain general of the military forces of the papacy. Giovanni was assassinated in 1497 in mysterious circumstances. Several contemporaries suggested that Cesare might have been his killer, as Giovanni's disappearance could finally open to him a long-awaited military career and also solve the jealousy over Sancha of Aragon, wife of Cesare's younger brother, Gioffre, and mistress of both Cesare and Giovanni. Cesare's role in the act has never been clear. However, he had no definitive motive, as he was likely to be given a powerful secular position, whether or not his brother lived. It is more likely that Giovanni was killed as a result of a sexual liaison.

On 17 August 1498, Cesare became the first person in history to resign the cardinalate. On the same day, Louis XII of France named Cesare Duke of Valentinois, and this title, along with his former position as Cardinal of Valencia, explains the nickname "Valentino".

Cesare's career was founded upon his father's ability to distribute patronage, along with his alliance with France (reinforced by his marriage with Charlotte d'Albret, sister of John III of Navarre), in the course of the Italian Wars. Louis XII invaded Italy in 1499: after Gian Giacomo Trivulzio had ousted its duke Ludovico Sforza, Cesare accompanied the king in his entrance into Milan.

At this point Alexander decided to profit from the favourable situation and carve out for Cesare a state of his own in northern Italy. To this end, he declared that all his vicars in Romagna and Marche were deposed. Though in theory subject directly to the pope, these rulers had been practically independent or dependent on other states for generations. In the view of the citizens, these vicars were cruel and petty. When Cesare eventually took power, he was viewed by the citizens as a great improvement.

Cesare was appointed commander of the papal armies with a number of Italian mercenaries, supported by 300 cavalry and 4,000 Swiss infantry sent by the King of France. Alexander sent him to capture Imola and Forlì, ruled by Caterina Sforza (mother of the Medici "condottiero" Giovanni dalle Bande Nere). Despite being deprived of his French troops after the conquest of those two cities, Borgia returned to Rome to celebrate a triumph and to receive the title of Papal Gonfalonier from his father. In 1500 the creation of twelve new cardinals granted Alexander enough money for Cesare to hire the "condottieri," Vitellozzo Vitelli, Gian Paolo Baglioni, Giulio and Paolo Orsini, and Oliverotto Euffreducci, who resumed his campaign in Romagna.

Giovanni Sforza, first husband of Cesare's sister Lucrezia, was soon ousted from Pesaro; Pandolfo Malatesta lost Rimini; Faenza surrendered, its young lord Astorre III Manfredi being later drowned in the Tiber by Cesare's order. In May 1501 the latter was created duke of Romagna. Hired by Florence, Cesare subsequently added the lordship of Piombino to his new lands.

While his "condottieri" took over the siege of Piombino (which ended in 1502), Cesare commanded the French troops in the sieges of Naples and Capua, defended by Prospero and Fabrizio Colonna. On 24 June 1501 his troops stormed the latter, causing the collapse of Aragonese power in southern Italy.

In June 1502 he set out for Marche, where he was able to capture Urbino and Camerino by treason. He planned to conquer Bologna next. However, his "condottieri", most notably Vitellozzo Vitelli and the Orsini brothers (Giulio, Paolo and Francesco), feared Cesare's cruelty and set up a plot against him. Guidobaldo da Montefeltro and Giovanni Maria da Varano returned to Urbino and Camerino, and Fossombrone revolted. The fact that his subjects had enjoyed his rule thus far meant that his opponents had to work much harder than they would have liked. He eventually recalled his loyal generals to Imola, where he waited for his opponents' loose alliance to collapse. Cesare called for a reconciliation, but imprisoned his "condottieri" in Senigallia, then called Sinigaglia, a feat described as a "wonderful deceiving" by Paolo Giovio, and had them executed.

Although he was an immensely capable general and statesman, Cesare had trouble maintaining his domain without continued Papal patronage. Niccolò Machiavelli cites Cesare's dependence on the good will of the Papacy, under the control of his father, to be the principal disadvantage of his rule. Machiavelli argued that, had Cesare been able to win the favour of the new Pope, he would have been a very successful ruler. The news of his father's death (1503) arrived when Cesare was planning the conquest of Tuscany. While he was convalescing in Castel Sant'Angelo, his troops controlled the conclave.

The new pope, Pope Pius III, supported Cesare Borgia and reconfirmed him as Gonfaloniere; but after a brief pontificate of twenty-six days, he died. Borgia's deadly enemy, Giuliano Della Rovere, then succeeded by dexterous diplomacy in tricking the weakened Cesare Borgia into supporting him by offering him money and continued papal backing for Borgia policies in the Romagna; promises which he disregarded upon election. He was elected as Pope Julius II by the near-unanimous vote of the cardinals. Realizing his mistake by then, Cesare tried to correct the situation to his favour, but Pope Julius II made sure of its failure at every turn.

Cesare Borgia, who was facing the hostility of Ferdinand II of Aragon, was betrayed while in Naples by Gonzalo Fernández de Córdoba, a man he had considered his ally, and imprisoned there, while his lands were retaken by the Papacy. In 1504 he was transferred to Spain and imprisoned first in the Castle of Chinchilla de Montearagón in La Mancha, but after an attempted escape he was moved north to the Castle of La Mota, Medina del Campo, near Segovia. He did manage to escape from the Castle of La Mota with assistance, and after running across Santander, Durango and Gipuzkoa, he made it to Pamplona on 3 December 1506, and was much welcomed by King John III of Navarre, who was missing an experienced military commander, ahead of the feared Castilian invasion (1512).

He recaptured Viana, Navarre, then in the hands of forces loyal to the count of Lerín, Ferdinand II of Aragon's conspiratorial ally in Navarre, but not the castle, which he then besieged. In the early morning of 11 March 1507, an enemy party of knights fled from the castle during a heavy storm. Outraged at the ineffectiveness of the siege, the Italian commander chased them only to find himself on his own. The party of knights discovered Borgia was alone, and trapped him in an ambush. Borgia received a fatal injury from a spear. He was then stripped of all his luxurious garments, valuables and a leather mask covering half his face (disfigured possibly by syphilis during his late years). Borgia was left lying naked, with just a red tile covering his genitals.

Borgia was originally buried in a marbled mausoleum John III had ordered built at the altar of the Church of Santa María in Viana, set on one of the stops on the Camino de Santiago. In the 16th century the bishop of Mondoñedo, Antonio de Guevara, published from memory what he had seen written on the tomb when he had paid a visit to the church. This epitaph underwent several changes in wording and meter throughout the years and the version most commonly cited today is that published by the priest and historian Francisco de Alesón in the 18th century. It reads:
Borgia was an old enemy of Ferdinand of Aragon, and he was fighting the count who paved the way for Ferdinand's 1512 Castilian invasion against John III and Catherine of Navarre. While the circumstances are not well known, the tomb was destroyed sometime between 1523 and 1608, during which time Santa María was undergoing renovation and expansion. Tradition goes that a bishop of Calahorra considered inappropriate to have the remains of "that degenerate" lying in the church, so the opportunity was taken to tear down the monument and expel Borgia's bones to where they were reburied under the street in front of the church to be trodden on by all who walked through the town.

Vicente Blasco Ibáñez, in "A los pies de Venus", writes that the then Bishop of Santa María had Borgia expelled from the church because his own father had died after being imprisoned under Alexander VI. It was held for many years that the bones were lost, although in fact local tradition continued to mark their place quite accurately and folklore sprung up around Borgia's death and ghost. The bones were in fact dug up twice and reburied once by historians (both local and international—the first dig in 1886 involved the French historian Charles Yriarte, who also published works on the Borgias) seeking the resting place of the infamous Cesare Borgia. After Borgia was unearthed for the second time in 1945 his bones were taken for a rather lengthy forensic examination by Victoriano Juaristi, a surgeon by trade and Borgia aficionado, and the tests concurred with the preliminary ones carried out in the 19th century. There was evidence that the bones belonged to Borgia.

Cesare Borgia's remains then were sent to Viana's town hall, directly across from Santa María, where they remained until 1953. They were then reburied immediately outside of the Church of Santa María, no longer under the street and in direct danger of being stepped on. A memorial stone was placed over it which, translated into English, declared Borgia the "Generalisimo" of the papal as well as the Navarrese forces. A movement was made in the late 80s to have Borgia dug up once more and put back into Santa María, but this proposal was ultimately rejected by church officials due to recent ruling against the interment of anyone who did not hold the title of pope or cardinal. Since Borgia had renounced the cardinalate it was decided that it would be inappropriate for his bones to be moved into the church. However, Fernando Sebastián Aguilar, the Archbishop of Pamplona, caved in after more than 50 years of petitions and Borgia was finally moved back inside the church on 11 March 2007, the day before the 500th anniversary of his death. "We have nothing against the transfer of his remains. Whatever he may have done in life, he deserves to be forgiven now," said the local church.

 Niccolò Machiavelli met the Duke on a diplomatic mission in his function as Secretary of the Florentine Chancellery. Machiavelli was at Borgia's court from 7 October 1502 through 18 January 1503. During this time he wrote regular dispatches to his superiors in Florence, many of which have survived and are published in Machiavelli's Collected Works. In "The Prince", Machiavelli uses Borgia as an example to elucidate the dangers of acquiring a principality by virtue of another. Although Cesare Borgia's father gave him the power to set up, Cesare ruled the Romagna with skill and tact for the most part. However, when his father died, and a rival to the Borgia family entered the Papal seat, Cesare was overthrown in a matter of months.

Machiavelli attributes two episodes to Cesare Borgia: the method by which the Romagna was pacified, which Machiavelli describes in chapter VII of "The Prince", and the assassination of his captains on New Year's Eve of 1502 in Senigallia.

Machiavelli's use of Borgia is subject to controversy. Some scholars see in Machiavelli's Borgia the precursor of state crimes in the 20th century. Others, including Macaulay and Lord Acton, have historicized Machiavelli's Borgia, explaining the admiration for such violence as an effect of the general criminality and corruption of the time.

Cesare Borgia briefly employed Leonardo da Vinci as military architect and engineer between 1502 and 1503. Cesare provided Leonardo with an unlimited pass to inspect and direct all ongoing and planned construction in his domain. While in Romagna, Leonardo built the canal from Cesena to the Porto Cesenatico. Before meeting Cesare, Leonardo had worked at the Milanese court of Ludovico Sforza for many years, until Louis XII of France drove Sforza out of Italy. After Cesare, Leonardo was unsuccessful in finding another patron in Italy. King Francis I of France was able to convince Leonardo to enter his service, and the last three years of Leonardo's life were spent working in France.

On 10 May 1499, Cesare married Charlotte of Albret (1480 – 11 March 1514). She was a sister of John III of Navarre. They were parents to a daughter, Louise Borgia, Duchess of Valentinois, (1500–1553) who first married Louis II de la Trémoille, Governor of Burgundy, and secondly Philippe de Bourbon (1499–1557), Seigneur de Busset.

Cesare was also father to at least 11 illegitimate children, among them Girolamo Borgia, who married Isabella Contessa di Carpi, and Lucrezia Borgia (the younger), who, after Cesare's death, was moved to Ferrara to the court of her aunt, the elder Lucrezia Borgia.




Nathaniel Lee wrote a play entitled "Caesar Borgia" (1680) in which he appears as the central character.
Alexandru Kirițescu wrote a play entitled "Borgia" (1948)



Cesare Borgia is mentioned in the song "B.I.B.L.E.", performed by Killah Priest, which appears on GZA's 1995 album "Liquid Swords", as well as Killah Priest's debut album "Heavy Mental". He is also mentioned in the song "Jeshurun" on Priest's album "Behind the Stained Glass".

Cesare Borgia is featured as the main antagonist and final boss in the 2010 video game "", the Pope's son and commander of the Papal army, a man with ambitions who wants to unite and rule all of Italy. He is considered one of the best "Assassin's Creed" villains. He also appears in the remastered version "".



Notes

Sources



</doc>
<doc id="7507" url="https://en.wikipedia.org/wiki?curid=7507" title="Chronicle">
Chronicle

A chronicle (, from Greek , from , "chronos", "time") is a historical account of facts and events ranged in chronological order, as in a time line. Typically, equal weight is given for historically important events and local events, the purpose being the recording of events that occurred, seen from the perspective of the chronicler. This is in contrast to a narrative or history, which sets selected events in a meaningful interpretive context and excludes those the author does not see as important.

Where a chronicler obtained the information varies; some chronicles are written from first-hand knowledge, some are from witnesses or participants in events, still others are accounts passed mouth to mouth prior to being written down. Some used written material: Charters, letters, or the works of earlier chroniclers. Still others are tales of such unknown origins so as to hold mythical status. Copyists also affected chronicles in creative copying, making corrections or in updating or continuing a chronicle with information not available to the original author(s). The reliability of a particular chronicle is an important determination for modern historians.

In modern times various contemporary newspapers or other periodicals have adopted "chronicle" as part of their name. Various fictional stories have also adopted "chronicle" as part of their title, to give an impression of epic proportion to their stories. A chronicle which traces world history is called a universal chronicle.

Scholars categorize the genre of chronicle into two subgroups: live chronicles, and dead chronicles. A "dead" chronicle is one where the author gathers his list of events up to the time of his writing, but does not record further events as they occur. A "live" chronicle is where one or more authors add to a chronicle in a regular fashion, recording contemporary events shortly after they occur. Because of the immediacy of the information, historians tend to value live chronicles, such as annals, over dead ones.

The term often refers to a book written by a chronicler in the Middle Ages describing historical events in a country, or the lives of a nobleman or a clergyman, although it is also applied to a record of public events. The earliest medieval chronicle to combine both retrospective ("dead") and contemporary ("live") entries, is the Chronicle of Ireland, which spans the years 431 to 911.

Chronicles are the predecessors of modern "time lines" rather than analytical histories. They represent accounts, in prose or verse, of local or distant events over a considerable period of time, both the lifetime of the individual chronicler and often those of several subsequent continuators. If the chronicles deal with events year by year, they are often called annals. Unlike the modern historian, most chroniclers tended to take their information as they found it, and made little attempt to separate fact from legend. The point of view of most chroniclers is highly localised, to the extent that many anonymous chroniclers can be sited in individual abbeys.

It is impossible to say how many chronicles exist, as the many ambiguities in the definition of the genre make it impossible to draw clear distinctions of what should or should not be included. However, the "Encyclopedia of the Medieval Chronicle" lists some 2,500 items written between 300 and 1500 AD.

The most important English chronicles are the "Anglo-Saxon Chronicle", started under the patronage of King Alfred in the 9th century and continued until the 12th century, and the "Chronicles of England, Scotland and Ireland" (1577–87) by Raphael Holinshed and other writers; the latter documents were important sources of materials for Elizabethan drama. Later 16th century Scottish chronicles, written after the Reformation, shape history according to Catholic or Protestant viewpoints.




</doc>
<doc id="7512" url="https://en.wikipedia.org/wiki?curid=7512" title="Concentration">
Concentration

In chemistry, concentration is the abundance of a constituent divided by the total volume of a mixture. Several types of mathematical description can be distinguished: mass concentration, molar concentration, number concentration, and volume concentration. The term concentration can be applied to any kind of chemical mixture, but most frequently it refers to solutes and solvents in solutions. The molar (amount) concentration has variants such as normal concentration and osmotic concentration.

Often in informal, non-technical language, concentration is described in a qualitative way, through the use of adjectives such as "dilute" for solutions of relatively low concentration and "concentrated" for solutions of relatively high concentration. To concentrate a solution, one must add more solute (for example, alcohol), or reduce the amount of solvent (for example, water). By contrast, to dilute a solution, one must add more solvent, or reduce the amount of solute. Unless two substances are "fully" miscible there exists a concentration at which no further solute will dissolve in a solution. At this point, the solution is said to be saturated. If additional solute is added to a saturated solution, it will not dissolve, except in certain circumstances, when supersaturation may occur. Instead, phase separation will occur, leading to coexisting phases, either completely separated or mixed as a suspension. The point of saturation depends on many variables such as ambient temperature and the precise chemical nature of the solvent and solute.

Concentrations are often called levels, reflecting the mental schema of levels on the vertical axis of a graph, which can be high or low (for example, "high serum levels of bilirubin" are concentrations of bilirubin in the blood serum that are greater than normal).

There are four quantities that describe concentration:

The mass concentration formula_1 is defined as the mass of a constituent formula_2 divided by the volume of the mixture formula_3:

The SI unit is kg/m (equal to g/L).

The molar concentration formula_5 is defined as the amount of a constituent formula_6 (in moles) divided by the volume of the mixture formula_3:

The SI unit is mol/m. However, more commonly the unit mol/L (= mol/dm) is used.

The number concentration formula_9 is defined as the number of entities of a constituent formula_10 in a mixture divided by the volume of the mixture formula_3:

The SI unit is 1/m.

The volume concentration formula_13 (not to be confused with volume fraction) is defined as the volume of a constituent formula_14 divided by the volume of the mixture formula_3:

Being dimensionless, it is expressed as a number, e.g., 0.18 or 18%; its unit is 1.

Several other quantities can be used to describe the composition of a mixture. Note that these should not be called concentrations.

Normality is defined as the molar concentration formula_5 divided by an equivalence factor formula_18. Since the definition of the equivalence factor depends on context (which reaction is being studied), IUPAC and NIST discourage the use of normality.

The molality of a solution formula_19 is defined as the amount of a constituent formula_6 (in moles) divided by the mass of the solvent formula_21 (not the mass of the solution):

The SI unit for molality is mol/kg.

The mole fraction formula_23 is defined as the amount of a constituent formula_6 (in moles) divided by the total amount of all constituents in a mixture formula_25:

The SI unit is mol/mol. However, the deprecated parts-per notation is often used to describe small mole fractions.

The mole ratio formula_27 is defined as the amount of a constituent formula_6 divided by the total amount of all "other" constituents in a mixture:

If formula_6 is much smaller than formula_25, the mole ratio is almost identical to the mole fraction.

The SI unit is mol/mol. However, the deprecated parts-per notation is often used to describe small mole ratios.

The mass fraction formula_32 is the fraction of one substance with mass formula_2 to the mass of the total mixture formula_34, defined as:

The SI unit is kg/kg. However, the deprecated parts-per notation is often used to describe small mass fractions.

The mass ratio formula_36 is defined as the mass of a constituent formula_2 divided by the total mass of all "other" constituents in a mixture:

If formula_2 is much smaller than formula_34, the mass ratio is almost identical to the mass fraction.

The SI unit is kg/kg. However, the deprecated parts-per notation is often used to describe small mass ratios.

Concentration depends on the variation of the volume of the solution with temperature due mainly to thermal expansion.



</doc>
<doc id="7514" url="https://en.wikipedia.org/wiki?curid=7514" title="Christine Lavin">
Christine Lavin

Christine Lavin (born January 2, 1952) is a New York City-based singer-songwriter and promoter of contemporary folk music. She has recorded numerous solo albums, and has also recorded with other female folk artists under the name Four Bitchin' Babes. She has also put together several compilation albums of contemporary folk artists, including her latest "Just One Angel", 22 singer/songwriters singing Christmas/Hanukah/Solstice/New Year's songs including actor Jeff Daniels, Grammy-winners Janis Ian and Julie Gold, and the Guitar Man Of Central Park David Ippolito.

She is known for her sense of humor, which is expressed in both her music and her onstage performances. Many of her songs alternate between emotional reflections on romance and outright comedy. One of Lavin's songs, ""Regretting What I Said to You When You Called Me 11:00 On a Friday Morning to Tell Me that at 1:00 Friday Afternoon You're Gonna Leave Your Office, Go Downstairs, Hail a Cab to Go Out to the Airport to Catch a Plane to Go Skiing in the Alps for Two Weeks, Not that I Wanted to Go With You, I Wasn't Able to Leave Town, I'm Not a Very Good Skier, I Couldn't Expect You to Pay My Way, But After Going Out With You for Three Years I DON'T Like Surprises!! Subtitled: A Musical Apology"" is notable for its long title. It is the eighth song on her 1984 album "Future Fossils", and is 3:04 (3 minutes and 4 seconds) long.

Lavin worked at Caffe Lena in Saratoga, New York, until Dave Van Ronk convinced her to move to New York City and make a career as a singer-songwriter. She followed his advice and accepted his offer of guitar lessons. Lavin was the original host of "Sunday Breakfast" on WFUV in New York City. She was a founding member of the Four Bitchin' Babes when they were formed in 1990.





</doc>
<doc id="7515" url="https://en.wikipedia.org/wiki?curid=7515" title="Cutter Expansive Classification">
Cutter Expansive Classification

The Cutter Expansive Classification system is a library classification system devised by Charles Ammi Cutter. The system was the basis for the top categories of the Library of Congress Classification.

Charles Ammi Cutter (1837–1903), inspired by the decimal classification of his contemporary Melvil Dewey, and with Dewey's initial encouragement, developed his own classification scheme for the Winchester Town Library and then the Boston Athenaeum, at which he served as librarian for twenty-four years. He began work on it around the year 1880, publishing an overview of the new system in 1882. The same classification would later be used, but with a different notation, also devised by Cutter, at the Cary Library in Lexington, Massachusetts.

Many libraries found this system too detailed and complex for their needs, and Cutter received many requests from librarians at small libraries who wanted the classification adapted for their collections. He devised the Expansive Classification in response, to meet the needs of growing libraries, and to address some of the complaints of his critics. Cutter completed and published an introduction and schedules for the first six classifications of his new system ("Expansive Classification: Part I: The First Six Classifications"), but his work on the seventh was interrupted by his death in 1903.

The Cutter Expansive Classification, although adopted by comparatively few libraries, mostly in New England, has been called one of the most logical and scholarly of American classifications. Library historian Leo E. LaMontagne writes:

Cutter produced the best classification of the nineteenth century. While his system was less "scientific" than that of J. P. Lesley, its other key features – notation, specificity, and versatility – make it deserving of the praise it has received.

Its top level divisions served as a basis for the Library of Congress classification, which also took over some of its features. It did not catch on as did Dewey's system because Cutter died before it was completely finished, making no provision for the kind of development necessary as the bounds of knowledge expanded and scholarly emphases changed throughout the twentieth century.

The Expansive Classification uses seven separate schedules, each designed to be used by libraries of different sizes. After the first, each schedule was an expansion of the previous one, and Cutter provided instructions for how a library might change from one expansion to another as it grows.

The first classification is meant for only the very small libraries. The first classification has only seven top level classes, and only eight classes in all.


Further expansions add more top level classes and subdivisions. Many subclasses arranged systematically, with common divisions, such as those by geography and language, following a consistent system throughout.

By the fifth classification all the letters of the alphabet are in use for top level classes. These are:


These schedules were not meant to be fixed, but were to be adapted to meet the needs of each library. For example, books on the English language may be put in X, and books on language in general in a subclass of X, or this can be reversed. The first option is less logical, but results in shorter marks for most English language libraries.

Most call numbers in the Expansive Classification follow conventions offering clues to the book's subject. The first line represents the subject, the second the author (and perhaps title), the third and fourth dates of editions, indications of translations, and critical works on particular books or authors. All numbers in the Expansive Classification are (or should be) shelved as if in decimal order.

Size of volumes is indicated by points (.), pluses (+), or slashes (/ or //).

For some subjects a numerical geographical subdivision follows the classification letters on the first line. The number 83 stands for the United States—hence, F83 is U.S. history, G83 U.S. travel, JU83 U.S. politics, WP83 U.S. painting. Geographical numbers are often further expanded decimally to represent more specific areas, sometimes followed by a capital letter indicating a particular city.

The second line usually represents the author's name by a capital letter plus one or more numbers arranged decimally. This may be followed by the first letter or letters of the title in lower-case, and/or sometimes the letters a, b, c indicating other printings of the same title. When appropriate, the second line may begin with a 'form' number—e.g., 1 stands for history and criticism of a subject, 2 for a bibliography, 5 for a dictionary, 6 for an atlas or maps, 7 for a periodical, 8 for a society or university publication, 9 for a collection of works by different authors.

On the third line a capital Y indicates a work about the author or book represented by the first two lines, and a capital E (for English—other letters are used for other languages) indicates a translation into English. If both criticism and translation apply to a single title, the number expands into four lines.

One of the features adopted by other systems, including Library of Congress, is the Cutter number. It is an alphanumeric device to code text so that it can be arranged in alphabetical order using the fewest characters. It contains one or two initial letters and Arabic numbers, treated as a decimal. To construct a Cutter number, a cataloguer consults a Cutter table as required by the classification rules. Although Cutter numbers are mostly used for coding the names of authors, the system can be used for titles, subjects, geographic areas, and more.

Initial letters Qa–Qt are assigned Q2–Q29, while entries beginning with numerals have a Cutter number A12–A19, therefore sorting before the first A entry.

So to make the three digit Cutter number for "Cutter", you would start with "C", then looking under "other consonants", find that "u" gives the number 8, and under "additional letters", "t" is 8, giving a Cutter number of "C88".




</doc>
<doc id="7516" url="https://en.wikipedia.org/wiki?curid=7516" title="Cem Karaca">
Cem Karaca

Muhtar Cem Karaca (5 April 1945 – 8 February 2004) was a prominent Turkish rock musician and one of the most important figures in the Anatolian rock movement. He is a graduate of Robert College. He worked with various Turkish rock bands such as Apaşlar, Kardaşlar, Moğollar and Dervişan. With these bands, he brought a new understanding and interpretation to Turkish Rock.

He was the only child of Mehmet İbrahim Karaca, a theatre actor of Azerbaijani origin, and İrma Felekyan (Toto Karaca), a popular opera, theatre and movie actress of Armenian origin. His first group was called "Dynamites" and was a classic rock cover band. Later he joined "Jaguars", an Elvis Presley cover band. In 1967, he started to write his own music, joining the band "Apaşlar" (The Rowdies), his first Turkish-language group. The same year, he participated in the Golden Microphone () contest, a popular music contest in which he won second place with his song "Emrah". In 1969, Karaca and bass-player Serhan Karabay left Apaşlar and started an original Anatolian group called "Kardaşlar" (The Brothers).

In 1972, Karaca joined the group "Moğollar" (The Mongols) and wrote one of his best-known songs, ""Namus Belası"". However, Cahit Berkay, the leader of Moğollar, wanted an international reputation for his band, and he left for France to take the group to the next level. Karaca, who wanted to continue his Anatolian beat sound, left Moğollar and started his own band "Dervişan" (Dervishes) in 1974. Karaca and Dervişan sang poetic and progressive songs.

In the 1970s, Turkey was dealing with political violence between supporters of the left and the right, separatist movements and the rise of Islamism. As the country fell into chaos, the government suspected Cem Karaca of involvement in rebel organisations. He was accused of treason for being a separatist thinker and a Marxist-Leninist. The Turkish government tried to portray Karaca as a man who was unknowingly writing songs to start a revolution. One politician was quoted as saying, "Karaca is simply calling citizens to a bloody war against the state." "Dervişan" was ultimately dissolved at the end of 1977. In 1978, he founded "Edirdahan", an acronym for "from Edirne to Ardahan"; the westernmost and the easternmost provinces of Turkey. He recorded one LP with "Edirdahan".

In early 1979, he left for West Germany for business reasons, where he started singing in German language, too, namely since autumn 1980 first a lyric of Nazim Hikmet - "Kız Çocuğu" (in English: "Little girl"): Cem performed the German verses alternating with his friend, manager, arranger and bandleader/musician "Ralf Mähnhöfer" attending Cem on grand piano solo or by the band "Anatology" singing the song in Turkish-language.

Turkey continued to spin out of control with military curfews and the 1980 Turkish coup d'état on September 12, 1980. General Kenan Evren took over the government and temporarily banned all the nation's political parties. After the coup, many intellectuals, including writers, artists and journalists, were arrested. A warrant was issued for the arrest of Karaca by the government of Turkey.

The state invited Karaca back several times, but Karaca, not knowing what would happen upon his return, decided not to come back.

While Karaca was in Germany his father died, but he could not return to attend the funeral. After some time, the Turkish government decided to strip Cem Karaca of his Turkish citizenship, keeping the arrest warrant active.

Several years later, in 1987, the prime minister and leader of the Turkish Motherland Party, Turgut Özal, issued an amnesty for Karaca. Shortly afterwards, he returned to Turkey. His return also brought a new album with it, "Merhaba Gençler ve Her zaman Genç Kalanlar" (""Hello, The Young and The Young at Heart""), one of his most influential works. His return home was received cheerfully by his fans, but during his absence Karaca had lost the young audience and acquired only few new listeners. He died on February 8, 2004 and was interred at Karacaahmet Cemetery in the Üsküdar district of Istanbul.





(1) It was released again with different cover, sort of songs and songs in 2003.
He has also appeared as a guest artist on several recordings.





</doc>
<doc id="7517" url="https://en.wikipedia.org/wiki?curid=7517" title="Calista Flockhart">
Calista Flockhart

Calista Kay Flockhart (born November 11, 1964) is an American actress. She is best known for starring as the title character in the legal comedy-drama series "Ally McBeal" (1997–2002), Kitty Walker in the drama series "Brothers & Sisters" (2006–2011) and Cat Grant in the superhero drama series "Supergirl" (2015–present). She has also been featured in a number of films, including the comedy film "The Birdcage" (1996), the romantic comedy film "A Midsummer Night's Dream" (1999), and the drama film "Things You Can Tell Just by Looking at Her" (2000). 

Flockhart has won a Golden Globe Award and Screen Actors Guild Award, and garnered three Emmy Award nominations.

Flockhart is married to Harrison Ford.

Calista Kay Flockhart was born in Freeport, Illinois, the daughter of Kay Calista, an English teacher, and Ronald Flockhart, a Kraft Foods executive. Her parents are retired and live in Morristown, Tennessee. She has one older brother, Gary. Her mother reversed her own first and middle names in naming her Calista Kay.

Because her father's job required the family to move often, Flockhart and her brother grew up in several places including Illinois, Iowa, Minnesota, New Jersey, and Norwich, New York. As a child, she wrote a play called "Toyland" which she performed to a small audience at a dinner party.

Flockhart attended Shawnee High School in Medford Township, New Jersey. Following graduation in 1983, Flockhart attended the Mason Gross School of the Arts at Rutgers University in New Brunswick, New Jersey. While there, she attended a specialized and competitive class, lasting from 6:00 a.m. to 6:00 p.m. In her sophomore year at Rutgers, Flockhart met aspiring actress Jane Krakowski, the best friend of her roommate. Later, they both would work together on "Ally McBeal".

People began recognizing Flockhart's acting ability when William Esper (Mason Gross' theatre director and Flockhart's acting teacher) made an exception to policy by allowing Flockhart to perform on the main stage. Though this venue usually is reserved for juniors and seniors, Harold Scott insisted that Flockhart perform there in his production of William Inge's "Picnic". Flockhart graduated with a Bachelor of Fine Arts degree in Theatre in 1988 as one of the few students who successfully completed the course. Rutgers inducted her into the Hall of Distinguished Alumni on May 3, 2003.

Flockhart moved to New York City in 1989 and began seeking auditions, living with three other women in a two-bedroom apartment and working as a waitress and aerobics instructor. She would remain in the city until 1997.

In spring 1989, Flockhart made her first television appearance in a minor role in an episode of "Guiding Light" as a babysitter. She made her professional debut on the New York stage, appearing in "Beside Herself" alongside Melissa Joan Hart, at the Circle Repertory Theatre. Two years later, Flockhart appeared in the television movie "Darrow". Though she later appeared in films "Naked in New York" (1993) and "Getting In" (1994), her first substantial speaking part in a film was in "Quiz Show", directed by Robert Redford.

Flockhart debuted on Broadway in 1994, as Laura in "The Glass Menagerie". Actress Julie Harris felt Flockhart should be hired without further auditions, claiming that she seemed ideal for the part. Flockhart received a Clarence Derwent Award for her performance. In 1995, Flockhart became acquainted with actors such as Dianne Wiest and Faye Dunaway when she appeared in the movie "Drunks". Later that year, Flockhart starred in "Jane Doe" as a drug addict. In 1996, Flockhart appeared as the daughter of Dianne Wiest and Gene Hackman's characters in "The Birdcage". Throughout that year, she continued to work on Broadway, playing the role of Natasha in Anton Chekhov's "Three Sisters".

In 1997, Flockhart was asked to audition for the starring role in David E. Kelley's Fox television series "Ally McBeal". Kelley, having heard of Flockhart, wanted her to audition for the contract part. Though Flockhart at first hesitated due to the necessary commitment to the show in a negotiable contract, she was swayed by the script and traveled to Los Angeles to audition for the part, which she won. She earned a Golden Globe Award for the role in 1998. Flockhart also appeared on the June 29, 1998, cover of "Time" magazine, placed as the newest iteration in the evolution of feminism, relating to the ongoing debate about the role depicted by her character.

Flockhart performed in a starring role as Kitty Walker, opposite Sally Field, Rachel Griffiths and Matthew Rhys, in the ABC critically acclaimed prime time series "Brothers & Sisters", which premiered in September 2006 in the time slot after "Desperate Housewives". The show was cancelled in May 2011 after running for five years. Flockhart's character was significant throughout the series' first four years, but her appearances were reduced for the 2010–2011 season, coinciding with the departure of TV husband Rob Lowe.

Flockhart played the role of Helena in "A Midsummer Night's Dream", a 1999 film version of Shakespeare's play. In 2000, she appeared in "Things You Can Tell Just by Looking at Her" and "", later accompanying Eve Ensler to Kenya in order to protest violence against women, particularly female genital mutilation. Flockhart also starred in the Off-Broadway production of Ensler's "The Vagina Monologues".

In 2004, Flockhart appeared as Matthew Broderick's deranged girlfriend in "The Last Shot". In the same year, Flockhart travelled to Spain for the filming of "Fragile", which premiered in September 2005 at the Venice Film Festival. She was offered the role of Susan Mayer on "Desperate Housewives" but declined, and the role later went to Teri Hatcher.

In 2014, Flockhart landed a role in "Full Circle"'s second season, as mob boss Ellen. It was expected to air in 2015. This had been Flockhart's first acting role in three years, after her hiatus when "Brothers & Sisters" ended.

In 2015, Flockhart was cast in the television series "Supergirl" as Cat Grant, a "self-made media magnate and founder of CatCo" and boss to Kara (Supergirl's alter ego). The series premiered on October 26, 2015, on CBS. Due to the network's wish to reduce the show's budget, it was moved to sister network The CW after its first season, along with a move to filming in Vancouver. Flockhart remained with the show (albeit as a recurring character), despite her previous aversion to working outside Los Angeles.

Flockhart has been in a relationship with actor Harrison Ford since their meeting at the Golden Globe Awards on January 20, 2002. They became engaged on Valentine's Day in 2009, and were married on June 15, 2010, in Santa Fe, New Mexico. The ceremony was presided over by Governor Bill Richardson and New Mexico Supreme Court Chief Justice Charles W. Daniels. Flockhart and Ford have one adopted son together, Liam Flockhart Ford (b. 2001) whom Flockhart adopted at birth.

Throughout her professional career, Flockhart has maintained her lean figure. However, many have commented that Flockhart had become dangerously thin, particularly when the actress made red carpet appearances in clothing that revealed an emaciated physique. She had maintained throughout the run of "Ally McBeal" that she was never diagnosed with either anorexia or bulimia, nor was she a user of illegal drugs. She did remark, however, that while starring in the show, she refrained from eating sweets, retaining her slimness with intense workouts and running. In 2006, she admitted that she had a problem at the time, and was "exercising too much" and "eating too little".

From 2004 to 2014, Flockhart served as the national spokesperson for Peace Over Violence.


! colspan="3" style="background: #DAA520;" | Theatre World Award


</doc>
<doc id="7519" url="https://en.wikipedia.org/wiki?curid=7519" title="Convolution">
Convolution

In mathematics (and, in particular, functional analysis) convolution is a mathematical operation on two functions ("f" and "g") to produce a third function that expresses how the shape of one is modified by the other. The term "convolution" refers to both the result function and to the process of computing it. Convolution is similar to cross-correlation. For discrete, real-valued functions, they differ only in a time reversal in one of the functions. For continuous functions, the cross-correlation operator is the adjoint of the convolution operator.

It has applications that include probability, statistics, computer vision, natural language processing, image and signal processing, engineering, and differential equations.

The convolution can be defined for functions on Euclidean space, and other groups. For example, periodic functions, such as the discrete-time Fourier transform, can be defined on a circle and convolved by "periodic convolution". (See row 11 at DTFT § Properties.) A "discrete convolution" can be defined for functions on the set of integers.

Generalizations of convolution have applications in the field of numerical analysis and numerical linear algebra, and in the design and implementation of finite impulse response filters in signal processing.

Computing the inverse of the convolution operation is known as deconvolution.

The convolution of "f" and "g" is written "f"∗"g", using an asterisk or star. It is defined as the integral of the product of the two functions after one is reversed and shifted. As such, it is a particular kind of integral transform:

While the symbol "t" is used above, it need not represent the time domain. But in that context, the convolution formula can be described as a weighted average of the function "f"("τ") at the moment "t" where the weighting is given by "g"(−"τ") simply shifted by amount "t". As "t" changes, the weighting function emphasizes different parts of the input function.

For functions "f", "g" supported on only formula_2 (i.e., zero for negative arguments), the integration limits can be truncated, resulting in

In this case, the Laplace transform is more appropriate than the Fourier transform below and boundary terms become relevant.

For the multi-dimensional formulation of convolution, see "domain of definition" (below).

A primarily engineering convention that one often sees is:

which has to be interpreted carefully to avoid confusion. For instance, "ƒ"("t")* "g"("t" − "t") is equivalent to ("ƒ"* "g")("t" − "t"), but "ƒ"("t" − "t")* "g"("t" − "t") is in fact equivalent to ("ƒ"* "g")("t − 2"t"").

Convolution describes the output (in terms of the input) of an important class of operations known as "linear time-invariant" (LTI). See LTI system theory for a derivation of convolution as the result of LTI constraints. In terms of the Fourier transforms of the input and output of an LTI operation, no new frequency components are created. The existing ones are only modified (amplitude and/or phase). In other words, the output transform is the pointwise product of the input transform with a third transform (known as a transfer function). See Convolution theorem for a derivation of that property of convolution. Conversely, convolution can be derived as the inverse Fourier transform of the pointwise product of two Fourier transforms.

One of the earliest uses of the convolution integral appeared in D'Alembert's derivation of Taylor's theorem in "Recherches sur différents points importants du système du monde," published in 1754.

Also, an expression of the type:

is used by Sylvestre François Lacroix on page 505 of his book entitled "Treatise on differences and series", which is the last of 3 volumes of the encyclopedic series: "Traité du calcul différentiel et du calcul intégral", Chez Courcier, Paris, 1797-1800. Soon thereafter, convolution operations appear in the works of Pierre Simon Laplace, Jean-Baptiste Joseph Fourier, Siméon Denis Poisson, and others. The term itself did not come into wide use until the 1950s or 60s. Prior to that it was sometimes known as "Faltung" (which means "folding" in German), "composition product", "superposition integral", and "Carson's integral".
Yet it appears as early as 1903, though the definition is rather unfamiliar in older uses.

The operation:

is a particular case of composition products considered by the Italian mathematician Vito Volterra in 1913.

When a function "g" is periodic, with period "T", then for functions, "f", such that "f"∗"g" exists, the convolution is also periodic and identical to:

where "t" is an arbitrary choice. The summation is called a periodic summation of the function "f".

When "g" is a periodic summation of another function, "g", then "f"∗"g" is known as a "circular" or "cyclic" convolution of "f" and "g".<br>
And if the periodic summation above is replaced by "f", the operation is called a "periodic" convolution of "f" and "g".

For complex-valued functions "f", "g" defined on the set Z of integers, the discrete convolution of "f" and "g" is given by:

The convolution of two finite sequences is defined by extending the sequences to finitely supported functions on the set of integers. When the sequences are the coefficients of two polynomials, then the coefficients of the ordinary product of the two polynomials are the convolution of the original two sequences. This is known as the Cauchy product of the coefficients of the sequences.

Thus when "g" has finite support in the set formula_9 (representing, for instance, a finite impulse response), a finite summation may be used:

When a function "g" is periodic, with period "N", then for functions, "f", such that "f"∗"g" exists, the convolution is also periodic and identical to:

The summation on "k" is called a periodic summation of the function "f".

If "g" is a periodic summation of another function, "g", then "f"∗"g" is known as a circular convolution of "f" and "g".

When the non-zero durations of both "f" and "g" are limited to the interval [0, "N" − 1], "f"∗"g" reduces to these common forms:

] \equiv (f * _N g)[n]

The notation ("f" ∗ "g") for "cyclic convolution" denotes convolution over the cyclic group of integers modulo "N".

Circular convolution arises most often in the context of fast convolution with a fast Fourier transform (FFT) algorithm.

In many situations, discrete convolutions can be converted to circular convolutions so that fast transforms with a convolution property can be used to implement the computation. For example, convolution of digit sequences is the kernel operation in multiplication of multi-digit numbers, which can therefore be efficiently implemented with transform techniques (; ).

The most common fast convolution algorithms use fast Fourier transform (FFT) algorithms via the circular convolution theorem. Specifically, the circular convolution of two finite-length sequences is found by taking an FFT of each sequence, multiplying pointwise, and then performing an inverse FFT. Convolutions of the type defined above are then efficiently implemented using that technique in conjunction with zero-extension and/or discarding portions of the output. Other fast convolution algorithms, such as the Schönhage–Strassen algorithm or the Mersenne transform, use fast Fourier transforms in other rings.

If one sequence is much longer than the other, zero-extension of the shorter sequence and fast circular convolution is not the most computationally efficient method available. Instead, decomposing the longer sequence into blocks and convolving each block allows for faster algorithms such as the Overlap–save method and Overlap–add method. A hybrid convolution method that combines block and FIR algorithms allows for a zero input-output latency that is useful for real-time convolution computations.

The convolution of two complex-valued functions on R is itself a complex-valued function on R, defined by:

is well-defined only if "f" and "g" decay sufficiently rapidly at infinity in order for the integral to exist. Conditions for the existence of the convolution may be tricky, since a blow-up in "g" at infinity can be easily offset by sufficiently rapid decay in "f". The question of existence thus may involve different conditions on "f" and "g":

If "f" and "g" are compactly supported continuous functions, then their convolution exists, and is also compactly supported and continuous . More generally, if either function (say "f") is compactly supported and the other is locally integrable, then the convolution "f"∗"g" is well-defined and continuous.

Convolution of "f" and "g" is also well defined when both functions are locally square integrable on R and supported on an interval of the form [a, +∞) (or both supported on [-∞, a]).

The convolution of "f" and "g" exists if "f" and "g" are both Lebesgue integrable functions in L(R), and in this case "f"∗"g" is also integrable . This is a consequence of Tonelli's theorem. This is also true for functions in formula_13, under the discrete convolution, or more generally for the convolution on any group.

Likewise, if "f" ∈ "L"(R) and "g" ∈ "L"(R) where 1 ≤ "p" ≤ ∞, then "f"∗"g" ∈ "L"(R) and

In the particular case "p" = 1, this shows that "L" is a Banach algebra under the convolution (and equality of the two sides holds if "f" and "g" are non-negative almost everywhere).
More generally, Young's inequality implies that the convolution is a continuous bilinear map between suitable "L" spaces. Specifically, if 1 ≤ "p","q","r" ≤ ∞ satisfy

then

so that the convolution is a continuous bilinear mapping from "L"×"L" to "L".
The Young inequality for convolution is also true in other contexts (circle group, convolution on Z). The preceding inequality is not sharp on the real line: when , there exists a constant such that:

The optimal value of was discovered in 1975.

A stronger estimate is true provided :
where formula_19 is the weak "L" norm. Convolution also defines a bilinear continuous map formula_20 for formula_21, owing to the weak Young inequality:

In addition to compactly supported functions and integrable functions, functions that have sufficiently rapid decay at infinity can also be convolved. An important feature of the convolution is that if "f" and "g" both decay rapidly, then "f"∗"g" also decays rapidly. In particular, if "f" and "g" are rapidly decreasing functions, then so is the convolution "f"∗"g". Combined with the fact that convolution commutes with differentiation (see Properties), it follows that the class of Schwartz functions is closed under convolution .

Under some circumstances, it is possible to define the convolution of a function with a distribution, or of two distributions. If "f" is a compactly supported function and "g" is a distribution, then "f"∗"g" is a smooth function defined by a distributional formula analogous to

More generally, it is possible to extend the definition of the convolution in a unique way so that the associative law

remains valid in the case where "f" is a distribution, and "g" a compactly supported distribution .

The convolution of any two Borel measures μ and ν of bounded variation is the measure λ defined by 
This agrees with the convolution defined above when μ and ν are regarded as distributions, as well as the convolution of L functions when μ and ν are absolutely continuous with respect to the Lebesgue measure.

The convolution of measures also satisfies the following version of Young's inequality
where the norm is the total variation of a measure. Because the space of measures of bounded variation is a Banach space, convolution of measures can be treated with standard methods of functional analysis that may not apply for the convolution of distributions.

The convolution defines a product on the linear space of integrable functions. This product satisfies the following algebraic properties, which formally mean that the space of integrable functions with the product given by convolution is a commutative associative algebra without identity . Other linear spaces of functions, such as the space of continuous functions of compact support, are closed under the convolution, and so also form commutative associative algebras.


Proof: By definition
Changing the variable of integration to formula_29 and the result follows.


Proof: This follows from using Fubini's theorem (i.e., double integrals can be evaluated as
iterated integrals in either order).

Proof: This follows from linearity of the integral.

for any real (or complex) number formula_33.

No algebra of functions possesses an identity for the convolution. The lack of identity is typically not a major inconvenience, since most collections of functions on which the convolution is performed can be convolved with a delta distribution or, at the very least (as is the case of "L") admit approximations to the identity. The linear space of compactly supported distributions does, however, admit an identity under the convolution. Specifically,
where δ is the delta distribution.


Some distributions have an inverse element for the convolution, "S", which is defined by
The set of invertible distributions forms an abelian group under the convolution.



Proof:


If "f" and "g" are integrable functions, then the integral of their convolution on the whole space is simply obtained as the product of their integrals:

This follows from Fubini's theorem. The same result holds if "f" and "g" are only assumed to be nonnegative measurable functions, by Tonelli's theorem.

In the one-variable case,

where "d"/"dx" is the derivative. More generally, in the case of functions of several variables, an analogous formula holds with the partial derivative:

A particular consequence of this is that the convolution can be viewed as a "smoothing" operation: the convolution of "f" and "g" is differentiable as many times as "f" and "g" are in total.

These identities hold under the precise condition that "f" and "g" are absolutely integrable and at least one of them has an absolutely integrable (L) weak derivative, as a consequence of Young's convolution inequality. For instance, when "f" is continuously differentiable with compact support, and "g" is an arbitrary locally integrable function,
These identities also hold much more broadly in the sense of tempered distributions if one of "f" or "g" is a compactly supported distribution or a Schwartz function and the other is a tempered distribution. On the other hand, two positive integrable and infinitely differentiable functions may have a nowhere continuous convolution.

In the discrete case, the difference operator "D" "f"("n") = "f"("n" + 1) − "f"("n") satisfies an analogous relationship:

The convolution theorem states that

where formula_50 denotes the Fourier transform of formula_51, and formula_52 is a constant that depends on the specific normalization of the Fourier transform. Versions of this theorem also hold for the Laplace transform, two-sided Laplace transform, Z-transform and Mellin transform.

See also the less trivial Titchmarsh convolution theorem.

The convolution commutes with translations, meaning that

where τf is the translation of the function "f" by "x" defined by

If "f" is a Schwartz function, then τ"f" is the convolution with a translated Dirac delta function τ"f" = "f"∗"τ" "δ". So translation invariance of the convolution of Schwartz functions is a consequence of the associativity of convolution.

Furthermore, under certain conditions, convolution is the most general translation invariant operation. Informally speaking, the following holds


Thus any translation invariant operation can be represented as a convolution. Convolutions play an important role in the study of time-invariant systems, and especially LTI system theory. The representing function "g" is the impulse response of the transformation "S".

A more precise version of the theorem quoted above requires specifying the class of functions on which the convolution is defined, and also requires assuming in addition that "S" must be a continuous linear operator with respect to the appropriate topology. It is known, for instance, that every continuous translation invariant continuous linear operator on "L" is the convolution with a finite Borel measure. More generally, every continuous translation invariant continuous linear operator on "L" for 1 ≤ "p" < ∞ is the convolution with a tempered distribution whose Fourier transform is bounded. To wit, they are all given by bounded Fourier multipliers.

If "G" is a suitable group endowed with a measure λ, and if "f" and "g" are real or complex valued integrable functions on "G", then we can define their convolution by

It is not commutative in general. In typical cases of interest "G" is a locally compact Hausdorff topological group and λ is a (left-) Haar measure. In that case, unless "G" is unimodular, the convolution defined in this way is not the same as formula_56. The preference of one over the other is made so that convolution with a fixed function "g" commutes with left translation in the group:

Furthermore, the convention is also required for consistency with the definition of the convolution of measures given below. However, with a right instead of a left Haar measure, the latter integral is preferred over the former.

On locally compact abelian groups, a version of the convolution theorem holds: the Fourier transform of a convolution is the pointwise product of the Fourier transforms. The circle group T with the Lebesgue measure is an immediate example. For a fixed "g" in "L"(T), we have the following familiar operator acting on the Hilbert space "L"(T):

The operator "T" is compact. A direct calculation shows that its adjoint "T* " is convolution with

By the commutativity property cited above, "T" is normal: "T"* "T" = "TT"* . Also, "T" commutes with the translation operators. Consider the family "S" of operators consisting of all such convolutions and the translation operators. Then "S" is a commuting family of normal operators. According to spectral theory, there exists an orthonormal basis {"h"} that simultaneously diagonalizes "S". This characterizes convolutions on the circle. Specifically, we have

which are precisely the characters of T. Each convolution is a compact multiplication operator in this basis. This can be viewed as a version of the convolution theorem discussed above.

A discrete example is a finite cyclic group of order "n". Convolution operators are here represented by circulant matrices, and can be diagonalized by the discrete Fourier transform.

A similar result holds for compact groups (not necessarily abelian): the matrix coefficients of finite-dimensional unitary representations form an orthonormal basis in "L" by the Peter–Weyl theorem, and an analog of the convolution theorem continues to hold, along with many other aspects of harmonic analysis that depend on the Fourier transform.

Let "G" be a (multiplicativel written) topological group.
If μ and ν are finite Borel measures on "G", then their convolution μ∗ν is defined as the pushforward measure of the group action and can be written as

for each measurable subset "E" of "G". The convolution is also a finite measure, whose total variation satisfies

In the case when "G" is locally compact with (left-)Haar measure λ, and μ and ν are absolutely continuous with respect to a λ, so that each has a density function, then the convolution μ∗ν is also absolutely continuous, and its density function is just the convolution of the two separate density functions.

If μ and ν are probability measures on the topological group then the convolution μ∗ν is the probability distribution of the sum "X" + "Y" of two independent random variables "X" and "Y" whose respective distributions are μ and ν.

Let ("X", Δ, ∇, "ε", "η") be a bialgebra with comultiplication Δ, multiplication ∇, unit η, and counit ε. The convolution is a product defined on the endomorphism algebra End("X") as follows. Let φ, ψ ∈ End("X"), that is, φ,ψ : "X" → "X" are functions that respect all algebraic structure of "X", then the convolution φ∗ψ is defined as the composition

The convolution appears notably in the definition of Hopf algebras . A bialgebra is a Hopf algebra if and only if it has an antipode: an endomorphism "S" such that

Convolution and related operations are found in many applications in science, engineering and mathematics.





</doc>
<doc id="7521" url="https://en.wikipedia.org/wiki?curid=7521" title="Calico">
Calico

Calico (in British usage since 1505) is a plain-woven textile made from unbleached and often not fully processed cotton. It may contain unseparated husk parts, for example. The fabric is far less fine than muslin, but less coarse and thick than canvas or denim, but it is still very cheap owing to its unfinished and undyed appearance.

The fabric was originally from the city of Calicut in southwestern India. It was made by the traditional weavers called cāliyans. The raw fabric was dyed and printed in bright hues, and calico prints became popular in Europe.

Calico originated in Calicut, (from which the name of the textile came) in southwestern India (in present-day Kerala) during the 11th century. The cloth was known as "cāliyan" to the natives.

It was mentioned in Indian literature by the 12th century when the writer Hēmacandra described calico fabric prints with a lotus design. By the 15th century calico from Gujǎrāt made its appearance in Egypt. Trade with Europe followed from the 17th century onwards.

Calico was woven using Sūrat cotton for both the warp and weft.

In the 18th century, England was famous for its woollen and worsted cloth. That industry, centred in the east and south in towns such as Norwich, jealously protected their product. Cotton processing was tiny: in 1701 only of cottonwool was imported into England, and by 1730 this had fallen to . This was due to commercial legislation to protect the woollen industry. Cheap calico prints, imported by the East India Company from Hindustān (India), had become popular. In 1700 an Act of Parliament passed to prevent the importation of dyed or printed calicoes from India, China or Persia. This caused demand to switch to imported grey cloth instead—calico that had not been finished—dyed or printed. These were printed with popular patterns in southern England. Also, Lancashire businessmen produced grey cloth with linen warp and cotton weft, known as fustian, which they sent to London for finishing. Cottonwool imports recovered though, and by 1720 were almost back to their 1701 levels. Again the woollen manufacturers, in true protectionist fashion, claimed that the imports were taking jobs away from workers in Coventry. A new law passed, enacting fines against anyone caught wearing printed or stained calico muslins. Neckcloths and fustians were exempted. The Lancashire manufacturers exploited this exemption; coloured cotton weft with linen warp were specifically permitted by the 1736 Manchester Act. There now was an artificial demand for woven cloth.

In 1764, of cotton-wool were imported. This change in consumption patterns, as a result of the restriction on imported finished goods, was a key part of the process that reduced the Indian economy from sophisticated textile production to the mere supply of raw materials. These events occurred under colonial rule, which started after 1757, and were described by Nehru and also some more recent scholars as "de-industrialization."

Early Indian chintz, that is, glazed calico with a large floral pattern. were primarily produced by painting techniques. Later, the hues were applied by wooden blocks, and the cloth manufacturers in Britain printing calico used wooden block printing. Calico printers at work are depicted in one of the stained glass windows made by Stephen Adam for the Maryhill Burgh Halls, Glasgow. Confusingly, linen and silk printed this way were known as "linen calicoes" and "silk calicoes". Early European calicoes (1680) would be cheap plain-weave white cotton fabric with equal weft and warp plain weave cotton fabric in, or cream or unbleached cotton, with a design block-printed using a single alizarin dye fixed with two mordants, giving a red and black pattern. Polychromatic prints were possible, using two sets of blocks and an additional blue dye. The Indian taste was for dark printed backgrounds while the European market preferred a pattern on a cream base. As the century progressed the European preference moved from the large chintz patterns to smaller, tighter patterns.

Thomas Bell patented a printing technique in 1783 that used copper rollers, and Livesey, Hargreaves and Company put the first machine that used it into operation near Preston, Lancashire in 1785. The production volume for printed cloth in Lancashire in 1750 was estimated at 50,000 pieces of In 1850 it was 20,000,000 pieces. After 1888, block printing was only used for short-run specialized jobs. After 1880, profits from printing fell due to overcapacity and the firms started to form combines. In the first, three Scottish firms formed the United Turkey Red Co. Ltd in 1897, and the second, in 1899, was the much larger Calico Printers' Association 46 printing concerns and 13 merchants combined, representing 85% of the British printing capacity. Some of this capacity was removed and in 1901 Calico had 48% of the printing trade. In 1916, they and the other printers formed and joined a trade association, which then set minimum prices for each 'price section' of the industry.

The trade association remained in operation until 1954, when the arrangement was challenged by the government Monopolies Commission. Over the intervening period much trade had been lost overseas.

In the UK, Australia and New Zealand:

In the US:

Printed calico was imported into the United States from Lancashire in the 1780s, and here a linguistic separation occurred, while Europe maintained the word calico for the fabric, in the States it was used to refer to the printed design.

These colorful, small-patterned printed fabrics gave rise to the use of the word calico to describe a cat coat color: "calico cat". The patterned fabric also gave its name to two species of North American crabs; see the calico crab.




</doc>
<doc id="7522" url="https://en.wikipedia.org/wiki?curid=7522" title="Calorimetry">
Calorimetry

Calorimetry is the science or act of measuring changes in state variables of a body for the purpose of deriving the heat transfer associated with changes of its state due, for example, to chemical reactions, physical changes, or phase transitions under specified constraints. Calorimetry is performed with a calorimeter. The word "calorimetry" is derived from the Latin word "calor", meaning heat and the Greek word "μέτρον" (metron), meaning measure. Scottish physician and scientist Joseph Black, who was the first to recognize the distinction between heat and temperature, is said to be the founder of the science of calorimetry.

Indirect calorimetry calculates heat that living organisms produce by measuring either their production of carbon dioxide and nitrogen waste (frequently ammonia in aquatic organisms, or urea in terrestrial ones), or from their consumption of oxygen. 
Lavoisier noted in 1780 that heat production can be predicted from oxygen consumption this way, using multiple regression. The dynamic energy budget theory explains why this procedure is correct. Heat generated by living organisms may also be measured by "direct calorimetry", in which the entire organism is placed inside the calorimeter for the measurement.

A widely used modern instrument is the differential scanning calorimeter, a device which allows thermal data to be obtained on small amounts of material. It involves heating the sample at a controlled rate and recording the heat flow either into or from the specimen.

Calorimetry requires that a reference material that changes temperature have known definite thermal constitutive properties. The classical rule, recognized by Clausius and by Kelvin, is that the pressure exerted by the calorimetric material is fully and rapidly determined solely by its temperature and volume; this rule is for changes that do not involve phase change, such as melting of ice. There are many materials that do not comply with this rule, and for them, the present formula of classical calorimetry does not provide an adequate account. Here the classical rule is assumed to hold for the calorimetric material being used, and the propositions are mathematically written:

The thermal response of the calorimetric material is fully described by its pressure formula_1 as the value of its constitutive function formula_2 of just the volume formula_3 and the temperature formula_4. All increments are here required to be very small. This calculation refers to a domain of volume and temperature of the body in which no phase change occurs, and there is only one phase present. An important assumption here is continuity of property relations. A different analysis is needed for phase change

When a small increment of heat is gained by a calorimetric body, with small increments, formula_5 of its volume, and formula_6 of its temperature, the increment of heat, formula_7, gained by the body of calorimetric material, is given by

where

The latent heat with respect to volume is the heat required for unit increment in volume at constant temperature. It can be said to be 'measured along an isotherm', and the pressure the material exerts is allowed to vary freely, according to its constitutive law formula_18. For a given material, it can have a positive or negative sign or exceptionally it can be zero, and this can depend on the temperature, as it does for water about 4 C. The concept of latent heat with respect to volume was perhaps first recognized by Joseph Black in 1762. The term 'latent heat of expansion' is also used. The latent heat with respect to volume can also be called the 'latent energy with respect to volume'. For all of these usages of 'latent heat', a more systematic terminology uses 'latent heat capacity'.

The heat capacity at constant volume is the heat required for unit increment in temperature at constant volume. It can be said to be 'measured along an isochor', and again, the pressure the material exerts is allowed to vary freely. It always has a positive sign. This means that for an increase in the temperature of a body without change of its volume, heat must be supplied to it. This is consistent with common experience.

Quantities like formula_7 are sometimes called 'curve differentials', because they are measured along curves in the formula_20 surface.

Constant-volume calorimetry is calorimetry performed at a constant volume. This involves the use of a constant-volume calorimeter. Heat is still measured by the above-stated principle of calorimetry.

This means that in a suitably constructed calorimeter, called a bomb calorimeter, the increment of volume formula_5 can be made to vanish, formula_22. For constant-volume calorimetry:

where

From the above rule of calculation of heat with respect to volume, there follows one with respect to pressure.

In a process of small increments, formula_26 of its pressure, and formula_6 of its temperature, the increment of heat, formula_7, gained by the body of calorimetric material, is given by

where

The new quantities here are related to the previous ones:

where

and

The latent heats formula_9 and formula_30 are always of opposite sign.

It is common to refer to the ratio of specific heats as

An early calorimeter was that used by Laplace and Lavoisier, as shown in the figure above. It worked at constant temperature, and at atmospheric pressure. The latent heat involved was then not a latent heat with respect to volume or with respect to pressure, as in the above account for calorimetry without phase change. The latent heat involved in this calorimeter was with respect to phase change, naturally occurring at constant temperature. This kind of calorimeter worked by measurement of mass of water produced by the melting of ice, which is a phase change.

For a time-dependent process of heating of the calorimetric material, defined by a continuous joint progression formula_53 of formula_54 and formula_55, starting at time formula_56 and ending at time formula_57, there can be calculated an accumulated quantity of heat delivered, formula_58 . This calculation is done by mathematical integration along the progression with respect to time. This is because increments of heat are 'additive'; but this does not mean that heat is a conservative quantity. The idea that heat was a conservative quantity was invented by Lavoisier, and is called the 'caloric theory'; by the middle of the nineteenth century it was recognized as mistaken. Written with the symbol formula_59, the quantity formula_58 is not at all restricted to be an increment with very small values; this is in contrast with formula_7.

One can write

This expression uses quantities such as formula_65 which are defined in the section below headed 'Mathematical aspects of the above rules'.

The use of 'very small' quantities such as formula_7 is related to the physical requirement for the quantity formula_2 to be 'rapidly determined' by formula_3 and formula_4; such 'rapid determination' refers to a physical process. These 'very small' quantities are used in the Leibniz approach to the infinitesimal calculus. The Newton approach uses instead 'fluxions' such as formula_70, which makes it more obvious that formula_2 must be 'rapidly determined'.

In terms of fluxions, the above first rule of calculation can be written

where

The increment formula_7 and the fluxion formula_65 are obtained for a particular time formula_73 that determines the values of the quantities on the righthand sides of the above rules. But this is not a reason to expect that there should exist a mathematical function formula_82. For this reason, the increment formula_7 is said to be an 'imperfect differential' or an 'inexact differential'. Some books indicate this by writing formula_84 instead of formula_7. Also, the notation "đQ" is used in some books. Carelessness about this can lead to error.<ref name="Planck 1923/1926 57">Planck, M. (1923/1926), page 57.</ref>

The quantity formula_62 is properly said to be a functional of the continuous joint progression formula_53 of formula_54 and formula_55, but, in the mathematical definition of a function, formula_62 is not a function of formula_20. Although the fluxion formula_65 is defined here as a function of time formula_73, the symbols formula_94 and formula_82 respectively standing alone are not defined here.

The above rules refer only to suitable calorimetric materials. The terms 'rapidly' and 'very small' call for empirical physical checking of the domain of validity of the above rules.

The above rules for the calculation of heat belong to pure calorimetry. They make no reference to thermodynamics, and were mostly understood before the advent of thermodynamics. They are the basis of the 'thermo' contribution to thermodynamics. The 'dynamics' contribution is based on the idea of work, which is not used in the above rules of calculation.

Empirically, it is convenient to measure properties of calorimetric materials under experimentally controlled conditions.

For measurements at experimentally controlled volume, one can use the assumption, stated above, that the pressure of the body of calorimetric material is can be expressed as a function of its volume and temperature.

For measurement at constant experimentally controlled volume, the isochoric coefficient of pressure rise with temperature, is defined by

For measurements at experimentally controlled pressure, it is assumed that the volume formula_3 of the body of calorimetric material can be expressed as a function formula_98 of its temperature formula_4 and pressure formula_1. This assumption is related to, but is not the same as, the above used assumption that the pressure of the body of calorimetric material is known as a function of its volume and temperature; anomalous behaviour of materials can affect this relation.

The quantity that is conveniently measured at constant experimentally controlled pressure, the isobaric volume expansion coefficient, is defined by

For measurements at experimentally controlled temperature, it is again assumed that the volume formula_3 of the body of calorimetric material can be expressed as a function formula_98 of its temperature formula_4 and pressure formula_1, with the same provisos as mentioned just above.

The quantity that is conveniently measured at constant experimentally controlled temperature, the isothermal compressibility, is defined by

Assuming that the rule formula_18 is known, one can derive the function formula_108 that is used above in the classical heat calculation with respect to pressure. This function can be found experimentally from the coefficients formula_109 and formula_110 through the mathematically deducible relation

Thermodynamics developed gradually over the first half of the nineteenth century, building on the above theory of calorimetry which had been worked out before it, and on other discoveries. According to Gislason and Craig (2005): "Most thermodynamic data come from calorimetry..." According to Kondepudi (2008): "Calorimetry is widely used in present day laboratories."

In terms of thermodynamics, the internal energy formula_112 of the calorimetric material can be considered as the value of a function formula_113 of formula_20, with partial derivatives formula_115 and formula_116.

Then it can be shown that one can write a thermodynamic version of the above calorimetric rules:

with

and

Again, further in terms of thermodynamics, the internal energy formula_112 of the calorimetric material can sometimes, depending on the calorimetric material, be considered as the value of a function formula_121 of formula_122, with partial derivatives formula_123 and formula_116, and with formula_3 being expressible as the value of a function formula_126 of formula_122, with partial derivatives formula_128 and formula_129 .

Then, according to Adkins (1975), it can be shown that one can write a further thermodynamic version of the above calorimetric rules:

with

and

Beyond the calorimetric fact noted above that the latent heats formula_9 and formula_30 are always of opposite sign, it may be shown, using the thermodynamic concept of work, that also

Calorimetry has a special benefit for thermodynamics. It tells about the heat absorbed or emitted in the isothermal segment of a Carnot cycle.

A Carnot cycle is a special kind of cyclic process affecting a body composed of material suitable for use in a heat engine. Such a material is of the kind considered in calorimetry, as noted above, that exerts a pressure that is very rapidly determined just by temperature and volume. Such a body is said to change reversibly. A Carnot cycle consists of four successive stages or segments:

(3) another isothermal change in volume from formula_140 to a volume formula_142 at constant temperature formula_143 such as to incur a flow or heat out of the body and just such as to precisely prepare for the following change

(4) another adiabatic change of volume from formula_142 back to formula_136 just such as to return the body to its starting temperature formula_138.

In isothermal segment (1), the heat that flows into the body is given by

and in isothermal segment (3) the heat that flows out of the body is given by

Because the segments (2) and (4) are adiabats, no heat flows into or out of the body during them, and consequently the net heat supplied to the body during the cycle is given by

This quantity is used by thermodynamics and is related in a special way to the net work done by the body during the Carnot cycle. The net change of the body's internal energy during the Carnot cycle, formula_150, is equal to zero, because the material of the working body has the special properties noted above.

The quantity formula_9, the latent heat with respect to volume, belongs to classical calorimetry. It accounts for the occurrence of energy transfer by work in a process in which heat is also transferred; the quantity, however, was considered before the relation between heat and work transfers was clarified by the invention of thermodynamics. In the light of thermodynamics, the classical calorimetric quantity is revealed as being tightly linked to the calorimetric material's equation of state formula_18. Provided that the temperature formula_153 is measured in the thermodynamic absolute scale, the relation is expressed in the formula

Advanced thermodynamics provides the relation

From this, further mathematical and thermodynamic reasoning leads to another relation between classical calorimetric quantities. The difference of specific heats is given by

Constant-volume calorimetry is calorimetry performed at a constant volume. This involves the use of a constant-volume calorimeter.

No work is performed in constant-volume calorimetry, so the heat measured equals the change in internal energy of the system. The heat capacity at constant volume is assumed to be independent of temperature.

Heat is measured by the principle of calorimetry.

where 

In "constant-volume calorimetry" the pressure is not held constant. If there is a pressure difference between initial and final states, the heat measured needs adjustment to provide the "enthalpy change". One then has

where 



</doc>
<doc id="7525" url="https://en.wikipedia.org/wiki?curid=7525" title="Charles Evans Hughes">
Charles Evans Hughes

Charles Evans Hughes Sr. (April 11, 1862 – August 27, 1948) was an American statesman, Republican politician, and the 11th Chief Justice of the United States. He was also the 36th Governor of New York, the Republican presidential nominee in the 1916 presidential election, and the 44th United States Secretary of State.

Born to Welsh immigrants in New York, Hughes became a prominent attorney and academic. After taking part in the Armstrong Investigation, he won election as the Governor of New York, serving in that position from 1907–1910. He became known as a progressive reformer and an admirer of Britain's New Liberalism, enacting legislation such as the Moreland Act. In 1910, President William Howard Taft appointed Hughes as an Associate Justice of the Supreme Court of the United States.

Hughes served as an Associate Justice until 1916, when he resigned from the bench to accept the Republican presidential nomination. In the 1916 election, incumbent Democratic President Woodrow Wilson narrowly prevailed over Hughes, partly because Wilson won the support of many progressives. After Warren G. Harding won the 1920 presidential election, Hughes accepted Harding's offer to serve as Secretary of State. Serving under Harding and Calvin Coolidge, Hughes negotiated the Washington Naval Treaty, which sought to prevent a naval arms race.

After leaving office in 1925, Hughes returned to private legal practice, arguing numerous cases before the Supreme Court. Following the resignation of Chief Justice Taft in 1930, shortly before his death, President Herbert Hoover appointed Hughes to lead the Supreme Court. Hughes emerged as a swing voter on the bench, positioned between the liberal Three Musketeers and the conservative Four Horsemen. Though the Hughes Court struck down several New Deal programs, it upheld the broad constitutionality of the New Deal programs under the Taxing and Spending Clause. Behind the scenes, Hughes used his influence to help defeat the Judicial Procedures Reform Bill of 1937. Hughes retired in 1941 and died in 1948.

Charles Evans Hughes was born in Glens Falls, New York, the son of a Welsh immigrant minister, the Rev. David C. Hughes, and Mary C. (Connelly) Hughes, a sister of State Senator Henry C. Connelly (1832–1912). He was active in the Northern Baptist church, a Mainline Protestant denomination.

Hughes's early education included attending Lafayette School in Newark, New Jersey and the Thirteenth Street School in New York City. At the age of 14, he enrolled at Madison University (now Colgate University), where he became a member of Delta Upsilon fraternity. He then transferred to Brown University, continuing as a member of Delta Upsilon. He graduated third in his class at the age of 19, having been elected to Phi Beta Kappa in his junior year. He read law and entered Columbia Law School in 1882, where he graduated in 1884 with highest honors.

In 1885, Hughes met Antoinette Carter, the daughter of a senior partner of the law firm where he worked, and they were married in 1888. They had one son, Charles Evans Hughes Jr. and three daughters. Their youngest child, Elizabeth Hughes Gossett, was one of the first humans injected with insulin, and later served as president of the Supreme Court Historical Society. Hughes was the grandfather of Charles Evans Hughes III and H. Stuart Hughes.

After graduating Hughes began working for Chamberlain, Carter & Hornblower where he met his future wife. In 1888, shortly after he was married, he became a partner in the firm, and the name was changed to Carter, Hughes & Cravath. Later the name was changed to Hughes, Hubbard & Reed. In 1891, Hughes left the practice of law to become a professor at Cornell Law School. In 1893, he returned to his old law firm in New York City to continue practicing until he ran for governor in 1906. He continued his association with Cornell as a special lecturer at the Law School from 1893 to 1895. He was also a special lecturer for New York University Law School, 1893–1900.

At that time, in addition to practicing law, Hughes taught at New York Law School with Woodrow Wilson, who would later defeat him for the Presidency. In 1905, he was appointed as counsel to the New York state legislative "Stevens Gas Commission", a committee investigating utility rates. His uncovering of corruption led to lower gas rates in New York City. In 1905, he was appointed counsel to the Armstrong Insurance Commission to investigate the insurance industry in New York. He was then a special assistant to the U.S. Attorney General, responsible for investigating railroads with ownership stakes in coal mines to determine whether they warranted anti-trust prosecutions.

Hughes served as the Governor of New York from 1907 to 1910. He defeated William Randolph Hearst in the 1906 election, and was the only Republican statewide candidate to win office. An admirer of Britain's New Liberal philosophy, Hughes campaigned on a platform to improve the state of New York's standard of living by moving it away from laissez-faire tradition and enacting social reforms similar to that which had been enacted in Britain. As a supporter of progressive policies, Hughes was able to play on the popularity of Theodore Roosevelt and weaken the power of the state's conservative Republican officials. In 1908, he was offered the vice-presidential nomination by William Howard Taft, but he declined it to run again for governor. Theodore Roosevelt became an important supporter of Hughes.

As the Governor, Hughes produced important reform legislation in three areas: improvement of the machinery and processes of government; extension of the state's regulatory authority over businesses engaged in public services; and expansion of governmental police and welfare functions. To counter political corruption, he secured campaign laws in 1906 and 1907 that limited political contributions by corporations and forced candidates to account for their receipts and expenses, legislation that was quickly copied in fifteen other states. He pushed the passage of the Moreland Act, which enabled the governor to oversee city and county officials as well as officials in semi-autonomous state bureaucracies. This allowed him to fire many corrupt officials. He also managed to have the powers of the state's Public Service Commissions increased and fought strenuously, if not completely successfully, to get their decisions exempted from judicial review.

When two bills were passed to reduce railroad fares, Hughes vetoed them on the grounds that the rates should be set by expert commissioners rather than by elected ones. His ideal was not government by the people but for the people. As Hughes put it, "you must have administration by administrative officers."

Hughes, however, would be unsuccessful in achieving one of his main goals as governor: primary voting reform. Hoping to achieve a compromise with the state's party bosses, Hughes rejected the option of a direct primary in which voters could choose between declared candidates and instead proposed a complicated system of nominations by party committees. The state's party bosses, however, rejected this compromise and the state legislature rejected the plan on three occasions in 1909 and 1910.

On social issues, Hughes strongly supported relatively limited social reforms. He endorsed the Page-Prentice Act of 1907, which set an eight-hour day and forty-eight-hour week for factory workers—but only for those under the age of sixteen. By employing the well-established legal distinction between ordinary and hazardous work, the governor also won legislative approval for a Dangerous Trades Act that barred young workers from thirty occupations. To enforce these and other regulations, in 1907 Hughes reorganized the Department of Labor and appointed a well-qualified commissioner. Two years later, the governor created a new bureau for immigrant issues in the Department of Labor and appointed reformer Frances Kellor to head it.

In his final year as the Governor, he had the state comptroller draw up an executive budget. This began a rationalization of state government and eventually it led to an enhancement of executive authority. He also signed the Worker's Compensation Act of 1910, which required a compulsory, employer-paid plan of compensation for workers injured in hazardous industries and a voluntary system for other workers; after the New York Court of Appeals ruled the law unconstitutional in 1911, a popular referendum was held that successfully made the law an amendment in the New York Constitution.

In 1908, Governor Hughes reviewed the clemency petition of Chester Gillette concerning the murder of Grace Brown. The governor denied the petition as well as an application for reprieve, and Gillette was electrocuted in March of that year.

When Hughes left office, a prominent journal remarked "One can distinctly see the coming of a New Statism ... [of which] Gov. Hughes has been a leading prophet and exponent". In 1926, Hughes was appointed by New York Governor Alfred E. Smith to be the chairman of a "State Reorganization Commission" through which Smith's plan to place the Governor as the head of a rationalized state government, was accomplished, bringing to realization what Hughes himself had envisioned.

In 1909, Hughes led an effort to incorporate Delta Upsilon fraternity. This was the first fraternity to incorporate, and he served as its first international president.

On April 25, 1910, President William H. Taft nominated Hughes for Associate Justice to fill the vacancy left by the death of Justice David J. Brewer. The Senate confirmed the nomination on May 2, 1910, and Hughes received his commission the same day. As an associate justice of the Supreme Court from 1910 to 1916, Hughes remained an advocate of regulation and authored decisions that weakened the legal foundations of laissez-faire capitalism. He also mastered a new set of issues regarding the Commerce Clause and, in a deliberately restrained manner, wrote constitutional decisions that expanded the regulatory powers of both the state and federal governments.

He wrote for the court in "Bailey v. Alabama" , which held that involuntary servitude encompassed more than just slavery, and "Interstate Commerce Comm. v. Atchison T & SF R Co." , holding that the Interstate Commerce Commission could regulate intrastate rates if they were significantly intertwined with interstate commerce.

On April 15, 1915, in the case of "Frank v. Mangum", the Supreme Court decided (7–2) to deny an appeal made by Leo Frank's attorneys, and instead upheld the decision of lower courts to sustain the guilty verdict against Frank. Justice Hughes and Justice Oliver Wendell Holmes Jr. were the two dissenting votes.

Hughes resigned from the Supreme Court on June 10, 1916, to be the Republican candidate for President in 1916. He is the last sitting Supreme Court justice to surrender his or her seat to run for elected office. He was also endorsed by the Progressive Party, thanks to the support given to him from former President Theodore Roosevelt. Other Republican figures such as former President William Howard Taft endorsed Hughes and felt the accomplishments he made as Governor of New York would establish him as formidable progressive alternative to Wilson. Many former leaders of the Progressive Party, however, endorsed Wilson because Hughes opposed the Adamson Act, the Sixteenth Amendment and diverted his focus away from progressive issues during the course of the campaign. Hughes was defeated by Woodrow Wilson in a close election (separated by 23 electoral votes and 594,188 popular votes). The election hinged on California, where Wilson managed to win by 3,800 votes and its 13 electoral votes and thus was returned for a second term; Hughes had lost the endorsement of the California governor and Roosevelt's 1912 Progressive running mate Hiram Johnson when he failed to show up for an appointment with him.

Despite coming close to winning the presidency, Hughes did not seek the Republican nomination again in 1920. Hughes also advocated ways to prevent the return of President Wilson's expanded government control over important industries such as the nation's railroads, which he felt would lead to the eventual destruction of individualism and political self-rule. After Robert LaFollette's Progressive Party advocated the return of such regulations during the 1924 US Presidential election, Hughes shifted rightwards, believing that the federal bureaucracy should now have limited powers over individual liberties and property rights and that common law should be strictly enforced.

Hughes returned to government office in March 1921 as Secretary of State under President Harding. On November 11, 1921, Armistice Day (later changed to Veterans Day), the Washington Naval Conference for the limitation of naval armament among the Great Powers began. The major naval powers of Britain, France, Italy, Japan and the United States were in attendance as well as other nations with concerns about territories in the Pacific—Belgium, the Netherlands, Portugal and China.

The American delegation was headed by Hughes and included Elihu Root, Henry Cabot Lodge, and Oscar Underwood, the Democratic minority leader in the Senate. The conference continued until February 1922 and included the Four-Power Treaty (December 13, 1921), Shantung Treaty (February 4, 1922), Five-Power Treaty, the Nine-Power Treaty (February 6, 1922), the "Six-power pact" that was an agreement between the Big Five Nations plus China to divide the German cable routes in the Pacific, and the Yap Island agreement.

Hughes continued in office after Harding died and was succeeded by Coolidge, but resigned after Coolidge was elected to a full term. On June 30, 1922, he signed the "Hughes–Peynado agreement" that ended the United States's six-year occupation of Dominican Republic.

After leaving the State Department, he again rejoined his old partners at the Hughes firm, which included his son and future United States Solicitor General Charles E. Hughes Jr., and was one of the nation's most sought-after advocates. From 1925 to 1930, for example, Hughes argued over 50 times before the U.S. Supreme Court. From 1926 to 1930, Hughes also served as a member of the Permanent Court of Arbitration and as a judge of the Permanent Court of International Justice in The Hague, Netherlands from 1928 to 1930. He was additionally a delegate to the Pan American Conference on Arbitration and Conciliation from 1928 to 1930. He was one of the co-founders in 1927 of the National Conference on Christians and Jews, now known as the National Conference for Community and Justice (NCCJ), along with S. Parkes Cadman and others, to oppose the Ku Klux Klan, anti-Catholicism, and anti-Semitism in the 1920s and 1930s.

From 1925 to 1926, Charles Evans Hughes represented the API (American Petroleum Institute) before the FOCB (Federal Oil Conservation Board).

In 1928, conservative business interests tried to interest Hughes in the Republican presidential nomination of 1928 instead of Herbert Hoover. Hughes, citing his age, turned down the offer.

Herbert Hoover, who had appointed Hughes's son as Solicitor General in 1929, appointed Hughes Chief Justice of the United States on February 3, 1930. Hughes was confirmed by the United States Senate on February 13, 1930, and received commission the same day, serving in this capacity until 1941. Hughes replaced former President William Howard Taft, a fellow Republican who had also lost a presidential election to Woodrow Wilson (in 1912) and who, in 1910, had appointed Hughes to his first tenure on the Supreme Court.

Hughes's appointment was opposed by progressive elements in both parties who felt that he was too friendly to big business. Idaho Republican William E. Borah said on the United States Senate floor that confirming Hughes would constitute "placing upon the Court as Chief Justice one whose views are known upon these vital and important questions and whose views, in my opinion, however sincerely entertained, are not which ought to be incorporated in and made a permanent part of our legal and economic system." In addition to his politics, at 67, Hughes was the oldest man ever nominated as Chief Justice. Nonetheless Hughes was confirmed as Chief Justice with a vote of 52 to 26.

Hughes as Chief Justice swore in President Franklin D. Roosevelt in 1933, 1937 and 1941.

Upon his return to the court, more progressives had joined the bench. Hughes seemed determined again to vote progressive and soon bring an end to the longstanding pro-business Lochner era. During his early years as Chief Justice, however, the fear he had developed for an overblown bureaucracy during World War I undermined his optimism. Showing his old progressive image, he upheld legislation protecting civil rights and civil liberties and wrote the opinion for the Court in "Near v. Minnesota" , which held prior restraint against the press is unconstitutional. Concerning economic regulation, he was still willing to uphold legislation that supported "freedom of opportunity" for individuals on the one hand and the "police power" of the state on the other but did not personally favor legislation that linked national economic planning and bureaucratic social welfare together. At first resisting Roosevelt's New Deal and building a consensus of centrist members of the court, Hughes used his influence to limit the collectivist scope of Roosevelt's changes and would often strike down New Deal legislation he felt was poorly drafted and did not clearly specify how they were constitutional. By 1935, Hughes felt the court's four conservative Justices had disregarded common law and sought to curb their power.

Hughes was often aligned with the court's three liberal Justices—Louis Brandeis, Harlan Fiske Stone, and Benjamin Cardozo—in finding some New Deal measures (such as the violation of the gold clauses in contracts and the confiscation of privately owned monetary gold) constitutional. On one occasion, Hughes would side with the conservatives in striking down the New Deal's Agricultural Adjustment Act in the 1936 case "United States v. Butler", which held that the law was unconstitutional because its so-called tax policy was a coercive regulation rather than a tax measure and the federal government lacked authority to regulate agriculture. But surprisingly he did not assign the majority opinion, a practice usually required for court's most senior justice who agrees with the majority opinion, and allowed Associate Justice Owen Roberts to speak for the entire majority in his own words. It was accepted that he did not agree with the argument that the federal government lacked authority over agriculture and was going to write a separate opinion upholding the act's regulation policy while striking down the act's taxation policy on the grounds that it was a coercive regulation rather than a tax measure. However, Roberts convinced Hughes that he would side with him and the three liberal justices in future cases pertaining to the nation's agriculture that involved the Constitution's General Welfare Clause if he agreed to join his opinion.

By 1936, Hughes sensed the growing hostility in the court and could do little about it. In the 1936 case "Carter v. Carter Coal Company", Hughes took a middle ground for doctrinal and court-management reasons. Writing his own opinion, he joined the three liberal justices in upholding the Bituminous Coal Conservation Act's marketing provision but sided with Roberts and the four conservatives in striking down the act's provision that regulated local labor. By 1937, as the court leaned more in his favor, Hughes would renounce the position he took in the "Carter" case regarding local labor and ruled that the procedural methods that governed the Wagner Act's labor regulation provisions bore resemblance to the procedural methods which governed the railroad rates that the Interstate Commerce Commission was allowed to maintain in the 1914 "Shreveport" decision; he thus demonstrated that Congress could use its commerce power to regulate local industrial labor as well.

In 1937, when Roosevelt attempted to pack the Court with six additional justices, Hughes worked behind the scenes to defeat the effort, which failed in the Senate, by rushing important New Deal legislation—such as Wagner Act and the Social Security Act—through the court and ensuring that the court's majority would uphold their constitutionality. The month after Roosevelt's court-packing announcement, Roberts, who had joined the four conservative Justices in striking down important New Deal legislation, sided with Hughes and the court's three liberal justices in striking down the court's ruling in the 1923 "Adkins v. Children's Hospital" case—which held that laws requiring minimum wage violated the Fifth Amendment's due process clause—and upholding the constitutionality of Washington state's minimum wage law in "West Coast Hotel Co. v. Parrish". Because Roberts had previously sided with the four conservative justices and used the "Adkins" decision as the basis for striking down a similar minimum wage law the state of New York enforced in "Morehead v. New York ex rel. Tipaldo", it was widely perceived that he only agreed to uphold the constitutionality of minimum wage as a result of the pressure that was put on the Supreme Court by the court-packing plan. However, Hughes and Roberts acknowledged that the Chief Justice had already convinced Roberts to change his method of voting months before Roosevelt announced his court-packing plan and that the effort he put into defeating the plan played only a small significance in determining how the court's majority made their decisions in future cases pertaining to New Deal legislation.

Following the overwhelming support that voters showed for the New Deal through Roosevelt's overwhelming re-election in November 1936, Hughes was not able to persuade Roberts to base his votes on political maneuvering and to side with him in future cases regarding New Deal-related policies. Roberts had voted to grant "certiorari" to hear the "Parrish" case before the election of 1936. Oral arguments occurred on December 16 and 17, 1936, with counsel for Parrish specifically asking the court to reconsider its decision in "Adkins v. Children's Hospital", which had been the basis for striking down a New York minimum wage law in "Morehead v. New York ex rel. Tipaldo" in the late spring of 1936.

Roberts indicated his desire to overturn "Adkins" immediately after oral arguments ended for the Parrish case on December 17, 1936. The initial conference vote on December 19, 1936, was 4-4; with this even division on the Court, the holding of the Washington Supreme Court, finding the minimum wage statute constitutional, would stand. The eight voting justices anticipated Justice Stone—absent due to illness—would be the fifth vote necessary for a majority opinion affirming the constitutionality of the minimum wage law. As Hughes desired a clear and strong 5-4 affirmation of the Washington Supreme Court's judgment, rather than a 4-4 default affirmation, he convinced the other justices to wait until Stone's return before deciding and announcing the case. In one of his notes from 1936, Hughes wrote that Roosevelt's re-election forced the court to depart from its "fortress in public opinion" and severely weakened its capability to base its rulings on personal or political beliefs.

President Roosevelt announced his court reform bill on February 5, 1937, the day of the first conference vote after Stone's February 1, 1937, return to the bench. Roosevelt later made his justifications for the bill to the public on March 9, 1937, during his ninth Fireside Chat. The Court's opinion in "Parrish" was not published until March 29, 1937, after Roosevelt's radio address. Hughes wrote in his autobiographical notes that Roosevelt's court reform proposal "had not the slightest effect on our [the court's] decision," but due to the delayed announcement of its decision the Court was characterized as retreating under fire.

Although Hughes wrote the opinion invalidating the National Recovery Administration in "Schechter Poultry Corp. v. United States"—though the decision was unanimously upheld by all of the court's Justices—he also wrote the opinions for the Court in "NLRB v. Jones & Laughlin Steel Corp.", "NLRB v. Friedman-Harry Marks Clothing Co." and "West Coast Hotel Co. v. Parrish" which approved some New Deal measures. Hughes supervised the move of the Court from its former quarters at the U.S. Capitol to the newly constructed Supreme Court building.

Hughes wrote 199 majority opinions in his time on the bench, from 1930 to 1941. "His opinions, in the view of one commentator, were concise and admirable, placing Hughes in the pantheon of great justices." His "remarkable intellectual and social gifts...made him a superb leader and administrator. He had a photographic memory that few, if any, of his colleagues could match. Yet he was generous, kind, and forebearing in an institution where egos generally come in only one size: extra large!"

On August 27, 1948, at the age of 86, Hughes died in what is now the Tiffany Cottage of the Wianno Club in Osterville, Massachusetts. He is interred at Woodlawn Cemetery in The Bronx, New York City.

For many years, he was a member of the Union League Club of New York and served as its president from 1917 to 1919.

In 1907, Hughes, then the Governor of New York, was elected to honorary membership in the Empire State Society of the Sons of the American Revolution. He was assigned national membership number 18,977.

In 1907, Gov. Charles Evans Hughes became the first president of the newly formed Northern Baptist Convention—based at Calvary Baptist Church in Washington, DC, of which Hughes was a member. He also served as President of the New York State Bar Association.

Citations
Bibliography


Archives

Legal opinions as Chief Justice

Books


</doc>
<doc id="7527" url="https://en.wikipedia.org/wiki?curid=7527" title="Concept album">
Concept album

A concept album is an album in which its tracks hold a larger purpose or meaning collectively than they do individually. This is typically achieved through a single central narrative or theme, which can be instrumental, compositional, or lyrical. Sometimes the term is referenced to albums considered to be of "uniform excellence" rather than an LP with an explicit musical or lyrical motif. The exact criterion for a "concept album" varies among critics, with no discernible consensus.

The format originates with folk singer Woody Guthrie's "Dust Bowl Ballads" (1940) and was subsequently popularized by traditional pop singer Frank Sinatra's 1940s–50s string of albums, although the term is more often associated with rock music. In the 1960s, several well-regarded concept albums were released by various rock bands, which eventually led to the invention of progressive rock and rock opera. Since then, many concept albums have been released across numerous musical genres.

Author Martina Elicker defines concepts as general ideas, thoughts, or abstract notions. There is no clear definition of what constitutes a "concept album". Fiona Sturges of "The Independent" stated that the concept album "was originally defined as a long-player where the songs were based on one dramatic idea – but the term is subjective." A precursor to this type of album can be found in the 19th century song cycle which ran into similar difficulties in classification. The extremely broad definitions of a "concept album" could potentially encompass all soundtracks, compilations, cast recordings, greatest hits albums, tribute albums, Christmas albums, and live albums.

The most common definitions refer to an expanded approach to a rock album (as a story, play, or opus), or a project that either revolves around a specific theme or a collection of related materials. AllMusic writes, "A concept album could be a collection of songs by an individual songwriter or a particular theme — these are the concept LPs that reigned in the '50s ... the phrase 'concept album' is inextricably tied to the late 1960s, when rock & rollers began stretching the limits of their art form."<ref name="AllMusic/Concept albums"></ref> Author Jim Cullen describes it as "a collection of discrete but thematically unified songs whose whole is greater than the sum of its parts ... sometimes [erroneously] assumed to be a product of the rock era." Author Roy Shuker defines concept albums and rock operas as albums that are "unified by a theme, which can be instrumental, compositional, narrative, or lyrical. ... In this form, the album changed from a collection of heterogeneous songs into a narrative work with a single theme, in which individual songs segue into one another."

Rick Wakeman, keyboardist from the band Yes, considers the first concept album to be Woody Guthrie's 1940 album "Dust Bowl Ballads". "The Independent" regards it as "perhaps" one of the first concept albums, consisting exclusively of semi-autobiographical songs about the hardships of American migrant labourers during the 1930s.

In the late 1940s, the LP record was introduced, with space age pop composers producing concept albums soon after. Themes included exploring wild life and dealing with emotions, with some albums meant to be played while dining or relaxing. This was accompanied in the mid 1950s with the invention of the gatefold, which allowed room for liner notes to explain the concept.

Singer Frank Sinatra recorded several concept albums prior to the 1960s rock era, including "In the Wee Small Hours" (1955) and "Frank Sinatra Sings for Only the Lonely" (1958). Sinatra is occasionally credited as the inventor of the concept album, beginning with "The Voice of Frank Sinatra" (1946), which led to similar work by Bing Crosby. According to biographer Will Friedwald, Sinatra "sequenced the songs so that the lyrics created a flow from track to track, affording an impression of a narrative, as in musical comedy or opera. ... [He was the] first pop singer to bring a consciously artistic attitude to recording."

In the context of 1960s popular music, albums became closely aligned with countercultural ideology, resulting in a recognised "album era" and the introduction of the rock concept album. The author Carys Wyn Jones writes that the Beach Boys' "Pet Sounds" (1966), the Beatles' "Revolver" (1966) and "Sgt. Pepper's Lonely Hearts Club Band" (1967), and the Who's "Tommy" (1969) are variously cited as "the first concept album", usually for their "uniform excellence rather than some lyrical theme or underlying musical motif". Commentators and historians frequently cite "Pet Sounds" as the first concept album of rock music, although "popular consensus", according to AllMusic, favours "Sgt. Pepper". According to music critic Tim Riley, "Strictly speaking, the Mothers of Invention's "Freak Out!" [1966] has claims as the first 'concept album', but "Sgt. Pepper" was the record that made that idea convincing to most ears." Musicologist Allan Moore says that "Even though previous albums had set a unified mood, it was on the basis of the influence of "Sgt. Pepper" that the penchant for the concept album was born." Adding to "Sgt. Pepper"s claim, the artwork reinforced its central theme by depicting the four Beatles in uniform as members of the Sgt. Pepper band, while the record omitted the gaps that usually separated album tracks.

Selected observers claim for other records as "early" or "first" concept albums. In the early 1960s, concept albums began featuring highly in American country music, however the fact has been largely unacknowledged by rock/pop fans and critics who would only begin noting "concept albums" as a phenomenon nearly ten years later. "The 100 Greatest Bands of All Time" (2015) states that the Ventures "pioneered the idea of the rock concept album years before the genre is generally acknowledged to have been born". Another is the Beach Boys' "Little Deuce Coupe" (1963). Writing in "101 Albums That Changed Popular Music", Chris Smith noted: "Though albums such as Frank Sinatra's 1955 "In the Wee Small Hours" and Marty Robbins' 1959 "Gunfighter Ballads and Trail Songs" had already introduced concept albums, "Little Deuce Coupe" was the first to comprise almost all original material rather than standard covers." Writing in his "Concise Dictionary of Popular Culture", Marcel Danesi identifies the Beatles' "Rubber Soul" (1965) and the Who's "The Who Sell Out" (1967) as other examples of early concept albums. Brian Boyd of "The Irish Times" names the Kinks' "Face to Face" (1966) as the first concept album: "Written entirely by Ray Davies, the songs were supposed to be linked by pieces of music, so that the album would play without gaps, but the record company baulked at such radicalism. It’s not one of the band’s finest works, but it did have an impact."

Author Bill Martin relates the assumed concept albums of the 1960s to progressive rock:

"Popmatters" Sarah Zupko notes that while the Who's "Tommy" is "popularly thought of as the first rock opera, an extra-long concept album with characters, a consistent storyline, and a slight bit of pomposity", it is preceded by the shorter concept albums "Ogdens' Nut Gone Flake" (Small Faces, 1968) and "S.F. Sorrow" (The Pretty Things, 1968). Author Jim Cullen states: "The concept album reached its apogee in the 1970s in ambitious records like Pink Floyd's "Dark Side of the Moon" (1973) and the Eagles' "Hotel California" (1976)." In 2015, "Rolling Stone" ranked "Dark Side of the Moon" at number one among the 50 greatest progressive rock albums of all-time, also noting the LP's stature as the second best-selling album of all time. Pink Floyd's "The Wall" (1979), a semi-autobiographical story modeled after the band's Roger Waters and Syd Barrett, is one of the most famous concept albums by any artist. In addition to "The Wall", Danesi highlights Genesis' "The Lamb Lies Down on Broadway" (1974) and Zappa's "Joe's Garage" (1979) as other culturally significant concept albums.

According to author Edward Macan, concept albums as a recurrent theme in progressive rock was directly inspired by the counterculture associated with "the proto-progressive bands of the 1960s", observing: "the consistent use of lengthy forms such as the programmatic song cycle of the concept album and the multimovement suite underscores the hippies' new, drug-induced conception of time."

With the emergence of MTV as a music video network which valued singles over albums, concept albums became less dominant in the 1980s. Some artists, however, still released concept albums and experienced success in the 1990s and 2000s. "NME"s Emily Barker cites Green Day's "American Idiot" (2004) as one of the "more notable" examples, having brought the concept album back to high-charting positions. Dorian Lynskey, writing for "GQ", noted a resurgence of concept albums in the 2010s due to streaming: "This is happening not in spite of the rise of streaming and playlists, but because of it. Threatened with redundancy in the digital era, albums have fought back by becoming more album-like." Cucchiara argues that "concept albums" should also describe "this new generation of concept albums, for one key reason. This is because the unison between the songs on a particular album has now been expanded into a broader field of visual and artistic design and marketing strategies that play into the themes and stories that form the album."


Notes
Citations
Bibliography


</doc>
<doc id="7530" url="https://en.wikipedia.org/wiki?curid=7530" title="Cro-hook">
Cro-hook

The cro-hook, is a special double-ended crochet hook used to make double-sided crochet. 
It employs the use of a long double-ended hook, which permits the maker to work stitches on or off from either end. Because the hook has two ends, two alternating colors of thread can be used simultaneously and freely interchanged, working loops over the hook. Crafts using a double-ended hook are commercially marketed as Cro-hook and Crochenit. Cro-hook is a variation of Tunisian crochet and also shows similarities with the Afghan stitch used to make Afghan scarves, but the fabric is typically softer with greater elasticity.



</doc>
<doc id="7531" url="https://en.wikipedia.org/wiki?curid=7531" title="Clavichord">
Clavichord

The clavichord is a European stringed keyboard instrument that was used largely in the late Medieval, through the Renaissance, Baroque and Classical eras. Historically, it was mostly used as a practice instrument and as an aid to composition, not being loud enough for larger performances. The clavichord produces sound by striking brass or iron strings with small metal blades called tangents. Vibrations are transmitted through the bridge(s) to the soundboard.

The name is derived from the Latin word "clavis", meaning "key" (associated with more common "clavus", meaning "nail, rod, etc.") and "chorda" (from Greek χορδή) meaning "string, especially of a musical instrument". An analogous name is used in other European languages (It. "clavicordio", "clavicordo"; Fr. "clavicorde"; Germ. "Klavichord"; Lat. "clavicordium"; Port. "clavicórdio"; Sp. "clavicordio"). Many languages also have another name derived from Latin "manus", meaning "hand" (It. "manicordo"; Fr. "manicorde", "manicordion"; Sp. "manicordio", "manucordio"). Other names refer to the monochord-like nature of a fully fretted clavichord (It. "monacordo" or "monocordo"; Sp. "monacordio"). Italian also used "sordino", a reference to its quiet sound (sordino usually designates a mute).

The clavichord was invented in the early fourteenth century. In 1404, the German poem "" mentions the terms "clavicimbalum" (a term used mainly for the harpsichord) and "clavichordium", designating them as the best instruments to accompany melodies.

One of the earliest references to the clavichord in England occurs in the privy-purse expenses of Elizabeth of York, queen of Henry VII, in an entry dated August 1502:
Item. The same day, Hugh Denys for money by him delivered to a stranger that gave the queen a payre of clavycordes. In crowns form his reward iiii libres.

The clavichord was very popular from the 16th century to the 18th century, but mainly flourished in German-speaking lands, Scandinavia, and the Iberian Peninsula in the latter part of this period. It had fallen out of use by 1850. In the late 1890s, Arnold Dolmetsch revived clavichord construction and Violet Gordon-Woodhouse, among others, helped to popularize the instrument. Although most of the instruments built before the 1730s were small (four octaves, four feet long), the latest instruments were built up to seven feet long with a six octave range.

Today clavichords are played primarily by Renaissance, Baroque, and Classical music enthusiasts. They attract many interested buyers, and are manufactured worldwide. There are now numerous clavichord societies around the world, and some 400 recordings of the instrument have been made in the past 70 years. Leading modern exponents of the instrument have included Christopher Hogwood and Thurston Dart.

The clavichord has also gained attention in other genres of music, in the form of the Clavinet, which is essentially an electric clavichord that uses a magnetic pickup to produce a signal for amplification. Stevie Wonder uses a Clavinet in many of his songs, such as "Superstition" and "Higher Ground". A Clavinet played through an instrument amplifier with guitar effect pedals is often associated with funky, disco-infused 1970s rock.

Guy Sigsworth has played clavichord in a modern setting with Björk, notably on the studio recording of "All Is Full of Love". Björk also made extensive use of and even played the instrument herself on the song "My Juvenile" of her 2007 album "Volta".

Tori Amos uses the instrument on "Little Amsterdam" from the album "Boys for Pele" and on the song "Smokey Joe" from her 2007 album "American Doll Posse". Amos also featured her use of the Clavinet on her 2004 recording "Not David Bowie", released as part of her 2006 box set, "".

In 1976 Oscar Peterson played (with Joe Pass on acoustic guitar) songs from "Porgy And Bess" on the clavichord. Keith Jarrett also recorded an album entitled "Book of Ways" (1986) in which he plays a series of clavichord improvisations. The Beatles' "For No One" (1966) features Paul McCartney playing the clavichord. Rick Wakeman plays the Clavinet in the track "The Battle" from the album "Journey to the Centre of the Earth".

In the clavichord, strings run transversely from the hitchpin rail at the left-hand end to tuning pegs on the right. Towards the right end they pass over a curved wooden bridge. The action is simple, with the keys being levers with a small brass tangent, a small piece of metal similar in shape and size to the head of a flat-bladed screwdriver, at the far end. The strings, which are usually of brass, or else a combination of brass and iron, are usually arranged in pairs, like a lute or mandolin. When the key is pressed, the tangent strikes the strings above, causing them to sound in a similar fashion to the "hammering" technique on a guitar. Unlike in a piano action, the tangent does not rebound from the string; rather, it stays in contact with the string as long as the key is held, acting as both the nut and as the initiator of sound. The volume of the note can be changed by striking harder or softer, and the pitch can also be affected by varying the force of the tangent against the string (known as "Bebung"). When the key is released, the tangent loses contact with the string and the vibration of the string is silenced by strips of damping cloth.

The action of the clavichord is unique among all keyboard instruments in that one part of the action simultaneously initiates the sound vibration while at the same time defining the endpoint of the vibrating string, and thus its pitch. Because of this intimate contact between the player's hand and the production of sound, the clavichord has been referred to as the most intimate of keyboard instruments. Despite its many (serious) limitations, including extremely low volume, it has considerable expressive power, the player being able to control attack, duration, volume, and even provide certain subtle effects of swelling of tone and a type of vibrato unique to the clavichord.

Since the string vibrates from the bridge only as far as the tangent, multiple keys with multiple tangents can be assigned to the same string. This is called "fretting". Early clavichords frequently had many notes played on each string, even going so far as the keyed monochord — an instrument with only one string—though most clavichords were triple- or double-fretted. Since only one note can be played at a time on each string, the fretting pattern is generally chosen so that notes rarely heard together (such as C and C) share a string pair. The advantages of this system compared with unfretted instruments (see below) include relative ease of tuning (with around half as many strings to keep in tune), greater volume (though still not really enough for use in chamber music), and a clearer, more direct sound. Among the disadvantages: temperament could not be re-set without bending the tangents; and playing required a further refinement of touch, since notes sharing a single string played in quick succession had to be slightly separated to avoid a disagreeable deadening of the sound, potentially disturbing a legato line.

Some clavichords have been built with a single pair of strings for each note. The first known reference to one was by Johann Speth in 1693 and the earliest such extant signed and dated clavichord was built in 1716 by Johann Michael Heinitz. Such instruments are referred to as "unfretted" whereas instruments using the same strings for several notes are called "fretted". Among the advantages to unfretted instruments are flexibility in tuning (the temperament can be easily altered) and the ability to play any music exactly as written without concern for "bad" notes. Disadvantages include a smaller volume, even though many or most unfretted instruments tend to be significantly larger than fretted instruments; and "many" more strings to keep in tune. Unfretted instruments tend to have a sweeter, less incisive tone due to the greater load on the bridge resulting from the greater number of strings, though the large, late (early 19th century) Swedish clavichords tend to be the loudest of any of the historic clavichords.

While clavichords were typically single manual instruments, they could be stacked, one clavichord on top of another, to provide multiple keyboards. With the addition of a pedal clavichord, which included a pedal keyboard for the lower notes, a clavichord could be used to practice organ repertoire. Most often, the addition of a pedal keyboard only involved connecting the keys of the pedalboard to the lower notes on the manual clavichord using string so the lower notes on the manual instrument could be operated by the feet. In the era of pipe organs, which used man-powered bellows that required several people to operate, and of churches only heated during church services if at all, organists used pedal harpsichords and pedal clavichords as practice instruments (see also: pedal piano). There is speculation that some works written for organ may have been intended for pedal clavichord. An interesting case is made by that Bach's "Eight Little Preludes and Fugues", now thought spurious, may actually be authentic. The keyboard writing seems unsuited to organ, but Speerstra argues that they are idiomatic on the pedal clavichord. As Speerstra and also note, the compass of the keyboard parts of Bach's six trio sonatas for organ (BWV 525–530) rarely go below the tenor C, so they could have been played on a single manual pedal clavichord, by moving the left hand down an octave, a customary practice in the 18th century.

Much of the musical repertoire written for harpsichord and organ from the period circa 1400–1800 can be played on the clavichord; however, it does not have enough (unamplified) volume to participate in chamber music, with the possible exception of providing accompaniment to a soft baroque flute, recorder, or single singer. J. S. Bach's son Carl Philipp Emanuel Bach was a great proponent of the instrument, and most of his German contemporaries regarded it as a central keyboard instrument, for performing, teaching, composing and practicing. The fretting of a clavichord provides new problems for some repertoire, but scholarship suggests that these problems are not insurmountable in Bach's Well-Tempered Clavier (). Among recent clavichord recordings, those by Christopher Hogwood ("The Secret Bach", "The Secret Handel", and "The Secret Mozart"), break new ground. In his liner notes, Hogwood pointed out that these composers would typically have played the clavichord in the privacy of their homes. In England, the composer Herbert Howells (1892–1983) wrote two significant collections of pieces for clavichord ("Lambert's Clavichord" and "Howells' Clavichord"), and Stephen Dodgson (1924–2013) wrote two clavichord suites.

Notes
Sources



</doc>
<doc id="7534" url="https://en.wikipedia.org/wiki?curid=7534" title="Centripetal force">
Centripetal force

A centripetal force (from Latin "centrum", "center" and "petere", "to seek") is a force that makes a body follow a curved path. Its direction is always orthogonal to the motion of the body and towards the fixed point of the instantaneous center of curvature of the path. Isaac Newton described it as "a force by which bodies are drawn or impelled, or in any way tend, towards a point as to a centre". In Newtonian mechanics, gravity provides the centripetal force responsible for astronomical orbits.

One common example involving centripetal force is the case in which a body moves with uniform speed along a circular path. The centripetal force is directed at right angles to the motion and also along the radius towards the centre of the circular path. The mathematical description was derived in 1659 by the Dutch physicist Christiaan Huygens.

The magnitude of the centripetal force on an object of mass "m" moving at tangential speed "v" along a path with radius of curvature "r" is:

where formula_2 is the centripetal acceleration.
The direction of the force is toward the center of the circle in which the object is moving, or the osculating circle (the circle that best fits the local path of the object, if the path is not circular).
The speed in the formula is squared, so twice the speed needs four times the force. The inverse relationship with the radius of curvature shows that half the radial distance requires twice the force. This force is also sometimes written in terms of the angular velocity "ω" of the object about the center of the circle, related to the tangential velocity by the formula

so that

Expressed using the orbital period "T" for one revolution of the circle,

the equation becomes

In particle accelerators, velocity can be very high (close to the speed of light in vacuum) so the same rest mass now exerts greater inertia (relativistic mass) thereby requiring greater force for the same centripetal acceleration, so the equation becomes:

where

is called the Lorentz factor.

More intuitively:

which is the rate of change of relativistic momentum (formula_10)

In the case of an object that is swinging around on the end of a rope in a horizontal plane, the centripetal force on the object is supplied by the tension of the rope. The rope example is an example involving a 'pull' force. The centripetal force can also be supplied as a 'push' force, such as in the case where the normal reaction of a wall supplies the centripetal force for a wall of death rider.

Newton's idea of a centripetal force corresponds to what is nowadays referred to as a central force. When a satellite is in orbit around a planet, gravity is considered to be a centripetal force even though in the case of eccentric orbits, the gravitational force is directed towards the focus, and not towards the instantaneous center of curvature.

Another example of centripetal force arises in the helix that is traced out when a charged particle moves in a uniform magnetic field in the absence of other external forces. In this case, the magnetic force is the centripetal force that acts towards the helix axis.

Below are three examples of increasing complexity, with derivations of the formulas governing velocity and acceleration.

Uniform circular motion refers to the case of constant rate of rotation. Here are two approaches to describing this case.

In two dimensions, the position vector formula_11, which has magnitude (length) formula_12 and directed at an angle formula_13 above the x-axis, can be expressed in Cartesian coordinates using the unit vectors formula_14 and formula_15:

Assume uniform circular motion, which requires three things.

Now find the velocity formula_21 and acceleration formula_22 of the motion by taking derivatives of position with respect to time.

Notice that the term in parenthesis is the original expression of formula_11 in Cartesian coordinates. Consequently,

negative shows that the acceleration is pointed towards the center of the circle (opposite the radius), hence it is called "centripetal" (i.e. "center-seeking"). While objects naturally follow a straight path (due to inertia), this centripetal acceleration describes the circular motion path caused by a centripetal force.

The image at right shows the vector relationships for uniform circular motion. The rotation itself is represented by the angular velocity vector Ω, which is normal to the plane of the orbit (using the right-hand rule) and has magnitude given by:

with "θ" the angular position at time "t". In this subsection, d"θ"/d"t" is assumed constant, independent of time. The distance traveled dℓ of the particle in time d"t" along the circular path is

which, by properties of the vector cross product, has magnitude "r"d"θ" and is in the direction tangent to the circular path.

Consequently,

Differentiating with respect to time,

Lagrange's formula states:

Applying Lagrange's formula with the observation that Ω • r("t") = 0 at all times,

In words, the acceleration is pointing directly opposite to the radial displacement r at all times, and has a magnitude:

where vertical bars |...| denote the vector magnitude, which in the case of r("t") is simply the radius "r" of the path. This result agrees with the previous section, though the notation is slightly different.

When the rate of rotation is made constant in the analysis of nonuniform circular motion, that analysis agrees with this one.

A merit of the vector approach is that it is manifestly independent of any coordinate system.

The upper panel in the image at right shows a ball in circular motion on a banked curve. The curve is banked at an angle "θ" from the horizontal, and the surface of the road is considered to be slippery. The objective is to find what angle the bank must have so the ball does not slide off the road. Intuition tells us that, on a flat curve with no banking at all, the ball will simply slide off the road; while with a very steep banking, the ball will slide to the center unless it travels the curve rapidly.

Apart from any acceleration that might occur in the direction of the path, the lower panel of the image above indicates the forces on the ball. There are "two" forces; one is the force of gravity vertically downward through the center of mass of the ball "mg, where "m" is the mass of the ball and g is the gravitational acceleration; the second is the upward normal force exerted by the road at a right angle to the road surface "ma. The centripetal force demanded by the curved motion is also shown above. This centripetal force is not a third force applied to the ball, but rather must be provided by the net force on the ball resulting from vector addition of the normal force and the force of gravity. The resultant or net force on the ball found by vector addition of the normal force exerted by the road and vertical force due to gravity must equal the centripetal force dictated by the need to travel a circular path. The curved motion is maintained so long as this net force provides the centripetal force requisite to the motion.

The horizontal net force on the ball is the horizontal component of the force from the road, which has magnitude |F| = "m"|a|sin"θ". The vertical component of the force from the road must counteract the gravitational force: |F| = "m"|a|cos"θ" = "m"|g|, which implies |a|=|g| / cos"θ". Substituting into the above formula for |F| yields a horizontal force to be:

On the other hand, at velocity |v| on a circular path of radius "r", kinematics says that the force needed to turn the ball continuously into the turn is the radially inward centripetal force F of magnitude:

Consequently, the ball is in a stable path when the angle of the road is set to satisfy the condition:

or,

As the angle of bank "θ" approaches 90°, the tangent function approaches infinity, allowing larger values for |v|/"r". In words, this equation states that for faster speeds (bigger |v|) the road must be banked more steeply (a larger value for "θ"), and for sharper turns (smaller "r") the road also must be banked more steeply, which accords with intuition. When the angle "θ" does not satisfy the above condition, the horizontal component of force exerted by the road does not provide the correct centripetal force, and an additional frictional force tangential to the road surface is called upon to provide the difference. If friction cannot do this (that is, the coefficient of friction is exceeded), the ball slides to a different radius where the balance can be realized.

These ideas apply to air flight as well. See the FAA pilot's manual.

As a generalization of the uniform circular motion case, suppose the angular rate of rotation is not constant. The acceleration now has a tangential component, as shown the image at right. This case is used to demonstrate a derivation strategy based on a polar coordinate system.

Let r("t") be a vector that describes the position of a point mass as a function of time. Since we are assuming circular motion, let r("t") = "R"·u, where "R" is a constant (the radius of the circle) and u is the unit vector pointing from the origin to the point mass. The direction of u is described by "θ", the angle between the x-axis and the unit vector, measured counterclockwise from the x-axis. The other unit vector for polar coordinates, u is perpendicular to u and points in the direction of increasing "θ". These polar unit vectors can be expressed in terms of Cartesian unit vectors in the "x" and "y" directions, denoted i and j respectively:

and

One can differentiate to find velocity:

where "ω" is the angular velocity d"θ"/d"t".

This result for the velocity matches expectations that the velocity should be directed tangentially to the circle, and that the magnitude of the velocity should be "rω". Differentiating again, and noting that

we find that the acceleration, a is:

Thus, the radial and tangential components of the acceleration are:

where |v| = "r" ω is the magnitude of the velocity (the speed).

These equations express mathematically that, in the case of an object that moves along a circular path with a changing speed, the acceleration of the body may be decomposed into a perpendicular component that changes the direction of motion (the centripetal acceleration), and a parallel, or tangential component, that changes the speed.

The above results can be derived perhaps more simply in polar coordinates, and at the same time extended to general motion within a plane, as shown next. Polar coordinates in the plane employ a radial unit vector u and an angular unit vector u, as shown above. A particle at position r is described by:

where the notation "ρ" is used to describe the distance of the path from the origin instead of "R" to emphasize that this distance is not fixed, but varies with time. The unit vector u travels with the particle and always points in the same direction as r("t"). Unit vector u also travels with the particle and stays orthogonal to u. Thus, u and u form a local Cartesian coordinate system attached to the particle, and tied to the path traveled by the particle. By moving the unit vectors so their tails coincide, as seen in the circle at the left of the image above, it is seen that u and u form a right-angled pair with tips on the unit circle that trace back and forth on the perimeter of this circle with the same angle "θ"("t") as r("t").

When the particle moves, its velocity is

To evaluate the velocity, the derivative of the unit vector u is needed. Because u is a unit vector, its magnitude is fixed, and it can change only in direction, that is, its change du has a component only perpendicular to u. When the trajectory r("t") rotates an amount d"θ", u, which points in the same direction as r("t"), also rotates by d"θ". See image above. Therefore, the change in u is

or

In a similar fashion, the rate of change of u is found. As with u, u is a unit vector and can only rotate without changing size. To remain orthogonal to u while the trajectory r("t") rotates an amount d"θ", u, which is orthogonal to r("t"), also rotates by d"θ". See image above. Therefore, the change du is orthogonal to u and proportional to d"θ" (see image above):

The image above shows the sign to be negative: to maintain orthogonality, if du is positive with d"θ", then du must decrease.

Substituting the derivative of u into the expression for velocity:

To obtain the acceleration, another time differentiation is done:

Substituting the derivatives of u and u, the acceleration of the particle is:

As a particular example, if the particle moves in a circle of constant radius "R", then d"ρ"/d"t" = 0, v = v, and:

where formula_60

These results agree with those above for nonuniform circular motion. See also the article on non-uniform circular motion. If this acceleration is multiplied by the particle mass, the leading term is the centripetal force and the negative of the second term related to angular acceleration is sometimes called the Euler force.

For trajectories other than circular motion, for example, the more general trajectory envisioned in the image above, the instantaneous center of rotation and radius of curvature of the trajectory are related only indirectly to the coordinate system defined by u and u and to the length |r("t")| = "ρ". Consequently, in the general case, it is not straightforward to disentangle the centripetal and Euler terms from the above general acceleration equation.

Local coordinates mean a set of coordinates that travel with the particle, and have orientation determined by the path of the particle. Unit vectors are formed as shown in the image at right, both tangential and normal to the path. This coordinate system sometimes is referred to as "intrinsic" or "path coordinates" or "nt-coordinates", for "normal-tangential", referring to these unit vectors. These coordinates are a very special example of a more general concept of local coordinates from the theory of differential forms.

Distance along the path of the particle is the arc length "s", considered to be a known function of time.

A center of curvature is defined at each position "s" located a distance "ρ" (the radius of curvature) from the curve on a line along the normal u ("s"). The required distance "ρ"("s") at arc length "s" is defined in terms of the rate of rotation of the tangent to the curve, which in turn is determined by the path itself. If the orientation of the tangent relative to some starting position is "θ"("s"), then "ρ"("s") is defined by the derivative d"θ"/d"s":

The radius of curvature usually is taken as positive (that is, as an absolute value), while the "curvature" "κ" is a signed quantity.

A geometric approach to finding the center of curvature and the radius of curvature uses a limiting process leading to the osculating circle. See image above.

Using these coordinates, the motion along the path is viewed as a succession of circular paths of ever-changing center, and at each position "s" constitutes non-uniform circular motion at that position with radius "ρ". The local value of the angular rate of rotation then is given by:

with the local speed "v" given by:

As for the other examples above, because unit vectors cannot change magnitude, their rate of change is always perpendicular to their direction (see the left-hand insert in the image above):

Consequently, the velocity and acceleration are:
and using the chain-rule of differentiation:

In this local coordinate system, the acceleration resembles the expression for nonuniform circular motion with the local radius "ρ"("s"), and the centripetal acceleration is identified as the second term.

Extending this approach to three dimensional space curves leads to the Frenet–Serret formulas.

Looking at the image above, one might wonder whether adequate account has been taken of the difference in curvature between "ρ"("s") and "ρ"("s" + d"s") in computing the arc length as d"s" = "ρ"("s")d"θ". Reassurance on this point can be found using a more formal approach outlined below. This approach also makes connection with the article on curvature.

To introduce the unit vectors of the local coordinate system, one approach is to begin in Cartesian coordinates and describe the local coordinates in terms of these Cartesian coordinates. In terms of arc length "s", let the path be described as:

Then an incremental displacement along the path d"s" is described by:

where primes are introduced to denote derivatives with respect to "s". The magnitude of this displacement is d"s", showing that:

This displacement is necessarily a tangent to the curve at "s", showing that the unit vector tangent to the curve is:

while the outward unit vector normal to the curve is

Orthogonality can be verified by showing that the vector dot product is zero. The unit magnitude of these vectors is a consequence of Eq. 1. Using the tangent vector, the angle "θ" of the tangent to the curve is given by:

The radius of curvature is introduced completely formally (without need for geometric interpretation) as:

The derivative of "θ" can be found from that for sin"θ":

Now:

in which the denominator is unity. With this formula for the derivative of the sine, the radius of curvature becomes:

where the equivalence of the forms stems from differentiation of Eq. 1:
With these results, the acceleration can be found:
as can be verified by taking the dot product with the unit vectors u("s") and u("s"). This result for acceleration is the same as that for circular motion based on the radius "ρ". Using this coordinate system in the inertial frame, it is easy to identify the force normal to the trajectory as the centripetal force and that parallel to the trajectory as the tangential force. From a qualitative standpoint, the path can be approximated by an arc of a circle for a limited time, and for the limited time a particular radius of curvature applies, the centrifugal and Euler forces can be analyzed on the basis of circular motion with that radius.

This result for acceleration agrees with that found earlier. However, in this approach, the question of the change in radius of curvature with "s" is handled completely formally, consistent with a geometric interpretation, but not relying upon it, thereby avoiding any questions the image above might suggest about neglecting the variation in "ρ".
To illustrate the above formulas, let "x", "y" be given as:

Then:

which can be recognized as a circular path around the origin with radius "α". The position "s" = 0 corresponds to ["α", 0], or 3 o'clock. To use the above formalism, the derivatives are needed:

With these results, one can verify that:

The unit vectors can also be found:

which serve to show that "s" = 0 is located at position ["ρ", 0] and "s" = "ρ"π/2 at [0, "ρ"], which agrees with the original expressions for "x" and "y". In other words, "s" is measured counterclockwise around the circle from 3 o'clock. Also, the derivatives of these vectors can be found:

To obtain velocity and acceleration, a time-dependence for "s" is necessary. For counterclockwise motion at variable speed "v"("t"):
where "v"("t") is the speed and "t" is time, and "s"("t" = 0) = 0. Then:
where it already is established that α = ρ. This acceleration is the standard result for non-uniform circular motion.





</doc>
<doc id="7535" url="https://en.wikipedia.org/wiki?curid=7535" title="Commodore">
Commodore

Commodore generally refers to Commodore (rank), a naval rank. It may also refer to:









</doc>
<doc id="7536" url="https://en.wikipedia.org/wiki?curid=7536" title="Conditioning">
Conditioning

Conditioning may refer to:







</doc>
<doc id="7538" url="https://en.wikipedia.org/wiki?curid=7538" title="Checksum">
Checksum

A checksum is a small-sized datum derived from a block of digital data for the purpose of detecting errors which may have been introduced during its transmission or storage. It is usually applied to an installation file after it is received from the download server. By themselves, checksums are often used to verify data integrity but are not relied upon to verify data authenticity.

The actual procedure which yields the checksum from a data input is called a checksum function or checksum algorithm. Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. This is especially true of cryptographic hash functions, which may be used to detect many data corruption errors and verify overall data integrity; if the computed checksum for the current data input matches the stored value of a previously computed checksum, there is a very high probability the data has not been accidentally altered or corrupted.

Checksum functions are related to hash functions, fingerprints, randomization functions, and cryptographic hash functions. However, each of those concepts has different applications and therefore different design goals. For instance a function returning the start of a string can provide a hash appropriate for some applications but will never be a suitable checksum. Checksums are used as cryptographic primitives in larger authentication algorithms. For cryptographic systems with these two specific design goals, see HMAC.

Check digits and parity bits are special cases of checksums, appropriate for small blocks of data (such as Social Security numbers, bank account numbers, computer words, single bytes, etc.). Some error-correcting codes are based on special checksums which not only detect common errors but also allow the original data to be recovered in certain cases.

The simplest checksum algorithm is the so-called longitudinal parity check, which breaks the data into "words" with a fixed number "n" of bits, and then computes the exclusive or (XOR) of all those words. The result is appended to the message as an extra word. To check the integrity of a message, the receiver computes the exclusive or of all its words, including the checksum; if the result is not a word consisting of "n" zeros, the receiver knows a transmission error occurred.

With this checksum, any transmission error which flips a single bit of the message, or an odd number of bits, will be detected as an incorrect checksum. However, an error which affects two bits will not be detected if those bits lie at the same position in two distinct words. Also swapping of two or more words will not be detected. If the affected bits are independently chosen at random, the probability of a two-bit error being undetected is 1/"n".

A variant of the previous algorithm is to add all the "words" as unsigned binary numbers, discarding any overflow bits, and append the two's complement of the total as the checksum. To validate a message, the receiver adds all the words in the same manner, including the checksum; if the result is not a word full of zeros, an error must have occurred. This variant too detects any single-bit error, but the promodular sum is used in SAE J1708.

The simple checksums described above fail to detect some common errors which affect many bits at once, such as changing the order of data words, or inserting or deleting words with all bits set to zero. The checksum algorithms most used in practice, such as Fletcher's checksum, Adler-32, and cyclic redundancy checks (CRCs), address these weaknesses by considering not only the value of each word but also its position in the sequence. This feature generally increases the cost of computing the checksum.

A message that is "m" bits long can be viewed as a corner of the "m"-dimensional hypercube. The effect of a checksum algorithm that yields an n-bit checksum is to map each "m"-bit message to a corner of a larger hypercube, with dimension . The 2 corners of this hypercube represent all possible received messages. The valid received messages (those that have the correct checksum) comprise a smaller set, with only 2 corners.

A single-bit transmission error then corresponds to a displacement from a valid corner (the correct message and checksum) to one of the "m" adjacent corners. An error which affects "k" bits moves the message to a corner which is "k" steps removed from its correct corner. The goal of a good checksum algorithm is to spread the valid corners as far from each other as possible, so as to increase the likelihood "typical" transmission errors will end up in an invalid corner.

General topic

Error correction

Hash functions

Related concepts



</doc>
<doc id="7540" url="https://en.wikipedia.org/wiki?curid=7540" title="Cultural evolution (disambiguation)">
Cultural evolution (disambiguation)

Cultural evolution is cultural change viewed from an evolutionary perspective.

It may also refer to:



</doc>
<doc id="7541" url="https://en.wikipedia.org/wiki?curid=7541" title="City University of New York">
City University of New York

The City University of New York (CUNY ) is the public university system of New York City, and the largest urban university system in the United States. CUNY and the State University of New York (SUNY) are separate and independent university systems, despite the fact that both public institutions receive funding from New York State. CUNY, however, is located in only New York City, while SUNY is located in the entire state, including New York City.

CUNY is the third-largest university system in the United States, in terms of enrollment, behind the State University of New York (SUNY), and the California State University system. More than 274,000-degree-credit students, continuing, and professional education students are enrolled at campuses located in all five New York City boroughs.

The university has one of the most diverse student bodies in the United States, with students hailing from 208 countries, but mostly from New York City. The black, white and Hispanic undergraduate populations each comprise more than a quarter of the student body, and Asian undergraduates make up 18 percent. Fifty-eight percent are female, and 28 percent are 25 or older.

"The following table is 'sortable'; click on a column heading to re-sort the table by values of that column."

CUNY employs 6,700 full-time faculty members and over 10,000 adjunct faculty members. Faculty and staff are represented by the Professional Staff Congress (PSC), a labor union and chapter of the American Federation of Teachers.


CUNY was created in 1961, by New York State legislation, signed into law by Governor Nelson Rockefeller. The legislation
integrated existing institutions and a new graduate school into a coordinated system of higher education for the city, under the control of the "Board of Higher Education of the City of New York", which had been created by New York State legislation in 1926. By 1979, the Board of Higher Education had become the "Board of Trustees of the CUNY".

The institutions that were merged in order to create CUNY were:


CUNY has served a diverse student body, especially those excluded from or unable to afford private universities. Its four-year colleges offered a high quality, tuition-free education to the poor, the working class and the immigrants of New York City who met the grade requirements for matriculated status. During the post-World War I era, when some Ivy League universities, such as Yale University, discriminated against Jews, many Jewish academics and intellectuals studied and taught at CUNY. The City College of New York developed a reputation of being "the Harvard of the proletariat."

As New York City's population—and public college enrollment—grew during the early 20th century and the city struggled for resources, the municipal colleges slowly began adopting selective tuition, also known as instructional fees, for a handful of courses and programs. During the Great Depression, with funding for the public colleges severely constrained, limits were imposed on the size of the colleges' free Day Session, and tuition was imposed upon students deemed "competent" but not academically qualified for the day program. Most of these "limited matriculation" students enrolled in the Evening Session, and paid tuition.

Demand in the United States for higher education rapidly grew after World War II, and during the mid-1940s a movement began to create community colleges to provide accessible education and training. In New York City, however, the community-college movement was constrained by many factors including "financial problems, narrow perceptions of responsibility, organizational weaknesses, adverse political factors, and another competing priorities."

Community colleges would have drawn from the same city coffers that were funding the senior colleges, and city higher education officials were of the view that the state should finance them. It wasn't until 1955, under a shared-funding arrangement with New York State, that New York City established its first community college, on Staten Island. Unlike the day college students attending the city's public baccalaureate colleges for free, the community college students had to pay tuition fees under the state-city funding formula. Community college students paid tuition fees for approximately 10 years.

Over time, tuition fees for limited-matriculated students became an important source of system revenues. In fall 1957, for example, nearly 36,000 attended Hunter, Brooklyn, Queens and City Colleges for free, but another 24,000 paid tuition fees of up to $300 a year – the equivalent of $2,413 in 2011. Undergraduate tuition and other student fees in 1957 comprised 17 percent of the colleges' $46.8 million in revenues, about $7.74 million — a figure equivalent to $62.4 million in 2011 buying power.

Three community colleges had been established by early 1961, when New York City's public colleges were codified by the state as a single university with a chancellor at the helm and an infusion of state funds. But the city's slowness in creating the community colleges as demand for college seats was intensifying, had resulted in mounting frustration, particularly on the part of minorities, that college opportunities were not available to them.

In 1964, as New York City's Board of Higher Education moved to take full responsibility for the community colleges, city officials extended the senior colleges' free tuition policy to them, a change that was included by Mayor Robert Wagner in his budget plans and took effect with the 1964–65 academic year.

In 1969, a group of Black and Puerto Rican students occupied City College demanding the racial integration of CUNY, which at the time had an overwhelmingly white student body.

Students at some campuses became increasingly frustrated with the university's and Board of Higher Education's handling of university administration. At Baruch College in 1967, over a thousand students protested the plan to make the college an upper-division school limited to junior, senior, and graduate students. At Brooklyn College in 1968, students attempted a sit-in to demand the admission of more black and Puerto Rican students and additional black studies curriculum. Students at Hunter College also demanded a Black studies program. Members of the SEEK program, which provided academic support for underprepared and underprivileged students, staged a building takeover at Queens College in 1969 to protest the decisions of the program's director, who would later be replaced by a black professor. Puerto Rican students at Bronx Community College filed a report with the New York State Division of Human Rights in 1970, contending that the intellectual level of the college was inferior and discriminatory. Hunter College was crippled for several days by a protest of 2,000 students who had a list of demands focusing on more student representation in college administration. Across CUNY, students boycotted their campuses in 1970 to protest a rise in student fees and other issues, including the proposed (and later implemented) open admissions plan.

Like many college campuses in 1970, CUNY faced a number of protests and demonstrations after the Kent State shootings and Cambodian Campaign. The Administrative Council of the City University of New York sent U.S. President Richard Nixon a telegram in 1970 stating, "No nation can long endure the alienation of the best of its young people." Some colleges, including John Jay College of Criminal Justice, historically the "college for cops," held teach-ins in addition to student and faculty protests.

In 1969, the Board of Trustees implemented a new admissions policy. The doors to CUNY were opened wide to all those demanding entrance, assuring all high school graduates entrance to the university without having to fulfill traditional requirements such as exams or grades. This policy was known as open admissions and nearly doubled the number of students enrolling in the CUNY system to 35,000 (compared to 20,000 the year before). With greater numbers came more diversity: Black and Hispanic student enrollment increased threefold. Remedial education, to supplement the training of under-prepared students, became a significant part of CUNY's offerings.

In fall 1976, during New York City's fiscal crisis, the free tuition policy was discontinued under pressure from the federal government, the financial community that had a role in rescuing the city from bankruptcy, and New York State, which would take over the funding of CUNY's senior colleges. Tuition, which had been in place in the State University of New York system since 1963, was instituted at all CUNY colleges.

Meanwhile, CUNY students were added to the state's need-based Tuition Assistance Program (TAP), which had been created to help private colleges. Full-time students who met the income eligibility criteria were permitted to receive TAP, ensuring for the first time that financial hardship would deprive no CUNY student of a college education. Within a few years, the federal government would create its own need-based program, known as Pell Grants, providing the neediest students with a tuition-free college education. By 2011, nearly six of ten full- time undergraduates qualified for a tuition-free education at CUNY due in large measure to state, federal and CUNY financial aid programs. CUNY's enrollment dipped after tuition was re-established, and there were further enrollment declines through the 1980s and into the 1990s.

In 1995, CUNY suffered another fiscal crisis when Governor George Pataki proposed a drastic cut in state financing. Faculty cancelled classes and students staged protests. By May, CUNY adopted deep cuts to college budgets and class offerings. By June, in order to save money spent on remedial programs, CUNY adopted a stricter admissions policy for its senior colleges: students deemed unprepared for college would not be admitted, this a departure from the 1970 Open Admissions program. That year's final state budget cut funding by $102 million, which CUNY absorbed by increasing tuition by $750 and offering a retirement incentive plan for faculty.

In 1999, a task force appointed by Mayor Rudolph Giuliani issued a report that described CUNY as "an institution adrift" and called for an improved, more cohesive university structure and management, as well as more consistent academic standards. Following the report, Matthew Goldstein, a mathematician and City College graduate who had led CUNY's Baruch College and briefly, Adelphi University, was appointed chancellor. CUNY ended its policy of open admissions to its four-year colleges, raised its admissions standards at its most selective four-year colleges (Baruch, Brooklyn, City, Hunter and Queens), and required new-enrollees who needed remediation, to begin their studies at a CUNY open-admissions community college.

CUNY's enrollment of degree-credit students reached 220,727 in 2005 and 262,321 in 2010 as the university broadened its academic offerings. The university added more than 2,000 full-time faculty positions, opened new schools and programs, and expanded the university's fundraising efforts to help pay for them. Fundraising increased from $35 million in 2000 to more than $200 million in 2012.

As of Autumn 2013, all CUNY undergraduates are required to take an administration-dictated common core of courses which have been claimed to meet specific "learning outcomes" or standards. Since the courses are accepted University wide, the administration claims it will be easier for students to transfer course credits between CUNY colleges. It also reduced the number of core courses some CUNY colleges had required, to a level below national norms, particularly in the sciences. The program is the target of several lawsuits by students and faculty, and was the subject of a "no confidence" vote by the faculty, who rejected it by an overwhelming 92% margin.

Chancellor Goldstein retired on July 1, 2013, and was replaced on June 1, 2014 by James Milliken, president of the University of Nebraska, and a graduate of University of Nebraska and New York University Law School. Milliken is retiring at the end of the 2017-18 academic year and a search for a replacement was underway as of February 2018.

The forerunner of today's City University of New York was governed by the Board of Education of New York City. Members of the Board of Education, chaired by the President of the board, served as "ex officio" trustees. For the next four decades, the board members continued to serve as "ex officio" trustees of the College of the City of New York and the city's other municipal college, the Normal College of the City of New York.

In 1900, the New York State Legislature created separate boards of trustees for the College of the City of New York and the Normal College, which became Hunter College in 1914. In 1926, the Legislature established the Board of Higher Education of the City of New York, which assumed supervision of both municipal colleges.

In 1961, the New York State Legislature established the City University of New York, uniting what had become seven municipal colleges at the time: the City College of New York, Hunter College, Brooklyn College, Queens College, Staten Island Community College, Bronx Community College and Queensborough Community College. In 1979, the CUNY Financing and Governance Act was adopted by the State and the Board of Higher Education became the City University of New York Board of Trustees.

Today, the City University is governed by the Board of Trustees composed of 17 members, ten of whom are appointed by the Governor of New York "with the advice and consent of the senate," and five by the Mayor of New York City "with the advice and consent of the senate." The final two trustees are "ex officio" members. One is the chair of the university's student senate, and the other is non-voting and is the chair of the university's faculty senate. Both the mayoral and gubernatorial appointments to the CUNY Board are required to include at least one resident of each of New York City's five boroughs. Trustees serve seven-year terms, which are renewable for another seven years. The Chancellor is elected by the Board of Trustees, and is the "chief educational and administrative officer" of the City University.

The administrative offices are in Midtown Manhattan.

CUNY has its own public safety force whose duties are to protect and serve all students and faculty members, and enforce all state and city laws at all of CUNY's universities. The force has more than 1000 officers, making it one of the largest public safety forces in New York City.

The Public Safety Department came under heavy criticism, from student groups, after several students protesting tuition increases tried to occupy the lobby of the Baruch College. The occupiers were forcibly removed from the area and several were arrested on November 21, 2011.

CUNY also has a broadcast TV service, CUNY TV (channel 75 on Spectrum, digital HD broadcast channel 25.3), which airs telecourses, classic and foreign films, magazine shows and panel discussions in foreign languages.

The City University Film Festival is CUNY's official film festival. The festival was founded in 2009 by Hunter College student Daniel Cowen.

CUNY graduates include 13 Nobel laureates, a Fields Medalist, a U.S. Secretary of State, a Supreme Court Justice, several New York City mayors, members of Congress, state legislators, scientists and artists.




</doc>
<doc id="7543" url="https://en.wikipedia.org/wiki?curid=7543" title="Computational complexity theory">
Computational complexity theory

Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.

A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millenium Prize Problems, is dedicated to the field of computational complexity. 

Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.

A computational problem can be viewed as an infinite collection of "instances" together with a "solution" for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g., 15) and the solution is "yes" if the number is prime and "no" otherwise (in this case, 15 is not prime and the answer is "no"). Stated another way, the "instance" is a particular input to the problem, and the "solution" is the output corresponding to the given input.

To further highlight the difference between a problem and an instance, consider the following instance of the decision version of the traveling salesman problem: Is there a route of at most 2000 kilometres passing through all of Germany's 15 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in Milan whose total length is at most 10 km. For this reason, complexity theory addresses computational problems and not particular problem instances.

When considering computational problems, a problem instance is a string over an alphabet. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, integers can be represented in binary notation, and graphs can be encoded directly via their adjacency matrices, or by encoding their adjacency lists in binary.

Even though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.

Decision problems are one of the central objects of study in computational complexity theory. A decision problem is a special type of computational problem whose answer is either "yes" or "no", or alternately either 1 or 0. A decision problem can be viewed as a formal language, where the members of the language are instances whose output is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer "yes", the algorithm is said to accept the input string, otherwise it is said to reject the input.

An example of a decision problem is the following. The input is an arbitrary graph. The problem consists in deciding whether the given graph is connected, or not. The formal language associated with this decision problem is then the set of all connected graphs — to obtain a precise definition of this language, one has to decide how graphs are encoded as binary strings.

A function problem is a computational problem where a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem—that is, the output isn't just yes or no. Notable examples include the traveling salesman problem and the integer factorization problem.

It is tempting to think that the notion of function problems is much richer than the notion of decision problems. However, this is not really the case, since function problems can be recast as decision problems. For example, the multiplication of two integers can be expressed as the set of triples ("a", "b", "c") such that the relation "a" × "b" = "c" holds. Deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers.

To measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as a function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with 2"n" vertices compared to the time taken for a graph with "n" vertices?

If the input size is "n", the time taken can be expressed as a function of "n". Since the time taken on different inputs of the same size can be different, the worst-case time complexity T("n") is defined to be the maximum time taken over all inputs of size "n". If T("n") is a polynomial in "n", then the algorithm is said to be a polynomial time algorithm. Cobham's thesis argues that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm.

A Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a general model of a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the Church–Turing thesis. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a RAM machine, Conway's Game of Life, cellular automata or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.

Many types of Turing machines are used to define complexity classes, such as deterministic Turing machines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing machines, symmetric Turing machines and alternating Turing machines. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.

A deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called randomized algorithms. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see non-deterministic algorithm.

Many machine models different from the standard multi-tape Turing machines have been proposed in the literature, for example random access machines. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary. What all these models have in common is that the machines operate deterministically.

However, some computational problems are easier to analyze in terms of more unusual resources. For example, a non-deterministic Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The non-deterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that non-deterministic time is a very important resource in analyzing computational problems.

For a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The "time required" by a deterministic Turing machine "M" on input "x" is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer ("yes" or "no"). A Turing machine "M" is said to operate within time "f"("n"), if the time required by "M" on each input of length "n" is at most "f"("n"). A decision problem "A" can be solved in time "f"("n") if there exists a Turing machine operating in time "f"("n") that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time "f"("n") on a deterministic Turing machine is then denoted by DTIME("f"("n")).

Analogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity.

The complexity of an algorithm is often expressed using big O notation.

The best, worst and average case complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size "n" may be faster to solve than others, we define the following complexities:

For example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the input is sorted or sorted in reverse order, and the algorithm takes time O("n") for this case. If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O("n" log "n"). The best case occurs when each pivoting divides the list in half, also needing O("n" log "n") time.

To classify the computation time (or similar resources, such as space consumption), one is interested in proving upper and lower bounds on the maximum amount of time required by the most efficient algorithm solving a given problem. The complexity of an algorithm is usually taken to be its worst-case complexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis of algorithms. To show an upper bound "T"("n") on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most "T"("n"). However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase "all possible algorithms" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of "T"("n") for a problem requires showing that no algorithm can have time complexity lower than "T"("n").

Upper and lower bounds are usually stated using the big O notation, which hides constant factors and smaller terms. This makes the bounds independent of the specific details of the computational model used. For instance, if "T"("n") = 7"n" + 15"n" + 40, in big O notation one would write "T"("n") = O("n").

A complexity class is a set of problems of related complexity. Simpler complexity classes are defined by the following factors:

Some complexity classes have complicated definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following:

But bounding the computation time above by some concrete function "f"("n") often yields complexity classes that depend on the chosen machine model. For instance, the language {"xx" | "x" is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, Cobham-Edmonds thesis states that "the time complexities in any two reasonable and general models of computation are polynomially related" . This forms the basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is FP.

Many important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following:
The logarithmic-space classes (necessarily) do not take into account the space needed to represent the problem.

It turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by Savitch's theorem.

Other important complexity classes include BPP, ZPP and RP, which are defined using probabilistic Turing machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are defined using quantum Turing machines. #P is an important complexity class of counting problems (not decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is the class of all decision problems.

For the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME("n") is contained in DTIME("n"), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.

More precisely, the time hierarchy theorem states that

The space hierarchy theorem states that

The time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE.

Many complexity classes are defined using the concept of a reduction. A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at most as difficult as another problem. For instance, if a problem "X" can be solved using an algorithm for "Y", "X" is no more difficult than "Y", and we say that "X" "reduces" to "Y". There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as polynomial-time reductions or log-space reductions.

The most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.

This motivates the concept of a problem being hard for a complexity class. A problem "X" is "hard" for a class of problems "C" if every problem in "C" can be reduced to "X". Thus no problem in "C" is harder than "X", since an algorithm for "X" allows us to solve any problem in "C". The notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems.

If a problem "X" is in "C" and hard for "C", then "X" is said to be "complete" for "C". This means that "X" is the hardest problem in "C". (Since many problems could be equally hard, one might say that "X" is one of the hardest problems in "C".) Thus the class of NP-complete problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, Π, to another problem, Π, would indicate that there is no known polynomial-time solution for Π. This is because a polynomial-time solution to Π would yield a polynomial-time solution to Π. Similarly, because all NP problems can be reduced to the set, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.

The complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The complexity class NP, on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are special non-deterministic Turing machines, it is easily observed that each problem in P is also member of the class NP.

The question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution. If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of integer programming problems in operations research, many problems in logistics, protein structure prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus NP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is a US$1,000,000 prize for resolving the problem.

It was shown by Ladner that if P ≠ NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.

The graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to László Babai and Eugene Luks has run time formula_3 for graphs with "n" vertices, although some recent work by Babai offers some potentially new perspectives on this.

The integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a prime factor less than "k". No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP will equal co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes time formula_4 to factor an integer "n". However, the best known quantum algorithm for this problem, Shor's algorithm, does run in polynomial time. Unfortunately, this fact doesn't say much about where the problem lies with respect to non-quantum complexity classes.

Many known complexity classes are suspected to be unequal, but this has not been proved. For instance P ⊆ NP ⊆ PP ⊆ PSPACE, but it is possible that P = PSPACE. If P is not equal to NP, then P is not equal to PSPACE either. Since there are many known complexity classes between P and PSPACE, such as RP, BPP, PP, BQP, MA, PH, etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory.

Along the same lines, co-NP is the class containing the complement problems (i.e. problems with the "yes"/"no" answers reversed) of NP problems. It is believed that NP is not equal to co-NP; however, it has not yet been proven. It is clear that if these two complexity classes are not equal then P is not equal to NP, since if P=NP we would also have P=co-NP, since problems in NP are dual to those in co-NP.

Similarly, it is not known if L (the set of all problems that can be solved in logarithmic space) is strictly contained in P or equal to P. Again, there are many complexity classes between the two, such as NL and NC, and it is not known if they are distinct or equal classes.

It is suspected that P and BPP are equal. However, it is currently open if BPP = NEXP.

A problem that can be solved in theory (e.g. given large but finite resources, especially time), but for which in practice "any" solution takes too many resources to be useful, is known as an '. Conversely, a problem that can be solved in practice is called a ', literally "a problem that can be handled". The term "infeasible" (literally "cannot be done") is sometimes used interchangeably with "intractable", though this risks confusion with a feasible solution in mathematical optimization.

Tractable problems are frequently identified with problems that have polynomial-time solutions (P, PTIME); this is known as the Cobham–Edmonds thesis. Problems that are known to be intractable in this sense include those that are EXPTIME-hard. If NP is not the same as P, then NP-hard problems are also intractable in this sense.

However, this identification is inexact: a polynomial-time solution with large exponent or large constant term grows quickly, and may be impractical for practical size problems; conversely, an exponential-time solution that grows slowly may be practical on realistic input, or a solution that takes a long time in the worst case may take a short time in most cases or the average case, and thus still be practical. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example, the decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT solvers routinely handle large instances of the NP-complete Boolean satisfiability problem.

To see why exponential-time algorithms are generally unusable in practice, consider a program that makes 2 operations before halting. For small "n", say 100, and assuming for the sake of example that the computer does 10 operations each second, the program would run for about 4 × 10 years, which is the same order of magnitude as the age of the universe. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. However, an exponential-time algorithm that takes 1.0001 operations is practical until "n" gets relatively large.

Similarly, a polynomial time algorithm is not always practical. If its running time is, say, "n", it is unreasonable to consider it efficient and it is still useless except on small instances. Indeed, in practice even "n" or "n" algorithms are often impractical on realistic sizes of problems.

An early example of algorithm complexity analysis is the running time analysis of the Euclidean algorithm done by Gabriel Lamé in 1844.

Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.

The beginning of systematic studies in computational complexity is attributed to the seminal 1965 paper "On the Computational Complexity of Algorithms" by Juris Hartmanis and Richard E. Stearns, which laid out the definitions of time complexity and space complexity, and proved the hierarchy theorems. In addition, in 1965 Edmonds suggested to consider a "good" algorithm to be one with running time bounded by a polynomial of the input size.

Earlier papers studying problems solvable by Turing machines with specific bounded resources include John Myhill's definition of linear bounded automata (Myhill 1960), Raymond Smullyan's study of rudimentary sets (1961), as well as Hisao Yamada's paper on real-time computations (1962). Somewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another specific complexity measure. As he remembers:

In 1967, Manuel Blum formulated a set of axioms (now known as Blum axioms) specifying desirable properties of complexity measures on the set of computable functions and proved an important result, the so-called speed-up theorem. The field began to flourish in 1971 when the Stephen Cook and Leonid Levin proved the existence of practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, "Reducibility Among Combinatorial Problems", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.

In the 1980s, much work was done on the average difficulty of solving NP-complete problems—both exactly and approximately. At that time, computational complexity theory was at its height, and it was widely believed that if a problem turned out to be NP-complete, then there was little chance of being able to work with the problem in a practical situation. However, it became increasingly clear that this is not always the case, and some authors claimed that general asymptotic results are often unimportant for typical problems arising in practice.





</doc>
<doc id="7544" url="https://en.wikipedia.org/wiki?curid=7544" title="Cadence">
Cadence

Cadence may refer to:






</doc>
<doc id="7546" url="https://en.wikipedia.org/wiki?curid=7546" title="Camelot">
Camelot

Camelot is a castle and court associated with the legendary King Arthur. Absent in the early Arthurian material, Camelot first appeared in 12th-century French romances and, after the Lancelot-Grail cycle, eventually came to be described as the fantastic capital of Arthur's realm and a symbol of the Arthurian world. 

The stories locate it somewhere in Great Britain and sometimes associate it with real cities, though more usually its precise location is not revealed. Most scholars regard it as being entirely fictional, its geography being perfect for chivalric romance writers. Nevertheless, arguments about the location of the "real Camelot" have occurred since the 15th century and continue to rage today in popular works and for tourism purposes.

The name's derivation is uncertain. It has numerous different spellings in medieval French Arthurian romance, including: "Camaalot", "Camalot", "Chamalot", "Camehelot" (sometimes read as "Camchilot"), "Camaaloth", "Caamalot", "Camahaloth", "Camaelot", "Kamaalot", "Kamaaloth", "Kaamalot", "Kamahaloth", "Kameloth", "Kamaelot", "Kamelot", "Kaamelot", "Cameloth", "Camelot" and "Gamalaot". Arthurian scholar Ernst Brugger suggested that it was a corruption of the site of Arthur's final battle, the Battle of Camlann, in Welsh tradition. Roger Sherman Loomis believed it was derived from "Cavalon", a place name that he suggested was a corruption of Avalon (under the influence of the Breton place name "Cavallon"). He further suggested that "Cavalon/Camelot" became Arthur's capital due to confusion with Arthur's other traditional court at "Carlion" ("Caer Lleon" in Welsh).

Others have suggested a derivation from the British Iron Age and Romano-British place name "Camulodunum", one of the first capitals of Roman Britain and which would have significance in Romano-British culture. Indeed, John Morris, the English historian who specialized in the study of the institutions of the Roman Empire and the history of Sub-Roman Britain, suggested in his book "The Age of Arthur" that as the descendants of Romanized Britons looked back to a golden age of peace and prosperity under Rome, the name "Camelot" of Arthurian legend may have referred to the capital of Britannia ("Camulodunum", modern Colchester) in Roman times. It is unclear, however, where Chrétien de Troyes would have encountered the name "Camulodunum", or why he would render it as "Camaalot", though Urban T. Holmes argued in 1929 that Chretien had access to Book 2 of Pliny's "Natural History", where it is rendered as "Camaloduno". Given Chrétien's known tendency to create new stories and characters, being the first to mention the hero Lancelot's love affair with Queen Guinevere for example, the name might also be entirely invented.

Arthur's court at Camelot is mentioned for the first time in Chrétien's poem "Lancelot, the Knight of the Cart", dating to the 1170s, though it does not appear in all the manuscripts. It is mentioned in passing, and is not described:

Nothing in Chrétien's poem suggests the level of importance Camelot would have in later romances. For Chrétien, Arthur's chief court was in Caerleon in Wales; this was the king's primary base in Geoffrey of Monmouth's "Historia Regum Britanniae" and subsequent literature. Chrétien depicts Arthur, like a typical medieval monarch, holding court at a number of cities and castles. 

It is not until the 13th-century French prose romances, including the Lancelot-Grail and the Post-Vulgate Cycle, that Camelot began to supersede Caerleon, and even then, many descriptive details applied to Camelot derive from Geoffrey's earlier grand depiction of the Welsh town. Most Arthurian romances of this period produced in English or Welsh did not follow this trend; Camelot was referred to infrequently, and usually in translations from French. One exception is "Sir Gawain and the Green Knight", which locates Arthur's court at "Camelot"; however, in Britain, Arthur's court was generally located at Caerleon, or at Carlisle, which is usually identified with the "Carduel" of the French romances.

In the late 15th century, Thomas Malory created the image of Camelot most familiar to English speakers today in his "Le Morte d'Arthur", a work based mostly on the French romances. He firmly identifies Camelot with Winchester in England, an identification that remained popular over the centuries, though it was rejected by Malory's own editor, William Caxton, who preferred a Welsh location.

The Lancelot-Grail Cycle and the texts it influenced depict the city of Camelot as standing along a river, downstream from Astolat. It is surrounded by plains and forests, and its magnificent cathedral, St. Stephen's, is the religious centre for Arthur's Knights of the Round Table. There, Arthur and Guinevere are married and there are the tombs of many kings and knights. In a mighty castle stands the Round Table; it is here that Galahad conquers the Siege Perilous, and where the knights see a vision of the Holy Grail and swear to find it. Jousts are held in a meadow outside the city. 

In the "Palamedes" and other works, the castle is eventually destroyed by King Mark of Cornwall after the loss of Arthur at the Battle of Camlann. However maddening to later scholars searching for Camelot's location, its imprecise geography serves the romances well, as Camelot becomes less a literal place than a powerful symbol of Arthur's court and universe. There is a Kamaalot featured as the home of Perceval's mother in the romance "Perlesvaus".

The romancers' versions of Camelot drew on earlier descriptions of Arthur's fabulous court. From Geoffrey's grand description of Caerleon, Camelot gains its impressive architecture, its many churches and the chivalry and courtesy of its inhabitants. Geoffrey's description in turn drew on an already established tradition in Welsh oral tradition of the grandeur of Arthur's court. The tale "Culhwch and Olwen", associated with the "Mabinogion" and perhaps written in the 11th century, draws a dramatic picture of Arthur's hall and his many powerful warriors who go from there on great adventures, placing it in Celliwig, an uncertain locale in Cornwall. 

Although the court at Celliwig is the most prominent in remaining early Welsh manuscripts, the various versions of the Welsh Triads agree in giving Arthur multiple courts, one in each of the areas inhabited by the Celtic Britons: Cornwall, Wales and the Hen Ogledd. This perhaps reflects the influence of widespread oral traditions common by 800 which are recorded in various place names and features such as Arthur's Seat, indicating Arthur was a hero known and associated with many locations across Brittonic areas of Britain as well as Brittany. Even at this stage Arthur could not be tied to one location. Many other places are listed as a location where Arthur holds court in the later romances, Carlisle and London perhaps being the most prominent.

Arthurian scholar Norris J. Lacy commented that "Camelot, located no where in particular, can be anywhere." The romancers' versions of Camelot draw on earlier traditions of Arthur's fabulous court. The Celliwig of "Culhwch and Olwen" appears in the Welsh Triads as well; this early Welsh material places Wales' greatest leader outside its national boundaries. Geoffrey's description of Caerleon is probably based on his personal familiarity with the town and its impressive Roman ruins; it is less clear that Caerleon was associated with Arthur before Geoffrey. Several French romances ("Perlesvaus", the Didot "Perceval" attributed to Robert de Boron, and even the early romances of Chrétien such as "Erec and Enide" and "Yvain, the Knight of the Lion") have Arthur hold court at "Carduel in Wales," a northern city based on the real Carlisle. Malory's identification of Camelot as Winchester was probably partially inspired by the latter city's history: it had been the capital of Wessex under Alfred the Great, and boasted the Winchester Round Table, an artifact constructed in the 13th century but widely believed to be the original by Malory's time. Caxton rejected the association, saying Camelot was in Wales and that its ruins could still be seen; this is a likely reference to the Roman ruins at Caerwent.

In 1542, John Leland reported the locals around Cadbury Castle, formerly known as Camalet, in Somerset considered it to be the original Camelot. This theory, which was repeated by later antiquaries, is bolstered, or may have derived from, Cadbury's proximity to the River Cam and the villages of Queen Camel and West Camel, and remained popular enough to help inspire a large-scale archaeological dig in the 20th century. These excavations, led by archaeologist Leslie Alcock from 1966–70, were titled "Cadbury-Camelot" and won much media attention. The dig revealed that the site seems to have been occupied as early as the 4th millennium and to have been refortified and occupied by a major Brittonic ruler and his war band from . This early medieval settlement continued until around 580. The works were by far the largest known fortification of the period, double the size of comparative "caers" and with Mediterranean artifacts representing extensive trade and Saxon ones showing possible conquest. The use of the name Camelot and the support of Geoffrey Ashe helped ensure much publicity for the finds, but Alcock himself later grew embarrassed by the supposed Arthurian connection to the site. Following the arguments of David Dumville, Alcock felt the site was too late and too uncertain to be a tenable Camelot. Modern archaeologists follow him in rejecting the name, calling it instead Cadbury Castle hill fort. Despite this, Cadbury remains widely associated with Camelot.

The name of the Romano-British town of Camulodunum in Essex was derived from the Celtic god Camulus. However, it was located well within territory usually thought to have been conquered early in the 5th century by Saxons, so it is unlikely to have been the location of any "true" Camelot. The town was definitely known as Colchester as early as the "Anglo-Saxon Chronicle" in 917. Even Colchester Museum argues strongly regarding the historical Arthur: "It would be impossible and inconceivable to link him to the Colchester area, or to Essex more generally," pointing out that the connection between the name Camulodunum and Colchester was unknown until the 18th century. Professor Peter Fields has suggested that another Camulodunum, a former Roman fort, is a likely location of King Arthur's Camelot and that "Slack, on the outskirts of Huddersfield in West Yorkshire," is where Arthur would have held court. This is because of the name, and also regarding its strategic location: it is but a few miles from the extreme South-West of Hen Ogledd (also making close to North Wales), and would have been a great flagship point in starving off attacks to the Celtic kingdoms from both the Angles and other attackers. Other places in Britain with names related to "Camel" have also been suggested, such as Camelford in Cornwall, located down the River Camel from where Geoffrey places Camlann, the scene of Arthur's final battle. The area's connections with Camelot and Camlann are merely speculative. Further north Camelon and its connections with Arthur's O'on have been mentioned in relation to Camelot, but Camelon may be an antiquarian neologism coined after the 15th century, with its earlier name being "Carmore" or "Carmure".

Camelot has become a permanent fixture in interpretations of the Arthurian legend. The symbolism of Camelot so impressed Alfred, Lord Tennyson that he wrote up a prose sketch on the castle as one of his earliest attempts to treat the legend.

Modern stories typically retain Camelot's lack of precise location and its status as a symbol of the Arthurian world, though they typically transform the castle itself into romantically lavish visions of a High Middle Ages palace. Some writers of the "realist" strain of modern Arthurian fiction have attempted a more sensible Camelot. Inspired by Alcock's Cadbury-Camelot excavation, some authors such as Marion Zimmer Bradley and Mary Stewart place their Camelots in that place and describe it accordingly.

Camelot lends its name to the musical "Camelot", which was adapted into a film of the same title, featuring the Castle of Coca, Segovia as Camelot. An Arthurian television series "Camelot" was also named after the castle, as were some other works including the video game "Camelot" and the comic book series "Camelot 3000". French television series "Kaamelott" presents a humorous alternative version of the Arthurian legend; Camelot Theme Park is a now-abandoned Arthurian theme park resort located in the English county of Lancashire.

In American contexts, Camelot is used to refer to the presidency of John F. Kennedy. In a 1963 "Life" interview, Jacqueline, his widow, referenced a line from the Lerner and Loewe musical to describe the Kennedy White House: "Don't let it be forgot, that once there was a spot, for one brief shining moment, that was known as Camelot." She indicated that it was one of Kennedy's favorite lyrics from the musical and added, "there'll be great Presidents again, but there'll never be another Camelot again. […] It will never be that way again."





</doc>
<doc id="7548" url="https://en.wikipedia.org/wiki?curid=7548" title="Contras">
Contras

The Contras were the various U.S.-backed and funded right-wing rebel groups that were active from 1979 to the early 1990s in opposition to the socialist Sandinista Junta of National Reconstruction junta in Nicaragua. Among the separate contra groups, the Nicaraguan Democratic Force (FDN) emerged as the largest by far. In 1987, virtually all contra organizations were united, at least nominally, into the Nicaraguan Resistance.

During their war against the Nicaraguan government, the Contras committed a large number of human rights violations and used terrorist tactics, carrying out more than 1300 terrorist attacks. These actions were frequently carried out systematically as a part of the strategy of the Contras. Supporters of the Contras tried to downplay these violations, particularly the Reagan administration in the US, which engaged in a campaign of white propaganda to alter public opinion in favor of the contras, while covertly encouraging the Contras to attack civilian targets.

From an early stage, the rebels received financial and military support from the United States government, and their military significance decisively depended on it. After US support was banned by Congress, the Reagan administration covertly continued it. These illegal activities culminated in the Iran–Contra affair.

The term "contra" comes from the Spanish "contra," which means "against" but in this case is short for , in English "the counter-revolution".

The Contras were not a monolithic group, but a combination of three distinct elements of Nicaraguan society:


The CIA and Argentine intelligence, seeking to unify the anti-Sandinista cause before initiating large-scale aid, persuaded 15 September Legion, the UDN and several former smaller groups to merge in September 1981 as the Nicaraguan Democratic Force ("Fuerza Democrática Nicaragüense", FDN). Although the FDN had its roots in two groups made up of former National Guardsmen (of the Somoza regime), its joint political directorate was led by businessman and former anti-Somoza activist Adolfo Calero Portocarrero. Edgar Chamorro later stated that there was strong opposition within the UDN against working with the Guardsmen and that the merging only took place because of insistence by the CIA.

Based in Honduras, Nicaragua's northern neighbor, under the command of former National Guard Colonel Enrique Bermúdez, the new FDN commenced to draw in other smaller insurgent forces in the north. Largely financed, trained, equipped, armed and organized by the U.S., it emerged as the largest and most active contra group.

In April 1982, Edén Pastora ("Comandante Cero"), one of the heroes in the fight against Somoza, organized the Sandinista Revolutionary Front (FRS) – embedded in the Democratic Revolutionary Alliance (ARDE) – and declared war on the Sandinista government. Himself a former Sandinista who had held several high posts in the government, he had resigned abruptly in 1981 and defected, believing that the newly found power had corrupted the Sandinista's original ideas. A popular and charismatic leader, Pastora initially saw his group develop quickly. He confined himself to operate in the southern part of Nicaragua; after a press conference he was holding on 30 May 1984 was bombed, he "voluntarily withdrew" from the contra struggle.

A third force, Misurasata, appeared among the Miskito, Sumo and Rama Amerindian peoples of Nicaragua's Atlantic coast, who in December 1981 found themselves in conflict with the authorities following the government's efforts to nationalize Indian land. In the course of this conflict, forced removal of at least 10,000 Indians to relocation centers in the interior of the country and subsequent burning of some villages took place. The Misurasata movement split in 1983, with the breakaway Misura group of Stedman Fagoth Muller allying itself more closely with the FDN, and the rest accommodating themselves with the Sandinistas: On 8 December 1984 a ceasefire agreement known as the Bogota Accord was signed by Misurasata and the Nicaraguan government. A subsequent autonomy statute in September 1987 largely defused Miskito resistance.

U.S. officials were active in attempting to unite the Contra groups. In June 1985 most of the groups reorganized as the United Nicaraguan Opposition (UNO), under the leadership of Adolfo Calero, Arturo Cruz and Alfonso Robelo, all originally supporters of the anti-Somoza revolution. After UNO's dissolution early in 1987, the Nicaraguan Resistance (RN) was organized along similar lines in May.

In front of the International Court of Justice, Nicaragua claimed that the contras were altogether a creation of the U.S. This claim was rejected. However, the evidence of a very close relationship between the contras and the United States was considered overwhelming and incontrovertible. The U.S. played a very large role in financing, training, arming, and advising the contras over a long period, and the contras only became capable of carrying out significant military operations as a result of this support.

The US government viewed the leftist Sandinistas as a threat to economic interests of American corporations in Nicaragua and to national security. US President Ronald Reagan stated in 1983 that "The defense of [the USA's] southern frontier" was at stake. "In spite of the Sandinista victory being declared fair, the United States continued to oppose the left-wing Nicaraguan government." and opposed its ties to Cuba and the Soviet Union. Ronald Reagan, who had assumed the American presidency in January 1981, accused the Sandinistas of importing Cuban-style socialism and aiding leftist guerrillas in El Salvador. The Reagan administration continued to view the Sandinistas as undemocratic despite the 1984 Nicaraguan elections being generally declared fair by foreign observers. Throughout the 1980s the Sandinista government was regarded as "Partly Free" by Freedom House, an organization financed by the U.S. government. 
On 4 January 1982, Reagan signed the top secret National Security Decision Directive 17 (NSDD-17), giving the CIA the authority to recruit and support the contras with $19 million in military aid. The effort to support the contras was one component of the Reagan Doctrine, which called for providing military support to movements opposing Soviet-supported, communist governments.

By December 1981, however, the United States had already begun to support armed opponents of the Sandinista government. From the beginning, the CIA was in charge. The arming, clothing, feeding and supervision of the contras became the most ambitious paramilitary and political action operation mounted by the agency in nearly a decade.

In the fiscal year 1984, the U.S. Congress approved $24 million in contra aid. However, since the contras failed to win widespread popular support or military victories within Nicaragua, opinion polls indicated that a majority of the U.S. public was not supportive of the contras, the Reagan administration lost much of its support regarding its contra policy within Congress after disclosure of CIA mining of Nicaraguan ports, and a report of the Bureau of Intelligence and Research commissioned by the State Department found Reagan's allegations about Soviet influence in Nicaragua "exaggerated", Congress cut off all funds for the contras in 1985 by the third Boland Amendment. The Boland Amendment had first been passed by Congress in December 1982. At this time, it only outlawed U.S. assistance to the contras "for the purpose of overthrowing the Nicaraguan government", while allowing assistance for other purposes. In October 1984, it was amended to forbid action by not only the Defense Department and the Central Intelligence Agency but all U.S. government agencies.

Nevertheless, the case for support of the contras continued to be made in Washington, D.C., by both the Reagan administration and The Heritage Foundation, which argued that support for the contras would counter Soviet influence in Nicaragua.

On 1 May 1985 President Reagan announced that his administration perceived Nicaragua to be "an unusual and extraordinary threat to the national security and foreign policy of the United States", and declared a "national emergency" and a trade embargo against Nicaragua to "deal with that threat". It "is now a given; it is true", the Washington Post declared in 1986, "the Sandinistas are communists of the Cuban or Soviet school"; that "The Reagan administration is right to take Nicaragua as a serious menace—to civil peace and democracy in Nicaragua and to the stability and security of the region"; that we must "fit Nicaragua back into a Central American mode" and "turn Nicaragua back toward democracy," and with the "Latin American democracies" "demand reasonable conduct by regional standard."

Soon after the embargo was established, Managua re-declared "a policy of nonalignment" and sought the aid of Western Europe, who were opposed to U.S. policy, to escape dependency on the Soviet Union. Since 1981 U.S. pressures had curtailed Western credit to and trade with Nicaragua, forcing the government to rely almost totally on the Eastern bloc for credit, other aid, and trade by 1985. In his 1997 study on U.S. low intensity warfare, Kermit D. Johnson, a former Chief of the U.S. Army Chaplains, contends that U.S. hostility toward the revolutionary government was motivated not by any concern for "national security", but rather by what the world relief organization Oxfam termed "the threat of a good example":

It was alarming that in just a few months after the Sandinista revolution, Nicaragua received international acclaim for its rapid progress in the fields of literacy and health. It was alarming that a socialist-mixed-economy state could do in a few short months what the Somoza dynasty, a U.S. client state, could not do in 45 years! It was truly alarming that the Sandinistas were intent on providing the very services that establish a government's political and moral legitimacy.

The government's program included increased wages, subsidized food prices, and expanded health, welfare, and education services. And though it nationalized Somoza's former properties, it preserved a private sector that accounted for between 50 and 60 percent of GDP.

The United States began to support Contra activities against the Sandinista government by December 1981, with the CIA at the forefront of operations. The CIA supplied the funds and the equipment, coordinated training programs, and provided intelligence and target lists. While the Contras had little military successes, they did prove adept at carrying out CIA guerrilla warfare strategies from training manuals which advised them to incite mob violence, "neutralize" civilian leaders and government officials and attack "soft targets" — including schools, health clinics and cooperatives. The agency added to the Contras' sabotage efforts by blowing up refineries and pipelines, and mining ports. Finally, according to former Contra leader Edgar Chamorro, CIA trainers also gave Contra soldiers large knives. "A commando knife [was given], and our people, everybody wanted to have a knife like that, to kill people, to cut their throats". In 1985 Newsweek published a series of photos taken by Frank Wohl, a conservative student admirer traveling with the Contras, entitled "Execution in the Jungle":

The victim dug his own grave, scooping the dirt out with his hands... He crossed himself. Then a contra executioner knelt and rammed a k-bar knife into his throat. A second enforcer stabbed at his jugular, then his abdomen. When the corpse was finally still, the contras threw dirt over the shallow grave — and walked away.

The CIA officer in charge of the covert war, Duane "Dewey" Clarridge, admitted to the House Intelligence Committee staff in a secret briefing in 1984 that the Contras were routinely murdering "civilians and Sandinista officials in the provinces, as well as heads of cooperatives, nurses, doctors and judges". But he claimed that this did not violate President Reagan's executive order prohibiting assassinations because the agency defined it as just 'killing'. "After all, this is war—a paramilitary operation," Clarridge said in conclusion. Edgar Chamorro explained the rationale behind this to a U.S. reporter. "Sometimes terror is very productive. This is the policy, to keep putting pressure until the people cry 'uncle'". The CIA manual for the Contras, "Tayacan", states that the Contras should gather the local population for a public tribunal to "shame, ridicule and humiliate" Sandinista officials to "reduce their influence". It also recommends gathering the local population to witness and take part in public executions. These types of activities continued throughout the war. After the signing of the Central American Peace Accord in August 1987, the year war related deaths and economic destruction reached its peak, the Contras eventually entered negotiations with the Sandinista government (1988), and the war began to deescalate.

By 1989 the US backed Contra war and economic isolation inflicted severe economic suffering on Nicaraguans. The US government knew that the Nicaraguans had been exhausted from the war, that had cost 30,865 lives, and that voters usually vote the incumbents out during economic decline. By the late 1980s Nicaragua's internal conditions had changed so radically that the US approach to the 1990 elections differed greatly from 1984. The Bush administration decided to promote an opposition victory and to denounce the country's electoral laws and procedures should there be a Sandinista victory. The United States, through the National Endowment for Democracy, organized a united opposition out of fourteen dissimilar microparties into the National Opposition Union (Unión Nacional Oppositora, UNO). It promoted their candidates including presidential nominee Violeta Chamorro who was received by President Bush at the White House. The US thus "micromanaged the opposition" and exerted massive external pressure on the electorate. The Contra war escalated over the year before the election. The US promised to end the war and the economic embargo should she win.

The UNO scored a decisive victory on 25 February 1990. Chamorro won with 55 percent of the presidential vote as compared to Ortega's 41 percent. Of 92 seats in the National Assembly, UNO gained 51, and the FSLN won 39. On 25 April 1990, Chamorro assumed presidency from Daniel Ortega.

With Congress blocking further contra aid, the Reagan administration sought to arrange funding and military supplies by means of third countries and private sources. Between 1984 and 1986, $34 million from third countries and $2.7 million from private sources were raised this way. The secret contra assistance was run by the National Security Council, with officer Lt. Col. Oliver North in charge. With the third-party funds, North created an organization called "The Enterprise", which served as the secret arm of the NSC staff and had its own airplanes, pilots, airfield, ship, operatives, and secret Swiss bank accounts. It also received assistance from personnel from other government agencies, especially from CIA personnel in Central America. This operation functioned, however, without any of the accountability required of U.S. government activities. The Enterprise's efforts culminated in the Iran–Contra Affair of 1986–1987, which facilitated contra funding through the proceeds of arms sales to Iran.

According to the London Spectator, U.S. journalists in Central America had long known that the CIA was flying in supplies to the Contras inside Nicaragua before the scandal broke. No journalist paid it any attention until the alleged CIA supply man, Eugene Hasenfus, was shot down and captured by the Nicaraguan army. Similarly, reporters neglected to investigate many leads indicating that Oliver North was running the Contra operation from his office in the National Security Council.

According to the National Security Archive, Oliver North had been in contact with Manuel Noriega, the military leader of Panama later convicted on drug charges, whom he personally met. The issue of drug money and its importance in funding the Nicaraguan conflict was the subject of various reports and publications. The contras were funded by drug trafficking, of which the United States was aware. Senator John Kerry's 1988 Committee on Foreign Relations report on Contra drug links concluded that "senior U.S. policy makers were not immune to the idea that drug money was a perfect solution to the Contras' funding problems".

The Reagan administration's support for the Contras continued to stir controversy well into the 1990s. In August 1996, "San Jose Mercury News" reporter Gary Webb published a series titled "Dark Alliance", alleging that the contras contributed to the rise of crack cocaine in California.

Gary Webb's career as a journalist was subsequently discredited by the leading U.S. papers, the New York Times, the Washington Post, and the LA Times, in collusion with the Central Intelligence Agency. An internal CIA report, entitled, "Managing a Nightmare", shows the agency used "a ground base of already productive relations with journalists" to counter what it called "a genuine public relations crisis." In the 1980s, Douglas Farah worked as a journalist, covering the civil wars in Central America for the Washington Post. According to Farah, while it was common knowledge that the Contras were involved in cocaine trafficking, the editors of the Washington Post refused to take it seriously:

If you’re talking about our intelligence community tolerating — if not promoting — drugs to pay for black ops, it’s rather an uncomfortable thing to do when you’re an establishment paper like the Post. If you were going to be directly rubbing up against the government, they wanted it more solid than it could probably ever be done.

An investigation by the United States Department of Justice also stated that their "review did not substantiate the main allegations stated and implied in the Mercury News articles." Regarding the specific charges towards the CIA, the DOJ wrote "the implication that the drug trafficking by the individuals discussed in the "Mercury News" articles was connected to the CIA was also not supported by the facts." The CIA also investigated and rejected the allegations.

During the time the US Congress blocked funding for the contras, the Reagan government engaged in a campaign to alter public opinion and change the vote in Congress on contra aid. For this purpose, the NSC established an interagency working group, which in turn coordinated the Office of Public Diplomacy for Latin America and the Caribbean (managed by Otto Reich), which conducted the campaign. The S/LPD produced and widely disseminated a variety of pro-contra publications, arranged speeches and press conferences. It also disseminated "white propaganda"—pro-contra newspaper articles by paid consultants who did not disclose their connection to the Reagan administration.

On top of that, Oliver North helped Carl Channell's tax-exempt organization, the National Endowment for the Preservation of Liberty, to raise $10 million, by arranging numerous briefings for groups of potential contributors at the premises of the White House and by facilitating private visits and photo sessions with President Reagan for major contributors. Channell in turn, used part of that money to run a series of television advertisements directed at home districts of Congressmen considered swing votes on contra aid. Out of the $10 million raised, more than $1 million was spent on pro-contra publicity.

In 1984 the Sandinista government filed a suit in the International Court of Justice (ICJ) against the United States ("Nicaragua v. United States"), which resulted in a 1986 judgment against the United States. The ICJ held that the U.S. had violated international law by supporting the contras in their rebellion against the Nicaraguan government and by mining Nicaragua's harbors. Regarding the alleged human rights violations by the contras, however, the ICJ took the view that the United States could only be held accountable for them if it would have been proven that the U.S. had effective control of the contra operations resulting in these alleged violations. Nevertheless, the ICJ found that the U.S. encouraged acts contrary to general principles of humanitarian law by producing the manual "Psychological Operations in Guerrilla Warfare (Operaciones sicológicas en guerra de guerrillas") and disseminating it to the contras. The manual, amongst other things, advised on how to rationalize killings of civilians and recommended to hire professional killers for specific selective tasks.

The United States, which did not participate in the merits phase of the proceedings, maintained that the ICJ's power did not supersede the Constitution of the United States and argued that the court did not seriously consider the Nicaraguan role in El Salvador, while it accused Nicaragua of actively supporting armed groups there, specifically in the form of supply of arms. The ICJ had found that evidence of a responsibility of the Nicaraguan government in this matter was insufficient. The U.S. argument was affirmed, however, by the dissenting opinion of ICJ member U.S. Judge Schwebel, who concluded that in supporting the contras, the United States acted lawfully in collective self-defence in El Salvador's support. The U.S. blocked enforcement of the ICJ judgment by the United Nations Security Council and thereby prevented Nicaragua from obtaining any actual compensation. The Nicaraguan government finally withdrew the complaint from the court in September 1992 (under the later, post-FSLN, government of Violeta Chamorro), following a repeal of the law requiring the country to seek compensation.

Americas Watch – which subsequently became part of Human Rights Watch – accused the Contras of:

Human Rights Watch released a report on the situation in 1989, which stated: "[The] contras were major and systematic violators of the most basic standards of the laws of armed conflict, including by launching indiscriminate attacks on civilians, selectively murdering non-combatants, and mistreating prisoners."

In his affidavit to the World Court, former contra Edgar Chamorro testified that "The CIA did not discourage such tactics. To the contrary, the Agency severely criticized me when I admitted to the press that the FDN had regularly kidnapped and executed agrarian reform workers and civilians. We were told that the only way to defeat the Sandinistas was to...kill, kidnap, rob and torture..."

Contra leader Adolfo Calero denied that his forces deliberately targeted civilians: "What they call a cooperative is also a troop concentration full of armed people. We are not killing civilians. We are fighting armed people and returning fire when fire is directed at us."

U.S. news media published several articles accusing Americas Watch and other bodies of ideological bias and unreliable reporting. It alleged that Americas Watch gave too much credence to alleged Contra abuses and systematically tried to discredit Nicaraguan human rights groups such as the Permanent Commission on Human Rights, which blamed the major human rights abuses on the Contras.

In 1985, the "Wall Street Journal" reported:
Human Rights Watch, the umbrella organization of Americas Watch, replied to these allegations: "Almost invariably, U.S. pronouncements on human rights exaggerated and distorted the real human rights violations of the Sandinista regime, and exculpated those of the U.S.-supported insurgents, known as the contras...The Bush administration is responsible for these abuses, not only because the contras are, for all practical purposes, a U.S. force, but also because the Bush administration has continued to minimize and deny these violations, and has refused to investigate them seriously."

By 1986 the contras were besieged by charges of corruption, human-rights abuses, and military ineptitude. A much-vaunted early 1986 offensive never materialized, and Contra forces were largely reduced to isolated acts of terrorism. In October 1987, however, the contras staged a successful attack in southern Nicaragua. Then on 21 December 1987, the FDN launched attacks at La Bonanza, La Siuna, and La Rosita in Zelaya province, resulting in heavy fighting. ARDE Frente Sur attacked at El Almendro and along the Rama road. These large-scale raids mainly became possible as the contras were able to use U.S.-provided Redeye missiles against Sandinista Mi-24 helicopter gunships, which had been supplied by the Soviets. Nevertheless, the Contras remained tenuously encamped within Honduras and were not able to hold Nicaraguan territory.

There were isolated protests among the population against the draft implemented by the Sandinista government, which even resulted in full-blown street clashes in Masaya in 1988. However, a June 1988 survey in Managua showed the Sandinista government still enjoyed strong support but that support had declined since 1984. Three times as many people identified with the Sandinistas (28%) than with all the opposition parties put together (9%). 59% did not identify with any political party. Of those polled, 85% opposed any further US aid to the Contras, 40% believed the Sandinista government to be democratic while 48% believed it to be not democratic. People identified the war as the largest problem but were less likely to blame it for economic problems compared to a December 1986 poll. 19% blamed the war and US blockade as the main cause of economic problems while 10% blamed the government. Political opposition groups were splintered and the Contras began to experience defections, although United States aid maintained them as a viable military force.

After a cutoff in U.S. military support, and with both sides facing international pressure to bring an end to the conflict, the contras agreed to negotiations with the FSLN. With the help of five Central American Presidents, including Ortega, the sides agreed that a voluntary demobilization of the contras should start in early December 1989. They chose this date to facilitate free and fair elections in Nicaragua in February 1990 (even though the Reagan administration had pushed for a delay of contra disbandment).

In the resulting February 1990 elections, Violeta Chamorro and her party the UNO won an upset victory of 55% to 41% over Daniel Ortega, even though polls leading up to the election had clearly indicated an FSLN victory.

Possible explanations include that the Nicaraguan people were disenchanted with the Ortega regime as well as the fact that already in November 1989, the White House had announced that the economic embargo against Nicaragua would continue unless Violeta Chamorro won. Also, there had been reports of intimidation from the side of the contras, with a Canadian observer mission claiming that 42 people were killed by the contras in "election violence" in October 1989. This led many commentators to conclude that Nicaraguans voted against the Sandinistas out of fear of a continuation of the contra war and economic deprivation.





</doc>
<doc id="7550" url="https://en.wikipedia.org/wiki?curid=7550" title="Craig Venter">
Craig Venter

John Craig Venter (born October 14, 1946) is an American biotechnologist, biochemist, geneticist, and businessman. He is known for being involved with sequencing the second human genome and assembled the first team to transfect a cell with a synthetic chromosome. Venter founded Celera Genomics, The Institute for Genomic Research (TIGR) and the J. Craig Venter Institute (JCVI). He was the co-founder of Human Longevity Inc., served as its CEO until 2017, and is executive chairman of the board of directors. He was listed on "Time" magazine's 2007 and 2008 Time 100 list of the most influential people in the world. In 2010, the British magazine "New Statesman" listed Craig Venter at 14th in the list of "The World's 50 Most Influential Figures 2010". He is a member of the USA Science and Engineering Festival's Advisory Board.

Venter was born in Salt Lake City, Utah, the son of Elizabeth and John Venter. In his youth, he did not take his education seriously, preferring to spend his time on the water in boats or surfing.

Although he was against the Vietnam War, Venter was drafted and enlisted in the United States Navy where he worked in the intensive-care ward of a field hospital. While in Vietnam, he attempted suicide by swimming out to sea, but as he got deeper into the sea and was approaching the circling of a shark, he changed his mind more than a mile out.
Being confronted with wounded, maimed, and dying [marines] on a daily basis instilled in him a desire to study medicine — although he later switched to biomedical research.

Venter began his college education at a community college, College of San Mateo in California, and later transferred to the University of California, San Diego, where he studied under biochemist Nathan O. Kaplan. He received a BS in biochemistry in 1972 and a PhD in physiology and pharmacology in 1975 from UCSD.

After working as an associate professor, and later as full professor, at the State University of New York at Buffalo, he joined the National Institutes of Health in 1984.

While an employee of the NIH, Venter learned how to identify mRNA and began to learn more about those expressed in the human brain. The short cDNA sequence fragments he was interested in are called expressed sequence tags, or ESTs. The NIH Office of Technology Transfer and Venter decided to take the ESTs discovered by others in an attempt to patent the genes identified based on studies of mRNA expression in the human brain. When Venter disclosed this strategy during a Congressional hearing, a firestorm of controversy erupted. The NIH later stopped the effort and abandoned the patent applications it had filed, following public outcry.

Venter was passionate about the power of genomics to radically transform healthcare. Venter believed that shotgun sequencing was the fastest and most effective way to get useful human genome data. The method was rejected by the Human Genome Project however, since some geneticists felt it would not be accurate enough for a genome as complicated as that of humans, that it would be logistically more difficult, and that it would cost significantly more.

Venter viewed the slow pace of progress in the Human Genome project as an opportunity to continue his interest in patenting genes, so he sought funding from the private sector to birth Celera Genomics. The company planned to profit from their work by creating genomic data to which users could subscribe for a fee. The goal consequently put pressure on the public genome program and spurred several groups to redouble their efforts to produce the full sequence. Venter's effort to publish a draft genome of his own DNA won him renown as the second person to sequence the human genome after the public effort, thus making it impossible to patent any of it.

In 2000, Venter and Francis Collins of the National Institutes of Health and U.S. Public Genome Project jointly made the announcement of the mapping of the human genome, a full three years ahead of the expected end of the Public Genome Program. The announcement was made along with U.S. President Bill Clinton, and UK Prime Minister Tony Blair. Venter and Collins thus shared an award for "Biography of the Year" from A&E Network.
On the 15 February 2001, the Human Genome Project consortium published the first Human Genome in the journal Nature, and was followed, one day later, by a Celera publication in Science. Despite some claims that shotgun sequencing was in some ways less accurate than the clone-by-clone method chosen by the Human Genome Project, the technique became widely accepted by the scientific community.

Venter was fired by Celera in early 2002. According to his biography, Venter was fired due to a conflict with the main investor, Tony White, specifically barring him from attending the White House ceremony celebrating the achievement of sequencing the human genome.

The Global Ocean Sampling Expedition (GOS) is an ocean exploration genome project with the goal of assessing the genetic diversity in marine microbial communities and to understand their role in nature's fundamental processes. Begun as a Sargasso Sea pilot sampling project in August 2003, Venter announced the full Expedition on 4 March 2004. The project, which used Venter's personal yacht, "Sorcerer II", started in Halifax, Canada, circumnavigated the globe and returned to the U.S. in January 2006.

Venter is currently the president of the J. Craig Venter Institute, which conducts research in synthetic biology. In June 2005, he co-founded Synthetic Genomics, a firm dedicated to using modified microorganisms to produce clean fuels and biochemicals. In July 2009, ExxonMobil announced a $600 million collaboration with Synthetic Genomics to research and develop next-generation biofuels. 
Venter continues to work on the creation of engineered diatomic microalgae for the production of biofuels.

Venter is seeking to patent the first partially synthetic species possibly to be named "Mycoplasma laboratorium". There is speculation that this line of research could lead to producing bacteria that have been engineered to perform specific reactions, for example, produce fuels, make medicines, combat global warming, and so on.

In May 2010, a team of scientists led by Venter became the first to successfully create what was described as "synthetic life". This was done by synthesizing a very long DNA molecule containing an entire bacterium genome, and introducing this into another cell, analogous to the accomplishment of Eckard Wimmer's group, who synthesized and ligated an RNA virus genome and "booted" it in cell lysate. The single-celled organism contains four "watermarks"
written into its DNA to identify it as synthetic and to help trace its descendants. The watermarks include 

On March 25, 2016 Venter reported the creation of Syn 3.0, a synthetic genome having the fewest genes of any freely living organism (473 genes). Their aim was to strip away all nonessential genes, leaving only the minimal set necessary to support life.
This stripped-down, fast reproducing cell is expected to be a valuable tool for researchers in the field.

On September 4, 2007, a team led by Sam Levy published the first complete (six-billion-letter) genome of an individual human—Venter's own DNA sequence. Some of the sequences in Venter's genome are associated with wet earwax, increased risk of antisocial behavior, Alzheimer's and cardiovascular diseases. This publication was especially interesting since it contained a diploid instead of a haploid genome and shows promise for personalized medicine via genotyping. This genome, dubbed HuRef by Levy and others, was a landmark accomplishment.

The Human Reference Genome Browser is a web application for the navigation and analysis of Venter's recently published genome. The HuRef database consists of approximately 32 million DNA reads sequenced using microfluidic Sanger sequencing, assembled into 4,528 scaffolds and 4.1 million DNA variations identified by genome analysis. These variants include single-nucleotide polymorphisms (SNPs), block substitutions, short and large indels, and structural variations like insertions, deletions, inversions and copy number changes.

The browser enables scientists to navigate the HuRef genome assembly and sequence variations, and to compare it with the NCBI human build 36 assembly in the context of the NCBI and Ensembl annotations. The browser provides a comparative view between NCBI and HuRef consensus sequences, the sequence multi-alignment of the HuRef assembly, Ensembl and dbSNP annotations, HuRef variants, and the underlying variant evidence and functional analysis. The interface also represents the haplotype blocks from which diploid genome sequence can be inferred and the relation of variants to gene annotations. The display of variants and gene annotations are linked to external public resources including dbSNP, Ensembl, Online Mendelian Inheritance in Man (OMIM) and Gene Ontology (GO).

Users can search the HuRef genome using HUGO gene names, Ensembl and dbSNP identifiers, HuRef contig or scaffold locations, or NCBI chromosome locations. Users can then easily and quickly browse any genomic region via the simple and intuitive pan and zoom controls; furthermore, data relevant to specific loci can be exported for further analysis.

On March 4, 2014 Venter and co-founders Peter Diamandis and Robert Hariri announced the formation of Human Longevity, Inc., a company focused on extending the healthy, "high performance" human lifespan. At the time of the announcement the company had already raised $70 million in venture financing, which was expected to last 18 months. Venter served as the chairman and chief executive officer (CEO) until January 2017, when he stepped aside to become executive chairman of the board of directors. The company said that it plans to sequence 40,000 genomes per year, with an initial focus on cancer genomes and the genomes of cancer patients.

Human Longevity's mission is to extend healthy human lifespan by the use of high-resolution big data diagnostics from genomics, metabolomics, microbiomics, and proteomics, and the use of stem cell therapy.

Venter is the author of two books, the first of which was ostensibly an autobiography titled "A Life Decoded". Venter's second book was titled "Life at the Speed of Light" in which he announced his theory that this is the generation in which there appears to be a dovetailing of the two previously diverse fields of science represented by computer programming and the genetic programming of life by DNA sequencing. He was applauded for his position on this by futurist Ray Kurzweil.

After a brief marriage to Barbara Rae-Venter, with whom he had a son, Christopher, he married his student, Claire M. Fraser, remaining married to her until 2005. In late 2008 he married Heather Kowalski. They live in the La Jolla neighborhood of San Diego, CA where Venter gut-renovated a $6 million home.

Venter is an atheist.

Venter himself recognized his own ADHD behavior in his adolescence, and later found ADHD-linked genes in his own DNA.

Venter appeared in the "Evolution" episode of the documentary television series "Understanding" in 2002.

Venter was featured on "The Colbert Report" on February 27, 2007, and again on October 30, 2007.

On December 4, 2007, Venter gave the Dimbleby lecture for the BBC in London.

In February 2008, he gave a speech about his current work at the TED conference.

Venter was interviewed on his boat by BBC One for the first episode of the TV show "Bang Goes the Theory", which aired on July 27, 2009.

On November 21, 2010 Steve Kroft profiled Venter and his research on "60 Minutes".

Venter appears in the 2-hour 2001 "NOVA" special, "Cracking the code of life".

Venter has been the subject of articles in several magazines, including "Wired", "The Economist", Australian science magazine "Cosmos", and "The Atlantic".

On May 16, 2004, Venter gave the commencement speech at Boston University.

In a 2007 interview with "New Scientist", when asked "Assuming you can make synthetic bacteria, what will you do with them?", Venter replied: "Over the next 20 years, synthetic genomics is going to become the standard for making anything. The chemical industry will depend on it. Hopefully, a large part of the energy industry will depend on it. We really need to find an alternative to taking carbon out of the ground, burning it, and putting it into the atmosphere. That is the single biggest contribution I could make."

He was on the 2007 Time 100 most influential people in the world list made by "Time" magazine. In 2007 he also received the Golden Eurydice Award for contributions to biophilosophy.

Venter delivered the 2008 convocation speech for Faculty of Science honours and specialization students at the University of Alberta. A transcription of the speech is available here.

Venter was featured in "Time" magazine's "The Top 10 Everything of 2008" article. Number three in 2008's Top 10 Scientific Discoveries was a piece outlining his work stitching together the 582,000 base pairs necessary to invent the genetic information for a whole new bacterium.

On May 20, 2010, Venter announced the creation of first self-replicating semi-synthetic bacterial cell.

In the June 2011 issue of "Men's Journal", Venter was featured as the "Survival Skills" celebrity of the month. He shared various anecdotes and advice, including stories of his time in Vietnam, as well as mentioning a bout with melanoma on his back, which subsequently resulted in his "giving a pound of flesh" to surgery.

Venter is mentioned in the season finale episode of the first season of the science fiction series "Orphan Black", a joint production of Space and BBC America. 
Venter has been a keynote speaker at the Congress of Future Medical Leaders (2014, 2015, 2016) and the Congress of Future Science and Technology Leaders (2015).


Venter has authored over 200 publications in scientific journals.






</doc>
<doc id="7552" url="https://en.wikipedia.org/wiki?curid=7552" title="Chemical evolution">
Chemical evolution

Chemical evolution may refer to:


</doc>
<doc id="7554" url="https://en.wikipedia.org/wiki?curid=7554" title="Carl Rogers">
Carl Rogers

Carl Ransom Rogers (January 8, 1902 – February 4, 1987) was an American psychologist and among the founders of the humanistic approach (or client-centered approach) to psychology. Rogers is widely considered to be one of the founding fathers of psychotherapy research and was honored for his pioneering research with the Award for Distinguished Scientific Contributions by the American Psychological Association (APA) in 1956.

The person-centered approach, his own unique approach to understanding personality and human relationships, found wide application in various domains such as psychotherapy and counseling (client-centered therapy), education (student-centered learning), organizations, and other group settings. For his professional work he was bestowed the Award for Distinguished Professional Contributions to Psychology by the APA in 1972. In a study by Steven J. Haggbloom and colleagues using six criteria such as citations and recognition, Rogers was found to be the sixth most eminent psychologist of the 20th century and second, among clinicians, only to Sigmund Freud.

Rogers was born on January 8, 1902, in Oak Park, Illinois, a suburb of Chicago. His father, Walter A. Rogers, was a civil engineer, a Congregationalist by denomination. His mother, Julia M. Cushing, was a homemaker and devout Baptist. The Congregationalists and the Baptists tended to be Calvinistic and Fundamentalists at the time. Carl was the fourth of their six children.

Rogers was intelligent and could read well before kindergarten. Following an education in a strict religious and ethical environment as an altar boy at the vicarage of Jimpley, he became a rather isolated, independent and disciplined person, and acquired a knowledge and an appreciation for the scientific method in a practical world. His first career choice was agriculture, at the University of Wisconsin–Madison, where he was a part of the fraternity of Alpha Kappa Lambda, followed by history and then religion. At age 20, following his 1922 trip to Peking, China, for an international Christian conference, he started to doubt his religious convictions. To help him clarify his career choice, he attended a seminar entitled "Why am I entering the Ministry?", after which he decided to change his career. In 1924, he graduated from University of Wisconsin and enrolled at Union Theological Seminary (New York City). He later became an atheist.

After two years he left the seminary to attend Teachers College, Columbia University, obtaining an MA in 1928 and a PhD in 1931. While completing his doctoral work, he engaged in child study. In 1930, Rogers served as director of the Society for the Prevention of Cruelty to Children in Rochester, New York. From 1935 to 1940 he lectured at the University of Rochester and wrote "The Clinical Treatment of the Problem Child" (1939), based on his experience in working with troubled children. He was strongly influenced in constructing his client-centered approach by the post-Freudian psychotherapeutic practice of Otto Rank, especially as embodied in the work of Rank's disciple, noted clinician and social work educator Jessie Taft. In 1940 Rogers became professor of clinical psychology at Ohio State University, where he wrote his second book, "Counseling and Psychotherapy" (1942). In it, Rogers suggested that the client, by establishing a relationship with an understanding, accepting therapist, can resolve difficulties and gain the insight necessary to restructure their life.

In 1945, he was invited to set up a counseling center at the University of Chicago. In 1947 he was elected President of the American Psychological Association. While a professor of psychology at the University of Chicago (1945–57), Rogers helped to establish a counseling center connected with the university and there conducted studies to determine the effectiveness of his methods. His findings and theories appeared in "Client-Centered Therapy" (1951) and "Psychotherapy and Personality Change" (1954). One of his graduate students at the University of Chicago, Thomas Gordon, established the Parent Effectiveness Training (P.E.T.) movement. Another student, Eugene T. Gendlin, who was getting his Ph.D. in philosophy, developed the practice of Focusing based on Rogerian listening. In 1956, Rogers became the first President of the American Academy of Psychotherapists. He taught psychology at the University of Wisconsin, Madison (1957–63), during which time he wrote one of his best-known books, "On Becoming a Person" (1961). Carl Rogers and Abraham Maslow (1908–70) pioneered a movement called humanistic psychology which reached its peak in the 1960s. In 1961, he was elected a Fellow of the American Academy of Arts and Sciences. Carl Rogers was also one of the people who questioned the rise of McCarthyism in 1950s. Through articles, he criticized society for its backward-looking affinities.

Rogers continued teaching at University of Wisconsin until 1963, when he became a resident at the new Western Behavioral Sciences Institute (WBSI) in La Jolla, California. Rogers left the WBSI to help found the Center for Studies of the Person in 1968. His later books include "Carl Rogers on Personal Power" (1977) and "Freedom to Learn for the 80's" (1983). He remained a resident of La Jolla for the rest of his life, doing therapy, giving speeches and writing until his sudden death in 1987. In 1987, Rogers suffered a fall that resulted in a fractured pelvis: he had life alert and was able to contact paramedics. He had a successful operation, but his pancreas failed the next night and he died a few days later after heart attack. 

Rogers's last years were devoted to applying his theories in situations of political oppression and national social conflict, traveling worldwide to do so. In Belfast, Northern Ireland, he brought together influential Protestants and Catholics; in South Africa, blacks and whites; in Brazil people emerging from dictatorship to democracy; in the United States, consumers and providers in the health field. His last trip, at age 85, was to the Soviet Union, where he lectured and facilitated intensive experiential workshops fostering communication and creativity. He was astonished at the numbers of Russians who knew of his work.

Together with his daughter, Natalie Rogers, and psychologists Maria Bowen, Maureen O'Hara, and John K. Wood, between 1974 and 1984, Rogers convened a series of residential programs in the US, Europe, Brazil and Japan, the Person-Centered Approach Workshops, which focused on cross-cultural communications, personal growth, self-empowerment, and learning for social change.

Rogers' theory of the self is considered to be humanistic, existential, and phenomenological. His theory is based directly on the "phenomenal field" personality theory of Combs and Snygg (1949). Rogers' elaboration of his own theory is extensive. He wrote 16 books and many more journal articles describing it. Prochaska and Norcross (2003) states Rogers "consistently stood for an empirical evaluation of psychotherapy. He and his followers have demonstrated a humanistic approach to conducting therapy and a scientific approach to evaluating therapy need not be incompatible."

His theory (as of 1953) was based on 19 propositions:

In relation to No. 17, Rogers is known for practicing "unconditional positive regard", which is defined as accepting a person "without negative judgment of ... [a person's] basic worth".

With regard to development, Rogers described principles rather than stages. The main issue is the development of a self-concept and the progress from an undifferentiated self to being fully differentiated.

In the development of the self-concept, he saw conditional and unconditional positive regard as key. Those raised in an environment of unconditional positive regard have the opportunity to fully actualize themselves. Those raised in an environment of conditional positive regard feel worthy only if they match conditions (what Rogers describes as "conditions of worth") that have been laid down for them by others.

Optimal development, as referred to in proposition 14, results in a certain process rather than static state. He describes this as "the good life", where the organism continually aims to fulfill its full potential. He listed the characteristics of a fully functioning person (Rogers 1961):

Rogers identified the "real self" as the aspect of one's being that is founded in the actualizing tendency, follows organismic valuing, needs and receives positive regard and self-regard. It is the "you" that, if all goes well, you will become. On the other hand, to the extent that our society is out of sync with the actualizing tendency, and we are forced to live with conditions of worth that are out of step with organismic valuing, and receive only conditional positive regard and self-regard, we develop instead an "ideal self". By ideal, Rogers is suggesting something not real, something that is always out of our reach, the standard we cannot meet. This gap between the real self and the ideal self, the "I am" and the "I should" is called "incongruity".

Rogers described the concepts of "congruence" and "incongruence" as important ideas in his theory. In proposition #6, he refers to the actualizing tendency. At the same time, he recognized the need for "positive regard". In a fully congruent person realizing their potential is not at the expense of experiencing positive regard. They are able to lead lives that are authentic and genuine. Incongruent individuals, in their pursuit of positive regard, lead lives that include falseness and do not realize their potential. Conditions put on them by those around them make it necessary for them to forgo their genuine, authentic lives to meet with the approval of others. They live lives that are not true to themselves, to who they are on the inside out.

Rogers suggested that the incongruent individual, who is always on the defensive and cannot be open to all experiences, is not functioning ideally and may even be malfunctioning. They work hard at maintaining/protecting their self-concept. Because their lives are not authentic this is a difficult task and they are under constant threat. They deploy "defense mechanisms" to achieve this. He describes two mechanisms: "distortion" and "denial". Distortion occurs when the individual perceives a threat to their self-concept. They distort the perception until it fits their self-concept.

This defensive behavior reduces the consciousness of the threat but not the threat itself. And so, as the threats mount, the work of protecting the self-concept becomes more difficult and the individual becomes more defensive and rigid in their self structure. If the incongruence is immoderate this process may lead the individual to a state that would typically be described as neurotic. Their functioning becomes precarious and psychologically vulnerable. If the situation worsens it is possible that the defenses cease to function altogether and the individual becomes aware of the incongruence of their situation. Their personality becomes disorganised and bizarre; irrational behavior, associated with earlier denied aspects of self, may erupt uncontrollably.

Rogers originally developed his theory to be the foundation for a system of therapy. He initially called this "non-directive therapy" but later replaced the term "non-directive" with the term "client-centered" and then later used the term "person-centered". Even before the publication of "Client-Centered Therapy" in 1951, Rogers believed that the principles he was describing could be applied in a variety of contexts and not just in the therapy situation. As a result, he started to use the term "person-centered approach" later in his life to describe his overall theory. Person-centered therapy is the application of the person-centered approach to the therapy situation. Other applications include a theory of personality, interpersonal relations, education, nursing, cross-cultural relations and other "helping" professions and situations. In 1946 Rogers co-authored "Counseling with Returned Servicemen" with John L. Wallen (the creator of the behavioral model known as "The Interpersonal Gap"), documenting the application of person-centered approach to counseling military personnel returning from the second world war.

The first empirical evidence of the effectiveness of the client-centered approach was published in 1941 at the Ohio State University by Elias Porter, using the recordings of therapeutic sessions between Carl Rogers and his clients. Porter used Rogers' transcripts to devise a system to measure the degree of directiveness or non-directiveness a counselor employed. The attitude and orientation of the counselor were demonstrated to be instrumental in the decisions made by the client.

The application to education has a large robust research tradition similar to that of therapy with studies having begun in the late 1930s and continuing today (Cornelius-White, 2007). Rogers described the approach to education in "Client-Centered Therapy" and wrote "Freedom to Learn" devoted exclusively to the subject in 1969. "Freedom to Learn" was revised two times. The new Learner-Centered Model is similar in many regards to this classical person-centered approach to education.
Rogers and Harold Lyon began a book prior to Rogers death, entitled "On Becoming an Effective Teacher—Person-centered Teaching, Psychology, Philosophy, and Dialogues with Carl R. Rogers and Harold Lyon", which was completed by Lyon and Reinhard Tausch and published in 2013 containing Rogers last unpublished writings on person-centered teaching. Rogers had the following five hypotheses regarding learner-centered education:

In 1970, Richard Young, Alton L. Becker, and Kenneth Pike published "Rhetoric: Discovery and Change", a widely influential college writing textbook that used a Rogerian approach to communication to revise the traditional Aristotelian framework for rhetoric. The Rogerian method of argument involves each side restating the other's position to the satisfaction of the other. In a paper, it can be expressed by carefully acknowledging and understanding the opposition, rather than dismissing them.

The application to cross-cultural relations has involved workshops in highly stressful situations and global locations including conflicts and challenges in South Africa, Central America, and Ireland. Along with Alberto Zucconi and Charles Devonshire, he co-founded the Istituto dell'Approccio Centrato sulla Persona (Person-Centered Approach Institute) in Rome, Italy.

His international work for peace culminated in the Rust Peace Workshop which took place in November 1985 in Rust, Austria. Leaders from 17 nations convened to discuss the topic "The Central America Challenge". The meeting was notable for several reasons: it brought national figures together as people (not as their positions), it was a private event, and was an overwhelming positive experience where members heard one another and established real personal ties, as opposed to stiffly formal and regulated diplomatic meetings.

Some scholars believe there is a politics implicit in Rogers's approach to psychotherapy. Toward the end of his life, Rogers came to that view himself. The central tenet of a Rogerian, person-centered politics is that public life does not have to consist of an endless series of winner-take-all battles among sworn opponents; rather, it can and should consist of an ongoing dialogue among all parties. Such dialogue would be characterized by respect among the parties, authentic speaking by each party, and – ultimately – empathic understanding among all parties. Out of such understanding, mutually acceptable solutions would (or at least could) flow.

During his last decade, Rogers facilitated or participated in a wide variety of dialogic activities among politicians, activists, and other social leaders, often outside the U.S. In addition, he lent his support to several non-traditional U.S. political initiatives, including the "12-Hour Political Party" of the Association for Humanistic Psychology and the founding of a "transformational" political organization, the New World Alliance. By the 21st century, interest in dialogic approaches to political engagement and change had become widespread, especially among academics and activists. Theorists of a specifically Rogerian, person-centered approach to politics as dialogue have made substantial contributions to that project.

Carl Rogers served on the board of the Human Ecology Fund from the late 50s into the 60s, which was a CIA-funded organization that provided grants to researchers looking into personality. In addition, he and other people in the field of personality and psychotherapy were given a lot of information about Khrushchev. 'We were asked to figure out what we thought of him and what would be the best way of dealing with him. And that seemed to be an entirely principled and legitimate aspect. I don't think we contributed very much, but, anyway, we tried.' ".






</doc>
<doc id="7555" url="https://en.wikipedia.org/wiki?curid=7555" title="Casimir effect">
Casimir effect

In quantum field theory, the Casimir effect and the Casimir–Polder force are physical forces arising from a quantized field. They are named after the Dutch physicist Hendrik Casimir who predicted them in 1948.

The Casimir effect can be understood by the idea that the presence of conducting metals and dielectrics alters the vacuum expectation value of the energy of the second quantized electromagnetic field. Since the value of this energy depends on the shapes and positions of the conductors and dielectrics, the Casimir effect manifests itself as a force between such objects.
Any medium supporting oscillations has an analogue of the Casimir effect. For example, beads on a string as well as plates submerged in noisy water or gas illustrate the Casimir force.

In modern theoretical physics, the Casimir effect plays an important role in the chiral bag model of the nucleon; in applied physics, it is significant in some aspects of emerging microtechnologies and nanotechnologies.

The typical example is of the two uncharged conductive plates in a vacuum, placed a few nanometers apart. In a classical description, the lack of an external field means that there is no field between the plates, and no force would be measured between them. When this field is instead studied using the quantum electrodynamic vacuum, it is seen that the plates do affect the virtual photons which constitute the field, and generate a net force – either an attraction or a repulsion depending on the specific arrangement of the two plates. Although the Casimir effect can be expressed in terms of virtual particles interacting with the objects, it is best described and more easily calculated in terms of the zero-point energy of a quantized field in the intervening space between the objects. This force has been measured and is a striking example of an effect captured formally by second quantization.

The treatment of boundary conditions in these calculations has led to some controversy. In fact, "Casimir's original goal was to compute the van der Waals force between polarizable molecules" of the conductive plates. Thus it can be interpreted without any reference to the zero-point energy (vacuum energy) of quantum fields.

Because the strength of the force falls off rapidly with distance, it is measurable only when the distance between the objects is extremely small. On a submicron scale, this force becomes so strong that it becomes the dominant force between uncharged conductors. In fact, at separations of 10 nm – about 100 times the typical size of an atom – the Casimir effect produces the equivalent of about 1 atmosphere of pressure (the precise value depending on surface geometry and other factors).

Dutch physicists Hendrik Casimir and Dirk Polder at Philips Research Labs proposed the existence of a force between two polarizable atoms and between such an atom and a conducting plate in 1947, this special form is called the Casimir–Polder force. After a conversation with Niels Bohr, who suggested it had something to do with zero-point energy, Casimir alone formulated the theory predicting a force between neutral conducting plates in 1948 which is called the Casimir effect in the narrow sense.

Predictions of the force were later extended to finite-conductivity metals and dielectrics, and recent calculations have considered more general geometries. Experiments before 1997 had observed the force qualitatively, and indirect validation of the predicted Casimir energy had been made by measuring the thickness of liquid helium films. However it was not until 1997 that a direct experiment by S. Lamoreaux quantitatively measured the force to within 5% of the value predicted by the theory. Subsequent experiments approach an accuracy of a few percent.

The causes of the Casimir effect are described by quantum field theory, which states that all of the various fundamental fields, such as the electromagnetic field, must be quantized at each and every point in space. In a simplified view, a "field" in physics may be envisioned as if space were filled with interconnected vibrating balls and springs, and the strength of the field can be visualized as the displacement of a ball from its rest position. Vibrations in this field propagate and are governed by the appropriate wave equation for the particular field in question. The second quantization of quantum field theory requires that each such ball-spring combination be quantized, that is, that the strength of the field be quantized at each point in space. At the most basic level, the field at each point in space is a simple harmonic oscillator, and its quantization places a quantum harmonic oscillator at each point. Excitations of the field correspond to the elementary particles of particle physics. However, even the vacuum has a vastly complex structure, so all calculations of quantum field theory must be made in relation to this model of the vacuum.

The vacuum has, implicitly, all of the properties that a particle may have: spin, or polarization in the case of light, energy, and so on. On average, most of these properties cancel out: the vacuum is, after all, "empty" in this sense. One important exception is the vacuum energy or the vacuum expectation value of the energy. The quantization of a simple harmonic oscillator states that the lowest possible energy or zero-point energy that such an oscillator may have is

Summing over all possible oscillators at all points in space gives an infinite quantity. Since only "differences" in energy are physically measurable (with the notable exception of gravitation, which remains beyond the scope of quantum field theory), this infinity may be considered a feature of the mathematics rather than of the physics. This argument is the underpinning of the theory of renormalization. Dealing with infinite quantities in this way was a cause of widespread unease among quantum field theorists before the development in the 1970s of the renormalization group, a mathematical formalism for scale transformations that provides a natural basis for the process.

When the scope of the physics is widened to include gravity, the interpretation of this formally infinite quantity remains problematic. There is currently no compelling explanation as to why it should not result in a cosmological constant that is many orders of magnitude larger than observed. However, since we do not yet have any fully coherent quantum theory of gravity, there is likewise no compelling reason as to why it should.

The Casimir effect for fermions can be understood as the spectral asymmetry of the fermion operator formula_2, where it is known as the Witten index.

Alternatively, a 2005 paper by Robert Jaffe of MIT states that "Casimir effects
can be formulated and Casimir forces can be computed without reference to zero-point energies. They are relativistic, quantum forces between charges and currents. The Casimir force (per unit
area) between parallel plates vanishes as alpha, the fine structure constant, goes to zero, and the standard result, which appears to be independent of alpha, corresponds to the alpha approaching infinity limit," and that "The Casimir force is simply the (relativistic, retarded) van der Waals force between the metal plates." Casimir and Polder's original paper used this method to derive the Casimir-Polder force. In 1978, Schwinger, DeRadd, and Milton published a similar derivation for the Casimir Effect between two parallel plates. In fact, the description in terms of van der Waals forces is the only correct description from the fundamental microscopic perspective, while other descriptions of Casimir force are merely effective macroscopic descriptions.

A third way of understanding Casimir forces has been suggested, based on canonical macroscopic quantum electrodynamics. In this interpretation, there exists a ground (vacuum) state of the "coupled" system of matter and fields, which determines the ground-state properties of the electromagnetic field, giving rise to a force. The Casimir force is fundamentally a property of the coupled system of matter and fields, in which the interaction between the plates is mediated by the zero-point fields. In more traditional interpretations, however, the emphasis has fallen either on the electromagnetic field or the fluctuating material in the plates.

Casimir's observation was that the second-quantized quantum electromagnetic field, in the presence of bulk bodies such as metals or dielectrics, must obey the same boundary conditions that the classical electromagnetic field must obey. In particular, this affects the calculation of the vacuum energy in the presence of a conductor or dielectric.

Consider, for example, the calculation of the vacuum expectation value of the electromagnetic field inside a metal cavity, such as, for example, a radar cavity or a microwave waveguide. In this case, the correct way to find the zero-point energy of the field is to sum the energies of the standing waves of the cavity. To each and every possible standing wave corresponds an energy; say the energy of the "n"th standing wave is formula_3. The vacuum expectation value of the energy of the electromagnetic field in the cavity is then

with the sum running over all possible values of "n" enumerating the standing waves. The factor of 1/2 is present because the zero-point energy of the n'th mode is formula_5, where formula_3 is the energy increment for the n'th mode. (It is the same 1/2 as appears in the equation formula_7.) Written in this way, this sum is clearly divergent; however, it can be used to create finite expressions.

In particular, one may ask how the zero-point energy depends on the shape "s" of the cavity. Each energy level formula_3 depends on the shape, and so one should write formula_9 for the energy level, and formula_10 for the vacuum expectation value. At this point comes an important observation: the force at point "p" on the wall of the cavity is equal to the change in the vacuum energy if the shape "s" of the wall is perturbed a little bit, say by formula_11, at point "p". That is, one has

This value is finite in many practical calculations.

Attraction between the plates can be easily understood by focusing on the one-dimensional situation. Suppose that a moveable conductive plate is positioned at a short distance "a" from one of two widely separated plates (distance "L" apart). With "a" « "L", the states within the slot of width "a" are highly constrained so that the energy "E" of any one mode is widely separated from that of the next. This is not the case in the large region "L", where there is a large number (numbering about "L" / "a") of states with energy evenly spaced between "E" and the next mode in the narrow slot – in other words, all slightly larger than "E". Now on shortening "a" by d"a" (< 0), the mode in the narrow slot shrinks in wavelength and therefore increases in energy proportional to −d"a"/"a", whereas all the "L" /"a" states that lie in the large region lengthen and correspondingly decrease their energy by an amount proportional to d"a"/"L" (note the denominator). The two effects nearly cancel, but the net change is slightly negative, because the energy of all the "L"/"a" modes in the large region are slightly larger than the single mode in the slot. Thus the force is attractive: it tends to make "a" slightly smaller, the plates attracting each other across the thin slot.


In the original calculation done by Casimir, he considered the space between a pair of conducting metal plates at distance formula_13 apart. In this case, the standing waves are particularly easy to calculate, because the transverse component of the electric field and the normal component of the magnetic field must vanish on the surface of a conductor. Assuming the plates lie parallel to the "xy"-plane, the standing waves are

where formula_15 stands for the electric component of the electromagnetic field, and, for brevity, the polarization and the magnetic components are ignored here. Here, formula_16 and formula_17 are the wave numbers in directions parallel to the plates, and

is the wave-number perpendicular to the plates. Here, "n" is an integer, resulting from the requirement that ψ vanish on the metal plates. The frequency of this wave is

where "c" is the speed of light. The vacuum energy is then the sum over all possible excitation modes. Since the area of the plates is large, we may sum by integrating over two of the dimensions in "k"-space. The assumption of periodic boundary conditions yields,

where "A" is the area of the metal plates, and a factor of 2 is introduced for the two possible polarizations of the wave. This expression is clearly infinite, and to proceed with the calculation, it is convenient to introduce a regulator (discussed in greater detail below). The regulator will serve to make the expression finite, and in the end will be removed. The zeta-regulated version of the energy per unit-area of the plate is

In the end, the limit formula_22 is to be taken. Here "s" is just a complex number, not to be confused with the shape discussed previously. This integral/sum is finite for "s" real and larger than 3. The sum has a pole at "s"=3, but may be analytically continued to "s"=0, where the expression is finite. The above expression simplifies to:

where polar coordinates formula_24 were introduced to turn the double integral into a single integral. The formula_25 in front is the Jacobian, and the formula_26 comes from the angular integration. The integral converges if Re["s"] > 3, resulting in

The sum diverges at "s" in the neighborhood of zero, but if the damping of large-frequency excitations corresponding to analytic continuation of the Riemann zeta function to "s"=0 is assumed to make sense physically in some way, then one has

But

and so one obtains

The analytic continuation has evidently lost an additive positive infinity, somehow exactly accounting for the zero-point energy (not included above) outside the slot between the plates, but which changes upon plate movement within a closed system. The Casimir force per unit area formula_31 for idealized, perfectly conducting plates with vacuum between them is

where

The force is negative, indicating that the force is attractive: by moving the two plates closer together, the energy is lowered. The presence of formula_33 shows that the Casimir force per unit area formula_31 is very small, and that furthermore, the force is inherently of quantum-mechanical origin.

NOTE: In Casimir's original derivation , a moveable conductive plate is positioned at a short distance "a" from one of two widely separated plates (distance "L" apart). The 0-point energy on "both" sides of the plate is considered. Instead of the above "ad hoc" analytic continuation assumption, non-convergent sums and integrals are computed using Euler–Maclaurin summation with a regularizing function (e.g., exponential regularization) not so anomalous as formula_38 in the above.

Casimir's analysis of idealized metal plates was generalized to arbitrary dielectric and realistic metal plates by Lifshitz and his students. Using this approach, complications of the bounding surfaces, such as the modifications to the Casimir force due to finite conductivity, can be calculated numerically using the tabulated complex dielectric functions of the bounding materials. Lifshitz's theory for two metal plates reduces to Casimir's idealized 1/"a" force law for large separations "a" much greater than the skin depth of the metal, and conversely reduces to the 1/"a" force law of the London dispersion force (with a coefficient called a Hamaker constant) for small "a", with a more complicated dependence on "a" for intermediate separations determined by the dispersion of the materials.

Lifshitz' result was subsequently generalized to arbitrary multilayer planar geometries as well as to anisotropic and magnetic materials, but for several decades the calculation of Casimir forces for non-planar geometries remained limited to a few idealized cases admitting analytical solutions. For example, the force in the experimental sphere–plate geometry was computed with an approximation (due to Derjaguin) that the sphere radius "R" is much larger than the separation "a", in which case the nearby surfaces are nearly parallel and the parallel-plate result can be adapted to obtain an approximate "R"/"a" force (neglecting both skin-depth and higher-order curvature effects). However, in the 2000s a number of authors developed and demonstrated a variety of numerical techniques, in many cases adapted from classical computational electromagnetics, that are capable of accurately calculating Casimir forces for arbitrary geometries and materials, from simple finite-size effects of finite plates to more complicated phenomena arising for patterned surfaces or objects of various shapes.

One of the first experimental tests was conducted by Marcus Sparnaay at Philips in Eindhoven (Netherlands), in 1958, in a delicate and difficult experiment with parallel plates, obtaining results not in contradiction with the Casimir theory, but with large experimental errors. Some of the experimental details as well as some background information on how Casimir, Polder and Sparnaay arrived at this point are highlighted in a 2007 interview with Marcus Sparnaay.

The Casimir effect was measured more accurately in 1997 by Steve K. Lamoreaux of Los Alamos National Laboratory, and by Umar Mohideen and Anushree Roy of the University of California, Riverside. In practice, rather than using two parallel plates, which would require phenomenally accurate alignment to ensure they were parallel, the experiments use one plate that is flat and another plate that is a part of a sphere with a large radius.

In 2001, a group (Giacomo Bressi, Gianni Carugno, Roberto Onofrio and Giuseppe Ruoso) at the University of Padua (Italy) finally succeeded in measuring the Casimir force between parallel plates using microresonators.

In order to be able to perform calculations in the general case, it is convenient to introduce a regulator in the summations. This is an artificial device, used to make the sums finite so that they can be more easily manipulated, followed by the taking of a limit so as to remove the regulator.

The heat kernel or exponentially regulated sum is

where the limit formula_40 is taken in the end. The divergence of the sum is typically manifested as

for three-dimensional cavities. The infinite part of the sum is associated with the bulk constant "C" which "does not" depend on the shape of the cavity. The interesting part of the sum is the finite part, which is shape-dependent. The Gaussian regulator

is better suited to numerical calculations because of its superior convergence properties, but is more difficult to use in theoretical calculations. Other, suitably smooth, regulators may be used as well. The zeta function regulator

is completely unsuited for numerical calculations, but is quite useful in theoretical calculations. In particular, divergences show up as poles in the complex "s" plane, with the bulk divergence at "s"=4. This sum may be analytically continued past this pole, to obtain a finite part at "s"=0.

Not every cavity configuration necessarily leads to a finite part (the lack of a pole at "s"=0) or shape-independent infinite parts. In this case, it should be understood that additional physics has to be taken into account. In particular, at extremely large frequencies (above the plasma frequency), metals become transparent to photons (such as X-rays), and dielectrics show a frequency-dependent cutoff as well. This frequency dependence acts as a natural regulator. There are a variety of bulk effects in solid state physics, mathematically very similar to the Casimir effect, where the cutoff frequency comes into explicit play to keep expressions finite. (These are discussed in greater detail in "Landau and Lifshitz", "Theory of Continuous Media".)

The Casimir effect can also be computed using the mathematical mechanisms of functional integrals of quantum field theory, although such calculations are considerably more abstract, and thus difficult to comprehend. In addition, they can be carried out only for the simplest of geometries. However, the formalism of quantum field theory makes it clear that the vacuum expectation value summations are in a certain sense summations over so-called "virtual particles".

More interesting is the understanding that the sums over the energies of standing waves should be formally understood as sums over the eigenvalues of a Hamiltonian. This allows atomic and molecular effects, such as the van der Waals force, to be understood as a variation on the theme of the Casimir effect. Thus one considers the Hamiltonian of a system as a function of the arrangement of objects, such as atoms, in configuration space. The change in the zero-point energy as a function of changes of the configuration can be understood to result in forces acting between the objects.

In the chiral bag model of the nucleon, the Casimir energy plays an important role in showing the mass of the nucleon is independent of the bag radius. In addition, the spectral asymmetry is interpreted as a non-zero vacuum expectation value of the baryon number, cancelling the topological winding number of the pion field surrounding the nucleon.

The dynamical Casimir effect is the production of particles and energy from an accelerated "moving mirror". This reaction was predicted by certain numerical solutions to quantum mechanics equations made in the 1970s. In May 2011 an announcement was made by researchers at the Chalmers University of Technology, in Gothenburg, Sweden, of the detection of the dynamical Casimir effect. In their experiment, microwave photons were generated out of the vacuum in a superconducting microwave resonator. These researchers used a modified SQUID to change the effective length of the resonator in time, mimicking a mirror moving at the required relativistic velocity. If confirmed this would be the first experimental verification of the dynamical Casimir effect.
A similar analysis can be used to explain Hawking radiation that causes the slow "evaporation" of black holes (although this is generally visualized as the escape of one particle from a virtual particle-antiparticle pair, the other particle having been captured by the black hole).

Constructed within the framework of quantum field theory in curved spacetime, the dynamical Casimir effect has been used to better understand acceleration radiation such as the Unruh effect.

There are few instances wherein the Casimir effect can give rise to repulsive forces between uncharged objects. Evgeny Lifshitz showed (theoretically) that in certain circumstances (most commonly involving liquids), repulsive forces can arise. This has sparked interest in applications of the Casimir effect toward the development of levitating devices. An experimental demonstration of the Casimir-based repulsion predicted by Lifshitz was carried out by Munday et al. Other scientists have also suggested the use of gain media to achieve a similar levitation effect, though this is controversial because these materials seem to violate fundamental causality constraints and the requirement of thermodynamic equilibrium (Kramers–Kronig relations). Casimir and Casimir-Polder repulsion can in fact occur for sufficiently anisotropic electrical bodies; for a review of the issues involved with repulsion see Milton et al.

It has been suggested that the Casimir forces have application in nanotechnology, in particular silicon integrated circuit technology based micro- and nanoelectromechanical systems, silicon array propulsion for space drives, and so-called Casimir oscillators.

The Casimir effect shows that quantum field theory allows the energy density in certain regions of space to be negative relative to the ordinary vacuum energy, and it has been shown theoretically that quantum field theory allows states where the energy can be "arbitrarily" negative at a given point, Many physicists such as Stephen Hawking, Kip Thorne, and others therefore argue that such effects might make it possible to stabilize a traversable wormhole. Miguel Alcubierre has suggested using the effect to obtain the negative energy density required for his Alcubierre Drive.

On 4 June 2013 it was reported that a conglomerate of scientists from Hong Kong University of Science and Technology, University of Florida, Harvard University, Massachusetts Institute of Technology, and Oak Ridge National Laboratory have for the first time demonstrated a compact integrated silicon chip that can measure the Casimir force.







</doc>
<doc id="7558" url="https://en.wikipedia.org/wiki?curid=7558" title="Coin">
Coin

A coin is a small, flat, (usually) round piece of metal or plastic used primarily as a medium of exchange or legal tender. They are standardized in weight, and produced in large quantities at a mint in order to facilitate trade. They are most often issued by a government.

Coins are usually metal or alloy, or sometimes made of synthetic materials. They are usually disc shaped. Coins made of valuable metal are stored in large quantities as bullion coins. Other coins are used as money in everyday transactions, circulating alongside banknotes. Usually the highest value coin in circulation (i.e. excluding bullion coins) is worth less than the lowest-value note. In the last hundred years, the face value of circulation coins has occasionally been lower than the value of the metal they contain, for example due to inflation. If the difference becomes significant, the issuing authority may decide to withdraw these coins from circulation, possibly issuing new equivalents with a different composition, or the public may decide to melt the coins down or hoard them (see Gresham's law).

Exceptions to the rule of face value being higher than content value also occur for some bullion coins made of copper, silver, or gold (and, rarely, other metals, such as platinum or palladium), intended for collectors or investors in precious metals. Examples of modern gold collector/investor coins include the British sovereign minted by the United Kingdom, the American Gold Eagle minted by the United States, the Canadian Gold Maple Leaf minted by Canada, and the Krugerrand, minted by South Africa. While the Eagle, Maple Leaf, and Sovereign coins have nominal (purely symbolic) face values, the Krugerrand does not.

Historically, a great quantity of coinage metals (including alloys) and other materials (e.g. porcelain) have been used to produce coins for circulation, collection, and metal investment: bullion coins often serve as more convenient stores of assured metal quantity and purity than other bullion.

The first coins were developed in Iron Age Anatolia around the 7th and 6th centuries BC. Coins spread rapidly in the 6th and 5th centuries BC, throughout Greece and Persia, and further to the Balkans.

Standardized Roman currency was used throughout the Roman Empire. Important Roman gold and silver coins were continued into the Middle Ages (see Gold dinar, Solidus, Aureus, Denarius). Ancient and early medieval coins in theory had the value of their metal content, although there have been many instances throughout history of governments inflating their currencies by debasing the metal content of their coinage, so that the inferior coins were worth less in metal than their face value. Fiat money first arose in medieval China, with the jiaozi paper money. Early paper money was introduced in Europe in the later Middle Ages, but some coins continued to have the value of the gold or silver they contained throughout the Early Modern period. The penny was minted as a silver coin until the 17th century.

The first circulating United States coins were cents (pennies), produced in 1793, and made entirely from copper. Silver content was reduced in many coins in the 19th century (use of billon), and the first coins made entirely of base metal (e.g. nickel, cupronickel, aluminium bronze), representing values higher than the value of their metal, were minted in the mid 19th century.
Coins were an evolution of "currency" systems of the Late Bronze Age, where standard-sized ingots, and tokens such as knife money, were used to store and transfer value. In the late Chinese Bronze Age, standardized cast tokens were made, such as those discovered in a tomb near Anyang. These were replicas in bronze of earlier Chinese currency, cowrie shells, so they were named Bronze Shell.

According to Aristotle (fr. 611,37, ed. V. Rose) and Pollux (Onamastikon IX.83), the first issuer of coins was Hermodike of Kyme

The earliest coins are mostly associated with Iron Age Anatolia, especially with the kingdom of Lydia.
Early electrum coins were not standardized in weight, and in their earliest stage may have been ritual objects, such as badges or medals, issued by priests.
Many early Lydian and Greek coins were minted under the authority of private individuals and are thus more akin to tokens or badges than to modern coins, though due to their numbers it is evident that some were official state issues, with King Alyattes of Lydia, 619–560 BC, being a frequently mentioned originator of coinage.

The first Lydian coins were made of electrum, a naturally occurring alloy of silver and gold that was further alloyed with added silver and copper.
Most of the early Lydian coins include no writing ("legend" or "inscription"), only an image of a symbolic animal. Therefore, the dating of these coins relies primarily on archaeological evidence, with the most commonly cited evidence coming from excavations at the Temple of Artemis at Ephesus, also called the Ephesian Artemision (which would later evolve into one of the Seven Wonders of the Ancient World). Because the oldest lion head "coins" were discovered in that temple, and they do not appear to have been used in commerce, these objects may not have been coins but badges or medals issued by the priests of that temple. Anatolian Artemis was the "Πὀτνια Θηρῶν" (Potnia Thêrôn, "Mistress of Animals"), whose symbol was the stag. It took some time before ancient coins were used for commerce and trade. Even the smallest-denomination electrum coins, perhaps worth about a day's subsistence, would have been too valuable for buying a loaf of bread. The first coins to be used for retailing on a large-scale basis were likely small silver fractions, Hemiobol, Ancient Greek coinage minted by the Ionian Greeks in the late sixth century BC.

A small percentage of early Lydian/Greek coins have a legend. A famous early electrum coin, the most ancient inscribed coin at present known, is from nearby Caria. This coin has a Greek legend reading "phaenos emi sema" interpreted variously as "I am the badge of Phanes", or "I am the sign of light", or "I am the tomb of light", or "I am the tomb of Phanes".
The coins of Phanes are known to be amongst the earliest of Greek coins, a hemihekte of the issue was found in the foundation deposit of the temple of Artemis at Ephesos (the oldest deposit of electrum coins discovered). One assumption is that Phanes was a wealthy merchant, another that this coin is associated with Apollo-Phanes and, due to the Deer, with Artemis (twin sister of the god of light Apollo-Phaneos). Although only seven Phanes type coins were discovered, it is also notable that 20% of all early electrum coins also have the lion of Artemis and the sun burst of Apollo-Phaneos.

Alternatively, Phanes may have been the Halicarnassian mercenary of Amasis mentioned by Herodotus, who escaped to the court of Cambyses, and became his guide in the invasion of Egypt in 527 or 525 BC. According to Herodotus, this Phanes was buried alive by a sandstorm, together with 50,000 Persian soldiers, while trying to conquer the temple of Amun–Zeus in Egypt. The fact that the Greek word "Phanes" also means light (or lamp), and the word "sema" also means tomb makes this coin a famous and controversial one.

Another candidate for the site of the earliest coins is Aegina, where Chelone ("turtle") coins were first minted circa 700 BC. Coins from Athens and Corinth appeared shortly thereafter, known to exist at least since the late 6th century BC.

Coinage followed Greek colonization and influence first around the Mediterranean and soon after to North Africa (including Egypt), Syria, Persia, and the Balkans.

Coins were minted in the Achaemenid Empire, including the gold "darics" and silver "sigloi". With the Achemenid conquest of Gandhara under Darius the Great c. 520 BC, the practice spread to the Indo-Gangetic Plain. The coins of this period were called "Puranas", "Karshapanas" or "Pana". These earliest Indian coins, however, are unlike those circulated in Persia, which were derived from the Greek/Anatolian type; they not disk-shaped but rather stamped bars of metal, suggesting that the innovation of stamped currency was added to a pre-existing form of token currency which had already been present in the Mahajanapada kingdoms of the Indian Iron Age. Mahajanapadas that minted their own coins included Gandhara, Kuntala, Kuru, Panchala, Shakya, Surasena and Surashtra.

In China, early round coins appeared in the 4th century BC.

The first Roman coins, which were crude, heavy cast bronzes, were issued c. 289 BC.

In the Philippines, gold, which was plentiful in many parts of the islands, invariably found its way into these objects that included the Piloncitos, small bead-like gold bits considered by the local numismatists as the earliest coin of ancient Filipinos, and gold barter rings.

Piloncitos are small—some are of the size of a corn kernel—and weigh from 0.09 to 2.65 grams of fine gold. Large Piloncitos weighing 2.65 grams approximate the weight of one mass. Piloncitos have been excavated from Mandaluyong, Bataan, the banks of the Pasig River, Batangas, Marinduque, Samar, Leyte and some areas in Mindanao. They have been found in large numbers in Indonesian archeological sites leading to questions of origin. Were Piloncitos made in the Philippines or imported? That gold was mined and worked here is evidenced by many Spanish accounts, like one in 1586 that said:

The first European coin to use Arabic numerals to date the year in which the coin was minted was the St. Gall silver "Plappart" of 1424.

Most coins presently are made of a base metal, and their value comes from their status as fiat money. This means that the value of the coin is decreed by government fiat (law), and thus is determined by the free market only in as much as national currencies are used in domestic trade and also traded internationally on foreign exchange markets. Thus, these coins are monetary tokens, just as paper currency is: they are usually not backed by metal, but rather by some form of government guarantee. Some have suggested that such coins not be considered to be "true coins" (see below). Thus, there is very little economic difference between notes and coins of equivalent face value.

Coins may be in circulation with fiat values lower than the value of their component metals, but they are never initially issued with such value, and the shortfall only arises over time due to inflation, as market values for the metal overtake the fiat declared face value of the coin. Examples are the pre-1965 US dime, quarter, half dollar, and dollar (nominally containing slightly less than a tenth, quarter, half, and full ounce of silver, respectively), US nickel, and pre-1982 US penny. As a result of the increase in the value of copper, the United States greatly reduced the amount of copper in each penny. Since mid-1982, United States pennies are made of 97.5% zinc, with the remaining 2.5% being a coating of copper. Extreme differences between fiat values and metal values of coins cause coins to be hoarded or removed from circulation by illicit smelters in order to realise the value of their metal content. This is an example of Gresham's law. The United States Mint, in an attempt to avoid this, implemented new interim rules on December 14, 2006, subject to public comment for 30 days, which criminalized the melting and export of pennies and nickels. Violators can be fined up to $10,000 and/or imprisoned for up to five years.

A coin's value as a collector's item or as an investment generally depends on its condition, specific historical significance, rarity, quality, beauty of the design and general popularity with collectors. If a coin is greatly lacking in all of these, it is unlikely to be worth much. The value of bullion coins is also influenced to some extent by those factors, but is largely based on the value of their gold, silver, or platinum content. Sometimes non-monetized bullion coins such as the Canadian Maple Leaf and the American Gold Eagle are minted with nominal face values less than the value of the metal in them, but as such coins are never intended for circulation, these face values have no relevance.

Coins can be used as creative medium of expression – from fine art sculpture to the penny machines that can be found in most amusement parks. In the Code of Federal Regulations (CFR) in the United States there are some regulations specific to nickels and pennies that are informative on this topic. 31 CFR § 82.1 forbids unauthorized persons from exporting, melting, or treating any 5 or 1 cent coins.

This has been a particular problem with nickels and dimes (and with some comparable coins in other currencies) because of their relatively low face value and unstable commodity prices. For a while, the copper in US pennies was worth more than one cent, so people would hoard pennies and then melt them down for their metal value. It cost more than face value to manufacture pennies or nickels, so any widespread loss of the coins in circulation could be expensive for the US Treasury. This was more of a problem when coins were still made of precious metals like silver and gold, so strict laws against alteration make more sense historically.

31 CFR § 82.2 goes on to state that: "(b) The prohibition contained in § 82.1 against the treatment of 5-cent coins and one-cent coins shall not apply to the treatment of these coins for educational, amusement, novelty, jewelry, and similar purposes as long as the volumes treated and the nature of the treatment makes it clear that such treatment is not intended as a means by which to profit solely from the value of the metal content of the coins."

Throughout history, monarchs and governments have often created more coinage than their supply of precious metals would allow if the coins were pure metal. By replacing some fraction of a coin's precious metal content with a base metal (often copper or nickel), the intrinsic value of each individual coin was reduced (thereby "debasing" the money), allowing the coining authority to produce more coins than would otherwise be possible. Debasement occasionally occurs in order to make the coin physically harder and therefore less likely to be worn down as quickly, but the more usual reason is to profit from the difference between face value and metal value. Debasement of money almost always leads to price inflation. Sometimes price controls are at the same time also instituted by the governing authority, but historically these have generally proved unworkable.

The United States is unusual in that it has only slightly modified its coinage system (except for the images and symbols on the coins, which have changed a number of times) to accommodate two centuries of inflation. The one-cent coin has changed little since 1856 (though its composition was changed in 1982 to remove virtually all copper from the coin) and still remains in circulation, despite a greatly reduced purchasing power. On the other end of the spectrum, the largest coin in common circulation is valued at 25 cents, a very low value for the largest denomination coin compared to many other countries. Increases in the prices of copper, nickel, and zinc meant that both the US one- and five-cent coins became worth more for their raw metal content than their face (fiat) value. In particular, copper one-cent pieces (those dated prior to 1982 and some 1982-dated coins) contained about two cents' worth of copper.

Some denominations of circulating coins that were formerly minted in the United States are no longer made. These include coins with a face value of a half cent, two cents, three cents, and twenty cents. (The half dollar and dollar coins are still produced, but mostly for vending machines and collectors.) In the past, the US also coined the following denominations for circulation in gold: One dollar, $2.50, three dollars, five dollars, ten dollars, and twenty dollars. In addition, cents were originally slightly larger than the modern quarter and weighed nearly half an ounce, while five-cent coins (known then as "half dimes") were smaller than a dime and made of a silver alloy. Dollar coins were also much larger, and weighed approximately an ounce. One-dollar gold coins are no longer produced and rarely used. The US also issues bullion and commemorative coins with the following denominations: 50¢, $1, $5, $10, $25, $50, and $100.

Circulating coins commonly suffered from "shaving" or "clipping": the public would cut off small amounts of precious metal from their edges to sell it and then pass on the mutilated coins at full value. Unmilled British sterling silver coins were sometimes reduced to almost half their minted weight. This form of debasement in Tudor England was commented on by Sir Thomas Gresham, whose name was later attached to Gresham's law. The monarch would have to periodically recall circulating coins, paying only the bullion value of the silver, and reminting them. This, also known as recoinage, is a long and difficult process that was done only occasionally. Many coins have milled or reeded edges, originally designed to make it easier to detect clipping.

Some convicted criminals from the British Isles who were sentenced to transportation to Australia in the 18th and 19th centuries used coins to leave messages of remembrance to loved ones left behind in Britain. The coins were defaced, smoothed and inscribed, either by stippling or engraving, with sometimes touching words of loss. These coins were called "convict love tokens" or "leaden hearts". A number of these tokens are in the collection of the National Museum of Australia.

The side of a coin carrying an image of a monarch, other authority ("see List of people on coins"), or a national emblem is called the "obverse" (colloquially, "heads"); the other side, carrying various types of information, is called the "reverse" (colloquially, "tails"). The year of minting is usually shown on the obverse, although some Chinese coins, most Canadian coins, the pre-2008 British 20p coin, the post-1999 American quarter, and all Japanese coins are exceptions.

The relation of the images on the obverse and reverse of a coin is the coin's orientation. Suppose the image on the obverse of the coin is right side up; if you turn the coin left or right on its horizontal axis, and the reverse of the coin is also right side up, then the coin is said to have medallic orientation—typical of the Euro and pound sterling; if, however, turning the coin left or right shows that the reverse image is upside down, then the coin is said to have coin orientation, characteristic of the United States dollar coin. 

Bimetallic coins are sometimes used for higher values and for commemorative purposes. In the 1990s, France used a tri-metallic coin. Common circulating bimetallic examples include the €1, €2, British £1, £2 and Canadian $2 and several peso coins in Mexico.

The "exergue" is the space on a coin beneath the main design, often used to show the coin's date, although it is sometimes left blank or containing a mint mark, privy mark, or some other decorative or informative design feature. Many coins do not have an exergue at all, especially those with few or no legends, such as the Victorian bun penny.
Not all coins are round; they come in a variety of shapes. The Australian 50-cent coin, for example, has twelve flat sides. Some coins have wavy edges, e.g. the $2 and 20-cent coins of Hong Kong and the 10-cent coins of Bahamas. Some are square-shaped, such as the 15-cent coin of the Bahamas and the 50-cent coin from Aruba. During the 1970s, Swazi coins were minted in several shapes, including squares, polygons, and wavy edged circles with 8 and 12 waves.

Some other coins, like the British 20 and 50 pence coins and the Canadian Loonie, have an odd number of sides, with the edges rounded off. This way the coin has a constant diameter, recognisable by vending machines whichever direction it is inserted.

A triangular coin with a face value of £5 (produced to commemorate the 2007/2008 Tutankhamun exhibition at The O2 Arena) was commissioned by the Isle of Man: it became legal tender on 6 December 2007. Other triangular coins issued earlier include: Cabinda coin, Bermuda coin, 2 Dollar Cook Islands 1992 triangular coin, Uganda Millennium Coin and Polish Sterling-Silver 10-Zloty Coin.

Some mediaeval coins, called bracteates, were so thin they were struck on only one side.

Many coins over the years have been manufactured with integrated holes such as Chinese "cash" coins, Japanese coins, Colonial French coins, etc. This may have been done to permit their being strung on cords, to facilitate storage and being carried.

The Royal Canadian Mint is now able to produce holographic-effect gold and silver coinage. However, this procedure is not limited to only bullion or commemorative coinage. The 500 yen coin from Japan was subject to a massive amount of counterfeiting. The Japanese government in response produced a circulatory coin with a holographic image.

The Royal Canadian Mint has also released several coins that are coloured, the first of which was in commemoration of Remembrance Day. The subject was a coloured poppy on the reverse of a 25-cent piece minted through a patented process.

An example of non-metallic composite coins (sometimes incorrectly called plastic coins) was introduced into circulation in Transnistria on 22 August 2014. Most of these coins are also non-circular, with different shapes corresponding to different coin values.

For a list of many pure metallic elements and their alloys which have been used in actual circulation coins and for trial experiments, see coinage metals.

To flip a coin to see whether it lands "heads" or "tails" is to use it as a two-sided dice in what is known in mathematics as a Bernoulli trial: if the probability of heads (in the parlance of Bernoulli trials, a "success") is exactly 0.5, the coin is fair.

Coins can also be spun on a flat surface such as a table. This results in the following phenomenon: as the coin falls over and rolls on its edge, it spins faster and faster (formally, the precession rate of the symmetry axis of the coin, i.e., the axis passing from one face of the coin to the other) before coming to an abrupt stop. This is mathematically modeled as a finite-time singularity – the precession rate is accelerating to infinity, before it suddenly stops, and has been studied using high speed photography and devices such as Euler's Disk. The slowing down is predominantly caused by rolling friction (air resistance is minor), and the singularity (divergence of the precession rate) can be modeled as a power law with exponent approximately −1/3.

Iron and copper coins have a characteristic metallic smell that is produced upon contact with oils in the skin. Perspiration is chemically reduced upon contact with these metals, which causes the skin oils to decompose, forming with iron the volatile molecule 1-octen-3-one.



</doc>
<doc id="7560" url="https://en.wikipedia.org/wiki?curid=7560" title="College of the City of New York">
College of the City of New York

College of the City of New York may refer to:




</doc>
<doc id="7561" url="https://en.wikipedia.org/wiki?curid=7561" title="Classical Kuiper belt object">
Classical Kuiper belt object

A classical Kuiper belt object, also called a cubewano ( "QB1-o"), is a low-eccentricity Kuiper belt object (KBO) that orbits beyond Neptune and is not controlled by an orbital resonance with Neptune. Cubewanos have orbits with semi-major axes in the 40–50 AU range and, unlike Pluto, do not cross Neptune’s orbit. That is, they have low-eccentricity and sometimes low-inclination orbits like the classical planets.

The name "cubewano" derives from the first trans-Neptunian object (TNO) found after Pluto and Charon, 15760 Albion, which until January 2018 had only had the provisional designation . Similar objects found later were often called "QB1-o's", or "cubewanos", after this object, though the term "classical" is much more frequently used in the scientific literature.

Objects identified as cubewanos include:

Haumea was provisionally listed as a cubewano by the Minor Planet Center in 2006, but turned out to be resonant.

Most cubewanos are found between the 2:3 orbital resonance with Neptune (populated by plutinos) and the 1:2 resonance. 50000 Quaoar, for example, has a near-circular orbit close to the ecliptic. Plutinos, on the other hand, have more eccentric orbits bringing some of them closer to the Sun than Neptune.

The majority of objects, the so-called "cold population", have low inclinations (<5°) and near-circular orbits, lying between 42 and 47 AU. A smaller population (the 'hot population') is characterised by highly inclined, more eccentric orbits.

The Deep Ecliptic Survey reports the distributions of the two populations; one with the inclination centered at 4.6° (named "Core") and another with inclinations extending beyond 30° ("Halo").

The vast majority of KBOs (more than two-thirds) have inclinations of less than 5° and eccentricities of less than 0.1. Their semi-major axes show a preference for the middle of the main belt; arguably, smaller objects close to the limiting resonances have been either captured into resonance or have their orbits modified by Neptune.

The 'hot' and 'cold' populations are strikingly different: more than 30% of all cubewanos are in low inclination, near-circular orbits. The parameters of the plutinos’ orbits are more evenly distributed, with a local maximum in moderate eccentricities in 0.15–0.2 range and low inclinations 5–10°.
See also the comparison with scattered disk objects.

When the orbital eccentricities of cubewanos and plutinos are compared, it can be seen that the cubewanos form a clear 'belt' outside Neptune's orbit, whereas the plutinos approach, or even cross Neptune's orbit. When orbital inclinations are compared, 'hot' cubewanos can be easily distinguished by their higher inclinations, as the plutinos typically keep orbits below 20°. (No clear explanation currently exists for the inclinations of 'hot' cubewanos.)

In addition to the distinct orbital characteristics, the two populations display different physical characteristics.

The difference in colour between the red cold population and more heterogeneous hot population was observed as early as in 2002.
Recent studies, based on a larger data set, indicate the cut-off inclination of 12° (instead of 5°) between the cold and hot populations and confirm the distinction between the homogenous red cold population and the bluish hot population.

Another difference between the low-inclination (cold) and high-inclination (hot) classical objects is the observed number of binary objects. Binaries are quite common on low-inclination orbits and are typically similar-brightness systems. Binaries are less common on high-inclination orbits and their components typically differ in brightness. This correlation, together with the differences in colour, support further the suggestion that the currently observed classical objects belong to at least two different overlapping populations, with different physical properties and orbital history.

There is no official definition of 'cubewano' or 'classical KBO'. However, the terms are normally used to refer to objects free from significant perturbation from Neptune, thereby excluding KBOs in orbital resonance with Neptune (resonant trans-Neptunian objects). The Minor Planet Center (MPC) and the Deep Ecliptic Survey (DES) do not list cubewanos (classical objects) using the same criteria. Many TNOs classified as cubewanos by the MPC are classified as ScatNear (possibly scattered by Neptune) by the DES. Dwarf planet Makemake is such a borderline classical cubewano/scatnear object. may be an inner cubewano near the plutinos. Furthermore, there is evidence that the Kuiper belt has an 'edge', in that an apparent lack of low-inclination objects beyond 47–49 AU was suspected as early as 1998 and shown with more data in 2001. Consequently, the traditional usage of the terms is based on the orbit’s semi-major axis, and includes objects situated between the 2:3 and 1:2 resonances, that is between 39.4 and 47.8 AU (with exclusion of these resonances and the minor ones in-between).

These definitions lack precision: in particular the boundary between the classical objects and the scattered disk remains blurred. As of 2010, there are 377 objects with perihelion (q) > 40 AU and aphelion (Q) < 47 AU.

Introduced by the report from the Deep Ecliptic Survey by J. L. Elliott et al. in 2005 uses formal criteria based on the mean orbital parameters. Put informally, the definition includes the objects that have never crossed the orbit of Neptune. According to this definition, an object qualifies as a classical KBO if:

An alternative classification, introduced by B. Gladman, B. Marsden and C. van Laerhoven in 2007, uses a 10-million-year orbit integration instead of the Tisserand's parameter. Classical objects are defined as not resonant and not being currently scattered by Neptune.

Formally, this definition includes as "classical" all objects with their "current" orbits that
Unlike other schemes, this definition includes the objects with major semi-axis less than 39.4 AU (2:3 resonance)—termed inner classical belt, or more than 48.7 (1:2 resonance) – termed outer classical belt, and reserves the term main classical belt for the orbits between these two resonances.

The first known collisional family in the classical Kuiper belt—a group of objects thought to be remnants from the breakup of a single body—is the Haumea family. It includes Haumea, its moons, and seven smaller bodies. The objects not only follow similar orbits but also share similar physical characteristics. Unlike many other KBO their surface contains large amounts of ice (HO) and no or very little tholins. The surface composition is inferred from their neutral (as opposed to red) colour and deep absorption at 1.5 and 2. μm in infrared spectrum. Several other collisional families might reside in the classical Kuiper belt.

No Kuiper belt object has ever been seen up close. Both Voyager spacecraft have passed through the region before the discovery of the Kuiper belt. The first mission to visit a classical KBO will be New Horizons. After its successful exploration of the Pluto system in 2015, the NASA spacecraft is expected to come within of the small KBO on January 1, 2019.

Here is a very generic list of classical Kuiper belt objects. , there are about 579 objects with q > 40 (AU) and Q < 48 (AU).


</doc>
<doc id="7564" url="https://en.wikipedia.org/wiki?curid=7564" title="Foreign policy of the United States">
Foreign policy of the United States

The foreign policy of the United States is its interactions with foreign nations and how it sets standards of interaction for its organizations, corporations and system citizens of the United States.

The officially stated goals of the foreign policy of the United States, including all the Bureaus and Offices in the United States Department of State, as mentioned in the "Foreign Policy Agenda" of the Department of State, are "to build and sustain a more democratic, secure, and prosperous world for the benefit of the American people and the international community." In addition, the United States House Committee on Foreign Affairs states as some of its jurisdictional goals: "export controls, including nonproliferation of nuclear technology and nuclear hardware; measures to foster commercial interaction with foreign nations and to safeguard American business abroad; international commodity agreements; international education; and protection of American citizens abroad and expatriation." U.S. foreign policy and foreign aid have been the subject of much debate, praise and criticism, both domestically and abroad.

Subject to the advice and consent role of the U.S. Senate, the President of the United States negotiates treaties with foreign nations, but treaties enter into force if ratified by two-thirds of the Senate. The President is also Commander in Chief of the United States Armed Forces, and as such has broad authority over the armed forces. Both the Secretary of State and ambassadors are appointed by the President, with the advice and consent of the Senate. The United States Secretary of State acts similarly to a foreign minister and under Executive leadership is the primary conductor of state-to-state diplomacy.

Congress is the only branch of government that has the authority to declare war. Furthermore, Congress writes the civilian and military budget, thus has vast power in military action and foreign aid. Congress also has power to regulate commerce with foreign nations.

The main trend regarding the history of U.S. foreign policy since the American Revolution is the shift from non-interventionism before and after World War I, to its growth as a world power and global hegemony during and since World War II and the end of the Cold War in the 20th century. Since the 19th century, U.S. foreign policy also has been characterized by a shift from the realist school to the idealistic or Wilsonian school of international relations.

Foreign policy themes were expressed considerably in George Washington's farewell address; these included among other things, observing good faith and justice towards all nations and cultivating peace and harmony with all, excluding both "inveterate antipathies against particular nations, and passionate attachments for others", "steer[ing] clear of permanent alliances with any portion of the foreign world", and advocating trade with all nations. These policies became the basis of the Federalist Party in the 1790s. But the rival Jeffersonians feared Britain and favored France in the 1790s, declaring the War of 1812 on Britain. After the 1778 alliance with France, the U.S. did not sign another permanent treaty until the North Atlantic Treaty in 1949. Over time, other themes, key goals, attitudes, or stances have been variously expressed by Presidential 'doctrines', named for them. Initially these were uncommon events, but since WWII, these have been made by most presidents.

Jeffersonians vigorously opposed a large standing army and any navy until attacks against American shipping by Barbary corsairs spurred the country into developing a naval force projection capability, resulting in the First Barbary War in 1801.

Despite two wars with European Powers—the War of 1812 and the 1898 Spanish–American War—American foreign policy was peaceful and marked by steady expansion of its foreign trade during the 19th century. The 1803 Louisiana Purchase doubled the nation's geographical area; Spain ceded the territory of Florida in 1819; annexation brought in the independent Texas Republic in 1845; a war with Mexico in 1848 added California, Arizona, Utah, Nevada, and New Mexico. The U.S. bought Alaska from the Russian Empire in 1867, and it annexed the independent Republic of Hawaii in 1898. Victory over Spain in 1898 brought the Philippines, and Puerto Rico, as well as oversight of Cuba. The short experiment in imperialism ended by 1908, as the U.S. turned its attention to the Panama Canal and the stabilization of regions to its south, including Mexico.

The 20th century was marked by two world wars in which the United States, along with allied powers, defeated its enemies and increased its international reputation. President Wilson's Fourteen Points was developed from his idealistic Wilsonianism program of spreading democracy and fighting militarism so as to end any wars. It became the basis of the German Armistice (really a surrender) and the 1919 Paris Peace Conference. The resulting Treaty of Versailles, due to European allies' punitive and territorial designs, showed insufficient conformity with these points and the U.S. signed separate treaties with each of its adversaries; due to Senate objections also, the U.S. never joined the League of Nations, which was established as a result of Wilson's initiative. In the 1920s, the United States followed an independent course, and succeeded in a program of naval disarmament, and refunding the German economy. Operating outside the League it became a dominant player in diplomatic affairs. New York became the financial capital of the world, but the Wall Street Crash of 1929 hurled the Western industrialized world into the Great Depression. American trade policy relied on high tariffs under the Republicans, and reciprocal trade agreements under the Democrats, but in any case exports were at very low levels in the 1930s.

The United States adopted a non-interventionist foreign policy from 1932 to 1938, but then President Franklin D. Roosevelt moved toward strong support of the Allies in their wars against Germany and Japan. As a result of intense internal debate, the national policy was one of becoming the Arsenal of Democracy, that is financing and equipping the Allied armies without sending American combat soldiers. Roosevelt mentioned four fundamental freedoms, which ought to be enjoyed by people "everywhere in the world"; these included the freedom of speech and religion, as well as freedom from want and fear. Roosevelt helped establish terms for a post-war world among potential allies at the Atlantic Conference; specific points were included to correct earlier failures, which became a step toward the United Nations. American policy was to threaten Japan, to force it out of China, and to prevent its attacking the Soviet Union. However, Japan reacted by an attack on Pearl Harbor in December 1941, and the United States was at war with Japan, Germany, and Italy. Instead of the loans given to allies in World War I, the United States provided Lend-Lease grants of $50,000,000,000. Working closely with Winston Churchill of Britain, and Joseph Stalin of the Soviet Union, Roosevelt sent his forces into the Pacific against Japan, then into North Africa against Italy and Germany, and finally into Europe starting with France and Italy in 1944 against the Germans. The American economy roared forward, doubling industrial production, and building vast quantities of airplanes, ships, tanks, munitions, and, finally, the atomic bomb. Much of the American war effort went to strategic bombers, which flattened the cities of Japan and Germany.

After the war, the U.S. rose to become the dominant non-colonial economic power with broad influence in much of the world, with the key policies of the Marshall Plan and the Truman Doctrine. Almost immediately, however, the world witnessed division into broad two camps during the Cold War; one side was led by the U.S. and the other by the Soviet Union, but this situation also led to the establishment of the Non-Aligned Movement. This period lasted until almost the end of the 20th century and is thought to be both an ideological and power struggle between the two superpowers. A policy of containment was adopted to limit Soviet expansion, and a series of proxy wars were fought with mixed results. In 1991, the Soviet Union dissolved into separate nations, and the Cold War formally ended as the United States gave separate diplomatic recognition to the Russian Federation and other former Soviet states.

In domestic politics, foreign policy is not usually a central issue. In 1945–1970 the Democratic Party took a strong anti-Communist line and supported wars in Korea and Vietnam. Then the party split with a strong, "dovish", pacifist element (typified by 1972 presidential candidate George McGovern). Many "hawks", advocates for war, joined the Neoconservative movement and started supporting the Republicans—especially Reagan—based on foreign policy. Meanwhile, down to 1952 the Republican Party was split between an isolationist wing, based in the Midwest and led by Senator Robert A. Taft, and an internationalist wing based in the East and led by Dwight D. Eisenhower. Eisenhower defeated Taft for the 1952 nomination largely on foreign policy grounds. Since then the Republicans have been characterized by a hawkish and intense American nationalism, and strong opposition to Communism, and strong support for Israel.

In the 21st century, U.S. influence remains strong but, in relative terms, is declining in terms of economic output compared to rising nations such as China, India, Russia, and the newly consolidated European Union. Substantial problems remain, such as climate change, nuclear proliferation, and the specter of nuclear terrorism. Foreign policy analysts Hachigian and Sutphen in their book "The Next American Century" suggest all five powers have similar vested interests in stability and terrorism prevention and trade; if they can find common ground, then the next decades may be marked by peaceful growth and prosperity.

In 2017 diplomats from other countries developed new tactics to deal with President Donald Trump. The "New York Times" reported on the eve of his first foreign trip as president:

Trump has numerous aides giving advice on foreign policy. The chief diplomat was Secretary of State Rex Tillerson. His major foreign policy positions, which sometimes are at odds with Trump, include:

In the United States, there are three types of treaty-related law:

In contrast to most other nations, the United States considers the three types of agreements as distinct. Further, the United States incorporates treaty law into the body of U.S. federal law. As a result, Congress can modify or repeal treaties afterward. It can overrule an agreed-upon treaty obligation even if this is seen as a violation of the treaty under international law. Several U.S. court rulings confirmed this understanding, including Supreme Court decisions in "Paquete Habana v. United States" (1900), and "Reid v. Covert" (1957), as well as a lower court ruling n "Garcia-Mir v. Meese" (1986). Further, the Supreme Court has declared itself as having the power to rule a treaty as void by declaring it "unconstitutional", although as of 2011, it has never exercised this power.

The State Department has taken the position that the Vienna Convention on the Law of Treaties represents established law. Generally, when the U.S. signs a treaty, it is binding. However, as a result of the "Reid v. Covert" decision, the U.S. adds a reservation to the text of every treaty that says, in effect, that the U.S. intends to abide by the treaty, but if the treaty is found to be in violation of the Constitution, then the U.S. legally can't abide by the treaty since the U.S. signature would be "ultra vires".

The United States has ratified and participates in many other multilateral treaties, including arms control treaties (especially with the Soviet Union), human rights treaties, environmental protocols, and free trade agreements.

The United States is a founding member of the United Nations and most of its specialized agencies, notably including the World Bank Group and International Monetary Fund. The U.S. has at times has withheld payment of dues due to disagreements with the UN.

The United States is also member of:

After it captured the islands from Japan during World War II, the United States administered the Trust Territory of the Pacific Islands from 1947 to 1986 (1994 for Palau). The Northern Mariana Islands became a U.S. territory (part of the United States), while Federated States of Micronesia, the Marshall Islands, and Palau became independent countries. Each has signed a Compact of Free Association that gives the United States exclusive military access in return for U.S. defense protection and conduct of military foreign affairs (except the declaration of war) and a few billion dollars of aid. These agreements also generally allow citizens of these countries to live and work in the United States with their spouses (and vice versa), and provide for largely free trade. The federal government also grants access to services from domestic agencies, including the Federal Emergency Management Agency, National Weather Service, the United States Postal Service, the Federal Aviation Administration, the Federal Communications Commission, and U.S. representation to the International Frequency Registration Board of the International Telecommunication Union.

The United States notably does not participate in various international agreements adhered to by almost all other industrialized countries, by almost all the countries of the Americas, or by almost all other countries in the world. With a large population and economy, on a practical level this can undermine the effect of certain agreements, or give other countries a precedent to cite for non-participation in various agreements.

In some cases the arguments against participation include that the United States should maximize its sovereignty and freedom of action, or that ratification would create a basis for lawsuits that would treat American citizens unfairly. In other cases, the debate became involved in domestic political issues, such as gun control, climate change, and the death penalty.

Examples include:

While America's relationships with Europe have tended to be in terms of multilateral frameworks, such as NATO, America's relations with Asia have tended to be based on a "hub and spoke" model using a series of bilateral relationships where states coordinate with the United States and do not collaborate with each other. On May 30, 2009, at the Shangri-La Dialogue Defense Secretary Robert M. Gates urged the nations of Asia to build on this hub and spoke model as they established and grew multilateral institutions such as ASEAN, APEC and the ad hoc arrangements in the area. However, in 2011 Gates said that the United States must serve as the "indispensable nation," for building multilateral cooperation.

As of 2014, the U.S. currently produces about 66% of the oil that it consumes. While its imports have exceeded domestic production since the early 1990s, new hydraulic fracturing techniques and discovery of shale oil deposits in Canada and the American Dakotas offer the potential for increased energy independence from oil exporting countries such as OPEC. Former U.S. President George W. Bush identified dependence on imported oil as an urgent ""national security concern"."

Two-thirds of the world's proven oil reserves are estimated to be found in the
Persian Gulf. Despite its distance, the Persian Gulf region was first proclaimed to be of national interest to the United States during World War II. Petroleum is of central importance to modern armies, and the United States—as the world's leading oil producer at that time—supplied most of the oil for the Allied armies. Many U.S. strategists were concerned that the war would dangerously reduce the U.S. oil supply, and so they sought to establish good relations with Saudi Arabia, a kingdom with large oil reserves.

The Persian Gulf region continued to be regarded as an area of vital importance to the United States during the Cold War. Three Cold War United States Presidential doctrines—the Truman Doctrine, the Eisenhower Doctrine, and the Nixon Doctrine—played roles in the formulation of the Carter Doctrine, which stated that the United States would use military force if necessary to defend its "national interests" in the Persian Gulf region. Carter's successor, President Ronald Reagan, extended the policy in October 1981 with what is sometimes called the ""Reagan Corollary to the Carter Doctrine"", which proclaimed that the United States would intervene to protect Saudi Arabia, whose security was threatened after the outbreak of the Iran–Iraq War. Some analysts have argued that the implementation of the Carter Doctrine and the Reagan Corollary also played a role in the outbreak of the 2003 Iraq War.

Almost all of Canada's energy exports go to the United States, making it the largest foreign source of U.S. energy imports: Canada is consistently among the top sources for U.S. oil imports, and it is the largest source of U.S. natural gas and electricity imports.

In 2007 the U.S. was Sub-Saharan Africa's largest single export market accounting for 28% of exports (second in total to the EU at 31%). 81% of U.S. imports from this region were petroleum products.

Foreign assistance is a core component of the State Department's international affairs budget, which is $49 billion in all for 2014. Aid is considered an essential instrument of U.S. foreign policy. There are four major categories of non-military foreign assistance: bilateral development aid, economic assistance supporting U.S. political and security goals, humanitarian aid, and multilateral economic contributions (for example, contributions to the World Bank and International Monetary Fund).

In absolute dollar terms, the United States government is the largest international aid donor ($23 billion in 2014). The U.S. Agency for International Development (USAID) manages the bulk of bilateral economic assistance; the Treasury Department handles most multilateral aid. In addition many private agencies, churches and philanthropies provide aid.

Although the United States is the largest donor in absolute dollar terms, it is actually ranked 19 out of 27 countries on the Commitment to Development Index. The CDI ranks the 27 richest donor countries on their policies that affect the developing world. In the aid component the United States is penalized for low net aid volume as a share of the economy, a large share of tied or partially tied aid, and a large share of aid given to less poor and relatively undemocratic governments.

Foreign aid is a highly partisan issue in the United States, with liberals, on average, supporting foreign aid much more than conservatives do.

As of 2016, the United States is actively conducting military operations against the Islamic State of Iraq and the Levant and Al-Qaeda under the Authorization for Use of Military Force Against Terrorists, including in areas of fighting in the Syrian Civil War and Yemeni Civil War. The Guantanamo Bay Naval Base holds what the federal government considers unlawful combatants from these ongoing activities, and has been a controversial issue in foreign relations, domestic politics, and Cuba–United States relations. Other major U.S. military concerns include stability in Afghanistan and Iraq after the recent invasions of those countries, and Russian military activity in Ukraine.

The United States is a founding member of NATO, an alliance of 28 North American and European nations formed to defend Western Europe against the Soviet Union during the Cold War. Under the NATO charter, the United States is compelled to defend any NATO state that is attacked by a foreign power. The United States itself was the first country to invoke the mutual defense provisions of the alliance, in response to the September 11 attacks.

The United States also has mutual military defense treaties with:

The United States has responsibility for the defense of the three Compact of Free Association states: Federated States of Micronesia, the Marshall Islands, and Palau.

In 1989, the United States also granted five nations the major non-NATO ally status (MNNA), and additions by later presidents have brought the list to 28 nations. Each such state has a unique relationship with the United States, involving various military and economic partnerships and alliances.

and lesser agreements with:

The U.S. participates in various military-related multi-lateral organizations, including:

The U.S. also operates hundreds of military bases around the world.

The United States has undertaken unilateral and multilateral military operations throughout its history (see Timeline of United States military operations). In the post-World War II era, the country has had permanent membership and veto power in the United Nations Security Council, allowing it to undertake any military action without formal Security Council opposition. With vast military expenditures, the United States is known as the sole remaining superpower after the collapse of the Soviet Union. The U.S. contributes a relatively small number of personnel for United Nations peacekeeping operations. It sometimes acts through NATO, as with the NATO intervention in Bosnia and Herzegovina, NATO bombing of Yugoslavia, and ISAF in Afghanistan, but often acts unilaterally or in ad-hoc coalitions as with the 2003 invasion of Iraq.

The United Nations Charter requires that military operations be either for self-defense or affirmatively approved by the Security Council. Though many of their operations have followed these rules, the United States and NATO have been accused of committing crimes against peace in international law, for example in the 1999 Yugoslavia and 2003 Iraq operations.

The U.S. provides military aid through many different channels. Counting the items that appear in the budget as 'Foreign Military Financing' and 'Plan Colombia', the U.S. spent approximately $4.5 billion in military aid in 2001, of which $2 billion went to Israel, $1.3 billion went to Egypt, and $1 billion went to Colombia. Since 9/11, Pakistan has received approximately $11.5 billion in direct military aid.

As of 2004, according to Fox News, the U.S. had more than 700 military bases in 130 different countries.

Estimated U.S. foreign military financing and aid by recipient for 2010:

According to a 2016 report by the Congressional Research Service, the U.S. topped the market in global weapon sales for 2015, with $40 billion sold. The largest buyers were Qatar, Egypt, Saudi Arabia, South Korea, Pakistan, Israel, the United Arab Emirates and Iraq.

The Strategic Defense Initiative (SDI) was a proposal by U.S. President Ronald Reagan on March 23, 1983 to use ground and space-based systems to protect the United States from attack by strategic nuclear ballistic missiles, later dubbed ""Star Wars"". The initiative focused on strategic defense rather than the prior strategic offense doctrine of mutual assured destruction (MAD). Though it was never fully developed or deployed, the research and technologies of SDI paved the way for some anti-ballistic missile systems of today.

In February 2007, the U.S. started formal negotiations with Poland and Czech Republic concerning construction of missile shield installations in those countries for a Ground-Based Midcourse Defense system (in April 2007, 57% of Poles opposed the plan). According to press reports the government of the Czech Republic agreed (while 67% Czechs disagree) to host a missile defense radar on its territory while a base of missile interceptors is supposed to be built in Poland.

Russia threatened to place short-range nuclear missiles on the Russia's border with NATO if the United States refuses to abandon plans to deploy 10 interceptor missiles and a radar in Poland and the Czech Republic. In April 2007, Putin warned of a new Cold War if the Americans deployed the shield in Central Europe. Putin also said that Russia is prepared to abandon its obligations under an Intermediate-Range Nuclear Forces Treaty of 1987 with the United States.

On August 14, 2008, the United States and Poland announced a deal to implement the missile defense system in Polish territory, with a tracking system placed in the Czech Republic. "The fact that this was signed in a period of very difficult crisis in the relations between Russia and the United States over the situation in Georgia shows that, of course, the missile defense system will be deployed not against Iran but against the strategic potential of Russia", Dmitry Rogozin, Russia's NATO envoy, said.

Keir A. Lieber and Daryl G. Press, argue in Foreign Affairs that U.S. missile defenses are designed to secure Washington's nuclear primacy and are chiefly directed at potential rivals, such as Russia and China. The authors note that Washington continues to eschew nuclear first strike and contend that deploying missile defenses "would be valuable primarily in an offensive context, not a defensive one; as an adjunct to a US First Strike capability, not as a stand-alone shield":

If the United States launched a nuclear attack against Russia (or China), the targeted country would be left with only a tiny surviving arsenal, if any at all. At that point, even a relatively modest or inefficient missile defense system might well be enough to protect against any retaliatory strikes.

This analysis is corroborated by the Pentagon's 1992 Defense Planning Guidance (DPG), prepared by then Secretary of Defense Richard Cheney and his deputies. The DPG declares that the United States should use its power to "prevent the reemergence of a new rival" either on former Soviet territory or elsewhere. The authors of the Guidance determined that the United States had to "Field a missile defense system as a shield against accidental missile launches or limited missile strikes by 'international outlaws'" and also must "Find ways to integrate the 'new democracies' of the former Soviet bloc into the U.S.-led system". The National Archive notes that Document 10 of the DPG includes wording about "disarming capabilities to destroy" which is followed by several blacked out words. "This suggests that some of the heavily excised pages in the still-classified DPG drafts may include some discussion of preventive action against threatening nuclear and other WMD programs."

Finally, Robert David English, writing in Foreign Affairs, observes that in addition to the deployment U.S. missile defenses, the DPG's second recommendation has also been proceeding on course. "Washington has pursued policies that have ignored Russian interests (and sometimes international law as well) in order to encircle Moscow with military alliances and trade blocs conducive to U.S. interests."

In United States history, critics have charged that presidents have used democracy to justify military intervention abroad. Critics have also charged that the U.S. helped local militaries overthrow democratically elected governments in Iran, Guatemala, and in other instances. Studies have been devoted to the historical success rate of the U.S. in exporting democracy abroad. Some studies of American intervention have been pessimistic about the overall effectiveness of U.S. efforts to encourage democracy in foreign nations. Until recently, scholars have generally agreed with international relations professor Abraham Lowenthal that U.S. attempts to export democracy have been "negligible, often counterproductive, and only occasionally positive." Other studies find U.S. intervention has had mixed results, and another by Hermann and Kegley has found that military interventions have improved democracy in other countries.

Professor Paul W. Drake argued that the U.S. first attempted to export democracy in Latin America through intervention from 1912 to 1932. Drake argued that this was contradictory because international law defines intervention as "dictatorial interference in the affairs of another state for the purpose of altering the condition of things." The study suggested that efforts to promote democracy failed because democracy needs to develop out of internal conditions, and can not be forcibly imposed. There was disagreement about what constituted "democracy"; Drake suggested American leaders sometimes defined democracy in a narrow sense of a nation having elections; Drake suggested a broader understanding was needed. Further, there was disagreement about what constituted a "rebellion"; Drake saw a pattern in which the U.S. State Department disapproved of any type of rebellion, even so-called "revolutions", and in some instances rebellions against dictatorships. Historian Walter LaFeber stated, "The world's leading revolutionary nation (the U.S.) in the eighteenth century became the leading protector of the status quo in the twentieth century."

Mesquita and Downs evaluated 35 U.S. interventions from 1945 to 2004 and concluded that in only one case, Colombia, did a "full fledged, stable democracy" develop within ten years following the intervention. Samia Amin Pei argued that nation building in developed countries usually unravelled four to six years after American intervention ended. Pei, based on study of a database on worldwide democracies called "Polity", agreed with Mesquita and Downs that U.S. intervention efforts usually don't produce real democracies, and that most cases result in greater authoritarianism after ten years.

Professor Joshua Muravchik argued U.S. occupation was critical for Axis power democratization after World War II, but America's failure to encourage democracy in the third world "prove ... that U.S. military occupation is not a sufficient condition to make a country democratic." The success of democracy in former Axis countries such as Italy were seen as a result of high national per-capita income, although U.S. protection was seen as a key to stabilization and important for encouraging the transition to democracy. Steven Krasner agreed that there was a link between wealth and democracy; when per-capita incomes of $6,000 were achieved in a democracy, there was little chance of that country ever reverting to an autocracy, according to an analysis of his research in the "Los Angeles Times".

Tures examined 228 cases of American intervention from 1973 to 2005, using Freedom House data. A plurality of interventions, 96, caused no change in the country's democracy. In 69 instances, the country became less democratic after the intervention. In the remaining 63 cases, a country became more democratic. However this does not take into account the direction the country would have gone with no U.S. intervention.

Hermann and Kegley found that American military interventions designed to protect or promote democracy increased freedom in those countries. Peceny argued that the democracies created after military intervention are still closer to an autocracy than a democracy, quoting Przeworski "while some democracies are more democratic than others, unless offices are contested, no regime should be considered democratic." Therefore, Peceny concludes, it is difficult to know from the Hermann and Kegley study whether U.S. intervention has only produced less repressive autocratic governments or genuine democracies.

Peceny stated that the United States attempted to export democracy in 33 of its 93 20th-century military interventions. Peceny argued that proliberal policies after military intervention had a positive impact on democracy.

A global survey done by Pewglobal indicated that at (as of 2014) least 33 surveyed countries have a positive view (50% or above) of the United States. With the top ten most positive countries being Philippines (92%), Israel (84%), South Korea (82%), Kenya (80%), El Salvador (80%), Italy (78%), Ghana (77%), Vietnam (76%), Bangladesh (76%), and Tanzania (75%). While 10 surveyed countries have the most negative view (Below 50%) of the United States. With the countries being Egypt (10%), Jordan (12%), Pakistan (14%), Turkey (19%), Russia (23%), Palestinian Territories (30%), Greece (34%), Argentina (36%), Lebanon (41%), Tunisia (42%). Americans' own view of the United States was viewed at 84%.
International opinion about the US has often changed with different executive administrations. For example in 2009, the French public favored the United States when President Barack Obama (75% favorable) replaced President George W. Bush (42%). After President Donald Trump took the helm in 2017, French public opinion about the US fell from 63% to 46%. These trends were also seen in other European countries.

United States foreign policy also includes covert actions to topple foreign governments that have been opposed to the United States. According to J. Dana Stuster, writing in "Foreign Policy", there are seven "confirmed cases" where the U.S.—acting principally through the Central Intelligence Agency (CIA), but sometimes with the support of other parts of the U.S. government, including the Navy and State Department—covertly assisted in the overthrow of a foreign government: Iran in 1953, Guatemala in 1954, Congo in 1960, the Dominican Republic in 1961, South Vietnam in 1963, Brazil in 1964, and Chile in 1973. Stuster states that this list excludes "U.S.-supported insurgencies and failed assassination attempts" such as those directed against Cuba's Fidel Castro, as well as instances where U.S. involvement has been alleged but not proven (such as Syria in 1949).

In 1953 the CIA, working with the British government, initiated "Operation Ajax" against the Prime Minister of Iran Mohammad Mossadegh who had attempted to nationalize Iran's oil, threatening the interests of the Anglo-Persian Oil Company. This had the effect of restoring and strengthening the authoritarian monarchical reign of Shah Mohammad Reza Pahlavi. In 1957, the CIA and Israeli Mossad aided the Iranian government in establishing its intelligence service, SAVAK, later blamed for the torture and execution of the regime's opponents.

A year later, in "Operation PBSUCCESS", the CIA assisted the local military in toppling the democratically elected left-wing government of Jacobo Árbenz in Guatemala and installing the military dictator Carlos Castillo Armas. The United Fruit Company lobbied for Árbenz overthrow as his land reforms jeopardized their land holdings in Guatemala, and painted these reforms as a communist threat. The coup triggered a decades long civil war which claimed the lives of an estimated 200,000 people (42,275 individual cases have been documented), mostly through 626 massacres against the Maya population perpetrated by the U.S.-backed Guatemalan military. An independent Historical Clarification Commission found that U.S. corporations and government officials "exercised pressure to maintain the country's archaic and unjust socio-economic structure," and that the CIA backed illegal counterinsurgency operations.

During the massacre of at least 500,000 alleged communists in 1960s Indonesia, U.S. government officials encouraged and applauded the mass killings while providing covert assistance to the Indonesian military which helped facilitate them. This included the U.S. Embassy in Jakarta supplying Indonesian forces with lists of up to 5,000 names of suspected members of the Communist Party of Indonesia (PKI), who were subsequently killed in the massacres. In 2001, the CIA attempted to prevent the publication of the State Department volume "Foreign Relations of the United States, 1964–1968", which documents the U.S. role in providing covert assistance to the Indonesian military for the express purpose of the extirpation of the PKI. In July 2016, an international panel of judges ruled the killings constitute crimes against humanity, and that the US, along with other Western governments, were complicit in these crimes.

In 1970, the CIA worked with coup-plotters in Chile in the attempted kidnapping of General René Schneider, who was targeted for refusing to participate in a military coup upon the election of Salvador Allende. Schneider was shot in the botched attempt and died three days later. The CIA later paid the group $35,000 for the failed kidnapping.

According to one peer-reviewed study, the U.S. intervened in 81 foreign elections between 1946 and 2000, while the Soviet Union or Russia intervened in 36.

Since the 1970s, issues of human rights have become increasingly important in American foreign policy. Congress took the lead in the 1970s. Following the Vietnam War, the feeling that U.S. foreign policy had grown apart from traditional American values was seized upon by Senator Donald M. Fraser (D, MI), leading the Subcommittee on International Organizations and Movements, in criticizing Republican Foreign Policy under the Nixon administration. In the early 1970s, Congress concluded the Vietnam War and passed the War Powers Act. As "part of a growing assertiveness by Congress about many aspects of Foreign Policy," Human Rights concerns became a battleground between the Legislative and the Executive branches in the formulation of foreign policy. David Forsythe points to three specific, early examples of Congress interjecting its own thoughts on foreign policy:

These measures were repeatedly used by Congress, with varying success, to affect U.S. foreign policy towards the inclusion of Human Rights concerns. Specific examples include El Salvador, Nicaragua, Guatemala and South Africa. The Executive (from Nixon to Reagan) argued that the Cold War required placing regional security in favor of U.S. interests over any behavioral concerns of national allies. Congress argued the opposite, in favor of distancing the United States from oppressive regimes. Nevertheless, according to historian Daniel Goldhagen, during the last two decades of the Cold War, the number of American client states practicing mass murder outnumbered those of the Soviet Union. John Henry Coatsworth, a historian of Latin America and the provost of Columbia University, suggests the number of repression victims in Latin America alone far surpassed that of the USSR and its East European satellites during the period 1960 to 1990. W. John Green contends that the United States was an "essential enabler" of "Latin America's political murder habit, bringing out and allowing to flourish some of the region's worst tendencies."

On December 6, 2011, Obama instructed agencies to consider LGBT rights when issuing financial aid to foreign countries. He also criticized Russia's law discriminating against gays, joining other western leaders in the boycott of the 2014 Winter Olympics in Russia.

In June 2014, a Chilean court ruled that the United States played a key role in the murders of Charles Horman and Frank Teruggi, both American citizens, shortly after the 1973 Chilean coup d'état.

United States foreign policy is influenced by the efforts of the U.S. government to control imports of illicit drugs, including cocaine, heroin, methamphetamine, and cannabis. This is especially true in Latin America, a focus for the U.S. War on Drugs. Those efforts date back to at least 1880, when the U.S. and China completed an agreement that prohibited the shipment of opium between the two countries.

Over a century later, the Foreign Relations Authorization Act requires the President to identify the major drug transit or major illicit drug-producing countries. In September 2005, the following countries were identified: Bahamas, Bolivia, Brazil, Burma, Colombia, Dominican Republic, Ecuador, Guatemala, Haiti, India, Jamaica, Laos, Mexico, Nigeria, Pakistan, Panama, Paraguay, Peru and Venezuela. Two of these, Burma and Venezuela are countries that the U.S. considers to have failed to adhere to their obligations under international counternarcotics agreements during the previous 12 months. Notably absent from the 2005 list were Afghanistan, the People's Republic of China and Vietnam; Canada was also omitted in spite of evidence that criminal groups there are increasingly involved in the production of MDMA destined for the United States and that large-scale cross-border trafficking of Canadian-grown cannabis continues. The U.S. believes that the Netherlands are successfully countering the production and flow of MDMA to the U.S.

Critics from the left cite episodes that undercut leftist governments or showed support for Israel. Others cite human rights abuses and violations of international law. Critics have charged that the U.S. presidents have used democracy to justify military intervention abroad. Critics also point to declassified records which indicate that the CIA under Allen Dulles and the FBI under J. Edgar Hoover aggressively recruited more than 1,000 Nazis, including those responsible for war crimes, to use as spies and informants against the Soviet Union in the Cold War.

The U.S. has faced criticism for backing right-wing dictators that systematically violated human rights, such as Augusto Pinochet of Chile, Alfredo Stroessner of Paraguay, Efraín Ríos Montt of Guatemala, Jorge Rafael Videla of Argentina, Hissène Habré of Chad Yahya Khan of Pakistan and Suharto of Indonesia. Critics have also accused the United States of facilitating and supporting state terrorism in the Global South during the Cold War, such as Operation "Condor", an international campaign of political assassination and state terror organized by right-wing military dictatorships in the Southern Cone of South America.

Journalists and human rights organizations have been critical of US-led airstrikes and targeted killings by drones which have in some cases resulted in collateral damage of civilian populations. In early 2017, the U.S. faced criticism from some scholars, activists and media outlets for dropping 26,171 bombs on seven different countries throughout 2016: Syria, Iraq, Afghanistan, Libya, Yemen, Somalia and Pakistan.

Studies have been devoted to the historical success rate of the U.S. in exporting democracy abroad. Some studies of American intervention have been pessimistic about the overall effectiveness of U.S. efforts to encourage democracy in foreign nations. Some scholars have generally agreed with international relations professor Abraham Lowenthal that U.S. attempts to export democracy have been "negligible, often counterproductive, and only occasionally positive." Other studies find U.S. intervention has had mixed results, and another by Hermann and Kegley has found that military interventions have improved democracy in other countries. A 2013 global poll in 68 countries with 66,000 respondents by Win/Gallup found that the U.S. is perceived as the biggest threat to world peace.

Regarding support for certain anti-Communist dictatorships during the Cold War, a response is that they were seen as a necessary evil, with the alternatives even worse Communist or fundamentalist dictatorships. David Schmitz says this policy did not serve U.S. interests. Friendly tyrants resisted necessary reforms and destroyed the political center (though not in South Korea), while the 'realist' policy of coddling dictators brought a backlash among foreign populations with long memories.

Many democracies have voluntary military ties with United States. See NATO, ANZUS, Treaty of Mutual Cooperation and Security between the United States and Japan, Mutual Defense Treaty with South Korea, and Major non-NATO ally. Those nations with military alliances with the U.S. can spend less on the military since they can count on U.S. protection. This may give a false impression that the U.S. is less peaceful than those nations.

Research on the democratic peace theory has generally found that democracies, including the United States, have not made war on one another. There have been U.S. support for coups against some democracies, but for example Spencer R. Weart argues that part of the explanation was the perception, correct or not, that these states were turning into Communist dictatorships. Also important was the role of rarely transparent United States government agencies, who sometimes mislead or did not fully implement the decisions of elected civilian leaders.

Empirical studies (see democide) have found that democracies, including the United States, have killed much fewer civilians than dictatorships. Media may be biased against the U.S. regarding reporting human rights violations. Studies have found that "The New York Times" coverage of worldwide human rights violations predominantly focuses on the human rights violations in nations where there is clear U.S. involvement, while having relatively little coverage of the human rights violations in other nations. For example, the bloodiest war in recent time, involving eight nations and killing millions of civilians, was the Second Congo War, which was almost completely ignored by the media.

Niall Ferguson argues that the U.S. is incorrectly blamed for all the human rights violations in nations they have supported. He writes that it is generally agreed that Guatemala was the worst of the US-backed regimes during the Cold War. However, the U.S. cannot credibly be blamed for all the 200,000 deaths during the long Guatemalan Civil War. The U.S. Intelligence Oversight Board writes that military aid was cut for long periods because of such violations, that the U.S. helped stop a coup in 1993, and that efforts were made to improve the conduct of the security services.
Today the U.S. states that democratic nations best support U.S. national interests. According to the U.S. State Department, "Democracy is the one national interest that helps to secure all the others. Democratically governed nations are more likely to secure the peace, deter aggression, expand open markets, promote economic development, protect American citizens, combat international terrorism and crime, uphold human and worker rights, avoid humanitarian crises and refugee flows, improve the global environment, and protect human health." According to former U.S. President Bill Clinton, "Ultimately, the best strategy to ensure our security and to build a durable peace is to support the advance of democracy elsewhere. Democracies don't attack each other." In one view mentioned by the U.S. State Department, democracy is also good for business. Countries that embrace political reforms are also more likely to pursue economic reforms that improve the productivity of businesses. Accordingly, since the mid-1980s, under President Ronald Reagan, there has been an increase in levels of foreign direct investment going to emerging market democracies relative to countries that have not undertaken political reforms. Leaked cables in 2010 suggested that the "dark shadow of terrorism still dominates the United States' relations with the world".

The United States officially maintains that it supports democracy and human rights through several tools Examples of these tools are as follows:










</doc>
<doc id="7565" url="https://en.wikipedia.org/wiki?curid=7565" title="Christmas in Poland">
Christmas in Poland

Christmas in Poland is a major annual celebration, as in most countries of the Christian world. The observance of Christmas developed gradually over the centuries, beginning in ancient times; combining old Polish pagan customs with the religious ones introduced after the Christianization of Poland by the Catholic Church. Later influences include mutual permeating of local traditions and various folk cultures. It is one of the most important religious holidays for Poles, who follow a somewhat strict traditional custom. Christmas trees are decorated and lit in family rooms on the day of Christmas Eve. Other trees are placed in most public areas and outside churches. Christmas in Poland is called "Boże Narodzenie", which translates to 'God's Birth'. 

The Day of Saint Nicholas on 6 December is the unofficial beginning of the festive season in Poland. Well-behaved children receive small gifts on the day, whereas naughty children receive a lump of coal or a twig, called "rózga". The highlight of the holiday is the most significant day, Christmas Eve on the 24 December. Polish Wigilia begins with the appearance of the first star, which corresponds to the Star of Bethlehem. During preparation, hay is spread beneath the tablecloth as a reminder that Jesus Christ was born in a manger. According to tradition, an empty place setting is symbolically left at the table for the Lord or, for a lost wanderer who may be in need of food or shelter. The supper begins with the breaking of the Christmas wafer known as "opłatek", a custom only observed in Poland and some parts of Lithuania. The meals must be meatless as a sign of fasting and twelve different dishes are made, thus symbolizing the Twelve Apostles. The celebration ends with the exchange of gifts and a midnight mass in churches. 

Other aspects of Polish Christmas include nativity plays called "Jasełka", outdoor nativity scenes, the singing of carols, most notably "God Is Born", and Kulig, a sleigh ride with Saint Nicholas.

Among the special tasks carried out in private homes during Advent (a time of waiting for the celebration of the Nativity of Jesus) is the baking of the Christmas piernik (gingerbread), and the making of Christmas decorations. Pierniks are made in a variety of shapes, including hearts, animals, and St. Nicholas figures. St. Nicholas does not play a major role on Christmas Day, but is celebrated on his Saint feast day of December 6. He visits good children in secret and leaves presents for them.

Traditionally, the Christmas trees are decorated with glass baubles, garlands and many homemade ornaments including painted eggshells, shiny red apples, walnuts, wrapped chocolate shapes, candles, etc. They are lit on Christmas Eve before Wigilia. At the top of each tree there is a star or a glittering tree topper. In many homes, sparklers are hung on the branches of the trees for wintery ambiance. Sometimes the trees are left standing until February 2, the feast day of St. Mary of the Candle of Lighting.

During Advent and all the way until Epiphany, or the baptism of Jesus (day of January 6), the "gwiazdory", or the star carriers walk through the villages. Some of them sing carols; others recite verses or put on "szopki", or "herody" (nativity scenes). The last two customs are inspired by the traditional manger scenes or "Jasełka" (crib). One tradition unique to Poland is the sharing of the "opłatek", a thin wafer into which a holy picture is pressed. In the old days people carried these wafers from house to house wishing their neighbors a Merry Christmas. Nowadays, opłatek is mostly shared with members of the family and immediate neighbors before the Christmas Eve supper (Wigilia in the Polish language). As each person shares pieces of the wafer with another, they are supposed to forgive each other any hurts that have occurred over the past year and wish them happiness in the coming year.

In Poland, Christmas Eve is a day first of fasting, then of feasting. The Wigilia feast begins at the appearance of the first star. There is no red meat served but fish, usually carp. The supper, which includes many traditional dishes and desserts, can sometimes last for over two hours. It is followed by the exchange of gifts. The next day, the Christmas Day, is often spent visiting friends. In Polish tradition, people combine religion and family closeness at Christmas. Although gift-giving plays a major role in the rituals, emphasis is placed more on the making of special foods and decorations.

On the night of Christmas Eve, so important is the appearance of the first star in remembrance of the Star of Bethlehem, that it has been given an affectionate name of "the little star" or Gwiazdka (the female counterpart of St. Nicholas). On that evening, children watch the sky anxiously hoping to be the first to cry out, "The star has come!" Only after it appears, the family members sit down to a dinner table.

According to tradition, bits of hay are spread beneath the tablecloth as a reminder that Christ was born in a manger. Others partake in the practice of placing money under the table cloth for each guest, in order to wish for prosperity in the coming year. Some practice the superstition that an even number of people must be seated around the table. In many homes an empty place setting is symbolically left at the table for the Baby Jesus or, for a lonely wanderer who may be in need of food, or if a deceased relative should come and would like to share in the meal.

The supper begins with the breaking of the opłatek. Everyone at the table breaks off a piece and eats it as a symbol of their unity with Christ. They then share a piece with each family member. A tradition exists among some families to serve twelve different dishes at Wigilia symbolizing the Twelve Apostles, or perhaps, an odd number of dishes for good luck (usually five, seven, or nine).

A traditional Wigilia supper in Poland includes fried carp and borscht (beetroot soup) with uszka (ravioli). Carp provides a main component of the Christmas Eve meal across Poland; carp fillet, carp in aspic and gefilte fish. Universal Polish Christmas foods are pierogi as well as some herring dishes, and for dessert, makowiec or noodles with poppy seed. Often, there is a compote of dry fruits for a drink.

The remainder of the evening is given to stories and songs around the Christmas tree. In some areas of the country, children are taught that "The Little Star" brings the gifts. As presents are unwrapped, carollers may walk from house to house receiving treats along the way.

Christmas Eve ends with "Pasterka", the Midnight Mass at the local church. The tradition commemorates the arrival of the shepards to Bethlehem and their paying of respect and bearing witness to the new born Messiah. The custom of Christmas night liturgy was introduced in the Christian churches after the second half of the 5th century. In Poland that custom arrived together with the coming of Christianity. The next day (December 25) begins with the early morning mass followed by daytime masses. According to scripture, the Christmas Day masses are interchangeable allowing for greater flexibility in choosing the religious services by individual parishioners.

Christmas carols are not celebrated in Poland until during-and-after the Christmas Vigil Mass called "Pasterka" held between 24 and 25 of December. The Christmas season often runs until February 2. The early hymns sung in Catholic church were brought to Poland by the Franciscan Brothers in the Middle Ages. The early Christmas music was Latin in origin. When the Polish words and melodies started to become popular, including many new secular pastorals (pastoralka, or shepherd's songs), they were not written down originally, but rather taught among people by heart. Notably, the song "God Is Born" ("Bóg się rodzi") with lyrics written by Franciszek Karpiński in 1792 became the Christmas hymn of Poland already in the court of King Stefan Batory. Many of the early Polish carols were collected in 1838 by in a book called "Pastorałki i Kolędy z Melodiami" (Pastorals and Carols with Melodies).

Poland produces some of the finest hand-blown glass Christmas ornaments in Europe. Families and collectors value these ornaments for high quality, traditional artwork, and unique decorations.

Polish blown-glass Christmas ornaments are generally manufactured only in winter season. The modern glass workshops and manufacturers tend to be localized in the southern regions of Poland.




</doc>
<doc id="7566" url="https://en.wikipedia.org/wiki?curid=7566" title="Carousel (musical)">
Carousel (musical)

Carousel is the second musical by the team of Richard Rodgers (music) and Oscar Hammerstein II (book and lyrics). The 1945 work was adapted from Ferenc Molnár's 1909 play "Liliom", transplanting its Budapest setting to the Maine coastline. The story revolves around carousel barker Billy Bigelow, whose romance with millworker Julie Jordan comes at the price of both their jobs. He participates in a robbery to provide for Julie and their unborn child; after it goes tragically wrong, he is given a chance to make things right. A secondary plot line deals with millworker Carrie Pipperidge and her romance with ambitious fisherman Enoch Snow. The show includes the well-known songs "If I Loved You", "June Is Bustin' Out All Over" and "You'll Never Walk Alone". Richard Rodgers later wrote that "Carousel" was his favorite of all his musicals.

Following the spectacular success of the first Rodgers and Hammerstein musical, "Oklahoma!" (1943), the pair sought to collaborate on another piece, knowing that any resulting work would be compared with "Oklahoma!", most likely unfavorably. They were initially reluctant to seek the rights to "Liliom"; Molnár had refused permission for the work to be adapted in the past, and the original ending was considered too depressing for the musical theatre. After acquiring the rights, the team created a work with lengthy sequences of music and made the ending more hopeful.

The musical required considerable modification during out-of-town tryouts, but once it opened on Broadway on April 19, 1945, it was an immediate hit with both critics and audiences. "Carousel" initially ran for 890 performances and duplicated its success in the West End in 1950. Though it has never achieved as much commercial success as "Oklahoma!", the piece has been repeatedly revived, recorded several times and was filmed in 1956. A production by Nicholas Hytner enjoyed success in 1992 in London, in 1994 in New York and on tour. Another Broadway revival opened in 2018. In 1999, "Time" magazine named "Carousel" the best musical of the 20th century.

Ferenc Molnár's Hungarian-language drama, "Liliom", premiered in Budapest in 1909. The audience was puzzled by the work, and it lasted only thirty-odd performances before being withdrawn, the first shadow on Molnár's successful career as a playwright. "Liliom" was not presented again until after World War I. When it reappeared on the Budapest stage, it was a tremendous hit.
Except for the ending, the plots of "Liliom" and "Carousel" are very similar. Andreas Zavocky (nicknamed Liliom, the Hungarian word for "lily", a slang term for "tough guy"), a carnival barker, falls in love with Julie Zeller, a servant girl, and they begin living together. With both discharged from their jobs, Liliom is discontented and contemplates leaving Julie, but decides not to do so on learning that she is pregnant. A subplot involves Julie's friend Marie, who has fallen in love with Wolf Biefeld, a hotel porter—after the two marry, he becomes the owner of the hotel. Desperate to make money so that he, Julie and their child can escape to America and a better life, Liliom conspires with lowlife Ficsur to commit a robbery, but it goes badly, and Liliom stabs himself. He dies, and his spirit is taken to heaven's police court. As Ficsur suggested while the two waited to commit the crime, would-be robbers like them do not come before God Himself. Liliom is told by the magistrate that he may go back to Earth for one day to attempt to redeem the wrongs he has done to his family, but must first spend sixteen years in a fiery purgatory.

On his return to Earth, Liliom encounters his daughter, Louise, who like her mother is now a factory worker. Saying that he knew her father, he tries to give her a star he stole from the heavens. When Louise refuses to take it, he strikes her. Not realizing who he is, Julie confronts him, but finds herself unable to be angry with him. Liliom is ushered off to his fate, presumably Hell, and Louise asks her mother if it is possible to feel a hard slap as if it was a kiss. Julie reminiscently tells her daughter that it is very possible for that to happen.

An English translation of "Liliom" was credited to Benjamin "Barney" Glazer, though there is a story that the actual translator, uncredited, was Rodgers' first major partner Lorenz Hart. The Theatre Guild presented it in New York City in 1921, with Joseph Schildkraut as Liliom, and the play was a success, running 300 performances. A 1940 revival, with Burgess Meredith and Ingrid Bergman was seen by both Hammerstein and Rodgers. Glazer, in introducing the English translation of "Liliom", wrote of the play's appeal:

And where in modern dramatic literature can such pearls be matched—Julie incoherently confessing to her dead lover the love she had always been ashamed to tell; Liliom crying out to the distant carousel the glad news that he is to be a father; the two thieves gambling for the spoils of their prospective robbery; Marie and Wolf posing for their portrait while the broken-hearted Julie stands looking after the vanishing Liliom, the thieves' song ringing in her ears; the two policemen grousing about pay and pensions while Liliom lies bleeding to death; Liliom furtively proffering his daughter the star he has stolen for her in heaven. ... The temptation to count the whole scintillating string is difficult to resist.

In the 1920s and 1930s, Rodgers and Hammerstein both became well known for creating Broadway hits with other partners. Rodgers, with Lorenz Hart, had produced a string of over two dozen musicals, including such popular successes as "Babes in Arms" (1937), "The Boys from Syracuse" (1938) and "Pal Joey" (1940). Some of Rodgers' work with Hart broke new ground in musical theatre: "On Your Toes" was the first use of ballet to sustain the plot (in the "Slaughter on Tenth Avenue" scene), while "Pal Joey" flouted Broadway tradition by presenting a knave as its hero. Hammerstein had written or co-written the words for such hits as "Rose-Marie" (1924), "The Desert Song" (1926), "The New Moon" (1927) and "Show Boat" (1927). Though less productive in the 1930s, he wrote material for musicals and films, sharing an Oscar for his song with Jerome Kern, "The Last Time I Saw Paris", which was included in the 1941 film "Lady Be Good".

By the early 1940s, Hart had sunk into alcoholism and emotional turmoil, becoming unreliable and prompting Rodgers to approach Hammerstein to ask if he would consider working with him. Hammerstein was eager to do so, and their first collaboration was "Oklahoma!" (1943). Thomas Hischak states, in his "The Rodgers and Hammerstein Encyclopedia", that "Oklahoma!" is "the single most influential work in the American musical theatre. In fact, the history of the Broadway musical can accurately be divided into what came before "Oklahoma!" and what came after it." An innovation for its time in integrating song, character, plot and dance, "Oklahoma!" would serve, according to Hischak, as "the model for Broadway shows for decades", and proved a huge popular and financial success. Once it was well-launched, what to do as an encore was a daunting challenge for the pair. Film producer Samuel Goldwyn saw "Oklahoma!" and advised Rodgers to shoot himself, which according to Rodgers "was Sam's blunt but funny way of telling me that I'd never create another show as good as "Oklahoma!"" As they considered new projects, Hammerstein wrote, "We're such fools. No matter what we do, everyone is bound to say, 'This is not another "Oklahoma!"' "

"Oklahoma!" had been a struggle to finance and produce. Hammerstein and Rodgers met weekly in 1943 with Theresa Helburn and Lawrence Langner of the Theatre Guild, producers of the blockbuster musical, who together formed what they termed "the Gloat Club". At one such luncheon, Helburn and Langner proposed to Rodgers and Hammerstein that they turn Molnár's "Liliom" into a musical. Both men refused—they had no feeling for the Budapest setting and thought that the unhappy ending was unsuitable for musical theatre. In addition, given the unstable wartime political situation, they might need to change the setting from Hungary while in rehearsal. At the next luncheon, Helburn and Langner again proposed "Liliom", suggesting that they move the setting to Louisiana and make Liliom a Creole. Rodgers and Hammerstein played with the idea over the next few weeks, but decided that Creole dialect, filled with "zis" and "zose", would sound corny and would make it difficult to write effective lyrics.

A breakthrough came when Rodgers, who owned a house in Connecticut, proposed a New England setting. Hammerstein wrote of this suggestion in 1945,

I began to see an attractive ensemble—sailors, whalers, girls who worked in the mills up the river, clambakes on near-by islands, an amusement park on the seaboard, things people could do in crowds, people who were strong and alive and lusty, people who had always been depicted on the stage as thin-lipped puritans—a libel I was anxious to refute ... as for the two leading characters, Julie with her courage and inner strength and outward simplicity seemed more indigenous to Maine than to Budapest. Liliom is, of course, an international character, indigenous to nowhere.
Rodgers and Hammerstein were also concerned about what they termed "the tunnel" of Molnár's second act—a series of gloomy scenes leading up to Liliom's suicide—followed by a dark ending. They also felt it would be difficult to set Liliom's motivation for the robbery to music. Molnár's opposition to having his works adapted was also an issue; he had famously turned down Giacomo Puccini when the great composer wished to transform "Liliom" into an opera, stating that he wanted the piece to be remembered as his, not Puccini's. In 1937, Molnár, who had recently emigrated to the United States, had declined another offer from Kurt Weill to adapt the play into a musical.

The pair continued to work on the preliminary ideas for a "Liliom" adaptation while pursuing other projects in late 1943 and early 1944—writing the film musical "State Fair" and producing "I Remember Mama" on Broadway. Meanwhile, the Theatre Guild took Molnár to see "Oklahoma!" Molnár stated that if Rodgers and Hammerstein could adapt "Liliom" as beautifully as they had modified "Green Grow the Lilacs" into "Oklahoma!", he would be pleased to have them do it. The Guild obtained the rights from Molnár in October 1943. The playwright received one percent of the gross and $2,500 for "personal services". The duo insisted, as part of the contract, that Molnár permit them to make changes in the plot. At first, the playwright refused, but eventually yielded. Hammerstein later stated that if this point had not been won, "we could never have made "Carousel"."

In seeking to establish through song Liliom's motivation for the robbery, Rodgers remembered that he and Hart had a similar problem in "Pal Joey". Rodgers and Hart had overcome the problem with a song that Joey sings to himself, "I'm Talking to My Pal". This inspired "Soliloquy". Both partners later told a story that "Soliloquy" was only intended to be a song about Liliom's dreams of a son, but that Rodgers, who had two daughters, insisted that Liliom consider that Julie might have a girl. However, the notes taken at their meeting of December 7, 1943 state: "Mr. Rodgers suggested a fine musical number for the end of the scene where Liliom discovers he is to be a father, in which he sings first with pride of the growth of a boy, and then suddenly realizes it might be a girl and changes completely."
Hammerstein and Rodgers returned to the "Liliom" project in mid-1944. Hammerstein was uneasy as he worked, fearing that no matter what they did, Molnár would disapprove of the results. "Green Grow the Lilacs" had been a little-known work; "Liliom" was a theatrical standard. Molnár's text also contained considerable commentary on the Hungarian politics of 1909 and the rigidity of that society. A dismissed carnival barker who hits his wife, attempts a robbery and commits suicide seemed an unlikely central character for a musical comedy. Hammerstein decided to use the words and story to make the audience sympathize with the lovers. He also built up the secondary couple, who are incidental to the plot in "Liliom"; they became Enoch Snow and Carrie Pipperidge. "This Was a Real Nice Clambake" was repurposed from a song, "A Real Nice Hayride", written for "Oklahoma!" but not used.

Molnár's ending was unsuitable, and after a couple of false starts, Hammerstein conceived the graduation scene that ends the musical. According to Frederick Nolan in his book on the team's works: "From that scene the song "You'll Never Walk Alone" sprang almost naturally." In spite of Hammerstein's simple lyrics for "You'll Never Walk Alone", Rodgers had great difficulty in setting it to music. Rodgers explained his rationale for the changed ending,

"Liliom" was a tragedy about a man who cannot learn to live with other people. The way Molnár wrote it, the man ends up hitting his daughter and then having to go back to purgatory, leaving his daughter helpless and hopeless. We couldn't accept that. The way we ended "Carousel" it may still be a tragedy but it's a hopeful one because in the final scene it is clear that the child has at last learned how to express herself and communicate with others.
When the pair decided to make "This Was a Real Nice Clambake" into an ensemble number, Hammerstein realized he had no idea what a clambake was like, and researched the matter. Based on his initial findings, he wrote the line, "First came codfish chowder". However, further research convinced him the proper term was "codhead chowder", a term unfamiliar to many playgoers. He decided to keep it as "codfish". When the song proceeded to discuss the lobsters consumed at the feast, Hammerstein wrote the line "We slit 'em down the back/And peppered 'em good". He was grieved to hear from a friend that lobsters are always slit down the front. The lyricist sent a researcher to a seafood restaurant and heard back that lobsters are always slit down the back. Hammerstein concluded that there is disagreement about which side of a lobster is the back. One error not caught involved the song "June Is Bustin' Out All Over", in which sheep are depicted as seeking to mate in late spring—they actually do so in the winter. Whenever this was brought to Hammerstein's attention, he told his informant that 1873 was a special year, in which sheep mated in the spring.

Rodgers early decided to dispense with an overture, feeling that the music was hard to hear over the banging of seats as latecomers settled themselves. In his autobiography, Rodgers complained that only the brass section can be heard during an overture because there are never enough strings in a musical's small orchestra. He determined to force the audience to concentrate from the beginning by opening with a pantomime scene accompanied by what became known as "The Carousel Waltz". The pantomime paralleled one in the Molnár play, which was also used to introduce the characters and situation to the audience. Author Ethan Mordden described the effectiveness of this opening:

Other characters catch our notice—Mr. Bascombe, the pompous mill owner, Mrs. Mullin, the widow who runs the carousel and, apparently, Billy; a dancing bear; an acrobat. But what draws us in is the intensity with which Julie regards Billy—the way she stands frozen, staring at him, while everyone else at the fair is swaying to the rhythm of Billy's spiel. And as Julie and Billy ride together on the swirling carousel, and the stage picture surges with the excitement of the crowd, and the orchestra storms to a climax, and the curtain falls, we realize that R & H have not only skipped the overture "and" the opening number but the exposition as well. They have plunged into the story, right into the middle of it, in the most intense first scene any musical ever had.
The casting for "Carousel" began when "Oklahoma!"'s production team, including Rodgers and Hammerstein, was seeking a replacement for the part of Curly (the male lead in "Oklahoma!"). Lawrence Langner had heard, through a relative, of a California singer named John Raitt, who might be suitable for the part. Langner went to hear Raitt, then urged the others to bring Raitt to New York for an audition. Raitt asked to sing "Largo al factotum", Figaro's aria from "The Barber of Seville", to warm up. The warmup was sufficient to convince the producers that not only had they found a Curly, they had found a Liliom (or Billy Bigelow, as the part was renamed). Theresa Helburn made another California discovery, Jan Clayton, a singer/actress who had made a few minor films for MGM. She was brought east and successfully auditioned for the part of Julie.

The producers sought to cast unknowns. Though many had played in previous Hammerstein or Rodgers works, only one, Jean Casto (cast as carousel owner Mrs. Mullin, and a veteran of "Pal Joey"), had ever played on Broadway before. It proved harder to cast the ensemble than the leads, due to the war—Rodgers told his casting director, John Fearnley, that the sole qualification for a dancing boy was that he be alive. Rodgers and Hammerstein reassembled much of the creative team that had made "Oklahoma!" a success, including director Rouben Mamoulian and choreographer Agnes de Mille. Miles White was the costume designer while Jo Mielziner (who had not worked on "Oklahoma!") was the scenic and lighting designer. Even though "Oklahoma!" orchestrator Russell Bennett had informed Rodgers that he was unavailable to work on "Carousel" due to a radio contract, Rodgers insisted he do the work in his spare time. He orchestrated "The Carousel Waltz" and "(When I Marry) Mister Snow" before finally being replaced by Don Walker. A new member of the creative team was Trude Rittmann, who arranged the dance music. Rittmann initially felt that Rodgers mistrusted her because she was a woman, and found him difficult to work with, but the two worked together on Rodgers' shows until the 1970s.

Rehearsals began in January 1945; either Rodgers or Hammerstein was always present. Raitt was presented with the lyrics for "Soliloquy" on a five-foot long sheet of paper—the piece ran nearly eight minutes. Staging such a long solo number presented problems, and Raitt later stated that he felt that they were never fully addressed. At some point during rehearsals, Molnár came to see what they had done to his play. There are a number of variations on the story. As Rodgers told it, while watching rehearsals with Hammerstein, the composer spotted Molnár in the rear of the theatre and whispered the news to his partner. Both sweated through an afternoon of rehearsal in which nothing seemed to go right. At the end, the two walked to the back of the theatre, expecting an angry reaction from Molnár. Instead, the playwright said enthusiastically, "What you have done is so beautiful. And you know what I like best? The ending!" Hammerstein wrote that Molnár became a regular attendee at rehearsals after that.

Like most of the pair's works, "Carousel" contains a lengthy ballet, "Billy Makes a Journey", in the second act, as Billy looks down to the Earth from "Up There" and observes his daughter. In the original production the ballet was choreographed by de Mille. It began with Billy looking down from heaven at his wife in labor, with the village women gathered for a "birthing". The ballet involved every character in the play, some of whom spoke lines of dialogue, and contained a number of subplots. The focus was on Louise, played by Bambi Linn, who at first almost soars in her dance, expressing the innocence of childhood. She is teased and mocked by her schoolmates, and Louise becomes attracted to the rough carnival people, who symbolize Billy's world. A youth from the carnival attempts to seduce Louise, as she discovers her own sexuality, but he decides she is more girl than woman, and he leaves her. After Julie comforts her, Louise goes to a children's party, where she is shunned. The carnival people reappear and form a ring around the children's party, with Louise lost between the two groups. At the end, the performers form a huge carousel with their bodies.

The play opened for tryouts in New Haven, Connecticut on March 22, 1945. The first act was well-received; the second act was not. Casto recalled that the second act finished about 1:30 a.m. The staff immediately sat down for a two-hour conference. Five scenes, half the ballet, and two songs were cut from the show as the result. John Fearnley commented, "Now I see why these people have hits. I never witnessed anything so brisk and brave in my life." De Mille said of this conference, "not three minutes had been wasted pleading for something cherished. Nor was there any idle joking. ... We cut and cut and cut and then we went to bed." By the time the company left New Haven, de Mille's ballet was down to forty minutes.

A major concern with the second act was the effectiveness of the characters He and She (later called by Rodgers "Mr. and Mrs. God"), before whom Billy appeared after his death. Mr. and Mrs. God were depicted as a New England minister and his wife, seen in their parlor. The couple was still part of the show at the Boston opening. Rodgers said to Hammerstein, "We've got to get God out of that parlor". When Hammerstein inquired where he should put the deity, Rodgers replied, "I don't care where you put Him. Put Him on a ladder for all I care, only get Him out of that parlor!" Hammerstein duly put Mr. God (renamed the Starkeeper) atop a ladder, and Mrs. God was removed from the show. Rodgers biographer Meryle Secrest terms this change a mistake, leading to a more fantastic afterlife, which was later criticized by "The New Republic" as "a Rotarian atmosphere congenial to audiences who seek not reality but escape from reality, not truth but escape from truth".

Hammerstein wrote that Molnár's advice, to combine two scenes into one, was key to pulling together the second act and represented "a more radical departure from the original than any change we had made". A reprise of "If I Loved You" was added in the second act, which Rodgers felt needed more music. Three weeks of tryouts in Boston followed the brief New Haven run, and the audience there gave the musical a warm reception. An even shorter version of the ballet was presented the final two weeks in Boston, but on the final night there, de Mille expanded it back to forty minutes, and it brought the house down, causing both Rodgers and Hammerstein to embrace her.

Two young female millworkers in 1873 Maine visit the town's carousel after work. One of them, Julie Jordan, attracts the attention of the barker, Billy Bigelow ("The Carousel Waltz"). When Julie lets Billy put his arm around her during the ride, Mrs. Mullin, the widowed owner of the carousel, tells Julie never to return. Julie and her friend, Carrie Pipperidge, argue with Mrs. Mullin. Billy arrives and, seeing that Mrs. Mullin is jealous, mocks her; he is fired from his job. Billy, unconcerned, invites Julie to join him for a drink. As he goes to get his belongings, Carrie presses Julie about her feelings toward him, but Julie is evasive ("You're a Queer One, Julie Jordan"). Carrie has a beau too, fisherman Enoch Snow ("(When I Marry) Mister Snow"), to whom she is newly engaged. Billy returns for Julie as the departing Carrie warns that staying out late means the loss of Julie's job. Mr. Bascombe, owner of the mill, happens by along with a policeman, and offers to escort Julie to her home, but she refuses and is fired. Left alone, she and Billy talk about what life might be like if they were in love, but neither quite confesses to the growing attraction they feel for each other ("If I Loved You").
Over a month passes, and preparations for the summer clambake are under way ("June Is Bustin' Out All Over"). Julie and Billy, now married, live at Julie's cousin Nettie's spa. Julie confides in Carrie that Billy, frustrated over being unemployed, hit her. Carrie has happier news—she is engaged to Enoch, who enters as she discusses him ("(When I Marry) Mister Snow (reprise))". Billy arrives with his ne'er-do-well whaler friend, Jigger. The former barker is openly rude to Enoch and Julie, then leaves with Jigger, followed by a distraught Julie. Enoch tells Carrie that he expects to become rich selling herring and to have a large family, larger perhaps than Carrie is comfortable having ("When the Children Are Asleep").

Jigger and his shipmates, joined by Billy, then sing about life on the sea ("Blow High, Blow Low"). The whaler tries to recruit Billy to help with a robbery, but Billy declines, as the victim—Julie's former boss, Mr. Bascombe—might have to be killed. Mrs. Mullin enters and tries to tempt Billy back to the carousel (and to her). He would have to abandon Julie; a married barker cannot evoke the same sexual tension as one who is single. Billy reluctantly mulls it over as Julie arrives and the others leave. She tells him that she is pregnant, and Billy is overwhelmed with happiness, ending all thoughts of returning to the carousel. Once alone, Billy imagines the fun he will have with Bill Jr.—until he realizes that his child might be a girl, and reflects soberly that "you've got to be a "father" to a girl" ("Soliloquy"). Determined to provide financially for his future child, whatever the means, Billy decides to be Jigger's accomplice.

The whole town leaves for the clambake. Billy, who had earlier refused to go, agrees to join in, to Julie's delight, as he realizes that being seen at the clambake is integral to his and Jigger's alibi ("Act I Finale").

Everyone reminisces about the huge meal and much fun ("This Was a Real Nice Clambake"). Jigger tries to seduce Carrie; Enoch walks in at the wrong moment, and declares that he is finished with her ("Geraniums In the Winder"), as Jigger jeers ("There's Nothin' So Bad for a Woman"). The girls try to comfort Carrie, but for Julie all that matters is that "he's your feller and you love him" ("What's the Use of Wond'rin'?"). Julie sees Billy trying to sneak away with Jigger and, trying to stop him, feels the knife hidden in his shirt. She begs him to give it to her, but he refuses and leaves to commit the robbery.

As they wait, Jigger and Billy gamble with cards. They stake their shares of the anticipated robbery spoils. Billy loses: his participation is now pointless. Unknown to Billy and Jigger, Mr. Bascombe, the intended victim, has already deposited the mill's money. The robbery fails: Bascombe pulls a gun on Billy while Jigger escapes. Billy stabs himself with his knife; Julie arrives just in time for him to say his last words to her and die. Julie strokes his hair, finally able to tell him that she loved him. Carrie and Enoch, reunited by the crisis, attempt to console Julie; Nettie arrives and gives Julie the resolve to keep going despite her despair ("You'll Never Walk Alone").

Billy's defiant spirit ("The Highest Judge of All") is taken Up There to see the Starkeeper, a heavenly official. The Starkeeper tells Billy that the good he did in life was not enough to get into heaven, but so long as there is a person alive who remembers him, he can return for a day to try to do good to redeem himself. He informs Billy that fifteen years have passed on Earth since the former barker's suicide, and suggests that Billy can get himself into heaven if he helps his daughter, Louise. He helps Billy look down from heaven to see her (instrumental ballet: "Billy Makes a Journey"). Louise has grown up to be lonely and bitter. The local children ostracize her because her father was a thief and a wife-beater. In the dance, a young ruffian, much like her father at that age, flirts with her and abandons her as too young. The dance concludes, and Billy is anxious to return to Earth and help his daughter. He steals a star to take with him, as the Starkeeper pretends not to notice.

Outside Julie's cottage, Carrie describes her visit to New York with the now-wealthy Enoch. Carrie's husband and their many children enter to fetch her—the family must get ready for the high school graduation later that day. Enoch Jr., the oldest son, remains behind to talk with Louise, as Billy and the Heavenly Friend escorting him enter, invisible to the other characters. Louise confides in Enoch Jr. that she plans to run away from home with an acting troupe. He says that he will stop her by marrying her, but that his father will think her an unsuitable match. Louise is outraged: each insults the other's father, and Louise orders Enoch Jr. to go away. Billy, able to make himself visible at will, reveals himself to the sobbing Louise, pretending to be a friend of her father. He offers her a gift—the star he stole from heaven. She refuses it and, frustrated, he slaps her hand. He makes himself invisible, and Louise tells Julie what happened, stating that the slap miraculously felt like a kiss, not a blow—and Julie understands her perfectly. Louise retreats to the house, as Julie notices the star that Billy dropped; she picks it up and seems to feel Billy's presence ("If I Loved You (Reprise)").

Billy invisibly attends Louise's graduation, hoping for one last chance to help his daughter and redeem himself. The beloved town physician, Dr. Seldon (who resembles the Starkeeper) advises the graduating class not to rely on their parents' success or be held back by their failure (words directed at Louise). Seldon prompts everyone to sing an old song, "You'll Never Walk Alone". Billy, still invisible, whispers to Louise, telling her to believe Seldon's words, and when she tentatively reaches out to another girl, she learns she does not have to be an outcast. Billy goes to Julie, telling her at last that he loved her. As his widow and daughter join in the singing, Billy is taken to his heavenly reward.

° denotes original Broadway cast

Act I
Act II

The original Broadway production opened at the Majestic Theatre on April 19, 1945. The dress rehearsal the day before had gone badly, and the pair feared the new work would not be well received. One successful last-minute change was to have de Mille choreograph the pantomime. The movement of the carnival crowd in the pantomime had been entrusted to Mamoulian, and his version was not working. Rodgers had injured his back the previous week, and he watched the opening from a stretcher propped in a box behind the curtain. Sedated with morphine, he could see only part of the stage. As he could not hear the audience's applause and laughter, he assumed the show was a failure. It was not until friends congratulated him later that evening that he realized that the curtain had been met by wild applause. Bambi Linn, who played Louise, was so enthusiastically received by the audience during her ballet that she was forced to break character, when she next appeared, and bow. Rodgers' daughter Mary caught sight of her friend, Stephen Sondheim, both teenagers then, across several rows; both had eyes wet with tears.

The original production ran for 890 performances, closing on May 24, 1947. The original cast included John Raitt (Billy), Jan Clayton (Julie), Jean Darling (Carrie), Eric Mattson (Enoch Snow), Christine Johnson (Nettie Fowler), Murvyn Vye (Jigger), Bambi Linn (Louise) and Russell Collins (Starkeeper). In December 1945, Clayton left to star in the Broadway revival of "Show Boat" and was replaced by Iva Withers; Raitt was replaced by Henry Michel in January 1947; Darling was replaced by Margot Moser.

After closing on Broadway, the show went on a national tour for two years. It played for five months in Chicago alone, visited twenty states and two Canadian cities, covered and played to nearly two million people. The touring company had a four-week run at New York City Center in January 1949.<ref name="NYT/Calta 1949-01-25">Calta, Louis. "'Carousel' opens tonight at City Center". "The New York Times", January 25, 1949, p. 27. Retrieved on December 21, 2010.</ref> Following the City Center run, the show was moved back to the Majestic Theatre in the hopes of filling the theatre until "South Pacific" opened in early April. However, ticket sales were mediocre, and the show closed almost a month early.<ref name="NYT/Calta 1949-02-28">Calta, Louis. "'Carousel' to end run on Saturday". "The New York Times", February 28, 1949, p. 15. Retrieved on December 21, 2010.</ref>

The musical premiered in the West End, London, at the Theatre Royal, Drury Lane on June 7, 1950. The production was restaged by Jerome Whyte, with a cast that included Stephen Douglass (Billy), Iva Withers (Julie) and Margot Moser (Carrie). "Carousel" ran in London for 566 performances, remaining there for over a year and a half.

"Carousel" was revived in 1954 and 1957 at City Center, presented by the New York City Center Light Opera Company. Both times, the production featured Barbara Cook, though she played Carrie in 1954 and Julie in 1957 (playing alongside Howard Keel as Billy). The production was then taken to Belgium to be performed at the 1958 Brussels World's Fair, with David Atkinson as Billy, Ruth Kobart as Nettie, and Clayton reprising the role of Julie, which she had originated.

In August 1965, Rodgers and the Music Theater of Lincoln Center produced "Carousel" for 47 performances. John Raitt reprised the role of Billy, with Jerry Orbach as Jigger and Reid Shelton as Enoch Snow. The roles of the Starkeeper and Dr. Seldon were played by Edward Everett Horton in his final stage appearance. The following year, New York City Center Light Opera Company brought "Carousel" back to City Center for 22 performances, with Bruce Yarnell as Billy and Constance Towers as Julie.

Nicholas Hytner directed a new production of "Carousel" in 1992, at London's Royal National Theatre, with choreography by Sir Kenneth MacMillan and designs by Bob Crowley. In this staging, the story begins at the mill, where Julie and Carrie work, with the music slowed down to emphasize the drudgery. After work ends, they move to the shipyards and then to the carnival. As they proceed on a revolving stage, carnival characters appear, and at last the carousel is assembled onstage for the girls to ride. Louise is seduced by the ruffian boy during her Act 2 ballet, set around the ruins of a carousel. Michael Hayden played Billy not as a large, gruff man, but as a frustrated smaller one, a time bomb waiting to explode. Hayden, Joanna Riding (Julie) and Janie Dee (Carrie) all won Olivier Awards for their performances. Patricia Routledge played Nettie. Enoch and Carrie were cast as an interracial couple whose eight children, according to the review in "The New York Times", looked like "a walking United Colors of Benetton ad". Clive Rowe, as Enoch, was nominated for an Olivier Award. The production's limited run from December 1992 through March 1993 was a sellout. It re-opened at the Shaftesbury Theatre in London in September 1993, presented by Cameron Mackintosh, where it continued until May 1994.

The Hytner production moved to New York's Vivian Beaumont Theater, where it opened on March 24, 1994, and ran for 322 performances. This won five Tony Awards, including best musical revival, as well as awards for Hytner, MacMillan, Crowley and Audra McDonald (as Carrie). The cast also included Sally Murphy as Julie, Shirley Verrett as Nettie, Fisher Stevens as Jigger and Eddie Korbich as Enoch. One change made from the London to the New York production was to have Billy strike Louise across the face, rather than on the hand. According to Hayden, "He does the one unpardonable thing, the thing we can't forgive. It's a challenge for the audience to like him after that." The Hytner "Carousel" was presented in Japan in May 1995. A U.S. national tour with a scaled-down production began in February 1996 in Houston and closed in May 1997 in Providence, Rhode Island. Producers sought to feature young talent on the tour, with Patrick Wilson as Billy and Sarah Uriarte Berry, and later Jennifer Laura Thompson, as Julie.

A revival opened at London's Savoy Theatre on December 2, 2008, after a week of previews, starring Jeremiah James (Billy), Alexandra Silber (Julie) and Lesley Garrett (Nettie). The production received warm to mixed reviews. It closed in June 2009, a month early. Michael Coveney, writing in "The Independent", admired Rodgers' music but stated, "Lindsay Posner's efficient revival doesn't hold a candle to the National Theatre 1992 version". A production at Theater Basel, Switzerland, in 2016 to 2017, with German dialogue, was directed by Alexander Charim and choreographed by Teresa Rotemberg. Bryony Dwyer, Christian Miedl and Cheryl Studer starred, respectively, as Julie Jordan, Billy Bigelow and Nettie Fowler. A semi-staged revival by the English National Opera opened at the London Coliseum in 2017. The production was directed by Lonny Price, conducted by David Charles Abell, and starred Alfie Boe as Billy, Katherine Jenkins as Julie and Nicholas Lyndhurst as the Starkeeper. The production received mixed to positive reviews.

The third Broadway revival began previews in February 2018 at the Imperial Theatre, with an official opening on April 12. It stars Jessie Mueller, Joshua Henry, Renée Fleming, Lindsay Mendez and Alexander Gemignani. The production is directed by Jack O'Brien and choreographed by Justin Peck. The song "There's Nothin' So Bad for a Woman" was cut from this revival. Ben Brantley wrote in "The New York Times", "The tragic inevitability of "Carousel" has seldom come across as warmly or as chillingly as it does in this vividly reimagined revival. ... [W]ith thoughtful and powerful performances by Mr. Henry and Ms. Mueller, the love story at the show's center has never seemed quite as ill-starred or, at the same time, as sexy. ... [T]he Starkeeper ... assumes new visibility throughout, taking on the role of Billy's angelic supervisor." Brantley strongly praised the choreography, all the performances and the designers. He was unconvinced, however, by the "mother-daughter dialogue that falls so abrasively on contemporary ears", where Julie tries to justify loving an abusive man, and other scenes in Act 2, particularly those set in heaven, and the optimism of the final scene. Most of the reviewers agreed that while the choreography and performances (especially the singing) are excellent, characterizing the production as sexy and sumptuous, O'Brien's direction does little to help the show deal with modern sensibilities about men's treatment of women, instead indulging in nostalgia.

A film version of the musical was made in 1956, starring Gordon MacRae and Shirley Jones. It follows the musical's story fairly closely, although a prologue, set in the Starkeeper's heaven, was added. The film was released only a few months after the release of the film version of "Oklahoma!" It garnered some good reviews, and the soundtrack recording was a best seller. As the same stars appeared in both pictures, however, the two films were often compared, generally to the disadvantage of "Carousel". Thomas Hischak, in "The Rodgers and Hammerstein Encyclopedia", later wondered "if the smaller number of "Carousel" stage revivals is the product of this often-lumbering<nowiki> [film] </nowiki> musical".

There was also an abridged (100 minute) 1967 network television version that starred Robert Goulet, with choreography by Edward Villella.

The New York Philharmonic presented a staged concert version of the musical from February 28 to March 2, 2013, at Avery Fisher Hall. Kelli O'Hara played Julie, with Nathan Gunn as Billy, Stephanie Blythe as Nettie, Jessie Mueller as Carrie, Jason Danieley as Enoch, Shuler Hensley as Jigger, John Cullum as the Starkeeper, and Kate Burton as Mrs. Mullin. Tiler Peck danced the role of Louise to choreography by Warren Carlyle. The production was directed by John Rando. Charles Isherwood of "The New York Times" wrote, "this is as gorgeously sung a production of this sublime 1945 Broadway musical as you are ever likely to hear." It was broadcast as part of the PBS "Live from Lincoln Center" series, premiering on April 26, 2013.

Rodgers designed "Carousel" to be an almost continuous stream of music, especially in Act 1. In later years, Rodgers was asked if he had considered writing an opera. He stated that he had been sorely tempted to, but saw "Carousel" in operatic terms. He remembered, "We came very close to opera in the Majestic Theatre. ... There's much that is operatic in the music."
Rodgers uses music in "Carousel" in subtle ways to differentiate characters and tell the audience of their emotional state. In "You're a Queer One, Julie Jordan", the music for the placid Carrie is characterized by even eighth-note rhythms, whereas the emotionally restless Julie's music is marked by dotted eighths and sixteenths; this rhythm will characterize her throughout the show. When Billy whistles a snatch of the song, he selects Julie's dotted notes rather than Carrie's. Reflecting the close association in the music between Julie and the as-yet unborn Louise, when Billy sings in "Soliloquy" of his daughter, who "gets hungry every night", he uses Julie's dotted rhythms. Such rhythms also characterize Julie's Act 2 song, "What's the Use of Wond'rin'". The stable love between Enoch and Carrie is strengthened by her willingness to let Enoch not only plan his entire life, but hers as well. This is reflected in "When the Children Are Asleep", where the two sing in close harmony, but Enoch musically interrupts his intended's turn at the chorus with the words "Dreams that won't be interrupted". Rodgers biographer Geoffrey Block, in his book on the Broadway musical, points out that though Billy may strike his wife, he allows her musical themes to become a part of him and never interrupts her music. Block suggests that, as reprehensible as Billy may be for his actions, Enoch requiring Carrie to act as "the little woman", and his having nine children with her (more than she had found acceptable in "When the Children are Asleep") can be considered to be even more abusive.

The twelve-minute "bench scene", in which Billy and Julie get to know each other and which culminates with "If I Loved You", according to Hischak, "is considered the most completely integrated piece of music-drama in the American musical theatre". The scene is almost entirely drawn from Molnár and is one extended musical piece; Stephen Sondheim described it as "probably the single most important moment in the revolution of contemporary musicals". "If I Loved You" has been recorded many times, by such diverse artists as Frank Sinatra, Barbra Streisand, Sammy Davis Jr., Mario Lanza and Chad and Jeremy. The D-flat major theme that dominates the music for the second act ballet seems like a new melody to many audience members. It is, however, a greatly expanded development of a theme heard during "Soliloquy" at the line "I guess he'll call me 'The old man' ".

When the pair discussed the song that would become "Soliloquy", Rodgers improvised at the piano to give Hammerstein an idea of how he envisioned the song. When Hammerstein presented his collaborator with the lyrics after two weeks of work (Hammerstein always wrote the words first, then Rodgers would write the melodies), Rodgers wrote the music for the eight-minute song in two hours. "What's the Use of Wond'rin' ", one of Julie's songs, worked well in the show but was never as popular on the radio or for recording, and Hammerstein believed that the lack of popularity was because he had concluded the final line, "And all the rest is talk" with a hard consonant, which does not allow the singer a vocal climax.
Irving Berlin later stated that "You'll Never Walk Alone" had the same sort of effect on him as the 23rd Psalm. When singer Mel Tormé told Rodgers that "You'll Never Walk Alone" had made him cry, Rodgers nodded impatiently. "You're supposed to." The frequently recorded song has become a widely accepted hymn. The cast recording of "Carousel" proved popular in Liverpool, like many Broadway albums, and in 1963, the Brian Epstein-managed band, Gerry and the Pacemakers had a number-one hit with the song. At the time, the top ten hits were played before Liverpool F.C. home matches; even after "You'll Never Walk Alone" dropped out of the top ten, fans continued to sing it, and it has become closely associated with the soccer team and the city of Liverpool. A BBC program, "Soul Music", ranked it alongside "Silent Night" and "Abide With Me" in terms of its emotional impact and iconic status.

The cast album of the 1945 Broadway production was issued on 78s, and the score was significantly cut—as was the 1950 London cast recording. Theatre historian John Kenrick notes of the 1945 recording that a number of songs had to be abridged to fit the 78 format, but that there is a small part of "Soliloquy" found on no other recording, as Rodgers cut it from the score immediately after the studio recording was made.

A number of songs were cut for the 1956 film, but two of the deleted numbers had been recorded and were ultimately retained on the soundtrack album. The expanded CD version of the soundtrack, issued in 2001, contains all of the singing recorded for the film, including the cut portions, and nearly all of the dance music. The recording of the 1965 Lincoln Center revival featured Raitt reprising the role of Billy. Studio recordings of "Carousel"'s songs were released in 1956 (with Robert Merrill as Billy, Patrice Munsel as Julie, and Florence Henderson as Carrie), 1962 and 1987. The 1987 version featured a mix of opera and musical stars, including Samuel Ramey, Barbara Cook and Sarah Brightman. Kenrick recommends the 1962 studio recording for its outstanding cast, including Alfred Drake, Roberta Peters, Claramae Turner, Lee Venora, and Norman Treigle.

Both the London (1993) and New York (1994) cast albums of the Hytner production contain portions of dialogue that, according to Hischak, speak to the power of Michael Hayden's portrayal of Billy. Kenrick judges the 1994 recording the best all-around performance of "Carousel" on disc, despite uneven singing by Hayden, due to Sally Murphy's Julie and the strong supporting cast (calling Audra McDonald the best Carrie he has heard). The Stratford Festival issued a recording in 2015.

The musical received almost unanimous rave reviews after its opening in 1945. According to Hischak, reviews were not as exuberant as for "Oklahoma!" as the critics were not taken by surprise this time. John Chapman of the "Daily News" termed it "one of the finest musical plays I have ever seen and I shall remember it always". "The New York Times"'s reviewer, Lewis Nichols, stated that "Richard Rodgers and Oscar Hammerstein 2d, who can do no wrong, have continued doing no wrong in adapting "Liliom" into a musical play. Their "Carousel" is on the whole delightful." Wilella Waldorf of the "New York Post", however, complained, ""Carousel" seemed to us a rather long evening. The "Oklahoma!" formula is becoming a bit monotonous and so are Miss de Mille's ballets. All right, go ahead and shoot!" "Dance Magazine" gave Linn plaudits for her role as Louise, stating, "Bambi doesn't come on until twenty minutes before eleven, and for the next forty minutes, she practically holds the audience in her hand". Howard Barnes in the "New York Herald Tribune" also applauded the dancing: "It has waited for Miss de Mille to come through with peculiarly American dance patterns for a musical show to become as much a dance as a song show."

When the musical returned to New York in 1949, "The New York Times" reviewer Brooks Atkinson described "Carousel" as "a conspicuously superior musical play ... "Carousel", which was warmly appreciated when it opened, seems like nothing less than a masterpiece now." In 1954, when "Carousel" was revived at City Center, Atkinson discussed the musical in his review:

"Carousel" has no comment to make on anything of topical importance. The theme is timeless and universal: the devotion of two people who love each other through thick and thin, complicated in this case by the wayward personality of the man, who cannot fulfill the responsibilities he has assumed.  ... Billy is a bum, but "Carousel" recognizes the decency of his motives and admires his independence. There are no slick solutions in "Carousel".

Stephen Sondheim noted the duo's ability to take the innovations of "Oklahoma!" and apply them to a serious setting: ""Oklahoma!" is about a picnic, "Carousel" is about life and death." Critic Eric Bentley, on the other hand, wrote that "the last scene of "Carousel" is an impertinence: I refuse to be lectured to by a musical comedy scriptwriter on the education of children, the nature of the good life, and the contribution of the American small town to the salvation of souls."

"New York Times" critic Frank Rich said of the 1992 London production: "What is remarkable about Mr. Hytner's direction, aside from its unorthodox faith in the virtues of simplicity and stillness, is its ability to make a 1992 audience believe in Hammerstein's vision of redemption, which has it that a dead sinner can return to Earth to do godly good." The Hytner production in New York was hailed by many critics as a grittier "Carousel", which they deemed more appropriate for the 1990s. Clive Barnes of the "New York Post" called it a "defining "Carousel"—hard-nosed, imaginative, and exciting."

Critic Michael Billington has commented that "lyrically ["Carousel"] comes perilously close to acceptance of the inevitability of domestic violence." BroadwayWorld.com stated in 2013 that "Carousel" is now "considered somewhat controversial in terms of its attitudes on domestic violence" because Julie chooses to stay with Billy despite the abuse; actress Kelli O'Hara noted that the domestic violence that Julie "chooses to deal with – is a real, existing and very complicated thing. And exploring it is an important part of healing it."

Rodgers considered "Carousel" his favorite of all his musicals and wrote, "it affects me deeply every time I see it performed". In 1999, "Time" magazine, in its "Best of the Century" list, named "Carousel" the Best Musical of the 20th century, writing that Rodgers and Hammerstein "set the standards for the 20th century musical, and this show features their most beautiful score and the most skillful and affecting example of their musical storytelling". Hammerstein's grandson, Oscar Andrew Hammerstein, in his book about his family, suggested that the wartime situation made "Carousel"'s ending especially poignant to its original viewers, "Every American grieved the loss of a brother, son, father, or friend ... the audience empathized with <nowiki>[Billy's]</nowiki> all-too-human efforts to offer advice, to seek forgiveness, to complete an unfinished life, and to bid a proper good-bye from beyond the grave." Author and composer Ethan Mordden agreed with that perspective:

If "Oklahoma!" developed the moral argument for sending American boys overseas, "Carousel" offered consolation to those wives and mothers whose boys would only return in spirit. The meaning lay not in the tragedy of the present, but in the hope for a future where no one walks alone.

"Note: The Tony Awards were not established until 1947, and so "Carousel" was not eligible to win any Tonys at its premiere.




</doc>
<doc id="7572" url="https://en.wikipedia.org/wiki?curid=7572" title="Christian alternative rock">
Christian alternative rock

Christian alternative rock is a form of alternative rock music that is lyrically grounded in a Christian worldview. Some critics have suggested that unlike CCM and older Christian rock, Christian alternative rock generally emphasizes musical style over lyrical content as a defining genre characteristic, though the degree to which the faith appears in the music varies from artist to artist.

Christian alternative music has its roots in the early 1980s, as the earliest efforts at Christian punk and new wave were recorded by artists like Andy McCarroll and Moral Support, Undercover, The 77s, Steve Scott, Adam Again, Quickflight, Daniel Amos, Youth Choir (later renamed The Choir), Lifesavers Underground, Michael Knott, The Altar Boys, Breakfast with Amy, Steve Taylor, 4-4-1, David Edwards and Vector. Early labels, most now-defunct, included Blonde Vinyl, Frontline, Exit, and Refuge.

By the 1990s, many of these bands and artists had disbanded, were no longer performing, or were being carried by independent labels because their music tended to be more lyrically complex (and often more controversial) than mainstream Christian pop. The modern market is currently supported by labels such as Tooth & Nail, Gotee and Floodgate. These companies are often children of, or partially owned, by general market labels such as Warner, EMI, and Capitol Records, giving successful artists an opportunity to "cross over" into mainstream markets.




</doc>
<doc id="7573" url="https://en.wikipedia.org/wiki?curid=7573" title="Clive Barker">
Clive Barker

Clive Barker (born 5 October 1952) is an English writer, film director, and visual artist. Barker came to prominence in the mid-1980s with a series of short stories, the "Books of Blood", which established him as a leading horror writer. He has since written many novels and other works, and his fiction has been adapted into films, notably the "Hellraiser" and "Candyman" series. He was also the executive producer of the film "Gods and Monsters".

Barker's paintings and illustrations have been featured in galleries in the United States as well as within his own books. He has created original characters and series for comic books, and some of his more popular horror stories have been adapted to comics.

Barker was born in Liverpool, Merseyside, the son of Joan Ruby (née Revill), a painter and school welfare officer, and Leonard Barker, a personnel director for an industrial relations firm. He was educated at Dovedale Primary School, Quarry Bank High School and the University of Liverpool, where he studied English and Philosophy.

When he was three years old, Barker witnessed the French skydiver Léo Valentin plummet to his death during a performance at an air show in Liverpool. Barker would later allude to Valentin in many of his stories.

Barker is an author of horror/fantasy. He began writing horror early in his career, mostly in the form of short stories (collected in "Books of Blood" 1 – 6) and the Faustian novel "The Damnation Game" (1985). Later he moved towards modern-day fantasy and urban fantasy with horror elements in "Weaveworld" (1987), "The Great and Secret Show" (1989), the world-spanning "Imajica" (1991), and "Sacrament" (1996).

When the "Books of Blood" were first published in the United States in paperback, Stephen King was quoted on the book covers: "I have seen the future of horror and his name is Clive Barker." As influences on his writing, Barker lists Herman Melville, Edgar Allan Poe, Ray Bradbury, William S. Burroughs, William Blake, and Jean Cocteau, among others.

He is the writer of the best-selling Abarat series, and plans on producing two more novels in the series.

In 2003, Barker received the Davidson/Valentini Award at the 15th GLAAD Media Awards.

Barker has been critical of organized religion throughout his career, but in early interviews, he stated that the Bible influences his work and spirituality. In a 2003 appearance on "Politically Incorrect", Barker even stated that he was a Christian after Ann Coulter implied he was not, although he later retracted this.

Barker said in a December 2008 online interview (published in March 2009) that he had polyps in his throat which were so severe that a doctor told him he was taking in ten percent of the air he was supposed to have been getting. He has had two surgeries to remove them and believes his resultant voice is an improvement over how it was prior to the surgeries. He said he did not have cancer and has given up cigars.

As of 2015, he is a member of the board of advisers for the Hollywood Horror Museum.

In a 20 August 1996 appearance on the radio call-in show "Loveline", Barker stated that during his teens he had several relationships with older women, and came to identify himself as homosexual by 18 or 19 years old. Barker has been openly gay since the early 1990s. His relationship with John Gregson lasted from 1975 until 1986. It was during this period, with the support that Gregson provided, that Barker was able to write the "Books of Blood" series and "The Damnation Game".

He later spent thirteen years with photographer David Armstrong, described as his husband in the introduction to "Coldheart Canyon"; they separated in 2009.

Barker lives in Beverly Hills with his partner, Johnny Ray Raymond Jr.

Barker has an interest in film production. He wrote the screenplays for "Underworld" and "Rawhead Rex" (1986), both directed by George Pavlou. Displeased by how his material was handled, he moved to directing with "Hellraiser" (1987), based on his novella "The Hellbound Heart". After his film "Nightbreed" (1990) flopped, Barker returned to write and direct "Lord of Illusions" (1995). The short story "The Forbidden", from Barker's "Books of Blood", provided the basis for the 1992 film "Candyman" and its two sequels. Barker was an executive producer of the film "Gods and Monsters" (1998), which received major critical acclaim. He had been working on a series of film adaptations of his "The Abarat Quintet" books under Disney's management, but because of creative differences, the project was cancelled.

In 2005, Barker and horror film producer Jorge Saralegui created the film production company Midnight Picture Show with the intent of producing two horror films per year.

In October 2006, Barker announced through his website that he will be writing the script to a forthcoming remake of the original "Hellraiser" film. He is developing a film based on his "Tortured Souls" line of toys from McFarlane Toys.

Barker is a prolific visual artist, often illustrating his own books. His paintings have been seen first on the covers of his official fan club magazine, "Dread", published by Fantaco in the early '90s; on the covers of the collections of his plays, "Incarnations" (1995) and "Forms of Heaven" (1996); and on the second printing of the original British publications of his "Books of Blood" series. Barker also provided the artwork for his young adult novel "The Thief of Always" and for the "Abarat" series. His artwork has been exhibited at Bert Green Fine Art in Los Angeles and Chicago, at the Bess Cutler Gallery in New York and La Luz De Jesus in Los Angeles. Many of his sketches and paintings can be found in the collection "Clive Barker, Illustrator", published in 1990 by Arcane/Eclipse Books, and in "Visions of Heaven and Hell", published in 2005 by Rizzoli Books.

He worked on the horror video game "Clive Barker's Undying", providing the voice for the character Ambrose. "Undying" was developed by DreamWorks Interactive and released in 2001. He worked on "Clive Barker's Jericho" for Codemasters, which was released in late 2007.

Barker created Halloween costume designs for Disguise Costumes.

Barker published his Razorline imprint via Marvel Comics in 1993.

Barker horror adaptations and spinoffs in comics include the Marvel/Epic Comics series "Hellraiser", "Nightbreed", "Pinhead", "The Harrowers", "Book of the Damned", and "Jihad"; Eclipse Books' series and graphic novels "Tapping The Vein", "Dread", "Son of Celluloid", "Revelations" "The Life of Death", "Rawhead Rex" and "The Yattering and Jack", and Dark Horse Comics' "Primal", among others. Barker served as a consultant and wrote issues of the Hellraiser anthology comic book.

In 2005, IDW published a three-issue adaptation of Barker's children's fantasy novel "The Thief of Always", written and painted by Kris Oprisko and Gabriel Hernandez. IDW is publishing a 12 issue adaptation of Barker's novel "The Great and Secret Show".

In December 2007, Chris Ryall and Clive Barker announced an upcoming collaboration of an original comic book series, "Torakator", to be published by IDW.

In October 2009, IDW published "Seduth", co-written by Barker. The work was released with three variant covers.

In 2011, Boom! Studios began publishing an original Hellraiser comic book series.

In 2013, Boom! Studios announced "Next Testament", the first original story by Barker to be published in comic book format.





















</doc>
<doc id="7574" url="https://en.wikipedia.org/wiki?curid=7574" title="Comic fantasy">
Comic fantasy

Comic fantasy is a subgenre of fantasy that is primarily humorous in intent and tone. Usually set in imaginary worlds, comic fantasy often includes puns on and parodies of other works of fantasy. It is sometimes known as low fantasy in contrast to high fantasy, which is primarily serious in intent and tone. The term "low fantasy" is used to represent other types of fantasy, however, so while comic fantasies may also correctly be classified as low fantasy, many examples of low fantasy are not comic in nature.

The subgenre rose in the nineteenth century. Elements of comic fantasy can be found in such nineteenth century works
as some of Hans Christian Andersen's fairy tales, Charles Dickens' "Christmas Books", and Lewis Carroll's Alice books. The first writer to specialize in the subgenre was "F. Anstey" in novels such as "Vice Versa" (1882), where magic disrupts Victorian society with humorous results. Anstey's work was popular enough to inspire several imitations, including E. Nesbit's light-hearted children's fantasies, "The Phoenix and the Carpet" (1904) and "The Story of the Amulet" (1906). The United States had several writers of comic fantasy, including James Branch Cabell, whose satirical fantasy "Jurgen, A Comedy of Justice" (1919) was the subject of an unsuccessful prosecution for obscenity. Another American writer in a similar vein was Thorne Smith, whose works (such as "Topper" and "The Night Life of the Gods") were popular and influential, and often adapted for film and television. Humorous fantasies narrated in a "gentleman's club" setting are common; they include John Kendrick Bangs' "A Houseboat on the Styx" (1895), Lord Dunsany's "Jorkens" stories, and Maurice Richardson's 
"The Exploits of Englebrecht" (1950).

According to Lin Carter, T. H. White's works exemplify comic fantasy, L. Sprague de Camp and Fletcher Pratt's Harold Shea stories are early exemplars. The overwhelming bulk of de Camp's fantasy was comic. Pratt and de Camp were among several contributors to "Unknown Worlds", a pulp magazine which emphasized fantasy with a comedic element. The work of Fritz Leiber also appeared in "Unknown Worlds", including his Fafhrd and the Gray Mouser stories, a jocose take on the sword and sorcery subgenre.

In more modern times, Terry Pratchett's "Discworld" books, Piers Anthony's "Xanth" books, Robert Asprin's "MythAdventures" of Skeeve and Aahz books, and Tom Holt's books provide good examples, as do many of the works by Christopher Moore. There are also comic-strips/graphic novels in the humorous fantasy genre, including Chuck Whelon's Pewfell series and the webcomics "8-Bit Theater" and "The Order of the Stick". Other recent authors in the genre include Toby Frost, Stuart Sharp, Nicholas Andrews, and DC Farmer, and the writing team of John P. Logsdon and Christopher P. Young.

The subgenre has also been represented in television, such as in the television series "I Dream of Jeannie", "Kröd Mändoon". Examples on radio are the BBC's "Hordes of the Things" and "ElvenQuest". Comic fantasy films can either be parodies ("Monty Python and the Holy Grail"), comedies with fantastical elements ("Being John Malkovich") or animated ("Shrek"). It has also been used in the movie "".



</doc>
<doc id="7575" url="https://en.wikipedia.org/wiki?curid=7575" title="CLU (programming language)">
CLU (programming language)

CLU is a programming language created at the Massachusetts Institute of Technology (MIT) by Barbara Liskov and her students between 1974 and 1975. While it did not find extensive use, it introduced many features that are used widely now, and is seen as a step in the development of object-oriented programming (OOP).

Key contributions include abstract data types, call-by-sharing, iterators, multiple return values (a form of parallel assignment), type-safe parameterized types, and type-safe variant types. It is also notable for its use of classes with constructors and methods, but without inheritance.

The syntax of CLU was based on ALGOL, then the starting point for most new language designs. The key addition was the concept of a "cluster", CLU's type extension system and the root of the language's name (CLUster). Clusters correspond generally to the concept of a "class" in an OO language, and have similar syntax. For instance, here is the CLU syntax for a cluster that implements complex numbers:

A cluster is a module that encapsulates all of its components except for those explicitly named in the "is" clause. These correspond to the public components of a class in recent OO languages. A cluster also defines a type that can be named outside the cluster (in this case, "complex_number"), but its representation type (rep) is hidden from external clients.

Cluster names are global, and no namespace mechanism was provided to group clusters or allow them to be created "locally" inside other clusters.

CLU does not perform implicit type conversions. In a cluster, the explicit type conversions "up" and "down" change between the abstract type and the representation. There is a universal type "any", and a procedure force[] to check that an object is a certain type. Objects may be mutable or immutable, the latter being "base types" such as integers, booleans, characters and strings.

Another key feature of the CLU type system are "iterators", which return objects from a collection serially, one after another. Iterators offer an identical application programming interface (API) no matter what data they are being used with. Thus the iterator for a collection of codice_1s can be used interchangeably with that for an array of codice_2s. A distinctive feature of CLU iterators is that they are implemented as coroutines, with each value being provided to the caller via a "yield" statement. Iterators like those in CLU are now a common feature of many modern languages, such as C#, Ruby, and Python, though recently they are often referred to as generators.

CLU also includes exception handling, based on various attempts in other languages; exceptions are raised using codice_3 and handled with codice_4. Unlike most other languages with exception handling, exceptions are not implicitly resignaled up the calling chain. Also unlike most other languages that provide exception handling, exceptions in CLU are considered part of ordinary execution flow and are considered a "normal" and efficient typesafe way to break out of loops or return from functions; this allows for direct assignment of return values "except when" other conditions apply. Exceptions that are neither caught nor resignaled explicitly are immediately converted into a special failure exception that typically terminates the program.

CLU is often credited as being the first language with type-safe variant types, called "oneofs", before the language ML had them.

A final distinctive feature in CLU is parallel assignment (multiple assignment), where more than one variable can appear on the left hand side of an assignment operator. For instance, writing codice_5 would exchange values of codice_6 and codice_7. In the same way, functions could return several values, like codice_8. Parallel assignment (though not multiple return values) predates CLU, appearing in CPL (1963), named "simultaneous assignment", but CLU popularized it and is often credited as the direct influence leading to parallel assignment in later languages.

All objects in a CLU program live in the heap, and memory management is automatic.

CLU supported type parameterized user-defined data abstractions. It was the first language to offer type-safe bounded parameterized types, using structure "where clauses" to express constraints on actual type arguments.

CLU has influenced many other languages in many ways. In approximate chronological order, these include:

CLU and Ada were major inspirations for C++ templates.

CLU's exception handling mechanisms influenced later languages like C++ and Java.

Sather, Python, and C# include iterators, which first appeared in CLU.

Perl and Lua took multiple assignment and multiple returns from function calls from CLU.

Python and Ruby borrowed several concepts from CLU, such as call by sharing, the "yield" statement, and multiple assignment



</doc>
<doc id="7577" url="https://en.wikipedia.org/wiki?curid=7577" title="History of the Soviet Union (1982–91)">
History of the Soviet Union (1982–91)

The history of the Soviet Union from 1982 through 1991 spans the period from Leonid Brezhnev's death and funeral until the dissolution of the Soviet Union. Due to the years of Soviet military buildup at the expense of domestic development, economic growth stagnated. Failed attempts at reform, a standstill economy, and the success of the United States against the Soviet Union's forces in the war in Afghanistan led to a general feeling of discontent, especially in the Baltic republics and Eastern Europe.

Greater political and social freedoms, instituted by the last Soviet leader, Mikhail Gorbachev, created an atmosphere of open criticism of the communist regime. The dramatic drop of the price of oil in 1985 and 1986 profoundly influenced actions of the Soviet leadership.

Nikolai Tikhonov, the Chairman of the Council of Ministers, was succeeded by Nikolai Ryzhkov, and Vasili Kuznetsov, the acting Chairman of the Presidium of the Supreme Soviet, was succeeded by Andrei Gromyko, the former Minister of Foreign Affairs.

Several republics began resisting central control, and increasing democratization led to a weakening of the central government. The Soviet Union finally collapsed in 1991 when Boris Yeltsin seized power in the aftermath of a failed coup that had attempted to topple reform-minded Gorbachev.

By 1982, the stagnation of the Soviet economy was obvious, as evidenced by the fact that the Soviet Union had been importing grain from the U.S. throughout the 1970s, but the system was so firmly entrenched that any real change seemed impossible. A huge rate of defense spending consumed large parts of the economy. The transition period that separated the Brezhnev and Gorbachev eras resembled the former much more than the latter, although hints of reform emerged as early as 1983.

Brezhnev died on 10 November 1982. Two days passed between his death and the announcement of the election of Yuri Andropov as the new General Secretary, suggesting to many outsiders that a power struggle had occurred in the Kremlin. Andropov maneuvered his way into power both through his KGB connections and by gaining the support of the military by promising not to cut defense spending. For comparison, some of his rivals such as Konstantin Chernenko were skeptical of a continued high military budget. Aged 69, he was the oldest person ever appointed as General Secretary and 11 years older than Brezhnev when he acquired that post. In June 1983, he assumed the post of chairman of the Presidium of the Supreme Soviet, thus becoming the ceremonial head of state. It had taken Brezhnev 13 years to acquire this post. Andropov began a thorough house-cleaning throughout the party and state bureaucracy, a decision made easy by the fact that the Central Committee had an average age of 69. He replaced more than one-fifth of the Soviet ministers and regional party first secretaries and more than one-third of the department heads within the Central Committee apparatus. As a result, he replaced the aging leadership with younger, more vigorous administrators. But Andropov's ability to reshape the top leadership was constrained by his own age and poor health and the influence of his rival (and longtime ally of Leonid Brezhnev) Konstantin Chernenko, who had previously supervised personnel matters in the Central Committee.

The transition of power from Brezhnev to Andropov was notably the first one in Soviet history to occur completely peacefully with no one being imprisoned, killed, or forced from office.

Andropov's domestic policy leaned heavily towards restoring discipline and order to Soviet society. He eschewed radical political and economic reforms, promoting instead a small degree of candor in politics and mild economic experiments similar to those that had been associated with the late Premier Alexei Kosygin's initiatives in the mid-1960s. In tandem with such economic experiments, Andropov launched an anti-corruption drive that reached high into the government and party ranks. Unlike Brezhnev, who possessed several mansions and a fleet of luxury cars, he lived quite simply. While visiting Budapest in early 1983, he expressed interest in Hungary's Goulash Communism and that the sheer size of the Soviet economy made strict top-down planning impractical. Changes were needed in a hurry for 1982 had witnessed the country's worst economic performance since World War II, with real GDP growth at almost zero percent.

In foreign affairs, Andropov continued Brezhnev's policies. US−Soviet relations deteriorated rapidly beginning in March 1983, when US President Ronald Reagan dubbed the Soviet Union an "evil empire". The official press agency TASS accused Reagan of "thinking only in terms of confrontation and bellicose, lunatic anti-communism". Further deterioration occurred as a result of the 1 Sep 1983 Soviet shootdown of Korean Air Lines Flight 007 near Moneron Island carrying 269 people including a sitting US congressman, Larry McDonald, and over Reagan's stationing of intermediate-range nuclear missiles in Western Europe. In Afghanistan, Angola, Nicaragua and elsewhere, under the Reagan Doctrine, the US began undermining Soviet-supported governments by supplying arms to anti-communist resistance movements in these countries.

President Reagan's decision to deploy medium-range Pershing II missiles in Western Europe met with mass protests in countries such as France and West Germany, sometimes numbering 1 million people at a time. Many Europeans became convinced that the US and not the Soviet Union was the more aggressive country, and there was fear over the prospect of a war, especially since there was a widespread conviction in Europe that the US, being separated from the Red Army by two oceans as opposed to a short land border, was insensitive to the people of Germany and other countries. Moreover, the memory of World War II was still strong and many Germans could not forget the destruction and mass rapes committed by Soviet troops in the closing days of that conflict. This attitude was helped along by the Reagan Administration's comments that a war between NATO and the Warsaw Pact would not necessarily result in the use of nuclear weapons.

Andropov's health declined rapidly during the tense summer and fall of 1983, and he became the first Soviet leader to miss the anniversary celebrations of the 1917 revolution that November. He died in February 1984 of kidney failure after disappearing from public view for several months. His most significant legacy to the Soviet Union was his discovery and promotion of Mikhail Gorbachev. Beginning in 1978, Gorbachev advanced in two years through the Kremlin hierarchy to full membership in the Politburo. His responsibilities for the appointment of personnel allowed him to make the contacts and distribute the favors necessary for a future bid to become general secretary. At this point, Western experts believed that Andropov was grooming Gorbachev as his successor. However, although Gorbachev acted as a deputy to the general secretary throughout Andropov's illness, Gorbachev's time had not yet arrived when his patron died early in 1984.

At 71, Konstantin Chernenko was in poor health, suffering from emphysema, and unable to play an active role in policy making when he was chosen, after lengthy discussion, to succeed Andropov. But Chernenko's short time in office did bring some significant policy changes. The personnel changes and investigations into corruption undertaken under Andropov's tutelage came to an end. Chernenko advocated more investment in consumer goods and services and in agriculture. He also called for a reduction in the CPSU's micromanagement of the economy and greater attention to public opinion. However, KGB repression of Soviet dissidents also increased. In February 1983, Soviet representatives withdrew from the World Psychiatric Organization in protest of that group's continued complaints about the use of psychiatry to suppress dissent. This policy was underlined in June when Vladimir Danchev, a broadcaster for Radio Moscow, referred to the Soviet troops in Afghanistan as "invaders" while conducting English-language broadcasts. After refusing to retract this statement, he was sent to a mental institution for several months. Valery Senderov, a leader of an unofficial union of professional workers, was sentenced to seven years in a labor camp early in the year for speaking out on discrimination practiced against Jews in education and the professions.

Although Chernenko had called for renewed "détente" with the West, little progress was made towards closing the rift in East−West relations during his rule. The Soviet Union boycotted the 1984 Summer Olympics in Los Angeles, retaliating for the United States-led boycott of the 1980 Summer Olympics in Moscow. In September 1984, the Soviet Union also prevented a visit to West Germany by East German leader Erich Honecker. Fighting in Afghanistan also intensified, but in the late autumn of 1984 the United States and the Soviet Union did agree to resume arms control talks in early 1985.

The war in Afghanistan, often referred to as the Soviet Union's "Vietnam War", led to increased public dissatisfaction with the Communist regime. Also, the Chernobyl disaster in 1986 added motive force to Gorbachev's glasnost and perestroika reforms, which eventually spiraled out of control and caused the Soviet system to collapse.

After years of stagnation, the "new thinking" (Anatoli Cherniaev, 2008: 131) of younger Communist apparatchik began to emerge. Following the death of terminally ill Konstantin Chernenko, the Politburo elected Mikhail Gorbachev to the position of General Secretary of the Communist Party of the Soviet Union (CPSU) in March 1985. At 54, Gorbachev was the youngest person since Joseph Stalin to become General Secretary and the country's first head of state born a Soviet citizen instead of a subject of the tsar. During his official confirmation on March 11, Foreign Minister Andrei Gromyko spoke of how the new Soviet leader had filled in for Chernenko as CC Secretariat, and praised his intelligence and flexible, pragmatic ideas instead of rigid adherence to party ideology. Gorbachev was aided by a lack of serious competition in the Politburo. He immediately began appointing younger men of his generation to important party posts, including Nikolai Ryzhkov, Secretary of Economics, Viktor Cherbrikov, KGB Chief, Foreign Minister Eduard Shevardnadze (replacing the 75-year-old Gromyko), Secretary of Defense Industries Lev Zaikov, and Secretary of Construction Boris Yeltsin. Removed from the Politburo and Secretariat was Grigory Romanov, who had been Gorbachev's most significant rival for the position of General Secretary. Gromyko's removal as Foreign Minister was the most unexpected change given his decades of unflinching, faithful service compared to the unknown, inexperienced Shevardnadze.

More predictably, the 80-year-old Nikolai Tikhonov, the Chairman of the Council of Ministers, was succeeded by Nikolai Ryzhkov, and Vasili Kuznetsov, the acting Chairman of the Presidium of the Supreme Soviet, was succeeded by Andrei Gromyko, the former Minister of Foreign Affairs.

Further down the chain, up to 40% of the first secretaries of the "oblasts" (provinces) were replaced with younger, better educated, and more competent men. The defense establishment was also given a thorough shakeup with the commanders of all 16 military districts replaced along with all theaters of military operation, as well as the three Soviet fleets. Not since World War II had the Soviet military had such a rapid turnover of officers. Sixty-eight-year-old Marshal Nikolai Ogarkov was fully rehabilitated after having fallen from favor in 1983–84 due to his handling of the KAL 007 shootdown and his ideas about improving Soviet strategic and tactical doctrines were made into an official part of defense policy, although some of his other ambitions such as developing the military into a smaller, tighter force based on advanced technology were not considered feasible for the time being. Many, but not all, of the younger army officers appointed during 1985 were proteges of Ogarkov.

Gorbachev got off to an excellent start during his first months in power. He projected an aura of youth and dynamism compared to his aged predecessors and made frequent walks in the streets of the major cities answering questions from ordinary citizens. He became the first leader that spoke with the Soviet people in person. When he made public speeches, he made clear that he was interested in constructive exchanges of ideas instead of merely reciting lengthy platitudes about the excellence of the Soviet system. He also spoke candidly about the slackness and run-down condition of Soviet society in recent years, blaming alcohol abuse, poor workplace discipline, and other factors for these situations. Alcohol was a particular nag of Gorbachev's, especially as he himself did not drink, and he made one of his major policy aims curbing the consumption of it.

In terms of foreign policy, the most important one, relations with the United States, remained twitchy through 1985. In October, Gorbachev made his first visit to a non-communist country when he traveled to France and was warmly received. The fashion-conscious French were also captivated by his wife Raisa and political pundits widely believed that the comparatively young Soviet leader would have a PR advantage over President Reagan, who was 20 years his senior.

Reagan and Gorbachev met for the first time in Geneva in November. The three weeks preceding the summit meeting were marked by an unprecedented Soviet media campaign against the Strategic Defense Initiative (SDI), taking advantage of opposition at home in the US to the program. When it finally took place, the two superpower leaders established a solid rapport that boded well for the future despite Reagan's refusal to compromise on abandonment of SDI. A joint communique by both parties stated that they were in agreement that nuclear war could not be won by either side and must never be allowed to happen. It was also agreed that Reagan and Gorbachev would carry out two more summit meetings in 1986–87.

Jimmy Carter had officially ended the policy of détente, by financially aiding the Mujahideen movement in neighboring Afghanistan, which served as a pretext for the Soviet intervention in Afghanistan six months later, with the aims of supporting the Afghan government, controlled by the People's Democratic Party of Afghanistan. Tensions between the superpowers increased during this time, when Carter placed trade embargoes on the Soviet Union and stated that the Soviet invasion of Afghanistan was "the most serious threat to the peace since the Second World War."

East-West tensions increased during the first term of U.S. President Ronald Reagan (1981–85), reaching levels not seen since the Cuban Missile Crisis as Reagan increased US military spending to 7% of the GDP. To match the USA's military buildup, the Soviet Union increased its own military spending to 27% of its GDP and froze production of civilian goods at 1980 levels, causing a sharp economic decline in the already failing Soviet economy. However, it is not clear where the number 27% of the GDP came from. This thesis is not confirmed by the extensive study on the causes of the dissolution of the Soviet Union by two prominent economists from the World Bank—William Easterly and Stanley Fischer from the Massachusetts Institute of Technology. "… the study concludes that the increased Soviet defense spending provoked by Mr. Reagan's policies was not the straw that broke the back of the Empire. The Afghan war and the Soviet response to Mr. Reagan's Star Wars program caused only a relatively small rise in defense costs. And the defense effort throughout the period from 1960 to 1987 contributed only marginally to economic decline."

Economically, the soviet leaders attempted to adopt the Chinese option—economic liberalization with preservation of political system instead of the Shock therapy (economics) that was going on in Latin America and Poland. However, Gorbachev reforms did not work because the Soviet Union economy was almost 80% state owned compared to the 20–30% in China. The gradual opening of markets was too slow and not deep enough to leave any significant economic reforms until it was too late to prevent the collapse of the USSR.

The US financed the training for the Mujahideen warlords such as Jalaluddin Haqqani, Gulbudin Hekmatyar and Burhanuddin Rabbani eventually culminated to the fall of the Soviet satellite the Democratic Republic of Afghanistan. While the CIA and MI6 and the People's Liberation Army of China financed the operation along with the Pakistan government against the Soviet Union, eventually the Soviet Union began looking for a withdrawal route and in 1988 the Geneva Accords were signed between Communist-Afghanistan and the Islamic Republic of Pakistan; under the terms Soviet troops were to withdraw. Once the withdrawal was complete the Pakistan ISI continued to support the Mujahideen against the Communist Government and by 1992, the government collapsed. US President Reagan also actively hindered the Soviet Union's ability to sell natural gas to Europe whilst simultaneously actively working to keep gas prices low, which kept the price of Soviet oil low and further starved the Soviet Union of foreign capital. This "long-term strategic offensive," which "contrasts with the essentially reactive and defensive strategy of "containment", accelerated the fall of the Soviet Union by encouraging it to overextend its economic base. The proposition that special operations by the CIA in Saudi Arabia affected the prices of Soviet oil was refuted by Marshall Goldman—one of the leading experts on the economy of the Soviet Union—in his latest book. He pointed out that the Saudis decreased their production of oil in 1985 (it reached a 16-year low), whereas the peak of oil production was reached in 1980. They increased the production of oil in 1986, reduced it in 1987 with a subsequent increase in 1988, but not to the levels of 1980 when production reached its highest level. The real increase happened in 1990, by which time the Cold War was almost over. In his book he asked why, if Saudi Arabia had such an effect on Soviet oil prices, did prices not fall in 1980 when the production of oil by Saudi Arabia reached its highest level—three times as much oil as in the mid-eighties—and why did the Saudis wait till 1990 to increase their production, five years after the CIA's supposed intervention? Why didn't the Soviet Union collapse in 1980 then?

However, this theory ignores the fact that the Soviet Union had already suffered several important setbacks during "reactive and defensive strategy" of "containment". In 1972, Nixon normalized US relations with China, thus creating pressure on the Soviet Union. In 1979, Egyptian president Anwar Sadat severed military and economic relations with the USSR after signing the Camp David Accords (by that time the USSR provided a lot of assistance to Egypt and supported it in all its military operations against Israel).

By the time Gorbachev ushered in the process that would lead to the dismantling of the Soviet administrative planned economy through his programs of "glasnost" (political openness), "uskoreniye" (speed-up of economic development) and "perestroika" (political and economic restructuring) announced in 1986, the Soviet economy suffered from both hidden inflation and pervasive supply shortages aggravated by an increasingly open black market that undermined the official economy. Additionally, the costs of superpower status—the military, space program, subsidies to client states—were out of proportion to the Soviet economy. The new wave of industrialization based upon information technology had left the Soviet Union desperate for Western technology and credits in order to counter its increasing backwardness.

The Law on Cooperatives enacted in May 1988 was perhaps the most radical of the economic reforms during the early part of the Gorbachev era. For the first time since Vladimir Lenin's New Economic Policy, the law permitted private ownership of businesses in the services, manufacturing, and foreign-trade sectors. Under this provision, cooperative restaurants, shops, and manufacturers became part of the Soviet scene.

"Glasnost" resulted in greater freedom of speech and the press becoming far less controlled. Thousands of political prisoners and many dissidents were also released. Soviet social science became free to explore and publish on many subjects that had previously been off limits, including conducting public opinion polls. The All−Union Center for Public Opinion Research (VCIOM)—the most prominent of several polling organizations that were started then— was opened. State archives became more accessible, and some social statistics that had been kept secret became open for research and publication on sensitive subjects such as income disparities, crime, suicide, abortion, and infant mortality. The first center for gender studies was opened within a newly formed Institute for the Socio−Economic Study of Human Population.

In January 1987, Gorbachev called for democratization: the infusion of democratic elements such as multi-candidate elections into the Soviet political process. A 1987 conference convened by Soviet economist and Gorbachev adviser Leonid Abalkin, concluded: "Deep transformations in the management of the economy cannot be realized without corresponding changes in the political system."

In June 1988, at the CPSU's Nineteenth Party Conference, Gorbachev launched radical reforms meant to reduce party control of the government apparatus. On 1 December 1988, the Supreme Soviet amended the Soviet constitution to allow for the establishment of a Congress of People's Deputies as the Soviet Union's new supreme legislative body.

Elections to the new Congress of People's Deputies were held throughout the USSR in March and April 1989. Gorbachev, as General Secretary of the Communist Party, could be forced to resign at any moment if the communist elite became dissatisfied with him. To proceed with reforms opposed by the majority of the communist party, Gorbachev aimed to consolidate power in a new position, President of the Soviet Union, which was independent from the CPSU and the soviets (councils) and whose holder could be impeached only in case of direct violation of the law. On 15 March 1990, Gorbachev was elected as the first executive president. At the same time, Article 6 of the constitution was changed to deprive the CPSU of a monopoly on political power.

Gorbachev's efforts to streamline the Communist system offered promise, but ultimately proved uncontrollable and resulted in a cascade of events that eventually concluded with the dissolution of the Soviet Union. Initially intended as tools to bolster the Soviet economy, the policies of "perestroika" and "glasnost" soon led to unintended consequences.

Relaxation under "glasnost" resulted in the Communist Party losing its absolute grip on the media. Before long, and much to the embarrassment of the authorities, the media began to expose severe social and economic problems the Soviet government had long denied and actively concealed. Problems receiving increased attention included poor housing, alcoholism, drug abuse, pollution, outdated Stalin-era factories, and petty to large-scale corruption, all of which the official media had ignored. Media reports also exposed crimes committed by Joseph Stalin and the Soviet regime, such as the gulags, his treaty with Adolf Hitler, and the Great Purges, which had been ignored by the official media. Moreover, the ongoing war in Afghanistan, and the mishandling of the 1986 Chernobyl disaster, further damaged the credibility of the Soviet government at a time when dissatisfaction was increasing.

In all, the positive view of Soviet lifelong presented to the public by the official media was rapidly fading, and the negative aspects of life in the Soviet Union were brought into the spotlight. This undermined the faith of the public in the Soviet system and eroded the Communist Party's social power base, threatening the identity and integrity of the Soviet Union itself.

Fraying amongst the members of the Warsaw Pact countries and instability of its western allies, first indicated by Lech Wałęsa's 1980 rise to leadership of the trade union Solidarity, accelerated, leaving the Soviet Union unable to depend upon its Eastern European satellite states for protection as a buffer zone. By 1989, following hid doctrine of "new political thinking", Gorbachev had repudiated the Brezhnev Doctrine in favor of non-intervention in the internal affairs of its Warsaw Pact allies ("Sinatra Doctrine"). Gradually, each of the Warsaw Pact countries saw their communist governments fall to popular elections and, in the case of Romania, a violent uprising. By 1990, the governments of Bulgaria, Czechoslovakia, East Germany, Hungary, Poland and Romania, all of which had been imposed after World War II, were brought down as revolutions swept Eastern Europe.

The Soviet Union also began experiencing upheaval as the political consequences of "glasnost" reverberated throughout the country. Despite efforts at containment, the upheaval in Eastern Europe inevitably spread to nationalities within the USSR. In elections to the regional assemblies of the Soviet Union's constituent republics, nationalists as well as radical reformers swept the board. As Gorbachev had weakened the system of internal political repression, the ability of the USSR's central Moscow government to impose its will on the USSR's constituent republics had been largely undermined. Massive peaceful protests in the Baltic republics such as the Baltic Way and the Singing Revolution drew international attention and bolstered independence movements in various other regions.

The rise of nationalism under "freedom of speech" soon re-awakened simmering ethnic tensions in various Soviet republics, further discrediting the ideal of a unified Soviet people. One instance occurred in February 1988, when the government in Nagorno-Karabakh, a predominantly ethnic Armenian region in the Azerbaijan SSR, passed a resolution calling for unification with the Armenian SSR. Violence against local Azerbaijanis was reported on Soviet television, provoking massacres of Armenians in the Azerbaijani city of Sumgait.

Emboldened by the liberalized atmosphere of "glasnost", public dissatisfaction with economic conditions was much more overt than ever before in the Soviet period. Although "perestroika" was considered bold in the context of Soviet history, Gorbachev's attempts at economic reform were not radical enough to restart the country's chronically sluggish economy in the late 1980s. The reforms made some inroads in decentralization, but Gorbachev and his team left intact most of the fundamental elements of the Stalinist system, including price controls, inconvertibility of the ruble, exclusion of private property ownership, and the government monopoly over most means of production.

The value of all consumer goods manufactured in 1990 in retail prices was about 459 billion rubles ($2.1 trillion). Nevertheless, the Soviet government had lost control over economic conditions. Government spending increased sharply as an increasing number of unprofitable enterprises required state support and consumer price subsidies to continue. Tax revenues declined as republic and local governments withheld tax revenues from the central government under the growing spirit of regional autonomy. The anti−alcohol campaign reduced tax revenues as well, which in 1982 accounted for about 12% of all state revenue. The elimination of central control over production decisions, especially in the consumer goods sector, led to the breakdown in traditional supplier−producer relationships without contributing to the formation of new ones. Thus, instead of streamlining the system, Gorbachev's decentralization caused new production bottlenecks.

The dissolution of the Soviet Union was a process of systematic disintegration, which occurred in the economy, social structure and political structure. It resulted in the abolition of the Soviet Federal Government ("the Union center") and independence of the USSR's republics on 25 December 1991. The process was caused by a weakening of the Soviet government, which led to disintegration and took place from about 19 January 1990 to 31 December 1991. The process was characterized by many of the republics of the Soviet Union declaring their independence and being recognized as sovereign nation-states.

Andrei Grachev, the Deputy Head of the Intelligence Department of the Central Committee, summed up the denouement of the downfall quite cogently:

"Gorbachev actually put the sort of final blow to the resistance of the Soviet Union by killing the fear of the people. It was still that this country was governed and kept together, as a structure, as a government structure, by the fear from Stalinist times."

The principal elements of the old Soviet political system were Communist Party dominance, the hierarchy of soviets, state socialism, and ethnic federalism. Gorbachev's programs of "perestroika" (restructuring) and "glasnost" (openness) produced radical unforeseen effects that brought that system down. As a means of reviving the Soviet state, Gorbachev repeatedly attempted to build a coalition of political leaders supportive of reform and created new arenas and bases of power. He implemented these measures because he wanted to resolve serious economic problems and political inertia that clearly threatened to put the Soviet Union into a state of long-term stagnation.

But by using structural reforms to widen opportunities for leaders and popular movements in the union republics to gain influence, Gorbachev also made it possible for nationalist, orthodox communist, and populist forces to oppose his attempts to liberalize and revitalize Soviet communism. Although some of the new movements aspired to replace the Soviet system altogether with a liberal democratic one, others demanded independence for the national republics. Still others insisted on the restoration of the old Soviet ways. Ultimately, Gorbachev could not forge a compromise among these forces and the consequence was the dissolution of the Soviet Union.

To restructure the Soviet administrative command system and implement a transition to a market economy, Yeltsin's shock program was employed within days of the dissolution of the Soviet Union. The subsidies to money-losing farms and industries were cut, price controls abolished, and the ruble moved towards convertibility. New opportunities for Yeltsin's circle and other entrepreneurs to seize former state property were created, thus restructuring the old state-owned economy within a few months.

After obtaining power, the vast majority of "idealistic" reformers gained huge possessions of state property using their positions in the government and became business oligarchs in a manner that appeared antithetical to an emerging democracy. Existing institutions were conspicuously abandoned prior to the establishment of new legal structures of the market economy such as those governing private property, overseeing financial markets, and enforcing taxation.

Market economists believed that the dismantling of the administrative command system in Russia would raise GDP and living standards by allocating resources more efficiently. They also thought the collapse would create new production possibilities by eliminating central planning, substituting a decentralized market system, eliminating huge macroeconomic and structural distortions through liberalization, and providing incentives through privatization.

Since the USSR's collapse, Russia faced many problems that free market proponents in 1992 did not expect. Among other things, 25% of the population lived below the poverty line, life expectancy had fallen, birthrates were low, and the GDP was halved. There was a sharp increase in economic inequality: between 1988/1989 and 1993/1995, the Gini ratio increased by an average of 9 points for all former socialist countries. These problems led to a series of crises in the 1990s, which nearly led to the election of Yeltsin's Communist challenger, Gennady Zyuganov, in the 1996 presidential election. In recent years, the economy of Russia has begun to improve greatly, due to major investments and business development and also due to high prices of natural resources.





</doc>
<doc id="7578" url="https://en.wikipedia.org/wiki?curid=7578" title="Corsican language">
Corsican language

Corsican ("corsu" or "lingua corsa" ) is a Romance language within the Italo-Dalmatian subfamily. It is closely related to Tuscan, and thus to Tuscan-based Italian. In its Northern varieties especially, mutual intelligibility between Corsican and both Italian and Tuscan is high. Varieties of Corsican are spoken, and to some extent written, on the islands of Corsica (France) and northern Sardinia (Italy). Corsican used to play the role of a vernacular, while Italian was the official language in Corsica until 1859; afterwards Italian was replaced by French, owing to the acquisition of the island by France from the Republic of Genoa in 1768. Over the next two centuries, the use of French grew to the extent that, by the Liberation in 1945, all islanders had a working knowledge of French. The 20th century saw a wholesale language shift, with islanders changing their language practices to the extent that there were no monolingual Corsican speakers left by the 1960s. By 1995, an estimated 65 percent of islanders had some degree of proficiency in Corsican, and a small minority, perhaps 10 percent, used Corsican as a first language.

As for Corsican, a bone of contention is whether it should be considered an Italian dialect or its own language. Usually, it is not possible to ascertain what an author means by these terms. For example, one might read from some scholars that Corsican either belongs to the "Centro-Southern Italian dialects" along with Tuscan and the Tuscan-based standard Italian, Neapolitan and others or that it is "closely related to the Tuscan dialect of Italian". 

The matter is controversial, as the island was historically and culturally bound to the Italian Mainland from the Middle Ages up until the XIXth century, and installed in diglossic system where both Corsican and Italian were perceived as two sociolinguistic levels of the same language; Corsican and Italian traditionally existed on a spectrum, whose proximity line was blurred enough that the locals needed little else but a change of register to communicate with the standard Italian-speaking elites and administration, by "tuscanising" their tongue, allowing for a practice of code-mixing typical of the Mainland Italian dialects. Despite the geographical proximity, it has indeed been noted that the closest linguistic neighbour to Corsican is not Sardinian, which is a separate group, but rather Tuscan and the extreme Southern Italian lects like Siculo-Calabrian.

One of the characteristics of standard Italian is the retention of the -"re" infinitive ending, as in Latin "mittere" "send". Such infinitival ending is lost in both and Tuscan and Corsican, which has "mette" / "metta", "to put". The Latin relative pronouns "qui"/"quae" "who", and "quod" "what", are inflected in Latin; whereas the relative pronoun in Italian for "who" is "chi" and "what" is "che"/"(che) cosa", it is an uninflected "chì" in Corsican. The biggest difference between standard Italian and Corsican is that the latter uses the "u" termination, whereas standard Italian uses the "o" ending. For example, the Italian demonstrative pronouns "questo" "this" and "quello" "that" become in Corsican "questu" or "quistu" and "quellu" or "quiddu". This feature was typical of the early Italian texts during the Middle Ages.

The Corsican language has been influenced by the languages of the major powers taking an interest in Corsican affairs; earlier by those of the medieval Italian powers: Tuscany (828–1077), Pisa (1077–1282) and Genoa (1282–1768), more recently by France (1768–present), which, since 1789, has promulgated the official Parisian French. The term "gallicised Corsican" refers to Corsican up to about the year 1950. The term "distanciated Corsican" refers to an idealized Corsican from which various agents have removed French or other elements.

The common relationship between Corsica and central Italy can be traced as far back as the Etruscans, who asserted their presence on the island in as early as 500 BC. In 40 AD, the natives of Corsica did not reportedly speak Latin. The Roman exile, Seneca the Younger, reports that both coast and interior were occupied by natives whose language he did not understand (see the Ligurian hypothesis). Whatever language was spoken is still visible in the toponymy or in some words, for instance in the Gallurese dialect spoken in Sardinia "zerru" 'pig'. A similar situation is valid for Sardinian and Sicilian. The occupation of the island by the Vandals around the year 469 marked the end of authoritative influence by Latin speakers (see Medieval Corsica). If the natives of that time spoke Latin, they must have acquired it during the late empire. It has been theorised that a Sardinian variety might have been spoken in Corsica, prior to the island's Tuscanisation under Pisan and Genoese rule.

The two most widely spoken forms of the Corsican language are the groups spoken in the Bastia and Corte area (generally throughout the northern half of the island, known as Haute-Corse, "Cismonte" or "Corsica suprana"), and the groups spoken around Sartène and Porto-Vecchio (generally throughout the southern half of the island, known as Corse-du-Sud, "Pumonti" or "Corsica suttana"). The dialect of Ajaccio has been described as in transition. The dialects spoken at Calvi and Bonifacio are closer to the Genoese dialect, also known as Ligurian.

This division along the Girolata-Porto Vecchio line was due to the massive immigration from Tuscany which took place in Corsica during the lower Middle Ages: as a result, the northern Corsican dialects became very close to a central Italian dialect like Tuscan, while the southern Corsican varieties could keep the original characteristics of the language which make it much more similar to Sicilian and, only to some extent, Sardinian.

The Northern Corsican macro variety ("Supranacciu", "Supranu", "Cismuntincu" or "Cismontano") is the most widespread on the island and standardised as well, and is spoken in North-West Corsica around the districts of Bastia and Corte. The dialect of Bastia and Cap Corse might be included among the Tuscan dialects, being they the closest form to standard Italian than any other Italo-Dalmatian dialects, with the exception of Florentine. All the dialects presenting, in addition to what has already been stated, the conditional tense as being formed by the blend of Infinitive and the Remote Past Tense of "avè" (e.g. "ella amarebbe" "she would love") are generally considered "Cismontani" dialects, situated north of a line uniting the villages of Piana, Vico, Vizzavona, Ghisoni and Ghisonaccia, and also covering the subgroups from the Cap Corse (which, unlike the rest of the island and similarly to Italian, uses "lu", "li", "la", "le" as definite articles), Bastia (besides i>e and a>e, u>o: "ottanta", "momentu", "toccà", "continentale"; a>o: "oliva", "orechja", "ocellu"), Balagna, Niolo and Corte (which retain the general Corsican traits: "distinu", "ghjinnaghju", "sicondu", "billezza", "apartu", "farru", "marcuri", "cantaraghju", "uttanta", "mumentu", "tuccà", "cuntinentale", "aliva", "arechja", "acellu").

Across the Northern and Southern borders of the line separating the Northern dialects from the Southern ones, there is a transitional area picking up linguistic phenomena associated to either of the two groups, with some local peculiarities. Along the Northern line, are the dialects around Piana and Calcatoggio, from Cinarca with Vizzavona (which form the conditional tense like in the South), and Fiumorbo through Ghisonaccia and Ghisoni, which present the retroflex sound "ɖ"; along the Southern line, one can spot the dialects of Ajaccio (having the cacuminal sound "ɖ" evolving from the "ll" nexus, the pronunciation of -"ghj"-, the feminine plurals ending in "i", some Northern words like "cane" and "accattà" instead of "ghjacaru" and "cumprà", as well as "ellu"/"ella" and not "eddu"/"edda"; minor variations: "sabbatu">"sabbitu", "u li dà">"ghi lu dà"; final syllable often stressed and truncated: "marinari">"marinà", "panatteri">"panattè", "castellu">"castè", "cuchjari">"cuchjà"), the Gravona area, Bastelica (which would be classified as Southern, but is also noted for its typical rhotacism: "Basterga") and Solenzara, which did not preserve the short Latin vowels: "seccu", "peru", "rossu", "croci", "pozzu").

The Southern Corsican macro variety ("Suttanacciu", "Suttanu", "Pumontincu" or "Oltramontano") is the most archaic and conservative group, spoken in the districts of Sartena and Porto-Vecchio. Unlike the Northern varieties and similarly to Sardinian, such group retains the distinction of the short Latin vowels "ĭ" e "ŭ" (e.g. "pilu", "bucca"). It is also strongly marked by the presence of the voiced retroflex stop, like Sicilian (e.g. "aceddu", "beddu", "quiddu", "ziteddu", "famidda"), and the conditional tense as being formed by the blend of Infinitive and the Imperfect Tense of "avè" (e.g. "idda amarìa" "she would love"). All the "Oltramontani" dialects are from an area located to the South of Porticcio, Bastelica, Col di Verde and Solenzara. Notable dialects are those from around Taravo (cacuminal -"dd"- only for the -"ll" nexus: "frateddu", "suredda", "beddu"; but preservation of the palatal lateral approximant: "piglià", "famiglia", "figliolu", "vogliu"; it does not preserve the short Latin vowels: "seccu", "peru", "rossu", "croci", "pozzu"), Sartena (preserving the short Latin vowels: "siccu", "piru", "russu", "cruci", "puzzu"; changing the "rn" nexus to "rr": "forru", "carri", "corru"; presenting the voiced retroflex stop in place of the palatal lateral approximant: "piddà", "famidda", "fiddolu", "voddu"; imperfect tense like "cantàvami", "cantàvani"; masculing plurals ending in "a": "l'ochja", "i poma"; having "eddu/edda/eddi" as personal pronouns), the Alta Rocca (the most conservative area in Corsica, being very close to the varieties spoken in Northern Sardinia), and the Southern region located between Porto-Vecchio and Bonifacio's hinterlands (masculin singulars always ending in "u": "fiumu", "paesu", "patronu"; masculin plurals always ending in "a": "i letta", "i solda", "i ponta", "i foca", "i mura", "i loca", "i balcona"; imperfect tense like "cantàiami", "cantàiani").

The Gallurese variety is spoken in the extreme north of Sardinia, including the region of Gallura and the archipelago of La Maddalena, and Sassarese is spoken in Sassari and in its neighbourhood, in the northwest of Sardinia. Whether the two should be included either in Corsican or in Sardinian as dialects or considered independent languages is still subject of debate.

On Maddalena archipelago the local dialect (called "Isulanu, Maddaleninu, Maddalenino") was brought by fishermen and shepherds from Bonifacio during immigration in the 17th and 18th centuries. Though influenced by Gallurese, it has maintained the original characteristics of Corsican. There are also numerous words of Genoese and Ponzese origin.

On October 14, 1997, Article 2 Item 4 of Law Number 26 of the Autonomous Region of Sardinia granted ""al dialetto sassarese e a quello gallurese"" – equal legal status with the other languages on Sardinia. They are thus legally defined as different languages from Sardinian by the Sardinian government.
The January 2007 estimated population of Corsica was 281,000, whereas the figure for the March 1999 census, when most of the studies—though not the linguistic survey work referenced in this article—were performed, was about 261,000 (see under Corsica). Only a fraction of the population at either time spoke Corsican with any fluency.

The use of Corsican language over French language has been declining. In 1980 about 70 percent of the population of the island "had some command of the Corsican language." In 1990 out of a total population of about 254,000 the percentage had declined to 50 percent, with only 10 percent using it as a first language. (These figures do not count varieties of Corsican spoken in Sardinia.) The language appeared to be in serious decline when the French government reversed its unsupportive stand and initiated some strong measures to save it.

According to an official survey run on behalf of the "Collectivité territoriale de Corse" which took place in April 2013, in Corsica the Corsican language has a number of speakers situated between 86,800 and 130,200, out of a total population amounting to 309,693 inhabitants. The percentage of those who have a solid knowledge of the language varies between a minimum of 25 percent in the 25-34 age group and the maximum of 65 percent in the over-65 age group: almost a quarter of the former age group does not understand Corsican, while only a small minority of the older people do not understand it. While 32 percent of the population of northern Corsica speaks Corsican quite well, this percentage drops to 22 percent for South Corsica. Moreover, 10 percent of the population of Corsica speaks only French, while 62 percent speak both French and Corsican. However, only 8 percent of the Corsicans know how to write correctly in Corsican, while about 60 percent of the population does not know how to write in Corsican. While 90 percent of the population is in favor of a Corsican-French bilingualism, 3 percent would like to have Corsican as the only official language in the island, and 7 percent would prefer French in this role.

UNESCO classifies Corsican as a "definitely endangered language." The Corsican language is a key vehicle for Corsican culture, which is notably rich in proverbs and in polyphonic song.

When the French Assembly passed the Deixonne Law in 1951, which made it possible for regional languages to be taught at school, Alsatian, Flemish and Corsican were not included on the ground of being classified as "dialectes allogènes" of German, Dutch and Italian respectively; only in 1974 were they too politically recognized as regional languages for their teaching on a voluntary basis.

The 1991 "Joxe Statute", in setting up the Collectivité Territoriale de Corse, also provided for the Corsican Assembly, and charged it with developing a plan for the optional teaching of Corsican. The University of Corsica Pasquale Paoli at Corte, Haute-Corse took a central role in the planning.

At the primary school level Corsican is taught up to a fixed number of hours per week (three in the year 2000) and is a voluntary subject at the secondary school level, but is required at the University of Corsica. It is available through adult education. It can be spoken in court or in the conduct of other government business if the officials concerned speak it. The Cultural Council of the Corsican Assembly advocates for its use, for example, on public signs.

According to the anthropologist Dumenica Verdoni, writing new literature in modern Corsican, known as the "Riacquistu", is an integral part of affirming Corsican identity. Some individuals have returned from careers in continental France to write in Corsican, including Dumenicu Togniotti, director of the "Teatru Paisanu", which produced polyphonic musicals, 1973–1982, followed in 1980 by Michel Raffaelli's "Teatru di a Testa Mora", and Saveriu Valentini's "Teatru Cupabbia" in 1984. Modern prose writers include Alanu di Meglio, Ghjacumu Fusina, Lucia Santucci, and Marcu Biancarelli.

There were writers working in Corsican in the 1700s and 1800s

Ferdinand Gregorovius, 19th century traveller and enthusiast of Corsican culture, reported that the preferred form of the literary tradition of his time was the "vocero", a type of polyphonic ballad originating from funeral obsequies. These laments were similar in form to the chorales of Greek drama except that the leader could improvise. Some performers were noted at this, such as the 1700s Mariola della Piazzole and Clorinda Franseschi. However, the trail of written popular literature of known date in Corsican currently goes no further back than the 17th century. An undated corpus of proverbs from communes may well precede it (see under "External links" below). Corsican has also left a trail of legal documents ending in the late 12th century. At that time the monasteries held considerable land on Corsica and many of the churchmen were notaries.

Between 1200 and 1425 the monastery of Gorgona, which belonged to the Order of Saint Benedict for much of that time and was in the territory of Pisa, acquired about 40 legal papers of various sorts related to Corsica. As the church was replacing Pisan prelates with Corsican ones there, the legal language shows a transition from entirely Latin through partially Latin and partially Corsican to entirely Corsican. The first known surviving document containing some Corsican is a bill of sale from Patrimonio dated to 1220. These documents were moved to Pisa before the monastery closed its doors and were published there. Research into earlier evidence of Corsican is ongoing.

Corsican is written in the standard Latin script, using 21 of the letters for native words. The letters j, k, w, x, and y are found only in foreign names and French vocabulary. The digraphs and trigraphs "chj", "ghj", "sc" and "sg" are also defined as "letters" of the alphabet in its modern scholarly form (compare the presence of "ch" or "ll" in the Spanish alphabet) and appear respectively after "c", "g" and "s".

The primary diacritic used is the grave accent, indicating word stress when it is not penultimate. In scholarly contexts, disyllables may be distinguished from diphthongs by use of the diaeresis on the former vowel (as in Italian and distinct from French and English). In older writing, the acute accent is sometimes found on stressed , the circumflex on stressed , indicating respectively () and () phonemes.

Corsican has been regarded as a dialect of Italian historically, similar to the Romance lects developed on the Italian peninsula, and in writing, it also resembles Italian (with the generalised substitution of -"u" for final -"o" and the articles "u" and "a" for "il/lo" and "la" respectively; however, both the dialect of Cap Corse and Gallurese retain the original articles "lu" and "la"). On the other hand, the phonemes of the modern Corsican dialects have undergone complex and sometimes irregular phenomena depending on phonological context, so the pronunciation of the language for foreigners familiar with other Romance languages is not straightforward.

As in Italian, the grapheme appears in some digraphs and trigraphs in which it does not represent the phonemic vowel. All vowels are pronounced except in a few well-defined instances. is not pronounced between and : "sciarpa" ; or initially in some words: "istu" .

Vowels may be nasalized before (which is assimilated to before or ) and the palatal nasal consonant represented by . The nasal vowels are represented by the vowel plus , or . The combination is a digraph or trigraph indicating the nasalized vowel. The consonant is pronounced in weakened form. The same combination of letters might not be the digraph or trigraph but might be just the non-nasal vowel followed by the consonant at full weight. The speaker must know the difference. Example of nasal: is pronounced and not .

The Northern and central dialects in the vicinity of the Taravo river adopt the Italian seven-vowel system, whereas all the Southern ones around the so-called "archaic zone" with its centre being the town of Sartene (including the Gallurese dialect spoken in Northern Sardinia) resort to a five-vowel system without length differentiation, like Sardinian.

The vowel inventory, or collection of phonemic vowels (and the major allophones), transcribed in IPA symbols, is:




</doc>
<doc id="7580" url="https://en.wikipedia.org/wiki?curid=7580" title="Commodore International">
Commodore International

Commodore International (or Commodore International Limited) was an American home computer and electronics manufacturer founded by Jack Tramiel. Commodore International (CI), along with its subsidiary Commodore Business Machines (CBM), participated in the development of the home–personal computer industry in the 1970s and 1980s. The company developed and marketed the world's best-selling desktop computer, the Commodore 64 (1982), and released its Amiga computer line in July 1985. With quarterly sales ending 1983 of $49 million, Commodore was one of the world's largest personal computer manufacturers.

The company that would become Commodore Business Machines, Inc. was founded in 1954 in Toronto as the Commodore Portable Typewriter Company by Polish immigrant and Auschwitz survivor Jack Tramiel. For a few years he had been living in New York, driving a taxicab, and running a small business repairing typewriters, when he managed to sign a deal with a Czechoslovakian company to manufacture their designs in Canada. He moved to Toronto to start production. By the late 1950s a wave of Japanese machines forced most North American typewriter companies to cease business, but Tramiel instead turned to adding machines.

In 1955, the company was formally incorporated as Commodore Business Machines, Inc. (CBM) in Canada. In 1962 Commodore went public on the New York Stock Exchange (NYSE), under the name of Commodore International Limited. In the late 1960s, history repeated itself when Japanese firms started producing and exporting adding machines. The company's main investor and chairman, Irving Gould, suggested that Tramiel travel to Japan to understand how to compete. Instead, Tramiel returned with the new idea to produce electronic calculators, which were just coming on the market.

Commodore soon had a profitable calculator line and was one of the more popular brands in the early 1970s, producing both consumer as well as scientific/programmable calculators. However, in 1975, Texas Instruments, the main supplier of calculator parts, entered the market directly and put out a line of machines priced at less than Commodore's cost for the parts. Commodore obtained an infusion of cash from Gould, which Tramiel used beginning in 1976 to purchase several second-source chip suppliers, including MOS Technology, Inc., in order to assure his supply. He agreed to buy MOS, which was having troubles of its own, only on the condition that its chip designer Chuck Peddle join Commodore directly as head of engineering.

Through the 1970s Commodore also produced numerous peripherals and consumer electronic products such as the Chessmate, a chess computer based around a MOS 6504 chip, released in 1978.

In December 2007, when Tramiel was visiting the Computer History Museum in Mountain View, California for the 25th anniversary of the Commodore 64, he was asked why he called his company Commodore. He said: "I wanted to call my company General, but there's so many Generals in the U.S.: General Electric, General Motors. Then I went to Admiral, but that was taken. So I wind up in Berlin, Germany, with my wife, and we were in a cab, and the cab made a short stop, and in front of us was an Opel Commodore." Tramiel gave this account in many interviews, but Opel's Commodore didn't debut until 1967, years after the company had been named.

Once Chuck Peddle had taken over engineering at Commodore, he convinced Jack Tramiel that calculators were already a dead end, and that they should turn their attention to home computers. Peddle packaged his single-board computer design in a metal case, initially with a keyboard using calculator keys, later with a full-travel QWERTY keyboard, monochrome monitor, and tape recorder for program and data storage, to produce the Commodore PET (Personal Electronic Transactor). From PET's 1977 debut, Commodore would be a computer company.

Commodore had been reorganized the year before into Commodore International, Ltd., moving its financial headquarters to the Bahamas and its operational headquarters to West Chester, Pennsylvania, near the MOS Technology site. The operational headquarters, where research and development of new products occurred, retained the name Commodore Business Machines, Inc. In 1980 Commodore launched production for the European market in Braunschweig (Germany).

By 1980, Commodore was one of the three largest microcomputer companies, and the largest in the Common Market. "BYTE" stated of the business computer market, however, that "the lack of a marketing strategy by Commodore, as well as its past nonchalant attitude toward the encouragement and development of good software, has hurt its credibility, especially in comparison to the other systems on the market". The author of "Programming the PET/CBM" (1982) stated in its introduction that "CBM's product manuals are widely recognized to be unhelpful; this is one of the reasons for the existence of this book".

The PET computer line was used primarily in schools, where its tough all-metal construction and ability to share printers and disk drives on a simple local area network were advantages, but PETs did not compete well in the home setting where graphics and sound were important. This was addressed with the introduction of the VIC-20 in 1981, which was introduced at a cost of US$299 and sold in retail stores. Commodore bought aggressive advertisements featuring William Shatner asking consumers "Why buy just a video game?" The strategy worked and the VIC-20 became the first computer to ship more than one million units. A total of 2.5 million units were sold over the machine's lifetime and helped Commodore's sales to Canadian schools. In another promotion aimed at schools (and as a way of getting rid of old unsold inventory) some PET models labeled "Teacher's PET" were given away as part of a "buy 2 get 1 free" promotion.

In 1982, Commodore introduced the Commodore 64 as the successor to the VIC-20. Thanks to a well-designed set of chips designed by MOS Technology, the Commodore 64, (also referred to as C64), possessed remarkable sound and graphics for its time and is often credited with starting the computer demo scene. Its US$595 price was high compared with that of the VIC-20, but it was still much less expensive than any other 64K computer on the market. Early C64 advertisements boasted, "You can't buy a better computer at twice the price." Australian adverts in the mid-1980s used a tune speaking the words "Are you keeping up with the Commodore? Because the Commodore is keeping up with you."

In 1983, Tramiel decided to focus on market share and cut the price of the VIC-20 and C64 dramatically, starting what would be called the "home computer war". TI responded by cutting prices on its TI-99/4A, which had been introduced in 1981. Soon there was an all-out price war involving Commodore, TI, Atari, and practically every vendor other than Apple Computer. Commodore began selling the VIC-20 and C64 through mass-market retailers such as K-Mart, in addition to traditional computer stores. By the end of this conflict, Commodore had shipped somewhere around 22 million C64s, making the C64 the best selling computer of all time.
At the June 1983 Consumer Electronics Show, Commodore lowered the retail price of the 64 to $300, and stores sold it for as little as $199. At one point the company was selling as many computers as the rest of the industry combined. Its prices for the VIC-20 and 64 were $50 lower than Atari's prices for the 600XL and 800XL. Commodore's strategy was to, according to a spokesman, devote 50% of its efforts to the under-$500 market, 30% on the $500–1000 market, and 20% on the over-$1000 market. Its vertical integration and Tramiel's focus on cost control helped Commodore do well during the price war, with $1 billion in 1983 sales. Although the company and Tramiel's focus on cost cutting over product testing caused many hardware defects in the 64, by early 1984 Synapse Software—the largest provider of third-party Atari 8-bit software—received 65% of sales from the Commodore market, and Commodore sold almost three times as many computers as Atari that year.

Despite its focus on the lower end of the market, Commodore's computers were also sold in upmarket department stores such as Harrod's. The company also attracted several high-profile customers. In 1984, the company's British branch became the first manufacturer to receive a royal warrant for computer business systems. NASA's Kennedy Space Center was another noted customer, with over 60 Commodore systems processing documentation, tracking equipment and employees, costing jobs, and ensuring the safety of hazardous waste.

Although by early 1984 "Creative Computing" compared Commodore to "a well-armed battleship [which] rules the micro waves" and threatened to destroy rivals like Atari and Coleco, Commodore's board of directors were as impacted as anyone else by the price spiral and decided they wanted out. An internal power struggle resulted; in January 1984, Tramiel resigned due to intense disagreement with the chairman of the board, Irving Gould. Gould replaced Tramiel with Marshall F. Smith, a steel executive who had no experience with computers or consumer marketing. Tramiel founded a new company, Tramel Technology (spelled differently so people would pronounce it correctly), and hired away a number of Commodore engineers to begin work on a next-generation computer design.

Now it was left to the remaining Commodore management to salvage the company's fortunes and plan for the future. It did so by buying a small startup company called Amiga Corporation in February 1983, for $25 million ($12.8 million in cash and 550,000 in common shares) which became a subsidiary of Commodore, called Commodore-Amiga, Inc. Commodore brought this new 32-bit computer design (initially codenamed "Lorraine") from 1979, and had been called High-Toro from 1980 to 1981 then later dubbed the Amiga, under Amiga Inc. in early 1982. There were three unsuccessful attempts to release the Amiga by Jay Miner and company. These were: 1982, 1983 and one more after Commodore bought Amiga in 1984, after which it was released only to the local public. Then in 1985 Commodore re-released it to the world. Cost was $1000-$1300.

But Tramiel had beaten Commodore to the punch. His design was 95% completed by June (which fueled speculation that his engineers had taken technology with them from Commodore). In July 1984 he bought the consumer side of Atari Inc. from Warner Communications which allowed him to strike back and release the Atari ST earlier in 1985 for about $800. The Atari ST was technology-wise almost out, however the Amiga was out sooner.

During development in 1981, Amiga had exhausted venture capital and was desperate for more financing. Jay Miner and company had approached former employer Atari, and the Warner-owned Atari had paid Amiga to continue development work. In return Atari was to get one-year exclusive use of the design as a video game console. After one year Atari would have the right to add a keyboard and market the complete Amiga computer. The Atari Museum has acquired the Atari-Amiga contract and Atari engineering logs revealing that the Atari Amiga was originally designated as the 1850XLD. As Atari was heavily involved with Disney at the time, it was later code-named "Mickey", and the 256K memory expansion board was codenamed "Minnie".

The following year, Tramiel discovered that Warner Communications wanted to sell Atari, which was rumored to be losing about $10,000 a day. Interested in Atari's overseas manufacturing and worldwide distribution network for his new computer, he approached Atari and entered negotiations. After several on-again/off-again talks with Atari in May and June 1984, Tramiel had secured his funding and bought Atari's Consumer Division (which included the console and home computer departments) in July.

As more execs and researchers left Commodore after the announcement to join up with Tramiel's new company Atari Corp., Commodore followed by filing lawsuits against four former engineers for theft of trade secrets in late July. This was intended, in effect, to bar Tramiel from releasing his new computer.

One of Tramiel's first acts after forming Atari Corp. was to fire most of Atari's remaining staff, and to cancel almost all ongoing projects, in order to review their continued viability. In late July/early August, Tramiel representatives discovered the original Amiga contract from the previous fall. Seeing a chance to gain some leverage, Tramiel immediately used the contract to counter-sue Commodore through its new subsidiary, Amiga, on August 13.

The Amiga crew, still suffering serious financial problems, had sought more monetary support from investors that entire spring. At around the same time that Tramiel was in negotiations with Atari, Amiga entered into discussions with Commodore. The discussions ultimately led to Commodore's intentions to purchase Amiga outright, which would (from Commodore's viewpoint) cancel any outstanding contracts - including Atari Inc.'s. This "interpretation" is what Tramiel used to counter-sue, and sought damages and an injunction to bar Amiga (and effectively Commodore) from producing any resembling technology. This was an attempt to render Commodore's new acquisition (and the source for its next generation of computers) useless. The resulting court case lasted for several years, with both companies releasing their respective products. In the end, the Amiga computer outlasted the Atari.

Throughout the life of the ST and Amiga platforms, a ferocious Atari-Commodore rivalry raged. While this rivalry was in many ways a holdover from the days when the Commodore 64 had first challenged the Atari 800 (among others) in a series of scathing television commercials, the events leading to the launch of the ST and Amiga only served to further alienate fans of each computer, who fought vitriolic holy wars on the question of which platform was superior. This was reflected in sales numbers for the two platforms until the release of the Amiga 500 in 1987, which led the Amiga sales to exceed the ST by about 1.5 to 1, despite reaching the market later. However, the battle was in vain, as neither platform captured a significant share of the world computer market and only the Apple Macintosh would survive the industry-wide shift to Microsoft Windows running on PC clones.

Adam Osborne stated in April 1981 that "the microcomputer industry abounds with horror stories describing the way Commodore treats its dealers and its customers." Many in the industry believed rumors in late 1983 that Commodore would discontinue the 64 despite its great success because they disliked the company's business practices, including poor treatment of dealers and introducing new computers incompatible with existing ones. One dealer said "It's too unsettling to be one of their dealers and not know where you stand with them." After Tramiel's departure, another journalist wrote that he "had never been able to establish very good relations with computer dealers ... computer retailers have accused Commodore of treating them as harshly as if they were suppliers or competitors, and as a result, many have become disenchanted with Commodore and dropped the product line". However, upon the 1987 introduction of the Amiga 2000, Commodore retreated from its earlier strategy of selling its computers to discount outlets and toy stores, and now favored authorized dealers. Software developers also disliked the company, with one stating that "Dealing with Commodore was like dealing with Attila the Hun." At the 1987 Comdex, an informal "InfoWorld" survey found that none of the developers present planned to write for Commodore platforms. Although Comdex was oriented toward business computing, not Commodore's traditional consumer market, such a response did not bode well for Commodore's efforts to establish the Amiga as a business platform.

Commodore faced the problem, when marketing the Amiga, of still being seen as the company that made cheap computers like the 64 and VIC. By the late 1980s, the personal computer market had become dominated by the IBM PC and Apple Macintosh platforms and Commodore's marketing efforts for the Amiga were less successful in breaking the new computer into this now-established market than its promotions for the 8-bit line had been in making Commodore the home computer leader. The company put effort into developing and promoting consumer products that would not be in demand for years, such as an Amiga 500-based HTPC called CDTV. As early as 1986, the mainstream press was predicting Commodore's demise, and in 1990 "Computer Gaming World" wrote of its "abysmal record of customer and technical support in the past". Nevertheless, as profits and the stock price began to slide, "The Philadelphia Inquirer's" Top 100 Businesses annual continued to list several Commodore executives among the highest-paid in the region and the paper documented the company's questionable hiring practices and large bonuses paid to executives amid shareholder discontent.

Commodore failed to update the Amiga to keep pace as the PC platform advanced. CBM continued selling Amiga 2000s with 7.14 MHz 68000 CPUs, even though the Amiga 3000 with 25 MHz 68030 was on the market. Apple by this time was using the 68040 and had relegated the 68000 to its lowest end model, the black and white Macintosh Classic. The 68000 was used in the Sega Genesis, one of the leading game consoles of the era, PCs fitted with high-color VGA graphics cards and SoundBlaster (or compatible) sound cards had finally caught up with the Amiga's performance and Commodore began to fade from the consumer market. Although the Amiga was originally conceived as a gaming machine, Commodore had always emphasized the Amiga's potential for professional applications. But the Amiga's high-performance sound and graphics were irrelevant for most of the day's MS-DOS-based routine business word-processing and data-processing requirements, and the machine could not successfully compete with PCs in a business market that was rapidly undergoing commoditization. Commodore introduced a range of PC compatible systems designed by its German division, and while the Commodore name was better known in the US than some of its competition, the systems' price and specs were only average.

In 1992, the A600 replaced the A500. It removed the numeric keypad, Zorro expansion slot, and other functionality, but added IDE, PCMCIA and a theoretically cost-reduced design. Designed as the Amiga 300, a nonexpandable model to sell for less than the Amiga 500, the 600 was forced to become a replacement for the 500 due to the unexpected higher cost of manufacture. Productivity developers increasingly moved to PC and Macintosh, while the console wars took over the gaming market. David Pleasance, managing director of Commodore UK, described the A600 as a 'complete and utter screw-up'.

In 1992, Commodore released the Amiga 1200 and Amiga 4000 computers, which featured an improved graphics chipset, the AGA. The custom-designed and custom-built AGA chipset cost Commodore more than the commodity chips used in IBM PCs, despite lagging them in performance. The advent of PC games using 3D graphics such as "Doom" and "Wolfenstein 3D" spelled the end of Amiga as a gaming platform, due to mismanagement.

In 1993, the 'make or break' system, according to Pleasance, was a 32-bit CD-ROM-based game console called the Amiga CD32, but it was not sufficiently profitable to put Commodore back in the black.

In 1992, all UK servicing and warranty repairs were outsourced to Wang Laboratories, which was replaced by ICL after failing to meet repair demand during the Christmas rush in 1992. By 1994, only the operations in Germany and the United Kingdom were still profitable. Commodore declared bankruptcy on April 29, 1994, and ceased to exist, causing the board of directors to "authorize the transfer of its assets to trustees for the benefit of its creditors", according to an official statement.

The company's computer systems, especially the C64 and Amiga series, retained a cult following decades after its demise.

Following its liquidation, Commodore's former assets went their separate ways, with none of the descendant companies repeating Commodore's early success. Both Commodore and Amiga product lines were produced in the 21st century, but separately with Amiga, Inc. being its own company and Commodore computers being produced by Commodore USA, an unrelated Florida-based company that had purchased the brand name. Other companies develop operating systems and manufacture computers for both Commodore and Amiga brands as well as software.

Commodore UK was the only subsidiary to survive the bankruptcy and even placed a bid to buy out the rest of the operation, or at least the former parent company. For a time it was considered the front runner in the bid, and numerous reports surfaced during the 1994–1995 time frame that Commodore UK had made the purchase. Commodore UK stayed in business by selling old inventory and making computer speakers and some other types of computer peripherals. However, Commodore UK withdrew its bid at the start of the auction process after several larger companies, including Gateway Computers and Dell Inc., became interested, primarily for Commodore's 47 patents relating to the Amiga. Ultimately, the successful bidder was German PC conglomerate Escom, and Commodore UK went into liquidation on August 30, 1995.

In 1995 Escom paid US$14 million for the assets of Commodore International. It separated the Commodore and Amiga operations into separate divisions and quickly started using the Commodore brand name on a line of PCs sold in Europe. However, it soon started losing money due to over-expansion, went bankrupt on July 15, 1996, and was liquidated.

In September 1997, the Commodore brand name was acquired by Dutch computer maker Tulip Computers NV.

In July 2004, Tulip announced a new series of products using the Commodore name: fPET, a flash memory-based USB Flash drive; mPET, a flash-based MP3 Player and digital recorder; eVIC, a 20 GB music player. Also, it licensed the Commodore trademark and "chicken lips" logo to the producers of the C64 DTV.

In late 2004, Tulip sold the Commodore trademarks to Yeahronimo Media Ventures for €22 million. The sale was completed in March 2005 after months of negotiations. Yeahronimo Media Ventures soon renamed itself to "Commodore International Corporation" and started an operation intended to relaunch the Commodore brand. The company launched its "Gravel" line of products: personal multimedia players equipped with Wi-Fi, with the hope the Commodore brand would help them take off. The "Gravel" was never a success and was discontinued. On June 24, 2009, CIC renamed itself to Reunite Investments. CIC's founder, Ben van Wijhe, bought a Hong Kong-based company called Asiarim. The brand is now owned by C= Holdings (formerly Commodore International B.V.): Reunite became the sole owner of it in 2010, after buying the remaining shares from the bankrupt Nedfield, then sold it to Commodore Licensing BV, a subsidiary of Asiarim, later in 2010. It was sold again on 7 November 2011: this transaction became the basis of a legal dispute between Asiarim (which, even after that date, made commercial use of the Commodore trademark, among others by advertising for sale Commodore-branded computers, and dealing licensing agreements for the trademarks) and the new owners, that was resolved by the United States District Court for the Southern District of New York on 16 December 2013 in favour of the new owners.

The Commodore Semiconductor Group (formerly MOS Technology, Inc.) was bought by its former management and in 1995, resumed operations under the name GMT Microelectronics, utilizing a troubled facility in Norristown, Pennsylvania that Commodore had closed in 1992. By 1999 it had $21 million in revenues and 183 employees. However, in 2001 the United States Environmental Protection Agency shut the plant down. GMT ceased operations and was liquidated.

Ownership of the remaining assets of Commodore International, including the copyrights and patents, and the Amiga trademarks, passed from Escom to U.S. PC clone maker Gateway 2000 in 1997, who retained the patents and sold the copyrights and trademarks, together with a license to use the patents, to Amiga, Inc., a Washington company founded, among others, by former Gateway subcontractors Bill McEwen and Fleecy Moss in 2000. On March 15, 2004, Amiga, Inc. announced that on April 23, 2003 it had transferred its rights over past and future versions of the Amiga OS (but not yet over other intellectual property) to Itec, LLC, later acquired by KMOS, Inc., a Delaware company. Shortly afterwards, on the basis of some loans and security agreements between Amiga, Inc. and Itec, LLC, the remaining intellectual property assets were also transferred from Amiga, Inc. to KMOS, Inc. On March 16, 2005, KMOS, Inc. announced that it had completed all registrations with the State of Delaware to change its corporate name to Amiga, Inc. The Commodore/Amiga copyrights were later sold to Cloanto. AmigaOS (as well as spin-offs MorphOS and AROS) is still maintained and updated. Several companies produce related hardware and software today.

Commodore's former US headquarters is currently the headquarters to QVC.

In February 2017 an exhibition room for about 200 Commodore products was opened in Braunschweig, commemorating the European production site of Commodore which had up to 2000 employees.

This product line consists of original Commodore products.

774D, 776M, 796M, 9R23, C108, C110, F4146R, F4902, MM3, Minuteman 6, P50, PR100, SR1800, SR4120D, SR4120R, SR4148D, SR4148R, SR4190R, SR4212, SR4912, SR4921RPN, SR5120D, SR5120R, SR5148D, SR5148R, SR5190R, SR59, SR7919, SR7949, SR9150R, SR9190R, US*3, and The Specialist series: M55 (The Mathematician), N60 (The Navigator), S61 (The Statistician).

"(listed chronologically)"


1000, 1024, 1070, 1080, 1081, 1083S, 1084, 1084S, 1084ST, 1085S, 1201, 1402, 1403, 1404, 1405, 1407, 1428, 1428x, 1432D, 1432V, 1701, 1702, 1703, 1801, 1802, 1803, 1900M/DM602, 1901/75BM13/M1, 1902, 1902A, 1930, 1930-II, 1930-III, 1934, 1935, 1936, 1936ALR, 1940, 1942, 1950, 1960, 1962, 2002, A2024, 2080, 76M13, CM-141, DM-14, DM602 

Commodore's own software had a poor reputation; "InfoWorld" in 1984, for example, stated that "so far, the normal standard for Commodore software is mediocrity". Third parties developed the vast majority of software for Commodore computers.




</doc>
<doc id="7581" url="https://en.wikipedia.org/wiki?curid=7581" title="Commodore (rank)">
Commodore (rank)

Commodore is a naval rank used in many navies that is superior to a navy captain, but below a rear admiral. Non-English-speaking nations often use the rank of flotilla admiral or counter admiral or senior captain as an equivalent, although counter admiral may also correspond to rear admiral.

Traditionally, "commodore" is the title for any officer assigned to command more than one ship at a time, even temporarily, much as "captain" is the traditional title for the commanding officer of a single ship even if the officer's official title in the service is a lower rank. As an official rank, a commodore typically commands a flotilla or squadron of ships as part of a larger task force or naval fleet commanded by an admiral. A commodore's ship is typically designated by the flying of a Broad pennant, as opposed to an admiral's flag. 

It is often regarded as a one-star rank with a NATO code of OF-6 (which is known in the U.S. as "rear admiral (lower half)"), but whether it is regarded as a flag rank varies between countries.

It is sometimes abbreviated: as "Cdre" in British Royal Navy, "CDRE" in the US Navy, "Cmdre" in the Royal Canadian Navy, "COMO" in the Spanish Navy and in some navies speaking the Spanish language, or "CMDE" as used in the Indian Navy or in some other Navies.

The rank of commodore derives from the French "commandeur", which was one of the highest ranks in the orders of knighthood, and in military orders the title of the knight in charge of a "commande" (a local part of the order's territorial possessions).

The Dutch Navy also used the rank of "commandeur" from the end of the 16th century for a variety of temporary positions, until it became a conventional permanent rank in 1955. The Royal Netherlands Air Force has adopted the English spelling of "commodore" for an equivalent rank.

In the Royal Navy, the position was introduced in the 17th Century to combat the cost of appointing more admirals—a costly business with a fleet as large as the Royal Navy's at that time.

The rank of commodore was at first a position created as a temporary title to be bestowed upon captains who commanded squadrons of more than one vessel. In many navies, the rank of commodore was merely viewed as a senior captain position, whereas other naval services bestowed upon the rank of commodore the prestige of flag officer status.

In 1899, the substantive rank of commodore was discontinued in the United States Navy, but revived during World War II in both the U.S. Navy and U.S. Coast Guard. It was discontinued as a rank in these services during the postwar period, but as an appointment, the title "commodore" was then used to identify senior U.S. Navy captains who commanded squadrons of more than one vessel or functional air wings or air groups that were not part of a carrier air wing or carrier air group. Concurrently, until the early 1980s, U.S. Navy and U.S. Coast Guard captains selected for promotion to the rank of rear admiral (lower half), would wear the same insignia as rear admiral (upper half), i.e., two silver stars for collar insignia or sleeve braid of one wide and one narrow gold stripe, even though they were actually only equivalent to one-star officers and paid at the one-star rate.

To correct this inequity, the rank of commodore as a single star flag officer was reinstated by both services in the early 1980s. This immediately caused confusion with those senior U.S. Navy captains commanding destroyer squadrons, submarine squadrons, functional air wings and air groups, and so on, who held the temporary "title" of commodore while in their major command billet. As a result of this confusion, the services soon renamed the new one-star rank as commodore admiral (CADM) within the first six months following the rank's reintroduction. However, this was considered an awkward title and the one-star flag rank was renamed a few months later to its current title of rear admiral (lower half), later abbreviated by the U.S. Navy and U.S. Coast Guard as RDML.

The "title" of commodore continues to be used in the U.S. Navy and Coast Guard for those senior captains in command of organizations consisting of groups of ships or submarines organized into squadrons; air wings or air groups of multiple aviation squadrons other than carrier air wings (the latter whose commanders still use the title "CAG"); explosive ordnance disposal (EOD), mine warfare and special warfare (SEAL) groups; and construction battalion (SeaBee) regiments. Although not flag officers, modern day commodores in the U.S. Navy rate a blue and white command pennant, also known as a broad pennant, that is normally flown at their headquarters facilities ashore or from ships that they are embarked aboard when they are the senior officer present afloat (SOPA).

In the Argentine Navy, the position of commodore was created in the late 1990s, and is usually, but not always, issued to senior captains holding rear-admirals' positions. It is not a rank but a distinction and, as such, can be issued by the chief of staff without congressional approval. Its equivalents are colonel-major in the Army and commodore-major in the Air Force. It is usually—but incorrectly—referred to as "navy commodore", to avoid confusion with the "air force commodore", which is equivalent to the navy's captain and army's colonel. The sleeve lace is identical to that of the Royal Navy, and wears one star on the epaulette.

The following articles deal with the rank of commodore (or its equivalent) as it is employed OF-6 one-star flag officer rank in various countries.

Commodore, in Spanish "comodoro", is a rank in the Argentine Air Force. This rank is the equivalent of a colonel in the Argentine Army, and a colonel or group captain in other air forces of the world. The Argentine rank below commodore is the rank of vice-commodore (Spanish "vicecomodoro") equivalent to a lieutenant-colonel in the Argentine Army, and a lieutenant-colonel or wing commander in other air forces.

Commodore is a rank in the Royal Netherlands Air Force. It is a one-star rank and has essentially the same rank insignia as the British air commodore.

Many air forces use the rank of air commodore. This rank was first used by the Royal Air Force and is now used in many countries such as Australia, Bangladesh, Greece, India, New Zealand, Nigeria, Pakistan, Thailand and Zimbabwe. It is the equivalent rank to the navy rank of "commodore", and the army ranks of brigadier and brigadier general.

The German air force used the concept of a unit commodore for the commander of a wing, usually in the rank of colonel (OF-5).

Commodore is also a title held by many captains as recognition of exceptional navigation ability and seagoing seniority in the Merchant Service, and by the directors of a few yacht clubs and boating associations. Commodores 'in command' as Master aboard Merchant Marine ships wear distinctive rank and cap insignia denoting their honorific high rank position.

During wartime, a shipping convoy will have a ranking officer—sometimes an active-duty naval officer, at other times a civilian master or retired naval officer—designated as the "convoy commodore". This title is not related to the individuals military rank (if any), but instead is the title of the senior individual responsible for the overall operation of the merchant ships and naval auxiliary ships that make up the convoy. The convoy commodore does not command the convoy escort forces (if any), which are commanded by a naval officer who serves as escort commander.

The U.S. Coast Guard Auxiliary also employs variants of the "title" of commodore. Members of the Auxiliary are civilian volunteers who do not have military rank, but who do wear modified U.S. Coast Guard uniforms and U.S. military-style officer rank insignia to indicate office. Auxiliary members who have been elected or appointed to positions in the highest levels of the organization, similar in nature to active and reserve rear admirals and vice admirals use the term commodore (e.g., District Commodore, Assistant National Commodore, Deputy National Commodore, National Commodore, etc.). These Coast Guard Auxiliarists may permanently append the title commodore, sometimes abbreviated COMO, to their names (e.g., Commodore James A. Smith, National Commodore; or COMO Jim Smith, (NACO)).

In the Philippine Coast Guard Auxiliary—PCGA—each of the directors in command of the ten Coast Guard Auxiliary districts are commodores, as well as most of the Deputy National Directors (some may be rear admirals). Commodore is appreviated to COMMO in the PCGA.

Vanderbilt University's intercollegiate athletics teams are nicknamed the "Commodores", a reference to Cornelius Vanderbilt's self-appointed title (he was the master of a large shipping fleet).

In the U.S. Sea Scouting program (which is part of the Boy Scouts of America), all National, Regional, Area, and Council committee chairs are titled as commodore, while senior committee members are addressed as vice commodore. Ship committee chairs do not hold this recognition.




</doc>
<doc id="7583" url="https://en.wikipedia.org/wiki?curid=7583" title="Cauchy–Riemann equations">
Cauchy–Riemann equations

In the field of complex analysis in mathematics, the Cauchy–Riemann equations, named after Augustin Cauchy and Bernhard Riemann, consist of a system of two partial differential equations which, together with certain continuity and differentiability criteria, form a necessary and sufficient condition for a complex function to be complex differentiable, that is, holomorphic. This system of equations first appeared in the work of Jean le Rond d'Alembert . Later, Leonhard Euler connected this system to the analytic functions . then used these equations to construct his theory of functions. Riemann's dissertation on the theory of functions appeared in 1851.

The Cauchy–Riemann equations on a pair of real-valued functions of two real variables "u"("x","y") and "v"("x","y") are the two equations:

Typically "u" and "v" are taken to be the real and imaginary parts respectively of a complex-valued function of a single complex variable , . Suppose that "u" and "v" are real-differentiable at a point in an open subset of C (C is the set of complex numbers), which can be considered as functions from R to R. This implies that the partial derivatives of "u" and "v" exist (although they need not be continuous) and we can approximate small variations of "f" linearly. Then is complex-differentiable at that point if and only if the partial derivatives of "u" and "v" satisfy the Cauchy–Riemann equations (1a) and (1b) at that point. The sole existence of partial derivatives satisfying the Cauchy–Riemann equations is not enough to ensure complex differentiability at that point. It is necessary that u and v be real differentiable, which is a stronger condition than the existence of the partial derivatives, but in general, weaker than continuous differentiability.

Holomorphy is the property of a complex function of being differentiable at every point of an open and connected subset of C (this is called a domain in C). Consequently, we can assert that a complex function "f", whose real and imaginary parts "u" and "v" are real-differentiable functions, is holomorphic if and only if, equations (1a) and (1b) are satisfied throughout the domain we are dealing with. Holomorphic functions are analytic and vice versa. This means that, in complex analysis, a function that is complex-differentiable in a whole domain (holomorphic) is the same as an analytic function. This is not true for real differentiable functions.

Suppose that "z" = "x" + "iy". The complex-valued function "f"("z") = "z" is differentiable at any point "z" in the complex plane. 
The real part "u"("x", "y") and the imaginary part "v"("x", "y") of "f"("z") are
respectively. The partial derivatives of these are 
These partial derivatives have the following relationships: 
Thus this complex-valued function "f"("z") satisfies the Cauchy–Riemann equations.

The equations are one way of looking at the condition on a function to be differentiable in the sense of complex analysis: in other words they encapsulate the notion of function of a complex variable by means of conventional differential calculus. In the theory there are several other major ways of looking at this notion, and the translation of the condition into other language is often needed.

First, the Cauchy–Riemann equations may be written in complex form

In this form, the equations correspond structurally to the condition that the Jacobian matrix is of the form

where formula_14 and formula_15. A matrix of this form is the matrix representation of a complex number. Geometrically, such a matrix is always the composition of a rotation with a scaling, and in particular preserves angles. The Jacobian of a function "f"("z") takes infinitesimal line segments at the intersection of two curves in z and rotates them to the corresponding segments in "f"("z"). Consequently, a function satisfying the Cauchy–Riemann equations, with a nonzero derivative, preserves the angle between curves in the plane. That is, the Cauchy–Riemann equations are the conditions for a function to be conformal.

Moreover, because the composition of a conformal transformation with another conformal transformation is also conformal, the composition of a solution of the Cauchy–Riemann equations with a conformal map must itself solve the Cauchy–Riemann equations. Thus the Cauchy–Riemann equations are conformally invariant.

Suppose that

is a function of a complex number "z". Then the complex derivative of "f" at a point "z" is defined by

provided this limit exists.

If this limit exists, then it may be computed by taking the limit as "h" → 0 along the real axis or imaginary axis; in either case it should give the same result. Approaching along the real axis, one finds

On the other hand, approaching along the imaginary axis,

The equality of the derivative of "f" taken along the two axis is

which are the Cauchy–Riemann equations (2) at the point "z".

Conversely, if "f" : C → C is a function which is differentiable when regarded as a function on R, then "f" is complex differentiable if and only if the Cauchy–Riemann equations hold. In other words, if u and v are real-differentiable functions of two real variables, obviously "u" + "iv" is a (complex-valued) real-differentiable function, but "u" + "iv" is complex-differentiable if and only if the Cauchy–Riemann equations hold.

Indeed, following , suppose "f" is a complex function defined in an open set Ω ⊂ C. Then, writing for every "z" ∈ Ω, one can also regard Ω as an open subset of R, and "f" as a function of two real variables "x" and "y", which maps Ω ⊂ R to C. We consider the Cauchy–Riemann equations at "z" = "z". So assume "f" is differentiable at "z", as a function of two real variables from Ω to C. This is equivalent to the existence of the following linear approximation

where "z" = "x" + "iy" and "η"(Δ"z") → 0 as Δ"z" → 0. Since formula_22 and formula_23, the above can be re-written as

Defining the two Wirtinger derivatives as

in the limit formula_26 the above equality can be written as

Now consider the potential values of formula_28 when the limit is taken the origin. For "z" along the real line, formula_29 so that formula_30. Similarly for purely imaginary "z" we have formula_31 so that the value of formula_32 is not well defined at the origin. It's easy to verify that formula_32 is not well defined at any complex "z", hence "f" is complex differentiable at "z" if and only if formula_34 at formula_35. But this is exactly the Cauchy–Riemann equations, thus "f" is differentiable at "z" if and only if the Cauchy–Riemann equations hold at "z".

The above proof suggests another interpretation of the Cauchy–Riemann equations. The complex conjugate of "z", denoted formula_36, is defined by

for real "x" and "y". The Cauchy–Riemann equations can then be written as a single equation

by using the Wirtinger derivative with respect to the conjugate variable. In this form, the Cauchy–Riemann equations can be interpreted as the statement that "f" is independent of the variable formula_36. As such, we can view analytic functions as true functions of "one" complex variable as opposed to complex functions of "two" real variables.

A standard physical interpretation of the Cauchy–Riemann equations going back to Riemann's work on function theory (see ) is that "u" represents a velocity potential of an incompressible steady fluid flow in the plane, and "v" is its stream function. Suppose that the pair of (twice continuously differentiable) functions formula_40 satisfies the Cauchy–Riemann equations. We will take "u" to be a velocity potential, meaning that we imagine a flow of fluid in the plane such that the velocity vector of the fluid at each point of the plane is equal to the gradient of "u", defined by

By differentiating the Cauchy–Riemann equations a second time, one shows that "u" solves Laplace's equation:
That is, "u" is a harmonic function. This means that the divergence of the gradient is zero, and so the fluid is incompressible.

The function "v" also satisfies the Laplace equation, by a similar analysis. Also, the Cauchy–Riemann equations imply that the dot product formula_43. This implies that the gradient of "u" must point along the formula_44 curves; so these are the streamlines of the flow. The formula_45 curves are the equipotential curves of the flow.

A holomorphic function can therefore be visualized by plotting the two families of level curves formula_45 and formula_44. Near points where the gradient of "u" (or, equivalently, "v") is not zero, these families form an orthogonal family of curves. At the points where formula_48, the stationary points of the flow, the equipotential curves of formula_45 intersect. The streamlines also intersect at the same point, bisecting the angles formed by the equipotential curves.

Another interpretation of the Cauchy–Riemann equations can be found in . Suppose that "u" and "v" satisfy the Cauchy–Riemann equations in an open subset of R, and consider the vector field

regarded as a (real) two-component vector. Then the second Cauchy–Riemann equation (1b) asserts that formula_51 is irrotational (its curl is 0):

The first Cauchy–Riemann equation (1a) asserts that the vector field is solenoidal (or divergence-free):

Owing respectively to Green's theorem and the divergence theorem, such a field is necessarily a conservative one, and it is free from sources or sinks, having net flux equal to zero through any open domain without holes. (These two observations combine as real and imaginary parts in Cauchy's integral theorem.) In fluid dynamics, such a vector field is a potential flow . In magnetostatics, such vector fields model static magnetic fields on a region of the plane containing no current. In electrostatics, they model static electric fields in a region of the plane containing no electric charge.

This interpretation can equivalently be restated in the language of differential forms. The pair "u","v" satisfy the Cauchy–Riemann equations if and only if the one-form formula_54 is both closed and coclosed (a harmonic differential form).

Another formulation of the Cauchy–Riemann equations involves the complex structure in the plane, given by
This is a complex structure in the sense that the square of "J" is the negative of the 2×2 identity matrix: formula_56. As above, if "u"("x","y"),"v"("x","y") are two functions in the plane, put

The Jacobian matrix of "f" is the matrix of partial derivatives

Then the pair of functions "u", "v" satisfies the Cauchy–Riemann equations if and only if the 2×2 matrix "Df" commutes with "J" 

This interpretation is useful in symplectic geometry, where it is the starting point for the study of pseudoholomorphic curves.

Other representations of the Cauchy–Riemann equations occasionally arise in other coordinate systems. If (1a) and (1b) hold for a differentiable pair of functions "u" and "v", then so do

for any coordinate system such that the pair (∇"n", ∇"s") is orthonormal and positively oriented. As a consequence, in particular, in the system of coordinates given by the polar representation , the equations then take the form

Combining these into one equation for "f" gives

The inhomogeneous Cauchy–Riemann equations consist of the two equations for a pair of unknown functions "u"("x","y") and "v"("x","y") of two real variables

for some given functions α("x","y") and β("x","y") defined in an open subset of R. These equations are usually combined into a single equation

where "f" = "u" + i"v" and "φ" = ("α" + i"β")/2.

If "φ" is "C", then the inhomogeneous equation is explicitly solvable in any bounded domain "D", provided "φ" is continuous on the closure of "D". Indeed, by the Cauchy integral formula,

for all ζ ∈ "D".

Suppose that is a complex-valued function which is differentiable as a function . Then Goursat's theorem asserts that "f" is analytic in an open complex domain Ω if and only if it satisfies the Cauchy–Riemann equation in the domain . In particular, continuous differentiability of "f" need not be assumed .

The hypotheses of Goursat's theorem can be weakened significantly. If is continuous in an open set Ω and the partial derivatives of "f" with respect to "x" and "y" exist in Ω, and satisfy the Cauchy–Riemann equations throughout Ω, then "f" is holomorphic (and thus analytic). This result is the Looman–Menchoff theorem.

The hypothesis that "f" obey the Cauchy–Riemann equations throughout the domain Ω is essential. It is possible to construct a continuous function satisfying the Cauchy–Riemann equations at a point, but which is not analytic at the point (e.g., "f"("z") = . Similarly, some additional assumption is needed besides the Cauchy–Riemann equations (such as continuity), as the following example illustrates 

which satisfies the Cauchy–Riemann equations everywhere, but fails to be continuous at "z" = 0.

Nevertheless, if a function satisfies the Cauchy–Riemann equations in an open set in a weak sense, then the function is analytic. More precisely :
This is in fact a special case of a more general result on the regularity of solutions of hypoelliptic partial differential equations.
There are Cauchy–Riemann equations, appropriately generalized, in the theory of several complex variables. They form a significant overdetermined system of PDEs. This is done using a straightforward generalization of the Wirtinger derivative, where the function in question is required to have the (partial) Wirtinger derivative with respect to each complex variable vanish.

As often formulated, the "d-bar operator" 

annihilates holomorphic functions. This generalizes most directly the formulation

where

Viewed as conjugate harmonic functions, the Cauchy–Riemann equations are a simple example of a Bäcklund transform. More complicated, generally non-linear Bäcklund transforms, such as in the sine-Gordon equation, are of great interest in the theory of solitons and integrable systems.

In Clifford algebra the complex number formula_69 is represented as formula_70 where formula_71. The fundamental derivative operator in Clifford algebra of Complex numbers is defined as formula_72. The function formula_73 is considered analytic if and only if formula_74, which can be calculated in following way:

Grouping by formula_76 and formula_77:

Henceforth in traditional notation:

Let Ω be an open set in the Euclidean space R. The equation for an orientation-preserving mapping formula_80 to be a conformal mapping (that is, angle-preserving) is that
where "Df" is the Jacobian matrix, with transpose formula_82, and "I" denotes the identity matrix . For , this system is equivalent to the standard Cauchy–Riemann equations of complex variables, and the solutions are holomorphic functions. In dimension , this is still sometimes called the Cauchy–Riemann system, and Liouville's theorem implies, under suitable smoothness assumptions, that any such mapping is a Möbius transformation.





</doc>
<doc id="7585" url="https://en.wikipedia.org/wiki?curid=7585" title="Chaim Topol">
Chaim Topol

Chaim Topol (, born September 9, 1935), also spelled Haym Topol, mononymously known as Topol, is an Israeli theatrical, film, and television actor, singer, comedian, voice artist, film producer, author, and illustrator. He is best known for his portrayal of Tevye the dairyman in the musical "Fiddler on the Roof" on stage and screen, a role he performed more than 3,500 times in shows and revivals from the late 1960s through 2009. 

Topol began his acting career during his Israeli army service in the Nahal entertainment troupe, and later toured Israel with kibbutz theatre and satirical theatre companies. He was a co-founder of the Haifa Theatre. His breakthrough film role came in 1964 as the title character in "Sallah Shabati", by Israeli writer Ephraim Kishon, for which he won a Golden Globe for Most Promising Newcomer—Male. He went on to appear in more than 30 films in Israel and the United States. In the 1960s through 1980s, he was Israel's "only internationally-recognized entertainer". He won a Golden Globe Award for Best Actor and was nominated for an Academy Award for Best Actor for his 1971 film portrayal of Tevye, and was nominated for a Tony Award for Best Actor for a 1991 Broadway revival of "Fiddler on the Roof".

He is a founder of Variety Israel, an organization serving children with special needs, and Jordan River Village, a year-round camp for Arab and Jewish children with life-threatening illnesses, for which he serves as chairman of the board. In 2015 he was awarded the Israel Prize for lifetime achievement.

Topol was born in Tel Aviv in 1935, in what was then Mandatory Palestine. His father, Jacob Topol, had immigrated to Mandatory Palestine from Russia in the early 1930s and worked as a plasterer; he also served in the Haganah paramilitary organization. His mother, Rel (née Goldman) Topol, was a seamstress. Though the young Chaim wanted to become a commercial artist, his elementary school teachers saw a theatrical side to him, and encouraged him to act in school plays and read stories to the class.

At age 14 he began working as a printer at the "Davar" newspaper while pursuing his high school studies at night. He graduated high school at age 17 and moved to Kibbutz Geva. A year later, he enlisted in the Israeli army and became a member of the Nahal entertainment troupe, singing and acting in traveling shows. He rose in rank to troupe commander.

Twenty-three days after being discharged from military service on October 2, 1956, and two days after marrying Galia Finkelstein, a fellow Nahal troupe member, Topol was called up to serve in the Sinai Campaign. He performed for soldiers stationed in the desert. After the war, he and his wife settled in Kibbutz Mishmar David, where Topol worked as a garage mechanic. Topol assembled a kibbutz theatre company made up of friends from his Nahal troupe; the group toured four days a week, worked on their respective kibbutzim for two days a week, and had one day off. The theatre company was in existence from early 1957 to the mid-1960s. Topol both sang and acted with the group, doing both "loudly".

Between 1960 and 1964, Topol performed with the "Batzal Yarok" ("Green Onion") satirical theatre company, which also toured Israel. Other members of the group included Uri Zohar, Nechama Hendel, Zaharira Harifai, Arik Einstein, and Oded Kotler. In 1960, Topol co-founded the Haifa Municipal Theatre with Yosef Milo, serving as assistant to the director and acting in plays by Shakespeare, Ionesco, and Brecht. In 1965 he performed in the Cameri Theatre in Tel Aviv.

Topol's first film appearance was in the 1961 film "I Like Mike", followed by the 1962 Israeli film "El Dorado". His breakthrough role came as the lead character in the 1964 film "Sallah Shabati". Adapted for the screen by Ephraim Kishon from his original play, the social satire depicts the hardships of a Mizrahi Jewish immigrant family in Israel in the 1950s, satirizing "just about every pillar of Israeli society: the Ashkenazi establishment, the pedantic bureaucracy, corrupt political parties, rigid kibbutz ideologues and ... the Jewish National Fund's tree-planting program". Topol, who was 29 during the filming, was familiar playing the role of the family patriarch, having performed skits from the play with his Nahal entertainment troupe during his army years. He contributed his own ideas for the part, playing the character as a more universal Sephardi Jew instead of specifically a Yemenite, Iraqi, or Moroccan Jew, and asking Kishon to change the character's first name from Saadia (a recognizably Yemenite name) to Sallah (a more general Mizrahi name).

The film won the Golden Globe Award for Best Foreign Language Film, and Topol won the 1964 Golden Gate Award for Best Actor at the San Francisco International Film Festival and the 1965 Golden Globe for Most Promising Newcomer—Male. "Sallah Shabati" was nominated for the Academy Award for Best Foreign Language Film, losing to the Italian-language "Yesterday, Today and Tomorrow".

In 1966, Topol made his English-language film debut as Abou Ibn Kaqden in the Mickey Marcus biopic "Cast a Giant Shadow".

Topol came to greatest prominence in his portrayal of Tevye the dairyman on stage and screen. He first played the role in 1966 in the Israeli production of "Fiddler on the Roof", replacing Shmuel Rodensky for 10 weeks when the lead actor fell ill. Harold Prince, producer of the original "Fiddler on the Roof" that opened on Broadway in 1964, had seen Topol in "Sallah Shabati" and called him to audition for the role of the fifty-something Tevye in a new production scheduled to open at Her Majesty's Theatre in London on February 16, 1967. Not yet fluent in English, Topol memorized the score from the Broadway cast album and was tutored in the lyrics by an Englishwoman. When Topol arrived at the audition, Prince was flabbergasted that this 30-year-old man had played Shabati, a character in his sixties. Topol explained, "A good actor can play an old man, a sad face, a happy man. Makeup is not an obstacle". Topol also surprised the producers with his familiarity with the staging, since he had acted in the Israeli production, and was hired. He spent six months in London learning his part phonetically with vocal coach Cicely Berry. Jerome Robbins, director and choreographer of the 1964 Broadway show who came over to direct the London production, "re-directed" the character of Tevye for Topol and helped the actor deliver a less caricatured performance. 

Topol's performance received positive reviews. A few months after the opening, he returned to Israel to serve in the army during the Six-Day War in June. He was assigned to an army entertainment troupe on the Golan Heights. All told, he appeared in 430 performances of the London production, which ran for a total of 2,030 performances.

It was during the London run that he began being called by his last name only, as the British producers were unable to pronounce the voiceless uvular fricative consonant Ḥet at the beginning of his first name, Chaim, instead calling him "Shame".
In casting the 1971 film version of "Fiddler on the Roof", director Norman Jewison and his production team sought an actor other than Zero Mostel for the lead role. This decision was a controversial one, as Mostel had made the role famous in the long-running Broadway musical and wanted to star in the film. But Jewison and his team felt Mostel would eclipse the character with his larger-than-life personality. Jewison flew to London in February 1968 to see Topol perform as Tevye during his last week with the London production, and chose him over Danny Kaye, Herschel Bernardi, Rod Steiger, Danny Thomas, Walter Matthau, Richard Burton, and Frank Sinatra, who had also expressed interest in the part.

Then 36 years old, Topol was made to look 20 years older and heavier with makeup and costuming. As in his role as Shabati, Topol used the technique of "locking his muscles" to convincingly play an older character. He later explained:
As a young man, I had to make sure that I didn't break the illusion for the audience. You have to tame yourself. I'm now someone who is supposed to be 50, 60 years old. I cannot jump. I cannot suddenly be young. You produce a certain sound [in your voice] that is not young.

For his performance, Topol won the Golden Globe Award for Best Actor in a Motion Picture – Musical or Comedy and the 1972 David di Donatello for Best Foreign Actor, sharing the latter with Elizabeth Taylor. He was nominated for the 1971 Academy Award for Best Actor, losing to Gene Hackman in "The French Connection".

In 1983 Topol reprised the role of Tevye in a revival of "Fiddler on the Roof" in West End theatre. In 1989, he played the role in a 30-city U.S. touring production. As he was by then the approximate age of the character, he commented, "I didn't have to spend the energy playing the age". In 1990–1991, he again starred as Tevye in a Broadway revival of "Fiddler" at the Gershwin Theatre. In 1991, he was nominated for a Tony Award for Best Performance by a Leading Actor in a Musical, losing to Jonathan Pryce in "Miss Saigon". Topol again played Tevye in a 1994 London revival, which became a touring production. In that production, his youngest daughter, Adi Topol Margalith, played one of his daughters.

Topol reprised the role of Tevye for a 1997–1998 touring production in Israel, as well as a 1998 show at the Regent Theatre in Melbourne. In September 2005 he returned to Australia for a "Fiddler on the Roof" revival at the Capitol Theatre in Sydney, followed by an April 2006 production at the Lyric Theatre in Brisbane and a June 2006 production at Her Majesty's Theatre in Melbourne. In May 2007, he starred in a production in the Auckland Civic Theatre.

On January 20, 2009, Topol began a farewell tour of "Fiddler on the Roof" as Tevye, opening in Wilmington, Delaware. He was forced to withdraw from the tour in Boston owing to a shoulder injury, and was replaced by Theodore Bikel and Harvey Fierstein, both of whom had portrayed Tevye on Broadway. Topol estimated that he performed the role more than 3,500 times.

In 1976, Topol originated the leading role of the baker, Amiable, in the new musical "The Baker's Wife", but was fired after eight months by producer David Merrick. In her autobiography, Patti LuPone, his co-star in the production, claimed that Topol had behaved unprofessionally on stage. The show's composer, Stephen Schwartz, claimed that Topol's behavior greatly disturbed the cast and directors and resulted in the production not reaching Broadway as planned. In 1988, Topol starred in the title role in "Ziegfield" at the London Palladium. He returned to the London stage in 2008 in the role of Honoré, from Maurice Chevalier's 1958 film "Gigi".

Topol appeared in more than 30 films in Israel and abroad. Among his notable English-language appearances are the title role in "Galileo" (1975), directed by Joseph Losey; Dr. Hans Zarkov in "Flash Gordon" (1980); and Milos Columbo in the James Bond film "For Your Eyes Only" (1981).

In Israel, Topol acted in and produced dozens of films and television series. As a voice artist, he dubbed the Hebrew-language versions of "The Jungle Book" and two films in the "Harry Potter" film series. He is also a playwright and screenwriter.
He was featured on two BBC One programs, the 6-part series "Topol's Israel" (1985) and earlier "It's Topol" (1968). A Hebrew-language documentary of his life, "Chaim Topol – Life as a Film", aired on Israel's Channel 1 in 2011, featuring interviews with his longtime actor friends in Israel and abroad.

A baritone, Topol recorded several singles and albums, including film soundtracks, children's songs, and Israeli war songs.

His autobiography, "Topol by Topol", was published in London by Weindenfel and Nicholson (1981). He also authored "To Life!" (1994) and "Topol's Treasure of Jewish Humor, Wit and Wisdom" (1995).

Topol has illustrated approximately 25 books in both Hebrew and English. He has also produced drawings of Israeli national figures. His sketches of Israeli presidents were reproduced in a 2013 stamp series issued by the Israel Philatelic Federation, as was his self-portrait as Tevye for a 2014 commemorative stamp marking the 50th anniversary of the Broadway debut of "Fiddler on the Roof".

In 1967, Topol founded Variety Israel, an organization serving children with special needs. He is also a co-founder and chairman of the board of Jordan River Village, a year-round camp for Arab and Jewish children with life-threatening illnesses, which opened in 2012.

At an October 1963 awards ceremony, Topol was a recipient of Israel's David's Harp award in arts and entertainment. He received a Best Actor award from the San Sebastián International Film Festival for his performance in the 1972 film "Follow Me!" In 2008, he was named an Outstanding Member of the Israel Festival for his contribution to Israeli culture. In 2014, the University of Haifa conferred upon Topol an honorary degree in recognition of his 50 years of activity in Israel's cultural and public life. In 2015, he received the Israel Prize for lifetime achievement.

Topol married Galia Finkelstein in October 1956. They have one son and two daughters. The couple resides in Galia's childhood home in Tel Aviv. Topol's hobbies include sketching and sculpting.




</doc>
<doc id="7586" url="https://en.wikipedia.org/wiki?curid=7586" title="Christadelphians">
Christadelphians

The Christadelphians () are a millenarian Christian group who hold a view of Biblical Unitarianism. There are approximately 50,000 Christadelphians in around 120 countries. The movement developed in the United Kingdom and North America in the 19th century around the teachings of John Thomas, who coined the name "Christadelphian" from the Greek for "Brethren in Christ".

Claiming to base their beliefs solely on the Bible, Christadelphians differ from mainstream Christianity in a number of doctrinal areas. For example, they reject the Trinity and the immortality of the soul, believing these to be corruptions of original Christian teaching. They were initially found predominantly in the developed English-speaking world, but expanded in developing countries after the Second World War. Congregations are traditionally referred to as 'ecclesias' and would not use the word 'church' due to its association with mainstream Christianity, although today it is more acceptable.

The Christadelphian religious group traces its origins to John Thomas (1805–1871), who emigrated to North America from England in 1832. Following a near shipwreck he vowed to find out the truth about life and God through personal Biblical study. Initially he sought to avoid the kind of sectarianism he had seen in England. In this he found sympathy with the rapidly emerging Restoration Movement in the US at the time. This movement sought a reform based upon the Bible alone as a sufficient guide and rejected all creeds. However, this liberality eventually led to dissent as John Thomas developed his personal beliefs and began to question mainstream orthodox Christian beliefs. Whilst the Restoration Movement accepted Thomas's right to have his own beliefs, when he started preaching that they were essential to salvation, it led to a fierce series of debates with a notable leader of the movement, Alexander Campbell. John Thomas believed that scripture, as God's word, did not support a multiplicity of differing beliefs, and challenged the leaders to continue with the process of restoring 1st-century Christian beliefs and correct interpretation through a process of debate. The history of this process appears in the book "Dr. Thomas, His Life and Work" (1873) by a Christadelphian, Robert Roberts.

During this period of formulating his ideas John Thomas was baptised twice, the second time after renouncing the beliefs he previously held. He based his new position on a new appreciation for the reign of Christ on David's throne. The abjuration of his former beliefs eventually led to the Restoration Movement disfellowshipping him when he toured England and they became aware of his abjuration in the United States of America.

The Christadelphian community in Britain effectively dates from Thomas's first lecturing tour (May 1848 – October 1850). His message was particularly welcomed in Scotland, and Campbellite, Unitarian and Adventist friends separated to form groups of "Baptised Believers". Two thirds of ecclesias, and members, in Britain before 1864 were in Scotland. In 1849, during his tour of Britain, he completed (a decade and a half before the name "Christadelphian" was conceived) "Elpis Israel" in which he laid out his understanding of the main doctrines of the Bible. Since his medium for bringing change was print and debate, it was natural for the origins of the Christadelphian body to be associated with books and journals, such as Thomas's "Herald of the Kingdom".

In his desire to seek to establish Biblical truth and test orthodox Christian beliefs through independent scriptural study he was not alone. Among other churches, he had links with Adventist movement and with Benjamin Wilson (who later set up the Church of God of the Abrahamic Faith in the 1860s). In terms of his rejection of the trinity, Thomas' views had certain similarities with Unitarianism which had developed in a formal way in Europe in the 16th century (although he formally described both Unitarianism and Socinianism as "works of the devil" for their failure to develop his doctrine of God-manifestation). See History of Unitarianism

Although the Christadelphian movement originated through the activities of John Thomas, he never saw himself as making his own disciples. He believed rather that he had rediscovered 1st century beliefs from the Bible alone, and sought to prove that through a process of challenge and debate and writing journals. Through that process a number of people became convinced and set up various fellowships that had sympathy with that position. Groups associated with John Thomas met under various names, including Believers, Baptised Believers, the Royal Association of Believers, Baptised Believers in the Kingdom of God, Nazarines (or Nazarenes) and The Antipas until the time of the American Civil War (1861–1865). At that time, church affiliation was required in the United States and in the Confederacy in order to register for conscientious objector status, and in 1864 Thomas chose for registration purposes the name "Christadelphian".

Through the teaching of John Thomas and the need in the American Civil War for a name, the Christadelphians emerged as a denomination, but they were formed into a lasting structure through a passionate follower of Thomas's interpretation of the Bible, Robert Roberts. In 1864 he began to publish "The Ambassador of the Coming Age" magazine. John Thomas, out of concern that someone else might start a publication and call it "The Christadelphian", urged Robert Roberts to change the name of his magazine to "The Christadelphian". which he did in 1869. His editorship of the magazine continued with some assistance until his death in 1898.Roberts was prominent in the period following the death of John Thomas in 1871, and helped craft the structures of the Christadelphian body.

Initially the denomination grew in the English-speaking world, particularly in the English Midlands and in parts of North America. In the early days after the death of John Thomas the group could have moved in a number of directions. Doctrinal issues arose, debates took place and statements of faith were created and amended as other issues arose. These attempts were felt necessary by many to both settle and define a doctrinal stance for the newly emerging denomination and to keep out error. As a result of these debates, several groups separated from the main body of Christadelphians, most notably the Suffolk Street fellowship and the Unamended fellowship.

The Christadelphian position on conscientious objection came to the fore with the introduction of conscription during the First World War. Varying degrees of exemption from military service were granted to Christadelphians in the United Kingdom, Canada, Australia, New Zealand and the United States. In the Second World War, this frequently required the person seeking exemption to undertake civilian work under the direction of the authorities.

During the Second World War the Christadelphians in Britain assisted in the Kindertransport, helping to relocate several hundred Jewish children away from Nazi persecution and founding a hostel Elpis Lodge. In Germany the small Christadelphian community founded by Albert Maier went underground from 1940–1945, and a leading brother, Albert Merz, was imprisoned as a conscientious objector and later executed.

After the Second World War, moves were taken to try to reunite various of the earlier divisions. By the end of the 1950s, most Christadelphians had united into one community, but there are still a number of small groups of Christadelphians who remain separate.

The post-war, and post-reunions, period saw an increase in co-operation and interaction between ecclesias, resulting in the establishment of a number of week-long Bible schools and the formation of national and international organisations such as the Christadelphian Bible Mission (for preaching and pastoral support overseas), the Christadelphian Support Network (for counselling), and the Christadelphian Meal-A-Day Fund (for charity and humanitarian work).

The period following the reunions was accompanied by expansion in the developing world, which now accounts for around 40% of Christadelphians.

In the absence of centralised organization, some differences exist amongst Christadelphians on matters of belief and practice. This is because each congregation (commonly styled 'ecclesias') is organized autonomously, typically following common practices which have altered little since the 19th century. Most ecclesias have a constitution, which includes a 'Statement of Faith', a list of 'Doctrines to be Rejected' and a formalized list of 'The Commandments of Christ'. With no central authority, individual congregations are responsible for maintaining orthodoxy in belief and practice, and the statement of faith is seen by many as useful to this end. The statement of faith acts as the official standard of most ecclesias to determine fellowship within and between ecclesias, and as the basis for co-operation between ecclesias. Congregational discipline and conflict resolution are applied using various forms of consultation, mediation, and discussion, with disfellowship (similar to excommunication) being the final response to those with unorthodox practices or beliefs.

The relative uniformity of organization and practice is undoubtedly due to the influence of a booklet, written early in Christadelphian history by Robert Roberts, called "A Guide to the Formation and Conduct of Christadelphian Ecclesias". It recommends a basically democratic arrangement by which congregational members elect 'brothers' to arranging and serving duties, and includes guidelines for the organization of committees, as well as conflict resolution between congregational members and between congregations. Christadelphians do not have paid ministers. Male members are assessed by the congregation for their eligibility to teach and perform other duties, which are usually assigned on a rotation basis, as opposed to having a permanently appointed preacher. Congregational governance typically follows a democratic model, with an elected arranging committee for each individual ecclesia. This unpaid committee is responsible for the day-to-day running of the ecclesia and is answerable to the rest of the ecclesia's members.

Inter-ecclesial organizations co-ordinate the running of, among other things, Christadelphian schools and elderly care homes, the Christadelphian Isolation League (which cares for those prevented by distance or infirmity from attending an ecclesia regularly) and the publication of .

No official membership figures are published, but the "Columbia Encyclopedia" gives an estimated figure of 50,000 Christadelphians. They are spread across approximately 120 countries; there are established churches (often referred to as "ecclesias") in many of those countries, along with isolated members. Estimates for the main centers of Christadelphian population are as follows: United Kingdom (18,000), Australia (10,653), Mozambique (9,400), Malawi (7,000), United States (6,500), Canada (3,375), Kenya (2,700), New Zealand (1,785), India (1,750), Tanzania (1,000). and Pakistan (900). Combining the estimates from the Christadelphian Bible Mission with the figures above, the numbers for each continent are as follows: Africa (22,100), Americas (10,500), Asia (4,150), Australasia (12,600), Europe (18,950). This puts the total figure at around 67,000.

The Christadelphian body consists of a number of "fellowships" – groups of ecclesias which associate with one another, often to the exclusion of ecclesias outside their group. They are to some degree localised. The Unamended Fellowship, for example, exists only in North America. Christadelphian fellowships have often been named after ecclesias or magazines who took a lead in developing a particular stance.

The majority of Christadelphians today (around 60,000) belong to what is commonly known as the "Central fellowship". The term "Central" came into use around 1933 to distinguish those majority ecclesias worldwide who were in fellowship with the Birmingham (Central) Ecclesia, from those minority ecclesias of the "Suffolk Street fellowship" who had been collectively withdrawn from in 1885 over disagreements surrounding the inspiration of the Bible. This had begun when Robert Ashcroft, a leading member, wrote an article which challenged Christadelphian belief, stating that some parts of the Holy Scriptures were with error and therefore uninspired. Although he later left the community, this led to a division in the main body where his doctrinal influence remained. Robert Ashcroft's sympathisers in Birmingham, England formed a separate Ecclesia in local Suffolk Street and ecclesias which supported their position became known as the "Suffolk Street fellowship". Those in local Birmingham, who maintained that the Holy Scriptures were divinely inspired in all their parts and were without error except as may be due to errors of transcription or translation, formed a separate Ecclesia in nearby Temperance Hall and ecclesias throughout the world which supported their position became known as the "Temperance Hall Fellowship". Meanwhile in Australia, division had ensued surrounding the subject of the "Clean Flesh" of Jesus Christ. The minority who believed Christ's nature was immaculate and therefore his flesh did not have the potential to sin formed the "Shield Fellowship". These soon became closely associated with the "Suffolk Street Fellowship" as their sympathisers, and also those known as the Dowieite community who had been previously disfellowshipped from the main body over disagreement surrounding the Biblical devil and evil spirits. The majority who believed Christ was born with the same flesh as his Brethren and thus had the ability to sin in his flesh but chose not to continued as members of the "Temperance Hall Fellowship". By 1939 the "Temperance Hall" members in Birmingham had relocated and were increasingly known as the "Birmingham Central Christadelphians", but before long the word "Birmingham" was dropped and the term "Central Fellowship" began to be used with some regularity among the next generations of Christadelphians in the UK. After the First and Second World Wars it was felt that a gradual relaxation of traditional fellowship practice needed to take place and the discussions of 1957-1958 resulted in a worldwide reunion between the majority Christadelphians of the "Temperance Hall Fellowship" and the minority "Suffolk Street Fellowship", closely followed in Australia by the minority "Shield Fellowship" and the Dowieite community. Birmingham Central Hall Christadelphians, where the formal meetings and unification papers were signed officially decreed the reunion as official, and the "Central Fellowship" was born. Reunion was also made official to the rest of the worldwide Christadelphians on the basis of the understanding of the atonement of Christ, expressed in a document called the Cooper-Carter Addendum (this was soon added to the BASF). 

The "Unamended fellowship", consisting of around 1,850 members, is found in the East Coast and Midwest USA and Ontario, Canada. This group separated in 1898 as a result of differing views on who would be raised to judgment at the return of Christ. The majority of Christadelphians believe that the judgment will include anyone who had sufficient knowledge of the gospel message, and is not limited to baptized believers. The majority in Britain, Australia and North America amended their statement of faith accordingly. Those who opposed the amendment became known as the "Unamended fellowship" and allowed the teaching that God either could not or would not raise those who had no covenant relationship with him. Opinions vary as to what the established position was on this subject prior to the controversy. Prominent in the formation of the Unamended fellowship was Thomas Williams, editor of the Christadelphian Advocate magazine. The majority of the Unamended Fellowship outside North America joined the Suffolk Street fellowship before its eventual incorporation into Central fellowship. There is also some co-operation between the Central (Amended) and Unamended Fellowships in North America – most recently in the Great Lakes region, where numerous Amended and Unamended ecclesias have opened fellowship to one another despite the failure of wider attempts at re-union under the North American Statement of Understanding (NASU). The "Central Fellowship" in North America is still often referred to today as the "Amended fellowship".

The "Berean Fellowship" was formed in 1923 as a result of varying views on military service in Britain, and on the atonement in North America. The majority of the North American Bereans re-joined the main body of Christadelphians in 1952. A number continue as a separate community, numbering around 200 in Texas, 100 in Kenya and 30 in Wales. Most of the divisions still in existence within the Christadelphian community today stem from further divisions of the Berean fellowship.

The "Dawn fellowship". are the result of an issue which arose in 1942 among the Berean fellowship regarding divorce and remarriage. The stricter party formed the Dawn Fellowship who, following re-union on the basis of unity of belief with the Lightstand fellowship in Australia in 2007 increased in number, . There are now thought to be around 800 members in the UK, Australia, Canada, India, Jamaica, Poland, the Philippines, Russia and Kenya.

The "Old Paths fellowship" was formed in 1957 by those in the "Temperance Hall Fellowship" who held that the reasons for separation from the "Suffolk Street fellowship" and its sympathising communities remained. They also strongly believed that the Biblical teaching of fellowship required full unity of belief on all fundamental principles of Bible Truth and thus the reunion should have been with the full agreement and understanding of all members rather than the result of the majority vote that prevailed. Ecclesias forming the Old Paths Fellowship arose in the UK, Australia, New Zealand and Canada numbering around 500 members in total. Due to a proportionally large number of members in the UK joining the Central Fellowship in and around 2014, their numbers have reduced to around 250 members in total (Around 100 members in the UK, and around 150 in Australasia). 

Other Fellowships which openly identify themselves as Christadelphians will have various numbers ranging from as few as 10 to over 200 members. Examples are the "Watchman Fellowship",<ref>


</doc>
<doc id="7587" url="https://en.wikipedia.org/wiki?curid=7587" title="Cable television">
Cable television

Cable television is a system of delivering television programming to paying subscribers via radio frequency (RF) signals transmitted through coaxial cables, or in more recent systems, light pulses through fiber-optic cables. This contrasts with broadcast television (also known as terrestrial television), in which the television signal is transmitted over the air by radio waves and received by a television antenna attached to the television; or satellite television, in which the television signal is transmitted by a communications satellite orbiting the Earth and received by a satellite dish on the roof. FM radio programming, high-speed Internet, telephone services, and similar non-television services may also be provided through these cables. Analog television was standard in the 20th century, but since the 2000s, cable systems have been upgraded to digital cable operation.

A "cable channel" (sometimes known as a "cable network") is a television network available via cable television. When available through satellite television, including direct broadcast satellite providers such as DirecTV, Dish Network and BSkyB, as well as via IPTV providers such as Verizon FIOS and AT&T U-verse is referred to as a "satellite channel". Alternative terms include "non-broadcast channel" or "programming service", the latter being mainly used in legal contexts. Examples of cable/satellite channels/cable networks available in many countries are HBO, MTV, Cartoon Network, E!, Eurosport and CNN International.

The abbreviation CATV is often used for cable television. It originally stood for "Community Access Television" or "Community Antenna Television", from cable television's origins in 1948. In areas where over-the-air TV reception was limited by distance from transmitters or mountainous terrain, large "community antennas" were constructed, and cable was run from them to individual homes. The origins of cable "broadcasting" for radio are even older as radio programming was distributed by cable in some European cities as far back as 1924.

To receive cable television at a given location, cable distribution lines must be available on the local utility poles or underground utility lines. Coaxial cable brings the signal to the customer's building through a "service drop", an overhead or underground cable. If the subscriber's building does not have a cable service drop, the cable company will install one. The standard cable used in the U.S. is RG-6, which has a 75 ohm impedance, and connects with a type F connector. The cable company's portion of the wiring usually ends at a distribution box on the building exterior, and built-in cable wiring in the walls usually distributes the signal to jacks in different rooms to which televisions are connected. Multiple cables to different rooms are split off the incoming cable with a small device called a splitter. There are two standards for cable television; older analog cable, and newer digital cable which can carry data signals used by digital television receivers such as HDTV equipment. All cable companies in the United States have switched to or are in the course of switching to digital cable television since it was first introduced in the late 1990s.

Most cable companies require a set-top box to view their cable channels, even on newer televisions with digital cable QAM tuners, because most digital cable channels are now encrypted, or "scrambled", to reduce cable service theft. A cable from the jack in the wall is attached to the input of the box, and an output cable from the box is attached to the television, usually the RF-IN or composite input on older TVs. Since the set-top box only decodes the single channel that is being watched, each television in the house requires a separate box. Some unencrypted channels, usually traditional over-the-air broadcast networks, can be displayed without a receiver box. The cable company will provide set top boxes based on the level of service a customer purchases, from basic set top boxes with a standard definition picture connected through the standard coaxial connection on the TV, to high-definition wireless DVR receivers connected via HDMI or component. Older analog television sets are "cable ready" and can receive the old analog cable without a set-top box. To receive digital cable channels on an analog television set, even unencrypted ones, requires a different type of box, a digital television adapter supplied by the cable company. A new distribution method that takes advantage of the low cost high quality DVB distribution to residential areas, uses TV gateways to convert the DVB-C, DVB-C2 stream to IP for distribution of TV over IP network in the home.

In the most common system, multiple television channels (as many as 500, although this varies depending on the provider's available channel capacity) are distributed to subscriber residences through a coaxial cable, which comes from a trunkline supported on utility poles originating at the cable company's local distribution facility, called the "headend". Many channels can be transmitted through one coaxial cable by a technique called frequency division multiplexing. At the headend, each television channel is translated to a different frequency. By giving each channel a different frequency "slot" on the cable, the separate television signals do not interfere with each other. At an outdoor cable box on the subscriber's residence the company's service drop cable is connected to cables distributing the signal to different rooms in the building. At each television, the subscriber's television or a set-top box provided by the cable company translates the desired channel back to its original frequency (baseband), and it is displayed onscreen. Due to widespread cable theft in earlier analog systems, the signals are typically encrypted on modern digital cable systems, and the set-top box must be activated by an activation code sent by the cable company before it will function, which is only sent after the subscriber signs up. If the subscriber fails to pay his bill, the cable company can send a signal to deactivate the subscriber's box, preventing reception.

There are also usually "upstream" channels on the cable to send data from the customer box to the cable headend, for advanced features such as requesting pay-per-view shows or movies, cable internet access, and cable telephone service. The "downstream" channels occupy a band of frequencies from approximately 50 MHz to 1 GHz, while the "upstream" channels occupy frequencies of 5 to 42 MHz. Subscribers pay with a monthly fee. Subscribers can choose from several levels of service, with "premium" packages including more channels but costing a higher rate. At the local headend, the feed signals from the individual television channels are received by dish antennas from communication satellites. Additional local channels, such as local broadcast television stations, educational channels from local colleges, and community access channels devoted to local governments (PEG channels) are usually included on the cable service. Commercial advertisements for local business are also inserted in the programming at the headend (the individual channels, which are distributed nationally, also have their own nationally oriented commercials).

Modern cable systems are large, with a single network and headend often serving an entire metropolitan area. Most systems use hybrid fiber-coaxial (HFC) distribution; this means the trunklines that carry the signal from the headend to local neighborhoods are optical fiber to provide greater bandwidth and also extra capacity for future expansion. At the headend, the radio frequency electrical signal carrying all the channels is modulated on a light beam and sent through the fiber. The fiber trunkline goes to several "distribution hubs", from which multiple fibers fan out to carry the signal to boxes called "optical nodes" in local communities. At the optical node, the light beam from the fiber is translated back to an electrical signal and carried by coaxial cable distribution lines on utility poles, from which cables branch out to a series of signal amplifiers and line extenders. These devices carry the signal to customers via passive RF devices called taps.

Cable television began in the United States as a commercial business in 1950, although there were small-scale systems by hobbyists in the 1940s.

The early systems simply received weak (broadcast) channels, amplified them, and sent them over unshielded wires to the subscribers, limited to a community or to adjacent communities. The receiving antenna would be higher than any individual subscriber could afford, thus bringing in stronger signals; in hilly or mountainous terrain it would be placed at a high elevation.

At the outset, cable systems only served smaller communities without television stations of their own, and which could not easily receive signals from stations in cities because of distance or hilly terrain. In Canada, however, communities with their own signals were fertile cable markets, as viewers wanted to receive American signals. Rarely, as in the college town of Alfred, New York, U.S. cable systems retransmitted Canadian channels.

Although early (VHF) television receivers could receive 12 channels (2-13), the maximum number of channels that could be broadcast in one city was 7: channels 2, 4, either 5 or 6, 7, 9, 11 and 13, as receivers at the time were unable to receive strong (local) signals on adjacent channels without distortion. (There were frequency gaps between 4 and 5, and between 6 and 7, which allowed both to be used in the same city.)

As equipment improved, all twelve channels could be utilized, except where a local VHF television station broadcast. Local broadcast channels were not usable for signals deemed to be priority, but technology allowed low-priority signals to be placed on such channels by synchronizing their blanking intervals. Similarly, a local VHF station could not be carried on its broadcast channel as the signals would arrive at the TV set slightly separated in time, causing "ghosting".

The bandwidth of the amplifiers also was limited, meaning frequencies over 250 MHz were difficult to transmit to distant portions of the coaxial network, and UHF channels could not be used at all. To expand beyond 12 channels, non-standard "midband" channels had to be used, located between the FM band and Channel 7, or "superband" beyond Channel 13 up to about 300 MHz; these channels initially were only accessible using separate tuner boxes that sent the chosen channel into the TV set on Channel 2, 3 or 4.

Before being added to the cable box itself, these midband channels were used for early incarnations of pay TV, e.g. The Z Channel (Los Angeles) and HBO but transmitted in the clear i.e. not scrambled as standard TV sets of the period could not pick up the signal nor could the average consumer `de-tune' the normal stations to be able to receive it.

Once tuners that could receive select mid-band and super-band channels began to be incorporated into standard television sets, broadcasters were forced to either install scrambling circuitry or move these signals further out of the range of reception for early cable-ready TVs and VCRs. However, once all 181 allocated cable channels had been incorporated, premium broadcasters were left with no choice but to scramble.

Unfortunately for pay-TV operators, the descrambling circuitry was often published in electronics hobby magazines such as "Popular Science" and "Popular Electronics" allowing anybody with anything more than a rudimentary knowledge of broadcast electronics to be able to build their own and receive the programming without cost.

Later, the cable operators began to carry FM radio stations, and encouraged subscribers to connect their FM stereo sets to cable. Before stereo and bilingual TV sound became common, Pay-TV channel sound was added to the FM stereo cable line-ups. About this time, operators expanded beyond the 12-channel dial to use the "midband" and "superband" VHF channels adjacent to the "high band" 7-13 of North American television frequencies. Some operators as in Cornwall, Ontario, used a dual distribution network with Channels 2-13 on each of the two cables.

During the 1980s, United States regulations not unlike public, educational, and government access (PEG) created the beginning of cable-originated live television programming. As cable penetration increased, numerous cable-only TV stations were launched, many with their own news bureaus that could provide more immediate and more localized content than that provided by the nearest network newscast.

Such stations may use similar on-air branding as that used by the nearby broadcast network affiliate, but the fact that these stations do not broadcast over the air and are not regulated by the FCC, their call signs are meaningless. These stations evolved partially into today's over-the-air digital subchannels, where a main broadcast TV station e.g. NBS 37* would – in the case of no local CNB or ABS station being available – rebroadcast the programming from a nearby affiliate but fill in with its own news and other community programming to suit its own locale. Many live local programs with local interests were subsequently created all over the United States in most major television markets in the early 1980s.

This evolved into today's many cable-only broadcasts of diverse programming, including cable-only produced television movies and miniseries. Cable specialty channels, starting with channels oriented to show movies and large sporting or performance events, diversified further, and "narrowcasting" became common. By the late 1980s, cable-only signals outnumbered broadcast signals on cable systems, some of which by this time had expanded beyond 35 channels. By the mid-1980s in Canada, cable operators were allowed by the regulators to enter into distribution contracts with cable networks on their own.

By the 1990s, tiers became common, with customers able to subscribe to different tiers to obtain different selections of additional channels above the basic selection. By subscribing to additional tiers, customers could get specialty channels, movie channels, and foreign channels. Large cable companies used addressable descramblers to limit access to premium channels for customers not subscribing to higher tiers, however the above magazines often published workarounds for that technology as well.

During the 1990s, the pressure to accommodate the growing array of offerings resulted in digital transmission that made more efficient use of the VHF signal capacity; fibre optics was common to carry signals into areas near the home, where coax could carry higher frequencies over the short remaining distance. Although for a time in the 1980s and 1990s, television receivers and VCRs were equipped to receive the mid-band and super-band channels. Due to the fact that the descrambling circuitry was for a time present in these tuners, depriving the cable operator of much of their revenue, such cable-ready tuners are rarely used now - requiring a return to the set-top boxes used from the 1970s onward.

The conversion to digital broadcasting has put all signals - broadcast and cable - into digital form, rendering analog cable television service mostly obsolete, functional in an ever-dwindling supply of select markets. Analog television sets are still accommodated, but their tuners are mostly obsolete, oftentimes dependent entirely on the set-top box.

Cable television is mostly available in North America, Europe, Australia and East Asia, and less so in South America and the Middle East. Cable television has had little success in Africa, as it is not cost-effective to lay cables in sparsely populated areas. So-called "wireless cable" or microwave-based systems are used instead.

Coaxial cables are capable of bi-directional carriage of signals as well as the transmission of large amounts of data. Cable television signals use only a portion of the bandwidth available over coaxial lines. This leaves plenty of space available for other digital services such as cable internet, cable telephony and wireless services, using both unlicensed and licensed spectrum. Broadband internet access is achieved over coaxial cable by using cable modems to convert the network data into a type of digital signal that can be transferred over coaxial cable. One problem with some cable systems is the older amplifiers placed along the cable routes are unidirectional thus in order to allow for uploading of data the customer would need to use an analog telephone modem to provide for the upstream connection. This limited the upstream speed to 31.2k and prevented the always-on convenience broadband internet typically provides. Many large cable systems have upgraded or are upgrading their equipment to allow for bi-directional signals, thus allowing for greater upload speed and always-on convenience, though these upgrades are expensive.

In North America, Australia and Europe, many cable operators have already introduced cable telephone service, which operates just like existing fixed line operators. This service involves installing a special telephone interface at the customer's premises that converts the analog signals from the customer's in-home wiring into a digital signal, which is then sent on the local loop (replacing the analog last mile, or plain old telephone service (POTS)) to the company's switching center, where it is connected to the public switched telephone network (PSTN). The biggest obstacle to cable telephone service is the need for nearly 100% reliable service for emergency calls. One of the standards available for digital cable telephony, PacketCable, seems to be the most promising and able to work with the quality of service (QOS) demands of traditional analog plain old telephone service (POTS) service. The biggest advantage to digital cable telephone service is similar to the advantage of digital cable, namely that data can be compressed, resulting in much less bandwidth used than a dedicated analog circuit-switched service. Other advantages include better voice quality and integration to a Voice over Internet Protocol (VoIP) network providing cheap or unlimited nationwide and international calling. In many cases, digital cable telephone service is separate from cable modem service being offered by many cable companies and does not rely on Internet Protocol (IP) traffic or the Internet.

Traditional cable television providers and traditional telecommunication companies increasingly compete in providing voice, video and data services to residences. The combination of television, telephone and Internet access is commonly called "triple play", regardless of whether CATV or telcos offer it.



</doc>
<doc id="7591" url="https://en.wikipedia.org/wiki?curid=7591" title="Cholera">
Cholera

Cholera is an infection of the small intestine by some strains of the bacterium "Vibrio cholerae". Symptoms may range from none, to mild, to severe. The classic symptom is large amounts of watery diarrhea that lasts a few days. Vomiting and muscle cramps may also occur. Diarrhea can be so severe that it leads within hours to severe dehydration and electrolyte imbalance. This may result in sunken eyes, cold skin, decreased skin elasticity, and wrinkling of the hands and feet. Dehydration can cause the skin to turn bluish. Symptoms start two hours to five days after exposure.
Cholera is caused by a number of types of "Vibrio cholerae", with some types producing more severe disease than others. It is spread mostly by unsafe water and unsafe food that has been contaminated with human feces containing the bacteria. Undercooked seafood is a common source. Humans are the only animal affected. Risk factors for the disease include poor sanitation, not enough clean drinking water, and poverty. There are concerns that rising sea levels will increase rates of disease. Cholera can be diagnosed by a stool test. A rapid dipstick test is available but is not as accurate.
Prevention methods against cholera include improved sanitation and access to clean water. Cholera vaccines that are given by mouth provide reasonable protection for about six months. They have the added benefit of protecting against another type of diarrhea caused by "E. coli". The primary treatment is oral rehydration therapy—the replacement of fluids with slightly sweet and salty solutions. Rice-based solutions are preferred. Zinc supplementation is useful in children. In severe cases, intravenous fluids, such as Ringer's lactate, may be required, and antibiotics may be beneficial. Testing to see which antibiotic the cholera is susceptible to can help guide the choice.
Cholera affects an estimated 3–5 million people worldwide and causes 28,800–130,000 deaths a year. Although it is classified as a pandemic as of 2010, it is rare in the developed world. Children are mostly affected. Cholera occurs as both outbreaks and chronically in certain areas. Areas with an ongoing risk of disease include Africa and south-east Asia. The risk of death among those affected is usually less than 5% but may be as high as 50%. No access to treatment results in a higher death rate. Descriptions of cholera are found as early as the 5th century BC in Sanskrit. The study of cholera in England by John Snow between 1849 and 1854 led to significant advances in the field of epidemiology. Seven large outbreaks have occurred over the last 200 years with millions of deaths.

The primary symptoms of cholera are profuse diarrhea and vomiting of clear fluid. These symptoms usually start suddenly, half a day to five days after ingestion of the bacteria. The diarrhea is frequently described as "rice water" in nature and may have a fishy odor. An untreated person with cholera may produce of diarrhea a day. Severe cholera, without treatment, kills about half of affected individuals. If the severe diarrhea is not treated, it can result in life-threatening dehydration and electrolyte imbalances. Estimates of the ratio of asymptomatic to symptomatic infections have ranged from 3 to 100. Cholera has been nicknamed the "blue death" because a person's skin may turn bluish-gray from extreme loss of fluids.

Fever is rare and should raise suspicion for secondary infection. Patients can be lethargic, and might have sunken eyes, dry mouth, cold clammy skin, or wrinkled hands and feet. Kussmaul breathing, a deep and labored breathing pattern, can occur because of acidosis from stool bicarbonate losses and lactic acidosis associated with poor perfusion. Blood pressure drops due to dehydration, peripheral pulse is rapid and thready, and urine output decreases with time. Muscle cramping and weakness, altered consciousness, seizures, or even coma due to electrolyte imbalances are common, especially in children.

Cholera has been found in two animal populations: shellfish and plankton.

Transmission is usually through the fecal-oral route of contaminated food or water caused by poor sanitation. Most cholera cases in developed countries are a result of transmission by food, while in the developing world it is more often water. Food transmission can occur when people harvest seafood such as oysters in waters infected with sewage, as "Vibrio cholerae" accumulates in planktonic crustaceans and the oysters eat the zooplankton.

People infected with cholera often have diarrhea, and disease transmission may occur if this highly liquid stool, colloquially referred to as "rice-water", contaminates water used by others. A single diarrheal event can cause a one-million fold increase in numbers of "V. cholerae" in the environment. The source of the contamination is typically other cholera sufferers when their untreated diarrheal discharge is allowed to get into waterways, groundwater or drinking water supplies. Drinking any contaminated water and eating any foods washed in the water, as well as shellfish living in the affected waterway, can cause a person to contract an infection. Cholera is rarely spread directly from person to person.

"V. cholerae" also exists outside the human body in natural water sources, either by itself or through interacting with phytoplankton, zooplankton, or biotic and abiotic detritus. Drinking such water can also result in the disease, even without prior contamination through fecal matter. Selective pressures exist however in the aquatic environment that may reduce the virulence of "V. cholerae". Specifically, animal models indicate that the transcriptional profile of the pathogen changes as it prepares to enter an aquatic environment. This transcriptional change results in a loss of ability of "V. cholerae" to be cultured on standard media, a phenotype referred to as 'viable but non-culturable' (VBNC) or more conservatively 'active but non-culturable' (ABNC). One study indicates that the culturability of "V. cholerae" drops 90% within 24 hours of entering the water, and furthermore that this loss in culturability is associated with a loss in virulence.

Both toxic and non-toxic strains exist. Non-toxic strains can acquire toxicity through a temperate bacteriophage.

About 100million bacteria must typically be ingested to cause cholera in a normal healthy adult. This dose, however, is less in those with lowered gastric acidity (for instance those using proton pump inhibitors). Children are also more susceptible, with two- to four-year-olds having the highest rates of infection. Individuals' susceptibility to cholera is also affected by their blood type, with those with type O blood being the most susceptible. Persons with lowered immunity, such as persons with AIDS or malnourished children, are more likely to experience a severe case if they become infected. Any individual, even a healthy adult in middle age, can experience a severe case, and each person's case should be measured by the loss of fluids, preferably in consultation with a professional health care provider.

The cystic fibrosis genetic mutation known as delta-F508 in humans has been said to maintain a selective heterozygous advantage: heterozygous carriers of the mutation (who are thus not affected by cystic fibrosis) are more resistant to "V. cholerae" infections. In this model, the genetic deficiency in the cystic fibrosis transmembrane conductance regulator channel proteins interferes with bacteria binding to the intestinal epithelium, thus reducing the effects of an infection.

When consumed, most bacteria do not survive the acidic conditions of the human stomach. The few surviving bacteria conserve their energy and stored nutrients during the passage through the stomach by shutting down protein production. When the surviving bacteria exit the stomach and reach the small intestine, they must propel themselves through the thick mucus that lines the small intestine to reach the intestinal walls where they can attach and thrive.

Once the cholera bacteria reach the intestinal wall, they no longer need the flagella to move. The bacteria stop producing the protein flagellin to conserve energy and nutrients by changing the mix of proteins which they express in response to the changed chemical surroundings. On reaching the intestinal wall, "V. cholerae" start producing the toxic proteins that give the infected person a watery diarrhea. This carries the multiplying new generations of "V. cholerae" bacteria out into the drinking water of the next host if proper sanitation measures are not in place.

The cholera toxin (CTX or CT) is an oligomeric complex made up of six protein subunits: a single copy of the A subunit (part A), and five copies of the B subunit (part B), connected by a disulfide bond. The five B subunits form a five-membered ring that binds to GM1 gangliosides on the surface of the intestinal epithelium cells. The A1 portion of the A subunit is an enzyme that ADP-ribosylates G proteins, while the A2 chain fits into the central pore of the B subunit ring. Upon binding, the complex is taken into the cell via receptor-mediated endocytosis. Once inside the cell, the disulfide bond is reduced, and the A1 subunit is freed to bind with a human partner protein called ADP-ribosylation factor 6 (Arf6). Binding exposes its active site, allowing it to permanently ribosylate the Gs alpha subunit of the heterotrimeric G protein. This results in constitutive cAMP production, which in turn leads to the secretion of water, sodium, potassium, and bicarbonate into the lumen of the small intestine and rapid dehydration. The gene encoding the cholera toxin was introduced into "V. cholerae" by horizontal gene transfer. Virulent strains of "V. cholerae" carry a variant of a temperate bacteriophage called CTXφ.

Microbiologists have studied the genetic mechanisms by which the "V. cholerae" bacteria turn off the production of some proteins and turn on the production of other proteins as they respond to the series of chemical environments they encounter, passing through the stomach, through the mucous layer of the small intestine, and on to the intestinal wall. Of particular interest have been the genetic mechanisms by which cholera bacteria turn on the protein production of the toxins that interact with host cell mechanisms to pump chloride ions into the small intestine, creating an ionic pressure which prevents sodium ions from entering the cell. The chloride and sodium ions create a salt-water environment in the small intestines, which through osmosis can pull up to six liters of water per day through the intestinal cells, creating the massive amounts of diarrhea. The host can become rapidly dehydrated unless an appropriate mixture of dilute salt water and sugar is taken to replace the blood's water and salts lost in the diarrhea.

By inserting separate, successive sections of "V. cholerae" DNA into the DNA of other bacteria, such as "E. coli" that would not naturally produce the protein toxins, researchers have investigated the mechanisms by which "V. cholerae" responds to the changing chemical environments of the stomach, mucous layers, and intestinal wall. Researchers have discovered a complex cascade of regulatory proteins controls expression of "V. cholerae" virulence determinants. In responding to the chemical environment at the intestinal wall, the "V. cholerae" bacteria produce the TcpP/TcpH proteins, which, together with the ToxR/ToxS proteins, activate the expression of the ToxT regulatory protein. ToxT then directly activates expression of virulence genes that produce the toxins, causing diarrhea in the infected person and allowing the bacteria to colonize the intestine. Current research aims at discovering "the signal that makes the cholera bacteria stop swimming and start to colonize (that is, adhere to the cells of) the small intestine."

Amplified fragment length polymorphism fingerprinting of the pandemic isolates of "V. cholerae" has revealed variation in the genetic structure. Two clusters have been identified: Cluster I and Cluster II. For the most part, Cluster I consists of strains from the 1960s and 1970s, while Cluster II largely contains strains from the 1980s and 1990s, based on the change in the clone structure. This grouping of strains is best seen in the strains from the African continent.

In many areas of the world, antibiotic resistance is increasing within cholera bacteria. In Bangladesh, for example, most cases are resistant to tetracycline, trimethoprim-sulfamethoxazole, and erythromycin. Rapid diagnostic assay methods are available for the identification of multi-drug resistant cases. New generation antimicrobials have been discovered which are effective against cholera bacteria in "in vitro" studies.

A rapid dipstick test is available to determine the presence of "V. cholerae". In those samples that test positive, further testing should be done to determine antibiotic resistance. In epidemic situations, a clinical diagnosis may be made by taking a patient history and doing a brief examination. Treatment is usually started without or before confirmation by laboratory analysis.

Stool and swab samples collected in the acute stage of the disease, before antibiotics have been administered, are the most useful specimens for laboratory diagnosis. If an epidemic of cholera is suspected, the most common causative agent is "V. cholerae" O1. If "V. cholerae" serogroup O1 is not isolated, the laboratory should test for "V. cholerae" O139. However, if neither of these organisms is isolated, it is necessary to send stool specimens to a reference laboratory.

Infection with "V. cholerae" O139 should be reported and handled in the same manner as that caused by "V. cholerae" O1. The associated diarrheal illness should be referred to as cholera and must be reported in the United States.

The World Health Organization (WHO) recommends focusing on prevention, preparedness, and response to combat the spread of cholera. They also stress the importance of an effective surveillance system. Governments can play a role in all of these areas.

Although cholera may be life-threatening, prevention of the disease is normally straightforward if proper sanitation practices are followed. In developed countries, due to nearly universal advanced water treatment and sanitation practices present there, cholera is rare. For example, the last major outbreak of cholera in the United States occurred in 1910–1911. Cholera is mainly a risk in developing countries.

Effective sanitation practices, if instituted and adhered to in time, are usually sufficient to stop an epidemic. There are several points along the cholera transmission path at which its spread may be halted:

Handwashing with soap or ash after using a toilet and before handling food or eating is also recommended for cholera prevention by WHO Africa.

Surveillance and prompt reporting allow for containing cholera epidemics rapidly. Cholera exists as a seasonal disease in many endemic countries, occurring annually mostly during rainy seasons. Surveillance systems can provide early alerts to outbreaks, therefore leading to coordinated response and assist in preparation of preparedness plans. Efficient surveillance systems can also improve the risk assessment for potential cholera outbreaks. Understanding the seasonality and location of outbreaks provides guidance for improving cholera control activities for the most vulnerable. For prevention to be effective, it is important that cases be reported to national health authorities.

A number of safe and effective oral vaccines for cholera are available. The World Health Organization has three prequalified oral cholera vaccines (OCVs): Dukoral, Sanchol, and Euvichol. Dukoral, an orally administered, inactivated whole cell vaccine, has an overall efficacy of about 52% during the first year after being given and 62% in the second year, with minimal side effects. It is available in over 60 countries. However, it is not currently recommended by the Centers for Disease Control and Prevention (CDC) for most people traveling from the United States to endemic countries. The vaccine that the FDA recommends, Vaxchora, is an oral attenuated live vaccine, that is effective as a single dose.

One injectable vaccine was found to be effective for two to three years. The protective efficacy was 28% lower in children less than 5 years old. However, as of 2010, it has limited availability. Work is under way to investigate the role of mass vaccination. The World Health Organization (WHO) recommends immunization of high-risk groups, such as children and people with HIV, in countries where this disease is endemic. If people are immunized broadly, herd immunity results, with a decrease in the amount of contamination in the environment.

An effective and relatively cheap method to prevent the transmission of cholera is the use of a folded "sari" (a long cloth garment) to filter drinking water. In Bangladesh this practice was found to decrease rates of cholera by nearly half. It involves folding a "sari" four to eight times. Between uses the cloth should be rinsed in clean water and dried in the sun to kill any bacteria on it. A nylon cloth appears to work as well but is not as affordable.

Continued eating speeds the recovery of normal intestinal function. The World Health Organization recommends this generally for cases of diarrhea no matter what the underlying cause. A CDC training manual specifically for cholera states: "Continue to breastfeed your baby if the baby has watery diarrhea, even when traveling to get treatment. Adults and older children should continue to eat frequently."

The most common error in caring for patients with cholera is to underestimate the speed
and volume of fluids required. In most cases, cholera can be successfully treated with oral rehydration therapy (ORT), which is highly effective, safe, and simple to administer. Rice-based solutions are preferred to glucose-based ones due to greater efficiency. In severe cases with significant dehydration, intravenous rehydration may be necessary. Ringer's lactate is the preferred solution, often with added potassium. Large volumes and continued replacement until diarrhea has subsided may be needed. Ten percent of a person's body weight in fluid may need to be given in the first two to four hours. This method was first tried on a mass scale during the Bangladesh Liberation War, and was found to have much success. Despite widespread beliefs, fruit juices and commercial fizzy drinks like cola, are not ideal for rehydration of people with serious infections of the intestines, and their excessive sugar content may even harm water uptake.

If commercially produced oral rehydration solutions are too expensive or difficult to obtain, solutions can be made. One such recipe calls for 1 liter of boiled water, 1/2 teaspoon of salt, 6 teaspoons of sugar, and added mashed banana for potassium and to improve taste.

As there frequently is initially acidosis, the potassium level may be normal, even though large losses have occurred. As the dehydration is corrected, potassium levels may decrease rapidly, and thus need to be replaced. This may be done by consuming foods high in potassium, like bananas or coconut water.

Antibiotic treatments for one to three days shorten the course of the disease and reduce the severity of the symptoms. Use of antibiotics also reduces fluid requirements. People will recover without them, however, if sufficient hydration is maintained. The World Health Organization only recommends antibiotics in those with severe dehydration.

Doxycycline is typically used first line, although some strains of "V. cholerae" have shown resistance. Testing for resistance during an outbreak can help determine appropriate future choices. Other antibiotics proven to be effective include cotrimoxazole, erythromycin, tetracycline, chloramphenicol, and furazolidone. Fluoroquinolones, such as ciprofloxacin, also may be used, but resistance has been reported.

Antibiotics improve outcomes in those who are both severely and not severely dehydrated. Azithromycin and tetracycline may work better than doxycycline or ciprofloxacin.

In Bangladesh zinc supplementation reduced the duration and severity of diarrhea in children with cholera when given with antibiotics and rehydration therapy as needed. It reduced the length of disease by eight hours and the amount of diarrhea stool by 10%. Supplementation appears to be also effective in both treating and preventing infectious diarrhea due to other causes among children in the developing world.

If people with cholera are treated quickly and properly, the mortality rate is less than 1%; however, with untreated cholera, the mortality rate rises to 50–60%.

For certain genetic strains of cholera, such as the one present during the 2010 epidemic in Haiti and the 2004 outbreak in India, death can occur within two hours of becoming ill.

Cholera affects an estimated 3–5 million people worldwide, and causes 58,000–130,000 deaths a year as of 2010. This occurs mainly in the developing world. In the early 1980s, death rates are believed to have been greater than three million a year. It is difficult to calculate exact numbers of cases, as many go unreported due to concerns that an outbreak may have a negative impact on the tourism of a country. Cholera remains both epidemic and endemic in many areas of the world. In October 2016, an outbreak of cholera began in war-ravaged Yemen. WHO called it "the worst cholera outbreak in the world".

Although much is known about the mechanisms behind the spread of cholera, this has not led to a full understanding of what makes cholera outbreaks happen in some places and not others. Lack of treatment of human feces and lack of treatment of drinking water greatly facilitate its spread, but bodies of water can serve as a reservoir, and seafood shipped long distances can spread the disease. Cholera was not known in the Americas for most of the 20th century, but it reappeared towards the end of that century.

The word cholera is from "kholera" from χολή "kholē" "bile". Cholera likely has its origins in the Indian subcontinent as evidenced by its prevalence in the region for centuries. Early outbreaks in the Indian subcontinent are believed to have been the result of poor living conditions as well as the presence of pools of still water, both of which provide ideal conditions for cholera to thrive. The disease first spread by trade routes (land and sea) to Russia in 1817, later to the rest of Europe, and from Europe to North America and the rest of the world. Seven cholera pandemics have occurred in the past 200 years, with the seventh pandemic originating in Indonesia in 1961.

The first cholera pandemic occurred in the Bengal region of India, near Calcutta starting in 1817 through 1824. The disease dispersed from India to Southeast Asia, the Middle East, Europe, and Eastern Africa through trade routes. The second pandemic lasted from 1829 to 1851 and particularly affected North America and Europe due to the result of advancements in transportation and global trade, and increased human migration, including soldiers. The third pandemic erupted in 1852, persisted until 1860, extended to North Africa, and reached South America, for the first time specifically affecting Brazil. The fourth pandemic lasted from 1863 to 1875 spread from India to Naples and Spain. The fifth pandemic was from 1881–1896 and started in India and spread to Europe, Asia, and South America. The sixth pandemic started 1899–1923. These epidemics were less fatal due to a greater understanding of the cholera bacteria. Egypt, the Arabian peninsula, Persia, India, and the Philippines were hit hardest during these epidemics, while other areas, like Germany in 1892 and Naples from 1910–1911, also experienced severe outbreaks. The seventh pandemic originated in 1961 in Indonesia and is marked by the emergence of a new strain, nicknamed "El Tor", which still persists (as of 2018) in developing countries.

Since it became widespread in the 19th century, cholera has killed tens of millions of people. In Russia alone, between 1847 and 1851, more than one million people perished of the disease. It killed 150,000 Americans during the second pandemic. Between 1900 and 1920, perhaps eight million people died of cholera in India. Cholera became the first reportable disease in the United States due to the significant effects it had on health. John Snow, in England, was the first to identify the importance of contaminated water as its cause in 1854. Cholera is now no longer considered a pressing health threat in Europe and North America due to filtering and chlorination of water supplies, but still heavily affects populations in developing countries.

In the past, vessels flew a yellow quarantine flag if any crew members or passengers were suffering from cholera. No one aboard a vessel flying a yellow flag would be allowed ashore for an extended period, typically 30 to 40 days. In modern sets of international maritime signal flags, the quarantine flag is yellow and black.

Historically many different claimed remedies have existed in folklore. Many of the older remedies were based on the miasma theory. Some believed that abdominal chilling made one more susceptible and flannel and cholera belts were routine in army kits. In the 1854–1855 outbreak in Naples homeopathic camphor was used according to Hahnemann. T. J. Ritter's "Mother's Remedies" book lists tomato syrup as a home remedy from northern America. Elecampane was recommended in the United Kingdom according to William Thomas Fernie.

Cholera cases are much less frequent in developed countries where governments have helped to establish water sanitation practices and effective medical treatments. The United States, for example, used to have a severe cholera problem similar to those in some developing countries. There were three large cholera outbreaks in the 1800s, which can be attributed to "Vibrio cholerae"'s spread through interior waterways like the Erie Canal and routes along the Eastern Seaboard. The island of Manhattan in New York City touched the Atlantic Ocean, where cholera collected just off the coast. At this time, New York City did not have as effective a sanitation system as it does today, so cholera was able to spread.

Cholera morbus is a historical term that was used to refer to gastroenteritis rather than specifically cholera.

The bacterium was isolated in 1854 by Italian anatomist Filippo Pacini, but its exact nature and his results were not widely known.

Spanish physician Jaume Ferran i Clua developed a cholera inoculation in 1885, the first to immunize humans against a bacterial disease.

Russian-Jewish bacteriologist Waldemar Haffkine developed the first cholera vaccine in July 1892.

One of the major contributions to fighting cholera was made by the physician and pioneer medical scientist John Snow (1813–1858), who in 1854 found a link between cholera and contaminated drinking water. Dr. Snow proposed a microbial origin for epidemic cholera in 1849. In his major "state of the art" review of 1855, he proposed a substantially complete and correct model for the cause of the disease. In two pioneering epidemiological field studies, he was able to demonstrate human sewage contamination was the most probable disease vector in two major epidemics in London in 1854. His model was not immediately accepted, but it was seen to be the more plausible, as medical microbiology developed over the next 30 years or so.

Cities in developed nations made massive investment in clean water supply and well-separated sewage treatment infrastructures between the mid-1850s and the 1900s. This eliminated the threat of cholera epidemics from the major developed cities in the world. In 1883, Robert Koch identified "V. cholerae" with a microscope as the bacillus causing the disease.

Robert Allan Phillips, working at the US Naval Medical Research Unit Two in Southeast Asia, evaluated the pathophysiology of the disease using modern laboratory chemistry techniques and developed a protocol for rehydration. His research led the Lasker Foundation to award him its prize in 1967.

More recently, in 2002, Alam, "et al.", studied stool samples from patients at the International Centre for Diarrhoeal Disease in Dhaka, Bangladesh. From the various experiments they conducted, the researchers found a correlation between the passage of "V. cholerae" through the human digestive system and an increased infectivity state. Furthermore, the researchers found the bacterium creates a hyperinfected state where genes that control biosynthesis of amino acids, iron uptake systems, and formation of periplasmic nitrate reductase complexes were induced just before defecation. These induced characteristics allow the cholera vibrios to survive in the "rice water" stools, an environment of limited oxygen and iron, of patients with a cholera infection.

In many developing countries, cholera still reaches its victims through contaminated water sources, and countries without proper sanitation techniques have greater incidence of the disease. Governments can play a role in this. In 2008, for example, the Zimbabwean cholera outbreak was due partly to the government's role, according to a report from the James Baker Institute. The Haitian government's inability to provide safe drinking water after the 2010 earthquake led to an increase in cholera cases as well.

Similarly, South Africa's cholera outbreak was exacerbated by the government's policy of privatizing water programs. The wealthy elite of the country were able to afford safe water while others had to use water from cholera-infected rivers.

According to Rita R. Colwell of the James Baker Institute, if cholera does begin to spread, government preparedness is crucial. A government's ability to contain the disease before it extends to other areas can prevent a high death toll and the development of an epidemic or even pandemic. Effective disease surveillance can ensure that cholera outbreaks are recognized as soon as possible and dealt with appropriately. Oftentimes, this will allow public health programs to determine and control the cause of the cases, whether it is unsanitary water or seafood that have accumulated a lot of "Vibrio cholerae" specimens. Having an effective surveillance program contributes to a government's ability to prevent cholera from spreading. In the year 2000 in the state of Kerala in India, the Kottayam district was determined to be "Cholera-affected"; this pronouncement led to task forces that concentrated on educating citizens with 13,670 information sessions about human health. These task forces promoted the boiling of water to obtain safe water, and provided chlorine and oral rehydration salts. Ultimately, this helped to control the spread of the disease to other areas and minimize deaths. On the other hand, researchers have shown that most of the citizens infected during the 1991 cholera outbreak in Bangladesh lived in rural areas, and were not recognized by the government's surveillance program. This inhibited physicians' abilities to detect cholera cases early.

According to Colwell, the quality and inclusiveness of a country's health care system affects the control of cholera, as it did in the Zimbabwean cholera outbreak. While sanitation practices are important, when governments respond quickly and have readily available vaccines, the country will have a lower cholera death toll. Affordability of vaccines can be a problem; if the governments do not provide vaccinations, only the wealthy may be able to afford them and there will be a greater toll on the country's poor. The speed with which government leaders respond to cholera outbreaks is important.

Besides contributing to an effective or declining public health care system and water sanitation treatments, government can have indirect effects on cholera control and the effectiveness of a response to cholera. A country's government can impact its ability to prevent disease and control its spread. A speedy government response backed by a fully functioning health care system and financial resources can prevent cholera's spread. This limits cholera's ability to cause death, or at the very least a decline in education, as children are kept out of school to minimize the risk of infection.




</doc>
<doc id="7592" url="https://en.wikipedia.org/wiki?curid=7592" title="Caldera">
Caldera

A caldera is a large cauldron-like hollow that forms following the evacuation of a magma chamber/reservoir. When large volumes of magma are erupted over a short time, structural support for the crust above the magma chamber is lost. The ground surface then collapses downward into the partially emptied magma chamber, leaving a massive depression at the surface (from one to dozens of kilometers in diameter). Although sometimes described as a crater, the feature is actually a type of sinkhole, as it is formed through subsidence and collapse rather than an explosion or impact. Only seven known caldera-forming collapses have occurred since the start of the 20th century, most recently at Bárðarbunga volcano in Iceland.

The word comes from Spanish ', and Latin ', meaning "cooking pot". In some texts the English term "cauldron" is also used. The term "caldera" was introduced into the geological vocabulary by the German geologist Leopold von Buch when he published his memoirs of his 1815 visit to the Canary Islands, where he first saw the Las Cañadas caldera on Tenerife, with Montaña Teide dominating the landscape, and then the Caldera de Taburiente on La Palma.

A collapse is triggered by the emptying of the magma chamber beneath the volcano, sometimes as the result of a large explosive volcanic eruption (see Tambora in 1815), but also during effusive eruptions on the flanks of a volcano (see Piton de la Fournaise in 2007) or in a connected fissure system (see Bárðarbunga in 2014–2015). If enough magma is ejected, the emptied chamber is unable to support the weight of the volcanic edifice above it. A roughly circular fracture, the "ring fault", develops around the edge of the chamber. Ring fractures serve as feeders for fault intrusions which are also known as ring dykes. Secondary volcanic vents may form above the ring fracture. As the magma chamber empties, the center of the volcano within the ring fracture begins to collapse. The collapse may occur as the result of a single cataclysmic eruption, or it may occur in stages as the result of a series of eruptions. The total area that collapses may be hundreds or thousands of square kilometers.

Some calderas are known to host rich ore deposits. One of the world's best-preserved mineralized calderas is the Sturgeon Lake Caldera in northwestern Ontario, Canada, which formed during the Neoarchean era about 2,700 million years ago.

If the magma is rich in silica, the caldera is often filled in with ignimbrite, tuff, rhyolite, and other igneous rocks. Silica-rich magma has a high viscosity, and therefore does not flow easily like basalt. As a result, gases tend to become trapped at high pressure within the magma. When the magma approaches the surface of the Earth, the rapid off-loading of overlying material causes the trapped gases to decompress rapidly, thus triggering explosive destruction of the magma and spreading volcanic ash over wide areas. Further lava flows may be erupted.

If volcanic activity continues, the center of the caldera may be uplifted in the form of a "resurgent dome" such as is seen at Cerro Galán, Lake Toba, Yellowstone, etc., by subsequent intrusion of magma. A "silicic" or "rhyolitic caldera" may erupt hundreds or even thousands of cubic kilometers of material in a single event. Even small caldera-forming eruptions, such as Krakatoa in 1883 or Mount Pinatubo in 1991, may result in significant local destruction and a noticeable drop in temperature around the world. Large calderas may have even greater effects.

When Yellowstone Caldera last erupted some 650,000 years ago, it released about 1,000 km of material (as measured in dense rock equivalent (DRE)), covering a substantial part of North America in up to two metres of debris. By comparison, when Mount St. Helens erupted in 1980, it released ~1.2 km (DRE) of ejecta. The ecological effects of the eruption of a large caldera can be seen in the record of the Lake Toba eruption in Indonesia.

About 74,000 years ago, this Indonesian volcano released about dense-rock equivalent of ejecta. This was the largest known eruption during the ongoing Quaternary period (the last 2.6 million years) and the largest known explosive eruption during the last 25 million years. In the late 1990s, anthropologist Stanley Ambrose proposed that a volcanic winter induced by this eruption reduced the human population to about 2,000–20,000 individuals, resulting in a population bottleneck. More recently, Lynn Jorde and Henry Harpending proposed that the human species was reduced to approximately 5,000-10,000 people. There is no direct evidence, however, that either theory is correct, and there is no evidence for any other animal decline or extinction, even in environmentally sensitive species. There is evidence that human habitation continued in India after the eruption.

Some volcanoes, such as the large shield volcanoes Kīlauea and Mauna Loa on the island of Hawaii, form calderas in a different fashion. The magma feeding these volcanoes is basalt, which is silica poor. As a result, the magma is much less viscous than the magma of a rhyolitic volcano, and the magma chamber is drained by large lava flows rather than by explosive events. The resulting calderas are also known as subsidence calderas and can form more gradually than explosive calderas. For instance, the caldera atop Fernandina Island collapsed in 1968 when parts of the caldera floor dropped .

Since the early 1960s, it has been known that volcanism has occurred on other planets and moons in the Solar System. Through the use of manned and unmanned spacecraft, volcanism has been discovered on Venus, Mars, the Moon, and Io, a satellite of Jupiter. None of these worlds have plate tectonics, which contributes approximately 60% of the Earth's volcanic activity (the other 40% is attributed to hotspot volcanism). Caldera structure is similar on all of these planetary bodies, though the size varies considerably. The average caldera diameter on Venus is 68 km. The average caldera diameter on Io is close to 40 km, and the mode is 6 km; Tvashtar Paterae is likely the largest caldera with a diameter of 290 km. The average caldera diameter on Mars is 48 km, smaller than Venus. Calderas on Earth are the smallest of all planetary bodies and vary from 1.6 to 80 km as a maximum.

The Moon has an outer shell of low-density crystalline rock that is a few hundred kilometers thick, which formed due to a rapid creation. The craters of the moon have been well preserved through time and were once thought to have been the result of extreme volcanic activity, but actually were formed by meteorites, nearly all of which took place in the first few hundred million years after the Moon formed. Around 500 million years afterward, the Moon's mantle was able to be extensively melted due to the decay of radioactive elements. Massive basaltic eruptions took place generally at the base of large impact craters. Also, eruptions may have taken place due to a magma reservoir at the base of the crust. This forms a dome, possibly the same morphology of a shield volcano where calderas universally are known to form. Although caldera-like structures are rare on the Moon, they are not completely absent. The Compton-Belkovich Volcanic Complex on the far side of the Moon is thought to be a caldera, possibly an ash-flow caldera.

The volcanic activity of Mars is concentrated in two major provinces: Tharsis and Elysium. Each province contains a series of giant shield volcanoes that are similar to what we see on Earth and likely are the result of mantle hot spots. The surfaces are dominated by lava flows, and all have one or more collapse calderas. Mars has the largest volcano in the Solar System, Olympus Mons, which is more than three times the height of Mount Everest, with a diameter of 520 km (323 miles). The summit of the mountain has six nested calderas.

Because there is no plate tectonics on Venus, heat is mainly lost by conduction through the lithosphere. This causes enormous lava flows, accounting for 80% of Venus' surface area. Many of the mountains are large shield volcanoes that range in size from 150–400 km in diameter and 2–4 km high. More than 80 of these large shield volcanoes have summit calderas averaging 60 km across.

Io, unusually, is heated by solid flexing due to the tidal influence of Jupiter and Io's orbital resonance with neighboring large moons Europa and Ganymede, which keeps its orbit slightly eccentric. Unlike any of the planets mentioned, Io is continuously volcanically active. For example, the NASA "Voyager 1" and "Voyager 2" spacecraft detected nine erupting volcanoes while passing Io in 1979. Io has many calderas with diameters tens of kilometers across.







</doc>
