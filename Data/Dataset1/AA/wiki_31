<doc id="4482" url="https://en.wikipedia.org/wiki?curid=4482" title="Liberal Party (UK)">
Liberal Party (UK)

The Liberal Party was one of the two major parties in the United Kingdom – with the opposing Conservative Party – in the nineteenth and early twentieth centuries.

The party arose from an alliance of Whigs and free trade Peelites and Radicals favourable to the ideals of the American and French Revolutions in the 1850s. By the end of the nineteenth century, it had formed four governments under William Gladstone. Despite being divided over the issue of Irish Home Rule, the party returned to government in 1906 with a landslide victory.

It passed the welfare reforms that created a basic British welfare state. Liberal H. H. Asquith was Prime Minister from 1908 to 1916, followed by David Lloyd George from 1916 to 1922. Although Asquith was Leader of the Liberal Party, the dominant figure was Lloyd George. Asquith was overwhelmed by the wartime role of coalition Prime Minister and Lloyd George replaced him as Prime Minister in late 1916, but Asquith remained as Liberal Party leader. The pair fought for years over control of the party, badly weakening it in the process. Historian Martin Pugh in "The Oxford Companion to British History" argues:

The government of Lloyd George was dominated by the Conservative Party, which finally deposed him in 1922. By the end of the 1920s, the Labour Party had replaced the Liberals as the Conservatives' main rival. The party went into decline after 1918 and by the 1950s won no more than six seats at general elections. Apart from notable by-election victories, the party's fortunes did not improve significantly until it formed the SDP–Liberal Alliance with the newly formed Social Democratic Party (SDP) in 1981. At the 1983 general election, the Alliance won over a quarter of the vote, but only 23 of the 650 seats it contested. At the 1987 general election, its share of the vote fell below 23% and the Liberal and Social Democratic parties merged in 1988 to form the Liberal Democrats. A splinter group reconstituted the Liberal Party in 1989. It was formed by party members opposed to the merger who saw the Liberal Democrats diluting Liberal ideals.

Prominent intellectuals associated with the Liberal Party include the philosopher John Stuart Mill, the economist John Maynard Keynes and social planner William Beveridge.

The Liberal Party grew out of the Whigs, who had their origins in an aristocratic faction in the reign of Charles II and the early 19th century Radicals. The Whigs were in favour of reducing the power of the Crown and increasing the power of Parliament. Although their motives in this were originally to gain more power for themselves, the more idealistic Whigs gradually came to support an expansion of democracy for its own sake. The great figures of reformist Whiggery were Charles James Fox (died 1806) and his disciple and successor Earl Grey. After decades in opposition, the Whigs returned to power under Grey in 1830 and carried the First Reform Act in 1832.

The Reform Act was the climax of Whiggism, but it also brought about the Whigs' demise. The admission of the middle classes to the franchise and to the House of Commons led eventually to the development of a systematic middle class liberalism and the end of Whiggery, although for many years reforming aristocrats held senior positions in the party. In the years after Grey's retirement, the party was led first by Lord Melbourne, a fairly traditional Whig, and then by Lord John Russell, the son of a Duke but a crusading radical, and by Lord Palmerston, a renegade Irish Tory and essentially a conservative, although capable of radical gestures.

As early as 1839, Russell had adopted the name of "Liberals", but in reality his party was a loose coalition of Whigs in the House of Lords and Radicals in the Commons. The leading Radicals were John Bright and Richard Cobden, who represented the manufacturing towns which had gained representation under the Reform Act. They favoured social reform, personal liberty, reducing the powers of the Crown and the Church of England (many Liberals were Nonconformists), avoidance of war and foreign alliances (which were bad for business) and above all free trade. For a century, free trade remained the one cause which could unite all Liberals.

In 1841 the Liberals lost office to the Conservatives under Sir Robert Peel, but their period in opposition was short because the Conservatives split over the repeal of the Corn Laws, a free trade issue; and a faction known as the Peelites (but not Peel himself, who died soon after) defected to the Liberal side. This allowed ministries led by Russell, Palmerston and the Peelite Lord Aberdeen to hold office for most of the 1850s and 1860s. A leading Peelite was William Ewart Gladstone, who was a reforming Chancellor of the Exchequer in most of these governments. The formal foundation of the Liberal Party is traditionally traced to 1859 and the formation of Palmerston's second government.

However, the Whig-Radical amalgam could not become a true modern political party while it was dominated by aristocrats and it was not until the departure of the "Two Terrible Old Men", Russell and Palmerston, that Gladstone could become the first leader of the modern Liberal Party. This was brought about by Palmerston's death in 1865 and Russell's retirement in 1868. After a brief Conservative government (during which the Second Reform Act was passed by agreement between the parties), Gladstone won a huge victory at the 1868 election and formed the first Liberal government. The establishment of the party as a national membership organisation came with the foundation of the National Liberal Federation in 1877. John Stuart Mill was also a Liberal MP from 1865 to 1868.

For the next thirty years Gladstone and Liberalism were synonymous. William Ewart Gladstone served as prime minister four times (1868–74, 1880–85, 1886, and 1892–94). His financial policies, based on the notion of balanced budgets, low taxes and "laissez-faire", were suited to a developing capitalist society, but they could not respond effectively as economic and social conditions changed. Called the "Grand Old Man" later in life, Gladstone was always a dynamic popular orator who appealed strongly to the working class and to the lower middle class. Deeply religious, Gladstone brought a new moral tone to politics, with his evangelical sensibility and his opposition to aristocracy. His moralism often angered his upper-class opponents (including Queen Victoria), and his heavy-handed control split the Liberal Party.

In foreign policy, Gladstone was in general against foreign entanglements, but he did not resist the realities of imperialism. For example, he ordered the occupation of Egypt by British forces in 1882. His goal was to create a European order based on co-operation rather than conflict and on mutual trust instead of rivalry and suspicion; the rule of law was to supplant the reign of force and self-interest. This Gladstonian concept of a harmonious Concert of Europe was opposed to and ultimately defeated by a Bismarckian system of manipulated alliances and antagonisms.

As prime minister 1868 to 1874, Gladstone headed a Liberal Party which was a coalition of Peelites like himself, Whigs and Radicals – he was now a spokesman for "peace, economy and reform." One major achievement was the Elementary Education Act of 1870, which provided England with an adequate system of elementary schools for the first time. He also secured the abolition of the purchase of commissions in the army and of religious tests for admission to Oxford and Cambridge; the introduction of the secret ballot in elections; the legalization of trade unions; and the reorganization of the judiciary in the Judicature Act.

Regarding Ireland, the major Liberal achievements were land reform, where he ended centuries of landlord oppression, and the disestablishment of the (Anglican) Church of Ireland through the Irish Church Act 1869.

In the 1874 general election Gladstone was defeated by the Conservatives under Disraeli during a sharp economic recession. He formally resigned as Liberal leader and was succeeded by the Marquess of Hartington, but he soon changed his mind and returned to active politics. He strongly disagreed with Disraeli's pro-Ottoman foreign policy and in 1880 he conducted the first outdoor mass-election campaign in Britain, known as the Midlothian campaign. The Liberals won a large majority in the 1880 election. Hartington ceded his place and Gladstone resumed office.

Among the consequences of the Third Reform Act (1884) was the giving of the vote to many Catholics in Ireland. In the 1885 general election the Irish Parliamentary Party held the balance of power in the House of Commons, and demanded Irish Home Rule as the price of support for a continued Gladstone ministry. Gladstone personally supported Home Rule, but a strong Liberal Unionist faction led by Joseph Chamberlain, along with the last of the Whigs, Hartington, opposed it. The Irish Home Rule bill gave all owners of Irish land a chance to sell to the state at a price equal to 20 years' purchase of the rents and allowing tenants to purchase the land. Irish nationalist reaction was mixed, Unionist opinion was hostile, and the election addresses during the 1886 election revealed English radicals to be against the bill also. Among the Liberal rank and file, several Gladstonian candidates disowned the bill, reflecting fears at the constituency level that the interests of the working people were being sacrificed to finance a rescue operation for the landed elite.

The result was a catastrophic split in the Liberal Party, and heavy defeat in the 1886 election at the hands of Lord Salisbury. There was a final weak Gladstone ministry in 1892, but it also was dependent on Irish support and failed to get Irish Home Rule through the House of Lords.

Historically, the aristocracy was divided between Conservatives and Liberals. However, when Gladstone committed to home rule for Ireland, Britain's upper classes largely abandoned the Liberal party, giving the Conservatives a large permanent majority in the House of Lords. High Society in London, following the Queen, largely ostracized home rulers, and Liberal clubs were badly split. Joseph Chamberlain took a major element of upper-class supporters out of the Party and into a third party called "Liberal Unionism" on the Irish issue. It collaborated with and eventually merged into the Conservative party. The Gladstonian liberals in 1891 adopted "The Newcastle Programme that included home rule for Ireland, disestablishment of the Church of England in Wales and Scotland, tighter controls on the sale of liquor, major extension of factory regulation, and various democratic political reforms. The Programme had a strong appeal to the Nonconformist middle-class Liberal element, which felt liberated by the departure of the aristocracy.

A major long-term consequence of the Third Reform Act was the rise of Lib-Lab candidates, in the absence of any committed Labour Party. The Act split all county constituencies (which were represented by multiple MPs) into single-member constituencies, roughly corresponding to population patterns. In areas with working class majorities, in particular coal-mining areas, Lib-Lab candidates were popular, and they received sponsorship and endorsement from trade unions. In the first election after the Act was passed (1885), thirteen were elected, up from two in 1874. The Third Reform Act also facilitated the demise of the Whig old guard: in two-member constituencies, it was common to pair a Whig and a radical under the Liberal banner. After the Third Reform Act, fewer Whigs were selected as candidates.

A broad range of interventionist reforms were introduced by the 1892–95 Liberal government. Amongst other measures, The standard of accommodation and of teaching in the public schools was improved, factory inspection was made more stringent, and ministers used their powers to increase the wages and reduce the working hours of large numbers of male workers employed by the state.

Historian Walter L. Arnstein, concludes:

Gladstone finally retired in 1894. Gladstone's support for Home Rule deeply divided the party, and it lost its upper and upper-middle-class base, while keeping support among Protestant nonconformists and the Celtic fringe. Historian R. C. K. Ensor reports that after 1886, the main Liberal Party was deserted by practically the entire whig peerage and the great majority of the upper-class and upper-middle-class members. High prestige London clubs that had a Liberal base were deeply split. Ensor notes that, "London society, following the known views of the Queen, practically ostracized home rulers." 

The new Liberal leader was the ineffectual Lord Rosebery. He led the party to a heavy defeat in the 1895 general election.

The Liberal Party lacked a unified ideological base in 1906. It contained numerous contradictory and hostile factions, such as imperialists and supporters of the Boers; near-socialists and laissez-faire classical liberals; suffragettes and opponents of women's suffrage; antiwar elements and supporters of the military alliance with France. Non-Conformist Dissenters – Protestants outside the Anglican fold – were a powerful element, dedicated to opposing the established church in terms of education and taxation. However, the Dissenters were losing support amid society at large and played a lesser role in party affairs after 1900. The Party, furthermore, also included Irish Catholics, and secularists from the labour movement. Many Conservatives (including Winston Churchill) had recently protested against high tariff moves by the Conservatives by switching to the anti-tariff Liberal camp, but it was unclear how many old Conservative traits they brought along, especially on military and naval issues.

The middle-class business, professional and intellectual communities were generally strongholds, although some old aristocratic families played important roles as well. The working class element was moving rapidly toward the newly emerging Labour Party. One uniting element was widespread agreement on the use of politics and Parliament as a device to upgrade and improve society and to reform politics. All Liberals were outraged when Conservatives used their majority in the House of Lords to block reform legislation.

The late nineteenth century saw the emergence of “New Liberalism” within the Liberal Party, which advocated state intervention as a means of guaranteeing freedom and removing obstacles to it such as poverty and unemployment. The policies of the New Liberalism are now known as social liberalism.

The New Liberals included intellectuals like L. T. Hobhouse, and John A. Hobson. They saw individual liberty as something achievable only under favourable social and economic circumstances. In their view, the poverty, squalor, and ignorance in which many people lived made it impossible for freedom and individuality to flourish. New Liberals believed that these conditions could be ameliorated only through collective action coordinated by a strong, welfare-oriented, and interventionist state.

After the historic 1906 victory, the Liberal Party introduced multiple reforms on range of issues, including health insurance, unemployment insurance, and pensions for elderly workers, thereby laying the groundwork for the future British welfare state. Some proposals failed, such as licensing fewer pubs, or rolling back Conservative educational policies. The People's Budget of 1909, championed by David Lloyd George and fellow Liberal Winston Churchill, introduced unprecedented taxes on the wealthy in Britain and radical social welfare programmes to the country's policies. It was the first budget with the expressed intent of redistributing wealth among the public. It imposed increased taxes on luxuries, liquor, tobacco, high incomes, and land -- taxation that disproportionately affected the rich. The new money was to be made available for new welfare programmes as well as new battleships. In 1911 Lloyd George succeeded in putting through Parliament his National Insurance Act, making provision for sickness and invalidism, and this was followed by his Unemployment Insurance Act.

Historian Peter Weiler argues that:

Contrasting Old Liberalism with New Liberalism, David Lloyd George noted in a 1908 speech that the old Liberals:

The Liberals languished in opposition for a decade, while the coalition of Salisbury and Chamberlain held power. The 1890s were marred by infighting between the three principal successors to Gladstone, party leader William Harcourt, former Prime Minister Lord Rosebery, and Gladstone's personal secretary, John Morley. This intrigue finally led Harcourt and Morley to resign their positions in 1898 as they continued to be at loggerheads with Rosebery over Irish home rule and issues relating to imperialism. Replacing Harcourt as party leader was Sir Henry Campbell-Bannerman. Harcourt's resignation briefly muted the turmoil in the party, but the beginning of the Second Boer War soon nearly broke the party apart, with Rosebery and a circle of supporters including important future Liberal figures H.H. Asquith, Edward Grey, and Richard Burdon Haldane forming a clique dubbed the "Liberal Imperialists" that supported the government in the prosecution of the war. On the other side, more radical members of the party formed a Pro-Boer faction that denounced the conflict and called for an immediate end to hostilities. Quickly rising to prominence among the Pro-Boers was David Lloyd George, a relatively new MP and a master of rhetoric, who took advantage of having a national stage to speak out on a controversial issue to make his name in the party. Harcourt and Morley also sided with this group, though with slightly different aims. Campbell-Bannerman tried to keep these forces together at the head of a moderate Liberal rump, but in 1901 he delivered a speech on the government's "methods of barbarism" in South Africa that pulled him further to the left and nearly tore the party in two. The party was saved after Salisbury's retirement in 1902 when his successor, Arthur Balfour, pushed a series of unpopular initiatives such as the Education Act 1902 and Joseph Chamberlain called for a new system of protectionist tariffs.

Campbell-Bannerman was able to rally the party around the traditional liberal platform of free trade and land reform and led them to the greatest election victory in their history. This would prove the last time the Liberals won a majority in their own right. Although he presided over a large majority, Sir Henry Campbell-Bannerman was overshadowed by his ministers, most notably H. H. Asquith at the Exchequer, Edward Grey at the Foreign Office, Richard Burdon Haldane at the War Office and David Lloyd George at the Board of Trade. Campbell-Bannerman retired in 1908 and died soon after. He was succeeded by Asquith, who stepped up the government's radicalism. Lloyd George succeeded Asquith at the Exchequer, and was in turn succeeded at the Board of Trade by Winston Churchill, a recent defector from the Conservatives.

The General Election of 1906 also represented a shift to the left by the Liberal Party. According to Rosemary Rees, almost half of the Liberal MPs elected in 1906 were supportive of the 'New Liberalism' (which advocated government action to improve people's lives),) while claims were made that “five-sixths of the Liberal party are left wing.” Other historians, however, have questioned the extent to which the Liberal Party experienced a leftward shift; according to Robert C. Self however, only between 50 and 60 Liberal MPs out of the 400 in the parliamentary party after 1906 were Social Radicals, with a core of 20 to 30. Nevertheless, important junior offices were held in the cabinet by what Duncan Tanner has termed "genuine New Liberals, Centrist reformers, and Fabian collectivists," and much legislation was pushed through by the Liberals in government. This included the regulation of working hours, National Insurance and welfare.
A political battle erupted over the People's Budget and resulted in the passage of an act ending the power of the House of Lords to block legislation. The cost was high, however, as the government was required by the king to call two general elections in 1910 to validate its position and ended up frittering away most of its large majority, being left once again dependent on the Irish Nationalists.

As a result, Asquith was forced to introduce a new third Home Rule bill in 1912. Since the House of Lords no longer had the power to block the bill, the Unionist's Ulster Volunteers led by Sir Edward Carson, launched a campaign of opposition that included the threat of armed resistance in Ulster and the threat of mass resignation of their commissions by army officers in Ireland in 1914 ("see Curragh Incident"). In their resistance to Home Rule the Ulster Protestants had the full support of the Conservatives, whose leader, Bonar Law, was of Ulster-Scots descent. The country seemed to be on the brink of civil war when the First World War broke out in August 1914. Historian George Dangerfield has argued that the multiplicity of crises in 1910 to 1914, before the war broke out, so weakened the Liberal coalition that it marked the "Strange Death of Liberal England". However, most historians date the collapse to the crisis of the First World War.

The Liberal Party might have survived a short war, but the totality of the Great War called for measures that the Party had long rejected. The result was the permanent destruction of ability of the Liberal Party to lead a government. Historian Robert Blake explains the dilemma:
Blake further notes that it was the Liberals, not the Conservatives who needed the moral outrage of Belgium to justify going to war, while the Conservatives called for intervention from the start of the crisis on the grounds of "realpolitik" and the balance of power. Lloyd George and Churchill, however, were zealous supporters of the war, and gradually forced the old peace-orientated Liberals out. Asquith was blamed for the poor British performance in first year. Fearful of losing an election, he had no choice but to form a coalition with Conservatives and Labour (on 17 May 1915). This coalition fell apart at the end of 1916, when the Conservatives withdrew their support from Asquith and gave it instead to Lloyd George, who became Prime Minister at the head of a new coalition largely made up of Conservatives. Asquith and his followers moved to the opposition benches in Parliament and the Liberal Party was deeply split once again.

Lloyd George abandoned many Liberal principles in his single-minded crusade to win the war at all costs. That brought him and like-minded Liberals into the new coalition on the ground long occupied by Conservatives. There was no more planning for world peace or liberal treatment of Germany, nor discomfit with aggressive and authoritarian measures of state power. More deadly to the future of the Party, says Wilson, was its repudiation by ideological Liberals, who decided sadly that it no longer represented their principles. Finally the presence of the vigorous new Labour Party on the left gave a new home to voters disenchanted with the Liberal performance.

In the 1918 general election Lloyd George, hailed as "the Man Who Won the War", led his coalition into a "khaki election". Lloyd George and the Conservative leader Bonar Law wrote a joint letter of support to candidates to indicate they were considered the official Coalition candidates – this "coupon", as it became known, was issued against many sitting Liberal MPs, often to devastating effect, though not against Asquith himself. The coalition won a massive victory as the Asquithian Liberals and Labour were decimated. Those remaining Liberal MPs who were opposed to the Coalition Government went into opposition under the parliamentary leadership of Sir Donald MacLean who also became Leader of the Opposition. Asquith, who had appointed MacLean, remained as overall Leader of the Liberal Party even though he lost his seat in 1918. Asquith returned to parliament in 1920 and resumed leadership. Between 1919-1923, the anti-Lloyd George Liberals were called Asquithian Liberals, Wee Free Liberals or Independent Liberals.

Lloyd George was increasingly under the influence of the rejuvenated Conservative party who numerically dominated the coalition. In 1922, the Conservative backbenchers rebelled against the continuation of the coalition, citing in particular Lloyd George's plan for war with Turkey in the Chanak Crisis, and his corrupt sale of honours. He resigned as Prime Minister and was succeeded by Bonar Law.

At the 1922 and 1923 elections the Liberals won barely a third of the vote and only a quarter of the seats in the House of Commons, as many radical voters abandoned the divided Liberals and went over to Labour. In 1922 Labour became the official opposition. A reunion of the two warring factions took place in 1923 when the new Conservative Prime Minister Stanley Baldwin committed his party to protective tariffs, causing the Liberals to reunite in support of free trade. The party gained ground in the 1923 general election but made most of its gains from Conservatives whilst losing ground to Labour – a sign of the party's direction for many years to come. The party remained the third largest in the House of Commons, but the Conservatives had lost their majority. There was much speculation and fear about the prospect of a Labour government, and comparatively little about a Liberal government, even though it could have plausibly presented an experienced team of ministers compared to Labour's almost complete lack of experience, as well as offering a middle ground that could obtain support from both Conservatives and Labour in crucial Commons divisions. But instead of trying to force the opportunity to form a Liberal government, Asquith decided instead to allow Labour the chance of office, in the belief that they would prove incompetent and this would set the stage for a revival of Liberal fortunes at Labour's expense. It was a fatal error.
Labour was determined to destroy the Liberals and become the sole party of the left. Ramsay MacDonald was forced into a snap election in 1924, and although his government was defeated, he achieved his objective of virtually wiping the Liberals out as many more radical voters now moved to Labour, whilst moderate middle-class Liberal voters concerned about socialism moved to the Conservatives. The Liberals were reduced to a mere forty seats in Parliament, only seven of which had been won against candidates from both parties; and none of these formed a coherent area of Liberal survival. The party seemed finished, and during this period some Liberals, such as Churchill, went over to the Conservatives, while others went over to Labour. (Several Labour ministers of later generations, such as Michael Foot and Tony Benn, were the sons of Liberal MPs.)

Asquith died in 1928 and the enigmatic figure of Lloyd George returned to the leadership and began a drive to produce coherent policies on many key issues of the day. In the 1929 general election he made a final bid to return the Liberals to the political mainstream, with an ambitious programme of state stimulation of the economy called "We Can Conquer Unemployment!", largely written for him by the Liberal economist John Maynard Keynes. The Liberals gained ground, but once again it was at the Conservatives' expense whilst also losing seats to Labour. Indeed, the urban areas of the country suffering heavily from unemployment, which might have been expected to respond the most to the radical economic policies of the Liberals, instead gave the party its worst results. By contrast most of the party's seats were won either due to the absence of a candidate from one of the other parties or in rural areas on the "Celtic fringe", where local evidence suggests that economic ideas were at best peripheral to the electorate's concerns. The Liberals now found themselves with 59 members, holding the balance of power in a Parliament where Labour was the largest party but lacked an overall majority. Lloyd George offered a degree of support to the Labour government in the hope of winning concessions, including a degree of electoral reform to introduce the alternative vote, but this support was to prove bitterly divisive as the Liberals increasingly divided between those seeking to gain what Liberal goals they could achieve, those who preferred a Conservative government to a Labour one and vice versa.

The last majority Liberal Government in Britain was elected in 1906.
The years preceding the First World War were marked by worker strikes and civil unrest and saw many violent confrontations between civilians and the police and armed forces. Other issues of the period included women's suffrage and the Irish Home Rule movement.
After the carnage of 1914–1918, the democratic reforms of the Representation of the People Act 1918 instantly tripled the number of people entitled to vote in Britain from seven to twenty-one million. The Labour Party benefited most from this huge change in the electorate, forming its first minority government in 1924.

In 1931 MacDonald's government fell apart in response to the Great Depression, and the Liberals agreed to join his National Government, dominated by the Conservatives. Lloyd George himself was ill and did not actually join. Soon, however, the Liberals faced another divisive crisis when a National Government was proposed to fight the 1931 general election with a mandate for tariffs. From the outside, Lloyd George called for the party to abandon the government completely in defence of free trade, but only a few MPs and candidates followed. Another group under Sir John Simon then emerged, who were prepared to continue their support for the government and take the Liberal places in the Cabinet if there were resignations. The third group under Sir Herbert Samuel pressed for the parties in government to fight the election on separate platforms. In doing so the bulk of Liberals remained supporting the government, but two distinct Liberal groups had emerged within this bulk – the Liberal Nationals (officially the "National Liberals" after 1947) led by Simon, also known as "Simonites", and the "Samuelites" or "official Liberals", led by Samuel who remained as the official party. Both groups secured about 34 MPs but proceeded to diverge even further after the election, with the Liberal Nationals remaining supporters of the government throughout its life. There were to be a succession of discussions about them rejoining the Liberals, but these usually foundered on the issues of free trade and continued support for the National Government. The one significant reunification came in 1946 when the Liberal and Liberal National party organisations in London merged.

The official Liberals found themselves a tiny minority within a government committed to protectionism. Slowly they found this issue to be one they could not support. In early 1932 it was agreed to suspend the principle of collective responsibility to allow the Liberals to oppose the introduction of tariffs. Later in 1932 the Liberals resigned their ministerial posts over the introduction of the Ottawa Agreement on Imperial Preference. However, they remained sitting on the government benches supporting it in Parliament, though in the country local Liberal activists bitterly opposed the government. Finally in late 1933 the Liberals crossed the floor of the House of Commons and went into complete opposition. By this point their number of MPs was severely depleted. In the 1935 general election, just 17 Liberal MPs were elected, along with Lloyd George and three followers as "independent Liberals". Immediately after the election the two groups reunited, though Lloyd George declined to play much of a formal role in his old party. Over the next ten years there would be further defections as MPs deserted to either the Liberal Nationals or Labour. Yet there were a few recruits, such as Clement Davies, who had deserted to the National Liberals in 1931 but now returned to the party during the Second World War and who would lead it after the war.

Samuel had lost his seat in the 1935 election and the leadership of the party fell to Sir Archibald Sinclair. With many traditional domestic Liberal policies now regarded as irrelevant, he focused the party on opposition to both the rise of Fascism in Europe and the appeasement foreign policy of the British government, arguing that intervention was needed, in contrast to the Labour calls for pacifism. Despite the party's weaknesses, Sinclair gained a high profile as he sought to recall the Midlothian Campaign and once more revitalise the Liberals as the party of a strong foreign policy.

In 1940 they joined Churchill's wartime coalition government, with Sinclair serving as Secretary of State for Air, the last British Liberal to hold Cabinet rank office for seventy years. However, it was a sign of the party's lack of importance that they were not included in the War Cabinet; some leading party members founded Radical Action, a group which called for liberal candidates to break the war-time electoral pact. At the 1945 general election, Sinclair and many of his colleagues lost their seats to both Conservatives and Labour, and the party returned just 12 MPs to Westminster. But this was just the beginning of the decline. In 1950, the general election saw the Liberals return just nine MPs. Another general election was called in 1951, and the Liberals were left with just six MPs; all but one of them were aided by the fact that the Conservatives refrained from fielding candidates in those constituencies.

In 1957 this total fell to five when one of the Liberal MPs died and the subsequent by-election was lost to the Labour Party, which selected the former Liberal Deputy Leader Megan Lloyd George as its own candidate. The Liberal Party seemed close to extinction. During this low period, it was often joked that Liberal MPs could hold meetings in the back of one taxi.

Through the 1950s and into the 1960s the Liberals survived only because a handful of constituencies in rural Scotland and Wales clung to their Liberal traditions, whilst in two English towns, Bolton and Huddersfield, local Liberals and Conservatives agreed to each contest only one of the town's two seats. Jo Grimond, for example, who became Liberal leader in 1956, was MP for the remote Orkney and Shetland islands. Under his leadership a Liberal revival began, marked by the Orpington by-election of March 1962 which was won by Eric Lubbock. There, the Liberals won a seat in the London suburbs for the first time since 1935.

The Liberals became the first of the major British political parties to advocate British membership of the European Economic Community. Grimond also sought an intellectual revival of the party, seeking to position it as a non-socialist radical alternative to the Conservative government of the day. In particular he canvassed the support of the young post-war university students and recent graduates, appealing to younger voters in a way that many of his recent predecessors had not, and asserting a new strand of Liberalism for the post-war world.

The new middle-class suburban generation began to find the Liberals' policies attractive again. Under Grimond (who retired in 1967) and his successor, Jeremy Thorpe, the Liberals regained the status of a serious third force in British politics, polling up to 20% of the vote but unable to break the duopoly of Labour and Conservative and win more than fourteen seats in the Commons. An additional problem was competition in the Liberal heartlands in Scotland and Wales from the Scottish National Party and Plaid Cymru who both grew as electoral forces from the 1960s onwards. Although Emlyn Hooson held on to the seat of Montgomeryshire, upon Clement Davies death in 1962, the party lost five Welsh seats between 1950 and 1966. In September 1966 the Welsh Liberal Party formed their own state party, moving the Liberal Party into a fully federal structure.

In local elections Liverpool remained a Liberal stronghold, with the party taking the plurality of seats on the elections to the new Liverpool Metropolitan Borough Council in 1973. In the February 1974 general election the Conservative government of Edward Heath won a plurality of votes cast, but the Labour Party gained a plurality of seats due to the Ulster Unionist MPs refusing to support the Conservatives after the Northern Ireland Sunningdale Agreement. The Liberals now held the balance of power in the Commons. Conservatives offered Thorpe the Home Office if he would join a coalition government with Heath. Thorpe was personally in favour of it, but the party insisted on a clear government commitment to introducing proportional representation and a change of Prime Minister. The former was unacceptable to Heath's Cabinet and the latter to Heath personally, so the talks collapsed. Instead a minority Labour government was formed under Harold Wilson but with no formal support from Thorpe. In the October 1974 general election the Liberals slipped back slightly and the Labour government won a wafer-thin majority.

Thorpe was subsequently forced to resign after allegations that he attempted to have his homosexual lover murdered by a hitman. The party's new leader, David Steel, negotiated the Lib-Lab pact with Wilson's successor as Prime Minister, James Callaghan. According to this pact, the Liberals would support the government in crucial votes in exchange for some influence over policy. The agreement lasted from 1977 to 1978, but proved mostly fruitless, for two reasons: the Liberals' key demand of proportional representation was rejected by most Labour MPs, whilst the contacts between Liberal spokespersons and Labour ministers often proved detrimental, such as between finance spokesperson John Pardoe and Chancellor of the Exchequer Denis Healey, who were mutually antagonistic.

The Conservative Party under the leadership of Margaret Thatcher won the 1979 general election, placing the Labour Party back in opposition, which served to push the Liberals back into the margins.

In 1981, defectors from a moderate faction of the Labour Party, led by former Cabinet ministers Roy Jenkins, David Owen and Shirley Williams, founded the Social Democratic Party (SDP). The new party and the Liberals quickly formed the SDP-Liberal Alliance, which for a while polled as high as 50% in the opinion polls and appeared capable of winning the next general election. Indeed, Steel was so confident of an Alliance victory that he told the 1981 Liberal conference, "Go back to your constituencies, and prepare for government!"

However, the Alliance was overtaken in the polls by the Tories in the aftermath of the Falkland Islands War and at the 1983 general election the Conservatives were re-elected by a landslide, with Labour once again forming the opposition. While the SDP-Liberal Alliance came close to Labour in terms of votes (a share of more than 25%), it only had 23 MPs compared to Labour's 209.

In the 1987 general election, the Alliance's share of the votes fell slightly and it now had 22 MPs. In the election's aftermath Steel proposed a merger of the two parties. Most SDP members voted in favour of the merger, but SDP leader David Owen objected and continued to lead a "rump" SDP.

In March 1988 the Liberal Party and Social Democratic Party merged to create the Social and Liberal Democrats, renamed the Liberal Democrats in October 1989. Over two-thirds of the members, and all the serving MPs, of the Liberal Party joined this party, led first jointly by Steel and the SDP leader Robert Maclennan.

A group of Liberal opponents of the merger with the Social Democrats, including Michael Meadowcroft (the former Liberal MP for Leeds West) and Paul Wiggin (who served on Peterborough City Council as a Liberal), continued with a new party organisation under the old name of "the Liberal Party". Meadowcroft joined the Liberal Democrats in 2007.

During the 19th century, the Liberal Party was broadly in favour of what would today be called classical liberalism: supporting "laissez-faire" economic policies such as free trade and minimal government interference in the economy (this doctrine was usually termed 'Gladstonian Liberalism' after the Victorian era Liberal Prime Minister William Ewart Gladstone). The Liberal Party favoured social reform, personal liberty, reducing the powers of the Crown and the Church of England (many of them were Nonconformists) and an extension of the electoral franchise. Sir William Harcourt, a prominent Liberal politician in the Victorian era, said this about liberalism in 1872:
If there be any party which is more pledged than another to resist a policy of restrictive legislation, having for its object social coercion, that party is the Liberal party. (Cheers.) But liberty does not consist in making others do what you think right, (Hear, hear.) The difference between a free Government and a Government which is not free is principally this—that a Government which is not free interferes with everything it can, and a free Government interferes with nothing except what it must. A despotic Government tries to make everybody do what it wishes; a Liberal Government tries, as far as the safety of society will permit, to allow everybody to do as he wishes. It has been the tradition of the Liberal party consistently to maintain the doctrine of individual liberty. It is because they have done so that England is the place where people can do more what they please than in any other country in the world...It is this practice of allowing one set of people to dictate to another set of people what they shall do, what they shall think, what they shall drink, when they shall go to bed, what they shall buy, and where they shall buy it, what wages they shall get and how they shall spend them, against which the Liberal party have always protested.

The political terms of "modern", "progressive" or "new" Liberalism began to appear in the mid to late 1880s and became increasingly common to denote the tendency in the Liberal Party to favour an increased role for the state as more important than the classical liberal stress on self-help and freedom of choice.

By the early 20th century the Liberals stance began to shift towards "New Liberalism", what would today be called social liberalism: a belief in personal liberty with a support for government intervention to provide minimum levels of welfare. This shift was best exemplified by the Liberal government of H. H. Asquith and his Chancellor David Lloyd George, whose Liberal reforms in the early 1900s created a basic welfare state.

David Lloyd George adopted a programme at the 1929 general election entitled "We Can Conquer Unemployment!", although by this stage the Liberals had declined to third-party status. The Liberals now (as expressed in the Liberal Yellow Book) regarded opposition to state intervention as being a characteristic of right-wing extremists.

After nearly becoming extinct in the 1940s and 50s, the Liberal Party revived its fortunes somewhat under the leadership of Jo Grimond in the 1960s, by positioning itself as a radical centrist non-socialist alternative to the Conservative and Labour Party governments of the time.

Since 1660, Nonconformist Protestants have played a major role in English politics. Relatively few MPs were dissenters. However the Dissenters were a major voting bloc in many areas, such as the East Midlands. They were very well organised and highly motivated and largely won over the Whigs and Liberals to their cause. Down to the 1830s, Dissenters demanded removal of political and civil disabilities that applied to them (especially those in the Test and Corporation Acts). The Anglican establishment strongly resisted until 1828. Numerous reforms of voting rights, especially that of 1832, increased the political power of Dissenters. They demanded an end to compulsory church rates, in which local taxes went only to Anglican churches. They finally achieved the end of religious tests for university degrees in 1905. Gladstone brought the majority of dissenters around to support for Home Rule for Ireland, putting the dissenting Protestants in league with the Irish Roman Catholics in an otherwise unlikely alliance. The dissenters gave significant support to moralistic issues, such as temperance and sabbath enforcement. The nonconformist conscience, as it was called, was repeatedly called upon by Gladstone for support for his moralistic foreign policy. In election after election, Protestant ministers rallied their congregations to the Liberal ticket. In Scotland, the Presbyterians played a similar role to the Nonconformist Methodists, Baptists and other groups in England and Wales.

By the 1820s the different Nonconformists, including Wesleyan Methodists, Baptists, Congregationalists, and Unitarians, had formed the Committee of Dissenting Deputies and agitated for repeal of the highly restrictive Test and Corporation Acts. These Acts excluded Nonconformists from holding civil or military office or attending Oxford or Cambridge, compelling them to set up their own Dissenting Academies privately.

The Tories tended to be in favour of these Acts and so the Nonconformist cause was linked closely to the Whigs, who advocated civil and religious liberty. After the Test and Corporation Acts were repealed in 1828, all the Nonconformists elected to Parliament were Liberals.

Nonconformists were angered by the Education Act 1902, which integrated Church of England denominational schools into the state system and provided for their support from taxes. John Clifford formed the National Passive Resistance Committee and by 1906 over 170 Nonconformists had gone to prison for refusing to pay school taxes. They included 60 Primitive Methodists, 48 Baptists, 40 Congregationalists, and 15 Wesleyan Methodists.

The political strength of Dissent faded sharply after 1920 with the secularisation of British society in the 20th century. The rise of the Labour Party reduced the Liberal Party strongholds into the nonconformist and remote "Celtic Fringe," where the party survived by an emphasis on localism and historic religious identity, thereby neutralising much of the class pressure on behalf of the Labour movement. Meanwhile, the Anglican church was a bastion of strength for the Conservative party. On the Irish issue, the Anglicans strongly supported unionism. Increasingly after 1850, the Roman Catholic element in England and Scotland was composed of recent immigrants from Ireland. They voted largely for the Irish Parliamentary Party, until its collapse in 1918.












</doc>
<doc id="4484" url="https://en.wikipedia.org/wiki?curid=4484" title="Bank of England">
Bank of England

The Bank of England, formally the Governor and Company of the Bank of England, is the central bank of the United Kingdom of Great Britain and Northern Ireland and the model on which most modern central banks have been based. Established in 1694, it is the second oldest central bank in operation today, after Sveriges Riksbank. The Bank of England is the world's 8th oldest bank. It was established to act as the English Government's banker and is still one of the bankers for the Government of the United Kingdom. The Bank was privately owned by stockholders from its foundation in 1694 until it was nationalised in 1946.

In 1998, it became an independent public organization, wholly owned by the Treasury Solicitor on behalf of the government, with independence in setting monetary policy.

The Bank is one of eight banks authorised to issue banknotes in the United Kingdom, but it has a monopoly on the issue of banknotes in England and Wales and regulates the issue of banknotes by commercial banks in Scotland and Northern Ireland.

The Bank's Monetary Policy Committee has a devolved responsibility for managing monetary policy. The Treasury has reserve powers to give orders to the committee "if they are required in the public interest and by extreme economic circumstances", but such orders must be endorsed by Parliament within 28 days. The Bank's Financial Policy Committee held its first meeting in June 2011 as a macro prudential regulator to oversee regulation of the UK's financial sector.

The Bank's headquarters have been in London's main financial district, the City of London, on Threadneedle Street, since 1734. It is sometimes known by the metonym "The Old Lady of Threadneedle Street" or "The Old Lady", a name taken from the satirical cartoon by James Gillray in 1797. The busy road junction outside is known as Bank junction.

As a regulator and central bank, the Bank of England has not offered consumer banking services for many years, but it still does manage some public-facing services such as exchanging superseded bank notes. Until 2016, the bank provided personal banking services as a popular privilege for employees.

England's crushing defeat by France, the dominant naval power, in naval engagements culminating in the 1690 Battle of Beachy Head, became the catalyst for England's rebuilding itself as a global power. England had no choice but to build a powerful navy. No public funds were available, and the credit of William III's government was so low in London that it was impossible for it to borrow the £1,200,000 (at 8% p.a.) that the government wanted.

To induce subscription to the loan, the subscribers were to be incorporated by the name of the Governor and Company of the Bank of England. The Bank was given exclusive possession of the government's balances, and was the only limited-liability corporation allowed to issue bank notes. The lenders would give the government cash (bullion) and issue notes against the government bonds, which can be lent again. The £1.2m was raised in 12 days; half of this was used to rebuild the navy.

As a side effect, the huge industrial effort needed, including establishing ironworks to make more nails and advances in agriculture feeding the quadrupled strength of the navy, started to transform the economy. This helped the new Kingdom of Great Britain – England and Scotland were formally united in 1707 – to become powerful. The power of the navy made Britain the dominant world power in the late 18th and early 19th centuries.

The establishment of the bank was devised by Charles Montagu, 1st Earl of Halifax, in 1694. The plan of 1691, which had been proposed by William Paterson three years before, had not then been acted upon. (It is worth noting though, that 58 years earlier, in 1636, Financier to the king Philip Burlamachi had proposed exactly the same idea in a letter addressed to Sir Francis Windebank.) He proposed a loan of £1.2m to the government; in return the subscribers would be incorporated as The Governor and Company of the Bank of England with long-term banking privileges including the issue of notes. The royal charter was granted on 27 July through the passage of the Tonnage Act 1694. Public finances were in such dire condition at the time that the terms of the loan were that it was to be serviced at a rate of 8% per annum, and there was also a service charge of £4,000 per annum for the management of the loan. The first governor was Sir John Houblon, who is depicted in the £50 note issued in 1994. The charter was renewed in 1742, 1764, and 1781.

The Bank's original home was in Walbrook, a street in the City of London, where during reconstruction in 1954 archaeologists found the remains of a Roman temple of Mithras (Mithras is – rather fittingly – said to have been worshipped as, amongst other things, the God of Contracts); the Mithraeum ruins are perhaps the most famous of all 20th-century Roman discoveries in the City of London and can be viewed by the public.

The Bank moved to its current location in Threadneedle Street in 1734, and thereafter slowly acquired neighbouring land to create the site necessary for erecting the Bank's original home at this location, under the direction of its chief architect Sir John Soane, between 1790 and 1827. (Sir Herbert Baker's rebuilding of the Bank in the first half of the 20th century, demolishing most of Soane's masterpiece, was described by architectural historian Nikolaus Pevsner as "the greatest architectural crime, in the City of London, of the twentieth century".) 

When the idea and reality of the National Debt came about during the 18th century, this was also managed by the Bank. During the American war of independence, business for the Bank was so good that George Washington remained a shareholder throughout the period. By the charter renewal in 1781 it was also the bankers' bank – keeping enough gold to pay its notes on demand until 26 February 1797 when war had so diminished gold reserves that – following an invasion scare caused by the Battle of Fishguard days earlier – the government prohibited the Bank from paying out in gold by the passing of the Bank Restriction Act 1797. This prohibition lasted until 1821.

The 1844 Bank Charter Act tied the issue of notes to the gold reserves and gave the Bank sole rights with regard to the issue of banknotes. Private banks that had previously had that right retained it, provided that their headquarters were outside London and that they deposited security against the notes that they issued. A few English banks continued to issue their own notes until the last of them was taken over in the 1930s. Scottish and Northern Irish private banks still have that right.

The bank acted as lender of last resort for the first time in the panic of 1866.

The last private bank in England to issue its own notes was Thomas Fox's Fox, Fowler and Company bank in Wellington, which rapidly expanded, until it merged with Lloyds Bank in 1927. They were legal tender until 1964. There are nine notes left in circulation; one is housed at Tone Dale House Wellington.

Britain remained on the gold standard until 1931 when the gold and foreign exchange reserves were transferred to the Treasury, but they continued to be managed by the Bank.

During the governorship of Montagu Norman, from 1920–44, the Bank made deliberate efforts to move away from commercial banking and become a central bank. In 1946, shortly after the end of Norman's tenure, the bank was nationalised by the Labour government.

After 1945 the Bank pursued the multiple goals of Keynesian economics, especially "easy money" and low interest rates to support aggregate demand. It tried to keep a fixed exchange rate, and attempted to deal with inflation and sterling weakness by credit and exchange controls.

In 1977, the Bank set up a wholly owned subsidiary called Bank of England Nominees Limited (BOEN), a private limited company, with two of its hundred £1 shares issued. According to its Memorandum & Articles of Association, its objectives are: "To act as Nominee or agent or attorney either solely or jointly with others, for any person or persons, partnership, company, corporation, government, state, organisation, sovereign, province, authority, or public body, or any group or association of them..." Bank of England Nominees Limited was granted an exemption by Edmund Dell, Secretary of State for Trade, from the disclosure requirements under Section 27(9) of the Companies Act 1976, because "it was considered undesirable that the disclosure requirements should apply to certain categories of shareholders." The Bank of England is also protected by its royal charter status, and the Official Secrets Act. BOEN is a vehicle for governments and heads of state to invest in UK companies (subject to approval from the Secretary of State), providing they undertake "not to influence the affairs of the company". BOEN is no longer exempt from company law disclosure requirements. Although a dormant company, dormancy does not preclude a company actively operating as a nominee shareholder. BOEN has two shareholders: the Bank of England, and the Secretary of the Bank of England.. 

In 1981 the reserve requirement for banks to hold a minimum fixed proportion of their deposits as reserves at the Bank of England was abolished: see reserve requirement for more details. The contemporary transition from Keynesian economics to Chicago economics was analysed by Kaldor in "The Scourge of Monetarism"

On 6 May 1997, following the 1997 general election which brought a Labour government to power for the first time since 1979, it was announced by the Chancellor of the Exchequer, Gordon Brown, that the Bank would be granted operational independence over monetary policy. Under the terms of the Bank of England Act 1998 (which came into force on 1 June 1998), the Bank's Monetary Policy Committee was given sole responsibility for setting interest rates to meet the Government's Retail Prices Index (RPI) inflation target of 2.5%. The target has changed to 2% since the Consumer Price Index (CPI) replaced the Retail Prices Index as the Treasury's inflation index. If inflation overshoots or undershoots the target by more than 1%, the Governor has to write a letter to the Chancellor of the Exchequer explaining why, and how he will remedy the situation.

The success of inflation targeting in the United Kingdom has been attributed to the Bank's focus on transparency. The Bank of England has been a leader in producing innovative ways of communicating information to the public, especially through its Inflation Report, which have been emulated by many other central banks.

Independent central banks that adopt an inflation target are known as Friedmanite central banks. Inflation targets combined with central bank independence have been characterised as a "starve the beast" strategy creating a lack of money in the public sector. This change in Labour's politics was described by Skidelsky in "The Return of the Master" as a mistake and as an adoption of the Rational Expectations Hypothesis as promulgated by Walters

The handing over of monetary policy to the Bank had been a key plank of the Liberal Democrats' economic policy since the 1992 general election. Conservative MP Nicholas Budgen had also proposed this as a private member's bill in 1996, but the bill failed as it had the support of neither the government nor the opposition.

Mark Carney assumed the post of Governor of the Bank of England on 1 July 2013. He succeeded Mervyn King, who took over on 30 June 2003. Carney, a Canadian, will serve an initial five-year term rather than the typical eight, and will seek UK citizenship. He is the first non-British citizen to hold the post. As of January 2014, the Bank also has four Deputy Governors.

BOEN was dissolved, following liquidation, in July 2017. 

There are two main areas which are tackled by the Bank to ensure it carries out these functions efficiently:

Note: It is important to note that "monetary" and "financial" are synonyms.

Stable prices and confidence in the currency are the two main criteria for monetary stability. Stable prices are maintained by seeking to ensure that price increases meet the Government's inflation target. The Bank aims to meet this target by adjusting the base interest rate, which is decided by the Monetary Policy Committee, and through its communications strategy, such as publishing yield curves.

The Bank works together with other institutions to secure both monetary and financial stability, including:
The 1997 memorandum of understanding describes the terms under which the Bank, the Treasury and the FSA work toward the common aim of increased financial stability. In 2010 the incoming Chancellor announced his intention to merge the FSA back into the Bank. As of 2012, the current director for financial stability is Andy Haldane.

The Bank acts as the government's banker, and it maintains the government's Consolidated Fund account. It also manages the country's foreign exchange and gold reserves. The Bank also acts as the bankers' bank, especially in its capacity as a lender of last resort.

The Bank has a monopoly on the issue of banknotes in England and Wales. Scottish and Northern Irish banks retain the right to issue their own banknotes, but they must be backed one for one with deposits at the Bank, excepting a few million pounds representing the value of notes they had in circulation in 1845. The Bank decided to sell its banknote printing operations to De La Rue in December 2002, under the advice of Close Brothers Corporate Finance Ltd.

Since 1998, the Monetary Policy Committee (MPC) has had the responsibility for setting the official interest rate. However, with the decision to grant the Bank operational independence, responsibility for government debt management was transferred in 1998 to the new Debt Management Office, which also took over government cash management in 2000. Computershare took over as the registrar for UK Government bonds (gilt-edged securities or "gilts") from the Bank at the end of 2004.

The Bank used to be responsible for the regulation and supervision of the banking and insurance industries. This responsibility was transferred to the Financial Services Authority in June 1998, but after the financial crises in 2008 new banking legislation transferred the responsibility for regulation and supervision of the banking and insurance industries back to the Bank.

In 2011 the interim Financial Policy Committee (FPC) was created as a mirror committee to the MPC to spearhead the Bank's new mandate on financial stability. The FPC is responsible for macro prudential regulation of all UK banks and insurance companies.

To help maintain economic stability, the Bank attempts to broaden understanding of its role, both through regular speeches and publications by senior Bank figures, a semiannual Financial Stability Report, and through a wider education strategy aimed at the general public. It maintains a free museum and runs the Target Two Point Zero competition for A-level students.

The Bank has operated, since January 2009, an Asset Purchase Facility (APF) to buy "high-quality assets financed by the issue of Treasury bills and the DMO's cash management operations" and thereby improve liquidity in the credit markets. It has, since March 2009, also provided the mechanism by which the Bank's policy of quantitative easing (QE) is achieved, under the auspices of the MPC. Along with the managing the £200 billion of QE funds, the APF continues to operate its corporate facilities. Both are undertaken by a subsidiary company of the Bank of England, the Bank of England Asset Purchase Facility Fund Limited (BEAPFF).

The Bank has issued banknotes since 1694. Notes were originally hand-written; although they were partially printed from 1725 onwards, cashiers still had to sign each note and make them payable to someone. Notes were fully printed from 1855. Until 1928 all notes were "White Notes", printed in black and with a blank reverse. In the 18th and 19th centuries White Notes were issued in £1 and £2 denominations. During the 20th century White Notes were issued in denominations between £5 and £1000.

Until the mid-19th century, commercial banks were allowed to issue their own banknotes, and notes issued by provincial banking companies were commonly in circulation. The Bank Charter Act 1844 began the process of restricting note issue to the Bank; new banks were prohibited from issuing their own banknotes and existing note-issuing banks were not permitted to expand their issue. As provincial banking companies merged to form larger banks, they lost their right to issue notes, and the English private banknote eventually disappeared, leaving the Bank with a monopoly of note issue in England and Wales. The last private bank to issue its own banknotes in England and Wales was Fox, Fowler and Company in 1921. However, the limitations of the 1844 Act only affected banks in England and Wales, and today three commercial banks in Scotland and four in Northern Ireland continue to issue their own banknotes, regulated by the Bank.

At the start of the First World War, the Currency and Bank Notes Act 1914 was passed, which granted temporary powers to HM Treasury for issuing banknotes to the values of £1 and 10/- (ten shillings). Treasury notes had full legal tender status and were not convertible into gold through the Bank; they replaced the gold coin in circulation to prevent a run on sterling and to enable raw material purchases for armament production. These notes featured an image of King George V (Bank of England notes did not begin to display an image of the monarch until 1960). The wording on each note was:

Treasury notes were issued until 1928, when the Currency and Bank Notes Act 1928 returned note-issuing powers to the banks. The Bank of England issued notes for ten shillings and one pound for the first time on 22 November 1928.

During the Second World War the German Operation Bernhard attempted to counterfeit denominations between £5 and £50, producing 500,000 notes each month in 1943. The original plan was to parachute the money into the UK in an attempt to destabilise the British economy, but it was found more useful to use the notes to pay German agents operating throughout Europe. Although most fell into Allied hands at the end of the war, forgeries frequently appeared for years afterwards, which led banknote denominations above £5 to be removed from circulation.

In 2006, over £53 million in banknotes belonging to the Bank was stolen from a depot in Tonbridge, Kent.

Modern banknotes are printed by contract with De La Rue Currency in Loughton, Essex.

The Bank is custodian to the official gold reserves of the United Kingdom and around 30 other countries. The vault, beneath the City of London, covers a floor space greater than that of the third-tallest building in the City, Tower 42, and needs keys that are long to open. As of April 2016, the Bank held around 400,000 bars, which is equivalent to 5,134 tonnes of gold. These gold deposits were estimated in July 2017 to have a current market value of £142,000,000,000. These estimates suggest the vault could hold as much as 3% of the gold mined throughout human history.

Following is a list of the Governors of the Bank of England since the beginning of the 20th century:

The Court of Directors is a unitary board that is responsible for setting the organisation's strategy and budget and taking key decisions on resourcing and appointments. It consists of five executive members from the Bank plus up to 9 non-executive members, all of whom are appointed by the Crown. The Chancellor selects the Chairman of the Court from among one of the non-executive members. The Court is required to meet at least 7 times a year. 

The Governor serves for a period of eight years, the Deputy Governors for five years, and the non-executive members for up to four years.

Since 2013, the Bank has had a chief operating officer (COO). , the Bank's COO has been Charlotte Hogg.

, the Bank's chief economist is Andrew Haldane.

Representing the Bullionist perspective, Economist David Ricardo argued that the Bank of England caused inflation and depreciation of the pound by over-issuing banknotes to purchase government securities because of the 1797 suspension of convertibility of pound into specie. The convertibility was suspended to prevent widespread conversion of banknotes into specie as the likelihood of war between England and France grew and fears of a French invasion increased. Before 1797, banknotes issued by the Bank of England were required to be convertible into specie. In the end, Ricardo won with his recommendation to return to convertibility to prevent further conflicts and to ensure price stability. The suspension remained in effect until 1821. After convertibility was reinstated in 1821, the discussion shifted to the Currency-Banking Controversy.

Mark Carney and the Bank were criticised by Jacob Rees-Mogg in the run-up to and after the referendum, for their 'unreserved support' for remain during the United Kingdom European Union membership referendum 





</doc>
<doc id="4485" url="https://en.wikipedia.org/wiki?curid=4485" title="Bakelite">
Bakelite

Bakelite ( ; sometimes spelled Baekelite), or polyoxybenzylmethylenglycolanhydride, is the first plastic made from synthetic components. It is a thermosetting phenol formaldehyde resin, formed from a condensation reaction of phenol with formaldehyde. It was developed by the Belgian-American chemist Leo Baekeland in Yonkers, New York, in 1907.

Bakelite was patented on December 7, 1909. The creation of a synthetic plastic was revolutionary for its electrical nonconductivity and heat-resistant properties in electrical insulators, radio and telephone casings and such diverse products as kitchenware, jewelry, pipe stems, children's toys, and firearms. The "retro" appeal of old Bakelite products has made them collectible.

Bakelite was designated a National Historic Chemical Landmark on November 9, 1993, by the American Chemical Society in recognition of its significance as the world's first synthetic plastic.

Baekeland was already wealthy due to his invention of Velox photographic paper when he began to investigate the reactions of phenol and formaldehyde in his home laboratory. Chemists had begun to recognize that many natural resins and fibers were polymers. Baekeland's initial intent was to find a replacement for shellac, a material in limited supply because it was made naturally from the excretion of lac insects (specifically "Kerria lacca"). Baekeland produced a soluble phenol-formaldehyde shellac called "Novolak", but it was not a market success.

Baekeland then began experimenting on strengthening wood by impregnating it with a synthetic resin, rather than coating it. By controlling the pressure and temperature applied to phenol and formaldehyde, Baekeland produced a hard moldable material which he named "Bakelite", after himself. It was the first synthetic thermosetting plastic ever produced, and Baekeland speculated on "the thousand and one ... articles" it could be used to make. Baekeland considered the possibilities of using a wide variety of filling materials, including; cotton, powdered bronze, and slate dust, but was most successful with wood and asbestos fibers.

Baekeland filed a substantial number of patents in the area. Bakelite, his "Method of making insoluble products of phenol and formaldehyde," was filed on July 13, 1907, and granted on December 7, 1909. Baekeland also filed for patent protection in other countries, including Belgium, Canada, Denmark, Hungary, Japan, Mexico, Russia, and Spain. He announced his invention at a meeting of the American Chemical Society on February 5, 1909.

Baekeland started semi-commercial production of his new material in his home laboratory, marketing it as a material for electrical insulators. By 1910, he was producing enough material to justify expansion. He formed the General Bakelite Company as a U.S. company to manufacture and market his new industrial material. He also made overseas connections to produce materials in other countries.

Bijker gives a detailed discussion of the development of Bakelite and the Bakelite Company's production of various applications of materials. As of 1911, the company's main focus was laminating varnish, whose sales volume vastly outperformed both molding material and cast resin. By 1912, molding material was gaining ground, but its sales volume for the company did not exceed that of laminating varnish until the 1930s.

As the sales figures also show, the Bakelite Company produced "Transparent" cast resin (which did not include filler) for a small ongoing market during the 1910s and 1920s. Blocks or rods of cast resin, also known as "artificial amber", were machined and carved to create items such as pipe stems, cigarette holders and jewelry. However, the demand for molded plastics led the Bakelite Company to concentrate on molding, rather than concentrating on cast solid resins.

The Bakelite Corporation was formed in 1922 after patent litigation favorable to Baekeland, from a merger of three companies: Baekeland's General Bakelite Company; the Condensite Company, founded by J.W. Aylesworth; and the Redmanol Chemical Products Company, founded by L.V. Redman. Under director of advertising and public relations Allan Brown, who came to Bakelite from Condensite, Bakelite was aggressively marketed as "The Material of A Thousand Uses". A filing for a trademark featuring the letter B above the mathematical symbol for infinity was made August 25, 1925, and claimed the mark was in use as of December 1, 1924. A wide variety of uses were listed in their trademark applications.

The first issue of "Plastics" magazine, October 1925, featured Bakelite on its cover, and included the article "Bakelite – What It Is" by Allan Brown. The range of colors available included "black, brown, red, yellow, green, gray, blue, and blends of two or more of these". The article emphasized that Bakelite came in various forms. "Bakelite is manufactured in several forms to suit varying requirements. In all these forms the fundamental basis is the initial Bakelite resin. This variety includes Clear Material, for jewelry, smokers' articles, etc.; cement, using in sealing electric light bulbs in metal bases; varnishes, for impregnating electric coils, etc.; lacquers, for protecting the surface of hardware; enamels, for giving resistive coating to industrial equipment; Laminated Bakelite, used for silent gears and insulation; and molding material, from which are formed innumerable articles of utility and beauty. The Molding Material is prepared ordinarily by the impregnation of cellulose substances with the initial 'uncured' resin." In a 1925 report, the United States Tariff Commission hailed the commercial manufacture of synthetic phenolic resin as "distinctly an American achievement", and noted that "the publication of figures, however, would be a virtual disclosure of the production of an individual company".

In England, Bakelite Limited, a merger of three British phenol formaldehyde resin suppliers (Damard Lacquer Company Limited of Birmingham, Mouldensite Limited of Darley Dale and Redmanol Chemical Products Company of London), was formed in 1926. A new Bakelite factory opened in Tyseley, Birmingham, around 1928. It was demolished in 1998.

A new factory was opened in Bound Brook, New Jersey, in 1931. In 1939, the companies were acquired by Union Carbide and Carbon Corporation. Union Carbide's phenolic resin business including the Bakelite and Bakelit registered trademarks are assigned to Hexion Inc.

In addition to the original Bakelite material, these companies eventually made a wide range of other products, many of which were marketed under the brand name "Bakelite plastics". These included other types of cast phenolic resins similar to Catalin, and urea-formaldehyde resins, which could be made in brighter colors than polyoxybenzylmethyleneglycolanhydride.

Once Baekeland's heat and pressure patents expired in 1927, Bakelite Corporation faced serious competition from other companies. Because molded Bakelite incorporated fillers to give it strength, it tended to be made in concealing dark colors. In 1927, beads, bangles and earrings were produced by the Catalin Company, through a different process which enabled them to introduce 15 new colors. Translucent jewelry, poker chips and other items made of phenolic resins were introduced in the 1930s or 1940s by the Catalin Company under the Prystal name. The creation of marbled phenolic resins may also be attributable to the Catalin Company.

Making Bakelite was a multi-stage process. It began with the heating of phenol and formaldehyde in the presence of a catalyst such as hydrochloric acid, zinc chloride, or the base ammonia. This created a liquid condensation product, referred to as "Bakelite A", which was soluble in alcohol, acetone, or additional phenol. Heated further, the product became partially soluble and could still be softened by heat. Sustained heating resulted in an "insoluble hard gum". However, the high temperatures required to create this tended to cause violent foaming of the mixture, which resulted in the cooled material being porous and breakable. Baekeland's innovative step was to put his "last condensation product" into an egg-shaped "Bakelizer". By heating it under pressure, at about , Baekeland was able to suppress the foaming that would otherwise occur. The resulting substance was extremely hard and both infusible and insoluble. 

Molded Bakelite forms in a condensation reaction of phenol and formaldehyde, with wood flour or asbestos fiber as a filler, under high pressure and heat in a time frame of a few minutes of curing. The result is a hard plastic material.

Bakelite's molding process had a number of advantages. Bakelite resin could be provided either as powder, or as preformed partially cured slugs, increasing the speed of the casting. Thermosetting resins such as Bakelite required heat and pressure during the molding cycle, but could be removed from the molding process without being cooled, again making the molding process faster. Also, because of the smooth polished surface that resulted, Bakelite objects required less finishing. Millions of parts could be duplicated quickly and relatively cheaply.

Another market for Bakelite resin was the creation of phenolic sheet materials. Phenolic sheet is a hard, dense material made by applying heat and pressure to layers of paper or glass cloth impregnated with synthetic resin. Paper, cotton fabrics, synthetic fabrics, glass fabrics and unwoven fabrics are all possible materials used in lamination. When heat and pressure are applied, polymerization transforms the layers into thermosetting industrial laminated plastic.

Bakelite phenolic sheet is produced in many commercial grades and with various additives to meet diverse mechanical, electrical and thermal requirements. Some common types include:

Bakelite has a number of important properties. It can be molded very quickly, decreasing production time. Moldings are smooth, retain their shape and are resistant to heat, scratches, and destructive solvents. It is also resistant to electricity, and prized for its low conductivity. It is not flexible.

Phenolic resin products may swell slightly under conditions of extreme humidity or perpetual dampness. When rubbed or burnt, Bakelite has a distinctive, acrid, sickly-sweet or fishy odor.

These characteristics made Bakelite particularly suitable as a molding compound, an adhesive or binding agent, a varnish, and as a protective coating. Bakelite was particularly suitable for the emerging electrical and automobile industries because of its extraordinarily high resistance to electricity, heat and chemical action.

The earliest commercial use of Bakelite in the electrical industry was the molding of tiny insulating bushings, made in 1908 for the Weston Electrical Instrument Corporation by Richard W. Seabury of the Boonton Rubber Company. Bakelite was soon used for non-conducting parts of telephones, radios and other electrical devices, including bases and sockets for light bulbs and electron tubes, supports for any type of electrical components, automobile distributor caps and other insulators.
By 1912, it was being used to make billiard balls, since its elasticity and the sound it made were similar to ivory.

During World War I, Bakelite was used widely, particularly in electrical systems. Important projects included the Liberty Motor, the wireless telephone and radio phone and the use of micarta-bakelite propellors in the NBS-1 bomber and the DH-4B aeroplane.

Bakelite's availability and ease and speed of molding helped to lower the costs and increase product availability so that both telephones and radios became common household consumer goods. It was also very important to the developing automobile industry. It was soon found in myriad other consumer products ranging from pipe stems and buttons to saxophone mouthpieces, cameras, early machine guns, and appliance casings.

Beginning in the 1920s, it became a popular material for jewelry. Designer Coco Chanel included Bakelite bracelets in her costume jewelry collections. Designers such as Elsa Schiaparelli used it for jewelry and also for specially designed dress buttons. Later, Diana Vreeland, editor of Vogue, was enthusiastic about Bakelite. Bakelite was also used to make presentation boxes for Breitling watches. Jewelry designers such as Jorge Caicedo Montes De Oca still use vintage Bakelite materials to make designer jewelry.

By 1930, designer Paul T. Frankl considered Bakelite a "Materia Nova", "expressive of our own age". By the 1930s, Bakelite was used for game pieces like chessmen, poker chips, dominoes and mahjong sets. Kitchenware made with Bakelite, including canisters and tableware, was promoted for its resistance to heat and to chipping. In the mid-1930s, Northland marketed a line of skis with a black "Ebonite" base, a coating of Bakelite. By 1935, it was used in solid-body electric guitars. Performers such as Jerry Byrd loved the tone of Bakelite guitars but found them difficult to keep in tune.

The British children's construction toy Bayko, launched in 1933, originally used Bakelite for many of its parts, and took its name from the material.

During World War II, Bakelite was used in a variety of wartime equipment including pilot's goggles and field telephones. It was also used for patriotic wartime jewelry. In 1943, the thermosetting phenolic resin was even considered for the manufacture of coins, due to a shortage of traditional material. Bakelite and other non-metal materials were tested for usage for the one cent coin in the US before the Mint settled on zinc-coated steel.

In 1947, Dutch art forger Han van Meegeren was convicted of forgery, after chemist and curator Paul B. Coremans proved that a purported Vermeer contained Bakelite, which van Meegeren had used as a paint hardener.

Bakelite was sometimes used as a substitute for metal in the magazine, pistol grip, fore grip, hand guard, and butt stock of firearms. The AKM and some early AK-74 rifles are frequently mistakenly identified as using Bakelite, but most were made with AG-S4.

By the late 1940s, newer materials were superseding Bakelite in many areas. Phenolics are less frequently used in general consumer products today due to their cost and complexity of production and their brittle nature. They still appear in some applications where their specific properties are required, such as small precision-shaped components, molded disc brake cylinders, saucepan handles, electrical plugs, switches and parts for electrical irons, as well as in the area of inexpensive board and tabletop games produced in China, Hong Kong and India. Items such as billiard balls, dominoes and pieces for board games such as chess, checkers, and backgammon are constructed of Bakelite for its look, durability, fine polish, weight, and sound. Common dice are sometimes made of Bakelite for weight and sound, but the majority are made of a thermoplastic polymer such as acrylonitrile butadiene styrene (ABS).
Bakelite continues to be used for wire insulation, brake pads and related automotive components, and industrial electrical-related applications. Bakelite stock is still manufactured and produced in sheet, rod and tube form for industrial applications in the electronics, power generation and aerospace industries, and under a variety of commercial brand names.

Phenolic resins have been commonly used in ablative heat shields. Soviet heatshields for ICBM warheads and spacecraft reentry consisted of asbestos textolite, impregnated with Bakelite. Bakelite is also used in the mounting of metal samples in metallography.

Bakelite items, particularly jewelry and radios, have become a popular collectible. The term "Bakelite" is sometimes used in the resale market to indicate various types of early plastics, including Catalin and Faturan, which may be brightly colored, as well as items made of Bakelite material.

The United States Patent and Trademark Office granted Baekeland a patent for a "Method of making insoluble products of phenol and formaldehyde" on December 7, 1909. Producing hard, compact, insoluble and infusible condensation products of phenols and formaldehyde marked the beginning of the modern plastics industry.





</doc>
<doc id="4487" url="https://en.wikipedia.org/wiki?curid=4487" title="Bean">
Bean

A bean is a seed of one of several genera of the flowering plant family Fabaceae, which are used for human or animal food.

The word "bean" and its Germanic cognates (e.g., "Bohne") have existed in common use in West Germanic languages since before the 12th century, referring to broad beans and other pod-borne seeds. This was long before the New World genus "Phaseolus" was known in Europe. After Columbian-era contact between Europe and the Americas, use of the word was extended to pod-borne seeds of "Phaseolus", such as the common bean and the runner bean, and the related genus "Vigna". The term has long been applied generally to many other seeds of similar form, such as Old World soybeans, peas, chickpeas (garbanzo beans), other vetches, and lupins, and even to those with slighter resemblances, such as coffee beans, vanilla beans, castor beans, and cocoa beans. Thus the term "bean" in general usage can mean a host of different species.

Seeds called "beans" are often included among the crops called "pulses" (legumes), although a narrower prescribed sense of "pulses" reserves the word for leguminous crops harvested for their dry grain. The term "bean" usually excludes legumes with tiny seeds and which are used exclusively for forage, hay, and silage purposes (such as clover and alfalfa). According to the United Nations Food and Agriculture Organization the term "bean" should include only species of "Phaseolus"; however, enforcing that prescription has proven difficult for several reasons. One is that in the past, several species, including "Vigna angularis" (adzuki bean), "mungo" (black gram), "radiata" (green gram), and "aconitifolia" (moth bean), were classified as "Phaseolus" and later reclassified. Another is that it is not surprising that the prescription on limiting the use of the word, because it tries to replace the word's older senses with a newer one, has never been consistently followed in general usage.

Unlike the closely related pea, beans are a summer crop that need warm temperatures to grow. Maturity is typically 55–60 days from planting to harvest. As the bean pods mature, they turn yellow and dry up, and the beans inside change from green to their mature colour. As a vine, bean plants need external support, which may be provided in the form of special "bean cages" or poles. Native Americans customarily grew them along with corn and squash (the so-called Three Sisters), with the tall cornstalks acting as support for the beans.

In more recent times, the so-called "bush bean" has been developed which does not require support and has all its pods develop simultaneously (as opposed to pole beans which develop gradually). This makes the bush bean more practical for commercial production.

Beans are one of the longest-cultivated plants. Broad beans, also called fava beans, in their wild state the size of a small fingernail, were gathered in Afghanistan and the Himalayan foothills. In a form improved from naturally occurring types, they were grown in Thailand since the early seventh millennium BCE, predating ceramics. They were deposited with the dead in ancient Egypt. Not until the second millennium BCE did cultivated, large-seeded broad beans appear in the Aegean, Iberia and transalpine Europe. In the "Iliad" (8th century BCE) is a passing mention of beans and chickpeas cast on the threshing floor.

Beans were an important source of protein throughout Old and New World history, and still are today.

The oldest-known domesticated beans in the Americas were found in Guitarrero Cave, an archaeological site in Peru, and dated to around the second millennium BCE. However, genetic analyses of the common bean "Phaseolus" shows that it originated in Mesoamerica, and subsequently spread southward, along with maize and squash, traditional companion crops.

Most of the kinds commonly eaten fresh or dried, those of the genus "Phaseolus", come originally from the Americas, being first seen by a European when Christopher Columbus, during his exploration of what may have been the Bahamas, found them growing in fields. Five kinds of "Phaseolus" beans were domesticated by pre-Columbian peoples: common beans ("Phaseolus vulgaris") grown from Chile to the northern part of what is now the United States, and lima and sieva beans ("Phaseolus lunatus"), as well as the less widely distributed teparies ("Phaseolus acutifolius"), scarlet runner beans ("Phaseolus coccineus") and polyanthus beans ("Phaseolus polyanthus") One especially famous use of beans by pre-Columbian people as far north as the Atlantic seaboard is the "Three Sisters" method of companion plant cultivation:

Dry beans come from both Old World varieties of broad beans (fava beans) and New World varieties (kidney, black, cranberry, pinto, navy/haricot).

Beans are a heliotropic plant, meaning that the leaves tilt throughout the day to face the sun. At night, they go into a folded "sleep" position.

Currently, the world genebanks hold about 40,000 bean varieties, although only a fraction are mass-produced for regular consumption.
Some bean types include:



Some kinds of raw beans contain a harmful tasteless toxin, lectin phytohaemagglutinin, that must be removed by cooking. Red kidney beans are particularly toxic, but other types also pose risks of food poisoning. A recommended method is to boil the beans for at least ten minutes; undercooked beans may be more toxic than raw beans.

Cooking beans, without bringing them to the boil, in a slow cooker at a temperature well below boiling may not destroy toxins. A case of poisoning by butter beans used to make falafel was reported; the beans were used instead of traditional broad beans or chickpeas, soaked and ground without boiling, made into patties, and shallow fried.

Bean poisoning is not well known in the medical community, and many cases may be misdiagnosed or never reported; figures appear not to be available. In the case of the UK National Poisons Information Service, available only to health professionals, the dangers of beans other than red beans were not flagged .

Fermentation is used in some parts of Africa to improve the nutritional value of beans by removing toxins. Inexpensive fermentation improves the nutritional impact of flour from dry beans and improves digestibility, according to research co-authored by Emire Shimelis, from the Food Engineering Program at Addis Ababa University. Beans are a major source of dietary protein in Kenya, Malawi, Tanzania, Uganda and Zambia.

It is common to make beansprouts by letting some types of bean, often mung beans, germinate in moist and warm conditions; beansprouts may be used as ingredients in cooked dishes, or eaten raw or lightly cooked. There have been many outbreaks of disease from bacterial contamination, often by "salmonella", "listeria", and "Escherichia coli", of beansprouts not thoroughly cooked, some causing significant mortality.

Many types of bean contain significant amounts of antinutrients that inhibit some enzyme processes in the body. Phytic acid and phytates, present in grains, nuts, seeds and beans, interfere with bone growth and interrupt vitamin D metabolism. Pioneering work on the effect of phytic acid was done by Edward Mellanby from 1939.

Beans are high in protein, complex carbohydrates, folate, and iron. Beans also have significant amounts of fiber and soluble fiber, with one cup of cooked beans providing between nine and 13 grams of fiber. Soluble fiber can help lower blood cholesterol.
Consuming beans adds significant amounts of fiber and soluble fiber to a diet, with one cup of cooked beans providing between nine and thirteen grams of fiber. Soluble fiber can help lower blood cholesterol. Adults are recommended to have up to two (female), and three (male) servings. 3/4 cup of cooked beans provide one serving.

Many edible beans, including broad beans and soybeans, contain oligosaccharides (particularly raffinose and stachyose), a type of sugar molecule also found in cabbage. An anti-oligosaccharide enzyme is necessary to properly digest these sugar molecules. As a normal human digestive tract does not contain any anti-oligosaccharide enzymes, consumed oligosaccharides are typically digested by bacteria in the large intestine. This digestion process produces flatulence-causing gases as a byproduct. Since sugar dissolves in water, another method of reducing flatulence associated with eating beans is to drain the water in which the beans have been cooked.

Some species of mold produce alpha-galactosidase, an anti-oligosaccharide enzyme, which humans can take to facilitate digestion of oligosaccharides in the small intestine. This enzyme, currently sold in the United States under the brand-names Beano and Gas-X Prevention, can be added to food or consumed separately. In many cuisines beans are cooked along with natural carminatives such as anise seeds, coriander seeds and cumin.

One effective strategy is to soak beans in alkaline (baking soda) water overnight before rinsing thoroughly. Sometimes vinegar is added, but only after the beans are cooked as vinegar interferes with the beans' softening.

Fermented beans will usually not produce most of the intestinal problems that unfermented beans will, since yeast can consume the offending sugars.

The world leader in production of Dry Beans (Phaseolus spp). is Myanmar (Burma), followed by India and Brazil. In Africa, the most important producer is Tanzania.




</doc>
<doc id="4489" url="https://en.wikipedia.org/wiki?curid=4489" title="Breast">
Breast

The breast is one of two prominences located on the upper ventral region of the torso of primates. In females, it serves as the mammary gland, which produces and secretes milk to feed infants. Both females and males develop breasts from the same embryological tissues. At puberty, estrogens, in conjunction with growth hormone, cause breast development in female humans and to a much lesser extent in other primates. Breast development in other primate females generally only occurs with pregnancy.

Subcutaneous fat covers and envelops a network of ducts that converge on the nipple, and these tissues give the breast its size and shape. At the ends of the ducts are lobules, or clusters of alveoli, where milk is produced and stored in response to hormonal signals. During pregnancy, the breast responds to a complex interaction of hormones, including estrogens, progesterone, and prolactin, that mediate the completion of its development, namely lobuloalveolar maturation, in preparation of lactation and breastfeeding.

Along with their major function in providing nutrition for infants, female breasts have social and sexual characteristics. Breasts have been featured in notable ancient and modern sculpture, art, and photography. They can figure prominently in a woman's perception of her body and sexual attractiveness. A number of Western cultures associate breasts with sexuality and tend to regard bare breasts in public as immodest or indecent. Breasts, especially the nipples, are an erogenous zone.

The English word "breast" derives from the Old English word "brēost" (breast, bosom) from Proto-Germanic "breustam" (breast), from the Proto-Indo-European base bhreus– (to swell, to sprout). The "breast" spelling conforms to the Scottish and North English dialectal pronunciations. The "Merriam-Webster Dictionary" states that "Middle English brest, [comes] from Old English brēost; akin to Old High German brust..., Old Irish brú [belly], [and] Russian bryukho"; the first known usage of the term was before the 12th century.

A large number of colloquial terms for breasts are used in English, ranging from fairly polite terms to vulgar or slang. Some vulgar slang expressions may be considered to be derogatory or sexist to women.

In women, the breasts overlie the pectoralis major muscles and usually extend from the level of the second rib to the level of the sixth rib in the front of the human rib cage; thus, the breasts cover much of the chest area and the chest walls. At the front of the chest, the breast tissue can extend from the clavicle (collarbone) to the middle of the sternum (breastbone). At the sides of the chest, the breast tissue can extend into the axilla (armpit), and can reach as far to the back as the latissimus dorsi muscle, extending from the lower back to the humerus bone (the longest bone of the upper arm). As a mammary gland, the breast is composed of differing layers of tissue, predominantly two types: adipose tissue; and glandular tissue, which affects the lactation functions of the breasts.
Morphologically the breast is tear-shaped. The superficial tissue layer (superficial fascia) is separated from the skin by 0.5–2.5 cm of subcutaneous fat (adipose tissue). The suspensory Cooper's ligaments are fibrous-tissue prolongations that radiate from the superficial fascia to the skin envelope. The female adult breast contains 14–18 irregular lactiferous lobes that converge at the nipple. The 2.0–4.5 mm milk ducts are immediately surrounded with dense connective tissue that support the glands. Milk exits the breast through the nipple, which is surrounded by a pigmented area of skin called the areola. The size of the areola can vary widely among women. The areola contains modified sweat glands known as Montgomery's glands. These glands secrete oily fluid that lubricate and protect the nipple during breastfeeding. Volatile compounds in these secretions may also serve as an olfactory stimulus for the newborn's appetite.

The dimensions and weight of the breast vary widely among women. A small-to-medium-sized breast weighs 500 grams (1.1 pounds) or less, and a large breast can weigh approximately 750 to 1,000 grams (1.7 to 2.2 pounds) or more. The tissue composition ratios of the breast also vary among women. Some women's breasts have varying proportions of glandular tissue than of adipose or connective tissues. The fat-to-connective-tissue ratio determines the density or firmness of the breast. During a woman's life, her breasts change size, shape, and weight due to hormonal changes during puberty, the menstrual cycle, pregnancy, breastfeeding, and menopause.

The breast is an apocrine gland that produces the milk used to feed an infant. The nipple of the breast is surrounded by the areola (nipple-areola complex). The areola has many sebaceous glands, and the skin color varies from pink to dark brown. The basic units of the breast are the terminal duct lobular units (TDLUs), which produce the fatty breast milk. They give the breast its offspring-feeding functions as a mammary gland. They are distributed throughout the body of the breast. Approximately two-thirds of the lactiferous tissue is within 30 mm of the base of the nipple. The terminal lactiferous ducts drain the milk from TDLUs into 4–18 lactiferous ducts, which drain to the nipple. The milk-glands-to-fat ratio is 2:1 in a lactating woman, and 1:1 in a non-lactating woman. In addition to the milk glands, the breast is also composed of connective tissues (collagen, elastin), white fat, and the suspensory Cooper's ligaments. Sensation in the breast is provided by the peripheral nervous system innervation by means of the front (anterior) and side (lateral) cutaneous branches of the fourth-, fifth-, and sixth intercostal nerves. The T-4 nerve (Thoracic spinal nerve 4), which innervates the dermatomic area, supplies sensation to the nipple-areola complex.

Approximately 75% of the lymph from the breast travels to the axillary lymph nodes on the same side of the body, whilst 25% of the lymph travels to the parasternal nodes (beside the sternum bone). A small amount of remaining lymph travels to the other breast and to the abdominal lymph nodes. The axillary lymph nodes include the pectoral (chest), subscapular (under the scapula), and humeral (humerus-bone area) lymph-node groups, which drain to the central axillary lymph nodes and to the apical axillary lymph nodes. The lymphatic drainage of the breasts is especially relevant to oncology because breast cancer is common to the mammary gland, and cancer cells can metastasize (break away) from a tumour and be dispersed to other parts of the body by means of the lymphatic system.

The morphologic variations in the size, shape, volume, tissue density, pectoral locale, and spacing of the breasts determine their natural shape, appearance, and position on a woman's chest. Breast size and other characteristics do not predict the fat-to-milk-gland ratio or the potential for the woman to nurse an infant. The size and the shape of the breasts are influenced by normal-life hormonal changes (thelarche, menstruation, pregnancy, menopause) and medical conditions (e.g. virginal breast hypertrophy). The shape of the breasts is naturally determined by the support of the suspensory Cooper's ligaments, the underlying muscle and bone structures of the chest, and by the skin envelope. The suspensory ligaments sustain the breast from the clavicle (collarbone) and the clavico-pectoral fascia (collarbone and chest) by traversing and encompassing the fat and milk-gland tissues. The breast is positioned, affixed to, and supported upon the chest wall, while its shape is established and maintained by the skin envelope. In most women, one breast is slightly larger than the other. More obvious and persistent asymmetry in breast size occurs in up to 25% of women.

While it has been a common belief that breastfeeding causes breasts to sag, researchers have found that a woman's breasts sag due to four key factors: cigarette smoking, number of pregnancies, gravity, and weight loss or gain.

The base of each breast is attached to the chest by the deep fascia over the pectoralis major muscles. The space between the breast and the pectoralis major muscle, called retromammary space, gives mobility to the breast. 
The chest (thoracic cavity) progressively slopes outwards from the thoracic inlet (atop the breastbone) and above to the lowest ribs that support the breasts. The inframammary fold, where the lower portion of the breast meets the chest, is an anatomic feature created by the adherence of the breast skin and the underlying connective tissues of the chest; the IMF is the lower-most extent of the anatomic breast. Normal breast tissue typically has a texture that feels nodular or granular, to an extent that varies considerably from woman to woman.
The study "The Evolution of the Human Breast" (2001) proposed that the rounded shape of a woman's breast evolved to prevent the sucking infant offspring from suffocating while feeding at the teat; that is, because of the human infant's small jaw, which did not project from the face to reach the nipple, he or she might block the nostrils against the mother's breast if it were of a flatter form (cf. chimpanzee). Theoretically, as the human jaw receded into the face, the woman's body compensated with round breasts.

The breasts are principally composed of adipose, glandular, and connective tissues. Because these tissues have hormone receptors, their sizes and volumes fluctuate according to the hormonal changes particular to thelarche (sprouting of breasts), menstruation (egg production), pregnancy (reproduction), lactation (feeding of offspring), and menopause (end of menstruation).

The morphological structure of the human breast is identical in males and females until puberty. For pubescent girls in thelarche (the breast-development stage), the female sex hormones (principally estrogens) in conjunction with growth hormone promote the sprouting, growth, and development of the breasts. During this time, the mammary glands grow in size and volume and begin resting on the chest. These development stages of secondary sex characteristics (breasts, pubic hair, etc.) are illustrated in the five-stage Tanner Scale.

During thelarche, the developing breasts are sometimes of unequal size, and usually the left breast is slightly larger. This condition of asymmetry is transitory and statistically normal in female physical and sexual development. Medical conditions can cause overdevelopment (e.g., virginal breast hypertrophy, macromastia) or underdevelopment (e.g., tuberous breast deformity, micromastia) in girls and women.

Approximately two years after the onset of puberty (a girl's first menstrual cycle), estrogen and growth hormone stimulate the development and growth of the glandular fat and suspensory tissues that compose the breast. This continues for approximately four years until the final shape of the breast (size, volume, density) is established at about the age of 21. Mammoplasia (breast enlargement) in girls begins at puberty, unlike all other primates in which breasts enlarge only during lactation.

During the menstrual cycle, the breasts are enlarged by premenstrual water retention and temporary growth.

The breasts reach full maturity only when a woman's first pregnancy occurs. Changes to the breasts are among the very first signs of pregnancy. The breasts become larger, the nipple-areola complex becomes larger and darker, the Montgomery's glands enlarge, and veins sometimes become more visible. Breast tenderness during pregnancy is common, especially during the first trimester. By mid-pregnancy, the breast is physiologically capable of lactation and some women can express colostrum, a form of breast milk.

Pregnancy causes elevated levels of the hormone prolactin, which has a key role in the production of milk. However, milk production is blocked by the hormones progesterone and estrogen until after delivery, when progesterone and estrogen levels plummet.

At menopause, breast atrophy occurs. The breasts can decrease in size when the levels of circulating estrogen decline. The adipose tissue and milk glands also begin to wither. The breasts can also become enlarged from adverse side effects of combined oral contraceptive pills. The size of the breasts can also increase and decrease in response to weight fluctuations. Physical changes to the breasts are often recorded in the stretch marks of the skin envelope; they can serve as historical indicators of the increments and the decrements of the size and volume of a woman's breasts throughout the course of her life.

The primary function of the breasts, as mammary glands, is the nourishing of an infant with breast milk. Milk is produced in milk-secreting cells in the alveoli. When the breasts are stimulated by the suckling of her baby, the mother's brain secretes oxytocin. High levels of oxytocin trigger the contraction of muscle cells surrounding the alveoli, causing milk to flow along the ducts that connect the alveoli to the nipple.

Full-term newborns have an instinct and a need to suck on a nipple, and breastfed babies nurse for both nutrition and for comfort. Breast milk provides all necessary nutrients for the first six months of life, and then remains an important source of nutrition, alongside solid foods, until at least one or two years of age.

The breast is susceptible to numerous benign and malignant conditions. The most frequent benign conditions are puerperal mastitis, fibrocystic breast changes and mastalgia.

Lactation unrelated to pregnancy is known as galactorrhea. It can be caused by certain drugs (such as antipsychotic medications), extreme physical stress, or endocrine disorders. Lactation in newborns is caused by hormones from the mother that crossed into the baby's bloodstream during pregnancy.

Breast cancer is the most common cause of cancer death among women and it is one of the leading causes of death among women. Factors that appear to be implicated in decreasing the risk of breast cancer are regular breast examinations by health care professionals, regular mammograms, self-examination of breasts, healthy diet, and exercise to decrease excess body fat.

Both females and males develop breasts from the same embryological tissues. Normally, males produce lower levels of estrogens and higher levels of androgens, namely testosterone, which suppress the effects of estrogens in developing excessive breast tissue.
In boys and men, abnormal breast development is manifested as gynecomastia, the consequence of a biochemical imbalance between the normal levels of estrogen and testosterone in the male body. Around 70% of boys temporarily develop breast tissue during adolescence. The condition usually resolves by itself within two years. When male lactation occurs, it is considered a symptom of a disorder of the pituitary gland.

Plastic surgery can be performed to augment or reduce the size of breasts, or reconstruct the breast in cases of deformative disease, such as breast cancer. Breast augmentation and breast lift (mastopexy) procedures are done only for cosmetic reasons, whereas breast reduction is sometimes medically indicated. In cases where a woman's breasts are severely asymmetrical, surgery can be performed to either enlarge the smaller breast, reduce the size of the larger breast, or both.

Breast augmentation surgery generally does not interfere with future ability to breastfeed. Breast reduction surgery more frequently leads to decreased sensation in the nipple-areola complex, and to low milk supply in women who choose to breastfeed. Implants can interfere with mammography (breast x-rays images).

In Christian iconography, some works of art depict women with their breasts in their hands or on a platter, signifying that they died as a martyr by having their breasts severed; one example of this is Saint Agatha of Sicily.

Femen is a feminist activist group which uses topless protests as part of their campaigns against sex tourism religious institutions, sexism, homophobia and to "defend [women's] right to abortion". Femen activists have been regularly detained by police in response to their protests.

There is a long history of female breasts being used by comedians as a subject for comedy fodder (e.g., British comic Benny Hill's burlesque/slapstick routines).

In European pre-historic societies, sculptures of female figures with pronounced or highly exaggerated breasts were common. A typical example is the so-called Venus of Willendorf, one of many Paleolithic Venus figurines with ample hips and bosom. Artifacts such as bowls, rock carvings and sacred statues with breasts have been recorded from 15,000 BC up to late antiquity all across Europe, North Africa and the Middle East.

Many female deities representing love and fertility were associated with breasts and breast milk. Figures of the Phoenician goddess Astarte were represented as pillars studded with breasts. Isis, an Egyptian goddess who represented, among many other things, ideal motherhood, was often portrayed as suckling pharaohs, thereby confirming their divine status as rulers. Even certain male deities representing regeneration and fertility were occasionally depicted with breast-like appendices, such as the river god Hapy who was considered to be responsible for the annual overflowing of the Nile.

Female breasts were also prominent in the Minoan civilization in the form of the famous Snake Goddess statuettes. In Ancient Greece there were several cults worshipping the "Kourotrophos", the suckling mother, represented by goddesses such as Gaia, Hera and Artemis. The worship of deities symbolized by the female breast in Greece became less common during the first millennium. The popular adoration of female goddesses decreased significantly during the rise of the Greek city states, a legacy which was passed on to the later Roman Empire.

During the middle of the first millennium BC, Greek culture experienced a gradual change in the perception of female breasts. Women in art were covered in clothing from the neck down, including female goddesses like Athena, the patron of Athens who represented heroic endeavor. There were exceptions: Aphrodite, the goddess of love, was more frequently portrayed fully nude, though in postures that were intended to portray shyness or modesty, a portrayal that has been compared to modern pin ups by historian Marilyn Yalom. Although nude men were depicted standing upright, most depictions of female nudity in Greek art occurred "usually with drapery near at hand and with a forward-bending, self-protecting posture". A popular legend at the time was of the Amazons, a tribe of fierce female warriors who socialized with men only for procreation and even removed one breast to become better warriors (the idea being that the right breast would interfere with the operation of a bow and arrow). The legend was a popular motif in art during Greek and Roman antiquity and served as an antithetical cautionary tale.

Many women regard their breasts as important to their sexual attractiveness, as a sign of femininity that is important to their sense of self.

Because breasts are mostly fatty tissue, their shape can -within limits- be molded by clothing, such as foundation garments. Bras are commonly worn by about 90% of Western women, and are often worn for support. The social norm in most Western cultures is to cover breasts in public, though the extent of coverage varies depending on the social context. Some religions ascribe a special status to the female breast, either in formal teachings or through symbolism. Islam forbids women from exposing their breasts in public.

Many cultures, including Western cultures in North America, associate breasts with sexuality and tend to regard bare breasts as immodest or indecent. In some cultures, like the Himba in northern Namibia, bare-breasted women are normal. In some African cultures, for example, the thigh is regarded as highly sexualised and never exposed in public, but breast exposure is not taboo. In a few Western countries and regions female toplessness at a beach is acceptable, although it may not be acceptable in the town center.

Social attitudes and laws regarding breastfeeding in public vary widely. In many countries, breastfeeding in public is common, legally protected, and generally not regarded as an issue. However, even though the practice may be legal or socially accepted, some mothers may nevertheless be reluctant to expose a breast in public to breastfeed due to actual or potential objections by other people, negative comments, or harassment. It is estimated that around 63% of mothers across the world have publicly breast-fed. Bare-breasted women are legal and culturally acceptable at public beaches in Australia and much of Europe. Filmmaker Lina Esco made a film entitled "Free The Nipple", which is about "...laws against female toplessness or restrictions on images of female, but not male, nipples", which Esco states is an example of sexism in society.

In some cultures, breasts play a role in human sexual activity. In Western culture, breasts have a "...hallowed sexual status, arguably more fetishized than either sex’s genitalia". Breasts and especially the nipples are among the various human erogenous zones. They are sensitive to the touch as they have many nerve endings; and it is common to press or massage them with hands or orally before or during sexual activity. During sexual arousal, breast size increases, venous patterns across the breasts become more visible, and nipples harden. Compared to other primates, human breasts are proportionately large throughout adult females' lives. Some writers have suggested that they may have evolved as a visual signal of sexual maturity and fertility.

Many people regard bare female breasts to be aesthetically pleasing or erotic, and they can elicit heightened sexual desires in men in many cultures. In the ancient Indian work the "Kama Sutra", light scratching of the breasts with nails and biting with teeth are considered erotic. Some people show a sexual interest in female breasts distinct from that of the person, which may be regarded as a breast fetish. A number of Western fashions include clothing which accentuate the breasts, such as the use of push-up bras and decollete (plunging neckline) gowns and blouses which show cleavage. While U.S. culture prefers breasts that are youthful and upright, some cultures venerate women with drooping breasts, indicating mothering and the wisdom of experience.

Research conducted at the Victoria University of Wellington showed that breasts are often the first thing men look at, and for a longer time than other body parts.
The writers of the study had initially speculated that the reason for this is due to endocrinology with larger breasts indicating higher levels of estrogen and a sign of greater fertility, but the researchers said that "Men may be looking more often at the breasts because they are simply aesthetically pleasing, regardless of the size."

Some women report achieving an orgasm from nipple stimulation, but this is rare. Research suggests that the orgasms are genital orgasms, and may also be directly linked to "the genital area of the brain". In these cases, it seems that sensation from the nipples travels to the same part of the brain as sensations from the vagina, clitoris and cervix. Nipple stimulation may trigger uterine contractions, which then produce a sensation in the genital area of the brain.

There are many mountains named after the breast because they resemble it in appearance and so are objects of religious and ancestral veneration as a fertility symbol and of well-being. In Asia, there was "Breast Mountain", which had a cave where the Buddhist monk Bodhidharma (Da Mo) spent much time in meditation. Other such breast mountains are Mount Elgon on the Uganda-Kenya border, Beinn Chìochan and the Maiden Paps in Scotland, the "Bundok ng Susong Dalaga" (Maiden's breast mountains) in Talim Island, Philippines, the twin hills known as the Paps of Anu ("Dá Chích Anann" or "the breasts of Anu"), near Killarney in Ireland, the 2,086 m high "Tetica de Bacares" or "La Tetica" in the Sierra de Los Filabres, Spain, and Khao Nom Sao in Thailand, Cerro Las Tetas in Puerto Rico and the Breasts of Aphrodite in Mykonos, among many others. In the United States, the Teton Range is named after the French word for "breast".

Bibliography



</doc>
<doc id="4492" url="https://en.wikipedia.org/wiki?curid=4492" title="Baghdad">
Baghdad

Baghdad (; ) is the capital of Iraq. The population of Baghdad, , is approximately 8,765,000, making it the largest city in Iraq, the second largest city in the Arab world (after Cairo, Egypt), and the second largest city in Western Asia (after Tehran, Iran).

Located along the Tigris River, the city was founded in the 8th century and became the capital of the Abbasid Caliphate. Within a short time of its inception, Baghdad evolved into a significant cultural, commercial, and intellectual center for the Islamic world. This, in addition to housing several key academic institutions (e.g., House of Wisdom), garnered the city a worldwide reputation as the "Centre of Learning". 

Baghdad was the largest city of the Middle Ages for much of the Abbasid era, peaking at a population of more than a million. The city was largely destroyed at the hands of the Mongol Empire in 1258, resulting in a decline that would linger through many centuries due to frequent plagues and multiple successive empires. With the recognition of Iraq as an independent state (formerly the British Mandate of Mesopotamia) in 1938, Baghdad gradually regained some of its former prominence as a significant center of Arab culture. 

In contemporary times, the city has often faced severe infrastructural damage, most recently due to the 2003 invasion of Iraq, and the subsequent Iraq War that lasted until December 2011. In recent years, the city has been frequently subjected to insurgency attacks. The war had resulted in a substantial loss of cultural heritage and historical artifacts as well. , Baghdad was listed as one of the least hospitable places in the world to live, and was ranked by Mercer as the worst of 221 major cities as measured by quality-of-life.

The name Baghdad is pre-Islamic, and its origin is disputed. The site where the city of Baghdad developed has been populated for millennia. By the 8th century AD, several villages had developed there, including a Persian hamlet called "Baghdad," the name which would come to be used for the Abbasid metropolis.

Arab authors, realizing the pre-Islamic origins of Baghdad's name, generally looked for its roots in Persian. They suggested various meanings, the most common of which was "bestowed by God". Modern scholars generally tend to favor this etymology, which views the word as a compound of "bagh" () "god" and "dād" () "given", In Old Persian the first element can be traced to "boghu" and is related to Slavic "bog" "god", while the second can be traced to "dadāti". A similar term in Middle Persian is the name "Mithradāt" ("Mihrdād" in New Persian), known in English by its Hellenistic form Mithridates, meaning "gift of Mithra" ("dāt" is the more archaic form of "dād", related to Latin "dat" and English "donor"). There are a number of other locations in the wider region whose names are compounds of the word "bagh", including Baghlan and Bagram in Afghanistan or a village called Bagh-šan in Iran. The name of the town Baghdati in Georgia shares the same etymological origins.

A few authors have suggested older origins for the name, in particular the name "Bagdadu" or "Hudadu" that existed in Old Babylonian (spelled with a sign that can represent both "bag" and "hu"), and the Babylonian Talmudic name of a place called "Baghdatha". Some scholars suggested Aramaic derivations.

When the Abbasid caliph, al-Mansur, founded a completely new city for his capital, he chose the name Madinat al-Salaam or "City of Peace". This was the official name on coins, weights, and other official usage, although the common people continued to use the old name. By the 11th century, "Baghdad" became almost the exclusive name for the world-renowned metropolis.

After the fall of the Umayyads, the first Muslim dynasty, the victorious Abbasid rulers wanted their own capital from which they could rule. They chose a site north of the Sassanid capital of Ctesiphon (and also just north of where ancient Babylon had once stood), and on 30 July 762 the caliph Al-Mansur commissioned the construction of the city. It was built under the supervision of the Barmakids. Mansur believed that Baghdad was the perfect city to be the capital of the Islamic empire under the Abbasids. Mansur loved the site so much he is quoted saying: "This is indeed the city that I am to found, where I am to live, and where my descendants will reign afterward".

The city's growth was helped by its excellent location, based on at least two factors: it had control over strategic and trading routes along the Tigris, and it had an abundance of water in a dry climate. Water exists on both the north and south ends of the city, allowing all households to have a plentiful supply, which was very uncommon during this time.

Baghdad eclipsed Ctesiphon, the capital of the Sassanians, which was located some to the southeast. Today, all that remains of Ctesiphon is the shrine town of Salman Pak, just to the south of Greater Baghdad. Ctesiphon itself had replaced and absorbed Seleucia, the first capital of the Seleucid Empire, which had earlier replaced the city of Babylon.

According to the traveler Ibn Battuta, Baghdad was one of the largest cities, not including the damage it has received. The residents are mostly Hanbal. Bagdad is also home to the grave of Abu Hanifa where there is a cell and a mosque above it. The Sultan of Bagdad, Abu Said Bahadur Khan, was a Tartar king who embraced Islamism.

In its early years, the city was known as a deliberate reminder of an expression in the Qur'an, when it refers to Paradise. It took four years to build (764–768). Mansur assembled engineers, surveyors, and art constructionists from around the world to come together and draw up plans for the city. Over 100,000 construction workers came to survey the plans; many were distributed salaries to start the building of the city. July was chosen as the starting time because two astrologers, Naubakht Ahvazi and Mashallah, believed that the city should be built under the sign of the lion, Leo. Leo is associated with fire and symbolises productivity, pride, and expansion.

The bricks used to make the city were on all four sides. Abu Hanifah was the counter of the bricks and he developed a canal, which brought water to the work site for both human consumption and the manufacture of the bricks. Marble was also used to make buildings throughout the city, and marble steps led down to the river's edge.

The basic framework of the city consists of two large semicircles about in diameter. The city was designed as a circle about in diameter, leading it to be known as the "Round City". The original design shows a single ring of residential and commercial structures along the inside of the city walls, but the final construction added another ring inside the first. Within the city there were many parks, gardens, villas, and promenades. In the center of the city lay the mosque, as well as headquarters for guards. The purpose or use of the remaining space in the center is unknown. The circular design of the city was a direct reflection of the traditional Persian Sasanian urban design. The Sasanian city of Gur in Fars, built 500 years before Baghdad, is nearly identical in its general circular design, radiating avenues, and the government buildings and temples at the centre of the city. This style of urban planning contrasted with Ancient Greek and Roman urban planning, in which cities are designed as squares or rectangles with streets intersecting each other at right angles.

The four surrounding walls of Baghdad were named Kufa, Basra, Khurasan, and Syria; named because their gates pointed in the directions of these destinations. The distance between these gates was a little less than . Each gate had double doors that were made of iron; the doors were so heavy it took several men to open and close them. The wall itself was about 44 m thick at the base and about 12 m thick at the top. Also, the wall was 30 m high, which included merlons, a solid part of an embattled parapet usually pierced by embrasures. This wall was surrounded by another wall with a thickness of 50 m. The second wall had towers and rounded merlons, which surrounded the towers. This outer wall was protected by a solid glacis, which is made out of bricks and quicklime. Beyond the outer wall was a water-filled moat.

The Golden Gate Palace, the residence of the caliph and his family, was in the middle of Baghdad, in the central square. In the central part of the building, there was a green dome that was 39 m high. Surrounding the palace was an esplanade, a waterside building, in which only the caliph could come riding on horseback. In addition, the palace was near other mansions and officer's residences. Near the Gate of Syria, a building served as the home for the guards. It was made of brick and marble. The palace governor lived in the latter part of the building and the commander of the guards in the front. In 813, after the death of caliph Al-Amin, the palace was no longer used as the home for the caliph and his family.
The roundness points to the fact that it was based on Arabic script. The two designers who were hired by Al-Mansur to plan the city's design were Naubakht, a Zoroastrian who also determined that the date of the foundation of the city would be astrologically auspicious, and Mashallah, a Jew from Khorasan, Iran.

The justification for the Abbasid Caliphate was based on the Abbasids being the descendants of the uncle of Muhammad and being part of the Quraysh tribe. They used Shi'a resentment, Khorasanian movement, and appeals to the ambitions and traditions of the newly conquered Persian aristocracy to overthrow the Umayyads.
The Abbasids sought to combine the hegemony of the Arab tribes with the imperial, court, ceremonial, and administrative structures of the Persians. The Abbasids considered themselves the inheriters of Arab-Islamic culture. Harun al-Rashid needed to place the capital in a place that was representative of Arab-Islamic identity and built the House of Wisdom, where ancient texts were translated from their original language, such as Greek, to Arabic. Al-Ma'mun is credited with the "Translation Movement" for this. 

Within a generation of its founding, Baghdad became a hub of learning and commerce. "Baytul-Hikmah" or the "House of Wisdom", initially founded as a library for private use by Harun al-Rashid, flourished into an unrivaled intellectual center of science, medicine, philosophy, and education and had the largest selection of books in the world by the middle of the 9th century. Baghdad was likely the largest city in the world from shortly after its foundation until the 930s, when it tied with Córdoba.
Several estimates suggest that the city contained over a million inhabitants at its peak. Many of the "One Thousand and One Nights" tales, widely known as the "Arabian Nights", are set in Baghdad during this period.

Among the notable features of Baghdad during this period were its exceptional libraries. Many of the Abbasid caliphs were patrons of learning and enjoyed collecting both ancient and contemporary literature. Although some of the princes of the previous Umayyad dynasty had begun to gather and translate Greek scientific literature, the Abbasids were the first to foster Greek learning on a large scale. Many of these libraries were private collections intended only for the use of the owners and their immediate friends, but the libraries of the caliphs and other officials soon took on a public or a semi-public character. Four great libraries were established in Baghdad during this period. The earliest was that of the famous Al Mamun, who was caliph from 813 to 833. Another was established by Sabur Ibn Ardashir in 991 or 993 for the literary men and scholars who frequented his academy. Unfortunately, this second library was plundered and burned by the Seljuks only seventy years after it was established. This was a good example of the sort of library built up out of the needs and interests of a literary society. The last two were examples of "madrasa" or theological college libraries. The Nizamiyah was founded by the Persian Nizam al Mulk, who was vizier of two early Seljuk sultans. It continued to operate even after the coming of the Mongols in 1258. The Mustansiriyah "madrasa", which owned an exceedingly rich library, was founded by Al Mustansir, the second last Abbasid caliph, who died in 1242. This would prove to be the last great library built by the caliphs of Baghdad.

By the 10th century, the city's population was between 1.2 million and 2 million. Baghdad's early meteoric growth eventually slowed due to troubles within the Caliphate, including relocations of the capital to Samarra (during 808–819 and 836–892), the loss of the western and easternmost provinces, and periods of political domination by the Iranian Buwayhids (945–1055) and Seljuk Turks (1055–1135).

The Seljuks were a clan of the Oghuz Turks from Central Asia that converted to the Sunni branch of Islam. In 1040, they destroyed the Ghaznavids, taking over their land and in 1055, Tughril Beg, the leader of the Seljuks, took over Baghdad. The Seljuks expelled the Buyid dynasty of Shiites that had ruled for some time and took over power and control of Baghdad. They ruled as Sultans in the name of the Abbasid caliphs (they saw themselves as being part of the Abbasid regime). Tughril Beg saw himself as the protector of the Abbasid Caliphs.

Sieges and wars in which Baghdad was involved are listed below:


In 1058, Baghdad was captured by the Fatimids under the Turkish general Abu'l-Ḥārith Arslān al-Basasiri, an adherent of the Ismailis along with the 'Uqaylid Quraysh. Not long before the arrival of the Saljuqs in Baghdad, al-Basasiri petitioned to the Fatimid Imam-Caliph al-Mustansir to support him in conquering Baghdad on the Ismaili Imam's behalf. It has recently come to light that the famed Fatimid "da'i", al-Mu'ayyad al-Shirazi, had a direct role in supporting al-Basasiri and helped the general to succeed in taking Mawṣil, Wāsit and Kufa. Soon after, by December 1058, a Shi'i "adhān" (call to prayer) was implemented in Baghdad and a "khutbah" (sermon) was delivered in the name of the Fatimid Imam-Caliph. Despite his Shi'i inclinations, Al-Basasiri received support from Sunnis and Shi'is alike, for whom opposition to the Saljuq power was a common factor.
On 10 February 1258, Baghdad was captured by the Mongols led by Hulegu, a grandson of Chingiz Khan (Genghis Khan), during the siege of Baghdad. Many quarters were ruined by fire, siege, or looting. The Mongols massacred most of the city's inhabitants, including the caliph Al-Musta'sim, and destroyed large sections of the city. The canals and dykes forming the city's irrigation system were also destroyed. During this time, in Baghdad, Christians and Shia were tolerated, while Sunnis were treated as enemies. The sack of Baghdad put an end to the Abbasid Caliphate, a blow from which the Islamic civilization never fully recovered.

At this point, Baghdad was ruled by the Ilkhanate, a breakaway state of the Mongol Empire, ruling from Iran. In 1401, Baghdad was again sacked, by the Central Asian Turkic conqueror Timur ("Tamerlane"). When his forces took Baghdad, he spared almost no one, and ordered that each of his soldiers bring back two severed human heads. Baghdad became a provincial capital controlled by the Mongol Jalayirid (1400–1411), Turkic Kara Koyunlu (1411–1469), Turkic Ak Koyunlu (1469–1508), and the Iranian Safavid (1508–1534) dynasties.

In 1534, Baghdad was captured by the Ottoman Turks. Under the Ottomans, Baghdad continued into a period of decline, partially as a result of the enmity between its rulers and Iranian Safavids, which did not accept the Sunni control of the city. Between 1623 and 1638, it returned to Iranian rule before falling back into Ottoman hands.

Baghdad has suffered severely from visitations of the plague and cholera, and sometimes two-thirds of its population has been wiped out.

For a time, Baghdad had been the largest city in the Middle East. The city saw relative revival in the latter part of the 18th century under a Mamluk government. Direct Ottoman rule was reimposed by Ali Rıza Pasha in 1831. From 1851 to 1852 and from 1861 to 1867, Baghdad was governed, under the Ottoman Empire by Mehmed Namık Pasha. The Nuttall Encyclopedia reports the 1907 population of Baghdad as 185,000.

Baghdad and southern Iraq remained under Ottoman rule until 1917, when captured by the British during World War I. In 1920, Baghdad became the capital of the British Mandate of Mesopotamia and after receiving independence in 1932, the capital of the Kingdom of Iraq. The city's population grew from an estimated 145,000 in 1900 to 580,000 in 1950. During the Mandate, Baghdad's substantial Jewish community comprised a quarter of the city's population.

On 1 April 1941, members of the "Golden Square" and Rashid Ali staged a coup in Baghdad. Rashid Ali installed a pro-German and pro-Italian government to replace the pro-British government of Regent Abdul Ilah. On 31 May, after the resulting Anglo-Iraqi War and after Rashid Ali and his government had fled, the Mayor of Baghdad surrendered to British and Commonwealth forces.

On 14 July 1958, members of the Iraqi Army, under Abd al-Karim Qasim, staged a coup to topple the Kingdom of Iraq. King Faisal II, former Prime Minister Nuri as-Said, former Regent Prince 'Abd al-Ilah, members of the royal family, and others were brutally killed during the coup. Many of the victim's bodies were then dragged through the streets of Baghdad.
During the 1970s, Baghdad experienced a period of prosperity and growth because of a sharp increase in the price of petroleum, Iraq's main export. New infrastructure including modern sewerage, water, and highway facilities were built during this period. The masterplans of the city (1967, 1973) were delivered by the Polish planning office Miastoprojekt-Kraków, mediated by Polservice. However, the Iran–Iraq War of the 1980s was a difficult time for the city, as money was diverted by Saddam Hussein to the army and thousands of residents were killed. Iran launched a number of missile attacks against Baghdad in retaliation for Saddam Hussein's continuous bombardments of Tehran's residential districts.

In 1991 and 2003, the Gulf War and the 2003 invasion of Iraq caused significant damage to Baghdad's transportation, power, and sanitary infrastructure as the US-led coalition forces launched massive aerial assaults in the city in the two wars. Also in 2003, the minor riot in the city (which took place on 21 July) caused some disturbance in the population.

The historic "Assyrian Quarter" of the city, Dora, which boasted a population of 150,000 Assyrians in 2003, made up over 3% of the capital's Assyrian population then. The community has been subject to kidnappings, death threats, vandalism, and house burnings by Al-Qaeda and other insurgent groups. As of the end of 2014, only 1,500 Assyrians remained in Dora. 

Points of interest include the National Museum of Iraq whose priceless collection of artifacts was looted during the 2003 invasion, and the iconic Hands of Victory arches. Multiple Iraqi parties are in discussions as to whether the arches should remain as historical monuments or be dismantled. Thousands of ancient manuscripts in the National Library were destroyed under Saddam's command.

Mutanabbi Street (Arabic: شارع المتنبي) is located near the old quarter of Baghdad; at Al Rasheed Street. It is the historic center of Baghdadi book-selling, a street filled with bookstores and outdoor book stalls. It was named after the 10th-century classical Iraqi poet Al-Mutanabbi. This street is well established for bookselling and has often been referred to as the heart and soul of the Baghdad literacy and intellectual community.

The zoological park used to be the largest in the Middle East. Within eight days following the 2003 invasion, however, only 35 of the 650 animals in the facility survived. This was a result of theft of some animals for human food, and starvation of caged animals that had no food. South African Lawrence Anthony and some of the zoo keepers cared for the animals and fed the carnivores with donkeys they had bought locally. Eventually, L. Paul Bremer, Director of the Coalition Provisional Authority in Iraq from 11 May 2003 to 28 June 2004 ordered protection of the zoo and U.S. engineers helped to reopen the facility.

Grand Festivities Square is the main square where public celebrations are held and is also the home to three important monuments commemmorating Iraqi's fallen soldiers and victories in war; namely Al-Shaheed Monument, the Victory Arch and the Unknown Soldier's Monument. 

Al-Shaheed Monument (), also known as the Martyr's Memorial, is a monument dedicated to the Iraqi soldiers who died in the Iran–Iraq War. However, now it is generally considered by Iraqis to be for all of the martyrs of Iraq, especially those allied with Iran and Syria currently fighting ISIS, not just of the Iran–Iraq War. The Monument was opened in 1983, and was designed by the Iraqi architect Saman Kamal and the Iraqi sculptor and artist Ismail Fatah Al Turk. During the 1970s and 1980s, Saddam Hussein's government spent a lot of money on new monuments, which included the al-Shaheed Monument.
Qushla (or Qishla, ) is a public square and the historical complex located in Rusafa neighborhood at the riverbank of Tigris. Qushla and it’s surrounding is where the historical features and cultural capitals of Baghdad are concentrated, from the Mutanabbi Street, Abbasid-era palace and bridges, Ottoman-era mosques to the Mustansariyah Madrasa. The square developed during the Ottoman era as a military barracks. Today, it is a place where the citizens of Baghdad find leisure such as reading poetry in gazebos. It is characterized by the iconic clock tower which was donated by George V. The entire area is currently submitted to the UNESCO World Heritage Site Tentative list.

Al-Kadhimiyyah Masjid is a shrine that is located in the Kādhimayn suburb of Baghdad. It contains the tombs of the seventh and ninth Twelver Shi'ite Imams, Musa al-Kadhim and Muhammad at-Taqi respectively, upon whom the title of "Kāẓimayn" (, "Two who swallow their anger") was bestowed. Many Shi'ites travel to the mosque from far away places to commemorate.

A'dhamiyyah is a predominantly Sunni area with a Masjid that is associated with the Sunni Imam Abu Hanifah. The name of "Al-A‘ẓamiyyah" () is derived from Abu Hanifah's title, "al-Imām al-A‘ẓam" (, the Great Imam).

Firdos Square is a public open space in Baghdad and the location of two of the best-known hotels, the Palestine Hotel and the Sheraton Ishtar, which are both also the tallest buildings in Baghdad. The square was the site of the statue of Saddam Hussein that was pulled down by U.S. coalition forces in a widely televised event during the 2003 invasion of Iraq.

Administratively, Baghdad Governorate is divided into districts which are further divided into sub-districts. Municipally, the governorate is divided into 9 municipalities, which have responsibility for local issues. Regional services, however, are coordinated and carried out by a mayor who oversees the municipalities. There is no single city council that singularly governs Baghdad at a municipal level. The governorate council is responsible for the governorate-wide policy.

These official subdivisions of the city served as administrative centres for the delivery of municipal services but until 2003 had no political function. Beginning in April 2003, the U.S. controlled Coalition Provisional Authority (CPA) began the process of creating new functions for these. The process initially focused on the election of neighbourhood councils in the official neighbourhoods, elected by neighbourhood caucuses.

The CPA convened a series of meetings in each neighbourhood to explain local government, to describe the caucus election process and to encourage participants to spread the word and bring friends, relatives and neighbours to subsequent meetings. Each neighbourhood process ultimately ended with a final meeting where candidates for the new neighbourhood councils identified themselves and asked their neighbours to vote for them.

Once all 88 (later increased to 89) neighbourhood councils were in place, each neighbourhood council elected representatives from among their members to serve on one of the city's nine district councils. The number of neighbourhood representatives on a district council is based upon the neighbourhood's population. The next step was to have each of the nine district councils elect representatives from their membership to serve on the 37 member Baghdad City Council. This three tier system of local government connected the people of Baghdad to the central government through their representatives from the neighbourhood, through the district, and up to the city council.

The same process was used to provide representative councils for the other communities in Baghdad Province outside of the city itself. There, local councils were elected from 20 neighbourhoods (Nahia) and these councils elected representatives from their members to serve on six district councils (Qada). As within the city, the district councils then elected representatives from among their members to serve on the 35 member Baghdad Regional Council.

The first step in the establishment of the system of local government for Baghdad Province was the election of the Baghdad Provincial Council. As before, the representatives to the Provincial Council were elected by their peers from the lower councils in numbers proportional to the population of the districts they represent. The 41 member Provincial Council took office in February 2004 and served until national elections held in January 2005, when a new Provincial Council was elected.

This system of 127 separate councils may seem overly cumbersome; however, Baghdad Province is home to approximately seven million people. At the lowest level, the neighbourhood councils, each council represents an average of 75,000 people.

The nine District Advisory Councils (DAC) are as follows:


The nine districts are subdivided into 89 smaller neighborhoods which may make up sectors of any of the districts above. The following is a "selection" (rather than a complete list) of these neighborhoods:
The city is located on a vast plain bisected by the Tigris river. The Tigris splits Baghdad in half, with the eastern half being called "Risafa" and the Western half known as "Karkh". The land on which the city is built is almost entirely flat and low-lying, being of alluvial origin due to the periodic large floods which have occurred on the river.

Baghdad has a subtropical desert climate (Köppen climate classification "BWh"), featuring extremely hot, dry summers and mild, damp winters. In the summer, from June through August, the average maximum temperature is as high as , accompanied by blazing sunshine. Rainfall has, in fact, been recorded on fewer than half a dozen occasions at this time of year and has never exceeded . Even at night temperatures in summer are seldom below . Baghdad's record highest temperature of 124 degrees Fahrenheit (51 degrees Celsius) was reached in July 2015. The humidity is typically under 50% in summer due to Baghdad's distance from the marshy southern Iraq and the coasts of Persian Gulf, and dust storms from the deserts to the west are a normal occurrence during the summer.

Winters boast temperatures typical of subtropical climates. From December through February, Baghdad has maximum temperatures averaging , though highs above are not unheard of. The average January low is , but lows below freezing occur a couple of times per year on average.

Annual rainfall, almost entirely confined to the period from November through March, averages approximately , but has been as high as and as low as . On 11 January 2008, light snow fell across Baghdad for the first time in memory.

Baghdad's population was estimated at 7.22 million in 2015. The city historically had a predominantly Sunni population, but by the early 21st century around 82% of the city's population were Iraqi Shia. At the beginning of the 21st century, some 1.5 million people migrated to Baghdad, most of them Shiites and a few Sunnis.

As early as 2003, about 20 percent of the population of the city was the result of mixed marriages between Shi'ites and Sunnis: they are often referred to as "Sushis". Following the sectarian violence in Iraq between the Sunni and Shia militia groups during the U.S. occupation of Iraq, the city's population became overwhelmingly Shia. Despite the government's promise to resettle Sunnis displaced by the violence, little has been done to bring this about. The Iraqi Civil War following ISIS' invasion in 2014 caused hundreds of thousands of Iraqi internally displaced people to flee to the city. The city currently has Sunni, Shia, Assyrian/Chaldean/Syriacs, Armenians and mixed neighborhoods.

Baghdad accounts for 22.2 per cent of Iraq's population and 40 per cent of the country's gross domestic product (PPP). Iraqi Airways, the national airline of Iraq, has its headquarters on the grounds of Baghdad International Airport in Baghdad. Al-Naser Airlines has its head office in Karrada, Baghdad.

Most Iraqi reconstruction efforts have been devoted to the restoration and repair of badly damaged urban infrastructure. More visible efforts at reconstruction through private development, like architect and urban designer Hisham N. Ashkouri's Baghdad Renaissance Plan and the Sindbad Hotel Complex and Conference Center have also been made. A plan was proposed by a Government agency to rebuild a tourist island in 2008. In late 2009, a construction plan was proposed to rebuild the heart of Baghdad, but the plan was never realized because corruption was involved in it.

The Baghdad Eye, a tall Ferris wheel, was proposed for Baghdad in August 2008. At that time, three possible locations had been identified, but no estimates of cost or completion date were given.<ref name="msnbc.msn.com/id/26425911"></ref> In October 2008, it was reported that Al-Zawraa Park was expected to be the site, and a wheel was installed there in March 2011.

Iraq's Tourism Board is also seeking investors to develop a "romantic" island on the River Tigris in Baghdad that was once a popular honeymoon spot for newlywed Iraqis. The project would include a six-star hotel, spa, an 18-hole golf course and a country club. In addition, the go-ahead has been given to build numerous architecturally unique skyscrapers along the Tigris that would develop the city's financial centre in Kadhehemiah.

In October 2008, the Baghdad Metro resumed service. It connects the center to the southern neighborhood of Dora.
In May 2010, a new residential and commercial project nicknamed Baghdad Gate was announced. This project not only addresses the urgent need for new residential units in Baghdad but also acts as a real symbol of progress in the war torn city, as Baghdad has not seen projects of this scale for decades.



The Mustansiriya Madrasah was established in 1227 by the Abbasid Caliph al-Mustansir. The name was changed to Al-Mustansiriya University in 1963. The University of Baghdad is the largest university in Iraq and the second largest in the Arab world.

Prior to the Gulf War multiple international schools operated in Baghdad, including:


Baghdad has always played a significant role in the broader Arab cultural sphere, contributing several significant writers, musicians and visual artists. Famous Arab poets and singers such as Nizar Qabbani, Umm Kulthum, Fairuz, Salah Al-Hamdani, Ilham al-Madfai and others have performed for the city.

The dialect of Arabic spoken in Baghdad today differs from that of other large urban centres in Iraq, having features more characteristic of nomadic Arabic dialects (Verseegh, "The Arabic Language"). It is possible that this was caused by the repopulating of the city with rural residents after the multiple sackings of the late Middle Ages.

For poetry written about Baghdad, see Reuven Snir (ed.), "Baghdad: The City in Verse" (Harvard, 2013)

Some of the important cultural institutions in the city include the National Theater, which was looted during the 2003 invasion of Iraq, but efforts are underway to restore the theatre. The live theatre scene received a boost during the 1990s, when UN sanctions limited the import of foreign films. As many as 30 movie theatres were reported to have been converted to live stages, producing a wide range of comedies and dramatic productions.

Institutions offering cultural education in Baghdad include The Music and Ballet School of Baghdad and the Institute of Fine Arts Baghdad. The Iraqi National Symphony Orchestra is a government funded symphony orchestra in Baghdad. The INSO plays primarily classical European music, as well as original compositions based on Iraqi and Arab instruments and music. Baghdad is also home to a number of museums which housed artifacts and relics of ancient civilization; many of these were stolen, and the museums looted, during the widespread chaos immediately after United States forces entered the city. 

During the 2003 occupation of Iraq, AFN Iraq ("Freedom Radio") broadcast news and entertainment within Baghdad, among other locations. There is also a private radio station called "Dijlah" (named after the Arabic word for the Tigris River) that was created in 2004 as Iraq's first independent talk radio station. Radio Dijlah offices, in the Jamia neighborhood of Baghdad, have been attacked on several occasions.

Priceless collection of artifacts in the National Museum of Iraq was looted during the 2003 US-led invasion. Thousands of ancient manuscripts in the National Library were destroyed under Saddam's command and the neglection of the occupying coalition forces.

Baghdad is home to some of the most successful football (soccer) teams in Iraq, the biggest being Al-Shorta (Police), Al-Quwa Al-Jawiya (Airforce club), Al-Zawra'a, and Talaba (Students). The largest stadium in Baghdad is Al-Shaab Stadium, which was opened in 1966. Another, but much larger stadium, is still in the opening stages of construction.

The city has also had a strong tradition of horse racing ever since World War I, known to Baghdadis simply as 'Races'. There are reports of pressures by the Islamists to stop this tradition due to the associated gambling.





Books:



</doc>
<doc id="4493" url="https://en.wikipedia.org/wiki?curid=4493" title="Outline of biology">
Outline of biology

Biology – The natural science that involves the study of life and living organisms, including their structure, function, growth, origin, evolution, distribution, and taxonomy.

Branch of biology – subdiscipline of biology, also referred to as a biological science. Note that biology and all of its branches are also life sciences.



Outline of ecology

Outline of evolution


Outline of cell biology

Outline of biochemistry

Outline of genetics






</doc>
<doc id="4495" url="https://en.wikipedia.org/wiki?curid=4495" title="British thermal unit">
British thermal unit

The British thermal unit (Btu or BTU) is a traditional unit of heat; it is defined as the amount of heat required to raise the temperature of one pound of water by one degree Fahrenheit. It is part of the United States customary units. Its counterpart in the metric system is the calorie, which is defined as the amount of heat required to raise the temperature of one gram of water by one degree Celsius. Heat is now known to be equivalent to energy, for which the SI unit is the joule; one BTU is about 1055 joules. While units of heat are often supplanted by energy units in scientific work, they are still important in many fields. As examples, in the United States the price of natural gas is quoted in dollars per million BTUs.

A BTU was originally defined as the amount of heat required to raise the temperature of 1 avoirdupois pound of liquid water by 1 degree Fahrenheit at a constant pressure of one atmosphere. There are several different definitions of the BTU that are now known to differ slightly. This reflects the fact that the temperature change of a mass of water due to the addition of a specific amount of heat (calculated in energy units, usually joules) depends slightly upon the water's initial temperature. As seen in the table below, definitions of the BTU based on different water temperatures vary by up to 0.5%. In the table, thermochemical and steam table (IT) values, which are now defined in terms of exact values in joules, have been rounded to four decimal places.

Units kBtu are used in building energy use tracking and heating system sizing. Energy Use Index (EUI) represents kBtu per square foot of conditioned floor area. "k" stands for 1,000.

The units MBtu and MMBtu are used in the natural gas and other industries to indicate 1,000 and 1,000,000 BTUs, respectively. There is an ambiguity in that the metric system uses the prefix "M" to indicate one million (1,000,000), and "MBtu" is also used to indicate one million BTUs. Because of this ambiguity, some authors have deprecated the use of MBtu.

Energy analysts accustomed to the metric "k" for 1,000 are more likely to use MBtu to represent one million, especially in documents where M represents one million in other energy or cost units, such as MW, MWh and $.

The unit "therm" is used to represent 100,000 (or 10) BTUs. A decatherm is 10 therms or one MBtu. The unit "quad" is commonly used to represent one quadrillion (10) BTUs.

One Btu is approximately:

A Btu can be approximated as the heat produced by burning a single wooden kitchen match or as the amount of energy it takes to lift a weight .


When used as a unit of power for heating and cooling systems, Btu "per hour" (Btu/h) is the correct unit, though this is often abbreviated to just "Btu". "MBH"—thousands of Btus per hour—is also common.



The Btu should not be confused with the Board of Trade Unit (B.O.T.U.), which is a much larger quantity of energy ().

The Btu is often used to express the conversion-efficiency of heat into electrical energy in power plants. Figures are quoted in terms of the quantity of heat in Btu required to generate 1 kW⋅h of electrical energy. A typical coal-fired power plant works at  Btu/kW⋅h, an efficiency of 32–33%.

The centigrade heat unit (CHU) is the amount of heat required to raise the temperature of one pound of water by one degree Celsius. It is equal to 1.8 BTU or 1899 joules. This unit was sometimes used in the United Kingdom as an alternative to BTU but is now obsolete.




</doc>
<doc id="4497" url="https://en.wikipedia.org/wiki?curid=4497" title="Bugatti">
Bugatti

Automobiles Ettore Bugatti was a French car manufacturer of high-performance automobiles, founded in 1909 in the then German city of Molsheim, Alsace by Italian-born Ettore Bugatti. Bugatti cars were known for their design beauty (Ettore Bugatti was from a family of artists and considered himself to be both an artist and constructor) and for their many race victories. Famous Bugattis include the Type 35 Grand Prix cars, the Type 41 "Royale", the Type 57 "Atlantic" and the Type 55 sports car.

The death of Ettore Bugatti in 1947 proved to be the end for the marque, and the death of his son Jean Bugatti in 1939 ensured there was not a successor to lead the factory. No more than about 8,000 cars were made. The company struggled financially, and released one last model in the 1950s, before eventually being purchased for its airplane parts business in the 1960s.

In the 1990s, an Italian entrepreneur revived it as a builder of limited production exclusive sports cars. Today, the name is owned by German automobile manufacturing group Volkswagen.

Founder Ettore Bugatti was born in Milan, Italy, and the automobile company that bears his name was founded in 1909 in Molsheim located in the Alsace region which was part of the German Empire from 1871 to 1919. The company was known both for the level of detail of its engineering in its automobiles, and for the artistic manner in which the designs were executed, given the artistic nature of Ettore's family (his father, Carlo Bugatti (1856–1940), was an important Art Nouveau furniture and jewelry designer).

During the war Ettore Bugatti was sent away, initially to Milan and later to Paris, but as soon as hostilities had been concluded he returned to his factory at Molsheim. Less than four months after the Versailles Treaty formalised the transfer of Alsace from Germany to France, Bugatti was able to obtain, at the last minute, a stand at the 15th Paris motor show in October 1919. He exhibited three light cars, all of them closely based on their pre-war equivalents, and each fitted with the same overhead camshaft 4-cylinder 1,368cc engine with four valves per cylinder. Smallest of the three was a "Type 13" with a racing body (constructed by Bugatti themselves) and using a chassis with a wheelbase. The others were a "Type 22" and a "Type 23" with wheelbases of respectively.

The company also enjoyed great success in early Grand Prix motor racing: in 1929 a privately entered Bugatti won the first ever Monaco Grand Prix. Racing success culminated with driver Jean-Pierre Wimille winning the 24 hours of Le Mans twice (in 1937 with Robert Benoist and 1939 with Pierre Veyron).

Bugatti cars were extremely successful in racing. The little Bugatti Type 10 swept the top four positions at its first race. The 1924 Bugatti Type 35 is probably the most successful racing car of all time, with over 2,000 wins. The Type 35 was developed by Bugatti with master engineer and racing driver Jean Chassagne who also drove it in the car’s first ever Grand Prix in 1924 Lyon. Bugattis swept to victory in the Targa Florio for five years straight from 1925 through 1929. Louis Chiron held the most podiums in Bugatti cars, and the modern marque revival Bugatti Automobiles S.A.S. named the 1999 Bugatti 18/3 Chiron concept car in his honour. But it was the final racing success at Le Mans that is most remembered—Jean-Pierre Wimille and Pierre Veyron won the 1939 race with just one car and meagre resources.

In the 1930s, Ettore Bugatti got involved in the creation of a racer airplane, hoping to beat the Germans in the Deutsch de la Meurthe prize. This would be the Bugatti 100P, which never flew. It was designed by Belgian engineer Louis de Monge who had already applied Bugatti Brescia engines in his "Type 7.5" lifting body.

Ettore Bugatti also designed a successful motorised railcar, the "" (Autorail Bugatti).

The death of Ettore Bugatti's son, Jean Bugatti, on 11 August 1939 marked a turning point in the company's fortunes. Jean died while testing a Type 57 tank-bodied race car near the Molsheim factory.

World War II left the Molsheim factory in ruins and the company lost control of the property. During the war, Bugatti planned a new factory at Levallois, a northwestern suburb of Paris. After the war, Bugatti designed and planned to build a series of new cars, including the Type 73 road car and Type 73C single seat racing car, but in all Bugatti built only five Type 73 cars.

Development of a 375 cc supercharged car was stopped when Ettore Bugatti died on 21 August 1947. Following Ettore Bugatti's death, the business declined further and made its last appearance as a business in its own right at a Paris Motor Show in October 1952.

After a long decline, the original incarnation of Bugatti ceased operations in 1952.

Bugattis are noticeably focused on design. Engine blocks were hand scraped to ensure that the surfaces were so flat that gaskets were not required for sealing, many of the exposed surfaces of the engine compartment featured "guilloché" (engine turned) finishes on them, and safety wires had been threaded through almost every fastener in intricately laced patterns. Rather than bolt the springs to the axles as most manufacturers did, Bugatti's axles were forged such that the spring passed through a carefully sized opening in the axle, a much more elegant solution requiring fewer parts. He famously described his arch competitor Bentley's cars as "the world's fastest lorries" for focusing on durability. According to Bugatti, "weight was the enemy".

Relatives of Harold Carr found a rare 1937 Bugatti Type 57S Atalante when cataloguing the doctor's belongings after his death in 2009. Carr's Type 57S is notable because it was originally owned by British race car driver Earl Howe. Because much of the car's original equipment is intact, it can be restored without relying on replacement parts.

On 10 July 2009, a 1925 Bugatti Brescia Type 22 which had lain at the bottom of Lake Maggiore on the border of Switzerland and Italy for 75 years was recovered from the lake. The Mullin Museum in Oxnard, California bought it at auction for $351,343 at Bonham's Rétromobile sale in Paris in 2010.

The company attempted a comeback under Roland Bugatti in the mid-1950s with the mid-engined Type 251 race car. Designed with help from Gioacchino Colombo, the car failed to perform to expectations and the company's attempts at automobile production were halted.

In the 1960s, Virgil Exner designed a Bugatti as part of his "Revival Cars" project. A show version of this car was actually built by Ghia using the last Bugatti Type 101 chassis, and was shown at the 1965 Turin Motor Show. Finance was not forthcoming, and Exner then turned his attention to a revival of Stutz.

Bugatti continued manufacturing airplane parts and was sold to Hispano-Suiza, also a former auto maker turned aircraft supplier, in 1963. Snecma took over Hispano-Suiza in 1968. After acquiring Messier, Snecma merged Messier and Bugatti into Messier-Bugatti in 1977.

Italian entrepreneur Romano Artioli acquired the Bugatti brand in 1987, and established Bugatti Automobili S.p.A.. Artioli commissioned architect Giampaolo Benedini to design the factory which was built in Campogalliano, Modena, Italy. Construction of the plant began in 1988, alongside the development of the first model, and it was inaugurated two years later—in 1990.

By 1989 the plans for the new Bugatti revival were presented by Paolo Stanzani and Marcello Gandini, designers of the Lamborghini Miura and Lamborghini Countach. The first production vehicle was the Bugatti EB110 GT. It used a carbon-fibre-reinforced polymer chassis, a 3.5-litre, 5-valve per cylinder, quad-turbocharged 60° V12 engine, a six-speed gearbox, and four-wheel drive.

Famed racing car designer Mauro Forghieri served as Bugatti's technical director from 1992 through 1994.

On 27 August 1993, through his holding company, ACBN Holdings S.A. of Luxembourg, Romano Artioli purchased Lotus Cars from General Motors. Plans were made to list Bugatti shares on international stock exchanges.

Bugatti presented a prototype large saloon called the EB112 in 1993.

Perhaps the most famous Bugatti EB110 owner was seven-time Formula One World Champion racing driver Michael Schumacher who purchased an EB110 in 1994. Schumacher sold his EB110, which had been repaired after a severe 1994 crash, to Modena Motorsport, a Ferrari service and race preparation garage in Germany.

By the time the EB110 came to market, the North American and European economies were in recession. Poor economic conditions forced the company to fail and operations ceased in September 1995. A model specific to the US market called the "Bugatti America" was in the preparatory stages when the company ceased operations.

Bugatti's liquidators sold Lotus Cars to Proton of Malaysia. German firm Dauer Racing purchased the EB110 licence and remaining parts stock in 1997 in order to produce five more EB110 SS vehicles. These five SS versions of the EB110 were greatly refined by Dauer. The Campogalliano factory was sold to a furniture-making company, which became defunct prior to moving in, leaving the building unoccupied. After Dauer stopped producing cars in 2011, Toscana-Motors GmbH of Germany purchased the remaining parts stock from Dauer.

Volkswagen AG acquired the Bugatti brand in 1998. Bugatti Automobiles S.A.S. commissioned Giorgetto Giugiaro of ItalDesign to produce Bugatti Automobiles's first concept vehicle, the EB118, a coupé that debuted at the 1998 Paris Auto Show. The EB118 concept featured a , W-18 engine. After its Paris debut, the EB118 concept was shown again in 1999 at the Geneva Auto Show and the Tokyo Motor Show. Bugatti introduced its next concepts, the EB 218 at the 1999 Geneva Motor Show and the 18/3 Chiron at the 1999 Frankfurt Motor Show (IAA).

Bugatti Automobiles S.A.S. began assembling its first regular-production vehicle, the Bugatti Veyron 16.4 (the 1001 BHP super car with an 8-litre W-16 engine with four turbochargers) in September 2005 at the Bugatti Molsheim, France assembly "studio". On 23 February 2015, Bugatti sold its last Veyron Grand Sport Vitesse, which was named La Finale.

The Bugatti Chiron is a mid-engined, two-seated sports car, designed by Achim Anscheidt, developed as the successor to the Bugatti Veyron. The Chiron was first revealed at the Geneva Motor Show on March 1, 2016.




</doc>
<doc id="4498" url="https://en.wikipedia.org/wiki?curid=4498" title="Benchmark">
Benchmark

Benchmark may refer to:





</doc>
<doc id="4499" url="https://en.wikipedia.org/wiki?curid=4499" title="Band">
Band

Band or BAND may refer to:












</doc>
<doc id="4501" url="https://en.wikipedia.org/wiki?curid=4501" title="Black Death">
Black Death

The Black Death, also known as the Great Plague, the Black Plague, or the Plague, was one of the most devastating pandemics in human history, resulting in the deaths of an estimated people in Eurasia and peaking in Europe from 1347 to 1351.<ref name="ABC/Reuters"></ref> The bacterium "Yersinia pestis", which results in several forms of plague, is believed to have been the cause. The plague created a series of religious, social and economic upheavals, which had profound effects on the course of European history.

The Black Death is thought to have originated in the dry plains of Central Asia, where it travelled along the Silk Road, reaching Crimea by 1343. From there, it was most likely carried by Oriental rat fleas living on the black rats that were regular passengers on merchant ships, spreading throughout the Mediterranean and Europe.

The Black Death is estimated to have killed 30–60% of Europe's total population. In total, the plague may have reduced the world population from an estimated 450 million down to 350–375 million in the 14th century. It took 200 years for the world population to recover to its previous level. The plague recurred as outbreaks in Europe until the 19th century.

The plague disease, caused by "Yersinia pestis", is enzootic (commonly present) in populations of fleas carried by ground rodents, including marmots, in various areas including Central Asia, Kurdistan, Western Asia, Northern India and Uganda. Due to climate change in Asia, rodents began to flee the dried out grasslands to more populated areas, spreading the disease. Nestorian graves dating to 1338–1339 near Lake Issyk Kul in Kyrgyzstan have inscriptions referring to plague and are thought by many epidemiologists to mark the outbreak of the epidemic, from which it could easily have spread to China and India. In October 2010, medical geneticists suggested that all three of the great waves of the plague originated in China. In China, the 13th-century Mongol conquest caused a decline in farming and trading. However, economic recovery had been observed at the beginning of the 14th century. In the 1330s, a large number of natural disasters and plagues led to widespread famine, starting in 1331, with a deadly plague arriving soon after. Epidemics that may have included plague killed an estimated 25 million Chinese and other Asians during the 15 years before it reached Constantinople in 1347.

The disease may have travelled along the Silk Road with Mongol armies and traders or it could have come via ship. By the end of 1346, reports of plague had reached the seaports of Europe: "India was depopulated, Tartary, Mesopotamia, Syria, Armenia were covered with dead bodies".

Plague was reportedly first introduced to Europe via Genoese traders at the port city of Kaffa in the Crimea in 1347. After a protracted siege, during which the Mongol army under Jani Beg was suffering from the disease, the army catapulted infected corpses over the city walls of Kaffa to infect the inhabitants. The Genoese traders fled, taking the plague by ship into Sicily and the south of Europe, whence it spread north. Whether or not this hypothesis is accurate, it is clear that several existing conditions such as war, famine, and weather contributed to the severity of the Black Death.

There appear to have been several introductions into Europe. The plague reached Sicily in October 1347, carried by twelve Genoese galleys, and rapidly spread all over the island. Galleys from Kaffa reached Genoa and Venice in January 1348, but it was the outbreak in Pisa a few weeks later that was the entry point to northern Italy. Towards the end of January, one of the galleys expelled from Italy arrived in Marseille.

From Italy, the disease spread northwest across Europe, striking France, Spain, Portugal and England by June 1348, then turned and spread east through Germany and Scandinavia from 1348 to 1350. It was introduced in Norway in 1349 when a ship landed at Askøy, then spread to Bjørgvin (modern Bergen) and Iceland. Finally it spread to northwestern Russia in 1351. The plague was somewhat less common in parts of Europe that had smaller trade relations with their neighbours, including the majority of the Basque Country, isolated parts of Belgium and the Netherlands, and isolated alpine villages throughout the continent.

Modern researchers do not think that the plague ever became endemic in Europe or its rat population. The disease repeatedly wiped out the rodent carriers so that the fleas died out until a new outbreak from Central Asia repeated the process. The outbreaks have been shown to occur roughly 15 years after a warmer and wetter period in areas where plague is endemic in other species such as gerbils.

The plague struck various regions in the Middle East during the pandemic, leading to serious depopulation and permanent change in both economic and social structures. It spread from China with the Mongols to a trading post in Crimea, called Kaffa, controlled by the Republic of Genoa. As infected rodents infected new rodents, the disease spread across the region, entering also from southern Russia. By autumn 1347, the plague reached Alexandria in Egypt, through the port's trade with Constantinople, and ports on the Black Sea. During 1347, the disease travelled eastward to Gaza, and north along the eastern coast to cities in Lebanon, Syria and Palestine, including Ashkelon, Acre, Jerusalem, Sidon, Damascus, Homs, and Aleppo. In 1348–1349, the disease reached Antioch. The city's residents fled to the north, However most of them ended up dying during the journey.

Mecca became infected in 1349. During the same year, records show the city of Mawsil (Mosul) suffered a massive epidemic, and the city of Baghdad experienced a second round of the disease.

Contemporary accounts of the plague are often varied or imprecise. The most commonly noted symptom was the appearance of buboes (or gavocciolos) in the groin, the neck and armpits, which oozed pus and bled when opened. Boccaccio's description:

The only medical detail that is questionable in Boccaccio's description is that the gavocciolo was an 'infallible token of approaching death', as, if the bubo discharges, recovery is possible.

This was followed by acute fever and vomiting of blood. Most victims died two to seven days after initial infection. Freckle-like spots and rashes, which could have been caused by flea-bites, were identified as another potential sign of the plague.

Some accounts, like that of Lodewijk Heyligen, whose master the Cardinal Colonna died of the plague in 1348, noted a distinct form of the disease that infected the lungs and led to respiratory problems and is identified with pneumonic plague.

Medical knowledge had stagnated during the Middle Ages. The most authoritative account at the time came from the medical faculty in Paris in a report to the king of France that blamed the heavens, in the form of a conjunction of three planets in 1345 that caused a "great pestilence in the air". This report became the first and most widely circulated of a series of plague tracts that sought to give advice to sufferers. That the plague was caused by bad air became the most widely accepted theory. Today, this is known as the miasma theory. The word "plague" had no special significance at this time, and only the recurrence of outbreaks during the Middle Ages gave it the name that has become the medical term.

The importance of hygiene was recognised only in the nineteenth century; until then it was common that the streets were filthy, with live animals of all sorts around and human parasites abounding. A transmissible disease will spread easily in such conditions. One development as a result of the Black Death was the establishment of the idea of quarantine in Dubrovnik in 1377 after continuing outbreaks.

The dominant explanation for the Black Death is the plague theory, which attributes the outbreak to "Yersinia pestis", also responsible for an epidemic that began in southern China in 1865, eventually spreading to India. The investigation of the pathogen that caused the 19th-century plague was begun by teams of scientists who visited Hong Kong in 1894, among whom was the French-Swiss bacteriologist Alexandre Yersin, after whom the pathogen was named "Yersinia pestis". The mechanism by which "Y. pestis" was usually transmitted was established in 1898 by Paul-Louis Simond and was found to involve the bites of fleas whose midguts had become obstructed by replicating "Y. pestis" several days after feeding on an infected host. This blockage results in starvation and aggressive feeding behaviour by the fleas, which repeatedly attempt to clear their blockage by regurgitation, resulting in thousands of plague bacteria being flushed into the feeding site, infecting the host. The bubonic plague mechanism was also dependent on two populations of rodents: one resistant to the disease, which act as hosts, keeping the disease endemic, and a second that lack resistance. When the second population dies, the fleas move on to other hosts, including people, thus creating a human epidemic.

The historian Francis Aidan Gasquet wrote about the Great Pestilence in 1893 and suggested that "it would appear to be some form of the ordinary Eastern or bubonic plague". He was able to adopt the epidemiology of the bubonic plague for the Black Death for the second edition in 1908, implicating rats and fleas in the process, and his interpretation was widely accepted for other ancient and medieval epidemics, such as the Justinian plague that was prevalent in the Eastern Roman Empire from 541 to 700 CE.

An estimate of the mortality rate for the modern bubonic plague, following the introduction of antibiotics, is 11%, although it may be higher in underdeveloped regions. Symptoms of the disease include fever of , headaches, painful aching joints, nausea and vomiting, and a general feeling of malaise. Left untreated, of those that contract the bubonic plague, 80 per cent die within eight days. Pneumonic plague has a mortality rate of 90 to 95 per cent. Symptoms include fever, cough, and blood-tinged sputum. As the disease progresses, sputum becomes free-flowing and bright red. Septicemic plague is the least common of the three forms, with a mortality rate near 100%. Symptoms are high fevers and purple skin patches (purpura due to disseminated intravascular coagulation). In cases of pneumonic and particularly septicemic plague, the progress of the disease is so rapid that there would often be no time for the development of the enlarged lymph nodes that were noted as buboes.

A number of alternative theories – implicating other diseases in the Black Death pandemic – have also been proposed by some modern scientists (see below – "Alternative Explanations").

In October 2010, the open-access scientific journal "PLoS Pathogens" published a paper by a multinational team who undertook a new investigation into the role of "Yersinia pestis" in the Black Death following the disputed identification by Drancourt and Raoult in 1998. They assessed the presence of DNA/RNA with polymerase chain reaction (PCR) techniques for "Y. pestis" from the tooth sockets in human skeletons from mass graves in northern, central and southern Europe that were associated archaeologically with the Black Death and subsequent resurgences. The authors concluded that this new research, together with prior analyses from the south of France and Germany, "ends the debate about the cause of the Black Death, and unambiguously demonstrates that "Y. pestis" was the causative agent of the epidemic plague that devastated Europe during the Middle Ages".

The study also found that there were two previously unknown but related clades (genetic branches) of the "Y. pestis" genome associated with medieval mass graves. These clades (which are thought to be extinct) were found to be ancestral to modern isolates of the modern "Y. pestis" strains "Y. p. orientalis" and "Y. p. medievalis", suggesting the plague may have entered Europe in two waves. Surveys of plague pit remains in France and England indicate the first variant entered Europe through the port of Marseille around November 1347 and spread through France over the next two years, eventually reaching England in the spring of 1349, where it spread through the country in three epidemics. Surveys of plague pit remains from the Dutch town of Bergen op Zoom showed the "Y. pestis" genotype responsible for the pandemic that spread through the Low Countries from 1350 differed from that found in Britain and France, implying Bergen op Zoom (and possibly other parts of the southern Netherlands) was not directly infected from England or France in 1349 and suggesting a second wave of plague, different from those in Britain and France, may have been carried to the Low Countries from Norway, the Hanseatic cities or another site.

The results of the Haensch study have since been confirmed and amended. Based on genetic evidence derived from Black Death victims in the East Smithfield burial site in England, Schuenemann et al. concluded in 2011 "that the Black Death in medieval Europe was caused by a variant of "Y. pestis" that may no longer exist." A study published in "Nature" in October 2011 sequenced the genome of "Y. pestis" from plague victims and indicated that the strain that caused the Black Death is ancestral to most modern strains of the disease.

DNA taken from 25 skeletons from the 14th century found in London have shown the plague is a strain of "Y. pestis" that is almost identical to that which hit Madagascar in 2013.

The plague theory was first significantly challenged by the work of British bacteriologist J. F. D. Shrewsbury in 1970, who noted that the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations. In 1984 zoologist Graham Twigg produced the first major work to challenge the bubonic plague theory directly, and his doubts about the identity of the Black Death have been taken up by a number of authors, including Samuel K. Cohn, Jr. (2002 and 2013), David Herlihy (1997), and Susan Scott and Christopher Duncan (2001).

It is recognised that an epidemiological account of the plague is as important as an identification of symptoms, but researchers are hampered by the lack of reliable statistics from this period. Most work has been done on the spread of the plague in England, and even estimates of overall population at the start vary by over 100% as no census was undertaken between the time of publication of the Domesday Book and the year 1377. Estimates of plague victims are usually extrapolated from figures from the clergy.

In addition to arguing that the rat population was insufficient to account for a bubonic plague pandemic, sceptics of the bubonic plague theory point out that the symptoms of the Black Death are not unique (and arguably in some accounts may differ from bubonic plague); that transference via fleas in goods was likely to be of marginal significance; and that the DNA results may be flawed and might not have been repeated elsewhere, despite extensive samples from other mass graves. Other arguments include the lack of accounts of the death of rats before outbreaks of plague between the 14th and 17th centuries; temperatures that are too cold in northern Europe for the survival of fleas; that, despite primitive transport systems, the spread of the Black Death was much faster than that of modern bubonic plague; that mortality rates of the Black Death appear to be very high; that, while modern bubonic plague is largely endemic as a rural disease, the Black Death indiscriminately struck urban and rural areas; and that the pattern of the Black Death, with major outbreaks in the same areas separated by 5 to 15 years, differs from modern bubonic plague—which often becomes endemic for decades with annual flare-ups.

McCormick has suggested that earlier archaeologists were simply not interested in the "laborious" processes needed to discover rat remains. Walløe complains that all of these authors "take it for granted that Simond's infection model, black rat → rat flea → human, which was developed to explain the spread of plague in India, is the only way an epidemic of "Yersinia pestis" infection could spread", whilst pointing to several other possibilities. Similarly, Green has argued that greater attention is needed to the range of (especially non-commensal) animals that might be involved in the transmission of plague.

A variety of alternatives to the "Y. pestis" have been put forward. Twigg suggested that the cause was a form of anthrax, and Norman Cantor thought it may have been a combination of anthrax and other pandemics. Scott and Duncan have argued that the pandemic was a form of infectious disease that they characterise as "hemorrhagic" plague similar to Ebola. Archaeologist Barney Sloane has argued that there is insufficient evidence of the extinction of a large number of rats in the archaeological record of the medieval waterfront in London and that the plague spread too quickly to support the thesis that the "Y. pestis" was spread from fleas on rats; he argues that transmission must have been person to person. This theory is supported by research in 2018 which suggested transmission was more likely by body lice and human fleas during the Second Pandemic.

However, no single alternative solution has achieved widespread acceptance. Many scholars arguing for the "Y. pestis" as the major agent of the pandemic suggest that its extent and symptoms can be explained by a combination of bubonic plague with other diseases, including typhus, smallpox and respiratory infections. In addition to the bubonic infection, others point to additional septicemic (a type of "blood poisoning") and pneumonic (an airborne plague that attacks the lungs before the rest of the body) forms of the plague, which lengthen the duration of outbreaks throughout the seasons and help account for its high mortality rate and additional recorded symptoms. In 2014, scientists with Public Health England announced the results of an examination of 25 bodies exhumed from the Clerkenwell area of London, as well as of wills registered in London during the period, which supported the pneumonic hypothesis.

There are no exact figures for the death toll; the rate varied widely by locality. In urban centres, the greater the population before the outbreak, the longer the duration of the period of abnormal mortality. It killed some people in Eurasia. According to medieval historian Philip Daileader in 2007:

The trend of recent research is pointing to a figure more like 45–50% of the European population dying during a four-year period. There is a fair amount of geographic variation. In Mediterranean Europe, areas such as Italy, the south of France and Spain, where plague ran for about four years consecutively, it was probably closer to 75–80% of the population. In Germany and England ... it was probably closer to 20%.

A death rate as high as 60% in Europe has been suggested by Norwegian historian Ole Benedictow:

Detailed study of the mortality data available points to two conspicuous features in relation to the mortality caused by the Black Death: namely the extreme level of mortality caused by the Black Death, and the remarkable similarity or consistency of the level of mortality, from Spain in southern Europe to England in north-western Europe. The data is sufficiently widespread and numerous to make it likely that the Black Death swept away around 60 per cent of Europe's population. It is generally assumed that the size of Europe's population at the time was around 80 million. This implies that around 50 million people died in the Black Death.

The most widely accepted estimate for the Middle East, including Iraq, Iran and Syria, during this time, is for a death rate of about a third. The Black Death killed about 40% of Egypt's population. Half of Paris's population of 100,000 people died. In Italy, the population of Florence was reduced from 110,000–120,000 inhabitants in 1338 down to 50,000 in 1351. At least 60% of the population of Hamburg and Bremen perished, and a similar percentage of Londoners may have died from the disease as well. In London approximately 62,000 people died between the years between 1346 and 1353. While contemporary reports account of mass burial pits being created in response to the large numbers of dead, recent scientific investigations of a burial pit in Central London found well-preserved individuals to be buried in isolated, evenly spaced graves, suggesting at least some pre-planning and Christian burials at this time. Before 1350, there were about 170,000 settlements in Germany, and this was reduced by nearly 40,000 by 1450. In 1348, the plague spread so rapidly that before any physicians or government authorities had time to reflect upon its origins, about a third of the European population had already perished. In crowded cities, it was not uncommon for as much as 50% of the population to die. The disease bypassed some areas, and the most isolated areas were less vulnerable to contagion. Monks and priests were especially hard-hit since they cared for victims of the Black Death.

Renewed religious fervour and fanaticism bloomed in the wake of the Black Death. Some Europeans targeted "various groups such as Jews, friars, foreigners, beggars, pilgrims", lepers, and Romani, thinking that they were to blame for the crisis. Lepers, and other individuals with skin diseases such as acne or psoriasis, were singled out and exterminated throughout Europe.

Because 14th-century healers were at a loss to explain the cause, Europeans turned to astrological forces, earthquakes, and the poisoning of wells by Jews as possible reasons for the plague's emergence. The governments of Europe had no apparent response to the crisis because no one knew its cause or how it spread. The mechanism of infection and transmission of diseases was little understood in the 14th century; many people believed the epidemic was a punishment by God for their sins. This belief led to the idea that the cure to the disease was to win God's forgiveness.

There were many attacks against Jewish communities. In February 1349, the citizens of Strasbourg murdered 2,000 Jews. In August 1349, the Jewish communities in Mainz and Cologne were annihilated. By 1351, 60 major and 150 smaller Jewish communities had been destroyed. These massacres eventually died out in Western Europe, only to continue on in Eastern Europe. During this period many Jews relocated to Poland, where they received a warm welcome from King Casimir the Great.

The plague repeatedly returned to haunt Europe and the Mediterranean throughout the 14th to 17th centuries. According to Biraben, the plague was present somewhere in Europe in every year between 1346 and 1671. The was particularly widespread in the following years: 1360–1363; 1374; 1400; 1438–1439; 1456–1457; 1464–1466; 1481–1485; 1500–1503; 1518–1531; 1544–1548; 1563–1566; 1573–1588; 1596–1599; 1602–1611; 1623–1640; 1644–1654; and 1664–1667. Subsequent outbreaks, though severe, marked the retreat from most of Europe (18th century) and northern Africa (19th century). According to Geoffrey Parker, "France alone lost almost a million people to the plague in the epidemic of 1628–31."

In England, in the absence of census figures, historians propose a range of preincident population figures from as high as 7 million to as low as 4 million in 1300, and a postincident population figure as low as 2 million. By the end of 1350, the Black Death subsided, but it never really died out in England. Over the next few hundred years, further outbreaks occurred in 1361–1362, 1369, 1379–1383, 1389–1393, and throughout the first half of the 15th century. An outbreak in 1471 took as much as 10–15% of the population, while the death rate of the plague of 1479–1480 could have been as high as 20%. The most general outbreaks in Tudor and Stuart England seem to have begun in 1498, 1535, 1543, 1563, 1589, 1603, 1625, and 1636, and ended with the Great Plague of London in 1665.

In 1466, perhaps 40,000 people died of the plague in Paris. During the 16th and 17th centuries, the plague was present in Paris around 30 per cent of the time. The Black Death ravaged Europe for three years before it continued on into Russia, where the disease was present somewhere in the country 25 times between 1350 and 1490. Plague epidemics ravaged London in 1563, 1593, 1603, 1625, 1636, and 1665, reducing its population by 10 to 30% during those years. Over 10% of Amsterdam's population died in 1623–1625, and again in 1635–1636, 1655, and 1664. Plague occurred in Venice 22 times between 1361 and 1528. The plague of 1576–1577 killed 50,000 in Venice, almost a third of the population. Late outbreaks in central Europe included the Italian Plague of 1629–1631, which is associated with troop movements during the Thirty Years' War, and the Great Plague of Vienna in 1679. Over 60% of Norway's population died in 1348–1350. The last plague outbreak ravaged Oslo in 1654.

In the first half of the 17th century, a plague claimed some 1.7 million victims in Italy, or about 14% of the population. In 1656, the plague killed about half of Naples' 300,000 inhabitants. More than 1.25 million deaths resulted from the extreme incidence of plague in 17th-century Spain. The plague of 1649 probably reduced the population of Seville by half. In 1709–1713, a plague epidemic that followed the Great Northern War (1700–1721, Sweden v. Russia and allies) killed about 100,000 in Sweden, and 300,000 in Prussia. The plague killed two-thirds of the inhabitants of Helsinki, and claimed a third of Stockholm's population. Europe's last major epidemic occurred in 1720 in Marseille.

The Black Death ravaged much of the Islamic world. Plague was present in at least one location in the Islamic world virtually every year between 1500 and 1850. Plague repeatedly struck the cities of North Africa. Algiers lost 30,000–50,000 inhabitants to it in 1620–1621, and again in 1654–1657, 1665, 1691, and 1740–1742. Plague remained a major event in Ottoman society until the second quarter of the 19th century. Between 1701 and 1750, thirty-seven larger and smaller epidemics were recorded in Constantinople, and an additional thirty-one between 1751 and 1800. Baghdad has suffered severely from visitations of the plague, and sometimes two-thirds of its population has been wiped out.

The third plague pandemic (1855–1859) started in China in the mid-19th century, spreading to all inhabited continents and killing 10 million people in India alone. Twelve plague outbreaks in Australia between 1900 and 1925 resulted in well over 1,000 deaths, chiefly in Sydney. This led to the establishment of a Public Health Department there which undertook some leading-edge research on plague transmission from rat fleas to humans via the bacillus "Yersinia pestis".

The first North American plague epidemic was the San Francisco plague of 1900–1904, followed by another outbreak in 1907–1908.

Modern treatment methods include insecticides, the use of antibiotics, and a plague vaccine. The plague bacterium could develop drug resistance and again become a major health threat. One case of a drug-resistant form of the bacterium was found in Madagascar in 1995. A further outbreak in Madagascar was reported in November 2014. In October 2017 the deadliest outbreak of the plague in modern times hit Madagascar, killing 170 people and infecting thousands.

The phrase (') was used in 1350 by Simon de Covino or Couvin, a Belgian astronomer, who wrote the poem "On the Judgment of the Sun at a Feast of Saturn" ('), which attributes the plague to a conjunction of Jupiter and Saturn. 
In 1908, Gasquet claimed that use of the name ' for the 14th-century epidemic first appeared in a 1631 book on Danish history by J. I. Pontanus: "Commonly and from its effects, they called it the black death" ('). The name spread through Scandinavia and then Germany, gradually becoming attached to the mid 14th-century epidemic as a proper name. 
However, ' is used to refer to a pestilential fever (') already in the 12th-century "On the Signs and Symptoms of Diseases" () by French physician Gilles de Corbeil.
In England, the phrase "Black Death" is first used to refer to the 14th-century epidemic in 1823.
Writers contemporary with the plague described the event as "great plague" or "great pestilence". 





</doc>
<doc id="4502" url="https://en.wikipedia.org/wiki?curid=4502" title="Biotechnology">
Biotechnology

Biotechnology is the broad area of science involving living systems and organisms to develop or make products, or "any technological application that uses biological systems, living organisms, or derivatives thereof, to make or modify products or processes for specific use" (UN Convention on Biological Diversity, Art. 2). Depending on the tools and applications, it often overlaps with the (related) fields of molecular biology, bio-engineering, biomedical engineering, biomanufacturing, molecular engineering, etc.

For thousands of years, humankind has used biotechnology in agriculture, food production, and medicine. The term is largely believed to have been coined in 1919 by Hungarian engineer Károly Ereky. In the late 20th and early 21st centuries, biotechnology has expanded to include new and diverse sciences such as genomics, recombinant gene techniques, applied immunology, and development of pharmaceutical therapies and diagnostic tests.

The wide concept of "biotech" or "biotechnology" encompasses a wide range of procedures for modifying living organisms according to human purposes, going back to domestication of animals, cultivation of the plants, and "improvements" to these through breeding programs that employ artificial selection and hybridization. Modern usage also includes genetic engineering as well as cell and tissue culture technologies. The American Chemical Society defines biotechnology as the application of biological organisms, systems, or processes by various industries to learning about the science of life and the improvement of the value of materials and organisms such as pharmaceuticals, crops, and livestock. As per European Federation of Biotechnology, biotechnology is the integration of natural science and organisms, cells, parts thereof, and molecular analogues for products and services. Biotechnology is based on the basic biological sciences (e.g. molecular biology, biochemistry, cell biology, embryology, genetics, microbiology) and conversely provides methods to support and perform basic research in biology.

Biotechnology is the research and development in the laboratory using bioinformatics for exploration, extraction, exploitation and production from any living organisms and any source of biomass by means of biochemical engineering where high value-added products could be planned (reproduced by biosynthesis, for example), forecasted, formulated, developed, manufactured, and marketed for the purpose of sustainable operations (for the return from bottomless initial investment on R & D) and gaining durable patents rights (for exclusives rights for sales, and prior to this to receive national and international approval from the results on animal experiment and human experiment, especially on the pharmaceutical branch of biotechnology to prevent any undetected side-effects or safety concerns by using the products).

By contrast, bioengineering is generally thought of as a related field that more heavily emphasizes higher systems approaches (not necessarily the altering or using of biological materials "directly") for interfacing with and utilizing living things. Bioengineering is the application of the principles of engineering and natural sciences to tissues, cells and molecules. This can be considered as the use of knowledge from working with and manipulating biology to achieve a result that can improve functions in plants and animals. Relatedly, biomedical engineering is an overlapping field that often draws upon and applies "biotechnology" (by various definitions), especially in certain sub-fields of biomedical or chemical engineering such as tissue engineering, biopharmaceutical engineering, and genetic engineering.

Although not normally what first comes to mind, many forms of human-derived agriculture clearly fit the broad definition of "'utilizing a biotechnological system to make products". Indeed, the cultivation of plants may be viewed as the earliest biotechnological enterprise.

Agriculture has been theorized to have become the dominant way of producing food since the Neolithic Revolution. Through early biotechnology, the earliest farmers selected and bred the best suited crops, having the highest yields, to produce enough food to support a growing population. As crops and fields became increasingly large and difficult to maintain, it was discovered that specific organisms and their by-products could effectively fertilize, restore nitrogen, and control pests. Throughout the history of agriculture, farmers have inadvertently altered the genetics of their crops through introducing them to new environments and breeding them with other plants — one of the first forms of biotechnology.

These processes also were included in early fermentation of beer. These processes were introduced in early Mesopotamia, Egypt, China and India, and still use the same basic biological methods. In brewing, malted grains (containing enzymes) convert starch from grains into sugar and then adding specific yeasts to produce beer. In this process, carbohydrates in the grains broke down into alcohols, such as ethanol. Later, other cultures produced the process of lactic acid fermentation, which produced other preserved foods, such as soy sauce. Fermentation was also used in this time period to produce leavened bread. Although the process of fermentation was not fully understood until Louis Pasteur's work in 1857, it is still the first use of biotechnology to convert a food source into another form.

Before the time of Charles Darwin's work and life, animal and plant scientists had already used selective breeding. Darwin added to that body of work with his scientific observations about the ability of science to change species. These accounts contributed to Darwin's theory of natural selection.

For thousands of years, humans have used selective breeding to improve production of crops and livestock to use them for food. In selective breeding, organisms with desirable characteristics are mated to produce offspring with the same characteristics. For example, this technique was used with corn to produce the largest and sweetest crops.

In the early twentieth century scientists gained a greater understanding of microbiology and explored ways of manufacturing specific products. In 1917, Chaim Weizmann first used a pure microbiological culture in an industrial process, that of manufacturing corn starch using "Clostridium acetobutylicum," to produce acetone, which the United Kingdom desperately needed to manufacture explosives during World War I.

Biotechnology has also led to the development of antibiotics. In 1928, Alexander Fleming discovered the mold "Penicillium". His work led to the purification of the antibiotic compound formed by the mold by Howard Florey, Ernst Boris Chain and Norman Heatley – to form what we today know as penicillin. In 1940, penicillin became available for medicinal use to treat bacterial infections in humans.

The field of modern biotechnology is generally thought of as having been born in 1971 when Paul Berg's (Stanford) experiments in gene splicing had early success. Herbert W. Boyer (Univ. Calif. at San Francisco) and Stanley N. Cohen (Stanford) significantly advanced the new technology in 1972 by transferring genetic material into a bacterium, such that the imported material would be reproduced. The commercial viability of a biotechnology industry was significantly expanded on June 16, 1980, when the United States Supreme Court ruled that a genetically modified microorganism could be patented in the case of "Diamond v. Chakrabarty". Indian-born Ananda Chakrabarty, working for General Electric, had modified a bacterium (of the genus "Pseudomonas") capable of breaking down crude oil, which he proposed to use in treating oil spills. (Chakrabarty's work did not involve gene manipulation but rather the transfer of entire organelles between strains of the "Pseudomonas" bacterium.

Revenue in the industry is expected to grow by 12.9% in 2008. Another factor influencing the biotechnology sector's success is improved intellectual property rights legislation—and enforcement—worldwide, as well as strengthened demand for medical and pharmaceutical products to cope with an ageing, and ailing, U.S. population.

Rising demand for biofuels is expected to be good news for the biotechnology sector, with the Department of Energy estimating ethanol usage could reduce U.S. petroleum-derived fuel consumption by up to 30% by 2030. The biotechnology sector has allowed the U.S. farming industry to rapidly increase its supply of corn and soybeans—the main inputs into biofuels—by developing genetically modified seeds that resist pests and drought. By increasing farm productivity, biotechnology boosts biofuel production.

Biotechnology has applications in four major industrial areas, including health care (medical), crop production and agriculture, non-food (industrial) uses of crops and other products (e.g. biodegradable plastics, vegetable oil, biofuels), and environmental uses.

For example, one application of biotechnology is the directed use of organisms for the manufacture of organic products (examples include beer and milk products). Another example is using naturally present bacteria by the mining industry in bioleaching. Biotechnology is also used to recycle, treat waste, clean up sites contaminated by industrial activities (bioremediation), and also to produce biological weapons.

A series of derived terms have been coined to identify several branches of biotechnology, for example:

The investment and economic output of all of these types of applied biotechnologies is termed as "bioeconomy".

In medicine, modern biotechnology finds many applications in areas such as pharmaceutical drug discoveries and production, pharmacogenomics, and genetic testing (or genetic screening).
Pharmacogenomics (a combination of pharmacology and genomics) is the technology that analyses how genetic makeup affects an individual's response to drugs. It deals with the influence of genetic variation on drug responses in patients by correlating gene expression or single-nucleotide polymorphisms with a drug's efficacy or toxicity. By doing so, pharmacogenomics aims to develop rational means to optimize drug therapy, with respect to the patients' genotype, to ensure maximum efficacy with minimal adverse effects. Such approaches promise the advent of "personalized medicine"; in which drugs and drug combinations are optimized for each individual's unique genetic makeup.

Biotechnology has contributed to the discovery and manufacturing of traditional small molecule pharmaceutical drugs as well as drugs that are the product of biotechnology – biopharmaceutics. Modern biotechnology can be used to manufacture existing medicines relatively easily and cheaply. The first genetically engineered products were medicines designed to treat human diseases. To cite one example, in 1978 Genentech developed synthetic humanized insulin by joining its gene with a plasmid vector inserted into the bacterium "Escherichia coli". Insulin, widely used for the treatment of diabetes, was previously extracted from the pancreas of abattoir animals (cattle or pigs). The resulting genetically engineered bacterium enabled the production of vast quantities of synthetic human insulin at relatively low cost. Biotechnology has also enabled emerging therapeutics like gene therapy. The application of biotechnology to basic science (for example through the Human Genome Project) has also dramatically improved our understanding of biology and as our scientific knowledge of normal and disease biology has increased, our ability to develop new medicines to treat previously untreatable diseases has increased as well.

Genetic testing allows the genetic diagnosis of vulnerabilities to inherited diseases, and can also be used to determine a child's parentage (genetic mother and father) or in general a person's ancestry. In addition to studying chromosomes to the level of individual genes, genetic testing in a broader sense includes biochemical tests for the possible presence of genetic diseases, or mutant forms of genes associated with increased risk of developing genetic disorders. Genetic testing identifies changes in chromosomes, genes, or proteins. Most of the time, testing is used to find changes that are associated with inherited disorders. The results of a genetic test can confirm or rule out a suspected genetic condition or help determine a person's chance of developing or passing on a genetic disorder. As of 2011 several hundred genetic tests were in use. Since genetic testing may open up ethical or psychological problems, genetic testing is often accompanied by genetic counseling.

Genetically modified crops ("GM crops", or "biotech crops") are plants used in agriculture, the DNA of which has been modified with genetic engineering techniques. In most cases, the main aim is to introduce a new trait that does not occur naturally in the species.

Examples in food crops include resistance to certain pests, diseases, stressful environmental conditions, resistance to chemical treatments (e.g. resistance to a herbicide), reduction of spoilage, or improving the nutrient profile of the crop. Examples in non-food crops include production of pharmaceutical agents, biofuels, and other industrially useful goods, as well as for bioremediation.

Farmers have widely adopted GM technology. Between 1996 and 2011, the total surface area of land cultivated with GM crops had increased by a factor of 94, from to 1,600,000 km (395 million acres). 10% of the world's crop lands were planted with GM crops in 2010. As of 2011, 11 different transgenic crops were grown commercially on 395 million acres (160 million hectares) in 29 countries such as the USA, Brazil, Argentina, India, Canada, China, Paraguay, Pakistan, South Africa, Uruguay, Bolivia, Australia, Philippines, Myanmar, Burkina Faso, Mexico and Spain.

Genetically modified foods are foods produced from organisms that have had specific changes introduced into their DNA with the methods of genetic engineering. These techniques have allowed for the introduction of new crop traits as well as a far greater control over a food's genetic structure than previously afforded by methods such as selective breeding and mutation breeding. Commercial sale of genetically modified foods began in 1994, when Calgene first marketed its Flavr Savr delayed ripening tomato. To date most genetic modification of foods have primarily focused on cash crops in high demand by farmers such as soybean, corn, canola, and cotton seed oil. These have been engineered for resistance to pathogens and herbicides and better nutrient profiles. GM livestock have also been experimentally developed; in November 2013 none were available on the market, but in 2015 the FDA approved the first GM salmon for commercial production and consumption.

There is a scientific consensus that currently available food derived from GM crops poses no greater risk to human health than conventional food, but that each GM food must be tested on a case-by-case basis before introduction. Nonetheless, members of the public are much less likely than scientists to perceive GM foods as safe. The legal and regulatory status of GM foods varies by country, with some nations banning or restricting them, and others permitting them with widely differing degrees of regulation.

GM crops also provide a number of ecological benefits, if not used in excess. However, opponents have objected to GM crops per se on several grounds, including environmental concerns, whether food produced from GM crops is safe, whether GM crops are needed to address the world's food needs, and economic concerns raised by the fact these organisms are subject to intellectual property law.
Industrial biotechnology (known mainly in Europe as white biotechnology) is the application of biotechnology for industrial purposes, including industrial fermentation. It includes the practice of using cells such as micro-organisms, or components of cells like enzymes, to generate industrially useful products in sectors such as chemicals, food and feed, detergents, paper and pulp, textiles and biofuels. In doing so, biotechnology uses renewable raw materials and may contribute to lowering greenhouse gas emissions and moving away from a petrochemical-based economy.

The environment can be affected by biotechnologies, both positively and adversely. Vallero and others have argued that the difference between beneficial biotechnology (e.g.bioremediation is to clean up an oil spill or hazard chemical leak) versus the adverse effects stemming from biotechnological enterprises (e.g. flow of genetic material from transgenic organisms into wild strains) can be seen as applications and implications, respectively. Cleaning up environmental wastes is an example of an application of environmental biotechnology; whereas loss of biodiversity or loss of containment of a harmful microbe are examples of environmental implications of biotechnology.

The regulation of genetic engineering concerns approaches taken by governments to assess and manage the risks associated with the use of genetic engineering technology, and the development and release of genetically modified organisms (GMO), including genetically modified crops and genetically modified fish. There are differences in the regulation of GMOs between countries, with some of the most marked differences occurring between the USA and Europe. Regulation varies in a given country depending on the intended use of the products of the genetic engineering. For example, a crop not intended for food use is generally not reviewed by authorities responsible for food safety. The European Union differentiates between approval for cultivation within the EU and approval for import and processing. While only a few GMOs have been approved for cultivation in the EU a number of GMOs have been approved for import and processing. The cultivation of GMOs has triggered a debate about coexistence of GM and non GM crops. Depending on the coexistence regulations incentives for cultivation of GM crops differ.

In 1988, after prompting from the United States Congress, the National Institute of General Medical Sciences (National Institutes of Health) (NIGMS) instituted a funding mechanism for biotechnology training. Universities nationwide compete for these funds to establish Biotechnology Training Programs (BTPs). Each successful application is generally funded for five years then must be competitively renewed. Graduate students in turn compete for acceptance into a BTP; if accepted, then stipend, tuition and health insurance support is provided for two or three years during the course of their Ph.D. thesis work. Nineteen institutions offer NIGMS supported BTPs. Biotechnology training is also offered at the undergraduate level and in community colleges.





</doc>
<doc id="4503" url="https://en.wikipedia.org/wiki?curid=4503" title="Battle of Poitiers">
Battle of Poitiers

The Battle of Poitiers was a major English victory in the Edwardian phase of the Hundred Years' War. It was fought on 19 September 1356 in Nouaillé, near the city of Poitiers in Aquitaine, western France. An army of English, Welsh, Breton and Gascon troops, many of them veterans of Crécy, led by Edward, the Black Prince, defeated a larger French and allied army led by King John II of France, leading to the capture of the king, his son, and much of the French nobility.

Poitiers was the second major English victory of the Hundred Years' War. Poitiers was fought ten years after the Battle of Crécy (the first major victory), and about half a century before the third, the Battle of Agincourt (1415).

The effect of the defeat on France was catastrophic, leaving the country in the hands of the Dauphin Charles. Charles faced populist revolts across the kingdom in the wake of the battle, which had destroyed the prestige of the French upper-class. The Edwardian phase of the war would end four years later in 1360, on favourable terms for England.

The town and battle were often referred to as "Poictiers" in contemporaneous recordings, a name commemorated in several warships of the Royal Navy.

Following the death of Charles IV of France in 1328, Philip, Count of Valois, had been crowned as his successor, over his closest male relative and legal successor, Edward III of England. Edward had been reluctant to pay homage to Philip in his role as Duke of Aquitaine, resulting in Philip confiscating these lands in 1337, precipitating war between the two nations. Three years later, Edward declared himself King of France. The war had begun well for the English. They had achieved naval domination early in the war at the Battle of Sluys in 1340, inflicted a severe defeat on the French army at Crécy in 1346, and captured Calais in 1347.

In the late 1340s and early 1350s, the Black Death decimated the population of Western Europe, bringing all significant efforts in campaigning to a halt, one such victim being Philip VI of France himself. In 1355, Edward III laid out plans for a second major campaign. His eldest son, Edward, the Black Prince, now an experienced soldier following the Crécy campaign, landed at Bordeaux in Aquitaine, leading his army on a march through southern France to Carcassonne. Unable to take the heavily fortified settlement, Edward withdrew to Bordeaux. In early 1356, the Duke of Lancaster led an army through Normandy, while Edward led his army on a great chevauchée from Bordeaux on 8 August 1356.

Edward's forces met little resistance, sacking numerous settlements, until they reached the Loire River at Tours. They were unable to take the castle or burn the town due to a heavy rainstorm. This delay allowed King John II to attempt to pin down and destroy Edward's army. John, who had been besieging Breteuil in Normandy, organised the bulk of his army at Chartres to the north of Tours. In order to increase the speed of his army's march, he dismissed between 15,000 and 20,000 of his lower quality infantry, just as Edward turned back to Bordeaux. The French rode hard and cut in front of the English army, crossing the bridge over the Vienne at Chauvigny. Learning of this, the Black Prince quickly moved his army south. Historians disagree over whether the outnumbered English commander was seeking battle or trying to avoid it. In any case, after preliminary maneuvers and failed negotiations for a truce, the two armies faced off, both ready for battle, near Poitiers on Monday, 19 September 1356.

Edward arrayed his army in a defensive posture among the hedges and orchards of the area, in front of the forest of Nouaillé. He deployed his front line of longbowmen behind a particularly prominent thick hedge, through which the road ran at right angles. The Earl of Douglas, commanding the Scottish division in the French army, advised King John that the attack should be delivered on foot, with horses being particularly vulnerable to English arrows. John heeded this advice, leaving its baggage behind and forming up on foot in front of the English. The English gained vantage points on the natural high ground in order for their longbowmen to have an advantage on the French heavily armored troops.

The English army was led by Edward, the Black Prince and composed primarily of English and Welsh troops, though there was a large contingent of Gascon and Breton soldiers with the army. Edward's army consisted of approximately 2,000 longbowmen, 3,000 men-at-arms and a force of 1,000 Gascon infantry.

Like the earlier engagement at Crécy, the power of the English army lay in the longbow, a tall, thick self-bow made of yew. Longbows had demonstrated their effectiveness against massed infantry and cavalry in several battles, such as Falkirk in 1298, Halidon Hill in 1333 and Crécy ten years prior, in 1346. Poitiers was the second of three major English victories of the Hundred Years' War attributed to the longbow, though its effectiveness against armoured French knights and men-at-arms has been disputed.

Geoffrey the Baker wrote that the English archers under the earl of Salisbury "made their arrows prevail over the [French] knights' armor," but the bowmen on the other flank, under Warwick, were initially ineffective against the mounted French men-at-arms who enjoyed the double protection of steel plate armor and large leather shields. Once Warwick's archers redeployed to a position where they could hit the unarmored sides and backs of the horses, however, they quickly routed the cavalry force opposing them. The archers were also unquestionably effective against common infantry, who did not have the wealth to afford plate armour.

The English army was an experienced force; many archers were veterans of the earlier Battle of Crécy, and two of the key commanders, Sir John Chandos, and Captal de Buch were both experienced soldiers. The English army's divisions were led by Edward, the Black Prince, the Earl of Warwick, the Earl of Salisbury, Sir John Chandos and Captal de Buch.

The French army was led by John II of France, and was composed largely of native French soldiers, though there was a contingent of German knights, and a large force of Scottish soldiers. The latter force was led by the Earl of Douglas and fought in the King's own division. The French army at the battle comprised approximately 8,000 men-at-arms and 3,000 common infantry, though John had made the decision to leave behind the vast majority of his infantry, numbering up to 20,000, in order to overtake and force the English to battle.

The French army was arrayed in three "battles" or divisions; the vanguard was led by the Dauphin Charles, the second by the Duke of Orléans, while the third, the largest, was led by the King himself.

Prior to the battle, the local prelate, Cardinal Hélie de Talleyrand-Périgord attempted to broker a truce between the two sides, as recorded in the writings of the English commander, Sir John Chandos. Attending the conference on the French side was John II of France, the Count of Tankerville, the Archbishop of Sens and Jean de Talaru. Representing the English was the Earl of Warwick, the Earl of Suffolk, Bartholomew de Burghersh, James Audley and Sir John Chandos. The English offered to hand over all of the war booty they had taken on their raids throughout France, as well as a seven-year truce. John, who believed his force could easily overwhelm the English, declined their proposal. John's counter suggestion that the Black Prince and his army should surrender was flatly rejected.

At the start of the battle, the English removed their baggage train from the field, prompting a hasty French assault, believing that what they saw was the English retreating. The fighting began with a charge by a forlorn hope of 300 German knights, led by Jean de Clermont. The attack was a disaster, with many of the knights shot down or killed by English soldiery. According to Froissart, the English archers then shot their bows at the massed French infantry. The Dauphin's division reached the English line. Exhausted by a long march in heavy equipment and harassed by the hail of arrows, the division was repulsed after approximately two hours of combat.

The retreating vanguard collided with the advancing division of the Duke of Orléans, throwing the French army into chaos. Seeing the Dauphin's troops falling back, Orléans' division fell back in confusion. The third, and strongest, division led by the King advanced forth, and the two withdrawing divisions coalesced and resumed their advance against the English. Believing that the retreat of the first two French divisions marked the withdrawal of the French, Edward had ordered a force under Captal de Buch to pursue. Sir John Chandos urged the Prince to launch this force upon the main body of the French army under the King. Seizing upon this idea, Edward ordered all his men-at-arms and knights to mount for the charge, while de Buch's men, already mounted, were instructed to advance around the French left flank and rear.

As the French advanced, the English launched their charge. Stunned by the attack, the impetus carried the English and Gascon forces right into the French line. Simultaneously, de Buch's mobile reserve of mounted troops fell upon the French left flank and rear. Fearful of encirclement, the cohesion of the French army disintegrated as many soldiers attempted to flee the field. Low on arrows, the English and Welsh archers abandoned their bows and ran forward to join the melée. Around this time, King John and his son, Philip the Bold, found themselves surrounded. As written by Froissart, an exiled French knight fighting with the English, Sir Denis Morbeke of Artois approached the king, requesting the King's surrender. The King is said to have replied, "To whom shall I yield me? Where is my cousin the Prince of Wales? If I might see him, I would speak with him". Denis replied; "Sir, he is not here; but yield you to me and I shall bring you to him". The king handed him his right gauntlet, saying; "I yield me to you".

Following the surrender of the King and his son Philip, the French army had broken up and left the field, ending the battle.

Following the battle, Edward resumed his march back to the English stronghold at Bordeaux. Jean de Venette, a Carmelite friar, vividly describes the chaos that ensued following the battle. The demise of the French nobility at the battle, only ten years from the catastrophe at Crécy, threw the kingdom into chaos. The realm was left in the hands of the Dauphin Charles, who faced popular rebellion across the kingdom in the wake of the defeat. Jean writes that the French nobles brutally repressed the rebellions, robbing, despoiling and pillaging the peasants' goods. Mercenary companies hired by both sides added to the destruction, who plundered the peasants and churches.

Charles, to the misery of the French peasantry, began to raise additional funds to pay for the ransom of his father, and to continue the war effort. Capitalising on the discontent in France, King Edward assembled his army at Calais in 1359 and led his army on a campaign against Rheims. Unable to take Rheims or the French capital, Paris, Edward moved his army to Chartres. Later, the Dauphin Charles offered to open negotiations, and Edward agreed.

The Treaty of Brétigny was ratified on 24 October 1360, ending the Edwardian phase of the Hundred Years' War. In it, Edward agreed to renounce his claims to the French throne, in exchange for full sovereign rights over an expanded Aquitaine and Calais, essentially restoring the former Angevin Empire.

One of the chief commanders at both Crécy and Poitiers was John de Vere, Earl of Oxford, mentioned above.

Another account states that John of Ghistelles perished at the Battle of Crécy so there is some ambiguity as to this individual.






</doc>
<doc id="4505" url="https://en.wikipedia.org/wiki?curid=4505" title="Backbone cabal">
Backbone cabal

The backbone cabal was an informal organization of large-site administrators of the worldwide distributed newsgroup-based discussion system Usenet. It existed from about 1983 at least into the 2000s.

The cabal was created in an effort to facilitate reliable propagation of new Usenet posts: While in the 1970s and 1980s many news servers only operated during night time to save on the cost of long distance communication, servers of the backbone cabal were available 24 hours a day. The administrators of these servers gained sufficient influence in the otherwise anarchic Usenet community to be able to push through controversial changes, for instance the Great Renaming of Usenet newsgroups during 1987.

As Usenet has few technologically or legally enforced hierarchies, just about the only ones that formed were social. People acquired power through persuasion (both publicly and privately), public debate, force of will (often via aggressive flames), garnering authority and respect by spending much time and effort contributing to the community (by being a maintainer of a FAQ, for example; see also Kibo, etc.).

Credit for organizing the backbone about 1983 is commonly attributed to Gene "Spaf" Spafford, although it is also claimed by Mary Ann Horton. Other prominent members of the cabal were Brian Reid, Richard Sexton, Chuq von Rospach, Neil Crellin and Rick Adams.

During most of its existence, the cabal (sometimes capitalized) steadfastly denied its own existence; those involved would often respond "There is no Cabal" (sometimes abbreviated as "TINC"), whenever the existence or activities of the group were speculated on in public. It is sometimes used humorously to dispel cabal-like organizational conspiracy theories, or as an ironic statement, indicating one who knows the existence of "the cabal" will invariably deny there is one.

This belief became a model for various conspiracy theories about various Cabals with dark nefarious objectives beginning with taking over Usenet or the Internet. Spoofs include the "Eric Conspiracy" of moustachioed hackers named "Eric"; ex-members of the P.H.I.R.M.; and the Lumber Cartel putatively funding anti-spam efforts to support the paper industry.

The result of this policy was an aura of mystery, even a decade after the cabal mailing list disbanded in late 1988 following an internal fight.




</doc>
<doc id="4506" url="https://en.wikipedia.org/wiki?curid=4506" title="Bongo (antelope)">
Bongo (antelope)

The bongo ("Tragelaphus eurycerus") is a herbivorous, mostly nocturnal forest ungulate. It is among the largest of the African forest antelope species.

Bongos are characterised by a striking reddish-brown coat, black and white markings, white-yellow stripes and long slightly spiralled horns. Indeed, bongos are the only tragelaphid in which both sexes have horns. They have a complex social interaction and are found in African dense forest mosaics.

The western or lowland bongo, "T. e. eurycerus", faces an ongoing population decline, and the IUCN Antelope Specialist Group considers it to be Near Threatened on the conservation status scale.

The eastern or mountain bongo, "T. e. isaaci", of Kenya, has a coat even more vibrant than that of "T. e. eurycerus". The mountain bongo is only found in the wild in one remote region of central Kenya. This bongo is classified by the IUCN Antelope Specialist Group as Critically Endangered, with more specimens in captivity than in the wild.

In 2000, the Association of Zoos and Aquariums in the USA (AZA) upgraded the bongo to a Species Survival Plan participant and in 2006 added the Bongo Restoration to Mount Kenya Project to its list of the Top Ten Wildlife Conservation Success Stories of the year. However, in 2013, it seems, these successes have been negated with reports of possibly only 100 mountain bongos left in the wild due to logging and poaching.

The scientific name of the bongo is "Tragelaphus eurycerus", and it belongs to the genus "Tragelaphus" and family Bovidae. It was first described by Irish naturalist William Ogilby in 1837. The generic name "Tragelaphus" is composed of two Greek words: "tragos", meaning a male goat; and "elaphos", meaning deer. The specific name "eurycerus" originated from the fusion of "eurus" (broad, widespread) and "keras" (an animal's horn). The common name "bongo" originated probably from the Kele language of Gabon. The first known use of the name "bongo" dates back to 1861.

Bongos are further classified into two subspecies: "T. e. eurycerus", the lowland or western bongo, and the far rarer "T. e. isaaci", the mountain or eastern bongo, restricted to the mountains of Kenya only. The eastern bongo is larger and heavier than the western bongo. Two other subspecies are described from West and Central Africa, but taxonomic clarification is required. They have been observed to live up to 19 years.

Bongos are found in tropical jungles with dense undergrowth up to an altitude of in Central Africa, with isolated populations in Kenya, and these West African countries: Cameroon, the Central African Republic, the Republic of the Congo, the Democratic Republic of Congo, the Ivory Coast, Equatorial Guinea, Gabon, Ghana, Guinea, Liberia, Sierra Leone, South Sudan.

Historically, bongos are found in three disjunct parts of Africa: East, Central and West. Today, all three populations’ ranges have shrunk in size due to habitat loss for agriculture and uncontrolled timber cutting, as well as hunting for meat.

Bongos favour disturbed forest mosaics that provide fresh, low-level green vegetation. Such habitats may be promoted by heavy browsing by elephants, fires, flooding, tree-felling (natural or by logging), and fallowing. Mass bamboo die-off provides ideal habitat in East Africa. They can live in bamboo forests.

Bongos are one of the largest of the forest antelopes. In addition to the deep chestnut colour of their coats, they have bright white stripes on their sides to help with camouflage.

Adults of both sexes are similar in size. Adult height is about at the shoulder and length is , including a tail of . Females weigh around , while males weigh about . Its large size puts it as the third-largest in the Bovidae tribe of Strepsicerotini, behind both the common and greater elands by about , and above the greater kudu by about .

Both sexes have heavy spiral horns; those of the male are longer and more massive. All bongos in captivity are from the isolated Aberdare Mountains of central Kenya.

The bongo sports a bright auburn or chestnut coat, with the neck, chest, and legs generally darker than the rest of the body. Coats of male bongos become darker as they age until they reach a dark mahogany-brown colour. Coats of female bongos are usually more brightly coloured than those of males. The eastern bongo is darker in color than the western and this is especially pronounced in older males which tend to be chestnut brown, especially on the forepart of their bodies.

The pigmentation in the coat rubs off quite easily; anecdotal reports suggest rain running off a bongo may be tinted red with pigment. The smooth coat is marked with 10–15 vertical white-yellow stripes, spread along the back from the base of the neck to the rump. The number of stripes on each side is rarely the same. It also has a short, bristly, brown ridge of dorsal hair from the shoulder to the rump; the white stripes run into this ridge.

A white chevron appears between the eyes, and two large white spots grace each cheek. Another white chevron occurs where the neck meets the chest. The large ears are to sharpen hearing, and the distinctive coloration may help bongos identify one another in their dark forest habitats. Bongos have no special secretion glands, so rely less on scent to find one another than do other similar antelopes. The lips of a bongo are white, topped with a black muzzle.

Bongos have two heavy and slightly spiralled horns that slope over their backs, and like many other antelope species, both male and female bongos have horns. Bongos are the only tragelaphids in which both sexes have horns. The horns of bongos are in the form of a lyre and bear a resemblance to those of the related antelope species of nyalas, sitatungas, bushbucks, kudus and elands.

Unlike deer, which have branched antlers shed annually, bongos and other antelopes have pointed horns they keep throughout their lives. Males have massive backswept horns, while females have smaller, thinner, and more parallel horns. The size of the horns range between . The horns twist once.

Like all other horns of antelopes, the core of a bongo's horn is hollow and the outer layer of the horn is made of keratin, the same material that makes up human fingernails, toenails. and hair. The bongo runs gracefully and at full speed through even the thickest tangles of lianas, laying its heavy spiralled horns on its back so the brush cannot impede its flight. Bongos are hunted for their horns by humans.

Like other forest ungulates, bongos are seldom seen in large groups. Males, called bulls, tend to be solitary, while females with young live in groups of six to eight. Bongos have seldom been seen in herds of more than 20. Gestation is about 285 days (9.5 months), with one young per birth, and weaning occurs at six months. Sexual maturity is reached at 24–27 months. The preferred habitat of this species is so dense and difficult to operate in, that few Europeans or Americans observed this species until the 1960s.
As young males mature and leave their maternal groups, they most often remain solitary, although rarely they join an older male. Adult males of similar size/age tend to avoid one another. Occasionally, they meet and spar with their horns in a ritualised manner and it is rare for serious fights to take place. However, such fights are usually discouraged by visual displays, in which the males bulge their necks, roll their eyes, and hold their horns in a vertical position while slowly pacing back and forth in front of the other male. They seek out females only during mating time. When they are with a herd of females, males do not coerce them or try to restrict their movements as do some other antelopes.

Although mostly nocturnal, they are occasionally active during the day. However, like deer, bongos may exhibit crepuscular behaviour. Bongos are both timid and easily frightened; after a scare, a bongo moves away at considerable speed, even through dense undergrowth. Once they find cover, they stay alert and face away from the disturbance, but peek every now and then to check the situation. The bongo's hindquarters are less conspicuous than the forequarters, and from this position the animal can quickly flee.

When in distress, the bongo emits a bleat. It uses a limited number of vocalisations, mostly grunts and snorts; females have a weak mooing contact-call for their young. Females prefer to use traditional calving grounds restricted to certain areas, while newborn calves lie in hiding for a week or more, receiving short visits by the mother to suckle.

The calves grow rapidly and can soon accompany their mothers in the nursery herds. Their horns grow rapidly and begin to show in 3.5 months. They are weaned after six months and reach sexual maturity at about 20 months.

Like many forest ungulates, bongos are herbivorous browsers and feed on leaves, bushes, vines, bark and pith of rotting trees, grasses/herbs, roots, cereals, and fruits.

Bongos require salt in their diets, and are known to regularly visit natural salt licks. Examination of bongo feces revealed that charcoal from trees burnt by lightning is consumed. This behavior is believed to be a means of getting salts and minerals into their diets. This behavior has also been reported in the okapi. Another similarity to the okapi, though the bongo is unrelated, is that the bongo has a long prehensile tongue which it uses to grasp grasses and leaves.

Suitable habitats for bongos must have permanent water available. As a large animal, the bongo requires an ample amount of food, and is restricted to areas with abundant year-round growth of herbs and low shrubs.

Few estimates of population density are available. Assuming average population densities of 0.25 animals per km in regions where it is known to be common or abundant, and 0.02 per km elsewhere, and with a total area of occupancy of 327,000 km, a total population estimate of around 28,000 is suggested. Only about 60% are in protected areas, suggesting the actual numbers of the lowland subspecies may only be in the low tens of thousands. In Kenya, their numbers have declined significantly and on Mt. Kenya, they were within the last decade due to illegal hunting with dogs. Although information on their status in the wild is lacking, lowland bongos are not presently considered endangered.

Bongos are susceptible to diseases such as rinderpest, which almost exterminated the species during the 1890s. "Tragelaphus eurycerus" may suffer from goitre. Over the course of the disease, the thyroid glands greatly enlarge (up to 10 x 20 cm) and may become polycystic. Pathogenesis of goiter in the bongo may reflect a mixture of genetic predisposition coupled with environmental factors, including a period of exposure to a goitrogen. Leopards and spotted hyenas are the primary natural predators (lions are seldom encountered due to differing habitat preferences); pythons sometimes eat bongo calves. Humans prey on them for their pelts, horns, and meat, with the species being a common local source for "bush meat". Bongo populations have been greatly reduced by hunting, poaching, and animal trapping, although some bongo refuges exist.

Although bongos are quite easy for humans to catch using snares, many people native to the bongos' habitat believed that if they ate or touched bongo, they would have spasms similar to epileptic seizures. Because of this superstition, bongos were less harmed in their native ranges than expected. However, these taboos are said no longer to exist, which may account for increased hunting by humans in recent times.

An international studbook is maintained to help manage animals held in captivity. Because of its bright colour, it is very popular in zoos and private collections. In North America, over 400 individuals are thought to be held, a population that probably exceeds that of the mountain bongo in the wild.

In 2000, the Association of Zoos and Aquariums (AZA) upgraded the bongo to a Species Survival Plan participant, which works to improve the genetic diversity of managed animal populations. The target population for participating zoos and private collections in North America is 250 animals. Through the efforts of zoos in North America, a reintroduction to the population in Kenya is being developed.

At least one collaborative effort for reintroduction between North American wildlife facilities has already been carried out. In 2004, 18 eastern bongos born in North American zoos gathered at White Oak Conservation in Yulee, Florida for release in Kenya. White Oak staff members traveled with the bongos to a Mt. Kenya holding facility, where they stayed until being reintroduced.

In the last few decades, a rapid decline in the numbers of wild mountain bongo has occurred due to poaching and human pressure on their habitat, with local extinctions reported in Cherangani and Chepalungu hills, Kenya.

The Bongo Surveillance Programme, working alongside the Kenya Wildlife Service, have recorded photos of bongos at remote salt licks in the Aberdare Forests using camera traps, and, by analyzing DNA extracted from dung, have confirmed the presence of bongo in Mount Kenya, Eburru and Mau forests. The programme estimate as few as 140 animals left in the wild – spread across four isolated populations. Whilst captive breeding programmes can be viewed as having been successful in ensuring survival of this species in Europe and North America, the situation in the wild has been less promising. Evidence exists of bongo surviving in Kenya. However, these populations are believed to be small, fragmented, and vulnerable to extinction.

Animal populations with impoverished genetic diversity are inherently less able to adapt to changes in their environments (such as climate change, disease outbreaks, habitat change, etc.). The isolation of the four remaining small bongo populations, which themselves would appear to be in decline, means a substantial amount of genetic material is lost each generation. Whilst the population remains small, the impact of transfers will be greater, so the establishment of a "metapopulation management plan" occurs concurrently with conservation initiatives to enhance "in situ" population growth, and this initiative is both urgent and fundamental to the future survival of mountain bongo in the wild.

The western/lowland bongo faces an ongoing population decline as habitat destruction and hunting pressures increase with the relentless expansion of human settlement. Its long-term survival will only be assured in areas which receive active protection and management. At present, such areas comprise about 30,000 km, and several are in countries where political stability is fragile. So, a realistic possibility exists whereby its status could decline to Threatened in the near future.

As the largest and most spectacular forest antelope, the western/lowland bongo is both an important flagship species for protected areas such as national parks, and a major trophy species which has been taken in increasing numbers in Central Africa by sport hunters during the 1990s. Both of these factors are strong incentives to provide effective protection and management of populations.

Trophy hunting has the potential to provide economic justification for the preservation of larger areas of bongo habitat than national parks, especially in remote regions of Central Africa, where possibilities for commercially successful tourism are very limited.

The eastern/mountain bongo’s survival in the wild is dependent on more effective protection of the surviving remnant populations in Kenya. If this does not occur, it will eventually become extinct in the wild. The existence of a healthy captive population of this subspecies offers the potential for its reintroduction.

In 2004, Dr. Jake Veasey, the head of the Department of Animal Management and Conservation at Woburn Safari Park and a member of the European Association of Zoos and Aquariums Population Management Advisory Group, with the assistance of Lindsay Banks, took over responsibility for the management and coordination of the European Endangered Species Programme for the eastern bongo. This includes some 250 animals across Europe and the Middle East.

Along with the Rothschild giraffe, the eastern bongo is arguably one of the most threatened large mammals in Africa, with recent estimates numbering less than 140 animals, below a minimum sustainable viable population. The situation is exacerbated because these animals are spread across four isolated populations. Whilst the bongo endangered species program can be viewed as having been successful in ensuring survival of this species in Europe, it has not yet become actively involved in the conservation of this species in the wild in a coordinated fashion. The plan is to engage in conservation activities in Kenya to assist in reversing the decline of the eastern bongo populations and genetic diversity in Africa, and in particular, applying population management expertise to help ensure the persistence of genetic diversity in the free ranging wild populations.
To illustrate significance of genetic diversity loss, assume the average metapopulation size is 35 animals based on 140 animals spread across four populations (140/4=35). Assuming stable populations, these populations will lose 8% of their genetic diversity every decade. By managing all four populations as one, through strategic transfers, gene loss is reduced from 8% to 2% per decade, without any increase in bongo numbers in Kenya. By managing the European and African populations as one – by strategic exports from Europe combined with "in situ" transfers, gene loss is reduced to 0.72% every 100 years, with both populations remaining stable. If populations in Kenya are allowed to grow through the implementation of effective conservation, including strategic transfers, gene loss can be effectively halted in this species and its future secured in the wild.

The initial aims of the project are: 

If effective protection were implemented immediately and bongo populations allowed to expand without transfers, then this would create a bigger population of genetically impoverished bongos. These animals would be less able to adapt to a dynamic environment. Whilst the population remains small, the impact of transfers will be greater. For this reason, the 'metapopulation management plan' must occur concurrently with conservation strategies to enhance "in situ" population growth. This initiative is both urgent and fundamental to the future survival of the mountain bongo in the wild.
In 2013, SafariCom telecommunications donated money to the Bongo Surveillance Programme to try to keep tabs on what are thought to be the last 100 eastern bongos left in the wild in the Mau Eburu Forest in central Kenya, whose numbers are still declining due to logging of their habitat and illegal poaching.

Mount Kenya Wildlife Conservancy runs a bongo rehabilitation program in collaboration with the Kenya Wildlife Service. The Conservancy aims to prevent extinction of the bongo through breeding and release back into the wild.

In 2002 the IUCN listed the western/lowland species as Near Threatened. These bongos may be endangered due to human environmental interaction, as well as hunting and illegal actions towards wildlife. CITES lists bongos as an Appendix III species, only regulating their exportation from a single country, Ghana. It is not protected by the US Endangered Species Act and is not listed by the USFWS.

The IUCN Antelope Specialist Group considers the western or lowland bongo, "T. e. eurycerus", to be Lower Risk (Near Threatened), and the eastern or mountain bongo, "T. e. isaaci", of Kenya, to be Critically Endangered. Other subspecific names have been used, but their validity has not been tested.



</doc>
<doc id="4507" url="https://en.wikipedia.org/wiki?curid=4507" title="Bunyip">
Bunyip

The bunyip is a large mythical creature from Australian Aboriginal mythology, said to lurk in swamps, billabongs, creeks, riverbeds, and waterholes.

The origin of the word "bunyip" has been traced to the Wemba-Wemba or Wergaia language of Aboriginal people of South-Eastern Australia. But the figure of the bunyip was part of traditional Aboriginal beliefs and stories throughout Australia, while its name varied according to tribal nomenclature. In his 2001 book, writer Robert Holden identified at least nine regional variations of the creature known as the bunyip across Aboriginal Australia. Europeans recorded various written accounts of bunyips in the early and mid-19th century, as they began to settle across the country.

The word "bunyip" is usually translated by Aboriginal Australians today as "devil" or "evil spirit". This contemporary translation may not accurately represent the role of the bunyip in pre-contact Aboriginal mythology or its possible origins before written accounts were made. Some modern sources allude to a linguistic connection between the bunyip and Bunjil, "a mythic 'Great Man' who made the mountains and rivers and man and all the animals." The word "bunyip" may have first appeared in print in English about the mid-1840s.

By the 1850s, "bunyip" was also used as a a "synonym for impostor, pretender, humbug and the like" in the broader Australian community. The term "bunyip aristocracy" was first coined in 1853 to describe Australians aspiring to be aristocrats. In the early 1990s, Prime Minister Paul Keating used this term to describe members of the conservative Liberal Party of Australia opposition.

The word "bunyip" can still be found in a number of Australian contexts, including place names such as the Bunyip River (which flows into Westernport Bay in southern Victoria) and the town of Bunyip, Victoria.

Descriptions of bunyips vary widely. George French Angus may have collected a description of a bunyip in his account of a "water spirit" from the Moorundi people of the Murray River before 1847, stating it is "much dreaded by them ... It inhabits the Murray; but ... they have some difficulty describing it. Its most usual form ... is said to be that of an enormous starfish." Robert Brough Smyth's "Aborigines of Victoria" (1878) devoted ten pages to the bunyip, but concluded "in truth little is known among the blacks respecting its form, covering or habits; they appear to have been in such dread of it as to have been unable to take note of its characteristics." Common features as reported in many 19th-century newspaper accounts include a dog-like face, a crocodile-like head, dark fur, a horse-like tail, flippers, and walrus-like tusks or horns, or a duck-like bill.

The Challicum bunyip, an outline image of a bunyip carved by Aborigines into the bank of Fiery Creek, near Ararat, Victoria, was first recorded by "The Australasian" newspaper in 1851. According to the report, the bunyip had been speared after killing an Aboriginal man. Antiquarian Reynell Johns claimed that until the mid-1850s, Aboriginal people made a "habit of visiting the place annually and retracing the outlines of the figure [of the bunyip] which is about 11 paces long and 4 paces in extreme breadth." The outline image no longer exists.

Non-Aboriginal Australians have made various attempts to understand and explain the origins of the bunyip as a physical entity over the past 150 years. Writing in 1933, Charles Fenner suggested that it was likely that the "actual origin of the bunyip myth lies in the fact that from time to time seals have made their way up the Murray and Darling (Rivers)". He provided examples of seals found as far inland as Overland Corner, Loxton, and Conargo and reminded readers that "the smooth fur, prominent 'apricot' eyes, and the bellowing cry are characteristic of the seal", especially southern elephant seals and leopard seals.

Another suggestion is that the bunyip may be a cultural memory of extinct Australian marsupials such as the "Diprotodon", "Zygomaturus", "Nototherium", or "Palorchestes". This connection was first formally made by Dr George Bennett of the Australian Museum in 1871. In the early 1990s, palaeontologist Pat Vickers-Rich and geologist Neil Archbold also cautiously suggested that Aboriginal legends "perhaps had stemmed from an acquaintance with prehistoric bones or even living prehistoric animals themselves ... When confronted with the remains of some of the now extinct Australian marsupials, Aborigines would often identify them as the bunyip." They also note that "legends about the" mihirung paringmal" of western Victorian Aborigines ... may allude to the ... extinct giant birds the Dromornithidae."

In a 2017 "Australian Birdlife" article, Karl Brandt suggested Aboriginal encounters with the southern cassowary inspired the myth. According to the first written description of the bunyip from 1845, the creature, which laid pale blue eggs of immense size, possessed deadly claws, powerful hind legs, a brightly coloured chest, and an emu-like head, characteristics shared with then undiscovered Australian cassowary. As the creature’s bill was described as having serrated projections, each "like the bone of the stingray", this bunyip was associated with the indigenous people of Far North Queensland, renowned for their spears tipped with stingray barbs and their proximity to the cassowary’s Australian range.

Another association to the bunyip is the shy Australasian bittern ("Botaurus poiciloptilus"). During the breeding season, the male call of this marsh-dwelling bird is a "low pitched boom"; hence, it is occasionally called the "bunyip bird".

During the early settlement of Australia by Europeans, the notion became commonly held that the bunyip was an unknown animal that awaited discovery. Unfamiliar with the sights and sounds of the island continent's peculiar fauna, early Europeans believed that the bunyip described to them was one more strange Australian animal and they sometimes attributed unfamiliar animal calls or cries to it. Scholars suggest also that 19th-century bunyip lore was reinforced by imported European folklore, such as that of the Irish Púca.

A large number of bunyip sightings occurred during the 1840s and 1850s, particularly in the southeastern colonies of Victoria, New South Wales and South Australia, as European settlers extended their reach. The following is not an exhaustive list of accounts:

One of the earliest accounts relating to a large unknown freshwater animal was in 1818, when Hamilton Hume and James Meehan found some large bones at Lake Bathurst in New South Wales. They did not call the animal a bunyip, but described the remains indicating the creature as very much like a hippopotamus or manatee. The Philosophical Society of Australasia later offered to reimburse Hume for any costs incurred in recovering a specimen of the unknown animal, but for various reasons, Hume did not return to the lake. Ancient "Diprotodon" skeletons have sometimes been compared to the hippopotamus; they are a land animal, but have sometimes been found in a lake or water course.

More significant was the discovery of fossilised bones of "some quadruped much larger than the ox or buffalo" in the Wellington Caves in mid-1830 by bushman George Rankin and later by Thomas Mitchell. Sydney's Reverend John Dunmore Lang announced the find as "convincing proof of the deluge", referring to Biblical accounts of the Flood. But British anatomist Sir Richard Owen identified the fossils as the gigantic marsupials "Nototherium" and "Diprotodon". At the same time, some settlers observed that "all natives throughout these ... districts have a tradition (of) a very large animal having at one time existed in the large creeks and rivers and by many it is said that such animals now exist."

In July 1845, "The Geelong Advertiser" announced the discovery of fossils found near Geelong, under the headline "Wonderful Discovery of a new Animal". This was a continuation of a story on 'fossil remains' from the previous issue. The newspaper continued, "On the bone being shown to an intelligent black, he at once recognised it as belonging to the bunyip, which he declared he had seen. On being requested to make a drawing of it, he did so without hesitation." The account noted a story of an Aboriginal woman being killed by a bunyip and the "most direct evidence of all" – that of a man named Mumbowran "who showed several deep wounds on his breast made by the claws of the animal". 

The account provided this description of the creature:
Shortly after this account appeared, it was repeated in other Australian newspapers. This appears to be the first use of the word "bunyip" in a written publication.

In January 1846, a peculiar skull was taken by a settler from the banks of Murrumbidgee River near Balranald, New South Wales. Initial reports suggested that it was the skull of something unknown to science. The squatter who found it remarked, "all the natives to whom it was shown called [it] a bunyip". By July 1847, several experts, including W. S. Macleay and Professor Owen, had identified the skull as the deformed foetal skull of a foal or calf. At the same time, the purported bunyip skull was put on display in the Australian Museum (Sydney) for two days. Visitors flocked to see it, and "The Sydney Morning Herald" reported that many people spoke out about their "bunyip sightings". Reports of this discovery used the phrase 'Kine Pratie' as well as Bunyip. Explorer William Hovell, who examined the skull, also called it a 'katen-pai'.

In March of that year "a bunyip or an immense Platibus" (Platypus) was sighted "sunning himself on the placid bosom of the Yarra, just opposite the Custom House" in Melbourne. "Immediately a crowd gathered" and three men set off by boat "to secure the stranger" which "disappeared" when they were "about a yard from him".

Another early written account is attributed to escaped convict William Buckley in his 1852 biography of thirty years living with the Wathaurong people. His 1852 account records "in ... Lake Moodewarri [now Lake Modewarre] as well as in most of the others inland ... is a ... very extraordinary amphibious animal, which the natives call Bunyip." Buckley's account suggests he saw such a creature on several occasions. He adds, "I could never see any part, except the back, which appeared to be covered with feathers of a dusky grey colour. It seemed to be about the size of a full grown calf ... I could never learn from any of the natives that they had seen either the head or tail." Buckley also claimed the creature was common in the Barwon River and cites an example he heard of an Aboriginal woman being killed by one. He emphasized the bunyip was believed to have supernatural powers.


Numerous tales of the bunyip in written literature appeared in the 19th and early 20th centuries. One of the earliest known is a story in Andrew Lang's "The Brown Fairy Book" (1904).



The Australian tourism boom of the 1970s brought a renewed interest in bunyip mythology.

Bunyip stories have also been published outside Australia.


In the 21st century the bunyip has been featured in works around the world. 



[[Category:Australian Aboriginal mythology]]
[[Category:Australian folklore]]
[[Category:Australian legendary creatures]]
[[Category:Water spirits]]
[[Category:Australian Aboriginal words and phrases]]

[[vi:Quỷ]]

</doc>
<doc id="4508" url="https://en.wikipedia.org/wiki?curid=4508" title="Brabant">
Brabant

Brabant is a region in the Low Countries. It may refer to:

In the Netherlands:

In Belgium:

In France:




</doc>
<doc id="4512" url="https://en.wikipedia.org/wiki?curid=4512" title="Boone, North Carolina">
Boone, North Carolina

Boone is a town located in the Blue Ridge Mountains of western North Carolina, United States. Boone's population was 17,122 in 2010. Boone is the county seat of Watauga County and the home of Appalachian State University.
The town is named for famous American pioneer and explorer Daniel Boone, and every summer since 1952 has hosted an outdoor amphitheatre drama, Horn in the West, portraying the British settlement of the area during the American Revolutionary War and featuring the contributions of its namesake. It is the largest community and the economic hub of the seven-county region of Western North Carolina known as the High Country.

In 2012, Boone was listed among the 10 best places to retire in the U.S. by "U.S. News".

 Boone took its name from the famous pioneer and explorer Daniel Boone, who on several occasions camped at a site generally agreed to be within the present city limits. Daniel's nephews, Jesse and Jonathan (sons of brother Israel Boone), were members of the town's first church, Three Forks Baptist, still in existence today.
Boone was served by the narrow gauge East Tennessee and Western North Carolina Railroad (nicknamed "Tweetsie") until the flood of 1940. The flood washed away much of the tracks and it was decided not to replace them.

Boone is the home of Appalachian State University, a constituent member of the University of North Carolina.
Appalachian State is the sixth largest university in the seventeen-campus system.
Caldwell Community College & Technical Institute also operates a satellite campus in Boone.

""Horn in the West"" is a dramatization of the life and times of the early settlers of the mountain area. It features Daniel Boone as one of its characters, and has been performed in an outdoor amphitheater near the town every summer since 1952.
The original actor in the role of "Daniel Boone" was Ned Austin. His "Hollywood Star" stands on a pedestal on King Street in downtown Boone. He was followed in the role by Glenn Causey, who portrayed the rugged frontiersman for 41 years, and whose image is still seen in many of the depictions of Boone featured in the area today.

Boone is a center for bluegrass musicians and Appalachian storytellers. Notable artists associated with Boone include the late, Grammy Award-winning bluegrass guitar player Doc Watson and the late guitarist Michael Houser, founding member of and lead guitarist for the band Widespread Panic, both Boone natives, as well as Old Crow Medicine Show, The Blue Rags, and Eric Church.

The Blair Farm, Daniel Boone Hotel, Jones House, John Smith Miller House, and US Post Office-Boone are listed on the National Register of Historic Places.

Boone is located at (36.211364, −81.668657) and has an elevation of 3,333 feet (1015.9 m) above sea level. An earlier survey gave the elevation as 3,332 ft and since then it has been published as having an elevation of 3,333 ft (1,016 m). Boone has the highest elevation of any town of its size (over 10,000 population) east of the Mississippi River. As such, Boone features, depending on the isotherm used, a humid continental climate (Köppen "Dfb"), a rarity for the Southeastern United States, bordering on a subtropical highland climate ("Cfb") and straddles the boundary between USDA Plant Hardiness Zones 6B and 7A; the elevation also results in enhanced precipitation, with of average annual precipitation. Compared to the lower elevations of the Carolinas, winters are long and cold, with frequent sleet and snowfall. The daily average temperature in January is , which gives Boone a winter climate more similar to coastal southern New England rather than the Southeast, where a humid subtropical climate predominates. Blizzard-like conditions are not unusual during many winters. Summers are warm, but far cooler and less humid than lower regions to the south and east, with a July daily average temperature of . Boone typically receives on average nearly of snowfall annually, far higher than the lowland areas in the rest of North Carolina. 
On January 18, 1966, the temperature fell to −18°F.

As of the census of 2000, there were 13,472 people, 4,374 households, and 1,237 families residing in the town. The population density was 2,307.0 people per square mile (890.7/km²). There were 4,748 housing units at an average density of 813.0 per square mile (313.9/km²). The racial makeup of the town was 93.98% White, 3.42% Black or African American, 0.30% Native American, 1.19% Asian, 0.05% Pacific Islander, 0.46% from other races, and 0.60% from two or more races. 1.64% of the population were Hispanic or Latino of any race.

There were 4,374 households out of which 9.6% had children under the age of 18 living with them, 21.0% were married couples living together, 5.6% had a female householder with no husband present, and 71.7% were non-families. 38.4% of all households were made up of individuals and 7.3% had someone living alone who was 65 years of age or older. The average household size was 1.97 and the average family size was 2.63.

The age distribution is 5.8% under 18, 65.9% from 18 to 24, 12.1% from 25 to 44, 9.1% from 45 to 64, and 7.1% who were 65 or older. The median age was 21 years. Both the overall age distribution and the median age are driven by the presence of the local university, Appalachian State. For every 100 females, there are 95.6 males. For every 100 females age 18 and over, there were 94.7 males.

The median household income is $20,541, and the median family income is $49,762. The per capita income is $12,256. 37.0% of the population and 9.2% of families were below the poverty line.

Men had a median income of $28,060 versus $20,000 for women. However, poverty statistics that are based on surveys of the entire population can be extremely misleading in communities dominated by students, such as Boone. Out of the total population, 6.3% of those under the age of 18 and 9.1% of those 65 and older were living below the poverty line.

Boone is mainly served by three local newspapers:


A smaller newspaper, "The Appalachian", is Appalachian State University's campus newspaper; it is published twice a week on Tuesdays and Thursdays. In addition to the locally printed papers, a monthly entertainment pamphlet named "Kraut Creek Revival" has limited circulation and is funded by a Denver, NC-based newspaper.


Boone operates under a mayor-council government. The city council consists of five members. The mayor presides over the council and casts a vote on issues only in the event of a tie. As of December 2015, the Town Council members are: Rennie Brantz, Mayor, and Councilors: Lynne Mason (Mayor Pro-Tem), Jennifer Teague, Loretta Clawson, Charlotte Mizelle and Jeannine Underdown Collins.

Industrial, commercial, and residential development in the town of Boone is a controversial issue due to its location in the mountains of Appalachia. On October 16, 2009, the town council accepted the "Boone 2030 Land Use Plan." While the document itself is not in any way actual law, it is used by the town council, board of adjustment, and other committees to guide decision making as to what types of development are appropriate.

In 2009, the North Carolina Department of Transportation began widening 1.1 miles of U.S. 421 (King Street) to a 4-to-6-lane divided highway with a raised concrete median from U.S. 321 (Hardin Street) to east of N.C. 194 (Jefferson Road), including a new entrance and exit to the new Watauga High School, at a cost of $16.2 million. The widening has displaced 25 businesses and 63 residences east of historic downtown King Street. The project was slated to be completed by December 31, 2011, but construction continued into the spring of 2012.





</doc>
<doc id="4513" url="https://en.wikipedia.org/wiki?curid=4513" title="Banshee">
Banshee

A banshee ( ; Modern Irish "bean sí", "baintsí", from , , "woman of the fairy mound" or "fairy woman") is a female spirit in Irish mythology who heralds the death of a family member, usually by wailing, shrieking, or keening. Her name is connected to the mythologically important tumuli or "mounds" that dot the Irish countryside, which are known as síde (singular "síd") in Old Irish.

There are many varying descriptions of the banshee. Sometimes she has long streaming hair and wears a grey cloak over a green dress, and her eyes are red from continual weeping. She may be dressed in white with red hair and a ghastly complexion, according to a firsthand account by Ann, Lady Fanshawe in her "Memoirs". Lady Wilde in "Ancient Legends of Ireland" provides another:
Sometimes the banshee assumes the form of some sweet singing virgin of the family who died young, and has been given the mission by the invisible powers to become the harbinger of coming doom to her mortal kindred. Or she may be seen at night as a shrouded woman, crouched beneath the trees, lamenting with veiled face, or flying past in the moonlight, crying bitterly. And the cry of this spirit is mournful beyond all other sounds on earth, and betokens certain death to some member of the family whenever it is heard in the silence of the night.
In Ireland and parts of Scotland, a traditional part of mourning is the keening woman ("bean chaointe"), who wails a lament - in , (Munster dialect), (Connaught dialect) or (Ulster dialect), "caoin" meaning "to weep, to wail". This keening woman may in some cases be a professional, and the best keeners would be in high demand.

Irish legend speaks of a lament being sung by a fairy woman; she would sing it when a family member died or was about to die, even if the person had died far away and news of their death had not yet come, so that the wailing of the banshee was the first warning the household had of the death.

She also predicts death. If someone is about to enter a situation where it is unlikely they will come out of alive she will warn people by screaming or wailing, giving rise to a banshee also being known as a wailing woman.

It is often stated that the Banshee laments only the descendants of the pure Milesian stock of Ireland, sometimes clarified as surnames prefixed with O' and Mac, and some accounts even state that each family has its own Banshee. One account, however, also included the Geraldines, as they had apparently become "more Irish than the Irish themselves".

When several banshees appear at once, it indicates the death of someone great or holy. The tales sometimes recounted that the woman, though called a fairy, was a ghost, often of a specific murdered woman, or a mother who died in childbirth.

The Ua Briain banshee is thought to be named Aibell and the ruler of 25 other banshees who would always be at her attendance. It is possible that this particular story is the source of the idea that the wailing of numerous banshees signifies the death of a great person.

Most, though not all, surnames associated with banshees have the "Ó" or "Mc/Mac" prefix - that is, surnames of Goidelic origin, indicating a family native to the Insular Celtic lands rather than those of the Norse, English, or Norman invaders. Accounts reach as far back as 1380 to the publication of the "Cathreim Thoirdhealbhaigh" ("Triumphs of Torlough") by Sean mac Craith. Mentions of banshees can also be found in Norman literature of that time.

In some parts of Leinster, she is referred to as the "bean chaointe" (keening woman) whose wail can be so piercing that it shatters glass. In Scottish folklore, a similar creature is known as the bean nighe or "ban nigheachain" (little washerwoman) or "nigheag na h-àth" (little washer at the ford) and is seen washing the bloodstained clothes or armour of those who are about to die. In Welsh folklore, a similar creature is known as the hag of the mist. 



</doc>
<doc id="4514" url="https://en.wikipedia.org/wiki?curid=4514" title="Genetically modified maize">
Genetically modified maize

Genetically modified maize (corn) is a genetically modified crop. Specific maize strains have been genetically engineered to express agriculturally-desirable traits, including resistance to pests and to herbicides. Maize strains with both traits are now in use in multiple countries. GM maize has also caused controversy with respect to possible health effects, impact on other insects and impact on other plants via gene flow. One strain, called Starlink, was approved only for animal feed in the US, but was found in food, leading to a series of recalls starting in 2000.

Corn varieties resistant to glyphosate herbicides were first commercialized in 1996 by Monsanto, and are known as "Roundup Ready Corn". They tolerate the use of Roundup. Bayer CropScience developed "Liberty Link Corn" that is resistant to glufosinate. Pioneer Hi-Bred has developed and markets corn hybrids with tolerance to imidazoline herbicides under the trademark "Clearfield" – though in these hybrids, the herbicide-tolerance trait was bred using tissue culture selection and the chemical mutagen ethyl methanesulfonate, not genetic engineering. Consequently, the regulatory framework governing the approval of transgenic crops does not apply for Clearfield.

As of 2011, herbicide-resistant GM corn was grown in 14 countries. By 2012, 26 varieties herbicide-resistant GM maize were authorised for import into the European Union., but such imports remain controversial. Cultivation of herbicide-resistant corn in the EU provides substantial farm-level benefits.

Bt corn is a variant of maize that has been genetically altered to express one or more proteins from the bacterium "Bacillus thuringiensis" including Delta endotoxins. The protein is poisonous to certain insect pests. Spores of the bacillus are widely used in organic gardening, although GM corn is not considered organic. The European corn borer causes about a billion dollars in damage to corn crops each year.

In recent years, traits have been added to ward off corn ear worms and root worms, the latter of which annually causes about a billion dollars in damages.

The Bt protein is expressed throughout the plant. When a vulnerable insect eats the Bt-containing plant, the protein is activated in its gut, which is alkaline. In the alkaline environment the protein partially unfolds and is cut by other proteins, forming a toxin that paralyzes the insect's digestive system and forms holes in the gut wall. The insect stops eating within a few hours and eventually starves.

In 1996, the first GM maize producing a Bt Cry protein was approved, which killed the European corn borer and related species; subsequent Bt genes were introduced that killed corn rootworm larvae.

Approved Bt genes include single and stacked (event names bracketed) configurations of: Cry1A.105 (MON89034), CryIAb (MON810), CryIF (1507), Cry2Ab (MON89034), Cry3Bb1 (MON863 and MON88017), Cry34Ab1 (59122), Cry35Ab1 (59122), mCry3A (MIR604), and Vip3A (MIR162), in both corn and cotton. Corn genetically modified to produce VIP was first approved in the US in 2010.

In 2018 a study found that Bt-corn protected nearby fields of non-Bt corn and nearby vegetable crops, reducing the use of pesticides on those crops. Data from 1976-1996 (before Bt corn was widespread) was compared to data after it was adopted (1996-2016). They examined levels of the European corn borer and corn earworm. Their larvae eat a variety of crops, including peppers and green beans. Between 1992 and 2016, the amount of insecticide applied to New Jersey pepper fields decreased by 85 percent. Another factor was the introduction of more effective pesticides that were applied less often.

GM sweet corn varieties include "Attribute", the brand name for insect-resistant sweet corn developed by Syngenta and Performance Series™ insect-resistant sweet corn developed by Monsanto.

In 2013 Monsanto launched the first transgenic drought tolerance trait in a line of corn hybrids called DroughtGard. The MON 87460 trait is provided by the insertion of the cspB gene from the soil microbe "Bacillus subtilis"; it was approved by the USDA in 2011 and by China in 2013.

Research has been done on adding a single E. coli gene to maize to enable it to be grown with an essential amino acid (methionine).

In 2007, South African researchers announced the production of transgenic maize resistant to maize streak virus (MSV), although it has not been released as a product.

While breeding cultivars for resistance to MSV isn't done in the public, the private sector, international research centers, and national programmes have done all of the breeding.

As of 2014, there have been a few MSV-tolerant cultivars released in Africa. A private company Seedco has released 5 MSV cultivars.

US Environmental Protection Agency (EPA) regulations require farmers who plant Bt corn to plant non-Bt corn nearby (called a refuge) to provide a location to harbor vulnerable pests. Typically, 20% of corn in a grower's fields must be refuge; refuge must be at least 0.5 miles from Bt corn for lepidopteran pests, and refuge for corn rootworm must at least be adjacent to a Bt field.

The theory behind these refuges is to slow the evolution of resistance to the pesticide. EPA regulations also require seed companies to train farmers how to maintain refuges, to collect data on the refuges and to report that data to the EPA. A study of these reports found that from 2003 to 2005 farmer compliance with keeping refuges was above 90%, but that by 2008 approximately 25% of Bt corn farmers did not keep refuges properly, raising concerns that resistance would develop.

Unmodified crops received most of the economic benefits of Bt corn in the US in 1996-2007, because of the overall reduction of pest populations. This reduction came because females laid eggs on modified and unmodified strains alike.

Seed bags containing both Bt and refuge seed have been approved by the EPA in the United States. These seed mixtures were marketed as "Refuge in a Bag" (RIB) to increase farmer compliance with refuge requirements and reduce additional work needed at planting from having separate Bt and refuge seed bags on hand. The EPA approved a lower percentage of refuge seed in these seed mixtures ranging from 5 to 10%. This strategy is likely to reduce the likelihood of Bt-resistance occurring for corn rootworm, but may increase the risk of resistance for lepidopteran pests, such as European corn borer. Increased concerns for resistance with seed mixtures include partially resistant larvae on a Bt plant being able to move to a susceptible plant to survive or cross pollination of refuge pollen on to Bt plants that can lower the amount of Bt expressed in kernels for ear feeding insects.

Resistant strains of the European corn borer have developed in areas with defective or absent refuge management.

In November 2009, Monsanto scientists found the pink bollworm had become resistant to first-generation Bt cotton in parts of Gujarat, India – that generation expresses one Bt gene, "Cry1Ac". This was the first instance of Bt resistance confirmed by Monsanto anywhere in the world. Bollworm resistance to first generation Bt cotton has been identified in Australia, China, Spain, and the United States. In 2012, a Florida field trial demonstrated that army worms were resistant to pesticide-containing GM corn produced by Dupont-Dow; armyworm resistance was first discovered in Puerto Rico in 2006, prompting Dow and DuPont to voluntarily stop selling the product on the island.

Regulation of GM crops varies between countries, with some of the most-marked differences occurring between the USA and Europe. Regulation varies in a given country depending on intended uses.

There is a scientific consensus that currently available food derived from GM crops poses no greater risk to human health than conventional food, but that each GM food needs to be tested on a case-by-case basis before introduction. Nonetheless, members of the public are much less likely than scientists to perceive GM foods as safe. The legal and regulatory status of GM foods varies by country, with some nations banning or restricting them, and others permitting them with widely differing degrees of regulation.

The scientific rigor of the studies regarding human health has been disputed due to alleged lack of independence and due to conflicts of interest involving governing bodies and some of those who perform and evaluate the studies.

GM crops provide a number of ecological benefits, but there are also concerns for their overuse, stalled research outside of the Bt seed industry, proper management and issues with Bt resistance arising from their misuse.

Critics have objected to GM crops on ecological, economic and health grounds. The economic issues derive from those organisms that are subject to intellectual property law, mostly patents. The first generation of GM crops lose patent protection beginning in 2015. Monsanto has claimed it will not pursue farmers who retain seeds of off-patent varieties. These controversies have led to litigation, international trade disputes, protests and to restrictive legislation in most countries.

Critics claim that Bt proteins could target predatory and other beneficial or harmless insects as well as the targeted pest. These proteins have been used as organic sprays for insect control in France since 1938 and the USA since 1958 with no ill effects on the environment reported. While "cyt" proteins are toxic towards the insect order Diptera (flies), certain "cry" proteins selectively target lepidopterans (moths and butterflies), while other "cyt" selectively target Coleoptera. As a toxic mechanism, "cry" proteins bind to specific receptors on the membranes of mid-gut (epithelial) cells, resulting in rupture of those cells. Any organism that lacks the appropriate gut receptors cannot be affected by the "cry" protein, and therefore Bt. Regulatory agencies assess the potential for the transgenic plant to impact nontarget organisms before approving commercial release.

A 1999 study found that in a lab environment, pollen from Bt maize dusted onto milkweed could harm the monarch butterfly. Several groups later studied the phenomenon in both the field and the laboratory, resulting in a risk assessment that concluded that any risk posed by the corn to butterfly populations under real-world conditions was negligible. A 2002 review of the scientific literature concluded that "the commercial large-scale cultivation of current Bt–maize hybrids did not pose a significant risk to the monarch population". A 2007 review found that "nontarget invertebrates are generally more abundant in Bt cotton and Bt maize fields than in nontransgenic fields managed with insecticides. However, in comparison with insecticide-free control fields, certain nontarget taxa are less abundant in Bt fields."

Gene flow is the transfer of genes and/or alleles from one species to another. Concerns focus on the interaction between GM and other maize varieties in Mexico, and of gene flow into refuges.

In 2009 the government of Mexico created a regulatory pathway for genetically modified maize, but because Mexico is the center of diversity for maize, gene flow could affect a large fraction of the world's maize strains. A 2001 report in "Nature" presented evidence that Bt maize was cross-breeding with unmodified maize in Mexico. The data in this paper was later described as originating from an artifact. "Nature" later stated, "the evidence available is not sufficient to justify the publication of the original paper". A 2005 large-scale study failed to find any evidence of contamination in Oaxaca. However, other authors also found evidence of cross-breeding between natural maize and transgenic maize.

A 2004 study found Bt protein in kernels of refuge corn.

In 2017, a large-scale study found "pervasive presence of transgenes and glyphosate in maize-derived food in Mexico"

The French High Council of Biotechnologies Scientific Committee reviewed the 2009 Vendômois "et al." study and concluded that it "..presents no admissible scientific element likely to ascribe any haematological, hepatic or renal toxicity to the three re-analysed GMOs." However, the French government applies the precautionary principle with respect to GMOs.

A review by Food Standards Australia New Zealand and others of the same study concluded that the results were due to chance alone.

A 2011 Canadian study looked at the presence of CryAb1 protein (BT toxin) in non-pregnant women, pregnant women and fetal blood. All groups had detectable levels of the protein, including 93% of pregnant women and 80% of fetuses at concentrations of 0.19 ± 0.30 and 0.04 ± 0.04 mean ± SD ng/ml, respectively. The paper did not discuss safety implications or find any health problems. The paper was found to be unconvincing by multiple authors and organizations. In a swine model, Cry1Ab-specific antibodies were not detected in pregnant sows or their offspring and no negative effects from feeding Bt maize to pregnant sows were observed.

In January 2013, the European Food Safety Authority released all data submitted by Monsanto in relation to the 2003 authorisation of maize genetically modified for glyphosate tolerance.

StarLink contains Cry9C, which had not previously been used in a GM crop. Starlink's creator, Plant Genetic Systems had applied to the US Environmental Protection Agency (EPA) to market Starlink for use in animal feed and in human food. However, because the Cry9C protein lasts longer in the digestive system than other Bt proteins, the EPA had concerns about its allergenicity, and PGS did not provide sufficient data to prove that Cry9C was not allergenic. As a result, PGS split its application into separate permits for use in food and use in animal feed. Starlink was approved by the EPA for use in animal feed only in May 1998.

StarLink corn was subsequently found in food destined for consumption by humans in the US, Japan, and South Korea. This corn became the subject of the widely publicized Starlink corn recall, which started when Taco Bell-branded taco shells sold in supermarkets were found to contain the corn. Sales of StarLink seed were discontinued. The registration for Starlink varieties was voluntarily withdrawn by Aventis in October 2000. (Pioneer had been bought by AgrEvo which then became Aventis CropScience at the time of the incident, which was later bought by Bayer

Fifty-one people reported adverse effects to the FDA; US Centers for Disease Control (CDC), which determined that 28 of them were possibly related to Starlink. However, the CDC studied the blood of these 28 individuals and concluded there was no evidence of hypersensitivity to the Starlink Bt protein.

A subsequent review of these tests by the Federal Insecticide, Fungicide, and Rodenticide Act Scientific Advisory Panel points out that while "the negative results decrease the probability that the Cry9C protein is the cause of allergic symptoms in the individuals examined ... in the absence of a positive control and questions regarding the sensitivity and specificity of the assay, it is not possible to assign a negative predictive value to this."

The US corn supply has been monitored for the presence of the Starlink Bt proteins since 2001.

In 2005, aid sent by the UN and the US to Central American nations also contained some StarLink corn. The nations involved, Nicaragua, Honduras, El Salvador and Guatemala refused to accept the aid.

On December 19, 2013 six Chinese citizens were indicted in Iowa on charges of plotting to steal genetically modified seeds worth tens of millions of dollars from Monsanto and DuPont. Mo Hailong, director of international business at the Beijing Dabeinong Technology Group Co., part of the Beijing-based DBN Group, was accused of stealing trade secrets after he was found digging in an Iowa cornfield.




</doc>
<doc id="4516" url="https://en.wikipedia.org/wiki?curid=4516" title="Body substance isolation">
Body substance isolation

Body substance isolation is a practice of isolating all body substances (blood, urine, feces, tears, etc.) of individuals undergoing medical treatment, particularly emergency medical treatment of those who might be infected with illnesses such as HIV, or hepatitis so as to reduce as much as possible the chances of transmitting these illnesses. BSI is similar in nature to universal precautions, but goes further in isolating workers from pathogens, including substances now known to carry HIV.

Practice of Universal precautions was introduced in 1985–88. In 1987, the practice of Universal precautions was adjusted by a set of rules known as body substance isolation. In 1996, both practices were replaced by the latest approach known as standard precautions (health care). Nowadays and in isolation, practice of body substance isolation has just historical significance.

Body substance isolation went further than universal precautions in isolating workers from pathogens, including substances now currently known to carry HIV. These pathogens fall into two broad categories, bloodborne (carried in the body fluids) and airborne. The practice of BSI was common in Pre-Hospital care and emergency medical services due to the often unknown nature of the patient and his/her disease or medical conditions. It was a part of the National Standards Curriculum for Prehospital Providers and Firefighters.

Types of body substance isolation included:

It was postulated that BSI precautions should be practiced in environment where treaters were exposed to bodily fluids, such as:

Such infection control techniques that were recommended following the AIDS outbreak in the 1980s. Every patient was treated as if infected and therefore precautions were taken to minimize risk. Other conditions which called for minimizing risks with BSI:

or any combination of the above.




</doc>
<doc id="4517" url="https://en.wikipedia.org/wiki?curid=4517" title="Boudica">
Boudica

Boudica (Latinised as Boadicea or Boudicea , and known in Welsh as ) was a queen of the British Celtic Iceni tribe who led an uprising against the occupying forces of the Roman Empire in AD 60 or 61, and died shortly after its failure, having supposedly poisoned herself. She is considered a British folk hero.

Boudica's husband, Prasutagus, ruled as a nominally independent ally of Rome and left his kingdom jointly to his daughters and the Roman emperor in his will. However, when he died, his will was ignored, and the kingdom was annexed and his property taken. According to Tacitus, Boudica was flogged and her daughters raped. Cassius Dio provides an alternative explanation for Boudica's response, saying that previous imperial donations to influential Britons were confiscated and the Roman financier and philosopher Seneca called in the loans he had forced on the reluctant Britons.

In AD 60 or 61, when the Roman governor Gaius Suetonius Paulinus was campaigning on the island of Anglesey off the northwest coast of Wales, Boudica led the Iceni, the Trinovantes, and others in revolt. They destroyed (modern Colchester), earlier the capital of the Trinovantes but at that time a , a settlement for discharged Roman soldiers and site of a temple to the former Emperor Claudius. Upon hearing of the revolt, Suetonius hurried to (modern London), the 20-year-old commercial settlement that was the rebels' next target. The Romans, having concluded that they lacked sufficient numbers to defend the settlement, evacuated and abandoned . Boudica led 100,000 Iceni, Trinovantes, and others to fight , and burned and destroyed and (modern-day St Albans).

An estimated 70,000–80,000 Romans and British were then killed in the three cities by those led by Boudica. Suetonius, meanwhile, regrouped his forces in the West Midlands, and, despite being heavily outnumbered, defeated the Britons in the Battle of Watling Street. The crisis caused Nero to consider withdrawing all Roman forces from Britain, but Suetonius' eventual victory over Boudica confirmed Roman control of the province. Boudica then either killed herself to avoid capture, or died of illness. The extant sources, Tacitus and Cassius Dio, differ.

Interest in these events revived in the English Renaissance and led to Boudica's fame in the Victorian era. Boudica has remained an important cultural symbol in the United Kingdom. The absence of native British literature during the early part of the first millennium means that knowledge of Boudica's rebellion comes solely from the writings of the Romans.

Boudica has been known by several versions of her name. Raphael Holinshed calls her Voadicia, while Edmund Spenser calls her Bunduca, a version of the name that was used in the popular Jacobean play "Bonduca", in 1612. William Cowper's poem, "Boadicea, an ode" (1782) popularised an alternative version of the name. From the 19th century until the late 20th century, Boadicea was the most common version of the name, which is probably derived from a mistranscription when a manuscript of Tacitus was copied in the Middle Ages.

Her name was clearly spelled Boudicca in the best manuscripts of Tacitus, but also , , and in the (later and probably secondary) epitome of Cassius Dio.

Kenneth Jackson concludes, based on later development of Welsh and Irish, that the name derives from the Proto-Celtic feminine adjective "*boudīkā", "victorious", that in turn is derived from the Celtic word "*boudā", "victory" (cf. Irish (Classical Irish ), "Buaidheach", Welsh ), and that the correct spelling of the name in Common Brittonic (the British Celtic language) is "Boudica", pronounced . The Gaulish version is attested in inscriptions as Boudiga in Bordeaux, Boudica in Lusitania, and Bodicca in Algeria.

The closest English equivalent to the vowel in the first syllable is the "ow" in "bow-and-arrow". John Rhys suggested that the most comparable Latin name, in meaning only, would be "Victorina".

Tacitus and Cassius Dio agree that Boudica was of royal descent. Dio describes her as tall, with tawny hair hanging down to below her waist, a harsh voice and a piercing glare. He says that she habitually wore a large golden necklace (perhaps a torc), a colourful tunic, and a thick cloak fastened by a brooch.

Boudica's husband, Prasutagus, was the king of the Iceni, a people who inhabited roughly what is now Norfolk. The Iceni initially voluntarily allied with Rome following Claudius's conquest of southern Britain in AD 43. They were proud of their independence, and had revolted in AD 47 when the then Roman governor Publius Ostorius Scapula planned to disarm all the peoples in the area of Britain under Roman control following a number of local uprisings. Ostorius defeated them and went on to put down other uprisings around Britain. The Iceni remained independent. Tacitus first mentioned Prasutagus when he wrote about Boudica's rebellion. It is unknown whether he became the king after the mentioned defeat of the Iceni. The client relationship with Rome ended after the end of the rebellion.

Tacitus wrote, "The Icenian king Prasutagus, celebrated for his long prosperity, had named the emperor his heir, together with his two daughters; an act of deference which he thought would place his kingdom and household beyond the risk of injury. The result was contrary — so much so that his kingdom was pillaged by centurions, his household by slaves; as though they had been prizes of war." He added that Boudica was lashed, her two daughters were raped, and that the estates of the leading Iceni men were confiscated.

Cassius Dio wrote: "An excuse for the war was found in the confiscation of the sums of money that Claudius had given to the foremost Britons; for these sums, as Decianus Catus, the procurator of the island maintained, were to be paid back." He also said that another reason was "the fact that Seneca, in the hope of receiving a good rate of interest, had lent to the islanders 40,000,000 sesterces that they did not want, and had afterwards called in this loan all at once and had resorted to severe measures in exacting it."

Tacitus did not say why Prasutagus's naming the emperor as his heir as well as his daughters was meant to avert the risk of injury. He did not explain why the Romans pillaged the kingdom, why they took the lands of the chiefs or why Boudica was flogged and her daughters were raped. Cassius Dio did not mention any of this. He said that the cause of the rebellion was the decision of the procurator of Britain (the chief financial officer) and Seneca (an advisor of the emperor Nero) to call in Prasutagus's debts and the harsh measures which were taken to collect them. Tacitus does not mention these events. However, he wrote: "Alarmed by this disaster and by the fury of the province which he had goaded into war by his rapacity, the procurator Catus crossed over into Gaul."

This was happening while the governor of Britain, Gaius Suetonius Paulinus, was away fighting in North Wales. It is unknown whether he approved of these actions. The centurions who pillaged the kingdom and who sent them are unknown. The text of Cassius Dio seems to suggest that Seneca, who was a private citizen, was responsible for the violence. It is unlikely that a legion was sent to the land of the Iceni as two of them were fighting at the island of Anglesey and the other two were stationed at their garrisons. Tacitus said, "It was against the veterans that their hatred was most intense. For these new settlers in the colony of Camulodunum drove people out of their houses, ejected them from their farms, called them captives and slaves ..."

In AD 60 or 61, while the current governor, Gaius Suetonius Paulinus, was leading a campaign against the island of (modern Anglesey) in the north of Wales, which was a refuge for British rebels and a stronghold of the druids, the Iceni conspired with their neighbours the Trinovantes, amongst others, to revolt. Boudica was chosen as their leader. Tacitus records that she addressed her army with these words, "It is not as a woman descended from noble ancestry, but as one of the people that I am avenging lost freedom, my scourged body, the outraged chastity of my daughters," and concluded, "This is a woman's resolve; as for men, they may live and be slaves." According to Tacitus, they drew inspiration from the example of Arminius, the prince of the Cherusci who had driven the Romans out of Germany in AD 9, and their own ancestors who had driven Julius Caesar from Britain. Dio says that at the outset Boudica employed a form of divination, releasing a hare from the folds of her dress and interpreting the direction in which it ran, and invoked Andraste, a British goddess of victory.

The rebels' first target was (Colchester), the former Trinovantian capital and, at that time, a Roman . The Roman veterans who had been settled there mistreated the locals, and a temple to the former emperor Claudius had been erected there at local expense, making the city a focus for resentment. The Roman inhabitants sought reinforcements from the procurator, Catus Decianus, but he sent only two hundred auxiliary troops. Boudica's army fell on the poorly defended city and destroyed it, besieging the last defenders in the temple for two days before it fell. Archaeologists have shown that the city was methodically demolished. The future governor Quintus Petillius Cerialis, then commanding the Legio IX "Hispana", attempted to relieve the city, but suffered an overwhelming defeat. His infantry was wiped out—only the commander and some of his cavalry escaped.

The location of this famous destruction of the is now claimed by some to be the village of Great Wratting, in Suffolk, which lies in the Stour Valley on the Icknield Way West of Colchester, and by a village in Essex. After this defeat, Catus Decianus fled to Gaul.

When news of the rebellion reached him, Suetonius hurried along Watling Street through hostile territory to . was a relatively new settlement, founded after the conquest of AD 43, but it had grown to be a thriving commercial centre with a population of travellers, traders, and, probably, Roman officials. Suetonius considered giving battle there, but considering his lack of numbers and chastened by Petillius's defeat, decided to sacrifice the city to save the province.

Alarmed by this disaster and by the fury of the province which he had goaded into war by his rapacity, the procurator Catus crossed over into Gaul. Suetonius, however, with wonderful resolution, marched amidst a hostile population to Londinium, which, though undistinguished by the name of a colony, was much frequented by a number of merchants and trading vessels. Uncertain whether he should choose it as a seat of war, as he looked round on his scanty force of soldiers, and remembered with what a serious warning the rashness of Petilius had been punished, he resolved to save the province at the cost of a single town. Nor did the tears and weeping of the people, as they implored his aid, deter him from giving the signal of departure and receiving into his army all who would go with him. Those who were chained to the spot by the weakness of their sex, or the infirmity of age, or the attractions of the place, were cut off by the enemy.

Londinium was abandoned to the rebels, who burnt it down, slaughtering anyone who had not evacuated with Suetonius. Archaeology shows a thick red layer of burnt debris covering coins and pottery dating before AD 60 within the bounds of Roman Londinium; while Roman-era skulls found in the Walbrook in 2013 were potentially linked to victims of the rebels. (St Albans) was next to be destroyed.

In the three settlements destroyed, between seventy and eighty thousand people are said to have been killed. Tacitus says that the Britons had no interest in taking or selling prisoners, only in slaughter by gibbet, fire, or cross. Dio's account gives more detail; that the noblest women were impaled on spikes and had their breasts cut off and sewn to their mouths, "to the accompaniment of sacrifices, banquets, and wanton behaviour" in sacred places, particularly the groves of Andraste.

While Boudica's army continued their assault in Verulamium (St. Albans), Suetonius regrouped his forces. According to Tacitus, he amassed a force including his own Legio XIV "Gemina", some "vexillationes" (detachments) of the XX "Valeria Victrix", and any available auxiliaries. The prefect of , Poenius Postumus, stationed near Exeter, ignored the call, and a fourth legion, , had been routed trying to relieve , but nonetheless the governor was able to call on almost ten thousand men.

Suetonius took a stand at an unidentified location, probably in the West Midlands somewhere along the Roman road now known as Watling Street, in a defile with a wood behind him — but his men were heavily outnumbered. Dio says that, even if they were lined up one deep, they would not have extended the length of Boudica's line. By now the rebel forces were said to have numbered 230,000-300,000. However, this number should be treated with scepticism — Dio's account is known only from a late epitome, and it is possible that some ancient sources might have had exaggerated enemy numbers, though the Roman historians usually recorded numbers including their own losses with scrupulous honesty, as can be witnessed when the accounts of Hannibal's invasions are read.

Boudica exhorted her troops from her chariot, her daughters beside her. Tacitus records her giving a short speech in which she presents herself not as an aristocrat avenging her lost wealth, but as an ordinary person, avenging her lost freedom, her battered body, and the abused chastity of her daughters. She said their cause was just, and the deities were on their side; the one legion that had dared to face them had been destroyed. She, a woman, was resolved to win or die; if the men wanted to live in slavery, that was their choice.

However, the lack of manoeuvrability of the British forces, combined with lack of open-field tactics to command these numbers, put them at a disadvantage to the Romans, who were skilled at open combat due to their superior equipment and discipline. Also, the narrowness of the field meant that Boudica could put forth only as many troops as the Romans could at a given time.

First, the Romans stood their ground and used volleys of (heavy javelins) to kill thousands of Britons who were rushing toward the Roman lines. The Roman soldiers, who had now used up their , were then able to engage Boudica's second wave in the open. As the Romans advanced in a wedge formation, the Britons attempted to flee, but were impeded by the presence of their own families, whom they had stationed in a ring of wagons at the edge of the battlefield, and were slaughtered. This is not the first instance of this tactic — the women of the Cimbri, in the Battle of Vercellae against Gaius Marius, were stationed in a line of wagons and acted as a last line of defence. Ariovistus of the Suebi is reported to have done the same thing in his battle against Julius Caesar. Tacitus reports that "according to one report almost eighty thousand Britons fell" compared with only four hundred Romans.

According to Tacitus in his "Annals", Boudica poisoned herself, though in the which was written almost twenty years prior he mentions nothing of suicide and attributes the end of the revolt to ("indolence"); Dio says she fell sick and died and then was given a lavish burial.

Postumus, on hearing of the Roman victory, fell on his sword. Catus Decianus, who had fled to Gaul, was replaced by Gaius Julius Alpinus Classicianus. Suetonius conducted punitive operations, but criticism by Classicianus led to an investigation headed by Nero's freedman Polyclitus. Fearing Suetonius's actions would provoke further rebellion, Nero replaced the governor with the more conciliatory Publius Petronius Turpilianus. The historian Gaius Suetonius Tranquillus tells us the crisis had almost persuaded Nero to abandon Britain. No historical records tell what had happened to Boudica's two daughters.

The location of Boudica's defeat is unknown. Many historians favour a site in the West Midlands, somewhere along the Roman road now known as Watling Street. Kevin K. Carroll suggests a site close to High Cross, Leicestershire, on the junction of Watling Street and the Fosse Way, which would have allowed the , based at Exeter, to rendezvous with the rest of Suetonius's forces, had they not failed to do so. Manduessedum (Mancetter), near the modern town of Atherstone in Warwickshire, has also been suggested, as has "The Rampart" near Messing in Essex, according to legend. More recently, a discovery of Roman artefacts in Kings Norton close to Metchley Camp has suggested another possibility, and a thorough examination of a stretch of Watling Street between St. Albans, Boudica's last known location, and the Fosse Way junction has suggested the Cuttle Mill area of Paulerspury in Northamptonshire, which has topography very closely matching that described by Tacitus of the scene of the battle, and where large quantities of human bones of both sexes, and including children, have been found over a wide area together with fragments of Roman pottery from the 1st century. 

In 2009, it was suggested that the Iceni were returning to East Anglia along the Icknield Way when they encountered the Roman army in the vicinity of Arbury Banks, Hertfordshire. In March 2010, evidence was published suggesting the site may be located at Church Stowe, Northamptonshire.

Tacitus took a particular interest in Britain as his father-in-law Gnaeus Julius Agricola served there three times (and was the subject of his first book). Agricola was a military tribune under Suetonius Paulinus, which almost certainly gave Tacitus an eyewitness source for Boudica's revolt. Cassius Dio's account is only known from an epitome, and his sources are uncertain. He is generally agreed to have based his account on that of Tacitus, but he simplifies the sequence of events and adds details, such as the calling in of loans, that Tacitus does not mention.

"Buddug" is likely to have been a figure of praise in the post-Roman Bardic tradition of the British Celts. In his 6th century work "On the Ruin and Conquest of Britain", the monk Gildas demonstrates his knowledge of this tradition, but is critical of the anti-Roman separatism:

""A treacherous lioness butchered the governors who had been left to give fuller voice and strength to the endeavors of Roman rule"".

"Buddug" has yet to be conclusively identified within the canon of medieval Welsh literature and she is not apparent in the , the or Geoffrey of Monmouth's "History of the Kings of Britain".

The area of King's Cross, London was previously a village known as Battle Bridge which was an ancient crossing of the River Fleet. The original name of the bridge was Broad Ford Bridge.

The name "Battle Bridge" led to a tradition that this was the site of a major battle between the Romans and the Iceni tribe led by Boudica. The tradition is not supported by any historical evidence and is rejected by modern historians. However, Lewis Spence's 1937 book "Boadicea — warrior queen of the Britons" went so far as to include a map showing the positions of the opposing armies. There is a belief that she was buried between platforms 9 and 10 in King's Cross station in London, England. There is no evidence for this and it is probably a post-World War II invention.

The first English writings appear during the reign of Queen Elizabeth following the rediscovery of the works of Tacitus. Polydore Vergil may have reintroduced her to British history as "Voadicea" in 1534. Raphael Holinshed also included her story in his "Chronicles" (1577), based on Tacitus and Dio, and inspired Shakespeare's younger contemporaries Francis Beaumont and John Fletcher to write a play, "Bonduca", in 1610. William Cowper wrote a popular poem, "Boadicea, an ode", in 1782.

It was in the Victorian era that Boudica's fame took on legendary proportions as Queen Victoria came to be seen as Boudica's "namesake", their names being identical in meaning. Victoria's Poet Laureate, Alfred, Lord Tennyson, wrote a poem, "Boadicea", and several ships were named after her.

"Boadicea and Her Daughters", a statue of the queen in her war chariot (anachronistically furnished with scythes after the Persian fashion) was executed by Thomas Thornycroft over the 1850s and 1860s with the encouragement of Prince Albert, who lent his horses for use as models. Thornycroft exhibited the head separately in 1864. It was cast in bronze in 1902, 17 years after Thornycroft's death, by his son Sir John, who presented it to the London County Council. They erected it on a plinth on the Victoria Embankment next to Westminster Bridge and the Houses of Parliament, inscribed with the following lines from Cowper's poem:<poem>
Regions Caesar never knew
Thy posterity shall sway.
</poem>

The great anti-imperialist rebel was now identified with the head of the British Empire, and her statue stood guard over the city she razed to the ground.

Boudica (Buddug) was chosen by the Welsh public as one of eleven statues of historical figures to be included in The Marble Hall at Cardiff City Hall. The statue was unveiled by David Lloyd George on 27 October 1916. The popularity of Buddug alongside other Welsh heroes such as St David and Owain Glyndwr was surprising to many--of the statues Buddug is the most ancient, the only female, and the only antecedent from outside the modern Welsh nation.





</doc>
<doc id="4518" url="https://en.wikipedia.org/wiki?curid=4518" title="Borneo">
Borneo

Borneo (; ) is the third largest island in the world and the largest in Asia. At the geographic centre of Maritime Southeast Asia, in relation to major Indonesian islands, it is located north of Java, west of Sulawesi, and east of Sumatra.

The island is politically divided among three countries: Malaysia and Brunei in the north, and Indonesia to the south. Approximately 73% of the island is Indonesian territory. In the north, the East Malaysian states of Sabah and Sarawak make up about 26% of the island. Additionally, the Malaysian federal territory of Labuan is situated on a small island just off the coast of Borneo. The sovereign state of Brunei, located on the north coast, comprises about 1% of Borneo's land area. A little more than half of the island is in the Northern Hemisphere including Brunei and the Malaysian portion, while the Indonesian portion spans both the Northern and Southern hemispheres.

Antipodal to an area of Amazon rainforest, Borneo is itself home to one of the oldest rainforests in the world.

The island is known by many names. Internationally it is known as "Borneo", after Brunei, derived from European contact with the kingdom in the 16th century during the Age of Exploration. The name "Brunei" possibly derives from the Sanskrit word "" (), meaning either "water" or Varuna, the Hindu god of rain. Indonesian natives called it "Kalimantan," which was derived from the Sanskrit word "Kalamanthana," meaning "burning weather island" (to describe its hot and humid tropical weather).

In earlier times, the island was known by other names. In 977, Chinese records began to use the term "Po-ni" to refer to Borneo. In 1225, it was also mentioned by the Chinese official Chau Ju-Kua (趙汝适). The Javanese manuscript Nagarakretagama, written by Majapahit court poet Mpu Prapanca in 1365, mentioned the island as "Nusa Tanjungnagara", which means the island of the Tanjungpura Kingdom.

 by the South China Sea to the north and northwest, the Sulu Sea to the northeast, the Celebes Sea and the Makassar Strait to the east, and the Java Sea and Karimata Strait to the south. To the west of Borneo are the Malay Peninsula and Sumatra. To the south and east are islands of Indonesia: Java and Sulawesi, respectively. To the northeast are the Philippine Islands. With an area of , it is the third-largest island in the world, and is the largest island of Asia (the largest continent). Its highest point is Mount Kinabalu in Sabah, Malaysia, with an elevation of . Before sea levels rose at the end of the last Ice Age, Borneo was part of the mainland of Asia, forming, with Java and Sumatra, the upland regions of a peninsula that extended east from present day Indochina. The South China Sea and Gulf of Thailand now submerge the former low-lying areas of the peninsula. Deeper waters separating Borneo from neighbouring Sulawesi prevented a land connection to that island, creating the divide known as Wallace's Line between Asian and Australia-New Guinea biological regions.
The largest river system is the Kapuas in West Kalimantan, with a length of . Other major rivers include the Mahakam in East Kalimantan (), the Barito in South Kalimantan (), Rajang in Sarawak () and Kinabatangan in Sabah (). Borneo has significant cave systems. In Sarawak, the Clearwater Cave has one of the world's longest underground rivers while Deer Cave is home to over three million bats, with guano accumulated to over deep. The Gomantong Caves in Sabah has been dubbed as the "Cockroach Cave" due to the presence of million of cockroaches inside the cave. The Gunung Mulu National Park in Sarawak and Sangkulirang-Mangkalihat Karst in East Kalimantan which particularly a karst areas contains thousands of smaller caves.

The Borneo rainforest is estimated to be around 140 million years old, making it one of the oldest rainforests in the world. It is the centre of the evolution and distribution of many endemic species of plants and animals, and the rainforest is one of the few remaining natural habitats for the endangered Bornean orangutan. It is an important refuge for many endemic forest species, including the Borneo elephant, the eastern Sumatran rhinoceros, the Bornean clouded leopard, the hose's palm civet and the dayak fruit bat.

Peat swamp forests occupy the entire coastline of Borneo. The soil of the peat swamp are comparatively infertile, while it is known to be the home of various bird species such as the hook-billed bulbul, helmeted hornbill and rhinoceros hornbill. There are about 15,000 species of flowering plants with 3,000 species of trees (267 species are dipterocarps), 221 species of terrestrial mammals and 420 species of resident birds in Borneo. There are about 440 freshwater fish species in Borneo (about the same as Sumatra and Java combined). In 2010, the World Wide Fund for Nature (WWF) stated that 123 species have been discovered in Borneo since the "Heart of Borneo" agreement was signed in 2007.
The WWF has classified the island into seven distinct ecoregions. Most are lowland regions:
The highest elevations of Mount Kinabalu are home to the Kinabalu mountain alpine meadow, an alpine shrubland notable for its numerous endemic species, including many orchids.

The island historically had extensive rainforest cover, but the area was reduced due to heavy logging by the Indonesian and Malaysian wood industry, especially with the large demands of raw materials from industrial countries along with the conversion of forest lands for large-scale agricultural purposes. Half of the annual global tropical timber acquisition comes from Borneo. Palm oil plantations have been widely developed and are rapidly encroaching on the last remnants of primary rainforest. Forest fires since 1997, started by the locals to clear the forests for plantations were exacerbated by an exceptionally dry El Niño season, worsening the annual shrinkage of the rainforest. During these fires, hotspots were visible on satellite images and the resulting haze frequently affected Brunei, Indonesia, Malaysia, and Singapore. The haze could also reach southern Thailand, Cambodia, Vietnam and the Philippines as evidenced on the 2015 Southeast Asian haze.

According to ancient Chinese (977), Indian and Japanese manuscripts, western coastal cities of Borneo had become trading ports by the first millennium AD. In Chinese manuscripts, gold, camphor, tortoise shells, hornbill ivory, rhinoceros horn, crane crest, beeswax, lakawood (a scented heartwood and root wood of a thick liana, "Dalbergia parviflora"), dragon's blood, rattan, edible bird's nests and various spices were described as among the most valuable items from Borneo. The Indians named Borneo "Suvarnabhumi" (the land of gold) and also "Karpuradvipa" (Camphor Island). The Javanese named Borneo "Puradvipa", or Diamond Island. Archaeological findings in the Sarawak river delta reveal that the area was a thriving centre of trade between India and China from the 6th century until about 1300.

Stone pillars bearing inscriptions in the Pallava script, found in Kutai along the Mahakam River in East Kalimantan and dating to around the second half of the 4th century, constitute some of the oldest evidence of Hindu influence in Southeast Asia. By the 14th century, Borneo became a vassal state of Majapahit (in present-day Indonesia), later changing its allegiance to the Ming dynasty of China. The religion of Islam entered the island in the 10th century, following the arrival of Muslim traders who later converted many indigenous peoples in the coastal areas.

The Sultanate of Brunei declared independence from Majapahit following the death of Majapahit Emperor in mid-14th century. During its golden age under Bolkiah from the 15th century to the 17th century, the Bruneian Empire ruled almost the entire coastal area of Borneo (lending its name to the island due to its influence in the region) and several islands in the Philippines. During the 1450s, Shari'ful Hashem Syed Abu Bakr, an Arab born in Johor, arrived in Sulu from Malacca. In 1457, he founded the Sultanate of Sulu; he titled himself as "Paduka Maulana Mahasari Sharif Sultan Hashem Abu Bakr". Following their independence in 1578 from Brunei's influence, the Sulu's began to expand their thalassocracy to parts of the northern Borneo. Both the sultanates who ruled northern Borneo had traditionally engaged in trade with China by means of the frequently-arriving Chinese junks. Despite the thalassocracy of the sultanates, Borneo's interior region remained free from the rule of any kingdoms.

Since the fall of Malacca in 1511, Portuguese merchants traded regularly with Borneo, and especially with Brunei from 1530. Having visited Brunei's capital, the Portuguese described the place as surrounded by a stone wall. While Borneo was seen as rich, the Portuguese did not make any attempts to conquer it. The Spanish visit to Brunei led to the Castilian War in 1578. The English began to trade with Sambas of southern Borneo in 1609, while the Dutch only began their trade in 1644: to Banjar and Martapura, also in the southern Borneo. The Dutch tried to settle the island of Balambangan, north of Borneo, in the second half of the 18th century, but withdrew by 1797. In 1812, the sultan in southern Borneo ceded his forts to the English East India Company. The English, led by Stamford Raffles, then tried to establish an intervention in Sambas but failed. Although they managed to defeat the Sultanate the next year and declared a blockade on all ports in Borneo except Brunei, Banjarmasin and Pontianak, the project was cancelled by the British Governor-General Lord Minto in India as it was too expensive. At the beginning of British and Dutch exploration on the island, they described the island of Borneo as full of head hunters, with the indigenous in the interior practising cannibalism, and the waters around the island infested with pirates, especially between the north eastern Borneo and the southern Philippines. The Malay and Sea Dayak pirates preyed on maritime shipping in the waters between Singapore and Hong Kong from their haven in Borneo, along with the attacks by Illanuns of the Moro Pirates from the southern Philippines, such as in the Battle off Mukah.
The Dutch began to intervene in the southern part of the island upon resuming contact in 1815, posting Residents to Banjarmasin, Pontianak and Sambas and Assistant-Residents to Landak and Mampawa. The Sultanate of Brunei in 1842 granted large parts of land in Sarawak to the English adventurer James Brooke, as a reward for his help in quelling a local rebellion. Brooke established the Kingdom of Sarawak and was recognised as its rajah after paying a fee to the Sultanate. He established a monarchy, and the Brooke dynasty (through his nephew and great-nephew) ruled Sarawak for 100 years; the leaders were known as the White Rajahs. Brooke also acquired the island of Labuan for Britain in 1846 through the Treaty of Labuan with the Sultan of Brunei, Omar Ali Saifuddin II on 18 December 1846. The region of northern Borneo came under the administration of North Borneo Chartered Company following the acquisition of territory from the Sultanates of Brunei and Sulu by a German businessman and adventurer named Baron von Overbeck, before it was passed to British Dent brothers (comprising Alfred Dent and Edward Dent). Further enroachment by the British reduced the territory of Brunei. This led the 26th Sultan of Brunei, Hashim Jalilul Alam Aqamaddin to appeal the British to stop, and as a result a Treaty of Protection was signed in 1888, rendering Brunei a British protectorate.
Before the acquisition by the British, the Americans also managed to establish their temporary presence in northwestern Borneo after acquiring a parcel of land from the Sultanate of Brunei. A company known as American Trading Company of Borneo was formed by Joseph William Torrey, Thomas Bradley Harris and several Chinese investors, establishing a colony named "Ellena" in the Kimanis area. The colony failed and was abandoned, due to denials of financial backing, especially by the US government, and to diseases and riots among the workers. Before Torrey left, he managed to sell the land to the German businessman, Overbeck. Meanwhile, the Germans under William Frederick Schuck was awarded a parcel of land in northeastern Borneo of the Sandakan Bay from the Sultanate of Sulu where he operate business and export large quantities of arms, opium, textiles and tobacco to Sulu before the land were also passed to Overbeck by the Sultanate.

Prior to the recognition of Spanish presence in the Philippine archipelago, a protocol known as the Madrid Protocol of 1885 was signed between the governments of the United Kingdom, Germany and Spain in Madrid to cement Spanish influence and recognise their sovereignty over the Sultanate of Sulu—in return for Spain's relinquishing its claim to the former possessions of the Sultanate in northern Borneo. The British administration then established the first railway network in northern Borneo, known as the North Borneo Railway. During this time, the British sponsored a large number of Chinese workers to migrate to northern Borneo to work in European plantation and mines, and the Dutch followed suit to increase their economic production. By 1888, North Borneo, Sarawak and Brunei in northern Borneo had become British protectorate. The area in southern Borneo was made Dutch protectorate in 1891. The Dutch who already claimed the whole Borneo were asked by Britain to delimit their boundaries between the two colonial territories to avoid further conflicts. The British and Dutch governments had signed the Anglo-Dutch Treaty of 1824 to exchange trading ports in Malay Peninsula and Sumatra that were under their controls and assert spheres of influence. This resulted in indirectly establishing British- and Dutch-controlled areas in the north (Malay Peninsula) and south (Sumatra and Riau Islands) respectively.

During World War II, Japanese forces gained control and occupied most areas of Borneo from 1941–45. In the first stage of the war, the British saw the Japanese advance to Borneo as motivated by political and territorial ambitions rather than economic factors. The occupation drove many people in the coastal towns to the interior, searching for food and escaping the Japanese. The Chinese residents in Borneo, especially with the Sino-Japanese War in Mainland China mostly resisted the Japanese occupation. Following the formation of resistance movements in northern Borneo such as the Jesselton Revolt, many innocent indigenous and Chinese people were executed by the Japanese for their alleged involvement.

In Kalimantan, the Japanese also killed many Malay intellectuals, executing all the Malay Sultans of West Kalimantan in the Pontianak incidents, together with Chinese people who were already against the Japanese for suspecting them to be threats. Sultan Muhammad Ibrahim Shafi ud-din II of Sambas was executed in 1944. The Sultanate was thereafter suspended and replaced by a Japanese council. The Japanese also set-up "Pusat Tenaga Rakjat" (PUTERA) in the Indonesian archipelago in 1943, although it was abolished the following year when it become too nationalistic. Some of the Indonesian nationalist like Sukarno and Hatta who had returned from Dutch exile began to co-operate with the Japanese. Shortly after his release, Sukarno became President of the Central Advisory Council, an advisory council for south Borneo, Celebes, and Lesser Sunda, set up in February 1945.

Since the fall of Singapore, the Japanese sent several thousand of British and Australian prisoners of war to camps in Borneo such as Batu Lintang camp. From the Sandakan camp site, only six of some 2,500 prisoners survived after they were forced to march in an event known as the Sandakan Death March. In addition, of the total of 17,488 Javanese labourers brought in by the Japanese during the occupation, only 1,500 survived mainly due to starvation, harsh working conditions and maltreatment. The Dayak and other indigenous people played a role in guerrilla warfare against the occupying forces, particularly in the Kapit Division. They temporarily revived headhunting of Japanese toward the end of the war, with Allied Z Special Unit provided assistance to them. Australia contributed significantly to the liberation of Borneo. The Australian Imperial Force was sent to Borneo to fight off the Japanese. Together with other Allies, the island was completely liberated in 1945.

Towards the end of the war, Japan decided to give an early independence to a new proposed country of Indonesia on 17 July 1945, with an Independence Committee meeting scheduled for 19 August 1945. However, following the surrender of Japan to the Allied forces, the meeting was shelved. Sukarno and Hatta continued the plan by unilaterally declaring independence, although the Dutch tried to retake their colonial possession in Borneo. The southern part of the island achieved its independence through the Proclamation of Indonesian Independence on 17 August 1945. The reaction was relatively muted with little open fighting in Pontianak or in the Chinese majority areas. While nationalist guerrillas supporting the inclusion of southern Borneo in the new Indonesian republic were active in Ketapang, and to lesser extent in Sambas where they rallied with the red-white flag which became the flag of Indonesia, most of the Chinese residents in southern Borneo expected to be liberate by Chinese Nationalist troops from Mainland China and to integrate their districts as an overseas province of China.

In May 1945, officials in Tokyo suggested that whether northern Borneo should be included in the proposed new country of Indonesia should be separately determined based on the desires of its indigenous people and following the disposition of Malaya. Sukarno and Mohammad Yamin meanwhile continuously advocated for a Greater Indonesian republic. As the President of the new republic, perceiving the British trying to maintain their presence in northern Borneo and Malay Peninsula, he decided to launch a military infiltration later known as the confrontation between 1962 until 1969. In 1961, Prime Minister Tunku Abdul Rahman of the independent Federation of Malaya desired to unite Malaya, the British colonies of Sarawak, North Borneo, Singapore and the Protectorate of Brunei under the proposed Federation of Malaysia. The idea was heavily opposed by the governments in both Indonesia and the Philippines as well from Communist sympathisers and nationalist in Borneo. As a response to the growing opposition, the British deployed their armed forces to guard their colonies against Indonesian and communist revolts, which was also participated by Australia and New Zealand.
The Philippines opposed the newly proposed federation, claiming the eastern part of North Borneo (today the Malaysian state of Sabah) as part of its territory as a former possession of the Sultanate of Sulu. The Philippine government mostly based their claim on the Sultanate of Sulu's cession agreement with the British North Borneo Company, as by now the Sultanate had come under the jurisdiction of the Philippine republican administration, which therefore should inherit the Sulu former territories. The Philippine government also claimed that the heirs of the Sultanate had ceded all their territorial rights to the republic.

The Sultanate of Brunei at the first welcomed the proposal of a new larger federation. Meanwhile, the Brunei People's Party led by A.M. Azahari desired to reunify Brunei, Sarawak and North Borneo into one federation known as the North Borneo Federation (), where the Sultan of Brunei would be the head of state for the federation—though Azahari had his own intention to abolish the Brunei Monarchy, to make Brunei more democratic, and to integrate the territory and other former British colonies in Borneo into Indonesia, with the support from the latter government. This directly led to the Brunei Revolt, which thwarted Azahari's attempt and forced him to escape to Indonesia. Brunei withdrew from being part of the new Federation of Malaysia due to some disagreements on other issues while political leaders in Sarawak and North Borneo continued to favour inclusion in a larger federation.

With the continuous opposition from Indonesia and the Philippines, the Cobbold Commission was established to discover the feeling of the native populations in northern Borneo; it found the people greatly in favour of federation, with various stipulations. The federation was successfully achieved with the inclusion of northern Borneo through the Malaysia Agreement on 16 September 1963. Until present, the area in northern Borneo still subjected to attacks by Moro Pirates since the 18th century, and militants such as the Abu Sayyaf since 2000 in the frequent cross border attacks. During the administration of Philippine President of Ferdinand Marcos, the President made some attempts to destabilise the state of Sabah, although his plan failed and resulted in the Jabidah massacre and later in insurgency in the southern Philippines.

The demonym for Borneo is Bornean.

Borneo has 21.3 million inhabitants (in 2014), a population density of . Most of the population lives in coastal cities, although the hinterland has small towns and villages along the rivers. The population consists mainly of Dayak ethnic groups, Malay, Banjar, Orang Ulu, Chinese and Kadazan-Dusun. The Chinese, who make up 29% of the population of Sarawak and 17% of total population in West Kalimantan, Indonesia are descendants of immigrants primarily from southeastern China.

In Sabah during the administration of Mustapha Harun of the United Sabah National Organisation (USNO) in the 1970s, thousands of Muslim immigrants and refugees from the southern Philippines of Mindanao and Sulawesi of Indonesia were given sanctuary and later identity cards in the bid to increase the Muslim population of the state: a policy later known as Project IC. Due to the high number of crimes attributed to the new migrant populations, ethnic tension between the indigenous and migrant populations has risen up to the present.

In Kalimantan since the 1990s, the Indonesian government has undertaken an intense transmigration program; to that end it has financed the relocation of poor, landless families from Java, Madura, and Bali. By 2001, transmigrants made up 21% of the population in Central Kalimantan. Since the 1990s, the indigenous Dayak and Malays have resisted encroachment by these migrants, and violent conflict has occurred between some transmigrant and indigenous populations. In the 1999 Sambas riots, Dayaks and Malays joined together to massacre thousands of the Madurese migrants. In Kalimantan, thousands were killed in 2001 fighting between Madurese transmigrants and the Dayak people in the Sampit conflict.

The following is a list of 20 largest cities in Borneo by population, based on 2010 census for Indonesia and 2010 census for Malaysia. Population data signifies number within official districts and does not include adjoining or nearby conurbation outside defined districts—such as Kota Kinabalu and Banjarbaru. In other instances, the district area is much larger than the actual city it represents thereby relatively inflating the population estimate by including the surrounding rural population—in cases such as Tawau and Palangka Raya.

The island of Borneo is divided administratively by three countries.

Brunei: Census of Population 2001
islands administered as Borneo, geologically part of Borneo, on nearshore islands (2.5 km off the main island of Borneo)
Citypopulation.de reports on Official Decennial Censuses in 2010 for both Indonesia and Malaysia, independent estimate for Brunei.

Borneo's economy depends mainly on agriculture, logging and mining, oil and gas, and ecotourism. Brunei's economy is highly dependent on the oil and gas production sector, and the country has become one of the largest oil producers in Southeast Asia. The Malaysian states of Sabah and Sarawak are both top exporters of timber. Sabah is also known as the agricultural producer of rubber, cacao, and vegetables, and for its fisheries, while both Sabah and Sarawak export liquefied natural gas (LNG) and petroleum. The Indonesian provinces of Kalimantan are mostly dependent on mining sectors despite also being involved in logging and oil and gas explorations.




</doc>
<doc id="4519" url="https://en.wikipedia.org/wiki?curid=4519" title="Ballpoint pen">
Ballpoint pen

A ballpoint pen, also known as a biro or ball pen, is a pen that dispenses ink (usually in paste form) over a metal ball at its point, i.e. over a "ball point". The metal commonly used is steel, brass, or tungsten carbide. It was conceived and developed as a cleaner and more reliable alternative to dip pens and fountain pens, and it is now the world's most-used writing instrument: millions are manufactured and sold daily. As a result, it has influenced art and graphic design and spawned an artwork genre.

Pen manufacturers produce designer ballpoint pens for the high-end and collectors' markets.

The Bic Cristal is a popular disposable type of ballpoint pen whose design is recognised by its place in the permanent collection of the Museum of Modern Art, New York.

The concept of using a "ball point" within a writing instrument as a method of applying ink to paper has existed since the late 19th century. In these inventions, the ink was placed in a thin tube whose end was blocked by a tiny ball, held so that it could not slip into the tube or fall out of the pen.

The first patent for a ballpoint pen was issued on 30 October 1888 to John J. Loud, who was attempting to make a writing instrument that would be able to write "on rough surfaces-such as wood, coarse wrapping-paper, and other articles" which then-common fountain pens could not. Loud's pen had a small rotating steel ball, held in place by a socket. Although it could be used to mark rough surfaces such as leather, as Loud intended, it proved to be too coarse for letter-writing. With no commercial viability, its potential went unexploited and the patent eventually lapsed.

The manufacture of economical, reliable ballpoint pens as we know them arose from experimentation, modern chemistry, and precision manufacturing capabilities of the early 20th century. Patents filed worldwide during early development are testaments to failed attempts at making the pens commercially viable and widely available. Early ballpoints did not deliver the ink evenly; overflow and clogging were among the obstacles inventors faced toward developing reliable ballpoint pens. If the ball socket was too tight, or the ink too thick, it would not reach the paper. If the socket was too loose, or the ink too thin, the pen would leak or the ink would smear. Ink reservoirs pressurized by piston, spring, capillary action, and gravity would all serve as solutions to ink-delivery and flow problems.

László Bíró, a Hungarian newspaper editor frustrated by the amount of time that he wasted filling up fountain pens and cleaning up smudged pages, noticed that inks used in newspaper printing dried quickly, leaving the paper dry and smudge free. He decided to create a pen using the same type of ink. Bíró enlisted the help of his brother György, a chemist, to develop viscous ink formulae for new ballpoint designs.
Bíró's innovation successfully coupled ink-viscosity with a ball-socket mechanism which acted compatibly to prevent ink from drying inside the reservoir while allowing controlled flow. Bíró filed a British patent on 15 June 1938.

In 1941, the Bíró brothers and a friend, Juan Jorge Meyne, fled Germany and moved to Argentina, where they formed "Bíró Pens of Argentina" and filed a new patent in 1943. Their pen was sold in Argentina as the "Birome" (portmanteau of the names Bíró and Meyne), which is how ballpoint pens are still known in that country. This new design was licensed by the British, who produced ballpoint pens for RAF aircrew as the "Biro". Ballpoint pens were found to be more versatile than fountain pens, especially at high altitudes, where fountain pens were prone to ink-leakage.

Bíró's patent, and other early patents on ballpoint pens often used the term "ball-point fountain pen".

Following World War II, many companies vied to commercially produce their own ballpoint pen design. In post-war Argentina, success of the Birome ballpoint was limited, but in mid-1945, the "Eversharp" Co., a maker of mechanical pencils, teamed up with Eberhard Faber Co. to license the rights from Birome for sales in the United States.

During the same period, American entrepreneur Milton Reynolds came across a Birome ballpoint pen during a business trip to Buenos Aires, Argentina. Recognizing commercial potential, he purchased several ballpoint samples, returned to the United States, and founded "Reynolds International Pen Company". Reynolds bypassed the Birome patent with sufficient design alterations to obtain an American patent, beating Eversharp and other competitors to introduce the pen to the U.S. market. Debuting at Gimbels department store in New York City on 29 October 1945, for US$12.50 each (1945 US dollar value, about $ in dollars), "Reynolds Rocket" became the first commercially successful ballpoint pen. Reynolds went to great extremes to market the pen, with great success; Gimbel's sold many thousands of pens within one week. In Britain, the Miles Martin pen company was producing the first commercially successful ballpoint pens there by the end of 1945.

Neither Reynolds' nor Eversharp's ballpoint lived up to consumer expectations in America. Ballpoint pen sales peaked in 1946, and consumer interest subsequently plunged due to market-saturation. By the early 1950s the ballpoint boom had subsided and Reynolds' company folded.

Paper Mate pens, among the emerging ballpoint brands of the 1950s, bought the rights to distribute their own ballpoint pens in Canada. Facing concerns about ink-reliability, Paper Mate pioneered new ink formulas and advertised them as "banker-approved". In 1954, Parker Pens released "The Jotter"—the company's first ballpoint—boasting additional features and technological advances which also included the use of tungsten-carbide textured ball-bearings in their pens. In less than a year, Parker sold several million pens at prices between three and nine dollars. In the 1960s, the failing Eversharp Co. sold its pen division to Parker and ultimately folded.

Marcel Bich also introduced a ballpoint pen to the American marketplace in the 1950s, licensed from Bíró and based on the Argentine designs. Bich shortened his name to Bic in 1953, becoming the ballpoint brand now recognised globally. Bic pens struggled until the company launched its "Writes The First Time, Every Time!" advertising campaign in the 1960s. Competition during this era forced unit prices to drop considerably.

Ballpoint pen ink is normally a paste containing around 25 to 40 percent dye. The dyes are suspended in a solvent of "oil". The most common of the oils are benzyl alcohol or phenoxyethanol, which mix with the dyes to create a smooth paste that dries quickly. The dyes used in blue and black ballpoint pens are basic dyes based on triarylmethane and acid dyes derived from diazo compounds or phthalocyanine. Common dyes in blue (and black) ink are Prussian blue, Victoria blue, methyl violet, crystal violet and phthalocyanine blue. The dye eosin is commonly used for red ink.

The inks are resistant to water after drying but can be defaced by certain solvents.

Ballpoint pens are produced in both disposable and refillable models. Refills allow for the entire internal ink reservoir, including a ballpoint and socket, to be replaced. Such characteristics are usually associated with designer-type pens or those constructed of finer materials. The simplest types of ballpoint pens are disposable and have a cap to cover the tip when the pen is not in use, or a mechanism for retracting the tip, which varies between manufacturers but is usually a spring- or screw-mechanism.

Rollerball pens employ the same ballpoint mechanics, but with the use of water-based inks instead of oil-based inks. Compared to oil-based ballpoints, rollerball pens are said to provide more fluid ink-flow, but the water-based inks will blot if held stationary against the writing surface. Water-based inks also remain wet longer when freshly applied and are thus prone to "smearing"—posing problems to left-handed people (or right handed people writing right-to-left script)—and "running", should the writing surface become wet.

Some ballpoint pens use a hybrid ink formulation whose viscosity is lower than that of standard ballpoint ink, but greater than rollerball ink. The ink dries faster than a standard ballpoint to prevent smearing when writing. These pens are better suited for left handed persons. Examples are the Uni Jetstream and Pilot Acroball ranges. These pens are also labelled "extra smooth", as they offer a smoother writing experience compared to normal ballpoint pens. 

Because of a ballpoint pen's reliance on gravity to coat the ball with ink, most cannot be used to write upside-down. However, technology developed by Fisher pens in the United States resulted in the production of what came to be known as the "Fisher Space Pen". Space Pens combine a more viscous ink with a pressurised ink reservoir that forces the ink toward the point. Unlike standard ballpoints, the rear end of a Space Pen's pressurized reservoir is sealed, eliminating evaporation and leakage, thus allowing the pen to write upside-down, in zero-gravity environments, and reportedly underwater. Astronauts have made use of these pens in outer space.

Ballpoint pens with "erasable ink" were pioneered by the Paper Mate pen company. The ink formulas of erasable ballpoints have properties similar to rubber cement, allowing the ink to be literally rubbed clean from the writing surface before drying and eventually becoming permanent. Erasable ink is much thicker than standard ballpoint inks, requiring pressurised cartridges to facilitate inkflow—meaning they can also write upside-down. Though these pens are equipped with erasers, any eraser will suffice.

The inexpensive, disposable Bic Cristal (also simply "Bic pen" or "Biro") is reportedly the most widely sold pen in the world. It was the Bic company's first product and is still synonymous with the company name. The Bic Cristal is part of the permanent collection at the Museum of Modern Art in New York City, acknowledged for its industrial design. Its hexagonal barrel mimics that of a wooden pencil and is transparent, showing the ink level in the reservoir. Originally a sealed streamlined cap, the modern pen cap has a small hole at the top to meet safety standards, helping to prevent suffocation if children suck it into the throat.

Multipens are pens that feature multiple varying colored ballpoint refills. Sometimes ballpoint refills are combined with another non-ballpoint refill.

Sometimes ballpoint pens combine a ballpoint tip on one end and touchscreen stylus on the other.

Ballpoint pens are sometimes provided free by businesses, such as hotels, as a form of advertising—printed with a company's name; a ballpoint pen is a relatively low cost advertisement that is highly effective (customers will use, and therefore see, a pen daily). Businesses and charities include ballpoint pens in direct mail campaigns to increase a customer's interest in the mailing. Ballpoints have also been produced to commemorate events, such as a pen commemorating the 1963 assassination of President John F. Kennedy.

Ballpoint pens have proven to be a versatile art medium for professional artists as well as amateur doodlers. Low cost, availability, and portability are cited by practitioners as qualities which make this common writing tool a convenient, alternative art supply. Some artists use them within mixed-media works, while others use them solely as their medium-of-choice.

Effects not generally associated with ballpoint pens can be achieved. Traditional pen-and-ink techniques such as stippling and cross-hatching can be used to create half-tones or the illusion of form and volume. For artists whose interests necessitate precision line-work, ballpoints are an obvious attraction; ballpoint pens allow for sharp lines not as effectively executed using a brush. Finely applied, the resulting imagery has been mistaken for airbrushed artwork and photography, causing reactions of disbelief which ballpoint artist Lennie Mace refers to as the "Wow Factor".

Famous 20th-century artists such as Andy Warhol, among others, have utilised ballpoint pens to some extent during their careers. Ballpoint pen artwork continues to attract interest in the 21st century, with contemporary artists gaining recognition for their specific use of ballpoint pens; for their technical proficiency, imagination, and innovation. Korean-American artist Il Lee has been creating large-scale, ballpoint-only abstract artwork since the late 1970s. Since the 1980s, Lennie Mace creates imaginative, ballpoint-only artwork of varying content and complexity, applied to unconventional surfaces including wood and denim. The artist coined terms such as "PENtings" and "Media Graffiti" to describe his varied output. More recently, British artist James Mylne has been creating photo-realistic artwork using mostly black ballpoints, sometimes with minimal mixed-media color. In the mid-2000s (decade), Juan Francisco Casas generated Internet attention for a series of large-scale, photo-realistic ballpoint duplications of his own snapshots of friends, utilising only blue pens.

Using ballpoint pens to create artwork is not without limitations. Color availability and sensitivity of ink to light are among concerns of ballpoint pen artists. Mistakes pose greater risks to ballpoint artists; once a line is drawn, it generally cannot be erased. Additionally, "blobbing" of ink on the drawing surface and "skipping" of ink-flow require consideration when using ballpoint pens for artistic purposes. Although the mechanics of ballpoint pens remain relatively unchanged, ink composition has evolved to solve certain problems over the years, resulting in unpredictable sensitivity to light and some extent of fading.

Although designs and construction vary between brands, basic components of all ballpoint pens are universal. Standard components of a ballpoint tip include the freely rotating "ball" itself (distributing the ink on the writing surface), a "socket" holding the ball in place, small "ink channels" that provide ink to the ball through the socket, and a self-contained "ink reservoir" supplying ink to the ball. In modern disposable pens, narrow plastic tubes contain the ink, which is compelled downward to the ball by gravity. Brass, steel, or tungsten carbide are used to manufacture the ball bearing-like points, then housed in a brass socket.

The function of these components can be compared with the ball-applicator of roll-on antiperspirant; the same technology at a larger scale. The ballpoint tip delivers the ink to the writing surface while acting as a "buffer" between the ink in the reservoir and the air outside, preventing the quick-drying ink from drying inside the reservoir. Modern ballpoints are said to have a two-year shelf life, on average.

A ballpoint tip that can write comfortably for a long period of time is not easy to produce as it requires high-precision machinery and thin high grade steel alloy plates. China, which currently (2017) produces about 80 percent of the world's ballpoint pens, relied up to 2017 on imported ballpoint tips and metal alloys.

The common ballpoint pen is a product of mass production, with components produced separately on assembly lines. Basic steps in the manufacturing process include the production of ink formulas, moulding of metal and plastic components, and assembly. Marcel Bich was involved in developing the production of inexpensive ballpoint pens.

The International Organization for Standardization has published standards for ball point and roller ball pens:






</doc>
<doc id="4524" url="https://en.wikipedia.org/wiki?curid=4524" title="Burroughs Corporation">
Burroughs Corporation

The Burroughs Corporation was a major American manufacturer of business equipment. The company was founded in 1886 as the American Arithmometer Company, and after the 1986 merger with Sperry UNIVAC was renamed Unisys. The company's history paralleled many of the major developments in computing. At its start, it produced mechanical adding machines, and later moved into programmable ledgers and then computers. It was one of the largest producers of mainframe computers in the world, also producing related equipment including typewriters and printers.

In 1886, the American Arithmometer Company was established in St. Louis, Missouri to produce and sell an adding machine invented by William Seward Burroughs (grandfather of Beat Generation author William S. Burroughs). In 1904, six years after Burroughs' death, the company moved to Detroit and changed its name to the Burroughs Adding Machine Company. It was soon the biggest adding machine company in America.

The adding machine range began with the basic, hand-cranked P100 which was only capable of adding. The design included some revolutionary features, foremost of which was the dashpot. The P200 offered a subtraction capability and the P300 provided a means of keeping 2 separate totals. The P400 provided a moveable carriage, and the P600 and top-of-the-range P612 offered some limited programmability based upon the position of the carriage. The range was further extended by the inclusion of the "J" series which provided a single finger calculation facility, and the "c" series of both manual and electrical assisted comptometers. In the late 1960s, the Burroughs sponsored "nixi-tube" provided an electronic display calculator. Burroughs developed a range of adding machines with different capabilities, gradually increasing in their capabilities. A revolutionary adding machine was the "Sensimatic", which was able to perform many business functions semi-automatically. It had a moving programmable carriage to maintain ledgers. It could store 9, 18 or 27 balances during the ledger posting operations and worked with a mechanical adder named a Crossfooter. The Sensimatic developed into the "Sensitronic" which could store balances on a magnetic stripe which was part of the ledger card. This balance was read into the accumulator when the card was inserted into the carriage. The Sensitronic was followed by the E1000, E2000, E3000, E4000, E6000 and the E8000, which were computer systems supporting card reader/punches and a line printer.

Later, Burroughs was selling more than adding machines, including typewriters. But the biggest shift in company history came in 1953: the Burroughs Adding Machine Company was renamed the Burroughs Corporation and began moving into computer products, initially for banking institutions. This move began with Burroughs' purchase in June 1956, of the ElectroData Corporation in Pasadena, California, a spinoff of the Consolidated Engineering Corporation which had designed test instruments and had a cooperative relationship with Caltech in Pasadena. ElectroData had built the Datatron 205 and was working on the Datatron 220. The first major computer product that came from this marriage was the B205 tube computer. In the late 1960s the L and TC series range was produced (e.g. the TC500—Terminal Computer 500) which had a golf ball printer and in the beginning a 1K (64 bit) disk memory. These were popular as branch terminals to the B5500/6500/6700 systems, and sold well in the banking sector, where they were often connected to non-Burroughs mainframes. In conjunction with these products, Burroughs also manufactured an extensive range of cheque processing equipment, normally attached as terminals to a larger system such as a B2700 or B1700.

Burroughs was one of the nine major United States computer companies in the 1960s, with IBM the largest, Honeywell, NCR Corporation, Control Data Corporation (CDC), General Electric (GE), Digital Equipment Corporation (DEC), RCA and Sperry Rand (UNIVAC line). In terms of sales, Burroughs was always a distant second to IBM. In fact, IBM's market share was so much larger than all of the others that this group was often referred to as "IBM and the Seven Dwarfs." By 1972 when GE and RCA were no longer in the mainframe business, the remaining five companies behind IBM became known as the BUNCH, an acronym based on their initials.

At the same time, Burroughs was very much a competitor. Like IBM, Burroughs tried to supply a complete line of products for its customers, including Burroughs-designed printers, disk drives, tape drives, computer printing paper, and even typewriter ribbons.

In the 1950s, Burroughs worked with the Federal Reserve Bank on the development and computer processing of magnetic ink character recognition (MICR) especially for the processing of bank cheques. Burroughs made special MICR/OCR sorter/readers which attached to their medium systems line of computers (2700/3700/4700) and this entrenched the company in the computer side of the banking industry.

The Burroughs Corporation developed three highly innovative architectures, based on the design philosophy of "language-directed design". Their machine instruction sets favored one or many high level programming languages, such as ALGOL, COBOL or FORTRAN. All three architectures were considered mainframe class machines:

In September 1986, Burroughs Corporation merged with Sperry Corporation to form Unisys. For a time, the combined company retained the Burroughs processors as the A- and V-systems lines. However, as the market for large systems shifted from proprietary architectures to common servers, the company eventually dropped the V-Series line, although customers continued to use V-series systems . Unisys continues to develop and market the A-Series, now known as ClearPath.

In 2010, UNISYS sold off its Payment Systems Division to Marlin Equity Partners, a California-based private investment firm, which incorporated it as Burroughs Payment Systems based in Plymouth, Michigan.

Burroughs B205 hardware has appeared as props in many Hollywood television and film productions from the late 1950s. For example, a B205 console was often shown in the television series "Batman" as the "Bat Computer"; also as the computer in "Lost in Space". B205 tape drives were often seen in series such as "The Time Tunnel" and "Voyage to the Bottom of the Sea". Craig Ferguson, American talk show host, comedian and actor was a Burroughs apprentice in Cumbernauld, Scotland.




</doc>
<doc id="4526" url="https://en.wikipedia.org/wiki?curid=4526" title="Brick">
Brick

A brick is building material used to make walls, pavements and other elements in masonry construction. Traditionally, the term brick referred to a unit composed of clay, but it is now used to denote any rectangular units laid in mortar. A brick can be composed of clay-bearing soil, sand, and lime, or concrete materials. Bricks are produced in numerous classes, types, materials, and sizes which vary with region and time period, and are produced in bulk quantities. Two basic categories of bricks are "fired" and "non-fired" bricks.

Block is a similar term referring to a rectangular building unit composed of similar materials, but is usually larger than a brick. Lightweight bricks (also called lightweight blocks) are made from expanded clay aggregate.

Fired bricks are one of the longest-lasting and strongest building materials, sometimes referred to as artificial stone, and have been used since circa 4000 BC. Air-dried bricks, also known as mudbricks, have a history older than fired bricks, and have an additional ingredient of a mechanical binder such as straw.

Bricks are laid in "courses" and numerous patterns known as "bonds", collectively known as brickwork, and may be laid in various kinds of mortar to hold the bricks together to make a durable structure.

The earliest bricks were "dried brick", meaning that they were formed from clay-bearing earth or mud and dried (usually in the sun) until they were strong enough for use. The oldest discovered bricks, originally made from shaped mud and dating before 7500 BC, were found at Tell Aswad, in the upper Tigris region and in southeast Anatolia close to Diyarbakir. Other more recent findings, dated between 7,000 and 6,395 BC, come from Jericho, Catal Hüyük, the ancient Egyptian fortress of Buhen, and the ancient Indus Valley cities of Mohenjo-daro, Harappa, and Mehrgarh. Ceramic, or "fired brick" was used as early as 3000 BC in early Indus Valley cities like Kalibangan.

The earliest fired bricks appeared in Neolithic China around 4400 BC at Chengtoushan, a walled settlement of the Daxi culture. These bricks were made of red clay, fired on all sides to above 600°C, and used as flooring for houses. By the Qujialing period (3300 BC), fired bricks were being used to pave roads and as building foundations at Chengtoushan.

Bricks continued to be used during 2nd millennium BC at a site near Xi'an. Fired bricks were found in Western Zhou (1046–771 BC) ruins, where they were produced on a large scale. The carpenter's manual "Yingzao Fashi", published in 1103 at the time of the Song dynasty described the brick making process and glazing techniques then in use. Using the 17th century encyclopaedic text "Tiangong Kaiwu", historian Timothy Brook outlined the brick production process of Ming Dynasty China: 

Early civilisations around the Mediterranean adopted the use of fired bricks, including the Ancient Greeks and Romans. The Roman legions operated mobile kilns, and built large brick structures throughout the Roman Empire, stamping the bricks with the seal of the legion.

During the Early Middle Ages the use of bricks in construction became popular in Northern Europe, after being introduced there from Northern-Western Italy. An independent style of brick architecture, known as brick Gothic (similar to Gothic architecture) flourished in places that lacked indigenous sources of rocks. Examples of this architectural style can be found in modern-day Denmark, Germany, Poland, and Russia.

This style evolved into Brick Renaissance as the stylistic changes associated with the Italian Renaissance spread to northern Europe, leading to the adoption of Renaissance elements into brick building. A clear distinction between the two styles only developed at the transition to Baroque architecture. In Lübeck, for example, Brick Renaissance is clearly recognisable in buildings equipped with terracotta reliefs by the artist Statius von Düren, who was also active at Schwerin (Schwerin Castle) and Wismar (Fürstenhof).

Long-distance bulk transport of bricks and other construction equipment remained prohibitively expensive until the development of modern transportation infrastructure, with the construction of canal, roads, and railways.

Production of bricks increased massively with the onset of the Industrial Revolution and the rise in factory building in England. For reasons of speed and economy, bricks were increasingly preferred as building material to stone, even in areas where the stone was readily available. It was at this time in London that bright red brick was chosen for construction to make the buildings more visible in the heavy fog and to help prevent traffic accidents.

The transition from the traditional method of production known as hand-moulding to a mechanised form of mass-production slowly took place during the first half of the nineteenth century. Possibly the first successful brick-making machine was patented by Henry Clayton, employed at the Atlas Works in Middlesex, England, in 1855, and was capable of producing up to 25,000 bricks daily with minimal supervision. His mechanical apparatus soon achieved widespread attention after it was adopted for use by the South Eastern Railway Company for brick-making at their factory near Folkestone. The Bradley & Craven Ltd ‘Stiff-Plastic Brickmaking Machine’ was patented in 1853, apparently predating Clayton. Bradley & Craven went on to be a dominant manufacturer of brickmaking machinery. Predating both Clayton and Bradley & Craven Ltd. however was the brick making machine patented by Richard A. Ver Valen of Haverstraw, New York in 1852.

The demand for high office building construction at the turn of the 20th century led to a much greater use of cast and wrought iron, and later, steel and concrete. The use of brick for skyscraper construction severely limited the size of the building – the Monadnock Building, built in 1896 in Chicago, required exceptionally thick walls to maintain the structural integrity of its 17 storeys.

Following pioneering work in the 1950s at the Swiss Federal Institute of Technology and the Building Research Establishment in Watford, UK, the use of improved masonry for the construction of tall structures up to 18 storeys high was made viable. However, the use of brick has largely remained restricted to small to medium-sized buildings, as steel and concrete remain superior materials for high-rise construction.

There are thousands of types of bricks that are named for their use, size, forming method, origin, quality, texture, and/or materials.

Categorized by manufacture method:

Categorized by use:

Specialized use bricks:

Bricks named for place of origin:

Three basic types of brick are un-fired, fired, and chemically set bricks. Each type is manufactured differently.

Unfired bricks, also known as mudbricks, are made from a wet, clay-containing soil mixed with straw or similar binders. They are air-dried until ready for use.

Fired bricks are burned in a kiln which makes them durable. Modern, fired, clay bricks are formed in one of three processes – soft mud, dry press, or extruded. Depending on the country, either the extruded or soft mud method is the most common, since they are the most economical.

Normally, bricks contain the following ingredients:


Three main methods are used for shaping the raw materials into bricks to be fired:


In many modern brickworks, bricks are usually fired in a continuously fired tunnel kiln, in which the bricks are fired as they move slowly through the kiln on conveyors, rails, or kiln cars, which achieves a more consistent brick product. The bricks often have lime, ash, and organic matter added, which accelerates the burning process.

The other major kiln type is the Bull's Trench Kiln (BTK), based on a design developed by British engineer W. Bull in the late 19th century.

An oval or circular trench is dug, 6–9 metres wide, 2-2.5 metres deep, and 100–150 metres in circumference. A tall exhaust chimney is constructed in the centre. Half or more of the trench is filled with "green" (unfired) bricks which are stacked in an open lattice pattern to allow airflow. The lattice is capped with a roofing layer of finished brick.

In operation, new green bricks, along with roofing bricks, are stacked at one end of the brick pile; cooled finished bricks are removed from the other end for transport to their destinations. In the middle, the brick workers create a firing zone by dropping fuel (coal, wood, oil, debris, and so on) through access holes in the roof above the trench.

The advantage of the BTK design is a much greater energy efficiency compared with clamp or scove kilns. Sheet metal or boards are used to route the airflow through the brick lattice so that fresh air flows first through the recently burned bricks, heating the air, then through the active burning zone. The air continues through the green brick zone (pre-heating and drying the bricks), and finally out the chimney, where the rising gases create suction that pulls air through the system. The reuse of heated air yields savings in fuel cost.

As with the rail process, the BTK process is continuous. A half-dozen labourers working around the clock can fire approximately 15,000–25,000 bricks a day. Unlike the rail process, in the BTK process the bricks do not move. Instead, the locations at which the bricks are loaded, fired, and unloaded gradually rotate through the trench.

The fired colour of tired clay bricks is influenced by the chemical and mineral content of the raw materials, the firing temperature, and the atmosphere in the kiln. For example, pink bricks are the result of a high iron content, white or yellow bricks have a higher lime content. Most bricks burn to various red hues; as the temperature is increased the colour moves through dark red, purple, and then to brown or grey at around . The names of bricks may reflect their origin and colour, such as London stock brick and Cambridgeshire White. "Brick tinting" may be performed to change the colour of bricks to blend-in areas of brickwork with the surrounding masonry.

An impervious and ornamental surface may be laid on brick either by salt glazing, in which salt is added during the burning process, or by the use of a slip, which is a glaze material into which the bricks are dipped. Subsequent reheating in the kiln fuses the slip into a glazed surface integral with the brick base.

Chemically set bricks are not fired but may have the curing process accelerated by the application of heat and pressure in an autoclave.

Calcium-silicate bricks are also called sandlime or flintlime bricks, depending on their ingredients. Rather than being made with clay they are made with lime binding the silicate material. The raw materials for calcium-silicate bricks include lime mixed in a proportion of about 1 to 10 with sand, quartz, crushed flint, or crushed siliceous rock together with mineral colourants. The materials are mixed and left until the lime is completely hydrated; the mixture is then pressed into moulds and cured in an autoclave for three to fourteen hours to speed the chemical hardening. The finished bricks are very accurate and uniform, although the sharp arrises need careful handling to avoid damage to brick and bricklayer. The bricks can be made in a variety of colours; white, black, buff, and grey-blues are common, and pastel shades can be achieved. This type of brick is common in Sweden, especially in houses built or renovated in the 1970s. In India these are known as fly ash bricks, manufactured using the FaL-G (fly ash, lime, and gypsum) process. Calcium-silicate bricks are also manufactured in Canada and the United States, and meet the criteria set forth in ASTM C73 – 10 Standard Specification for Calcium Silicate Brick (Sand-Lime Brick).

Bricks formed from concrete are usually termed as blocks, and are typically pale grey. They are made from a dry, small aggregate concrete which is formed in steel moulds by vibration and compaction in either an "egglayer" or static machine. The finished blocks are cured, rather than fired, using low-pressure steam. Concrete blocks are manufactured in a much wider range of shapes and sizes than clay bricks and are also available with a wider range of face treatments – a number of which simulate the appearance of clay bricks.

Concrete bricks are available in many colours and as an engineering brick made with sulfate-resisting Portland cement or equivalent. When made with adequate amount of cement they are suitable for harsh environments such as wet conditions and retaining walls. They are made to standards BS 6073, EN 771-3 or ASTM C55. Concrete bricks contract or shrink so they need movement joints every 5 to 6 metres, but are similar to other bricks of similar density in thermal and sound resistance and fire resistance.

Compressed earth blocks are made mostly from slightly moistened local soils compressed with a mechanical hydraulic press or manual lever press. A small amount of a cement binder may be added, resulting in a "stabilised compressed earth block".

For efficient handling and laying, bricks must be small enough and light enough to be picked up by the bricklayer using one hand (leaving the other hand free for the trowel). Bricks are usually laid flat, and as a result, the effective limit on the width of a brick is set by the distance which can conveniently be spanned between the thumb and fingers of one hand, normally about four inches (about 100 mm). In most cases, the length of a brick is about twice its width, about eight inches (about 200 mm) or slightly more. This allows bricks to be laid "bonded" in a structure which increases stability and strength (for an example, see the illustration of bricks laid in "English bond", at the head of this article). The wall is built using alternating courses of "stretchers", bricks laid longways, and "headers", bricks laid crossways. The headers tie the wall together over its width. In fact, this wall is built in a variation of "English bond" called "English cross bond" where the successive layers of stretchers are displaced horizontally from each other by half a brick length. In true "English bond", the perpendicular lines of the stretcher courses are in line with each other.

A bigger brick makes for a thicker (and thus more insulating) wall. Historically, this meant that bigger bricks were necessary in colder climates (see for instance the slightly larger size of the Russian brick in table below), while a smaller brick was adequate, and more economical, in warmer regions. A notable illustration of this correlation is the Green Gate in Gdansk; built in 1571 of imported Dutch brick, too small for the colder climate of Gdansk, it was notorious for being a chilly and drafty residence. Nowadays this is no longer an issue, as modern walls typically incorporate specialised insulation materials.

The correct brick for a job can be selected from a choice of colour, surface texture, density, weight, absorption, and pore structure, thermal characteristics, thermal and moisture movement, and fire resistance.

In England, the length and width of the common brick has remained fairly constant over the centuries (but see brick tax), but the depth has varied from about two inches (about 51 mm) or smaller in earlier times to about two and a half inches (about 64 mm) more recently. In the United Kingdom, the usual size of a modern brick is 215 × 102.5 × 65 mm (about × ×  inches), which, with a nominal 10 mm ( inch) mortar joint, forms a "unit size" of 225 × 112.5 × 75 mm (9 × × 3 inches), for a ratio of 6:3:2.

In the United States, modern standard bricks are specified for various uses; most are sized at about 8 ×   ×  inches (203 × 92 × 57 mm). The more commonly used is the modular brick   ×   ×  inches (194 × 92 × 57 mm). This modular brick of with a mortar joint eases the calculation of the number of bricks in a given wall.

Some brickmakers create innovative sizes and shapes for bricks used for plastering (and therefore not visible on the inside of the building) where their inherent mechanical properties are more important than their visual ones. These bricks are usually slightly larger, but not as large as blocks and offer the following advantages:

Blocks have a much greater range of sizes. Standard co-ordinating sizes in length and height (in mm) include 400×200, 450×150, 450×200, 450×225, 450×300, 600×150, 600×200, and 600×225; depths (work size, mm) include 60, 75, 90, 100, 115, 140, 150, 190, 200, 225, and 250. They are usable across this range as they are lighter than clay bricks. The density of solid clay bricks is around 2000 kg/m³: this is reduced by frogging, hollow bricks, and so on, but aerated autoclaved concrete, even as a solid brick, can have densities in the range of 450–850 kg/m³.

Bricks may also be classified as "solid" (less than 25% perforations by volume, although the brick may be "frogged," having indentations on one of the longer faces), "perforated" (containing a pattern of small holes through the brick, removing no more than 25% of the volume), "cellular" (containing a pattern of holes removing more than 20% of the volume, but closed on one face), or "hollow" (containing a pattern of large holes removing more than 25% of the brick's volume). Blocks may be solid, cellular or hollow

The term "frog" can refer to the indentation or the implement used to make it. Modern brickmakers usually use plastic frogs but in the past they were made of wood.

The compressive strength of bricks produced in the United States ranges from about 1000 lbf/in² to 15,000 lbf/in² (7 to 105 MPa or N/mm² ), varying according to the use to which the brick are to be put. In England clay bricks can have strengths of up to 100 MPa, although a common house brick is likely to show a range of 20–40 MPa.

In the United States, bricks have been used for both buildings and pavements. Examples of brick use in buildings can be seen in colonial era buildings and other notable structures around the country. Bricks have been used in pavements especially during the late 19th century and early 20th century. The introduction of asphalt and concrete reduced the use of brick pavements, but it is used as a method of traffic calming or as a decorative surface in pedestrian precincts. For example, in the early 1900s, most of the streets in the city of Grand Rapids, Michigan, were paved with bricks. Today, there are only about 20 blocks of brick-paved streets remaining (totalling less than 0.5 percent of all the streets in the city limits). Much like in Grand Rapids, municipalities across the United States began replacing brick streets with inexpensive asphalt concrete by the mid-20th century.

Bricks in the metallurgy and glass industries are often used for lining furnaces, in particular refractory bricks such as silica, magnesia, chamotte and neutral (chromomagnesite) refractory bricks. This type of brick must have good thermal shock resistance, refractoriness under load, high melting point, and satisfactory porosity. There is a large refractory brick industry, especially in the United Kingdom, Japan, the United States, Belgium and the Netherlands.

In Northwest Europe, bricks have been used in construction for centuries. Until recently, almost all houses were built almost entirely from bricks. Although many houses are now built using a mixture of concrete blocks and other materials, many houses are skinned with a layer of bricks on the outside for aesthetic appeal.

Engineering bricks are used where strength, low water porosity or acid (flue gas) resistance are needed.

In the UK a red brick university is one founded in the late 19th or early 20th century. The term is used to refer to such institutions collectively to distinguish them from the older Oxbridge institutions, and refers to the use of bricks, as opposed to stone, in their buildings.

Colombian architect Rogelio Salmona was noted for his extensive use of red bricks in his buildings and for using natural shapes like spirals, radial geometry and curves in his designs. Most buildings in Colombia are made of brick, given the abundance of clay in equatorial countries like this one.

Starting in the 20th century, the use of brickwork declined in some areas due to concerns with earthquakes. Earthquakes such as the San Francisco earthquake of 1906 and the 1933 Long Beach earthquake revealed the weaknesses of unreinforced brick masonry in earthquake-prone areas. During seismic events, the mortar cracks and crumbles, and the bricks are no longer held together. Brick masonry with steel reinforcement, which helps hold the masonry together during earthquakes, was used to replace many of the unreinforced masonry buildings. Retrofitting older unreinforced masonry structures has been mandated in many jurisdictions.




</doc>
<doc id="4527" url="https://en.wikipedia.org/wiki?curid=4527" title="Béla Bartók">
Béla Bartók

Béla Viktor János Bartók (; ; 25 March 1881 – 26 September 1945) was a Hungarian composer, pianist and an ethnomusicologist. He is considered one of the most important composers of the 20th century; he and Liszt are regarded as Hungary's greatest composers . Through his collection and analytical study of folk music, he was one of the founders of comparative musicology, which later became ethnomusicology. 

Bartók was born in the Banatian town of Nagyszentmiklós in the Kingdom of Hungary, Austria-Hungary (since 1920 Sânnicolau Mare, Romania) on 25 March 1881. Bartók had a diverse ancestry. On his father's side, the Bartók family was a Hungarian lower noble family, originating from Borsodszirák, Borsod . 

Although his paternal grandmother was from a Catholic Serbian family, Bartók's father, also named Béla, considered himself to be an ethnic-born Hungarian. Béla Bartók's mother, Paula (née Voit), an ethnic German, spoke Hungarian fluently . She was a native of Turčiansky Svätý Martin (now Martin, Slovakia). Paula also had Magyar (Teréz Fegyveres) and Slavic (Polereczky: Magyarized Slavic) ancestors.

Béla displayed notable musical talent very early in life: according to his mother, he could distinguish between different dance rhythms that she played on the piano before he learned to speak in complete sentences . By the age of four he was able to play 40 pieces on the piano and his mother began formally teaching him the next year.

Béla was a small and sickly child and suffered from severe eczema until the age of five. In 1888, when he was seven, his father (the director of an agricultural school) died suddenly. His mother then took him and his sister, Erzsébet, to live in Nagyszőlős (today Vinogradiv, Ukraine) and then to Pozsony ￼￼￼ (present-day Bratislava, Slovakia).

He gave his first public recital aged 11 in Nagyszőlős, to a warm critical reception . Among the pieces he played was his own first composition, written two years previously: a short piece called "The Course of the Danube" . Shortly thereafter László Erkel accepted him as a pupil .

From 1899 to 1903, Bartók studied piano under István Thomán, a former student of Franz Liszt, and composition under János Koessler at the Royal Academy of Music in Budapest. There he met Zoltán Kodály, who made a strong impression on him and became a lifelong friend and colleague. In 1903, Bartók wrote his first major orchestral work, "Kossuth", a symphonic poem which honored Lajos Kossuth, hero of the Hungarian Revolution of 1848.

The music of Richard Strauss, whom he met in 1902 at the Budapest premiere of "Also sprach Zarathustra," strongly influenced his early work. When visiting a holiday resort in the summer of 1904, Bartók overheard a young nanny, Lidi Dósa from Kibéd in Transylvania, sing folk songs to the children in her care. This sparked his lifelong dedication to folk music.

From 1907, he also began to be influenced by the French composer Claude Debussy, whose compositions Kodály had brought back from Paris. Bartók's large-scale orchestral works were still in the style of Johannes Brahms and Richard Strauss, but he wrote a number of small piano pieces which showed his growing interest in folk music. The first piece to show clear signs of this new interest is the String Quartet No. 1 in A minor (1908), which contains folk-like elements.

In 1907, Bartók began teaching as a piano professor at the Royal Academy. This position freed him from touring Europe as a pianist and enabled him to work in Hungary. Among his notable students were Fritz Reiner, Sir Georg Solti, György Sándor, Ernő Balogh, and Lili Kraus. After Bartók moved to the United States, he taught Jack Beeson and Violet Archer.

In 1908, he and Kodály traveled into the countryside to collect and research old Magyar folk melodies. Their growing interest in folk music coincided with a contemporary social interest in traditional national culture. They made some surprising discoveries. Magyar folk music had previously been categorised as Gypsy music. The classic example is Franz Liszt's famous "Hungarian Rhapsodies" for piano, which he based on popular art songs performed by Romani bands of the time. In contrast, Bartók and Kodály discovered that the old Magyar folk melodies were based on pentatonic scales, similar to those in Asian folk traditions, such as those of Central Asia, Anatolia and Siberia.

Bartók and Kodály quickly set about incorporating elements of such Magyar peasant music into their compositions. They both frequently quoted folk song melodies "verbatim" and wrote pieces derived entirely from authentic songs. An example is his two volumes entitled "For Children" for solo piano, containing 80 folk tunes to which he wrote accompaniment. Bartók's style in his art music compositions was a synthesis of folk music, classicism, and modernism. His melodic and harmonic sense was profoundly influenced by the folk music of Hungary, Romania, and other nations. He was especially fond of the asymmetrical dance rhythms and pungent harmonies found in Bulgarian music. Most of his early compositions offer a blend of nationalist and late Romanticism elements.

In 1909, at the age of 28, Bartók married Márta Ziegler (1893–1967), aged 16. Their son, Béla Bartók III, was born on 22 August 1910. After nearly 15 years together, Bartók divorced Márta in June 1923. Two months after his divorce, he married Ditta Pásztory (1903–1982), a piano student, ten days after proposing to her. She was aged 19, he 42. Their son, Péter, was born in 1924 .

In 1911, Bartók wrote what was to be his only opera, "Bluebeard's Castle", dedicated to Márta. He entered it for a prize by the Hungarian Fine Arts Commission, but they rejected his work as not fit for the stage . In 1917 Bartók revised the score for the 1918 première, and rewrote the ending. Following the 1919 revolution in which he actively participated, he was pressured by the Horthy regime to remove the name of the librettist Béla Balázs from the opera , as he was blacklisted and had left the country for Vienna. "Bluebeard's Castle" received only one revival, in 1936, before Bartók emigrated. For the remainder of his life, although he was passionately devoted to Hungary, its people and its culture, he never felt much loyalty to the government or its official establishments.

After his disappointment over the Fine Arts Commission competition, Bartók wrote little for two or three years, preferring to concentrate on collecting and arranging folk music. He collected first in the Carpathian Basin (then the Kingdom of Hungary), where he notated Hungarian, Slovak, Romanian, and Bulgarian folk music. He also collected in Moldavia, Wallachia, and (in 1913) Algeria. The outbreak of World War I forced him to stop the expeditions; and he returned to composing, writing the ballet "The Wooden Prince" (1914–16) and the String Quartet No. 2 in (1915–17), both influenced by Debussy.

Raised as a Catholic, by his early adulthood Bartók had become an atheist. He later became attracted to Unitarianism and publicly converted to the Unitarian faith in 1916. Although Bartók was not conventionally religious, according to his son Béla Bartók III, "he was a nature lover: he always mentioned the miraculous order of nature with great reverence." As an adult, Béla III later became lay president of the Hungarian Unitarian Church .

Bartók wrote another ballet, "The Miraculous Mandarin", influenced by Igor Stravinsky, Arnold Schoenberg, as well as Richard Strauss. A modern story of prostitution, robbery, and murder, it was started in 1918, but not performed until 1926 because of its sexual content. He next wrote his two violin sonatas (written in 1921 and 1922 respectively), which are harmonically and structurally some of his most complex pieces.

In 1927–28, Bartók wrote his Third and Fourth String Quartets, after which his compositions demonstrated his mature style. Notable examples of this period are "Music for Strings, Percussion and Celesta" (1936) and Divertimento for String Orchestra (1939). The Fifth String Quartet was composed in 1934, and the Sixth String Quartet (his last) in 1939.

In 1936 he travelled to Turkey to collect and study folk music. He worked in collaboration with Turkish composer Ahmet Adnan Saygun mostly around Adana (; ).

In 1940, as the European political situation worsened after the outbreak of World War II, Bartók was increasingly tempted to flee Hungary. He was strongly opposed to the Nazis and Hungary's siding with Germany. After the Nazis came to power in the early 1930s, Bartók refused to give concerts in Germany and broke away from his publisher there. His anti-fascist political views caused him a great deal of trouble with the establishment in Hungary. Having first sent his manuscripts out of the country, Bartók reluctantly emigrated to the U.S. with his wife Ditta in October that year. They settled in New York City after arriving on the night of October 29–30, 1940 via a steamer from Lisbon. After joining them in 1942, their son, Péter Bartók, enlisted in the United States Navy where he served in the Pacific during the remainder of the war and later settled in Florida where he became a recording and sound engineer. His oldest son, Béla Bartók III, remained in Hungary where he survived the war and later worked as a railroad official until his retirement in the early 1980s.

Although he became an American citizen in 1945, shortly before his death , Bartók never felt fully at home in the USA. He initially found it difficult to compose. Although well known in America as a pianist, ethnomusicologist and teacher, he was not well known as a composer. There was little American interest in his music during his final years. He and his wife Ditta gave some concerts, although demand for them was low. Bartók, who had made some recordings in Hungary, also recorded for Columbia Records after he came to the US; many of these recordings (some with Bartók's own spoken introductions) were later issued on LP and CD .

Supported by a research fellowship from Columbia University, for several years, Bartók and Ditta worked on a large collection of Serbian and Croatian folk songs in Columbia's libraries. Bartók's economic difficulties during his first years in America were mitigated by publication royalties, teaching and performance tours. While his finances were always precarious, he did not live and die in poverty as was the common myth. He had enough friends and supporters to ensure that there was sufficient money and work available for him to live on. Bartók was a proud man and did not easily accept charity. Despite being short on cash at times, he often refused money that his friends offered him out of their own pockets. Although he was not a member of the ASCAP, the society paid for any medical care he needed during his last two years. Bartók reluctantly accepted this .

The first symptoms of his health problems began late in 1940, when his right shoulder began to show signs of stiffening. In 1942, symptoms increased and he started having bouts of fever, but no underlying disease was diagnosed, in spite of medical examinations. Finally, in April 1944, leukemia was diagnosed, but by this time, little could be done .

As his body slowly failed, Bartók found more creative energy, and he produced a final set of masterpieces, partly thanks to the violinist Joseph Szigeti and the conductor Fritz Reiner (Reiner had been Bartók's friend and champion since his days as Bartók's student at the Royal Academy). Bartók's last work might well have been the String Quartet No. 6 but for Serge Koussevitzky's commission for the Concerto for Orchestra. Koussevitsky's Boston Symphony Orchestra premièred the work in December 1944 to highly positive reviews. The Concerto for Orchestra quickly became Bartók's most popular work, although he did not live to see its full impact. In 1944, he was also commissioned by Yehudi Menuhin to write a Sonata for Solo Violin. In 1945, Bartók composed his Piano Concerto No. 3, a graceful and almost neo-classical work, as a surprise 42nd birthday present for Ditta, but he died just over a month before her birthday, with the scoring not quite finished. He had also sketched his Viola Concerto, but had barely started the scoring at his death, leaving completed only the viola part and sketches of the orchestral part.
Béla Bartók died at age 64 in a hospital in New York City from complications of leukemia (specifically, of secondary polycythemia) on 26 September 1945. His funeral was attended by only ten people. Among them were his wife Ditta, their son Péter, and his pianist friend György Sándor .

Bartók's body was initially interred in Ferncliff Cemetery in Hartsdale, New York. During the final year of communist Hungary in the late 1980s, the Hungarian government, along with his two sons, Béla III and Péter, requested that his remains be exhumed and transferred back to Budapest for burial, where Hungary arranged a state funeral for him on 7 July 1988. He was reinterred at Budapest's Farkasréti Cemetery, next to the remains of Ditta, who died in 1982, the year after his centenary .

The two unfinished works were later completed by his pupil Tibor Serly. György Sándor was the soloist in the first performance of the Third Piano Concerto on February 8, 1946. Ditta Pásztory-Bartók later played and recorded it. The Viola Concerto was revised and published in the 1990s by Bartók's son, Peter ; this version may be closer to what Bartók intended .

Concurrently, Peter Bartók, in association with Argentinian musician Nelson Dellamaggiore, worked to reprint and revise past editions of the Third Piano Concerto .


Bartók's music reflects two trends that dramatically changed the sound of music in the 20th century: the breakdown of the diatonic system of harmony that had served composers for the previous two hundred years ; and the revival of nationalism as a source for musical inspiration, a trend that began with Mikhail Glinka and Antonín Dvořák in the last half of the 19th century . In his search for new forms of tonality, Bartók turned to Hungarian folk music, as well as to other folk music of the Carpathian Basin and even of Algeria and Turkey; in so doing he became influential in that stream of modernism which exploited indigenous music and techniques .

One characteristic style of music is his Night music, which he used mostly in slow movements of multi-movement ensemble or orchestral compositions in his mature period. It is characterised by "eerie dissonances providing a backdrop to sounds of nature and lonely melodies" . An example is the third movement (Adagio) of his "Music for Strings, Percussion and Celesta".

His music can be grouped roughly in accordance with the different periods in his life.

The works of his youth are of a late-Romantic style. Between 1890 and 1894 (nine to 13 years of age) he wrote 31 piano pieces with corresponding opus numbers. He started numbering his works anew with "opus 1" in 1894 with his first large-scale work, a piano sonata in G minor . Up to 1902, Bartók wrote in total 74 works which can be considered in Romantic style. Most of these early compositions are either scored for piano solo or include a piano. Additionally, there is some chamber music for strings.

Under the influence of Richard Strauss, especially "Also sprach Zarathustra" , Bartók composed in 1903 "Kossuth", a symphonic poem in ten tableaux. In 1904 followed his "Rhapsody for piano and orchestra" which he numbered opus 1 again, marking it himself as the start of a new era in his music. An even more important occurrence of this year was his overhearing the eighteen-year-old nanny Lidi Dósa from Transylvania sing folk songs, sparking Bartók's lifelong dedication to folk music . When criticised for not composing his own melodies Bartók pointed out that Molière and Shakespeare mostly based their plays on well-known stories too. Regarding the incorporation of folk music into art music he said:
The question is, what are the ways in which peasant music is taken over and becomes transmuted into modern music? We may, for instance, take over a peasant melody unchanged or only slightly varied, write an accompaniment to it and possibly some opening and concluding phrases. This kind of work would show a certain analogy with Bach's treatment of chorales. ... Another method ... is the following: the composer does not make use of a real peasant melody but invents his own imitation of such melodies. There is no true difference between this method and the one described above. ... There is yet a third way ... Neither peasant melodies nor imitations of peasant melodies can be found in his music, but it is pervaded by the atmosphere of peasant music. In this case we may say, he has completely absorbed the idiom of peasant music which has become his musical mother tongue. 

Bartók became first acquainted with Debussy's music in 1907 and regarded his music highly. In an interview in 1939 Bartók said

Debussy's great service to music was to reawaken among all musicians an awareness of harmony and its possibilities. In that, he was just as important as Beethoven, who revealed to us the possibilities of progressive form, or as Bach, who showed us the transcendent significance of counterpoint. Now, what I am always asking myself is this: is it possible to make a synthesis of these three great masters, a living synthesis that will be valid for our time? Debussy's influence is present in the Fourteen Bagatelles (1908). These made Ferruccio Busoni exclaim 'At last something truly new!' . Until 1911, Bartók composed widely differing works which ranged from adherence to romantic-style, to folk song arrangements and to his modernist opera "Bluebeard's Castle". The negative reception of his work led him to focus on folk music research after 1911 and abandon composition with the exception of folk music arrangements (; ).

His pessimistic attitude towards composing was lifted by the stormy and inspiring contact with Klára Gombossy in the summer of 1915 . This interesting episode in Bartók's life remained hidden until it was researched by Denijs Dille between 1979 and 1989 . Bartók started composing again, including the Suite for piano opus 14 (1916), and "The Miraculous Mandarin" (1918) and he completed "The Wooden Prince" (1917).

Bartók felt the result of World War I as a personal tragedy . Many regions he loved were severed from Hungary: Transylvania, the Banat where he was born, and Pozsony where his mother lived. Additionally, the political relations between Hungary and the other successor states to the Austro-Hungarian empire prohibited his folk music research outside of Hungary . Bartók also wrote the noteworthy "Eight Improvisations on Hungarian Peasant Songs" in 1920, and the sunny "Dance Suite" in 1923, the year of his second marriage.

In 1926, Bartók needed a significant piece for piano and orchestra with which he could tour in Europe and America. In the preparation for writing his First Piano Concerto, he wrote his Sonata, "Out of Doors", and "Nine Little Pieces", all for solo piano . He increasingly found his own voice in his maturity. The style of his last period—named "Synthesis of East and West" —is hard to define let alone to put under one term. In his mature period, Bartók wrote relatively few works but most of them are large-scale compositions for large settings. Only his voice works have programmatic titles and his late works often adhere to classical forms.

Among his masterworks are all the six string quartets (1908, 1917, 1927, 1928, 1934, and 1939), the "Cantata Profana" (1930, Bartók declared that this was the work he felt and professed to be his most personal "credo" , the "Music for Strings, Percussion and Celesta" (1936), the Concerto for Orchestra (1943) and the Third Piano Concerto (1945).

Bartók also made a lasting contribution to the literature for younger students: for his son Péter's music lessons, he composed "Mikrokosmos", a six-volume collection of graded piano pieces.

Paul Wilson lists as the most prominent characteristics of Bartók's music from late 1920s onwards the influence of the Carpathian basin and European art music, and his changing attitude toward (and use of) tonality, but without the use of the traditional harmonic functions associated with major and minor scales .

Although Bartók claimed in his writings that his music was always tonal, he rarely uses the chords or scales of tonality, and so the descriptive resources of tonal theory are of limited use. George and Elliott focus on alternative methods of signaling tonal centers, via axes of inversional symmetry. Others view Bartók's axes of symmetry in terms of atonal analytic protocols. Richard argues that inversional symmetry is often a byproduct of another atonal procedure, the formation of chords from transpositionally related dyads. Atonal pitch-class theory also furnishes the resources for exploring polymodal chromaticism, projected sets, privileged patterns, and large set types used as source sets such as the equal tempered twelve tone aggregate, octatonic scale (and alpha chord), the diatonic and "heptatonia secunda" seven-note scales, and less often the whole tone scale and the primary pentatonic collection .

He rarely used the simple aggregate actively to shape musical structure, though there are notable examples such as the second theme from the first movement of his Second Violin Concerto, commenting that he "wanted to show Schoenberg that one can use all twelve tones and still remain tonal" . More thoroughly, in the first eight measures of the last movement of his Second Quartet, all notes gradually gather with the twelfth (G) sounding for the first time on the last beat of measure 8, marking the end of the first section. The aggregate is partitioned in the opening of the Third String Quartet with C–D–D–E in the accompaniment (strings) while the remaining pitch classes are used in the melody (violin 1) and more often as 7–35 (diatonic or "white-key" collection) and 5–35 (pentatonic or "black-key" collection) such as in no. 6 of the "Eight Improvisations". There, the primary theme is on the black keys in the left hand, while the right accompanies with triads from the white keys. In measures 50–51 in the third movement of the Fourth Quartet, the first violin and cello play black-key chords, while the second violin and viola play stepwise diatonic lines . On the other hand, from as early as the Suite for piano, Op. 14 (1914), he occasionally employed a form of serialism based on compound interval cycles, some of which are maximally distributed, multi-aggregate cycles (; ).
Ernő Lendvai (1971) analyses Bartók's works as being based on two opposing tonal systems, that of the acoustic scale and the axis system, as well as using the golden section as a structural principle.

Milton Babbitt, in his 1949 critique of Bartók's string quartets, criticized Bartók for using tonality and non tonal methods unique to each piece. Babbitt noted that "Bartók's solution was a specific one, it cannot be duplicated" . Bartók's use of "two organizational principles"—tonality for large scale relationships and the piece-specific method for moment to moment thematic elements—was a problem for Babbitt, who worried that the "highly attenuated tonality" requires extreme non-harmonic methods to create a feeling of closure .

The cataloguing of Bartók's works is somewhat complex. Bartók assigned opus numbers to his works three times, the last of these series ending with the Sonata for Violin and Piano No. 1, Op. 21 in 1921. He ended this practice because of the difficulty of distinguishing between original works and ethnographic arrangements, and between major and minor works. Since his death, three attempts—two full and one partial—have been made at cataloguing. The first, and still most widely used, is András Szőllősy's chronological Sz. numbers, from 1 to 121. Denijs Dille subsequently reorganised the juvenilia (Sz. 1–25) thematically, as DD numbers 1 to 77. The most recent catalogue is that of László Somfai; this is a chronological index with works identified by BB numbers 1 to 129, incorporating corrections based on the Béla Bartók Thematic Catalogue.

On 1 January 2016, his works entered the public domain in the European Union.


On 18 March 2016 Decca Classics released "Béla Bartók: The Complete Works", the first ever complete compilation of all of Bartók's compositions, including new recordings of never-before-recorded early piano and vocal works. However, none of the composer's own performances are included in this 32-disc set .





</doc>
<doc id="4528" url="https://en.wikipedia.org/wiki?curid=4528" title="Bill Haley">
Bill Haley

William John Clifton Haley (; July 6, 1925 – February 9, 1981) was an American rock and roll musician. He is credited by many with first popularizing this form of music in the early 1950s with his group Bill Haley & His Comets and million-selling hits such as "Rock Around the Clock", "See You Later, Alligator", "Shake, Rattle and Roll", "Rocket 88", "Skinny Minnie", and "Razzle Dazzle". He has sold over 25 million records worldwide.

Bill Haley was born July 6, 1925 in Highland Park, Michigan, as William John Clifton Haley. In 1929, the four-year-old Haley underwent an inner-ear mastoid operation which accidentally severed an optic nerve, leaving him blind in his left eye for the rest of his life. It is said that he adopted his trademark kiss curl over his right eye to draw attention from his left, but it also became his "gimmick", and added to his popularity. As a result of the effects of the Great Depression on the Detroit area, his father moved the family to Bethel, Pennsylvania, when Bill was seven years old. Haley's father William Albert Haley was from Kentucky and played the banjo and mandolin, and his mother, Maude Green, who was originally from Ulverston in Lancashire, England, was a technically accomplished keyboardist with classical training. Haley told the story that when he made a simulated guitar out of cardboard, his parents bought him a real one.

One of his first appearances was in 1938 for a Bethel Junior baseball team entertainment event, performing guitar and songs when he was 13 years old.

The anonymous sleeve notes accompanying the 1956 Decca album "Rock Around The Clock" describe Haley's early life and career: "When Bill Haley was fifteen [c. 1940] he left home with his guitar and very little else and set out on the hard road to fame and fortune. The next few years, continuing this story in a fairy-tale manner, were hard and poverty-stricken, but crammed full of useful experience. Apart from learning how to exist on one meal a day and other artistic exercises, he worked at an open-air park show, sang and yodelled with any band that would have him, and worked with a traveling medicine show. Eventually he got a job with a popular group known as the "Down Homers" while they were in Hartford, Connecticut. Soon after this he decided, as all successful people must decide at some time or another, to be his own boss again – and he has been that ever since.' These notes fail to account for his early band, known as the Four Aces of Western Swing. During the 1940s Haley was considered one of the top cowboy yodelers in America as "Silver Yodeling Bill Haley".

The sleeve notes conclude: "For six years Bill Haley was a musical director of Radio Station WPWA in Chester, Pennsylvania, and led his own band all through this period. It was then known as Bill Haley's Saddlemen, indicating their definite leaning toward the tough Western style. They continued playing in clubs as well as over the radio around Philadelphia, and in 1951 made their first recordings on Ed Wilson's Keystone Records in Philadelphia." On June 14, 1951 the Saddlemen recorded a cover of "Rocket 88".

During the Labor Day weekend in 1952, the Saddlemen were renamed Bill Haley with Haley's Comets (inspired by the supposedly official pronunciation of Halley's Comet, a name suggested by WPWA radio station program director, Bob Johnson, where Bill Haley had a live radio program from noon to 1 pm), and in 1953, Haley's recording of "Crazy Man, Crazy" (co-written by him and his bass player, Marshall Lytle, although Lytle would not receive credit until 2001) became the first rock and roll song to hit the American charts, peaking at number 15 on "Billboard" and number 11 on "Cash Box". Soon after, the band's name was revised to Bill Haley & His Comets.

In 1953, "Rock Around the Clock" was recorded by Haley. Initially, it was relatively successful, peaking at number 23 on the "Billboard" pop singles chart and staying on the charts for a few weeks. A month later it re-entered at number one.

Haley soon had another worldwide hit with "Shake, Rattle and Roll", which went on to sell a million copies and was the first rock 'n' roll song to enter the British singles charts in December 1954, becoming a gold record. He retained elements of the original (which was slow blues), but sped it up with some country music aspects into the song (specifically, Western swing) and changed up the lyrics. Haley and his band were important in launching the music known as "Rock and Roll" to a wider audience after a period of it being considered an underground genre.

When "Rock Around the Clock" appeared as the theme song of the 1955 film "Blackboard Jungle" starring Glenn Ford, it soared to the top of the American "Billboard" chart for eight weeks. The single is commonly used as a convenient line of demarcation between the "rock era" and the music industry that preceded it. "Billboard" separated its statistical tabulations into 1890–1954 and 1955–present. After the record rose to number one, Haley was quickly given the title "Father of Rock and Roll" by the media, and by teenagers who had come to embrace the new style of music. With the song's success, the age of rock music began overnight and ended the dominance of the jazz and pop standards performed by Frank Sinatra, Jo Stafford, Perry Como, Bing Crosby, and others. Nevertheless, in the United Kingdom, Haley was supported by former Dankworth Seven lead vocalist Frank Holder among others.

"Rock Around the Clock" was the first record to sell over one million copies in both Britain and Germany. Later on in 1957, Haley became the first major American rock singer to tour Europe. Haley continued to score hits throughout the 1950s such as "See You Later, Alligator" and he starred in the first rock and roll musical films "Rock Around the Clock" and "Don't Knock the Rock", both in 1956. Haley was already 30 years old, so he was soon eclipsed in the United States by the younger, sexier Elvis Presley, but continued to enjoy great popularity in Latin America, Europe, and Australia during the 1960s.

Bill Haley and the Comets performed "Rock Around the Clock" on the "Texaco Star Theater" hosted by Milton Berle on Tuesday, May 31, 1955, on NBC in an" a cappella" and lip-synched version. Berle predicted that the song would go number one: "A group of entertainers who are going right to the top." Berle also sang and danced to the song which was performed by the entire cast of the show. This was one of the earliest nationally televised performances by a rock and roll band and provided the new musical genre with a much wider audience.

Bill Haley and the Comets were the first rock and roll act to appear on the iconic American musical variety series the "Ed Sullivan Show" on Sunday, August 7, 1955, on CBS in a broadcast that originated from the Shakespeare Festival Theater in Hartford, Connecticut. They performed a live version of "Rock Around the Clock" with Franny Beecher on lead guitar and Dick Richards on drums. The band made their second appearance on the show on Sunday, April 28, 1957, performing the songs "Rudy's Rock" and "Forty Cups of Coffee".

Bill Haley and the Comets appeared on "American Bandstand" hosted by Dick Clark on ABC twice in 1957, on the prime time show October 28, 1957, and on the regular daytime show on November 27, 1957. The band also appeared on Dick Clark's "Saturday Night Beechnut Show", also known as "The Dick Clark Show", a primetime TV series from New York on March 22, 1958, during the first season and on February 20, 1960, performing "Rock Around the Clock", "Shake, Rattle and Roll", and "Tamiami".

Bill Haley was married three times:

Bill Haley had at least ten children. John W. Haley, his eldest son, wrote "Sound and Glory", a biography of Haley, and had another child, Sharyn, with his first wife, Dorothy Crowe. His youngest daughter, Gina Haley, is a professional musician based in Texas. Scott Haley is an athlete. His youngest son Pedro is also a musician.

He also had a daughter, Martha Maria, from his last marriage with Martha Velasco.

Bill Haley Jr. (Haley's second son and first with Joan Barbara "Cuppy" Haley-Hahn) publishes a regional business magazine in Southeastern Pennsylvania ("Route 422 Business Advisor"). He sang and played guitar with a band called "Bill Haley and the Satellites," and released a CD in 2011.

An admitted alcoholic (as indicated in a 1974 radio interview for the BBC), Haley fought a battle with alcohol into the 1970s. Nonetheless, he and his band continued to be a popular touring act, benefiting from a 1950s nostalgia movement that began in the late 1960s and the signing of a lucrative record deal with the European Sonet label. After performing for Queen Elizabeth II at the Royal Variety Performance on November 10, 1979, Haley made his final performances in South Africa in May and June 1980. Before the South African tour, he was diagnosed with a brain tumor, and a planned tour of Germany in the autumn of 1980 was cancelled.

The October 25, 1980 issue of German tabloid "Bild" reported that Haley had a brain tumor, quoting British manager Patrick Malynn as saying that Haley "had taken a fit [and] didn't recognize anyone any more" after being taken to his home in Beverly Hills, in addition to the paper quoting a doctor that the tumor was inoperable. The "Berliner Zeitung" reported a few days later that Haley had collapsed after a performance in Texas and was taken to the hospital in his hometown of Harlingen, Texas. However, this account is questionable, as Bill Haley did not perform in the United States at all in 1980.

Despite his ill health, Haley began compiling notes for possible use as a basis for either a biographical film based on his life, or a published autobiography (accounts differ), and plans were made for him to record an album in Memphis, Tennessee, when the brain tumor began affecting his behavior and he went back to his home in Harlingen, where he died early in the morning hours of February 9, 1981.

Martha, Bill's widow, who was with him in these troubling times, denies he had a brain tumor as does his old, very close friend, Hugh McCallum. Martha and friends related that Bill did not want to go on the road any more and that ticket sales for that planned tour of Germany in the fall of 1980 were slow. According to McCallum, "It's my unproven gut feeling that that [the brain tumor] was said to curtail talks about the tour and play the sympathy card."

His drinking problem was getting worse. By this time, Bill and Martha fought all the time and she told him to stop drinking or move out. He then did move out into a room in their pool house. Martha still took care of him and sometimes, he would come in the house to eat, but he ate very little. "There were days we never saw him," said his daughter Martha Maria.

In addition to Haley's drinking problems, he was having serious mental problems, as well; Martha Maria said, "It was like sometimes he was drunk even when he wasn't drinking." After he'd been jailed by the Harlingen Police, Martha had the judge put Haley in the hospital, where he was seen by a psychiatrist, who said Bill's brain was overproducing a chemical, like adrenaline. The doctor prescribed a medication to stop the overproduction, but said Bill would have to stop drinking. Martha said, "This is pointless." She took him home, however, fed him and gave him his first dose. As soon as he felt better, he went back out to his room in the pool house, and the downward spiral continued until his passing.

Haley's death certificate listed "Natural causes: Most likely heart attack" as the 'Immediate Cause' of death. The next lines, 'Due to, or as a consequence of" were blank.

Haley made a succession of bizarre, mostly monologue late-night phone calls to friends and relatives in which he seemed incoherently drunk or ill. Haley's first wife has been quoted as saying, "He would call and ramble and dwell on the past, his mind was really warped." A belligerent phone call to a business associate was taped and gives evidence of Haley's troubled state of mind.

Media reports immediately following his death indicated Haley displayed deranged and erratic behavior in his final weeks, although little is known about Haley's final days beyond a biography by John Swenson, released a year later, which described him painting the windows of his home black. After a small funeral service, Haley was cremated, although his widow Martha would not say what was done with the ashes.

Haley was posthumously inducted into the Rock and Roll Hall of Fame in 1987. His son Pedro represented him at the ceremony. He received a star on the Hollywood Walk of Fame on February 8, 1960, for his contributions to the music industry at 6350 Hollywood Boulevard. The Comets were separately inducted into the Hall of Fame as a group in 2012, after a rule change allowed the induction of backing groups.

Songwriters Tom Russell and Dave Alvin addressed Haley's demise in musical terms with "Haley's Comet" on Alvin's 1991 album "Blue Blvd." Dwight Yoakam sang backup on the tribute.

Surviving members of the 1954–55 contingent of Haley's Comets reunited in the late 1980s and continued to perform for many years around the world. They released a concert DVD in 2004 on Hydra Records, played the Viper Room in West Hollywood in 2005, and performed at Dick Clark's American Bandstand Theater in Branson, Missouri, beginning in 2006–07. As of 2014, only two members of this particular contingent are still alive (Joey Ambrose and Dick Richards), but they continue to perform in Branson and in Europe. At least two other groups also continue to perform in North America under the Comets name as of 2014.

In March 2007, the Original Comets opened the Bill Haley Museum in Munich, Germany. On October 27, 2007, ex-Comets guitar player Bill Turner opened the Bill Haley Museum for the public.

Two of Haley's children, Bill Haley Jr. and Gina Haley, are themselves musicians and have in recent years recorded albums of their father's music and headlined tribute musical shows.

In February 2006, the International Astronomical Union announced the naming of asteroid 79896 Billhaley to mark the 25th anniversary of Bill Haley's death.


Unlike his contemporaries, Bill Haley has rarely been portrayed on screen. Following the success of "The Buddy Holly Story" in 1978, Haley expressed interest in having his life story committed to film, but this never came to fruition. In the 1980s and early 1990s, numerous media reports emerged stating that plans were underway to do a biopic based upon Haley's life, with Beau Bridges, Jeff Bridges and John Ritter all at one point being mentioned as actors in line to play Haley (according to "Goldmine Magazine", Ritter attempted to buy the film rights to "Sound and Glory").

Bill Haley has also been portrayed – not always in a positive light – in several "period" films:

In March 2005, the British network Sky TV reported that Tom Hanks was planning to produce a biopic on the life of Bill Haley, with production tentatively scheduled to begin in 2006. However, this rumor was quickly debunked by Hanks.

Before the formation of Bill Haley and the Saddlemen, which later became the Comets, Haley released several singles with other groups. Dates are approximate due to lack of documentation.

As Bill Haley and the Four Aces of Western Swing:

1948
1949

As Johnny Clifton and His String Band:

1950

Many Haley discographies list two 1946 recordings by the Down Homers released on the Vogue Records label as featuring Haley. Haley historian Chris Gardner, as well as surviving members of the group, have confirmed that the two singles: "Out Where the West Winds Blow"/"Who's Gonna Kiss You When I'm Gone" (Vogue R736) and "Boogie Woogie Yodel"/"Baby I Found Out All About You" (Vogue R786) do not feature Haley. However, the tracks were nonetheless included in the compilation box set "Rock 'n' Roll Arrives" released by Bear Family Records in 2006.

Bill Haley recorded prolifically during the 1940s, often at the radio stations where he worked, or in formal studio settings. Virtually none of these recordings were ever released. Liner notes for a 2003 CD release by Hydra Records entitled "Bill Haley and Friends Vol. 2: The Legendary Cowboy Recordings" reveal that several additional Cowboy label single releases were planned for the Four Aces, but this never occurred.

A number of previously unreleased Haley country-western recordings from the 1946–1950 period began to emerge near the end of Haley's life, some of which were released by the Arzee label, with titles such as "Yodel Your Blues Away" and "Rose of My Heart." Still more demos, alternate takes, and wholly unheard-before recordings have been released since Haley's death. Notable examples of such releases include the albums "Golden Country Origins" by Grassroots Records of Australia and "Hillbilly Haley" by the British label, Rollercoaster, as well as the aforementioned German release by Hydra Records. In 2006, Bear Family Records of Germany released what is considered to be the most comprehensive (yet still incomplete) collection of Haley's 1946–1950 recordings as part of its Haley box set "Rock n' Roll Arrives".

Bill Haley's compositions included "Four Leaf Clover Blues" in 1948, "Rose of My Heart", "Yodel Your Blues Away", "Crazy Man, Crazy", "What'Cha Gonna Do", "Fractured", "Live It Up", "Farewell, So Long, Goodbye", "Real Rock Drive", "Rocking Chair on the Moon", "Sundown Boogie", "Green Tree Boogie", "Tearstains on My Heart", "Down Deep in My Heart", "Straight Jacket", "Birth of the Boogie", "Two Hound Dogs", "Rock-A-Beatin' Boogie", "Hot Dog Buddy Buddy", "R-O-C-K", "Rudy's Rock", "Calling All Comets", "Tonight's the Night", "Hook, Line and Sinker", "Sway with Me", "Paper Boy (On Main Street U.S.A.)", "Skinny Minnie", "B.B. Betty", "Eloise", "Whoa Mabel!", "Vive la Rock and Roll", "I've Got News For You", "So Right Tonight", "Jamaica D.J.", "Ana Maria", "Yucatán Twist", "Football Rock and Roll", "Let the Good Times Roll Again" in 1979, and "Chick Safari" in 1960.

He also wrote or co-wrote songs for other artists such as "I've Got News for You" for Penny Smith in 1955 on Kahill, "Calypso Rock" for Dave Day and The Red Coats on Kapp in 1956, "Half Your Heart" with Robert J. Hayes for Kitty Nation in 1956 on Wing, "I Oughta" and "Everything But You" for Dotti Malone in 1956 also on Wing, "A.B.C. Rock" and "Rocky the Rockin' Rabbit" for Sally Starr on Arcade, "Toodle-Oo-Bamboo" for Ray Coleman and His Skyrockets on Skyrocket Records in 1959, "Always Together" for the Cook Brothers on Arcade in 1960, "Crazy Street" for The Matys Brothers on Coral Records, "The Cat" for Cappy Bianco, and "(Ya Gotta) Sing For the Ladies" and "Butterfly Love" for Ginger Shannon and Johnny Montana in 1960 on Arcade as well as the unissued "I'm Shook" in 1958.

"NME" – October 1955
"NME" – January 1957

In 1982, Haley's "Rock Around the Clock" was inducted into the Grammy Hall of Fame, a special Grammy award established in 1973 to honor recordings at least 25 years old and with "qualitative or historical significance".

In December 2017, Haley was inducted into the Rhythm and Blues Hall Of Fame.




</doc>
<doc id="4529" url="https://en.wikipedia.org/wiki?curid=4529" title="Northern bobwhite">
Northern bobwhite

The northern bobwhite, Virginia quail or (in its home range) bobwhite quail ("Colinus virginianus") is a ground-dwelling bird native to the United States, Mexico, and the Caribbean. It is a member of the group of species known as New World quails (Odontophoridae). They were initially placed with the Old World quails in the pheasant family (Phasianidae), but are not particularly closely related. The name "bobwhite" derives from its characteristic whistling call. Despite its secretive nature, the northern bobwhite is one of the most familiar quails in eastern North America because it is frequently the only quail in its range. Habitat degradation has likely contributed to the northern bobwhite population in eastern North America declining by roughly 85% from 1966-2014. This population decline is apparently range-wide and continuing.

There are 21 subspecies of northern bobwhite, many of which are hunted extensively as game birds. One subspecies, the masked bobwhite ("Colinus virginianus ridgwayi"), is listed as endangered with wild populations located in the northern Mexican state of Sonora and a reintroduced population in Buenos Aires National Wildlife Refuge in southern Arizona.

This is a moderately-sized quail and is the only small galliform native to eastern North America. The bobwhite can range from in length with a wingspan. As indicated by body mass, weights increase in birds found further north, as corresponds to Bergmann's rule. In Mexico, northern bobwhites weigh from whereas in the north they average and large males can attain as much as . Among standard measurements, the wing chord is , the tail is the culmen is and the tarsus is . It has the typical chunky, rounded shape of a quail. The bill is short, curved and brown-black in color. This species is sexually dimorphic. Males have a white throat and brow stripe bordered by black. The overall rufous plumage has gray mottling on the wings, white scalloped stripes on the flanks, and black scallops on the whitish underparts. The tail is gray. Females are similar but are duller overall and have a buff throat and brow without the black border. Both sexes have pale legs and feet.

There are twenty-one recognized subspecies in 3 groups. 1 subspecies is extinct.

The northern bobwhite's diet consists of plant material and small invertebrates, such as ticks, snails, grasshoppers, beetles, spiders, crickets, and leafhoppers. Plant sources include seeds, wild berries, partridge peas, and cultivated grains. It forages on the ground in open areas with some spots of taller vegetation.

Optimal nutrient requirements for bobwhite vary depending on the age of bird and time of year. For example, optimal protein and energy requirements for egg laying hens (23% protein) is much higher than males (16%). 

The northern bobwhite can be found year-round in agricultural fields, grassland, open woodland areas, roadsides and wood edges. Its range covers the southeastern quadrant of the United States from the Great Lakes and southern Minnesota east to Pennsylvania and southern Massachusetts, and extending west to southern Nebraska, Kansas, Oklahoma, Colorado front-range foothills to 7000 feet, and all but westernmost Texas. It is absent from the southern tip of Florida and the highest elevations of the Appalachian Mountains, but occurs in eastern Mexico and in Cuba. Isolated populations have been introduced in Oregon and Washington. The northern bobwhite has also been introduced to New Zealand.

The clear whistle "bob-WHITE" or "bob-bob-WHITE" call is very recognizable. The syllables are slow and widely spaced, rising in pitch a full octave from beginning to end. Other calls include lisps, peeps, and more rapidly whistled warning calls. Another phrase to used mimic the call of the bobwhite is "Bobwhite, your (bob) peas (bob) ripe (white)?".

Like most game birds, the northern bobwhite is shy and elusive. When threatened, it will crouch and freeze, relying on camouflage to stay undetected, but will flush into low flight if closely disturbed. It is generally solitary or paired early in the year, but family groups are common in the late summer and winter roosts may have two dozen or more birds in a single covey.

The species was once considered monogamous, but with the advent of radio telemetry, the sexual behavior of bobwhites has better been described as ambisexual polygamy. Either parent may incubate a clutch for 23 days, and the precocial young leave the nest shortly after hatching. The main source of nest failure is predation, with nest success averaging 28% across their range. However, the nest success of stable populations is typically much higher than this average, and the aforementioned estimate includes values for declining populations. 

Brooding behavior varies in that amalgamation (kidnapping, adopting, creching, gang brooding) may occur. An incubating parent may alternatively stay with their young. A hen may renest up to 4 times until she has a successful nest. However, it is extremely rare for bobwhites to hatch more than 2 successful nests within one nesting season.

Northern bobwhite were introduced into Italy in 1927, and are reported in the plains and hills in the northwest of the country. Other reports from the EU are in France, Spain, and Yugoslavia. As bobwhites are highly productive and popular aviary subjects, it is reasonable to expect other introductions have been made in other parts of the EU, especially in the UK and Ireland, where game-bird breeding, liberation, and naturalisation are relatively common practices.

From 1898 to 1902, some 1300 birds were imported from America and released in many parts of the North and South Islands, from Northland to Southland. The bird was briefly on the Nelson game shooting licence, but: "It would seem that the committee was a little too eager in placing these Quail on the licence, or the shooters of the day were over-zealous and greedy in their bag limits, for the Virginian Quail, like the Mountain Quail were soon thing of the past." The Taranaki (Acclimatisation) Society released a few in 1900 and was confidant that in a year or two they might offer good sport; two years later, broods were reported and the species was said to be "steadily increasing"; but after another two years they seemed "to have disappeared" and that was the end of them. The Otago (Acclimatisation) Society imported more in 1948, but these releases "did no good". After 1923, no more genuinely wild birds were sighted until 1952, when a small population was found North-West of Wairoa in the Ruapapa Road area. Since then bobwhite have been found at several localities around Waikaremoana, in farmland, open bush and along roadsides.

More birds have been imported into New Zealand by private individuals since the 1990s and a healthy captive population is now held by backyard aviculturalists and have been found to be easily cared for, bred, and are popular for their song and good looks. A larger proportion of the national captive population belong to a few game preserves and game bird breeders. Though the birds would be self-sustaining in the wild if they were protected; it is tricky to guess what the effect of an annual population subsidy and hunting has on any of the original populations from the Acclimatisation Society releases. It would be fair to suggest most birds in the wild are no more than one generation from captive stock.

An albino hen was present in a covey in Bayview, Hawkes Bay for a couple of seasons sometime around 2000.

Bobwhites are popular throughout the world, with healthy captive populations everywhere where bird-keeping is enjoyed. Certain countries/states require permits and record keeping, as the possibility that an introduced population may compete with or spread diseases with native quail is a real threat.

Bobwhites are generally compatible with most parrots, softbills and dove. This species should, however, be the only ground-dwelling species in the aviary. Most individuals will do little damage to finches, but one should watch that nests are not being crushed when the species perches at night. Single pairs are preferred, unless the birds have been raised together as a group since they were chicks. Some fighting will occur between cocks at breeding time. One cock may be capable of servicing several hens at once, but the fertility seems to be highest in the eggs from the "preferred" hen. Aviary style is a compromise between what is tolerated by the bird and what is best for the bird. Open parrot-style type aviaries may be used, but some birds will remain flighty and shy in this situation. In a planted aviary, this species will generally settle down to become quite tame and confiding. Parents with chicks will roost on the ground, forming a circular arrangement, with heads facing outwards. In the early morning and late afternoon, the cock will utter its call, which, although not loud, carries well and may offend noise-sensitive neighbors. Most breeding facilities keep birds in breeding groups, on wire up off the ground. This keeps the birds clean and generally avoids diseases and parasites which can devastate a covey. Cages with mesh floors for pairs and trios are also employed, but usually where there is a photo-period manipulation to keep birds breeding through winter.

In the wild the bobwhite feeds on a variety of seeds of weeds, grasses, as well as insects. These are generally collected on the ground or from low foliage. Birds in the aviary are easily catered for with a commercial small seed mix (finch, budgerigar, or small parrot mix) when supplemented with greenfeed. Live food is not usually necessary for breeding, but will be ravenously accepted. High protein foods such as chicken grower crumble are more convenient to supply and will be useful for the stimulation of breeding birds. Extra calcium is required, especially by laying hens; it can be supplied in the form of shell grit, or cuttlefish bone.

In an open aviary hens will lay all over the show if a nesting site and privacy are not provided. Hens that do this may lay upwards of 80 eggs in a season which can be taken for artificial incubation - and chicks hand raised. Otherwise hens with nesting cover, that do make a nest (on the ground) will build up 8–25 eggs in a clutch, with eggs being laid daily

Some captive bobwhite hybrids recorded are between blue quail (scaled quail), Gambel's quail, California quail, mountain quail and it has long been suggested that there are Japanese quail hybrids being bred commercially; there is a distinct lack of photographic proof to substantiate this. Inter-subspecific hybrids have been common.

Several mutations have long been established, including Californian Jumbo, Wisconsin Jumbo, Northern Giant, Albino, Snowflake, Blonde, Fawn, Barred, Silver, and Red.

The Central American spot-bellied bobwhite looks very similar, but lacks black facial coloration. The Asian rain quail is smaller in size and has a black breast.



</doc>
<doc id="4531" url="https://en.wikipedia.org/wiki?curid=4531" title="Bipolar disorder">
Bipolar disorder

Bipolar disorder, previously known as manic depression, is a mental disorder that causes periods of depression and periods of abnormally elevated mood. The elevated mood is significant and is known as mania or hypomania, depending on its severity, or whether symptoms of psychosis are present. During mania, an individual behaves or feels abnormally energetic, happy, or irritable. Individuals often make poorly thought out decisions with little regard to the consequences. The need for sleep is usually reduced during manic phases. During periods of depression, there may be crying, a negative outlook on life, and poor eye contact with others. The risk of suicide among those with the illness is high at greater than 6 percent over 20 years, while self-harm occurs in 30–40 percent. Other mental health issues such as anxiety disorders and substance use disorder are commonly associated.
The causes are not clearly understood, but both environmental and genetic factors play a role. Many genes of small effect contribute to risk. Environmental risk factors include a history of childhood abuse, and long-term stress. About 85% of the risk is attributed to genetics. The condition is divided into bipolar I disorder if there has been at least one manic episode, with or without depressive episodes, and bipolar II disorder if there has been at least one hypomanic episode (but no manic episodes) and one major depressive episode. In those with less severe symptoms of a prolonged duration, the condition cyclothymic disorder may be diagnosed. If due to drugs or medical problems, it is classified separately. Other conditions that may present in a similar manner include attention deficit hyperactivity disorder, personality disorders, schizophrenia and substance use disorder as well as a number of medical conditions. Medical testing is not required for a diagnosis, though blood tests or medical imaging can be done to rule out other problems.
Treatment commonly includes psychotherapy as well as medications such as mood stabilizers and antipsychotics. Examples of mood stabilizers that are commonly used include lithium and various anticonvulsants. Involuntary treatment in a hospital may be needed if a person is a risk to themselves or others but refuses treatment. Severe behavioral problems, such as agitation or combativeness, may be managed with short term antipsychotics or benzodiazepines. In periods of mania, it is recommended that antidepressants be stopped. If antidepressants are used for periods of depression, they should be used with a mood stabilizer. Electroconvulsive therapy (ECT), while not very well studied, may be tried for those who do not respond to other treatments. If treatments are stopped, it is recommended that this be done slowly. Many individuals have financial, social or work-related problems due to the illness. These difficulties occur a quarter to a third of the time, on average. The risk of death from natural causes such as heart disease is twice that of the general population. This is due to poor lifestyle choices and the side effects from medications.
Bipolar disorder affects approximately 1% of the global population. In the United States, about 3% are estimated to be affected at some point in their life. The most common age at which symptoms begin is 25. Rates appear to be similar in females and males. The economic costs of the disorder has been estimated at $45 billion for the United States in 1991. A large proportion of this was related to a higher number of missed work days, estimated at 50 per year. People with bipolar disorder often face problems with social stigma.

Both mania and depression are characterized by disruptions in normal mood, psychomotor activity, circadian rhythm, and cognition. Mania can present with varying levels of mood disturbance, ranging from euphoria that is associated with "classic mania" to dysphoria and irritability. The core symptom of mania involves an increase in energy of psychomotor activity. Mania can also present with increased self esteem or grandiosity, rapid speech, the subjective feeling of rapid thoughts, disinhibited social behavior, or impulsivity. Mania is distinguished from hypomania by length, where hypomania requires four consecutive days, and mania requires more than a week. Unlike mania, hypomania is not always associated with impaired functioning. The biological mechanisms responsible for switching from a manic or hypomanic episode to a depressive episode, or vice versa, remain poorly understood.

Mania is a distinct period of at least one week of elevated or irritable mood, which can range from euphoria to delirium, and those experiencing hypo- or mania may exhibit three or more of the following behaviors: speak in a rapid, uninterruptible manner, short attention span, racing thoughts, increased goal-oriented activities, agitation, or they may exhibit behaviors characterized as impulsive or high-risk, such as hypersexuality or excessive spending. To meet the definition for a manic episode, these behaviors must impair the individual's ability to socialize or work. If untreated, a manic episode usually lasts three to six months.

People with hypomania or mania may experience a decreased need of sleep, impaired judgment, and speak excessively and very rapidly. Manic individuals often have a history of substance abuse developed over years as a form of "self-medication". At the more extreme, a person in a full blown manic state can experience psychosis; a break with reality, a state in which thinking is affected along with mood. They may feel unstoppable, or as if they have been "chosen" and are on a "special mission", or have other grandiose or delusional ideas. This may lead to violent behavior and, sometimes, hospitalization in an inpatient psychiatric hospital. The severity of manic symptoms can be measured by rating scales such as the Young Mania Rating Scale, though questions remain about the reliability of these scales.

The onset of a manic or depressive episode is often foreshadowed by sleep disturbances. Mood changes, psychomotor and appetite changes, and an increase in anxiety can also occur up to three weeks before a manic episode develops.

Hypomania is the milder form of mania, defined as at least four days of the same criteria as mania, but does not cause a significant decrease in the individual's ability to socialize or work, lacks psychotic features such as delusions or hallucinations, and does not require psychiatric hospitalization. Overall functioning may actually increase during episodes of hypomania and is thought to serve as a defense mechanism against depression by some. Hypomanic episodes rarely progress to full blown manic episodes. Some people who experience hypomania show increased creativity while others are irritable or demonstrate poor judgment.

Hypomania may feel good to some persons who experience it, though most people who experience hypomania state that the stress of the experience is very painful. Bipolar people who experience hypomania, however, tend to forget the effects of their actions on those around them. Even when family and friends recognize mood swings, the individual will often deny that anything is wrong. What might be called a "hypomanic event", if not accompanied by depressive episodes, is often not deemed problematic, unless the mood changes are uncontrollable, volatile, or mercurial. Most commonly, symptoms continue for a few weeks to a few months.

Symptoms of the depressive phase of bipolar disorder include persistent feelings of sadness, irritability or anger, loss of interest in previously enjoyed activities, excessive or inappropriate guilt, hopelessness, sleeping too much or not enough, changes in appetite and/or weight, fatigue, problems concentrating, self-loathing or feelings of worthlessness, and thoughts of death or suicide. In severe cases, the individual may develop symptoms of psychosis, a condition also known as severe bipolar disorder with psychotic features. These symptoms include delusions and hallucinations. A major depressive episode persists for at least two weeks, and may result in suicide if left untreated.

The earlier the age of onset, the more likely the first few episodes are to be depressive. Since a diagnosis of bipolar disorder requires a manic or hypomanic episode, many affected individuals are initially misdiagnosed as having major depression and then incorrectly treated with prescribed antidepressants.

In bipolar disorder, mixed state is a condition during which symptoms of both mania and depression occur simultaneously. Individuals experiencing a mixed state may have manic symptoms such as grandiose thoughts while simultaneously experiencing depressive symptoms such as excessive guilt or feeling suicidal. Mixed states are considered to be high-risk for suicidal behavior since depressive emotions such as hopelessness are often paired with mood swings or difficulties with impulse control. Anxiety disorders occur more frequently as a comorbidity in mixed bipolar episodes than in non-mixed bipolar depression or mania. Substance abuse (including alcohol) also follows this trend, thereby appearing to depict bipolar symptoms as no more than a consequence of substance abuse.

Associated features are clinical phenomena that often accompany the disorder but are not part of the diagnostic criteria. In adults with the condition, bipolar disorder is often accompanied by changes in cognitive processes and abilities. These include reduced attentional and executive capabilities and impaired memory. How the individual processes the universe also depends on the phase of the disorder, with differential characteristics between the manic, hypomanic and depressive states. Some studies have found a significant association between bipolar disorder and creativity. Those with bipolar disorder may have difficulty in maintaining relationships. There are several common childhood precursors seen in children who later receive a diagnosis of bipolar disorder: mood abnormalities (including major depressive episodes) and attention deficit hyperactivity disorder (ADHD).

The diagnosis of bipolar disorder can be complicated by coexisting (comorbid) psychiatric conditions including the following: obsessive-compulsive disorder, substance abuse, eating disorders, attention deficit hyperactivity disorder, social phobia, premenstrual syndrome (including premenstrual dysphoric disorder), or panic disorder. A careful longitudinal analysis of symptoms and episodes, enriched if possible by discussions with friends and family members, is crucial to establishing a treatment plan where these comorbidities exist.

The causes of bipolar disorder likely vary between individuals and the exact mechanism underlying the disorder remains unclear. Genetic influences are believed to account for 60–80 percent of the risk of developing the disorder indicating a strong hereditary component. The overall heritability of the bipolar spectrum has been estimated at 0.71. Twin studies have been limited by relatively small sample sizes but have indicated a substantial genetic contribution, as well as environmental influence. For bipolar disorder type I, the rate at which identical twins (same genes) will both have bipolar disorder type I (concordance) is estimated at around 40 percent, compared to about 5 percent in fraternal twins. A combination of bipolar I, II, and cyclothymia similarly produced rates of 42 percent and 11 percent (identical and fraternal twins, respectively), with a relatively lower ratio for bipolar II that likely reflects heterogeneity. There is overlap with major (unipolar) depression and if this is also counted in the co-twin the concordance with bipolar disorder rises to 67 percent in identical twins and 19 percent in fraternal twins. The relatively low concordance between fraternal twins brought up together suggests that shared family environmental effects are limited, although the ability to detect them has been limited by small sample sizes.

Behavioral genetic studies have suggested that many chromosomal regions and candidate genes are related to bipolar disorder susceptibility with each gene exerting a mild to moderate effect. The risk of bipolar disorder is nearly ten-fold higher in first degree-relatives of those affected with bipolar disorder when compared to the general population; similarly, the risk of major depressive disorder is three times higher in relatives of those with bipolar disorder when compared to the general population.

Although the first genetic linkage finding for mania was in 1969, the linkage studies have been inconsistent. The largest and most recent genome-wide association study (GWAS) failed to find any particular locus that exerts a large effect reinforcing the idea that no single gene is responsible for bipolar disorder in most cases. Polymorphisms in BDNF, DRD4, DAO, and TPH1 have been frequently associated with bipolar disorder and were initially successful in a meta-analysis, but failed after correction for multiple testing. On the other hand, two polymorphisms in TPH1 were identified as being associated with bipolar disorder.

Due to the inconsistent findings in GWAS, multiple studies have undertaken the approach of analyzing single-nucleotide polymorphisms (SNPs) in biological pathways. Signaling pathways traditionally associated with bipolar disorder that have been supported by these studies include CRH signaling, cardiac β-adrenergic signaling, Phospholipase C signaling, glutamate receptor signaling, cardiac hypertrophy signaling, Wnt signaling, Notch signaling, and endothelin 1 signaling. Of the 16 genes identified in these pathways, three were found to be dysregulated in the dorsolateral prefrontal cortex portion of the brain in post-mortem studies, CACNA1C, GNG2, and ITPR2.

Findings point strongly to heterogeneity, with different genes being implicated in different families. Robust and replicable genome-wide significant associations showed several common SNPs, including variants within the genes CACNA1C, ODZ4, and NCAN.

Advanced paternal age has been linked to a somewhat increased chance of bipolar disorder in offspring, consistent with a hypothesis of increased new genetic mutations.

Environmental factors play a significant role in the development and course of bipolar disorder, and individual psychosocial variables may interact with genetic dispositions. It is probable that recent life events and interpersonal relationships contribute to the onset and recurrence of bipolar mood episodes, just as they do for unipolar depression. In surveys, 30–50 percent of adults diagnosed with bipolar disorder report traumatic/abusive experiences in childhood, which is associated on average with earlier onset, a higher rate of suicide attempts, and more co-occurring disorders such as PTSD. The number of reported stressful events in childhood is higher in those with an adult diagnosis of bipolar spectrum disorder compared to those without, particularly events stemming from a harsh environment rather than from the child's own behavior.

Less commonly, bipolar disorder or a bipolar-like disorder may occur as a result of or in association with a neurological condition or injury. Conditions like these and injuries include (but are not limited to) stroke, traumatic brain injury, HIV infection, multiple sclerosis, porphyria, and rarely temporal lobe epilepsy.

Abnormalities in the structure and/or function of certain brain circuits could underlie bipolar. Meta-analyses of structural MRI studies in bipolar disorder report decreased volume in the left rostral anterior cingulate cortex(ACC), fronto-insular cortex, ventral prefrontal cortex, and claustrum. Increases have been reported in the volume of the lateral ventricles, globus pallidus, subgenual anterior cingulate, and amygdala as well as in the rates of deep white matter hyperintensities. Functional MRI findings suggest that abnormal modulation between ventral prefrontal and limbic regions, especially the amygdala, likely contributes to poor emotional regulation and mood symptoms. Pharmacological treatment of mania increases ventral prefrontal cortex (vPFC) activity, normalizing it relative to controls, suggesting that vPFC hypoactivity is an indicator of mood state. On the other hand, pretreatment hyperactivity in the amygdala is reduced post treatment but still increased relative to controls, suggesting that it is a trait marker.

Manic and depressive episodes tend to be characterized by ventral versus dorsal dysfunction in the ventral prefrontal cortex. During attentional tasks and resting, mania is associated with decreased Orbitofrontal cortex activity, while depression is associated with increased resting metabolism. Consistent with affective disorders due to lesions, mania and depression are lateralized in ventral prefrontal cortex (vPFC) dysfunction, with depression primarily being associated with the left vPFC, and mania the right vPFC. Abnormal vPFC activity, along with amygdala hyperactivity is found during euthymia as well as in healthy relatives of those with bipolar, indicating possible trait features.

Euthymic bipolar people show decreased activity in the lingual gyrus, while people who are manic demonstrate decreased activity in the inferior frontal cortex, while no differences were found in people with bipolar depression. People with bipolar have increased activation of left hemisphere ventral limbic areas and decreased activation of right hemisphere cortical structures related to cognition.

One proposed model for bipolar suggests that hypersensitivity of reward circuits consisting of fronto-striatal circuits causes mania and hyposensitivity of these circuits cause depression.

According to the "kindling" hypothesis, when people who are genetically predisposed toward bipolar disorder experience stressful events, the stress threshold at which mood changes occur becomes progressively lower, until the episodes eventually start (and recur) spontaneously. There is evidence supporting an association between early-life stress and dysfunction of the hypothalamic-pituitary-adrenal axis (HPA axis) leading to its over activation, which may play a role in the pathogenesis of bipolar disorder.

Some of the brain components which have been proposed to play a role are the mitochondria and a sodium ATPase pump. Circadian rhythms and regulation of the hormone melatonin also seem to be altered.

Dopamine, a known neurotransmitter responsible for mood cycling, has been shown to have increased transmission during the manic phase. The dopamine hypothesis states that the increase in dopamine results in secondary homeostatic down regulation of key systems and receptors such as an increase in dopamine mediated G protein-coupled receptors. This results in decreased dopamine transmission characteristic of the depressive phase. The depressive phase ends with homeostatic up regulation potentially restarting the cycle over again.

Glutamate is significantly increased within the left dorsolateral prefrontal cortex during the manic phase of bipolar disorder, and returns to normal levels once the phase is over. The increase in GABA is possibly caused by a disturbance in early development causing a disturbance of cell migration and the formation of normal lamination, the layering of brain structures commonly associated with the cerebral cortex.

Medications used to treat bipolar may exert their effect by modulating intracellular signaling, such as through depleting myo-inositol levels, inhibition of cAMP signaling, and through altering G coupled proteins. Consistent with this, elevated levels of G, G, and G have been reported in brain and blood samples, along with increased protein kinase A expression and sensitivity.

Decreased levels of 5-hydroxyindoleacetic acid, a byproduct of serotonin, are present in the cerebrospinal fluid of persons with bipolar disorder during both the depressed and manic phases. Increased dopaminergic activity has been hypothesized in manic states due to the ability of dopamine agonists to stimulate mania in people with bipolar disorder. Decreased sensitivity of regulatory α adrenergic receptors as well as increased cell counts in the locus ceruleus indicated increased noradrenergic activity in manic people. Low plasma GABA levels on both sides of the mood spectrum have been found. One review found no difference in monoamine levels, but found abnormal norepinephrine turnover in people with bipolar disorder. Tyrosine depletion was found to reduce the effects of methamphetamine in people with bipolar disorder as well as symptoms of mania, implicating dopamine in mania. VMAT2 binding was found to be increased in one study of people with bipolar mania.

Attempts at prevention of bipolar disorder have focused on stress (such as childhood adversity or highly conflictual families) which, although not a diagnostically specific causal agent for bipolar, does place genetically and biologically vulnerable individuals at risk for a more severe course of illness. There has been debate regarding the causal relationship between usage of cannabis and bipolar disorder.

Bipolar disorder is commonly diagnosed during adolescence or early adulthood, but onset can occur throughout the life cycle. The disorder can be difficult to distinguish from unipolar depression and the average delay in diagnosis is 5–10 years after symptoms begin. Diagnosis of bipolar disorder takes several factors into account and considers the self-reported experiences of the symptomatic individual, abnormal behavior reported by family members, friends or co-workers, observable signs of illness as assessed by a clinician, and often a medical work-up to rule-out medical causes. In diagnosis, caregiver-scored rating scales, specifically the mother, has been found to be more accurate than teacher and youth report in predicting identifying youths with bipolar disorder. Assessment is usually done on an outpatient basis; admission to an inpatient facility is considered if there is a risk to oneself or others. The most widely used criteria for diagnosing bipolar disorder are from the American Psychiatric Association's (APA) "Diagnostic and Statistical Manual of Mental Disorders", Fifth Edition (DSM-5) and the World Health Organization's (WHO) "International Statistical Classification of Diseases and Related Health Problems", 10th Edition (ICD-10). The ICD-10 criteria are used more often in clinical settings outside of the U.S. while the DSM criteria are used clinically within the U.S. and are the prevailing criteria used internationally in research studies. The DSM-5, published in 2013, included further and more accurate specifiers compared to its predecessor, the DSM-IV-TR. Semi structured interviews such as the Kiddie Schedule for Affective Disorders and Schizophrenia (KSADS) and the Structured Clinical Interview for DSM-IV (SCID) are used for diagnostic confirmation of bipolar disorder.

Several rating scales for the screening and evaluation of bipolar disorder exist, including the Bipolar spectrum diagnostic scale, Mood Disorder Questionnaire, the General Behavior Inventory and the Hypomania Checklist. The use of evaluation scales cannot substitute a full clinical interview but they serve to systematize the recollection of symptoms. On the other hand, instruments for screening bipolar disorder tend to have lower sensitivity.

There are several other mental disorders with symptoms similar to those seen in bipolar disorder. These disorders include schizophrenia, major depressive disorder, attention deficit hyperactivity disorder (ADHD), and certain personality disorders, such as borderline personality disorder.

Although there are no biological tests that are diagnostic of bipolar disorder, blood tests and/or imaging may be carried out to exclude medical illnesses with clinical presentations similar to that of bipolar disorder. Neurologic diseases such as multiple sclerosis, complex partial seizures, strokes, brain tumors, Wilson's disease, traumatic brain injury, Huntington's disease, and complex migraines can mimic features of bipolar disorder. An EEG may be used to exclude neurological disorders such as epilepsy, and a CT scan or MRI of the head may be used to exclude brain lesions. Additionally, disorders of the endocrine system such as hypothyroidism, hyperthyroidism, and Cushing's disease are in the differential as is the connective tissue disease systemic lupus erythematosus. Infectious causes of mania that may appear similar to bipolar mania include herpes encephalitis, HIV, influenza, or neurosyphilis. Certain vitamin deficiencies such as pellagra (niacin deficiency), Vitamin B12 deficiency, folate deficiency, and Wernicke Korsakoff syndrome (thiamine deficiency) can also lead to mania.

A review of current and recent medications and drug use is considered to rule out these causes; common medications that can cause manic symptoms include antidepressants, prednisone, Parkinson's disease medications, thyroid hormone, stimulants (including cocaine and methamphetamine), and certain antibiotics.

Bipolar spectrum disorders includes: bipolar I disorder, bipolar II disorder, cyclothymic disorder and cases where subthreshold symptoms are found to cause clinically significant impairment or distress. These disorders involve major depressive episodes that alternate with manic or hypomanic episodes, or with mixed episodes that feature symptoms of both mood states. The concept of the bipolar spectrum is similar to that of Emil Kraepelin's original concept of manic depressive illness.

Unipolar hypomania without accompanying depression has been noted in the medical literature. There is speculation as to whether this condition may occur with greater frequency in the general, untreated population; successful social function of these potentially high-achieving individuals may lead to being labeled as normal, rather than as individuals with substantial dysregulation.

The DSM and the ICD characterize bipolar disorder as a spectrum of disorders occurring on a continuum. The DSM-5 lists three specific subtypes:


When relevant, specifiers for "peripartum onset" and "with rapid cycling" should be used with any subtype. Individuals who have subthreshold symptoms that cause clinically significant distress or impairment, but do not meet full criteria for one of the three subtypes may be diagnosed with other specified or unspecified bipolar disorder. Other specified bipolar disorder is used when a clinician chooses to provide an explanation for why the full criteria were not met (e.g., hypomania without a prior major depressive episode).

Most people who meet criteria for bipolar disorder experience a number of episodes, on average 0.4 to 0.7 per year, lasting three to six months. "Rapid cycling", however, is a course specifier that may be applied to any of the above subtypes. It is defined as having four or more mood disturbance episodes within a one-year span and is found in a significant proportion of individuals with bipolar disorder. These episodes are separated from each other by a remission (partial or full) for at least two months or a switch in mood polarity (i.e., from a depressive episode to a manic episode or vice versa). The definition of rapid cycling most frequently cited in the literature (including the DSM) is that of Dunner and Fieve: at least four major depressive, manic, hypomanic or mixed episodes are required to have occurred during a 12-month period. Ultra-rapid (days) and ultra-ultra rapid or ultradian (within a day) cycling have also been described. The literature examining the pharmacological treatment of rapid cycling is sparse and there is no clear consensus with respect to its optimal pharmacological management.

There are a number of pharmacological and psychotherapeutic techniques used to treat bipolar disorder. Individuals may use self-help and pursue recovery.

Hospitalization may be required especially with the manic episodes present in bipolar I. This can be voluntary or (local legislation permitting) involuntary (called civil or involuntary commitment). Long-term inpatient stays are now less common due to deinstitutionalization, although these can still occur. Following (or in lieu of) a hospital admission, support services available can include drop-in centers, visits from members of a community mental health team or an Assertive Community Treatment team, supported employment and patient-led support groups, intensive outpatient programs. These are sometimes referred to as partial-inpatient programs.

Psychotherapy is aimed at alleviating core symptoms, recognizing episode triggers, reducing negative expressed emotion in relationships, recognizing prodromal symptoms before full-blown recurrence, and, practicing the factors that lead to maintenance of remission. Cognitive behavioral therapy, family-focused therapy, and psychoeducation have the most evidence for efficacy in regard to relapse prevention, while interpersonal and social rhythm therapy and cognitive-behavioral therapy appear the most effective in regard to residual depressive symptoms. Most studies have been based only on bipolar I, however, and treatment during the acute phase can be a particular challenge. Some clinicians emphasize the need to talk with individuals experiencing mania, to develop a therapeutic alliance in support of recovery.

A number of medications are used to treat bipolar disorder. The medication with the best evidence is lithium, which is an effective treatment for acute manic episodes, preventing relapses, and bipolar depression. Lithium reduces the risk of suicide, self-harm, and death in people with bipolar disorder. It is unclear if ketamine is useful in bipolar as of 2015.

Lithium and the anticonvulsants carbamazepine, lamotrigine, and valproic acid are used as mood stabilizers to treat bipolar disorder. These mood stabilizers are used for long-term mood stabilization but have not demonstrated the ability to quickly treat acute bipolar depression. Lithium is preferred for long-term mood stabilization. Carbamazepine effectively treats manic episodes, with some evidence it has greater benefit in rapid-cycling bipolar disorder, or those with more psychotic symptoms or a more schizoaffective clinical picture. It is less effective in preventing relapse than lithium or valproate. Valproate has become a commonly prescribed treatment and effectively treats manic episodes. Lamotrigine has some efficacy in treating bipolar depression, and this benefit is greatest in more severe depression. It has also been shown to have some benefit in preventing bipolar disorder relapses, though there are concerns about the studies done, and is of no benefit in rapid cycling subtype of bipolar disorder. The effectiveness of topiramate is unknown.

Antipsychotic medications are effective for short-term treatment of bipolar manic episodes and appear to be superior to lithium and anticonvulsants for this purpose. Atypical antipsychotics are also indicated for bipolar depression refractory to treatment with mood stabilizers. Olanzapine is effective in preventing relapses, although the supporting evidence is weaker than the evidence for lithium.

Antidepressants are not recommended for use alone in the treatment of bipolar disorder and have not been found to be of any benefit over that found with mood stabilizers. Atypical antipsychotic medications (e.g., aripiprazole) are preferred over antidepressants to augment the effects of mood stabilizers due to the lack of efficacy of antidepressants in bipolar disorder.

Short courses of benzodiazepines may be used in addition to other medications until mood stabilizing become effective. Electroconvulsive therapy (ECT) is an effective form of treatment for acute mood disturbances in those with bipolar disorder, especially when psychotic or catatonic features are displayed. ECT is also recommended for use in pregnant women with bipolar disorder.

Contrary to widely held views, stimulants are relatively safe in bipolar disorder, and considerable evidence suggests they may even produce an antimanic effect. In cases of comorbid ADHD and bipolar, stimulants may help improve both conditions.

Several studies have suggested that omega 3 fatty acids may have beneficial effects on depressive symptoms, but not manic symptoms. However, only a few small studies of variable quality have been published and there is not enough evidence to draw any firm conclusions.

A lifelong condition with periods of partial or full recovery in between recurrent episodes of relapse, bipolar disorder is considered to be a major health problem worldwide because of the increased rates of disability and premature mortality. It is also associated with co-occurring psychiatric and medical problems, and high rates of initial under- or misdiagnosis, causing a delay in appropriate treatment interventions and contributing to poorer prognoses. After a diagnosis is made, it remains difficult to achieve complete remission of all symptoms with the currently available psychiatric medications and symptoms often become progressively more severe over time.

Compliance with medications is one of the most significant factors that can decrease the rate and severity of relapse and have a positive impact on overall prognosis. However, the types of medications used in treating BD commonly cause side effects and more than 75% of individuals with BD inconsistently take their medications for various reasons.

Of the various types of the disorder, rapid cycling (four or more episodes in one year) is associated with the worst prognosis due to higher rates of self-harm and suicide. Individuals diagnosed with bipolar who have a family history of bipolar disorder are at a greater risk for more frequent manic/hypomanic episodes. Early onset and psychotic features are also associated with worse outcomes, as well as subtypes that are nonresponsive to lithium.

Early recognition and intervention also improve prognosis as the symptoms in earlier stages are less severe and more responsive to treatment. Onset after adolescence is connected to better prognoses for both genders, and being male is a protective factor against higher levels of depression. For women, better social functioning prior to developing bipolar disorder and being a parent are protective towards suicide attempts.

People with bipolar disorder often experience a decline in cognitive functioning during (or possibly before) their first episode, after which a certain degree of cognitive dysfunction typically becomes permanent, with more severe impairment during acute phases and moderate impairment during periods of remission. As a result, two-thirds of people with BD continue to experience impaired psychosocial functioning in between episodes even when their mood symptoms are in full remission. A similar pattern is seen in both BD-I and BD-II, but people with BD-II experience a lesser degree of impairment. Cognitive deficits typically increase over the course of the illness. Higher degrees of impairment correlate with the number of previous manic episodes and hospitalizations, and with the presence of psychotic symptoms. Early intervention can slow the progression of cognitive impairment, while treatment at later stages can help reduce distress and negative consequences related to cognitive dysfunction.

Despite the overly ambitious goals that are frequently part of manic episodes, symptoms of mania undermine the ability to achieve these goals and often interfere with an individual's social and occupational functioning. One third of people with BD remain unemployed for one year following a hospitalization for mania. Depressive symptoms during and between episodes, which occur much more frequently for most people than hypomanic or manic symptoms over the course of illness, are associated with lower functional recovery in between episodes, including unemployment or underemployment for both BD-I and BD-II. However, the course of illness (duration, age of onset, number of hospitalizations, and presence or not of rapid cycling) and cognitive performance are the best predictors of employment outcomes in individuals with bipolar disorder, followed by symptoms of depression and years of education.

A naturalistic study from first admission for mania or mixed episode (representing the hospitalized and therefore most severe cases) found that 50 percent achieved syndromal recovery (no longer meeting criteria for the diagnosis) within six weeks and 98 percent within two years. Within two years, 72 percent achieved symptomatic recovery (no symptoms at all) and 43 percent achieved functional recovery (regaining of prior occupational and residential status). However, 40 percent went on to experience a new episode of mania or depression within 2 years of syndromal recovery, and 19 percent switched phases without recovery.

Symptoms preceding a relapse (prodromal), specially those related to mania, can be reliably identified by people with bipolar disorder. There have been intents to teach patients coping strategies when noticing such symptoms with encouraging results.

Bipolar disorder can cause suicidal ideation that leads to suicidal attempts. Individuals whose bipolar disorder begins with a depressive or mixed affective episode seem to have a poorer prognosis and an increased risk of suicide. One out of two people with bipolar disorder attempt suicide at least once during their lifetime and many attempts are successfully completed. The annual average suicide rate is 0.4 percent, which is 10–20 times that of the general population. The standardized mortality ratio from suicide in bipolar disorder is between 18 and 25. The lifetime risk of suicide has been estimated to be as high as 20 percent in those with bipolar disorder.

Bipolar disorder is the sixth leading cause of disability worldwide and has a lifetime prevalence of about 1 to 3 percent in the general population. However, a reanalysis of data from the National Epidemiological Catchment Area survey in the United States suggested that 0.8 percent of the population experience a manic episode at least once (the diagnostic threshold for bipolar I) and a further 0.5 percent have a hypomanic episode (the diagnostic threshold for bipolar II or cyclothymia). Including sub-threshold diagnostic criteria, such as one or two symptoms over a short time-period, an additional 5.1 percent of the population, adding up to a total of 6.4 percent, were classified as having a bipolar spectrum disorder. A more recent analysis of data from a second US National Comorbidity Survey found that 1 percent met lifetime prevalence criteria for bipolar I, 1.1 percent for bipolar II, and 2.4 percent for subthreshold symptoms.

There are conceptual and methodological limitations and variations in the findings. Prevalence studies of bipolar disorder are typically carried out by lay interviewers who follow fully structured/fixed interview schemes; responses to single items from such interviews may suffer limited validity. In addition, diagnoses (and therefore estimates of prevalence) vary depending on whether a categorical or spectrum approach is used. This consideration has led to concerns about the potential for both underdiagnosis and overdiagnosis.

The incidence of bipolar disorder is similar in men and women as well as across different cultures and ethnic groups. A 2000 study by the World Health Organization found that prevalence and incidence of bipolar disorder are very similar across the world. Age-standardized prevalence per 100,000 ranged from 421.0 in South Asia to 481.7 in Africa and Europe for men and from 450.3 in Africa and Europe to 491.6 in Oceania for women. However, severity may differ widely across the globe. Disability-adjusted life year rates, for example, appear to be higher in developing countries, where medical coverage may be poorer and medication less available. Within the United States, Asian Americans have significantly lower rates than their African and European American counterparts.

Late adolescence and early adulthood are peak years for the onset of bipolar disorder. One study also found that in 10 percent of bipolar cases, the onset of mania had happened after the patient had turned 50.

Variations in moods and energy levels have been observed as part of the human experience throughout history. The words "melancholia", an old word for depression, and "mania" originated in Ancient Greece. The word melancholia is derived from "melas" (), meaning "black", and "chole" (), meaning "bile" or "gall", indicative of the term's origins in pre-Hippocratic humoral theory. Within the humoral theories, mania was viewed as arising from an excess of yellow bile, or a mixture of black and yellow bile. The linguistic origins of mania, however, are not so clear-cut. Several etymologies were proposed by the Ancient Roman physician Caelius Aurelianus, including the Greek word "ania", meaning "to produce great mental anguish", and "manos", meaning "relaxed" or "loose", which would contextually approximate to an excessive relaxing of the mind or soul. There are at least five other candidates, and part of the confusion surrounding the exact etymology of the word mania is its varied usage in the pre-Hippocratic poetry and mythology.

In the early 1800s, French psychiatrist Jean-Étienne Dominique Esquirol's lypemania, one of his affective monomanias, was the first elaboration on what was to become modern depression. The basis of the current conceptualisation of bipolar illness can be traced back to the 1850s; In 1850, Jean-Pierre Falret presented a description to the Academy the Paris Psychiatric Society in the course of which he mentioned "circular insanity" (la folie circulaire, ); the lecture was summarized in 1851 in the "Gazette des hôpitaux" ("Hospital Gazette"). Three years later, in 1854, Jules-Gabriel-François Baillarger (1809–1890) described to the French Imperial Académie Nationale de Médecine a biphasic mental illness causing recurrent oscillations between mania and melancholia, which he termed "folie à double forme" (, "madness in double form"). Baillarger's original paper, "De la folie à double forme," appeared in the medical journal Annales médico-psychologiques (Medico-psychological annals) in 1854.

These concepts were developed by the German psychiatrist Emil Kraepelin (1856–1926), who, using Kahlbaum's concept of cyclothymia, categorized and studied the natural course of untreated bipolar patients. He coined the term "manic depressive psychosis", after noting that periods of acute illness, manic or depressive, were generally punctuated by relatively symptom-free intervals where the patient was able to function normally.

The term "manic–depressive "reaction"" appeared in the first version of the DSM in 1952, influenced by the legacy of Adolf Meyer. Subtyping into "unipolar" depressive disorders and bipolar disorders was first proposed by German psychiatrists Karl Kleist and Karl Leonhard in the 1950s and they have been regarded as separate conditions since publication of the DSM-III. The subtypes bipolar II and rapid cycling have been included since the DSM-IV, based on work from the 1970s by David Dunner, Elliot Gershon, Frederick Goodwin, Ronald Fieve, and Joseph Fleiss.

There are widespread problems with social stigma, stereotypes, and prejudice against individuals with a diagnosis of bipolar disorder.

Kay Redfield Jamison, a clinical psychologist and professor of psychiatry at the Johns Hopkins University School of Medicine, profiled her own bipolar disorder in her memoir "An Unquiet Mind" (1995). In his autobiography "" (2008), Chris Joseph describes his struggle between the creative dynamism which allowed the creation of his multimillion-pound advertising agency Hook Advertising, and the money-squandering dark despair of his bipolar illness.

Several dramatic works have portrayed characters with traits suggestive of the diagnosis that has been the subject of discussion by psychiatrists and film experts alike. A notable example is "Mr. Jones" (1993), in which Mr. Jones (Richard Gere) swings from a manic episode into a depressive phase and back again, spending time in a psychiatric hospital and displaying many of the features of the syndrome. In "The Mosquito Coast" (1986), Allie Fox (Harrison Ford) displays some features including recklessness, grandiosity, increased goal-directed activity and mood lability, as well as some paranoia. Psychiatrists have suggested that Willy Loman, the main character in Arthur Miller's classic play "Death of a Salesman", suffers from bipolar disorder, though that specific term for the condition did not exist when the play was written.

TV specials, for example the BBC's "", MTV's "True Life: I'm Bipolar", talk shows, and public radio shows, and the greater willingness of public figures to discuss their own bipolar disorder, have focused on psychiatric conditions, thereby, raising public awareness.

On April 7, 2009, the nighttime drama "90210" on the CW network, aired a special episode where the character Silver was diagnosed with bipolar disorder. Stacey Slater, a character from the BBC soap EastEnders, has been diagnosed with the disorder. The storyline was developed as part of the BBC's Headroom campaign. The Channel 4 soap "Brookside" had earlier featured a story about bipolar disorder when the character Jimmy Corkhill was diagnosed with the condition. 2011 Showtime's political thriller drama "Homeland" protagonist Carrie Mathison is bipolar, which she has kept secret since her school days. In April 2014, ABC premiered a medical drama, "Black Box", in which the main character, a world-renowned neuroscientist, is bipolar.

A link between mental illness and professional success or creativity has been suggested, including in accounts by Socrates, Seneca the Younger, and Cesare Lombroso. Despite prominence in popular culture, the link between creativity and bipolar has not been rigorously studied. This area of study also is likely affected by confirmation bias. Some evidence suggests that some heritable component of bipolar disorder overlaps with heritable components of creativity. Probands of people with bipolar disorder are more likely to be professionally successful, as well as to demonstrate temperamental traits similar to bipolar disorder. Furthermore, while studies of the frequency of bipolar disorder in creative population samples have been conflicting, studies that have a positive finding report that full blown bipolar disorder is rare.

In the 1920s, Emil Kraepelin noted that manic episodes are rare before puberty. In general, bipolar disorder in children was not recognized in the first half of the twentieth century. This issue diminished with an increased following of the DSM criteria in the last part of the twentieth century. The DSM5 does not specifically have bipolar disorder in children and instead refers to it as disruptive mood dysregulation disorder.

While in adults the course of bipolar disorder is characterized by discrete episodes of depression and mania with no clear symptomatology between them, in children and adolescents very fast mood changes or even chronic symptoms are the norm. Pediatric bipolar disorder is commonly characterized by outbursts of anger, irritability and psychosis, rather than euphoric mania, which is more likely to be seen in adults. Early onset bipolar disorder is more likely to manifest as depression rather than mania or hypomania.

The diagnosis of childhood bipolar disorder is controversial, although it is not under discussion that the typical symptoms of bipolar disorder have negative consequences for minors suffering them. The debate is mainly centered on whether what is called bipolar disorder in children refers to the same disorder as when diagnosing adults, and the related question of whether the criteria for diagnosis for adults are useful and accurate when applied to children. Regarding diagnosis of children, some experts recommend following the DSM criteria. Others believe that these criteria do not correctly separate children with bipolar disorder from other problems such as ADHD, and emphasize fast mood cycles. Still others argue that what accurately differentiates children with bipolar disorder is irritability. The practice parameters of the AACAP encourage the first strategy. American children and adolescents diagnosed with bipolar disorder in community hospitals increased 4-fold reaching rates of up to 40 percent in 10 years around the beginning of the 21st century, while in outpatient clinics it doubled reaching 6 percent. Studies using DSM criteria show that up to 1 percent of youth may have bipolar disorder.

Treatment involves medication and psychotherapy. Drug prescription usually consists in mood stabilizers and atypical antipsychotics. Among the former, lithium is the only compound approved by the FDA for children. Psychological treatment combines normally education on the disease, group therapy and cognitive behavioral therapy. Chronic medication is often needed.

Current research directions for bipolar disorder in children include optimizing treatments, increasing the knowledge of the genetic and neurobiological basis of the pediatric disorder and improving diagnostic criteria. Some treatment research suggests that psychosocial interventions that involve the family, psychoeducation, and skills building (through therapies such as CBT, DBT, and IPSRT) can benefit in a pharmocotherapy. Unfortunately, the literature and research on the effects of psychosocial therapy on BPSD is scarce, making it difficult to determine the efficacy of various therapies. The DSM-5 has proposed a new diagnosis which is considered to cover some presentations currently thought of as childhood-onset bipolar.

There is a relative lack of knowledge about bipolar disorder in late life. There is evidence that it becomes less prevalent with age but nevertheless accounts for a similar percentage of psychiatric admissions; that older bipolar patients had first experienced symptoms at a later age; that later onset of mania is associated with more neurologic impairment; that substance abuse is considerably less common in older groups; and that there is probably a greater degree of variation in presentation and course, for instance individuals may develop new-onset mania associated with vascular changes, or become manic only after recurrent depressive episodes, or may have been diagnosed with bipolar disorder at an early age and still meet criteria. There is also some weak and not conclusive evidence that mania is less intense and there is a higher prevalence of mixed episodes, although there may be a reduced response to treatment. Overall, there are likely more similarities than differences from younger adults. In the elderly, recognition and treatment of bipolar disorder may be complicated by the presence of dementia or the side effects of medications being taken for other conditions.






</doc>
<doc id="4536" url="https://en.wikipedia.org/wiki?curid=4536" title="Blitz">
Blitz

Blitz, German for "lightning", may refer to:














</doc>
<doc id="4537" url="https://en.wikipedia.org/wiki?curid=4537" title="Burt Lancaster">
Burt Lancaster

Burton Stephen Lancaster (November 2, 1913 – October 20, 1994) was an American actor and producer. Initially known for playing "tough guys", he went on to achieve success with more complex and challenging roles. He was nominated four times for Academy Awards and won once for his work in "Elmer Gantry" in 1960. He also won a Golden Globe for that performance and BAFTA Awards for "The Birdman of Alcatraz" (1962) and "Atlantic City" (1980).

During the 1950s, his production company Hecht-Hill-Lancaster was highly successful, making films such as "Trapeze" (1956), "Sweet Smell of Success" (1957), "Run Silent, Run Deep" (1958), and "Separate Tables" (1958). The American Film Institute ranks Lancaster as #19 of the greatest male stars of classic Hollywood cinema.

Burton Stephen Lancaster was born on November 2, 1913, in Manhattan, New York, at his parents' home at 209 East 106th Street, the son of Elizabeth ("née" Roberts) and mailman James Henry Lancaster. Both of his parents were Protestants of working class origin. All four of his grandparents were Irish immigrants to the United States, from the province of Ulster; his maternal grandparents were from Belfast and were descendants of English immigrants to Ireland. Lancaster grew up in East Harlem and spent much of his time on the streets, where he developed great interest and skill in gymnastics while attending DeWitt Clinton High School, where he was a basketball star. Before he graduated from DeWitt Clinton, his mother died of a cerebral hemorrhage. Lancaster was accepted by New York University with an athletic scholarship, but subsequently dropped out.

At the age of 19, Lancaster met Nick Cravat, with whom he developed a lifelong partnership. Together they learned to act in local theatre productions and circus arts at Union Settlement, one of the city's oldest settlement houses. They formed the acrobat duo Lang and Cravat in the 1930s and soon joined the Kay Brothers circus. However, in 1939, an injury forced Lancaster to give up the profession, with great regret. He then found temporary work, first as a salesman for Marshall Fields and then as a singing waiter in various restaurants.

With the United States having then entered World War II, Lancaster joined the United States Army in 1942 and performed with the Army's 21st Special Services Division, one of the military groups organized to follow the troops on the ground and provide USO entertainment to keep up morale. He served with General Mark Clark's Fifth Army in Italy from 1943 to 1945.

Although initially unenthusiastic about acting, after returning to New York from his Army service, Lancaster auditioned for a Broadway play and was offered a role. Although Harry Brown's "A Sound of Hunting" had a run of only three weeks, Lancaster's performance attracted the interest of a Hollywood agent, Harold Hecht and, through him, Lancaster was brought to the attention of producer Hal B. Wallis, who signed him to an eight-movie contract. Lancaster's first filmed movie was "Desert Fury". Fortunately for Lancaster, producer Mark Hellinger approached him to star in "The Killers", in 1946, which was completed and released prior to "Desert Fury" and to great critical success.

The tall, muscular actor won significant acclaim and appeared in two more films the following year. Subsequently, he played in a variety of films, especially in dramas, thrillers, and military and adventure films. In two, "The Flame and the Arrow" and "The Crimson Pirate," his friend from his circus years, Nick Cravat, played a key supporting role, and both actors impressed audiences with their acrobatic prowess.

In 1953, Lancaster played one of his best-remembered roles with Deborah Kerr in "From Here to Eternity." The American Film Institute acknowledged the iconic status of the scene from that film in which Deborah Kerr and he make love on a Hawaiian beach amid the crashing waves. The organization named it one of "AFI's top 100 Most Romantic Films" of all time.

Lancaster won the 1960 Academy Award for Best Actor, a Golden Globe Award, and the New York Film Critics Award for his performance in "Elmer Gantry". He followed this with widely diverse roles, including a Nazi war criminal on trial for his life in "Judgment at Nuremberg", a convict serving a life sentence in "Birdman of Alcatraz", and a proud Italian nobleman in "The Leopard". He also played a US Air Force general attempting a coup in the political thriller "Seven Days in May".
In 1966, at the age of 52, Lancaster appeared nude in director Frank Perry's film, "The Swimmer" in what the critic Roger Ebert called "his finest performance". Prior to working on "The Swimmer", Lancaster was terrified of the water because he did not know how to swim. In preparation for the film, he took swimming lessons from UCLA swim coach Bob Horn. The film was not released until 1968, when it proved to be a commercial failure, though Lancaster remained proud of the movie and his performance.

Lancaster co-starred with Lee Marvin in the 1966 western adventure film, "The Professionals", and with Dean Martin in the first of the so-called disaster film blockbusters, "Airport", one of the biggest box-office hits of 1970 and, at that time, reportedly the highest-grossing film in the history of Universal Pictures.

During the latter part of his career, Lancaster left adventure and acrobatic films behind and portrayed more distinguished characters. This period brought him work on several international productions, with directors such as Luchino Visconti, Bernardo Bertolucci and Louis Malle. His last Oscar nomination was for Malle's 1980 film "Atlantic City".

Lancaster sought demanding roles, and if he liked a part or a director, he was prepared to work for much lower pay than he might have earned elsewhere. He even helped to finance movies in whose artistic value he believed. He also mentored directors such as Sydney Pollack and John Frankenheimer and appeared in several television films. Lancaster's last film was "Field of Dreams" (1989).

For his contribution to the motion picture industry, Lancaster has a star on the Hollywood Walk of Fame at 6801 Hollywood Boulevard.

Lancaster was an early and successful actor/producer. When approached to venture into the film business, after having been in the theater for only a brief period, he chose not to sign with a major studio. Instead he signed with agent Harold Hecht, who promised him the opportunity to produce their own movies within five years of hitting Hollywood. Hecht kept his promise and the two formed a partnership production company under the name Norma Productions, named after Lancaster's wife. Their first movie together was "Kiss the Blood Off My Hands", released in 1948. Hecht and Lancaster produced two additional films in the early 1950s under the Norma Productions company; "The Flame and the Arrow" in 1950 and "Ten Tall Men" in 1951, two swashbucklers selected to showcase Lancaster's acrobatic skills.

In 1951 the actor/producer duo changed the company's name to Hecht-Lancaster Productions. The first film under the new name was another swashbuckler: "The Crimson Pirate", released in 1952. This was followed by "Apache" two years later. In 1954 Lancaster was hired to star in the Warner Brothers film "His Majesty O'Keefe". This proved to be a turning point for both Lancaster and his company. Lancaster had insisted that the film be produced by Hecht. "His Majesty O'Keefe" featured a new Hollywood writer on board. James Hill immediately hit it off with Lancaster and Hecht, and he was invited to co-produce upcoming Hecht-Lancaster films, giving up his writer position. His first as a producer was "Vera Cruz", released in 1954. 

Without Hill, Hecht and Lancaster produced "Marty", a 1955 film starring Ernest Borgnine, which won both the Academy Award for Best Picture and the Palme d'Or award at the Cannes Film Festival. "Marty" was also the first film produced by the company not to feature Lancaster in an acting role. "Vera Cruz" had been a huge success but "Marty" secured Hecht-Lancaster as one of the most successful independent production companies in Hollywood at the time. "The Kentuckian" (1955) was directed by Lancaster in his directorial debut.

In 1955 Hill was made an equal partner in the company and the name was upgraded to Hecht-Hill-Lancaster, releasing their first film in 1956, "Trapeze", in which Lancaster performed many of his own stunts. "Trapeze" went on to become the production company's top box office success. Following "Trapeze" Lancaster worked with Tony Curtis again on "Sweet Smell of Success" (released in 1957), a co-production between Hecht-Hill-Lancaster and Curtis' own company Curtleigh Productions (co-owned with his wife, Janet Leigh). In 1956 Lancaster and Hecht entered the music industry with the companies Hecht-Lancaster & Buzzell Music and Calyork Music.

Hecht-Hill-Lancaster produced seven additional films in the late 1950s; four starring Lancaster; "Run Silent, Run Deep" (1958), "Separate Tables" (1958), "The Devil's Disciple" (1959) and "The Unforgiven" (1960), and three without Lancaster: "The Bachelor Party" (1956), "Take a Giant Step" (1959) and "Summer of the Seventeenth Doll" (1960). Additionally Hecht-Hill-Lancaster served as the production company for the 1960-1961 TV series "Whiplash". The "H-H-L" team impressed Hollywood with its success; as "Life" wrote in 1957, "[a]fter the independent production of a baker's dozen of pictures, it has yet to have its first flop ... (They were also good pictures.)." 
The Hecht-Hill-Lancaster Productions company dissolved in 1960, after Hill ruptured his relation with both Hecht and Lancaster. Hill went on to produce a single additional film, "The Happy Thieves", in a new production company, Hillworth Productions, co-owned with his wife Rita Hayworth. Hecht and Lancaster worked on two more films together: "The Young Savages", released in 1961, and "Birdman of Alcatraz", released in 1962 through Norma Productions as the production company's final film. Hecht went on to produce five films without Lancaster's assistance, through his company Harold Hecht Films Productions between 1961 and 1967, including another Academy Award winner, "Cat Ballou", starring Lee Marvin and Jane Fonda. Lancaster and Hecht would reunite twelve years after "Birdman of Alcatraz" for what ended up being Hecht's final film, "Ulzana's Raid", in 1972.

In 1967, Lancaster formed a new partnership with Roland Kibbee, who had already worked as a writer on five Lancaster projects; "Ten Tall Men", "The Crimson Pirate", "Three Sailors and a Girl" (in which Lancaster made a cameo appearance), "Vera Cruz" and "The Devil's Disciple". Through Norlan Productions, Lancaster and Kibbee produced "The Scalphunters" in 1968, "Valdez Is Coming" in 1971 (which was also written by Kibbee) and "The Midnight Man" in 1974. "The Midnight Man" was written, produced and directed by both Kibbee and Lancaster, and would be the actor's final film as a producer. In his career, Lancaster produced twenty-three films, directed two and wrote for one.

Lancaster appeared in a total of 17 films produced by his agent Harold Hecht. Eight of these were co-produced by James Hill. He also appeared in eight films produced by Hal B. Wallis and two with producer Mark Hellinger. Although Lancaster's work alongside Kirk Douglas was mostly known as a successful pair of actors, Douglas in fact produced four films for the pair, through his production companies Bryna Productions and Joel Productions. Roland Kibbee also produced three Lancaster films. Lancaster was also cast in two Stanley Kramer productions.

John Frankenheimer directed five films with Lancaster: "The Young Savages" (1961), "Birdman of Alcatraz" (1962), "Seven Days in May" (1964), "The Train" (1964), and "The Gypsy Moths" (1969). He was directed four times by Robert Aldrich, three times by Robert Siodmak and Sydney Pollack, and twice by Byron Haskin, Daniel Mann, John Sturges, John Huston, Richard Brooks, Alexander Mackendrick, Luchino Visconti, and Michael Winner.

Roland Kibbee wrote for seven Lancaster films. Lancaster used make-up veteran Robert Schiffer in 20 credited films, hiring Schiffer on nearly all the films he produced.

Lancaster vigorously guarded his private life. He was married three times. His first two marriages, to June Ernst from 1935 to 1946 and Norma Anderson from 1946 to 1969, ended in divorce. His third marriage, to Susan Martin, lasted from September 1990 until his death in 1994. All five of his children were with Anderson: Bill (who became an actor and screenwriter), James, Susan, Joanna, and Sighle (pronounced "Sheila"). Friends say he claimed he was romantically involved with Deborah Kerr during the filming of "From Here to Eternity" in 1953. However, Kerr stated that while there was a spark of attraction, nothing ever happened. He reportedly had an affair with Joan Blondell.

In her 1980 autobiography, Shelley Winters claimed to have had a long affair with him. Recent biographers and others believe that Lancaster was bisexual, and that he had intimate relationships with men as well as women. According to testimony in Kate Buford's "Burt Lancaster: An American Life", Lancaster was devotedly loyal to his friends and family. Old friends from his childhood remained his friends for life.

Lancaster was a vocal supporter of liberal political causes, and frequently spoke out in support of racial minorities, including at the March on Washington in 1963. He was a vocal opponent of the Vietnam War and political movements such as McCarthyism, and he helped pay for the successful defense of a soldier accused of "fragging" (murdering) another soldier during that war. In 1968, Lancaster actively supported the presidential candidacy of antiwar Senator Eugene McCarthy of Minnesota, and frequently spoke on his behalf during the Democratic primaries. He campaigned heavily for George McGovern in the 1972 presidential election. In 1985, Lancaster joined the fight against AIDS after his close friend, Rock Hudson, contracted the disease. He campaigned for Michael Dukakis in the 1988 presidential election.

The centennial of Lancaster's birth was honored at New York City's Film Society of Lincoln Center in May 2013 with the screening of 12 of the actor's finest films, from "The Killers" of 1946 to "Atlantic City" in 1981.

Despite his Protestant background and upbringing, Lancaster identified himself as an atheist later in life.

As Lancaster grew older, he became increasingly plagued by atherosclerosis, barely surviving a routine gall bladder operation in January 1980. Following two minor heart attacks, he had to undergo an emergency quadruple coronary bypass in 1983, after which he was extremely weak. However, he still managed to continue acting. In 1988, Lancaster was well enough to attend a Congressional hearing with old colleagues such as James Stewart and Ginger Rogers to protest media magnate Ted Turner's plan to colorize various black-and-white films from the 1930s and 1940s.

Lancaster's acting career ended after he suffered a stroke on November 30, 1990, which left him partly paralyzed and largely unable to speak. He died in his apartment in Century City, California, from a third heart attack at 4:50 am on October 20, 1994, at the age of 80. just 13 days shy of his 81st birthday. He was cremated and his ashes were buried under a large oak tree in Westwood Memorial Park located in Westwood Village, California. A small, square ground plaque inscribed only with "BURT LANCASTER 1913–1994" marks his final resting place. Upon his death, as he requested, he had no memorial or funeral service.

For a number of years exhibitors voted Lancaster as among the most popular stars:

Spanish music group Hombres G released an album named "La cagaste, Burt Lancaster" ("You messed up, Burt Lancaster") in 1986. Thomas Hart Benton painted a scene from "The Kentuckian" as part of the film's marketing. Lancaster posed for the painting, also known as "The Kentuckian".



</doc>
<doc id="4540" url="https://en.wikipedia.org/wiki?curid=4540" title="Balts">
Balts

The Balts or Baltic people (, ) are an Indo-European ethno-linguistic group who speak the Baltic languages, a branch of the Indo-European language family, which was originally spoken by tribes living in the area east of Jutland peninsula in the west and in the Moscow, Oka and Volga rivers basins in the east. One of the features of Baltic languages is the number of conservative or archaic features retained. Among the Baltic peoples are modern Lithuanians, Latvians (including Latgalians) — all Eastern Balts — as well as the Old Prussians, Yotvingians and Galindians — the Western Balts — whose languages and cultures are now extinct.

German medieval chronicler Adam of Bremen in the latter part of the 11th century AD was the first writer to use the term Baltic in its modern sense to mean the sea of that name. Although he must have been familiar with the ancient name, Balcia, meaning a supposed island in the Baltic Sea, and although he may have been aware of the Baltic words containing the stem balt-, "white", as "swamp", he reports that he followed the local use of balticus from baelt ("belt") because the sea stretches to the east "in modum baltei" ("in the manner of a belt"). This is the first reference to "the Baltic or Barbarian Sea, a day's journey from Hamburg."

The Germans, however, preferred some form of "East Sea" (in different languages) until after about 1600, when they began to use forms of "Baltic Sea." Around 1840 the German nobles of the Governorate of Livonia devised the term "Balts" to mean themselves, the German upper classes of Livonia, excluding the Latvian and Estonian lower classes. They spoke an exclusive dialect, Baltic German. For all practical purposes that was the Baltic language until 1919.

In 1845 Georg Heinrich Ferdinand Nesselmann proposed a distinct language group for Latvian, Lithuanian and Old Prussian to be called Baltic. The term became prelevant after Latvia and Lithuania gained independence in 1918. Up until early 20th century "Latvian" and "Lithuanian" could be used to mean the entire language family.

The Balts or Baltic peoples, defined as speakers of one of the Baltic languages, a branch of the Indo-European language family, are descended from a group of Indo-European tribes who settled the area between the lower Vistula and southeast shore of the Baltic Sea and upper Daugava and Dnieper rivers. Because the thousands of lakes and swamps in this area contributed to the Balts' geographical isolation, the Baltic languages retain a number of conservative or archaic features.

It is possible that around 3,500–2,500 B.C., there was massive migration of peoples representing the Corded Ware culture. They came from the southeast and spread all across Eastern and Central Europe, reaching even southern Finland. It is believed that Corded Ware culture peoples were Indo-European ancestors of many Europeans, including Balts. It is thought that those Indo-European newcomers were quite numerous and in the Eastern Baltic assimilated earlier indigenous cultures (Europidic cultures such as the Narva culture and Neman culture). Over time the new people formed the Baltic peoples and they spread in the area from the Baltic sea in the west to the Volga in the east.

Some of the major authorities on Balts, such as Kazimieras Būga, Max Vasmer, Vladimir Toporov and Oleg Trubachyov, in conducting etymological studies of eastern European river names, were able to identify in certain regions names of specifically Baltic provenance, which most likely indicate where the Balts lived in prehistoric times. This information is summarized and synthesized by Marija Gimbutas in "The Balts" (1963) to obtain a likely proto-Baltic homeland. Its borders are approximately: from a line on the Pomeranian coast eastward to include or nearly include the present-day sites of Berlin, Warsaw, Kiev, and Kursk, northward through Moscow to the River Berzha, westward in an irregular line to the coast of the Gulf of Riga, north of Riga.

A possible early reference to a Baltic people occurs in 98 CE, when Tacitus names a tribe living near the Baltic Sea ("Mare Svebicum") as the Aesti ("Aestiorum gentes") and describes them as amber gatherers. However, it is not clear if the Aesti mentioned by Tacitus were: (1) a (now-extinct) Baltic people (possibly synonymous with the Brus/Prūsa), or; (2) a Finno-Ugric people (e.g. modern Estonians). The Aesti appear to have inhabited the Sambian peninsula (in or near the present Kaliningrad Oblast.

Over time, the area of Baltic habitation shrank, due to assimilation by other groups, and invasions. According to one of the theories which has gained considerable traction over the years, one of the western Baltic tribes, the Galindians, Galindae, or Goliad, migrated to the Eastern end of Baltic realm around the 4th century CE, and settled around modern day Moscow, Russia. Finally, according to Slavic chronicles of the time, they warred with Slavs, and perhaps, were defeated and assimilated some time in the 11th to 13th centuries.

Balts became differentiated into Western and Eastern Balts in the late centuries BCE. The eastern Baltic region was inhabited by ancestors of the Western Balts: Brus/Prūsa ("Old Prussians"), Sudovians/Jotvingians, Scalvians, Nadruvians, and Curonians. The Eastern Balts, including the hypothesised Dniepr Balts, were living in modern-day Belarus, Ukraine and Russia.

Subsequent Germanic and Gothic domination in the first half of the first millennium CE in Northern and Eastern Europe, as well as later Slavic expansion, caused large migrations of the Balts — first, the Galindae or Galindians towards the east, and later, Eastern Balts towards the west — until, in the 13th and 14th centuries, they reached the general area that the present-day Balts inhabit. Many other Eastern and Southern Balts either assimilated with other Balts, or Slavs in the 4th–7th centuries and were gradually slavicized.

In the 12th and the 13th centuries, internal struggles, as well as invasions by Ruthenians and Poles and later the expansion of the Teutonic Order resulted in an almost complete annihilation of the Galindians, Curonians, and Yotvingians. Gradually Old Prussians became Germanized or some Lithuanized during period from the 15th to the 17th centuries, especially after the Reformation in Prussia. The cultures of the Lithuanians and Latgalians/Latvians survived and became the ancestors of the populations of the modern countries of Latvia and Lithuania.

Old Prussian was closely related to the other extinct Western Baltic languages, Curonian, Galindian and Sudovian. It is more distantly related to the surviving Eastern Baltic languages, Lithuanian and Latvian. Compare the Prussian word "seme" ("zemē"), the Latvian "zeme", the Lithuanian "žemė" ("land" in English).

Old Prussian contained a few borrowings specifically from Gothic (e.g., Old Prussian "ylo" "awl," as with Lithuanian "ýla", Latvian "īlens") and even North Germanic.

Modern Baltic peoples






</doc>
<doc id="4541" url="https://en.wikipedia.org/wiki?curid=4541" title="Burnt-in timecode">
Burnt-in timecode

Burnt-in timecode (often abbreviated to BITC by analogy to VITC) is a human-readable on-screen version of the timecode information for a piece of material superimposed on a video image. BITC is sometimes used in conjunction with "real" machine-readable timecode, but more often used in copies of original material on to a non-broadcast format such as VHS, so that the VHS copies can be traced back to their master tape and the original time codes easily located. 

Many professional VTRs can "burn" (overlay) the tape timecode onto one of their outputs. This output (which usually also displays the setup menu or on-screen display) is known as the "super out" or "monitor out". The "character" switch or menu item turns this behaviour on or off. The "character" function is also used to display the timecode on the preview monitors in linear editing suites.

Videotapes that are recorded with timecode numbers overlaid on the video are referred to as "window dubs", named after the "window" that displays the burnt-in timecode on-screen.

When editing was done using magnetic tapes that were subject to damage from excessive wear, it was common to use a window dub as a working copy for the majority of the editing process. Editing decisions would be made using a window dub, and no specialized equipment was needed to write down an edit decision list which would then be replicated from the high-quality masters.

Timecode can also be superimposed on video using a dedicated overlay device, often called a "window dub inserter". This inputs a video signal and its separate timecode audio signal, reads the timecode, superimposes the timecode display over the video, and outputs the combined display (usually via composite), all in real time. Stand-alone timecode generator / readers often have the window dub function built-in. 

Some consumer cameras, in particular DV cameras, can "burn" (overlay) the tape timecode onto the composite output. This output typically is semi-transparent and may include other tape information. It is usually activated by turning on the 'display' info in one of the camera's sub-menus. While not as 'professional' as an overlay as created by a professional VCRs, it is a cheap alternative that is just as accurate.

Timecode is stored in the metadata areas of captured DV AVI files, and some software is able to "burn" (overlay) this into the video frames. For example, DVMP Pro is able to "burn" timecode or other items of DV metadata (such as date and time, iris, shutter speed, gain, white balance mode, etc.) into DV AVI files.

OCR techniques can be used to read BITC in situations where other forms of timecode are not available.



</doc>
<doc id="4542" url="https://en.wikipedia.org/wiki?curid=4542" title="Bra–ket notation">
Bra–ket notation

In quantum mechanics, bra–ket notation is a standard notation for describing quantum states. It can also be used to denote abstract vectors and linear functionals in mathematics. 
The notation begins with using angle brackets, ⟨ and ⟩, and a vertical bar, |, to denote the scalar product of vectors or the action of a linear functional on a vector in a complex vector space. The scalar product or action is written as

The right part is called the ket ; it is a vector, typically represented as a column vector and written 

The left part is called the bra, ; it is the Hermitian conjugate of the ket with the same label, typically represented as a row vector and is written

A combination of bras, kets, and operators is interpreted using matrix multiplication. A bra and a ket with the same label are Hermitian conjugates of each other.

Bra-ket notation was introduced in 1939 by Paul Dirac and is also known as the Dirac notation. 

The bra-ket notation has a precursor in Hermann Grassmann's use of the notation
formula_4
for his inner products nearly 100 years earlier.

Bra–ket notation is a notation for linear algebra, particularly focused on vectors, inner products, linear operators, Hermitian conjugation, and the dual space, for both finite-dimensional and infinite-dimensional complex vector spaces. It is specifically designed to ease the types of calculations that frequently come up in quantum mechanics.

Its use in quantum mechanics is quite widespread. Many phenomena that are explained using quantum mechanics are usually explained using bra–ket notation.

In simple cases, a ket can be described as a column vector, a bra with the same label is its conjugate transpose (which is a row vector), and writing bras, kets, and linear operators next to each other implies matrix multiplication. However, kets may also exist in uncountably-infinite-dimensional vector spaces, such that they cannot be literally written as a column vector. Also, writing a column vector as a list of numbers requires picking a basis, whereas one can write "" without committing to any particular basis. This is helpful because quantum mechanics calculations involve frequently switching between different bases (e.g. position basis, momentum basis, energy eigenbasis, etc.), so it is better to have the basis vectors (if any) written out explicitly. In some situations involving two important basis vectors they will be referred to simply as "" and "".

The standard mathematical notation for the inner product, preferred as well by some physicists, expresses exactly the same thing as the bra–ket notation,

Bras and kets can also be configured in other ways, such as the outer product

which can also be represented as a matrix multiplication (i.e., a column vector times a row vector equals a matrix).

If the ket is an element of a vector space, the bra is technically an element of its dual space—see Riesz representation theorem.

In mathematics, the term "vector" is used to refer generally to any element of any vector space. In physics, however, the term "vector" is much more specific: "Vector" refers almost exclusively to quantities like displacement or velocity, which have three components that relate directly to the three dimensions of the real world. Such vectors are typically denoted with over arrows () or boldface ().

In quantum mechanics, a quantum state is typically represented as an element of an abstract complex vector space—for example the infinite-dimensional vector space of all possible wavefunctions (functions mapping each point of 3D space to a complex number). Since the term "vector" is already used for something else (see previous paragraph), it is very common to refer to these elements of abstract complex vector spaces as "kets", and to write them using ket notation.

Ket notation, invented by Dirac, uses vertical bars and angular brackets: . When this notation is used, these quantities are called "kets", and is read as "ket-A". These kets can be manipulated using the usual rules of linear algebra, for example:

Note how any symbols, letters, numbers, or even words—whatever serves as a convenient label—can be used as the label inside a ket. For example, the last line above involves infinitely many different kets, one for each real number . In other words, the symbol "" has a specific and universal mathematical meaning, while just the "" by itself does not. For example, might or might not be equal to . Nevertheless, for convenience, there is usually some logical scheme behind the labels inside kets, such as the common practice of labeling energy eigenkets in quantum mechanics through a listing of their quantum numbers.

An inner product is a generalization of the dot product. The inner product of two vectors is a scalar. In neutral notation (notation dedicated to the inner product "only"), this might be written , where and are elements of the abstract vector space, i.e. both are "kets".

Bra–ket notation uses a specific notation for inner products:

Bra–ket notation splits this inner product (also called a "bracket") into two pieces, the "bra" and the "ket":

where is called a bra, read as "bra-A", and is a ket as above.

The purpose of "splitting" the inner product into a bra and a ket is that "both" the bra and the ket are meaningful "on their own", and can be used in other contexts besides within an inner product. There are two main ways to think about the meanings of separate bras and kets. Accordingly, the interpretation of the expression has a second interpretation, namely that of the action of a linear functional per below.

For a finite-dimensional vector space, using a fixed orthonormal basis, the inner product can be written as a matrix multiplication of a row vector with a column vector:
Based on this, the bras and kets can be defined as:
and then it is understood that a bra next to a ket implies matrix multiplication.

The conjugate transpose (also called "Hermitian conjugate") of a bra is the corresponding ket and vice versa:
because if one starts with the bra
then performs a complex conjugation, and then a matrix transpose, one ends up with the ket

A more abstract definition, which is equivalent but more easily generalized to infinite-dimensional spaces, is to say that bras are linear functionals on the space of kets, i.e. linear transformations that input a ket and output a complex number. The bra linear functionals are defined to be consistent with the inner product. Thus, if is the linear functional corresponding to under the Riesz representation theorem, then

i.e. it produces "the same" complex number as the inner product does. The terminology for the right hand side is though "not" inner product, which always involves two "kets". Confusing this is harmless, since the same number is produced in the end.

In mathematics terminology, the vector space of bras is the dual space to the vector space of kets, and corresponding bras and kets are related by the Riesz representation theorem.

Bra–ket notation can be used even if the vector space is not a Hilbert space.

In quantum mechanics, it is common practice to write down kets which have infinite norm, i.e. non-normalizable wavefunctions. Examples include states whose wavefunctions are Dirac delta functions or infinite plane waves. These do not, technically, belong to the Hilbert space itself. However, the definition of "Hilbert space" can be broadened to accommodate these states (see the Gelfand–Naimark–Segal construction or rigged Hilbert spaces). The bra–ket notation continues to work in an analogous way in this broader context.

Banach spaces are a different generalization of Hilbert spaces. In a Banach space , the vectors may be notated by kets and the continuous linear functionals by bras. Over any vector space without topology, we may also notate the vectors by kets and the linear functionals by bras. In these more general contexts, the bracket does not have the meaning of an inner product, because the Riesz representation theorem does not apply.

The mathematical structure of quantum mechanics is based in large part on linear algebra:

Since virtually every calculation in quantum mechanics involves vectors and linear operators, it can involve, and often does involve, bra–ket notation. A few examples follow:

The Hilbert space of a spin-0 point particle is spanned by a "position basis" , where the label extends over the set of all points in position space. This label is the eigenvalue of the position operator acting on such a basis state, formula_16. Since there are an uncountably infinite number of vector components in the basis, this is an uncountably infinite-dimensional Hilbert space. The dimensions of the Hilbert space (usually infinite) and position space (usually 1, 2 or 3) are not to be conflated.

Starting from any ket in this Hilbert space, we can "define" a complex scalar function of , known as a wavefunction:
On the left side, is a function mapping any point in space to a complex number; on the right side, is a ket.

It is then customary to define linear operators acting on wavefunctions in terms of linear operators acting on kets, by

For instance, the momentum operator has the following form,

One occasionally encounters an expression such as
though this is something of an abuse of notation. The differential operator must be understood to be an abstract operator, acting on kets, that has the effect of differentiating wavefunctions once the expression is projected into the position basis,
even though, in the momentum basis, the operator amounts to a mere multiplication operator (by ).

In quantum mechanics the expression is typically interpreted as the probability amplitude for the state to collapse into the state . Mathematically, this means the coefficient for the projection of onto . It is also described as the projection of state onto state .

A stationary spin- particle has a two-dimensional Hilbert space. One orthonormal basis is:
where is the state with a definite value of the spin operator equal to + and is the state with a definite value of the spin operator equal to −.

Since these are a basis, "any" quantum state of the particle can be expressed as a linear combination (i.e., quantum superposition) of these two states:
where and are complex numbers.

A "different" basis for the same Hilbert space is:
defined in terms of rather than .

Again, "any" state of the particle can be expressed as a linear combination of these two:

In vector form, you might write
depending on which basis you are using. In other words, the "coordinates" of a vector depend on the basis used.

There is a mathematical relationship between , , and ; see change of basis.

There are a few conventions and abuses of notation that are generally accepted by the physics community, but which might confuse the non-initiated.

It is common to use the same symbol for "labels" and "constants" in the same equation. For example, , where the symbol is used simultaneously as the "name of the operator" , its "eigenvector" and the associated "eigenvalue" .

Something similar occurs in component notation of vectors. While (uppercase) is traditionally associated with wavefunctions, (lowercase) may be used to denote a "label", a "wave function" or "complex constant" in the same context, usually differentiated only by a subscript.

The main abuses are including operations inside the vector labels. This is done for a fast notation of scaling vectors. E.g. if the vector is scaled by , it might be denoted by , which makes no sense since is a label, not a function or a number, so you can't perform operations on it.

This is especially common when denoting vectors as tensor products, where part of the labels are moved outside the designed slot, e.g. . Here part of the labeling that should state that all three vectors are different was moved outside the kets, as subscripts 1 and 2. And a further abuse occurs, since is meant to refer to the norm of the first vector—which is a "label" denoting a "value".

A linear operator is a map that inputs a ket and outputs a ket. (In order to be called "linear", it is required to have certain properties.) In other words, if is a linear operator and is a ket, then is another ket.

In an -dimensional Hilbert space, can be written as an column vector, and then is an matrix with complex entries. The ket can be computed by normal matrix multiplication.

Linear operators are ubiquitous in the theory of quantum mechanics. For example, observable physical quantities are represented by self-adjoint operators, such as energy or momentum, whereas transformative processes are represented by unitary linear operators such as rotation or the progression of time.

Operators can also be viewed as acting on bras "from the right hand side". Specifically, if is a linear operator and is a bra, then is another bra defined by the rule

In an -dimensional Hilbert space, can be written as a row vector, and (as in the previous section) is an matrix. Then the bra can be computed by normal matrix multiplication.

If the same state vector appears on both bra and ket side,
then this expression gives the expectation value, or mean or average value, of the observable represented by operator for the physical system in the state .

A convenient way to define linear operators on a Hilbert space is given by the outer product: if is a bra and is a ket, the outer product
denotes the rank-one operator with the rule 

For a finite-dimensional vector space, the outer product can be understood as simple matrix multiplication:
The outer product is an matrix, as expected for a linear operator.

One of the uses of the outer product is to construct projection operators. Given a ket of norm 1, the orthogonal projection onto the subspace spanned by is

Just as kets and bras can be transformed into each other (making into ), the element from the dual space corresponding to is , where denotes the Hermitian conjugate (or adjoint) of the operator . In other words,

If is expressed as an matrix, then is its conjugate transpose.

Self-adjoint operators, where , play an important role in quantum mechanics; for example, an observable is always described by a self-adjoint operator. If is a self-adjoint operator, then is always a real number (not complex). This implies that expectation values of observables are real.

Bra–ket notation was designed to facilitate the formal manipulation of linear-algebraic expressions. Some of the properties that allow this manipulation are listed herein. In what follows, and denote arbitrary complex numbers, denotes the complex conjugate of , and denote arbitrary linear operators, and these properties are to hold for any choice of bras and kets.



Given any expression involving complex numbers, bras, kets, inner products, outer products, and/or linear operators (but not addition), written in bra–ket notation, the parenthetical groupings do not matter (i.e., the associative property holds). For example:
and so forth. The expressions on the right (with no parentheses whatsoever) are allowed to be written unambiguously "because" of the equalities on the left. Note that the associative property does "not" hold for expressions that include nonlinear operators, such as the antilinear time reversal operator in physics.

Bra–ket notation makes it particularly easy to compute the Hermitian conjugate (also called "dagger", and denoted ) of expressions. The formal rules are:

These rules are sufficient to formally write the Hermitian conjugate of any such expression; some examples are as follows:


Two Hilbert spaces and may form a third space by a tensor product. In quantum mechanics, this is used for describing composite systems. If a system is composed of two subsystems described in and respectively, then the Hilbert space of the entire system is the tensor product of the two spaces. (The exception to this is if the subsystems are actually identical particles. In that case, the situation is a little more complicated.)

If is a ket in and is a ket in , the direct product of the two kets is a ket in . This is written in various notations:

See quantum entanglement and the EPR paradox for applications of this product.

Consider a complete orthonormal system ("basis"),
for a Hilbert space , with respect to the norm from an inner product . 

From basic functional analysis, it is known that any ket can also be written as
with the inner product on the Hilbert space.

From the commutativity of kets with (complex) scalars, it follows that
must be the "identity operator", which sends each vector to itself. 

This, then, can be inserted in any expression without affecting its value; for example
where, in the last identity, the Einstein summation convention has been used.

In quantum mechanics, it often occurs that little or no information about the inner product of two arbitrary (state) kets is present, while it is still possible to say something about the expansion coefficients and of those vectors with respect to a specific (orthonormalized) basis. In this case, it is particularly useful to insert the unit operator into the bracket one time or more.

For more information, see Resolution of the identity, 

Since , plane waves follow, .

The object physicists are considering when using bra–ket notation is a Hilbert space (a complete inner product space).

Let be a Hilbert space and a vector in . What physicists would denote by is the vector itself. That is,

Let be the dual space of . This is the space of linear functionals on . The isomorphism is defined by , where for every we define
where , , and are just different notations for expressing an inner product between two elements in a Hilbert space (or for the first three, in any inner product space). Notational confusion arises when identifying and with and respectively. This is because of literal symbolic substitutions. Let and let . This gives

One ignores the parentheses and removes the double bars. Some properties of this notation are convenient since we are dealing with linear operators and composition acts like a ring multiplication.

Moreover, mathematicians usually write the dual entity not at the first place, as the physicists do, but at the second one, and they usually use not an asterisk but an overline (which the physicists reserve for averages and the Dirac spinor adjoint) to denote complex conjugate numbers; i.e., for scalar products mathematicians usually write
whereas physicists would write for the same quantity





</doc>
<doc id="4543" url="https://en.wikipedia.org/wiki?curid=4543" title="Blue">
Blue

Blue is one of the three primary colours of pigments in painting and traditional colour theory, as well as in the RGB colour model. It lies between violet and green on the spectrum of visible light. The eye perceives blue when observing light with a dominant wavelength between approximately 450 and 495 nanometres. Most blues contain a slight mixture of other colors; azure contains some green, while ultramarine contains some violet. The clear daytime sky and the deep sea appear blue because of an optical effect known as Rayleigh scattering. An optical effect called Tyndall scattering explains blue eyes. Distant objects appear more blue because of another optical effect called atmospheric perspective.
Blue has been an important colour in art and decoration since ancient times. The semi-precious stone lapis lazuli was used in ancient Egypt for jewellery and ornament and later, in the Renaissance, to make the pigment ultramarine, the most expensive of all pigments. In the eighth century Chinese artists used cobalt blue to colour fine blue and white porcelain. In the Middle Ages, European artists used it in the windows of Cathedrals. Europeans wore clothing coloured with the vegetable dye woad until it was replaced by the finer indigo from America. In the 19th century, synthetic blue dyes and pigments gradually replaced mineral pigments and synthetic dyes. Dark blue became a common colour for military uniforms and later, in the late 20th century, for business suits. Because blue has commonly been associated with harmony, it was chosen as the colour of the flags of the United Nations and the European Union.

Surveys in the US and Europe show that blue is the colour most commonly associated with harmony, faithfulness, confidence, distance, infinity, the imagination, cold, and sometimes with sadness. In US and European public opinion polls it is the most popular colour, chosen by almost half of both men and women as their favourite colour. The same surveys also showed that blue was the colour most associated with the masculine, just ahead of black, and was also the colour most associated with intelligence, knowledge, calm and concentration.

Blue is the colour of light between violet and green on the visible spectrum. Hues of blue include indigo and ultramarine, closer to violet; pure blue, without any mixture of other colours; Cyan, which is midway in the spectrum between blue and green, and the other blue-greens turquoise, teal, and aquamarine.

Blue also varies in shade or tint; darker shades of blue contain black or grey, while lighter tints contain white. Darker shades of blue include ultramarine, cobalt blue, navy blue, and Prussian blue; while lighter tints include sky blue, azure, and Egyptian blue. (For a more complete list see the List of colours).

Blue pigments were originally made from minerals such as lapis lazuli, cobalt and azurite, and blue dyes were made from plants; usually woad in Europe, and "Indigofera tinctoria", or true indigo, in Asia and Africa. Today most blue pigments and dyes are made by a chemical process.
The modern English word "blue" comes from Middle English "bleu" or "blewe", from the Old French "bleu", a word of Germanic origin, related to the Old High German word "blao". In heraldry, the word azure is used for blue.

In Russian and some other languages, there is no single word for blue, but rather different words for light blue (голубой, goluboy) and dark blue (синий, siniy). See Colour term.

Several languages, including Japanese, Thai, Korean, and Lakota Sioux, use the same word to describe blue and green. For example, in Vietnamese the colour of both tree leaves and the sky is "xanh". In Japanese, the word for blue (青 ao) is often used for colours that English speakers would refer to as green, such as the colour of a traffic signal meaning "go". (For more on this subject, see Distinguishing blue from green in language)

Linguistic research indicates that languages do not begin by having a word for the colour blue. Colour names often developed individually in natural languages, typically beginning with black and white (or dark and light), and then adding red, and only much later – usually as the last main category of color accepted in a language – adding the colour blue, probably when blue pigments could be manufactured reliably in the culture using that language.

Human eyes perceive blue when observing light which has a dominant wavelength of roughly 450–495 nanometres. Blues with a higher frequency and thus a shorter wavelength gradually look more violet, while those with a lower frequency and a longer wavelength gradually appear more green. Pure blue, in the middle, has a wavelength of 470 nanometres.

Isaac Newton included blue as one of the seven colours in his first description the visible spectrum, He chose seven colours because that was the number of notes in the musical scale, which he believed was related to the optical spectrum. He included indigo, the hue between blue and violet, as one of the separate colours, though today it is usually considered a hue of blue.

In painting and traditional colour theory, blue is one of the three primary colours of pigments (red, yellow, blue), which can be mixed to form a wide gamut of colours. Red and blue mixed together form violet, blue and yellow together form green. Mixing all three primary colours together produces a dark grey. From the Renaissance onwards, painters used this system to create their colours. (See RYB colour system.)

The RYB model was used for colour printing by Jacob Christoph Le Blon as early as 1725. Later, printers discovered that more accurate colours could be created by using combinations of magenta, cyan, yellow and black ink, put onto separate inked plates and then overlaid one at a time onto paper. This method could produce almost all the colours in the spectrum with reasonable accuracy.

In the 19th century the Scottish physicist James Clerk Maxwell found a new way of explaining colours, by the wavelength of their light. He showed that white light could be created by combining red, blue and green light, and that virtually all colours could be made by different combinations of these three colours. His idea, called additive colour or the RGB colour model, is used today to create colours on televisions and computer screens. The screen is covered by tiny pixels, each with three fluorescent elements for creating red, green and blue light. If the red, blue and green elements all glow at once, the pixel looks white. As the screen is scanned from behind with electrons, each pixel creates its own designated colour, composing a complete picture on the screen.

On the HSV colour wheel, the complement of blue is yellow; that is, a colour corresponding to an equal mixture of red and green light. On a colour wheel based on traditional colour theory (RYB) where blue was considered a primary colour, its complementary colour is considered to be orange (based on the Munsell colour wheel).

Blue pigments were made from minerals, especially lapis lazuli and azurite (. These minerals were crushed, ground into powder, and then mixed with a quick-drying binding agent, such as egg yolk (tempera painting); or with a slow-drying oil, such as linseed oil, for oil painting. To make blue stained glass, cobalt blue (cobalt(II) aluminate: )pigment was mixed with the glass. Other common blue pigments made from minerals are ultramarine (), cerulean blue (primarily cobalt (II) stanate: ), and Prussian blue (milori blue: primarily ).

Natural dyes to colour cloth and tapestries were made from plants. Woad and true indigo were used to produce indigo dye used to colour fabrics blue or indigo. Since the 18th century, natural blue dyes have largely been replaced by synthetic dyes.
"Reflex blue" used to be the name of a common blue pigment in ink manufacturing. 
In the 1960s, the name was adopted into the proprietary Pantone Matching System (PMS) to refer to this specific pigment. Pantone "Reflex Blue" has the particularity of being identified only by this name, and not by a number code.


Of the colours in the visible spectrum of light, blue has a very short wavelength, while red has the longest wavelength. When sunlight passes through the atmosphere, the blue wavelengths are scattered more widely by the oxygen and nitrogen molecules, and more blue comes to our eyes. This effect is called Rayleigh scattering, after Lord Rayleigh, the British physicist who discovered it. It was confirmed by Albert Einstein in 1911.

Near sunrise and sunset, most of the light we see comes in nearly tangent to the Earth's surface, so that the light's path through the atmosphere is so long that much of the blue and even green light is scattered out, leaving the sun rays and the clouds it illuminates red. Therefore, when looking at the sunset and sunrise, the colour red is more perceptible than any of the other colours.

The sea is seen as blue for largely the same reason: the water absorbs the longer wavelengths of red and reflects and scatters the blue, which comes to the eye of the viewer. The colour of the sea is also affected by the colour of the sky, reflected by particles in the water; and by algae and plant life in the water, which can make it look green; or by sediment, which can make it look brown.

The farther away an object is, the more blue it often appears to the eye. For example, mountains in the distance often appear blue. This is the effect of atmospheric perspective; the farther an object is away from the viewer, the less contrast there is between the object and its background colour, which is usually blue. In a painting where different parts of the composition are blue, green and red, the blue will appear to be more distant, and the red closer to the viewer. The cooler a colour is, the more distant it seems.

•A blue giant is the largest type of stars. A blue supergiant is even bigger.

Blue eyes do not actually contain any blue pigment. Eye colour is determined by two factors: the pigmentation of the eye's iris and the scattering of light by the turbid medium in the stroma of the iris. In humans, the pigmentation of the iris varies from light brown to black. The appearance of blue, green, and hazel eyes results from the Rayleigh scattering of light in the stroma, an optical effect similar to what accounts for the blueness of the sky. The irises of the eyes of people with blue eyes contain less dark melanin than those of people with brown eyes, which means that they absorb less short-wavelength blue light, which is instead reflected out to the viewer. Eye colour also varies depending on the lighting conditions, especially for lighter-coloured eyes.

Blue eyes are most common in Ireland, the Baltic Sea area and Northern Europe, and are also found in Eastern, Central, and Southern Europe. Blue eyes are also found in parts of Western Asia, most notably in Afghanistan, Syria, Iraq, and Iran. In Estonia, 99% of people have blue eyes. In Denmark 30 years ago, only 8% of the population had brown eyes, though through immigration, today that number is about 11%. In Germany, about 75% have blue eyes.

In the United States, as of 2006, one out of every six people, or 16.6% of the total population, and 22.3% of the white population, have blue eyes, compared with about half of Americans born in 1900, and a third of Americans born in 1950. Blue eyes are becoming less common among American children. In the US, boys are 3–5 per cent more likely to have blue eyes than girls.

Lasers emitting in the blue region of the spectrum became widely available to the public in 2010 with the release of inexpensive high-powered 445-447 nm laser diode technology. Previously the blue wavelengths were accessible only through DPSS which are comparatively expensive and inefficient, however these technologies are still widely used by the scientific community for applications including optogenetics, Raman spectroscopy, and particle image velocimetry, due to their superior beam quality. Blue gas lasers are also still commonly used for holography, DNA sequencing, optical pumping, and other scientific and medical applications.

Blue was a latecomer among colours used in art and decoration, as well as language and literature. Reds, blacks, browns, and ochres are found in cave paintings from the Upper Paleolithic period, but not blue. Blue was also not used for dyeing fabric until long after red, ochre, pink and purple. This is probably due to the perennial difficulty of making good blue dyes and pigments. The earliest known blue dyes were made from plants – woad in Europe, indigo in Asia and Africa, while blue pigments were made from minerals, usually either lapis lazuli or azurite.

Lapis lazuli, a semi-precious stone, has been mined in Afghanistan for more than three thousand years, and was exported to all parts of the ancient world. In Iran and Mesopotamia, it was used to make jewellery and vessels. In Egypt, it was used for the eyebrows on the funeral mask of King Tutankhamun (1341–1323 BC). Importing lapis lazuli by caravan across the desert from Afghanistan to Egypt was very expensive. Beginning in about 2500 BC, the ancient Egyptians began to produce their own blue pigment known as Egyptian blue by grinding silica, lime, copper, and alkalai, and heating it to . This is considered the first synthetic pigment. Egyptian blue was used to paint wood, papyrus and canvas, and was used to colour a glaze to make faience beads, inlays, and pots. It was particularly used in funeral statuary and figurines and in tomb paintings. Blue was considered a beneficial colour which would protect the dead against evil in the afterlife. Blue dye was also used to colour the cloth in which mummies were wrapped.

In Egypt blue was associated with the sky and with divinity. The Egyptian god Amun could make his skin blue so that he could fly, invisible, across the sky. Blue could also protect against evil; many people around the Mediterranean still wear a blue amulet, representing the eye of God, to protect them from misfortune. Blue glass was manufactured in Mesopotamia and Egypt as early as 2500 BC, using the same copper ingredients as Egyptian blue pigment. They also added cobalt, which produced a deeper blue, the same blue produced in the Middle Ages in the stained glass windows of the cathedrals of Saint-Denis and Chartres. The Ishtar Gate of ancient Babylon (604–562 BC) was decorated with deep blue glazed bricks used as a background for pictures of lions, dragons and aurochs.

The ancient Greeks classified colours by whether they were light or dark, rather than by their hue. The Greek word for dark blue, "kyaneos", could also mean dark green, violet, black or brown. The ancient Greek word for a light blue, "glaukos", also could mean light green, grey, or yellow. The Greeks imported indigo dye from India, calling it indikon. They used Egyptian blue in the wall paintings of Knossos, in Crete, (2100 BC). It was not one of the four primary colours for Greek painting described by Pliny the Elder (red, yellow, black, and white), but nonetheless it was used as a background colour behind the friezes on Greek temples and to colour the beards of Greek statues.

The Romans also imported indigo dye, but blue was the colour of working class clothing; the nobles and rich wore white, black, red or violet. Blue was considered the colour of mourning, and the colour of barbarians. Julius Caesar reported that the Celts and Germans dyed their faces blue to frighten their enemies, and tinted their hair blue when they grew old. Nonetheless, the Romans made extensive use of blue for decoration. According to Vitruvius, they made dark blue pigment from indigo, and imported Egyptian blue pigment. The walls of Roman villas in Pompeii had frescoes of brilliant blue skies, and blue pigments were found in the shops of colour merchants. The Romans had many different words for varieties of blue, including "caeruleus", "caesius", "glaucus", "cyaneus", "lividus", "venetus", "aerius", and "ferreus", but two words, both of foreign origin, became the most enduring; "blavus", from the Germanic word "blau", which eventually became "bleu" or blue; and "azureus", from the Arabic word "lazaward", which became azure.

Dark blue was widely used in the decoration of churches in the Byzantine Empire. In Byzantine art Christ and the Virgin Mary usually wore dark blue or purple. Blue was used as a background colour representing the sky in the magnificent mosaics which decorated Byzantine churches.

In the Islamic world, blue was of secondary importance to green, believed to be the favourite colour of the Prophet Mohammed. At certain times in Moorish Spain and other parts of the Islamic world, blue was the colour worn by Christians and Jews, because only Muslims were allowed to wear white and green. Dark blue and turquoise decorative tiles were widely used to decorate the facades and interiors of mosques and palaces from Spain to Central Asia. Lapis lazuli pigment was also used to create the rich blues in Persian miniatures.

In the art and life of Europe during the early Middle Ages, blue played a minor role. The nobility wore red or purple, while only the poor wore blue clothing, coloured with poor-quality dyes made from the woad plant. Blue played no part in the rich costumes of the clergy or the architecture or decoration of churches. This changed dramatically between 1130 and 1140 in Paris, when the Abbe Suger rebuilt the Saint Denis Basilica. He installed stained glass windows coloured with cobalt, which, combined with the light from the red glass, filled the church with a bluish violet light. The church became the marvel of the Christian world, and the colour became known as the "bleu de Saint-Denis". In the years that followed even more elegant blue stained glass windows were installed in other churches, including at Chartres Cathedral and Sainte-Chapelle in Paris.

Another important factor in the increased prestige of the colour blue in the 12th century was the veneration of the Virgin Mary, and a change in the colours used to depict her clothing. In earlier centuries her robes had usually been painted in sombre black, grey, violet, dark green or dark blue. In the 12th century the Roma Catholic Church dictated that painters in Italy (and the rest of Europe consequently) to paint the Virgin Mary with the new most expensive pigment imported from Asia; ultramarine. Blue became associated with holiness, humility and virtue.

Ultramarine was made from lapis lazuli, from the mines of Badakshan, in the mountains of Afghanistan, near the source of the Oxus River. The mines were visited by Marco Polo in about 1271; he reported, "here is found a high mountain from which they extract the finest and most beautiful of blues." Ground lapis was used in Byzantine manuscripts as early as the 6th century, but it was impure and varied greatly in colour. Ultramarine refined out the impurities through a long and difficult process, creating a rich and deep blue. It was called "bleu outremer" in French and "blu oltremare" in Italian, since it came from the other side of the sea. It cost far more than any other colour, and it became the luxury colour for the Kings and Princes of Europe.

King Louis IX of France, better known as Saint Louis (1214–1270), became the first king of France to regularly dress in blue. This was copied by other nobles. Paintings of the mythical King Arthur began to show him dressed in blue. The coat of arms of the kings of France became an azure or light blue shield, sprinkled with golden fleur-de-lis or lilies. Blue had come from obscurity to become the royal colour.

Once blue became the colour of the king, it also became the colour of the wealthy and powerful in Europe. In the Middle Ages in France and to some extent in Italy, the dyeing of blue cloth was subject to license from the crown or state. In Italy, the dyeing of blue was assigned to a specific guild, the "tintori di guado," and could not be done by anyone else without severe penalty. The wearing of blue implied some dignity and some wealth.

Besides ultramarine, several other blues were widely used in the Middle Ages and later in the Renaissance. Azurite, a form of copper carbonate, was often used as a substitute for ultramarine. The Romans used it under the name lapis armenius, or Armenian stone. The British called it azure of Amayne, or German azure. The Germans themselves called it bergblau, or mountain stone. It was mined in France, Hungary, Spain and Germany, and it made a pale blue with a hint of green, which was ideal for painting skies. It was a favourite background colour of the German painter Albrecht Dürer.

Another blue often used in the Middle Ages was called tournesol or folium. It was made from the plant Crozophora tinctoria, which grew in the south of France. It made a fine transparent blue valued in medieval manuscripts.

Another common blue pigment was smalt, which was made by grinding blue cobalt glass into a fine powder. It made a deep violet blue similar to ultramarine, and was vivid in frescoes, but it lost some of its brilliance in oil paintings. It became especially popular in the 17th century, when ultramarine was difficult to obtain. It was employed at times by Titian, Tintoretto, Veronese, El Greco, Van Dyck, Rubens and Rembrandt.

In the Renaissance, a revolution occurred in painting; artists began to paint the world as it was actually seen, with perspective, depth, shadows, and light from a single source. Artists had to adapt their use of blue to the new rules. In medieval paintings, blue was used to attract the attention of the viewer to the Virgin Mary, and identify her. In Renaissance paintings, artists tried to create harmonies between blue and red, lightening the blue with lead white paint and adding shadows and highlights. Raphael was a master of this technique, carefully balancing the reds and the blues so no one colour dominated the picture.

Ultramarine was the most prestigious blue of the Renaissance, and patrons sometimes specified that it be used in paintings they commissioned. The contract for the "Madone des Harpies" by Andrea del Sarto (1514) required that the robe of the Virgin Mary be coloured with ultramarine costing "at least five good florins an ounce." Good ultramarine was more expensive than gold; in 1508 the German painter Albrecht Dürer reported in a letter that he had paid twelve ducats – the equivalent of forty-one grams of gold – for just thirty grams of ultramarine.

Often painters or clients saved money by using less expensive blues, such as azurite smalt, or pigments made with indigo, but this sometimes caused problems. Pigments made from azurite were less expensive, but tended to turn dark and green with time. An example is the robe of the Virgin Mary in The "Madonna Enthroned with Saints" by Raphael in the Metropolitan Museum in New York. The Virgin Mary's azurite blue robe has degraded into a greenish-black.

The introduction of oil painting changed the way colours looked and how they were used. Ultramarine pigment, for instance, was much darker when used in oil painting than when used in tempera painting, in frescoes. To balance their colours, Renaissance artists like Raphael added white to lighten the ultramarine. The sombre dark blue robe of the Virgin Mary became a brilliant sky blue. Titian created his rich blues by using many thin glazes of paint of different blues and violets which allowed the light to pass through, which made a complex and luminous colour, like stained glass. He also used layers of finely ground or coarsely ground ultramarine, which gave subtle variations to the blue.
In about the 9th century, Chinese artisans abandoned the Han blue colour they had used for centuries, and began to use cobalt blue, made with cobalt salts of alumina, to manufacture fine blue and white porcelain, The plates and vases were shaped, dried, the paint applied with a brush, covered with a clear glaze, then fired at a high temperature. Beginning in the 14th century, this type of porcelain was exported in large quantity to Europe where it inspired a whole style of art, called Chinoiserie. European courts tried for many years to imitate Chinese blue and white porcelain, but only succeeded in the 18th century after a missionary brought the secret back from China.

Other famous white and blue patterns appeared in Delft, Meissen, Staffordshire, and Saint Petersburg, Russia.
While blue was an expensive and prestigious colour in European painting, it became a common colour for clothing during the Renaissance. The rise of the colour blue in fashion in the 12th and 13th centuries led to a blue dye industry in several cities, notably Amiens, Toulouse, and Erfurt. They made a dye called pastel from woad, a plant common in Europe, which had been used to make blue dye by the Celts and German tribes. Blue became a colour worn by domestics and artisans, not just nobles. In 1570, when Pope Pius V listed the colours that could be used for ecclesiastical dress and for altar decoration, he excluded blue, because he considered it too common.

The process of making blue with woad was long and noxious – it involved soaking the leaves of the plant for from three days to a week in human urine, ideally urine from men who had been drinking a great deal of alcohol, which was said to improve the colour. The fabric was then soaked for a day in the resulting mixture, then put out in the sun, where as it dried it turned blue.

The pastel industry was threatened in the 15th century by the arrival from India of the same dye (indigo), obtained from a shrub widely grown in Asia. The Asian indigo dye precursors is more readily obtained. In 1498, Vasco de Gama opened a trade route to import indigo from India to Europe. In India, the indigo leaves were soaked in water, fermented, pressed into cakes, dried into bricks, then carried to the ports London, Marseille, Genoa, and Bruges. Later, in the 17th century, the British, Spanish, and Dutch established indigo plantations in Jamaica, South Carolina, the Virgin Islands and South America, and began to import American indigo to Europe.

The countries with large and prosperous pastel industries tried to block the use of indigo. The German government outlawed the use of indigo in 1577, describing it as a "pernicious, deceitful and corrosive substance, the Devil's dye." In France, Henry IV, in an edict of 1609, forbade under pain of death the use of "the false and pernicious Indian drug". It was forbidden in England until 1611, when British traders established their own indigo industry in India and began to import it into Europe.

The efforts to block indigo were in vain; the quality of indigo blue was too high and the price too low for pastel made from woad to compete. In 1737 both the French and German governments finally allowed the use of indigo. This ruined the dye industries in Toulouse and the other cities that produced pastel, but created a thriving new indigo commerce to seaports such as Bordeaux, Nantes and Marseille.

Another war of the blues took place at the end of the 19th century, between indigo and synthetic indigo, discovered in 1868 by the German chemist Johann Friedrich Wilhelm Adolf von Baeyer. The German chemical firm BASF put the new dye on the market in 1897, in direct competition with the British-run indigo industry in India, which produced most of the world's indigo. In 1897 Britain sold ten thousand tons of natural indigo on the world market, while BASF sold six hundred tons of synthetic indigo. The British industry cut prices and reduced the salaries of its workers, but it was unable to compete; the synthetic indigo was more pure, made a more lasting blue, and was not dependent upon good or bad harvests. In 1911, India sold only 660 tons of natural indigo, while BASF sold 22,000 tons of synthetic indigo. In 2002, more than 38,000 tons of synthetic indigo was produced, often for the production of blue jeans.

In the 17th century, Frederick William, Elector of Brandenburg, was one of the first rulers to give his army blue uniforms. The reasons were economic; the German states were trying to protect their pastel dye industry against competition from imported indigo dye. When Brandenburg became the Kingdom of Prussia in 1701, the uniform colour was adopted by the Prussian army. Most German soldiers wore dark blue uniforms until the First World War, with the exception of the Bavarians, who wore light blue.

Thanks in part to the availability of indigo dye, the 18th century saw the widespread use of blue military uniforms. Prior to 1748, British naval officers simply wore upper-class civilian clothing and wigs. In 1748, the British uniform for naval officers was officially established as an embroidered coat of the colour then called marine blue, now known as navy blue. When the Continental Navy of the United States was created in 1775, it largely copied the British uniform and colour.

In the late 18th century, the blue uniform became a symbol of liberty and revolution. In October 1774, even before the United States declared its independence, George Mason and one hundred Virginia neighbours of George Washington organised a voluntary militia unit (the Fairfax County Independent Company of Volunteers) and elected Washington the honorary commander. For their uniforms they chose blue and buff, the colours of the Whig Party, the opposition party in England, whose policies were supported by George Washington and many other patriots in the American colonies.

When the Continental Army was established in 1775 at the outbreak of the American Revolution, the first Continental Congress declared that the official uniform colour would be brown, but this was not popular with many militias, whose officers were already wearing blue. In 1778 the Congress asked George Washington to design a new uniform, and in 1779 Washington made the official colour of all uniforms blue and buff. Blue continued to be the colour of the field uniform of the US Army until 1902, and is still the colour of the dress uniform.

In France the Gardes Françaises, the elite regiment which protected Louis XVI, wore dark blue uniforms with red trim. In 1789, the soldiers gradually changed their allegiance from the king to the people, and they played a leading role in the storming of the Bastille. After the fall of Bastille, a new armed force, the Garde Nationale, was formed under the command of the Marquis de Lafayette, who had served with George Washington in America. Lafayette gave the Garde Nationale dark blue uniforms similar to those of the Continental Army. Blue became the colour of the revolutionary armies, opposed to the white uniforms of the Royalists and the Austrians.

Napoleon Bonaparte abandoned many of the doctrines of the French Revolution but he kept blue as the uniform colour for his army, although he had great difficulty obtaining the blue dye, since the British controlled the seas and blocked the importation of indigo to France. Napoleon was forced to dye uniforms with woad, which had an inferior blue colour. The French army wore a dark blue uniform coat with red trousers until 1915, when it was found to be a too visible target on the battlefields of World War I. It was replaced with uniforms of a light blue-grey colour called horizon blue.

Blue was the colour of liberty and revolution in the 18th century, but in the 19th it increasingly became the colour of government authority, the uniform colour of policemen and other public servants. It was considered serious and authoritative, without being menacing. In 1829, when Robert Peel created the first London Metropolitan Police, he made the colour of the uniform jacket a dark, almost black blue, to make the policemen look different from soldiers, who until then had patrolled the streets. The traditional blue jacket with silver buttons of the London "bobbie" was not abandoned until the mid-1990s, when it was replaced by a light blue shirt and a jumper or sweater of the colour officially known as NATO blue.

The New York City Police Department, modelled after the London Metropolitan Police, was created in 1844, and in 1853, they were officially given a navy blue uniform, the colour they wear today.

Navy blue is one of the most popular school uniform colors, with the Toronto Catholic District School Board adopting a dress code policy which requires students system-wide to wear white tops and navy blue bottoms.
During the 17th and 18th centuries, chemists in Europe tried to discover a way to create synthetic blue pigments, avoiding the expense of importing and grinding lapis lazuli, azurite and other minerals. The Egyptians had created a synthetic colour, Egyptian blue, three thousand years BC, but the formula had been lost. The Chinese had also created synthetic pigments, but the formula was not known in the west.

In 1709 a German druggist and pigment maker named Johann Jacob Diesbach accidentally discovered a new blue while experimenting with potassium and iron sulphides. The new colour was first called Berlin blue, but later became known as Prussian blue. By 1710 it was being used by the French painter Antoine Watteau, and later his successor Nicolas Lancret. It became immensely popular for the manufacture of wallpaper, and in the 19th century was widely used by French impressionist painters.

Beginning in the 1820s, Prussian blue was imported into Japan through the port of Nagasaki. It was called "bero-ai", or Berlin blue, and it became popular because it did not fade like traditional Japanese blue pigment, "ai-gami", made from the dayflower. Prussian blue was used by both Hokusai, in his famous wave paintings, and Hiroshige.

In 1824 the Societé pour l'Encouragement d'Industrie in France offered a prize for the invention of an artificial ultramarine which could rival the natural colour made from lapis lazuli. The prize was won in 1826 by a chemist named Jean Baptiste Guimet, but he refused to reveal the formula of his colour. In 1828, another scientist, Christian Gmelin then a professor of chemistry in Tübingen, found the process and published his formula. This was the beginning of new industry to manufacture artificial ultramarine, which eventually almost completely replaced the natural product.

In 1878 a German chemist named a. Von Baeyer discovered a synthetic substitute for indigotine, the active ingredient of indigo. This product gradually replaced natural indigo, and after the end of the First World War, it brought an end to the trade of indigo from the East and West Indies.

In 1901 a new synthetic blue dye, called Indanthrone blue, was invented, which had even greater resistance to fading during washing or in the sun. This dye gradually replaced artificial indigo, whose production ceased in about 1970. Today almost all blue clothing is dyed with an indanthrone blue.
The invention of new synthetic pigments in the 18th and 19th centuries considerably brightened and expanded the palette of painters. J.M.W. Turner experimented with the new cobalt blue, and of the twenty colours most used by the Impressionists, twelve were new and synthetic colours, including cobalt blue, ultramarine and cerulean blue.

Another important influence on painting in the 19th century was the theory of complementary colours, developed by the French chemist Michel Eugene Chevreul in 1828 and published in 1839. He demonstrated that placing complementary colours, such as blue and yellow-orange or ultramarine and yellow, next to each other heightened the intensity of each colour "to the apogee of their tonality." In 1879 an American physicist, Ogden Rood, published a book charting the complementary colours of each colour in the spectrum. This principle of painting was used by Claude Monet in his "Impression – Sunrise – Fog" (1872), where he put a vivid blue next to a bright orange sun, (1872) and in "Régate à Argenteuil" (1872), where he painted an orange sun against blue water. The colours brighten each other. Renoir used the same contrast of cobalt blue water and an orange sun in "Canotage sur la Seine" (1879–1880). Both Monet and Renoir liked to use pure colours, without any blending.

Monet and the impressionists were among the first to observe that shadows were full of colour. In his "La Gare Saint-Lazare", the grey smoke, vapour and dark shadows are actually composed of mixtures of bright pigment, including cobalt blue, cerulean blue, synthetic ultramarine, emerald green, Guillet green, chrome yellow, vermilion and ecarlate red. Blue was a favourite colour of the impressionist painters, who used it not just to depict nature but to create moods, feelings and atmospheres. Cobalt blue, a pigment of cobalt oxide-aluminium oxide, was a favourite of Auguste Renoir and Vincent van Gogh. It was similar to smalt, a pigment used for centuries to make blue glass, but it was much improved by the French chemist Louis Jacques Thénard, who introduced it in 1802. It was very stable but extremely expensive. Van Gogh wrote to his brother Theo, "'Cobalt [blue] is a divine colour and there is nothing so beautiful for putting atmosphere around things ..."

Van Gogh described to his brother Theo how he composed a sky: "The dark blue sky is spotted with clouds of an even darker blue than the fundamental blue of intense cobalt, and others of a lighter blue, like the bluish white of the Milky Way ... the sea was very dark ultramarine, the shore a sort of violet and of light red as I see it, and on the dunes, a few bushes of prussian blue."
Blue had first become the high fashion colour of the wealthy and powerful in Europe in the 13th century, when it was worn by Louis IX of France, better known as Saint Louis (1214-1270). Wearing blue implied dignity and wealth, and blue clothing was restricted to the nobility. However, blue was replaced by black as the power colour in the 14th century, when European princes, and then merchants and bankers, wanted to show their seriousness, dignity and devoutness (see Black).

Blue gradually returned to court fashion in the 17th century, as part of a palette of peacock-bright colours shown off in extremely elaborate costumes. The modern blue business suit has its roots in England in the middle of the 17th century. Following the London plague of 1665 and the London fire of 1666, King Charles II of England ordered that his courtiers wear simple coats, waistcoats and breeches, and the palette of colours became blue, grey, white and buff. Widely imitated, this style of men's fashion became almost a uniform of the London merchant class and the English country gentleman.

During the American Revolution, the leader of the Whig Party in England, Charles James Fox, wore a blue coat and buff waistcoat and breeches, the colours of the Whig Party and of the uniform of George Washington, whose principles he supported. The men's suit followed the basic form of the military uniforms of the time, particularly the uniforms of the cavalry.

In the early 19th century, during the Regency of the future King George IV, the blue suit was revolutionised by a courtier named George Beau Brummel. Brummel created a suit that closely fitted the human form. The new style had a long tail coat cut to fit the body and long tight trousers to replace the knee-length breeches and stockings of the previous century. He used plain colours, such as blue and grey, to concentrate attention on the form of the body, not the clothes. Brummel observed, "If people turn to look at you in the street, you are not well dressed." This fashion was adopted by the Prince Regent, then by London society and the upper classes. Originally the coat and trousers were different colours, but in the 19th century the suit of a single colour became fashionable. By the late 19th century the black suit had become the uniform of businessmen in England and America. In the 20th century, the black suit was largely replaced by the dark blue or grey suit.

At the beginning of the 20th century, many artists recognised the emotional power of blue, and made it the central element of paintings. During his Blue Period (1901–1904) Pablo Picasso used blue and green, with hardly any warm colours, to create a melancholy mood. In Russia, the symbolist painter Pavel Kuznetsov and the Blue Rose art group (1906–1908) used blue to create a fantastic and exotic atmosphere. In Germany, Wassily Kandinsky and other Russian émigrés formed the art group called Der Blaue Reiter (The Blue Rider), and used blue to symbolise spirituality and eternity. Henri Matisse used intense blues to express the emotions he wanted viewers to feel. Matisse wrote, "A certain blue penetrates your soul."

In the art of the second half of the 20th century, painters of the abstract expressionist movement began to use blue and other colours in pure form, without any attempt to represent anything, to inspire ideas and emotions. Painter Mark Rothko observed that colour was "only an instrument;" his interest was "in expressing human emotions tragedy, ecstasy, doom, and so on."

In fashion blue, particularly dark blue, was seen as a colour which was serious but not grim. In the mid-20th century, blue passed black as the most common colour of men's business suits, the costume usually worn by political and business leaders. Public opinion polls in the United States and Europe showed that blue was the favourite colour of over fifty per cent of respondents. Green was far behind with twenty per cent, while white and red received about eight per cent each.

In 1873 a German immigrant in San Francisco, Levi Strauss, invented a sturdy kind of work trousers, made of denim fabric and coloured with indigo dye, called blue jeans. In 1935, they were raised to the level of high fashion by Vogue magazine. Beginning in the 1950s, they became an essential part of uniform of young people in the United States, Europe, and around the world.

Blue was also seen as a colour which was authoritative without being threatening. Following the Second World War, blue was adopted as the colour of important international organisations, including the United Nations, the Council of Europe, UNESCO, the European Union, and NATO. United Nations peacekeepers wear blue helmets to stress their peacekeeping role. Blue is used by the NATO Military Symbols for Land Based Systems to denote friendly forces, hence the term "blue on blue" for friendly fire, and Blue Force Tracking for location of friendly units. The People's Liberation Army of China (formerly known as the "Red Army") uses the term "Blue Army" to refer to hostile forces during exercises.

The 20th century saw the invention of new ways of creating blue, such as chemiluminescence, making blue light through a chemical reaction.

In the 20th century, it also became possible to own your own colour of blue. The French artist Yves Klein, with the help of a French paint dealer, created a specific blue called International Klein blue, which he patented. It was made of ultramarine combined with a resin called Rhodopa, which gave it a particularly brilliant colour. The baseball team the Los Angeles Dodgers developed its own blue, called Dodger blue, and several American universities invented new blues for their colours.

With the dawn of the World Wide Web, blue has become the standard colour for hyperlinks in graphic browsers (though in most browsers links turn purple if you visit their target), to make their presence within text obvious to readers.

Various shades of blue are used as the national colours for many nations.

Blue was first used as a gender signifier just prior to World War I (for either girls or boys), and first established as a male gender signifier in the 1940s.



Many sporting teams make blue their official colour, or use it as detail on kit of a different colour. In addition, the colour is present on the logos of many sports associations. Along with red, blue is the most commonly used non-white colours for teams.


In international association football, blue is a common colour on kits, as a majority of nations wear the colours of their national flag. A notable exception is four-time FIFA World Cup winners Italy, who wear a blue kit based on the "Azzuro Savoia" (Savoy blue) of the royal House of Savoy which unified the Italian states. The team themselves are known as "Gli Azzurri" (the Blues). Another World Cup winning nation with a blue shirt is France, who are known as "Les Bleus" (the Blues). Two neighbouring countries with two World Cup victories each, Argentina and Uruguay wear a light blue shirt, the former with white stripes. Uruguay are known as the "La Celeste", Spanish for 'the sky blue one', while Argentina are known as "Los Albicelestes", Spanish for 'the sky blue and whites'.

Blue features on the logo of football's governing body FIFA, as well as featuring highly in the design of their website. The European governing body of football, UEFA, uses two tones of blue to create a map of Europe in the centre of their logo. The Asian Football Confederation, Oceania Football Confederation and CONCACAF (the governing body of football in North and Central America and the Caribbean) use blue text on their logos.

In Major League Baseball, the premier baseball league in the United States and Canada, blue is one of the three colours, along with white and red, on the league's official logo. A team from Toronto, Ontario are nicknamed the Blue Jays. Seventeen other teams either regularly feature blue hats or utilise the colour in their uniforms.

The National Basketball Association, the premier basketball league in the United States and Canada, also has blue as one of the colours on their logo, along with red and white also, as did its female equivalent, the WNBA, until March 28, 2011, when the latter adopted an orange and white logo. Former NBA player Theodore Edwards was nicknamed "Blue". Fifteen NBA teams feature the colour in their uniforms.

The National Football League, the premier American football league in the United States, also uses blue as one of three colours, along with white and red, on their official logo. Thirteen NFL teams prominently feature the colour .

The National Hockey League, the premier Ice hockey league in Canada and the United States, uses blue on its official logo. Blue is the main colour of many teams in the league: the Buffalo Sabres, Columbus Blue Jackets, Edmonton Oilers, New York Islanders, New York Rangers, St. Louis Blues, Toronto Maple Leafs, Tampa Bay Lightning, Vancouver Canucks and the Winnipeg Jets.




</doc>
<doc id="4544" url="https://en.wikipedia.org/wiki?curid=4544" title="Blind Willie McTell">
Blind Willie McTell

Blind Willie McTell (born William Samuel McTier; May 5, 1898 – August 19, 1959) was a Piedmont blues and ragtime singer and guitarist. He played with a fluid, syncopated fingerstyle guitar technique, common among many exponents of Piedmont blues. Unlike his contemporaries, he came to use twelve-string guitars exclusively. McTell was also an adept slide guitarist, unusual among ragtime bluesmen. His vocal style, a smooth and often laid-back tenor, differed greatly from many of the harsher voices of Delta bluesmen such as Charley Patton. McTell performed in various musical styles, including blues, ragtime, religious music and hokum.

McTell was born in Thomson, Georgia. He learned to play the guitar in his early teens. He soon became a street performer in several Georgia cities, including Atlanta and Augusta, and first recorded in 1927 for Victor Records. He never produced a major hit record, but he had a prolific recording career with different labels and under different names in the 1920s and 1930s. In 1940, he was recorded by the folklorist John A. Lomax and Ruby Terrill Lomax for the folk song archive of the Library of Congress. He was active in the 1940s and 1950s, playing on the streets of Atlanta, often with his longtime associate Curley Weaver. Twice more he recorded professionally. His last recordings originated during an impromptu session recorded by an Atlanta record store owner in 1956. McTell died three years later, having suffered for years from diabetes and alcoholism. Despite his lack of commercial success, he was one of the few blues musicians of his generation who continued to actively play and record during the 1940s and 1950s. He did not live to see the American folk music revival, in which many other bluesmen were "rediscovered."

McTell's influence extended over a wide variety of artists, including the Allman Brothers Band, who covered his "Statesboro Blues," and Bob Dylan, who paid tribute to him in his 1983 song "Blind Willie McTell," the refrain of which is "And I know no one can sing the blues like Blind Willie McTell." Other artists influenced by McTell include Taj Mahal, Alvin Youngblood Hart, Ralph McTell, Chris Smither, Jack White, and the White Stripes.

He was born William Samuel McTier in Thomson, Georgia. Most sources give the date of his birth as 1898, but researchers Bob Eagle and Eric LeBlanc suggest 1903, on the basis of his entry in the 1910 census. McTell was born blind in one eye and lost his remaining vision by late childhood. He attended schools for the blind in Georgia, New York and Michigan and showed proficiency in music from an early age, first playing the harmonica and accordion, learning to read and write music in Braille, and turning to the six-string guitar in his early teens. His family background was rich in music; both of his parents and an uncle played the guitar. He was related to the bluesman and gospel pioneer Thomas A. Dorsey. McTell's father left the family when Willie was young. After his mother died, in the 1920s, he left his hometown and became an itinerant musician, or "songster." He began his recording career in 1927 for Victor Records in Atlanta.

McTell married Ruth Kate Williams, now better known as Kate McTell, in 1934. She accompanied him on stage and on several recordings before becoming a nurse in 1939. For most of their marriage, from 1942 until his death, they lived apart, she in Fort Gordon, near Augusta, and he working around Atlanta.

In the years before World War II, McTell traveled and performed widely, recording for several labels under different names: Blind Willie McTell (for Victor and Decca), Blind Sammie (for Columbia), Georgia Bill (for Okeh), Hot Shot Willie (for Victor), Blind Willie (for Vocalion and Bluebird), Barrelhouse Sammie (for Atlantic), and Pig & Whistle Red (for Regal). The appellation "Pig & Whistle" was a reference to a chain of barbecue restaurants in Atlanta; McTell often played for tips in the parking lot of a Pig 'n Whistle restaurant. He also played behind a nearby building that later became Ray Lee's Blue Lantern Lounge. Like Lead Belly, another songster who began his career as a street artist, McTell favored the somewhat unwieldy and unusual twelve-string guitar, whose greater volume made it suitable for outdoor playing.

In 1940 John A. Lomax and his wife, Ruby Terrill Lomax, a professor of classics at the University of Texas at Austin, interviewed and recorded McTell for the Archive of American Folk Song of the Library of Congress in a two-hour session held in their hotel room in Atlanta. These recordings document McTell's distinctive musical style, which bridges the gap between the raw country blues of the early part of the 20th century and the more conventionally melodious, ragtime-influenced East Coast Piedmont blues sound. The Lomaxes also elicited from the singer traditional songs (such as "The Boll Weevil" and "John Henry") and spirituals (such as "Amazing Grace"), which were not part of his usual commercial repertoire. In the interview, John A. Lomax is heard asking if McTell knows any "complaining" songs (an earlier term for protest songs), to which the singer replies somewhat uncomfortably and evasively that he does not. The Library of Congress paid McTell $10, the equivalent of $154.56 in 2011, for this two-hour session. The material from this 1940 session was issued in 1960 as an LP and later as a CD, under the somewhat misleading title "The Complete Library of Congress Recordings", notwithstanding the fact that it was truncated, in that it omitted some of John A. Lomax's interactions with the singer and entirely omitted the contributions of Ruby Terrill Lomax.

McTell recorded for Atlantic Records and Regal Records in 1949, but these recordings met with less commercial success than his previous works. He continued to perform around Atlanta, but his career was cut short by ill health, mostly due to diabetes and alcoholism. In 1956, an Atlanta record store manager, Edward Rhodes, discovered McTell playing in the street for quarters and enticed him with a bottle of corn liquor into his store, where he captured a few final performances on a tape recorder. These recordings were released posthumously by Prestige/Bluesville Records as "Last Session". Beginning in 1957, McTell was a preacher at Mt. Zion Baptist Church in Atlanta.

McTell died of a stroke in Milledgeville, Georgia, in 1959. He was buried at Jones Grove Church, near Thomson, Georgia, his birthplace. A fan paid to have a gravestone erected on his resting place. The name given on his gravestone is Willie Samuel McTier. He was inducted into the Blues Foundation's Blues Hall of Fame in 1981 and the Georgia Music Hall of Fame in 1990.

In his recording of "Statesboro Blues," he pronounces his surname "MacTell", with the stress on the first syllable.

One of McTell's most famous songs, "Statesboro Blues," was frequently covered by the Allman Brothers Band and was one of their earliest signature songs; it also contributes to Canned Heat's "Goin' Up the Country." A short list of some of the artists who have performed the song includes Taj Mahal, David Bromberg, Dave Van Ronk, The Devil Makes Three and Ralph McTell, who changed his name on account of liking the song. Ry Cooder covered McTell's "Married Man's a Fool" on his 1973 album, "Paradise and Lunch". Jack White, of the White Stripes considers McTell an influence; the White Stripes album "De Stijl" (2000) is dedicated to him and features a cover of his song "Southern Can Is Mine." The White Stripes also covered McTell's "Lord, Send Me an Angel", releasing it as a single in 2000. In 2013 Jack White's Third Man Records teamed up with Document Records to issue "The Complete Recorded Works in Chronological Order of Charley Patton, Blind Willie McTell and the Mississippi Sheiks".

Bob Dylan paid tribute to McTell on at least four occasions. In his 1965 song "Highway 61 Revisited," the second verse begins, "Georgia Sam he had a bloody nose," a reference to one of McTell's many recording names. Dylan's song "Blind Willie McTell" was recorded in 1983 and released in 1991 on "The Bootleg Series Volumes 1-3". Dylan also recorded covers of McTell's "Broke Down Engine" and "Delia" on his 1993 album, "World Gone Wrong"; Dylan's song "Po' Boy", on the album "Love and Theft" (2001), contains the lyric "had to go to Florida dodging them Georgia laws," which comes from McTell's "Kill It Kid."

The Bath-based band Kill It Kid is named after the song of the same title.

A blues bar in Atlanta is named after McTell and regularly features blues musicians and bands. The Blind Willie McTell Blues Festival is held annually in Thomson, Georgia.






</doc>
<doc id="4545" url="https://en.wikipedia.org/wiki?curid=4545" title="BDSM">
BDSM

BDSM is a variety of often erotic practices or roleplaying involving bondage, discipline, dominance and submission, sadomasochism, and other related interpersonal dynamics. Given the wide range of practices, some of which may be engaged in by people who do not consider themselves as practicing BDSM, inclusion in the BDSM community or subculture is usually dependent upon self-identification and shared experience.
The term "BDSM" is first recorded in a Usenet posting from 1991, and is interpreted as a combination of the abbreviations B/D (Bondage and Discipline), D/s (Dominance and submission), and S/M (Sadism and Masochism). BDSM is now used as a catch-all phrase covering a wide range of activities, forms of interpersonal relationships, and distinct subcultures. BDSM communities generally welcome anyone with a non-normative streak who identifies with the community; this may include cross-dressers, body modification enthusiasts, animal roleplayers, rubber fetishists, and others.

Activities and relationships within a BDSM context are often characterized by the participants taking on complementary, but unequal roles; thus, the idea of informed consent of both the partners is essential. The terms "submissive" and "dominant" are often used to distinguish these roles: the dominant partner ("dom") takes psychological control over the submissive ("sub"). The terms "top" and "bottom" are also used: the top is the instigator of an action while the bottom is the receiver of the action. The two sets of terms are subtly different: for example, someone may choose to act as bottom to another person, for example, by being whipped, purely recreationally, without any implication of being psychologically dominated by them, or a submissive may be ordered to massage their dominant partner. Despite the bottom performing the action and the top receiving they have not necessarily switched roles.

The abbreviations "sub" and "dom" are frequently used instead of "submissive" and "dominant". Sometimes the female-specific terms "mistress", "domme" or "dominatrix" are used to describe a dominant woman, instead of the gender-neutral term "dom". Individuals who can change between top/dominant and bottom/submissive roles—whether from relationship to relationship or within a given relationship—are known as "switches". The precise definition of roles and self-identification is a common subject of debate within the community.

"BDSM" is an umbrella term for certain kinds of erotic behavior between consenting adults. There are distinct subcultures under this umbrella term. Terminology for roles varies widely among the subcultures. "Top" and "dominant" are widely used for those partner(s) in the relationship or activity who are, respectively, the physically active or controlling participants. "Bottom" and "submissive" are widely used for those partner(s) in the relationship or activity who are, respectively, the physically receptive or controlled participants. The interaction between tops and bottoms—where physical or mental control of the bottom is surrendered to the top—is sometimes known as "power exchange", whether in the context of an encounter or a relationship.

BDSM actions can often take place during a specific period of time agreed to by both parties, referred to as "play", a "scene", or a "session". Participants usually derive pleasure from this, even though many of the practices—such as inflicting pain or humiliation or being restrained — would be unpleasant under other circumstances. Explicit sexual activity, such as sexual penetration, may occur within a session, but is not essential. Such explicit sexual interaction is, for legal reasons, seen only rarely in public play spaces, and it is sometimes specifically banned by the rules of a party or playspace. Whether it is a public "playspace"—ranging from a party at an established community dungeon to a hosted play "zone" at a nightclub or social event—the parameters of allowance can vary. Some have a policy of panties/nipple sticker for women (underwear for men) and some allow full nudity with explicit sexual interaction allowed.
The fundamental principles for the exercise of BDSM require that it should be performed with the informed consent of all involved parties. Since the 1980s, many practitioners and organizations have adopted the motto (originally from the statement of purpose of GMSMA—a gay SM activist organization) "safe, sane and consensual", commonly abbreviated as "SSC", which means that everything is based on safe activities, that all participants be of sufficiently sound/sane mind to consent, and that all participants do consent. It is mutual consent that makes a clear legal and ethical distinction between BDSM and such crimes as sexual assault or domestic violence.

Some BDSM practitioners prefer a code of behavior that differs from "SSC" and is described as "risk-aware consensual kink" (RACK), indicating a preference for a style in which the "individual" responsibility of the involved parties is emphasized more strongly, with each participant being responsible for his or her own well-being. Advocates of RACK argue that SSC can hamper discussion of risk because no activity is truly "safe", and that discussion of even low-risk possibilities is necessary for truly informed consent. They further argue that setting a discrete line between "safe" and "not-safe" activities ideologically denies consenting adults the right to evaluate risks vs rewards for themselves; that some adults will be drawn to certain activities regardless of the risk; and that BDSM play—particularly higher-risk play or edgeplay—should be treated with the same regard as extreme sports, with both respect and the demand that practitioners educate themselves and practice the higher-risk activities to decrease risk. RACK may be seen as focusing primarily upon awareness and informed consent, rather than accepted safe practices. Consent is the most important criterion here. The consent and compliance for a sadomasochistic situation can be granted only by people who can judge the potential results. For their consent, they must have relevant information (extent to which the scene will go, potential risks, if a safeword will be used, what that is, and so on) at hand and the necessary mental capacity to judge. The resulting consent and understanding is occasionally summarized in a written "contract", which is an agreement of what can and cannot take place.

In general, BDSM play is usually structured such that it is possible for the consenting partner to withdraw his or her consent at any point during a scene; for example, by using a safeword that was agreed on in advance. Use of the agreed safeword (or occasionally a "safe symbol" such as dropping a ball or ringing a bell, especially when speech is restricted) is seen by some as an explicit withdrawal of consent. Failure to honor a safeword is considered serious misconduct and could even change the sexual consent situation into a crime, depending on the relevant law, since the bottom or top has explicitly revoked his or her consent to any actions that follow the use of the safeword (see Legal status). For other scenes, particularly in established relationships, a safeword may be agreed to signify a warning ("this is getting too intense") rather than explicit withdrawal of consent; and a few choose not to use a safeword at all.

The initialism "BDSM" includes these psychological and physiological facets:

This model for differentiating among these aspects of BDSM is increasingly used in literature today. Nevertheless, it is only an attempt at phenomenological differentiation. Individual tastes and preferences in the area of human sexuality may overlap among these areas, which are discussed separately here.

Bondage and discipline are two aspects of BDSM that do not seem to relate to each other because of the type of activities involved, but they have conceptual similarities, and that is why they appear jointly. Contrary to the other two types, B&D does not define the tops and bottoms itself, and is used to describe the general activities with either partner being the receiver and the giver.

The term "bondage" describes the practice of physical restraint. Bondage is usually, but not always, a sexual practice. While bondage is a very popular variation within the larger field of BDSM, it is nevertheless sometimes differentiated from the rest of this field. Studies among BDSM practitioners in the US have shown that about half of all men find the idea of bondage to be erotic; and many women do as well. Strictly speaking, bondage means binding the partner by tying their appendages together; for example, by the use of handcuffs or ropes, or by lashing their arms to an object. Bondage can also be achieved by spreading the appendages and fastening them with chains or ropes to a St. Andrew's cross or spreader bars.

The term "discipline" describes psychological restraining, with the use of rules and punishment to control overt behavior. Punishment can be pain caused physically (such as caning), humiliation caused psychologically (such as a public flagellation) or loss of freedom caused physically (for example, chaining the submissive partner to the foot of a bed). Another aspect is the structured training of the bottom.

"Dominance and submission" (also known as D&s, Ds or D/s) is a set of behaviors, customs and rituals relating to the giving and accepting of control of one individual over another in an erotic or lifestyle context. It explores the more mental aspect of BDSM. This is also the case in many relationships not considering themselves as sadomasochistic; it is considered to be a part of BDSM if it is practiced purposefully. The range of its individual characteristics is thereby wide.
Often, "contracts" are set out in writing to record the formal consent of the parties to the power exchange, stating their common vision of the relationship dynamic. The purpose of this kind of agreement is primarily to encourage discussion and negotiation in advance, and then to document that understanding for the benefit of all parties. Such documents have not been recognized as being legally binding, nor are they intended to be. These agreements are binding in the sense that the parties have the expectation that the negotiated rules will be followed. Often other friends and community members may witness the signing of such a document in a ceremony, and so parties violating their agreement can result in loss of face, respect or status with their friends in the community.

In general, as compared to conventional relationships, BDSM participants go to great lengths to negotiate the important aspects of their relationships in advance, and to take great care in learning about and following safe practices.

In D/S, the dominant is the top and the submissive is the bottom. In S/M, the sadist is usually the top and the masochist the bottom, but these roles are frequently more complicated or jumbled (as in the case of being dominant, masochists who may arrange for their submissive to carry out S/M activities on them). As in B/D, the declaration of the top/bottom may be required, though sadomasochists may also play without any power exchange at all, with both partners equally in control of the play.

The term "sadomasochism" is derived from the words "sadism" and "masochism". These terms differ somewhat from the same terms used in psychology, since those require that the sadism or masochism cause significant distress or involve non-consenting partners. "Sadomasochism" refers to the aspects of BDSM surrounding the exchange of physical or emotional pain. Sadism describes sexual pleasure derived by inflicting pain, degradation, humiliation on another person or causing another person to suffer. On the other hand, the masochist enjoys being hurt, humiliated, or suffering within the consensual scenario. Sadomasochistic scenes sometimes reach a level that appear more extreme or cruel than other forms of BDSM—for example, when a masochist is brought to tears or is severely bruised—and is occasionally unwelcome at BDSM events or parties. Sadomasochism does not imply enjoyment through causing or receiving pain in other situations (for example, accidental injury, medical procedures).
The terms "sadism" and "masochism" are derived from the names of the Marquis de Sade and Leopold von Sacher-Masoch, based on the content of the authors' works. Although the names of de Sade and Sacher-Masoch are attached to the terms sadism and masochism respectively, the scenes described in de Sade's works do not meet modern BDSM standards of informed consent. BDSM is solely based on consensual activities, and based on its system and laws. The concepts presented by de Sade are not in accordance with the BDSM culture, even though they are sadistic in nature. In 1843 the Ruthenian physician Heinrich Kaan published "Psychopathia sexualis" ("Psychopathy of Sex"), a writing in which he converts the sin conceptions of Christianity into medical diagnoses. With his work the originally theological terms "perversion", "aberration" and "deviation" became part of the scientific terminology for the first time. The German psychiatrist Richard von Krafft Ebing introduced the terms "sadism" and "masochism" to the medical community in his work "Neue Forschungen auf dem Gebiet der Psychopathia sexualis" ("New research in the area of Psychopathy of Sex") in 1890.

In 1905, Sigmund Freud described "sadism" and "masochism" in his "Three Essays on the Theory of Sexuality" as diseases developing from an incorrect development of the child psyche and laid the groundwork for the scientific perspective on the subject in the following decades. This led to the first time use of the compound term "sado-masochism" (German "sado-masochismus") by the Viennese psychoanalytic Isidor Isaak Sadger in their work, "Über den sado-masochistischen Komplex" ("Regarding the sadomasochistic complex") in 1913.

In the later 20th century, BDSM activists have protested against these conceptual models, as they were derived from the philosophies of two singular historical figures. Both Freud and Krafft-Ebing were psychiatrists; their observations on sadism and masochism were dependent on psychiatric patients, and their models were built on the assumption of psychopathology. BDSM activists argue that it is illogical to attribute human behavioural phenomena as complex as sadism and masochism to the 'inventions' of two historic individuals. Advocates of BDSM have sought to distinguish themselves from widely held notions of antiquated psychiatric theory by the adoption of the initialized term, "BDSM" as a distinction from the now common usage of those psychological terms, abbreviated as "S&M".

On a physical level, BDSM is commonly misconceived to be "all about pain". Most often, though, BDSM practitioners are primarily concerned with power, humiliation, and pleasure. Of the three categories of BDSM, only sadomasochism specifically requires pain, but this is typically a means to an end, as a vehicle for feelings of humiliation, dominance, etc. The aspects of D/S and B/D may not include physical suffering at all, but include the sensations experienced by different emotions of the mind.

Dominance & submission of power is an entirely different experience, and is not always psychologically associated with physical pain. Many BDSM activities might not involve any kind of pain or humiliation, but just the exchange of power and control. During the activities, the practitioners may feel endorphins comparable to the so-called "runner's high" or to the afterglow of orgasm. The corresponding trance-like mental state is also known as "subspace" for the submissive, or "topspace" for the dominant. Some use the term "body stress" to describe this physiological sensation. This experience of algolagnia is important, but is not the only motivation for many BDSM practitioners. The philosopher Edmund Burke defines this sensation of pleasure derived from pain by the word "sublime". Research has shown that couples engaging in consensual BDSM tend to show hormonal changes that indicate decreases in stress and increases in emotional bonding.

There is a wide array of BDSM practitioners who take part in sessions for which they do not receive any personal gratification. They enter such situations solely with the intention to allow their partners to fulfill their own needs or fetishes. Professional dominants do this in exchange of money for the session activities, but non-professionals do it for the sake of their partners.

In some BDSM sessions, the top exposes the bottom to a wide range of sensual experiences, for example: pinching, biting, scratching with fingernails, erotic spanking or the use of objects such as crops, whips, liquid wax, ice cubes, Wartenberg wheels, and erotic electrostimulation devices. Fixation by handcuffs, ropes or chains may be used as well. The repertoire of possible "toys" is limited only by the imagination of both partners. To some extent, everyday items like clothes-pins, wooden spoons or plastic wrap are used as pervertables. It is commonly considered that a pleasurable BDSM experience during a session is very strongly dependent upon the top's competence and experience and the bottom's physical and mental state at the time of the session. Trust and sexual arousal help the partners enter a shared mindset.

Some types of BDSM play include, but are not limited to:

Aside from the general advice related to safe sex, BDSM sessions often require a wider array of safety precautions than vanilla sex (sexual behaviour without BDSM elements). In theory, to ensure consent related to BDSM activity, pre-play negotiations are commonplace, especially among partners who do not know each other very well. In practice, pick-up scenes at clubs or parties may sometimes be low in negotiation (much as pick-up sex from singles bars may not involve much negotiation or disclosure). These negotiations concern the interests and fantasies of each partner and establish a framework of both acceptable and unacceptable activities. This kind of discussion is a typical "unique selling proposition" of BDSM sessions and quite commonplace. Additionally, safewords are often arranged to provide for an immediate stop of any activity if any participant should so desire.

Safewords are words or phrases that are called out when things are either not going as planned or have crossed a threshold one cannot handle. They are something both parties can remember and recognize and are, by definition, not words commonly used playfully during any kind of scene. Words such as "no", "stop", and "don't", are often inappropriate as a safeword if the roleplaying aspect includes the illusion of non-consent. The most commonly used safewords are "red" and "yellow", with "red" meaning that play must stop immediately, and "yellow" meaning that the activity needs to slow down. At most clubs and group-organized BDSM parties and events, dungeon monitors (DMs) provide an additional safety net for the people playing there, ensuring that house rules are followed and safewords respected.

BDSM participants are expected to understand practical safety aspects. For instance, they are expected to recognize that parts of the body can be damaged, such as nerves and blood vessels by contusion, or that skin that can be scarred. Using crops, whips, or floggers, the top's fine motor skills and anatomical knowledge can make the difference between a satisfying session for the bottom and a highly unpleasant experience that may even entail severe physical harm. The very broad range of BDSM "toys" and physical and psychological control techniques often requires a far-reaching knowledge of details related to the requirements of the individual session, such as anatomy, physics, and psychology. Despite these risks, BDSM activities usually result in far less severe injuries than sports like boxing and football, and BDSM practitioners do not visit emergency rooms any more often than the general population.

It is necessary to be able to identify each person's psychological "squicks" or triggers in advance to avoid them. Such losses of emotional balance due to sensory or emotional overload are a fairly commonly discussed issue. It is important to follow participants' reactions empathetically and continue or stop accordingly. For some players, sparking "freakouts" or deliberately using triggers may be a desired outcome. Safewords are one way for BDSM practices to protect both parties. However, partners should be aware of each other's psychological states and behaviors to prevent instances where the "freakouts" prevent the use of safewords.

At one end of the spectrum are those who are indifferent to, or even reject physical stimulation. At the other end of the spectrum are bottoms who enjoy discipline and erotic humiliation but are not willing to be subordinate to the person who applies it. The bottom is frequently the partner who specifies the basic conditions of the session and gives instructions, directly or indirectly, in the negotiation, while the top often respects this guidance. Other bottoms often called "brats" try to incur punishment from their tops by provoking them or "misbehaving". Nevertheless, a purist "school" exists within the BDSM community, which regards such "topping from the bottom" as rude or even incompatible with the standards of BDSM relations.


BDSM practitioners sometimes regard the practice of BDSM in their sex life as roleplaying and so often use the terms "play" and "playing" to describe activities where in their roles. Play of this sort for a specified period of time is often called a "session", and the contents and the circumstances of play are often referred to as the "scene". It is also common in personal relationships to use the term "kink play" for BDSM activities, or more specific terms for the type of activity. The relationships can be of varied types.

Early writings on BDSM both by the academic and BDSM community spoke little of long-term relationships with some in the gay leather community suggesting short-term play relationships to be the only feasible relationship models, and recommending people to get married and "play" with BDSM outside of marriage. In recent times though writers of BDSM and sites for BDSM have been more focused on long-term relationships.

A 2003 study, the first to look at these relationships, fully demonstrated that "quality long-term functioning relationships" exist among practitioners of BDSM, with either sex being the top or bottom (homosexual couples were not looked at). Respondents in the study expressed their BDSM orientation to be built into who they are, but considered exploring their BDSM interests an ongoing task, and showed flexibility and adaptability in order to match their interests with their partners. The "perfect match" where both in the relationship shared the same tastes and desires was rare, and most relationships required both partners to take up or put away some of their desires. The BDSM activities that the couples partook in varied in sexual to nonsexual significance for the partners who reported doing certain BDSM activities for "couple bonding, stress release, and spiritual quests". The most reported issue amongst respondents was not finding enough time to be in role with most adopting a lifestyle wherein both partners maintain their dominant or submissive role throughout the day.

Amongst the respondents, it was typically the bottoms who wanted to play harder, and be more restricted into their roles when there was a difference in desire to play in the relationship. The author of the study, Bert Cutler, speculated that tops may be less often in the mood to play due to the increased demand for responsibility on their part: being aware of the safety of the situation and prepared to remove the bottom from a dangerous scenario, being conscious of the desires and limits of the bottom, and so on. The author of the study stressed that successful long-term BDSM relationships came after "early and thorough disclosure" from both parties of their BDSM interests.

Many of those engaged in long-term BDSM relationships learned their skills from larger BDSM organizations and communities There was a lot of discussion by the respondents on the amount of control the top possessed in the relationships with almost non-existent discussion of the top "being better, or smarter, or of more value" than the bottom.
Couples were generally of the same mind of whether or not they were in an ongoing relationship, but in such cases the bottom was not locked up constantly, but that their role in the context of the relationship was always present, even when the top was doing non-dominant activities such as household chores, or the bottom being in a more dominant position. In its conclusion the study states:

The respondents valued themselves, their partners, and their relationships. All couples expressed considerable goodwill toward their partners. The power exchange between the cohorts appears to be serving purposes beyond any sexual satisfaction, including experiencing a sense of being taken care of and bonding with a partner.

The study further goes on to list three aspects that made the successful relationships work: early disclosure of interests and continued transparency, a commitment to personal growth, and the use of the dominant/submissive roles as a tool to maintain the relationship. In his closing remarks, the author of the study theorizes that due to the serious potential for harm, couples in BDSM relationships develop increased communication that may be higher than in mainstream relationships.

A professional dominatrix or professional dominant, often referred to within the culture as a "pro-dom(me)", offers services encompassing the range of bondage, discipline, and dominance in exchange for money. The term "dominatrix" is little-used within the non-professional BDSM scene. A non-professional dominant woman is more commonly referred to simply as a "domme", "dominant", or "femdom" (short for female dominance). There are also services provided by professional female submissives ("pro-subs"). A professional submissive consents to her client's dominant behavior within negotiated limits, and often works within a professional dungeon. Professional submissives, although far more rare, do exist. Most of the people who work as subs normally have tendencies towards such activities, especially when sadomasochism is involved. Males also work as professional "tops" in BDSM, and are called "masters" or "doms". However it is much more rare to find a male in this profession. A male "pro-dom" typically only works with male clientele.

In BDSM, a "scene" is the stage or setting where BDSM activity takes place, as well as the activity itself. The physical place where a BDSM activity takes place is usually called a dungeon, though some prefer less dramatic terms, including "playspace", or "club". A BDSM activity can, but need not, involve sexual activity or sexual roleplay. A characteristic of many BDSM relationships is the power exchange from the bottom to the dominant partner, and bondage features prominently in BDSM scenes and sexual roleplay.

'The Scene' (including use of the definite article 'the') is also used in the BDSM community to refer to the BDSM community as a whole. Thus someone who is on 'the Scene', and prepared to play in public, might take part in 'a scene' at a public play party.

A scene can take place in private between two or more people, and can involve a domestic arrangement, such as servitude or a casual or committed lifestyle master/slave relationship. BDSM elements may involve settings of slave training or punishment for breaches of instructions.

A scene can also take place in a club, where the play can be viewed by others. When a scene takes place in a public setting, it may be because the participants enjoy being watched by others, or because of the equipment available, or because having third parties present adds safety for play partners who have only recently met.

Standard social etiquette rules still apply when at a BDSM event, such as not intimately touching someone you do not know, not touching someone else's belongings (including toys), and abiding by dress codes. Many events open to the public also have rules addressing alcohol consumption, recreational drugs, cell phones, and photography.

A specific scene takes place within the general conventions and etiquette of BDSM, such as requirements for mutual consent and agreement as to the limits of any BDSM activity. This agreement can be incorporated into a formal contract. In addition, most clubs have additional rules which regulate how onlookers may interact with the actual participants in a scene.. As is the general rule in BDSM, these are founded on the catchphrase "safe, sane, and consensual".

BDSM play parties are events in which BDSM practitioners and other similarly interested people meet in order to communicate, share experiences and knowledge, and to "play" in an erotic atmosphere. The parties show similarities with ones in the dark culture, being based on a more or less strictly enforced dress code; most often clothing made of latex, leather or vinyl/PVC, lycra and so on., emphasizing the body's shape and the primary and secondary sexual characteristic. The requirement for such dress codes differ. While some events have none, others have a policy in order to create a more coherent atmosphere and to prevent onlookers from taking part.

At these parties, BDSM can be publicly performed on a stage, or more privately in separate "dungeons". A reason for the relatively fast spread of this kind of event is the opportunity to use a wide range of "playing equipment", which in most apartments or houses is unavailable. Slings, St. Andrew's crosses (or similar restraining constructs), spanking benches, and punishing supports or cages are often made available. The problem of noise disturbance is also lessened at these events, while in the home setting many BDSM activities can be limited by this factor. In addition, such parties offer both exhibitionists and voyeurs a forum to indulge their inclinations without social criticism. Sexual intercourse is not permitted within most public BDSM play spaces or not often seen in others, because it is not the emphasis of this kind of play. In order to ensure the maximum safety and comfort for the participants certain standards of behavior have evolved; these include aspects of courtesy, privacy, respect and safewords. Today BDSM parties are taking place in most of the larger cities in the Western world.

This scene appears particularly on the Internet, in publications, and in meetings such as at fetish clubs (like Torture Garden), SM parties, gatherings called munches, and erotic fairs like Venus Berlin. The annual Folsom Street Fair is the world's largest BDSM event and is held in San Francisco. It has its roots in the gay leather movement. The weekend long festivities include a wide range of sadomasochistic erotica in a public clothing optional space between 8th and 13th streets with nightly parties associated with the organization.

There are also conventions such as Living in Leather and Black Rose.

It has often been assumed that a preference for BDSM is a consequence of childhood abuse. Research indicates that there is no evidence for this claim. Some reports suggest that people abused as children may have more BDSM injuries and have difficulty with safe words being recognized as meaning stop the previously consensual behavior, thus, it is possible that people choosing BDSM as part of their lifestyle, who also were previously abused, may have had more police or hospital reports of injuries. There is also a link between transgender individuals who have been abused and violence occurring in BDSM activities

There are a number of reasons commonly given for why a sadomasochist finds the practice of S&M enjoyable, and the answer is largely dependent on the individual. For some, taking on a role of compliance or helplessness offers a form of therapeutic escape; from the stresses of life, from responsibility, or from guilt. For others, being under the power of a strong, controlling presence may evoke the feelings of safety and protection associated with childhood. They likewise may derive satisfaction from earning the approval of that figure "(see: Servitude (BDSM))". A sadist, on the other hand, may enjoy the feeling of power and authority that comes from playing the dominant role, or receive pleasure vicariously through the suffering of the masochist. It is poorly understood, though, what ultimately connects these emotional experiences to sexual gratification, or how that connection initially forms.
Joseph Merlino, author and psychiatry adviser to the "New York Daily News", said in an interview that a sadomasochistic relationship, as long as it is consensual, is not a psychological problem:

It is agreed on by some psychologists that experiences during early sexual development can have a profound effect on the character of sexuality later in life. Sadomasochistic desires, however, seem to form at a variety of ages. Some individuals report having had them before puberty, while others do not discover them until well into adulthood. According to one study, the majority of male sadomasochists (53%) developed their interest before the age of 15, while the majority of females (78%) developed their interest afterwards (Breslow, Evans, and Langley 1985). The prevalence of sadomasochism within the general population is unknown. Despite female sadists being less visible than males, some surveys have resulted in comparable amounts of sadistic fantasies between females and males. The results of such studies demonstrate that one's sex does not determine preference for sadism.

Following a phenomenological study of nine individuals involved in sexual masochistic sessions who regarded pain as central to their experience, sexual masochism was described as an addiction-like tendency, with several features resembling that of drug addiction: craving, intoxication, tolerance and withdrawal. It was also demonstrated how the first masochistic experience is placed on a pedestal, with subsequent use aiming at retrieving this lost sensation, much as described in the descriptive literature on addiction. The addictive pattern presented in this study suggests an association with behavioral spin as found in problem gamblers.

BDSM is practiced in all social strata and is common in both heterosexual and homosexual men and women in varied occurrences and intensities. The spectrum ranges from couples with no connections to the subculture outside of their bedrooms or homes, without any awareness of the concept of BDSM, playing "tie-me-up-games", to public scenes on St. Andrew's crosses at large events such as the Folsom Street Fair in San Francisco. Estimation on the overall percentage of BDSM related sexual behaviour vary but it is no longer assumed to be uncommon.

Alfred Kinsey stated in his 1953 nonfiction book "Sexual Behavior in the Human Female" that 12% of females and 22% of males reported having an erotic response to a sadomasochistic story. In that book erotic responses to being bitten were given as:
A non-representative survey on the sexual behaviour of American students published in 1997 and based on questionnaires had a response rate of about 8–9%. Its results showed 15% of homosexual and bisexual males, 21% of lesbian and female bisexual students, 11% of heterosexual males and 9% of female heterosexual students committed to BDSM related fantasies. In all groups the level of practical BDSM experiences were around 6%. Within the group of openly lesbian and bisexual females the quote was significantly higher, at 21%. Independent of their sexual orientation, about 12% of all questioned students, 16% of lesbians and female bisexuals and 8% of heterosexual males articulated an interest in spanking. Experience with this sexual behaviour was indicated by 30% of male heterosexuals, 33% of female bisexuals and lesbians, and 24% of the male gay and bisexual men and female heterosexual women. Even though this study was not considered representative, other surveys indicate similar dimensions in a differing target groups.

A representative study done from 2001 to 2002 in Australia found that 1.8% of sexually active people (2.2% men, 1.3% women but no significant sex difference) had engaged in BDSM activity in the previous year. Of the entire sample, 1.8% men and 1.3% women had been involved in BDSM. BDSM activity was significantly more likely among bisexuals and homosexuals of both sexes. But among men in general, there was no relationship effect of age, education, language spoken at home, or relationship status. Among women, in this study, activity was most common for those between 16 and 19 years of age and least likely for females over 50 years. Activity was also significantly more likely for women who had a regular partner they did not live with, but was not significantly related with speaking a language other than English or education.

Another representative study, published in 1999 by the German Institut für rationale Psychologie, found that about 2/3 of the interviewed women stated a desire to be at the mercy of their sexual partners from time to time. 69% admitted to fantasies dealing with sexual submissiveness, 42% stated interest in explicit BDSM techniques, 25% in bondage. A 1976 study in the general US population suggests three percent have had positive experiences with Bondage or master-slave roleplaying. Overall 12% of the interviewed females and 18% of the males were willing to try it. A 1990 Kinsey Institute report stated that 5% to 10% of Americans occasionally engage in sexual activities related to BDSM. 11% of men and 17% of women reported trying bondage. Some elements of BDSM have been popularized through increased media coverage since the middle 1990s. Thus both black leather clothing, sexual jewellery such as chains and dominance roleplay appear increasingly outside of BDSM contexts.

According to yet another survey of 317,000 people in 41 countries, about 20% of the surveyed have at least used masks, blindfolds or other bondage utilities once, and 5% explicitly connected themselves with BDSM. In 2004, 19% mentioned spanking as one of their practices and 22% confirmed the use of blindfolds or handcuffs.

A 1985 study found 52 out of 182 female respondents (28%) were involved in sadomasochistic activities.

A 2009 study on two separate samples of male undergraduate students in Canada found that 62 to 65%, depending on the sample, had entertained sadistic fantasies, and 22 to 39% engaged in sadistic behaviors during sex. The figures were 62 and 52% for bondage fantasies, and 14 to 23% for bondage behaviors. A 2014 study involving a mixed sample of Canadian college students and online volunteers, both male and female, reported that 19% of male samples and 10% of female samples rated the sadistic scenarios described in a questionnaire as being at least "slightly arousing" on a scale that ranged from "very repulsive" to "very arousing"; the difference was statistically significant. The corresponding figures for the masochistic scenarios were 15% for male students and 17% for female students, a non-significant difference. In a 2011 study on 367 middle-aged and elderly men recruited from the broader community in Berlin, 21.8% of the men self-reported sadistic fantasies and 15.5% sadistic behaviors; 24.8% self-reported any such fantasy and/or behavior. The corresponding figures for self-reported masochism were 15.8% for fantasy, 12.3% for behavior, and 18.5% for fantasy and/or behavior. In a 2008 study on gay men in Puerto Rico, 14.8% of the over 425 community volunteers reported any sadistic fantasy, desire or behavior in their lifetime; the corresponding figure for masochism was 15.7%. A 2017 cross-sectional representative survey among the general Belgian population demonstrated a substantial prevalence of BDSM fantasies and activities; 12.5% of the population performed one of more BDSM-practices on a regular basis.

Reflecting changes in social norms, modern medical opinion is now moving away from regarding BDSM activities as medical disorders, unless they are nonconsensual or involve significant distress or harm. 

In the past, the Diagnostic and Statistical Manual of Mental Disorders (DSM), the American Psychiatric Association's manual, defined some BDSM activities as sexual disorders. Following campaigns from advocacy organizations including the National Coalition for Sexual Freedom, the current version of the DSM, DSM-5, excludes consensual BDSM from diagnosis when the sexual interests cause no harm or distress.

The World Health Organization's International Classification of Diseases (ICD) has made similar moves in recent years.

Section F65 of the current revision, ICD-10, indicates that "mild degrees of sadomasochistic stimulation are commonly used to enhance otherwise normal sexual activity". The diagnostic guidelines for the ICD-10 state that this class of diagnosis should only be made "if sadomasochistic activity is the most important source of stimulation or necessary for sexual gratification".

In Europe, an organization called ReviseF65 has worked to remove sadomasochism from the ICD. In 1995, Denmark became the first European Union country to have completely removed sadomasochism from its national classification of diseases. This was followed by Sweden in 2009, Norway in 2010 and Finland 2011. Recent surveys on the spread of BDSM fantasies and practices show strong variations in the range of their results. Nonetheless, researchers assume that 5 to 25 percent of the population practices sexual behavior related to pain or dominance and submission. The population with related fantasies is believed to be even larger.

The ICD is in the process of revision, and recent drafts have reflected these changes in social norms. , the final advance preview of the ICD-11 has de-pathologised most things listed in ICD-10 section F65, characterizing as pathological only those activities which are either coercive, or involving significant risk of injury or death, or distressing to the individual committing them, and specifically excluding consensual sexual sadism and masochism from being regarded as pathological. The final advance text is to be officially presented to the members of the WHO in 2019, ready to come into effect in 2022.

Some people who feel attracted by the situations usually compiled under the term BDSM reach a point where they decide to come out of the closet, though many sadomasochists keep themselves closeted. Even so, depending upon a survey's participants, about 5 to 25 percent of the US population show affinity to the subject. Other than a few artists and writers, practically no celebrities are publicly known as sadomasochists.

Public knowledge of one's BDSM lifestyle can have devastating vocational and social effects for sadomasochists. Many face severe professional consequences or social rejection if they are exposed, either voluntarily or involuntarily, as sadomasochists.

Within feminist circles the discussion has been split roughly into two camps: some who see BDSM as an aspect or reflection of oppression (for example, Alice Schwarzer) and, on the other side, pro-BDSM feminists, often grouped under the banner of sex-positive feminism (see Samois); both of them can be traced back to the 1970s.

Some feminists have criticized BDSM for eroticizing power and violence, and for reinforcing misogyny. They argue that women who engage in BDSM are making a choice that is ultimately bad for women. Feminist defenders of BDSM argue that consensual BDSM activities are enjoyed by many women and validate the sexual inclinations of these women. They argue that there is no connection between consensual kinky activities and sex crimes, and that feminists should not attack other women's sexual desires as being "anti-feminist". They also state that the main point of feminism is to give an individual woman free choices in her life; which includes her sexual desire. While some feminists suggest connections between consensual BDSM scenes and non-consensual rape and sexual assault, other sex-positive ones find the notion insulting to women.

It is often mentioned that in BDSM, roles are not fixed to gender, but personal preferences. The dominant partner in a heterosexual relationship may be the woman rather than the man; or BDSM may be part of male/male or female/female sexual relationships. Finally, some people switch, taking either a dominant or submissive role on different occasions. Several studies investigating the possibility of correlation between BDSM pornography and the violence against women also indicate a lack of correlation. As an example, Japan is listed as the country with the lowest sexual crime rate out of all the industrialized nations, despite being known for its distinct BDSM and bondage pornography (see Pornography in Japan). In 1991 a lateral survey came to the conclusion that between 1964 and 1984, despite the increase in amount and availability of sadomasochistic pornography in the US, Germany, Denmark and Sweden there is no correlation with the national number of rapes to be found.

Operation Spanner in the UK proves that BDSM practitioners still run the risk of being stigmatized as criminals. In 2003, the media coverage of Jack McGeorge showed that simply participating and working in BDSM support groups poses risks to one's job, even in countries where no law restricts it. Here a clear difference can be seen to the situation of homosexuality. The psychological strain appearing in some individual cases is normally neither articulated nor acknowledged in public. Nevertheless, it leads to a difficult psychological situation in which the person concerned can be exposed to high levels of emotional stress.

In the stages of "self awareness", he or she realizes their desires related to BDSM scenarios or decides to be open for such. Some authors call this "internal coming-out". Two separate surveys on this topic independently came to the conclusion that 58 percent and 67 percent of the sample respectively, had realized their disposition before their 19th birthday. Other surveys on this topic show comparable results. Independent of age, coming-out can potentially result in a difficult life crisis, sometimes leading to thoughts or acts of suicide. While homosexuals have created support networks in the last decades, sadomasochistic support networks are just starting to develop in most countries. In German speaking countries they are only moderately more developed. The Internet is the prime contact point for support groups today, allowing for local and international networking. In the US Kink Aware Professionals (KAP) a privately funded, non-profit service provides the community with referrals to psychotherapeutic, medical, and legal professionals who are knowledgeable about and sensitive to the BDSM, fetish, and leather community. In the US and the UK, the Woodhull Freedom Foundation & Federation, National Coalition for Sexual Freedom (NCSF) and Sexual Freedom Coalition (SFC) have emerged to represent the interests of sadomasochists. The German Bundesvereinigung Sadomasochismus is committed to the same aim of providing information and driving press relations. In 1996 the website and mailing list Datenschlag went online in German and English providing the largest bibliography, as well as one of the most extensive historical collections of sources related to BDSM.

Richters et al. (2008) study also found that people who engaged in BDSM were more likely to have experienced a wider range of sexual practices (e.g. oral or anal sex, more than one partner, group sex, phone sex, viewed pornography, used a sex toy, fisting, rimming, etc.). They were, however, not any more likely to have been coerced, unhappy, anxious, or experiencing sexual difficulties. On the contrary, men who had engaged in BDSM scored lower on a psychological distress scale than men who did not.

There have been few studies on the psychological aspects of BDSM using modern scientific standards. Psychotherapist Charles Moser has said there is no evidence for the theory that BDSM has common symptoms or any common psychopathology, emphasizing that there is no evidence that BDSM practitioners have any special psychiatric other problems based on their sexual preferences.

Problems do sometimes occur in the area of self classification by the person concerned. During the phase of the "coming-out", self-questioning related to one's own "normality" is quite common. According to Moser, the discovery of BDSM preferences "can" result in fear of the current non-BDSM relationship's destruction. This, combined with the fear of discrimination in everyday life, leads in some cases to a double life which can be highly burdensome. At the same time, the denial of BDSM preferences can induce stress and dissatisfaction with one's own "vanilla"-lifestyle, feeding the apprehension of finding no partner. Moser states that BDSM practitioners having problems finding BDSM partners would probably have problems in finding a non-BDSM partner as well. The wish to remove BDSM preferences is another possible reason for psychological problems since it is not possible in most cases. Finally, the scientist states that BDSM practitioners seldom commit violent crimes. From his point of view, crimes of BDSM practitioners usually have no connection with the BDSM components existing in their life. Moser's study comes to the conclusion that there is no scientific evidence, which could give reason to refuse members of this group work- or safety certificates, adoption possibilities, custody or other social rights or privileges. The Swiss psychoanalyst Fritz Morgenthaler shares a similar perspective in his book, "Homosexuality, Heterosexuality, Perversion" (1988). He states that possible problems result not necessarily from the non-normative behavior, but in most cases primarily from the real or feared reactions of the social environment towards their own preferences. In 1940 psychoanalyst Theodor Reik reached implicitly the same conclusion in his standard work "Aus Leiden Freuden. Masochismus und Gesellschaft".

Moser's results are further supported by a 2008 Australian study by Richters "et al." on the demographic and psychosocial features of BDSM participants. The study found that BDSM practitioners were no more likely to have experienced sexual assault than the control group, and were not more likely to feel unhappy or anxious. The BDSM males reported higher levels of psychological well-being than the controls. It was concluded that "BDSM is simply a sexual interest or subculture attractive to a minority, not a pathological symptom of past abuse or difficulty with 'normal' sex."

Several recent studies have been conducted on the gender differences and personality traits of BDSM practitioners. Wismeijer & van Assen (2013) found that "the association of BDSM role and gender was strong and significant" with only 8% of women in the study being dominant compared to 75% being submissive.; Hébert & Weaver (2014) found that 9% of women in their study were dominant compared to 88% submissive; and Weierstall1 & Giebel (2017) likewise found a significant difference, with 19% of women in the study as dominant compared to 74% as submissive. They concluded that "men more often display an engagement in dominant practices, whereas females take on the submissive part. This result is inline with a recent study about mate preferences that has shown that women have a generally higher preference for a dominant partner than men do (Giebel, Moran, Schawohl, & Weierstall, 2015). Women also prefer dominant men, and even men who are aggressive, for a short-term relationship and for the purpose of sexual intercourse (Giebel, Weierstall, Schauer, & Elbert, 2013)".. Similarly, studies on sexual fantasy differences between men and women show the latter prefer submissive and passive fantasies over dominant and active ones, with rape and force being common..

One common belief of BDSM and kink is that women are more likely to take on masochistic roles than men. Roy Baumeister (2010) actually had more male masochists in his study than female, and fewer male dominants than female. The lack of statistical significance in these gender differences suggests that no assumptions should be made regarding gender and masochistic roles in BDSM. One explanation why we might think otherwise lies in our social and cultural ideals about femininity; masochism may emphasize certain stereotypically feminine elements through activities like feminization of men and ultra-feminine clothing for women. But such tendencies of the submissive masochistic role should not be interpreted as a connection between it and the stereotypical female role—many masochistic scripts do not include any of these tendencies.

Baumeister found that masochistic males experienced greater: severity of pain, frequency of humiliation (status-loss, degrading, oral), partner infidelity, active participation by other persons, and cross dressing. Trends also suggested that male masochism included more bondage and oral sex than female (though the data was not significant). Female masochists, on the other hand, experienced greater: frequency in pain, pain as punishment for 'misdeeds' in the relationship context, display humiliation, genital intercourse, and presence of non-participating audiences. The exclusiveness of dominant males in a heterosexual relationship happens because, historically, men in power preferred multiple partners. Finally, Baumeister observes a contrast between the 'intense sensation' focus of male masochism to a more 'meaning and emotion' centred female masochistic script.

Prior argues that although some of these women may appear to be engaging in traditional subordinate or submissive roles, BDSM allows women in both dominant and submissive roles to express and experience personal power through their sexual identities. In a study that she conducted in 2013, she found that the majority of the women she interviewed identified as bottom, submissive, captive, or slave/sex slave. In turn, Prior was able to answer whether or not these women found an incongruity between their sexual identities and feminist identity. Her research found that these women saw little to no incongruity, and in fact felt that their feminist identity supported identities of submissive and slave. For them these are sexually and emotionally fulfilling roles and identities that, in some cases, feed other aspects of their lives. Prior contends that third wave feminism provides a space for women in BDSM communities to express their sexual identities fully, even when those identities seem counter-intuitive to the ideals of feminism. Furthermore, women who do identify as submissive, sexually or otherwise, find a space within BDSM where they can fully express themselves as integrated, well-balanced, and powerful women.

Levitt, Moser, & Jamison's 1994 study provides a general, if outdated, description of characteristics of women in the sadomasochistic (S/M) subculture. They state that women in S/M tend to have higher education, become more aware of their desires as a young adult, are less likely to be married than the general population. The researchers found the majority of females identified as heterosexual and submissive, a substantial minority were versatile—able to switch between dominant and submissive roles—and a smaller minority identified with the dominant role exclusively. Oral sex, bondage and master-slave script were among the most popular activities, while feces/watersports were the least popular.

Though BDSM in itself can be considered a sexual orientation or identity, and is considered one by some of its practitioners, the BDSM and kink scene is more often seen as a diverse pansexual community. Often this is a non-judgmental community where gender, sexuality, orientation, preferences are accepted as is or worked at to become something a person can be happy with. In research, studies have focused on bisexuality and its parallels with BDSM, as well as gay-straight differences between practitioners.

Demographically, Nordling et al.'s (2006) study found no differences in age, but 43% of gay male respondents compared to 29% of straight males had university level education. The gay men also had higher incomes than the general population, and tended to work in white collar jobs while straight men tended toward blue collar ones. Because there were not enough female respondents (22), no conclusions could be drawn from them.

Sexually speaking, the same 2006 study by Nordling et al. found that gay males were aware of their S/M preferences and took part in them at an earlier age, preferring leather, anal sex, rimming, dildos and special equipment or uniform scenes. In contrast, straight men preferred verbal humiliation, mask and blindfolds, gags, rubber/latex outfits, caning, vaginal sex, and cross-dressing among other activities. From the questionnaire, researchers were able to identify four separate sexual themes: hyper-masculinity, giving and receiving pain, physical restriction (i.e. bondage), and psychological humiliation. Gay men preferred activities that tended towards hyper-masculinity while straight men showed greater preference for humiliation. Though there were not enough female respondents to draw a similar conclusion with, the fact that there is a difference in gay and straight men suggests strongly that S/M (and BDSM in general) can not be considered a homogenous phenomenon. As Nordling et al. (2006) puts it, "People who identify as sadomasochists mean different things by these identifications." (54)

In Steve Lenius' original 2001 paper he explored the acceptance of bisexuality in a supposedly pansexual BDSM community. The reasoning behind this is that 'coming-out' had become primarily the territory of the gay and lesbian, with bisexuals feeling the push to be one or the other (and being right only half the time either way). What he found in 2001, was that people in BDSM were open to discussion about the topic of bisexuality and pansexuality and all controversies they bring to the table, but personal biases and issues stood in the way of actively using such labels. A decade later, Lenius (2011) looks back on his study and considers if anything has changed. He concluded that the standing of bisexuals in the BDSM and kink community was unchanged, and believed that positive shifts in attitude were moderated by society's changing views towards different sexualities and orientations. But Lenius (2011) does emphasize that the pansexual promoting BDSM community helped advance greater acceptance of alternative sexualities.

Brandy Lin Simula (2012), on the other hand, argues that BDSM actively resists gender conforming and identified three different types of BDSM bisexuality: gender-switching, gender-based styles (taking on a different gendered style depending on gender of partner when playing), and rejection of gender (resisting the idea that gender matters in their play partners). Simula (2012) explains that practitioners of BDSM routinely challenge our concepts of sexuality by pushing the limits on pre-existing ideas of sexual orientation and gender norms. For some, BDSM and kink provides a platform in creating identities that are fluid, ever-changing.

Psychiatry has an insensitive history in the area of BDSM. There have been many involvements by institutions of political power to marginalize subgroups and sexual minorities. Mental health professionals have a long history of holding negative assumptions and stereotypes about the BDSM community. Beginning with the DSM-II, Sexual Sadism and Sexual Masochism have been listed as sexually deviant behaviours. Sadism and masochism were also found in the personality disorder section. This negative assumption has not changed significantly which is evident in the continued inclusion of Sexual Sadism and Sexual Masochism as paraphilias in the DSM-IV-TR. The DSM-V, however, has depathologized the language around paraphilias in a way that signifies "the APA's intent to not demand treatment for healthy consenting adult sexual expression". These biases and misinformation can result in pathologizing and unintentional harm to clients who identify as sadists and/or masochists and medical professionals who have been trained under older editions of the DSM can be slow to change in their ways of clinical practice.

According to Kolmes et al. (2006), major themes of biased and inadequate care to BDSM clients are:


These same researchers suggested that therapists should be open to learning more about BDSM, to show comfort in talking about BDSM issues, and to understand and promote "safe, sane, consensual" BDSM.

There has also been research which suggests BDSM can be a beneficial way for victims of sexual assault to deal with their trauma, most notably by Corie Hammers, but this work is limited in scope and to date, has not undergone empirical testing as a treatment.

Both terms were introduced to the medical field by German psychiatrist Richard von Krafft-Ebing in his 1886 compilation of case studies "Psychopathia Sexualis". Pain and physical violence are not essential in Krafft-Ebing's conception, and he defined "masochism" (German "masochismus") entirely in terms of control. Sigmund Freud, a psychoanalyst and a contemporary of Krafft-Ebing, noted that both were often found in the same individuals, and combined the two into a single dichotomous entity known as "sadomasochism" (German "sadomasochismus", often abbreviated as "S&M" or "S/M"). This observation is commonly verified in both literature and practice; many sadists and masochists define themselves as "switchable"—capable of taking pleasure in either role. However it has also been argued (Deleuze, "Coldness and Cruelty") that the concurrence of sadism and masochism in Freud's model should not be taken for granted.

Freud introduced the terms "primary" and "secondary" masochism. Though this idea has come under a number of interpretations, in a primary masochism the masochist undergoes a complete, not just a partial, rejection by the model or courted object (or sadist), possibly involving the model taking a rival as a preferred mate. This complete rejection is related to the death drive ("todestrieb") in Freud's psychoanalysis. In a secondary masochism, by contrast, the masochist experiences a less serious, more feigned rejection and punishment by the model. Secondary masochism, in other words, is the relatively casual version, more akin to a charade, and most commentators are quick to point out its contrivedness.

Rejection is not desired by a primary masochist in quite the same sense as the feigned rejection occurring within a mutually consensual relationship—or even where the masochist happens to be the one having actual initiative power (this is the confusion of the distinctions of casual appearance and discrete motives which underlies the analyses of Deleuze and Sartre, for example). In "Things Hidden Since the Foundation of the World", René Girard attempts to resuscitate and reinterpret Freud's distinction of primary and secondary masochism, in connection with his own philosophy.

Both Krafft-Ebing and Freud assumed that sadism in men resulted from the distortion of the aggressive component of the male sexual instinct. Masochism in men, however, was seen as a more significant aberration, contrary to the nature of male sexuality. Freud doubted that masochism in men was ever a primary tendency, and speculated that it may exist only as a transformation of sadism. Sadomasochism in women received comparatively little discussion, as it was believed that it occurred primarily in men. Both also assumed that masochism was so inherent to female sexuality that it would be difficult to distinguish as a separate inclination.

Havelock Ellis, in "Studies in the Psychology of Sex", argued that there is no clear distinction between the aspects of sadism and masochism, and that they may be regarded as complementary emotional states. He also made the important point that sadomasochism is concerned only with pain in regard to sexual pleasure, and not in regard to cruelty, as Freud had suggested. In other words, the sadomasochist generally desires that the pain be inflicted or received in love, not in abuse, for the pleasure of either one or both participants. This mutual pleasure may even be essential for the satisfaction of those involved.

Here, Ellis touches upon the often paradoxical nature of widely reported consensual S&M practices. It is described as not simply pain to initiate pleasure, but violence—"or the simulation of involuntary violent acts"—said to express love. This irony is highly evident in the observation by many, that not only are popularly practiced sadomasochistic activities usually performed at the express request of the masochist, but that it is often the designated masochist who may direct such activities, through subtle emotional cues perceived or mutually understood and consensually recognized by the designated sadist.

In his essay "Coldness and Cruelty", (originally "Présentation de Sacher-Masoch", 1967) Gilles Deleuze rejects the term "sadomasochism" as artificial, especially in the context of the quintessentially modern masochistic work, Sacher-Masoch's "Venus In Furs". Deleuze's counterargument is that the tendency toward masochism is based on intensified desire brought on or enhanced by the acting out of frustration at the delay of gratification. Taken to its extreme, an intolerably indefinite delay is 'rewarded' by punitive perpetual delay, manifested as unwavering coldness. The masochist derives pleasure from, as Deleuze puts it, the "Contract": the process by which he can control another individual and turn the individual into someone cold and callous. The sadist, in contrast, derives pleasure from the "Law": the unavoidable power that places one person below another. The sadist attempts to destroy the ego in an effort to unify the id and super-ego, in effect gratifying the most base desires the sadist can express while ignoring or completely suppressing the will of the ego, or of the conscience. Thus, Deleuze attempts to argue that masochism and sadism arise from such different impulses that the combination of the two terms is meaningless and misleading. A masochist's perception of their own self-subjugating sadistic desires and capacities are treated by Deleuze as reactions to prior experience of sadistic objectification. (E.g. in terms of psychology, compulsively defensive appeasement of pathological guilt feelings as opposed to the volition of a strong free will.) The epilogue of "Venus In Furs" shows the character of Severin has become embittered by his experiment in the alleged control of masochism, and advocates instead the domination of women.

Before Deleuze, however, Sartre had presented his own theory of sadism and masochism, at which Deleuze's deconstructive argument, which took away the symmetry of the two roles, was probably directed. Because the pleasure or power in looking at the victim figures prominently in sadism and masochism, Sartre was able to link these phenomena to his famous philosophy of the "Look of the Other". Sartre argued that masochism is an attempt by the "For-itself" (consciousness) to reduce itself to nothing, becoming an object that is drowned out by the "abyss of the Other's subjectivity". By this Sartre means that, given that for the "for-itself" desire to attain a point of view in which it is both subject and object, one possible strategy is to gather and intensify every feeling and posture in which the self appears as an object to be rejected, tested, and humiliated; and in this way the for-itself strives toward a point of view in which there is only one subjectivity in the relationship, which would be both that of the abuser and the abused. Conversely, Sartre held sadism to be the effort to annihilate the subjectivity of the victim. That means that the sadist is exhilarated by the emotional distress of the victim because they seek a subjectivity that views the victim as both subject and object.

This argument may appear stronger if it is understood that this "look of the other" theory is either only an aspect of the faculties of desire, or somehow its primary faculty. This does not account for the turn that Deleuze took for his own theory of these matters, but the premise of "desire as 'look'" is associated with theoretical distinctions always detracted by Deleuze, in what he regarded as its essential error to recognize "desire as lack"—which he identified in the philosophical temperament of Plato, Socrates, and Lacan. For Deleuze, insofar as desire is a lack it is reducible to the "look".

Finally, after Deleuze, René Girard included his account of sado-masochism in "Things Hidden Since the Foundation of The World", originally "Des choses cachées depuis la fondation du monde", 1978, making the chapter on masochism a coherent part of his theory of mimetic desire. In this view of sado-masochism, the violence of the practices are an expression of a peripheral rivalry that has developed around the actual love-object. There is clearly a similarity to Deleuze, since both in the violence surrounding the memory of mimetic crisis and its avoidance, and in the resistance to affection that is focused on by Deleuze, there is an understanding of the value of the love object in terms of the processes of its valuation, acquisition and the test it imposes on the suitor.

Nichols (2006) compiled some common clinical issues: countertransference, non-disclosure, coming-out, partner/families, and bleed-through.

Countertransference is a common problem in clinical settings. Despite having no evidence, therapists may find themselves believing that their client’s pathology is "self-evident". Therapists may feel intense disgust and aversive reactions. Feelings of countertransference can interfere with therapy. Another common problem is when clients conceal their sexual preferences from their therapists. This can compromise any therapy. To avoid non-disclosure, therapists are encouraged to communicate their openness in indirect ways with literatures and artworks in the waiting room. Therapists can also deliberately bring up BDSM topics during the course of therapy. With less informed therapists, sometimes they over-focus on clients’ sexuality which detracts from original issues such as family relationships, depression, etc. A special subgroup that needs counselling is the "newbie". Individuals just coming out might have internalized shame, fear, and self-hatred about their sexual preferences. Therapists need to provide acceptance, care, and model positive attitude; providing reassurance, psychoeducation, and bibliotherapy for these clients is crucial. The average age when BDSM individuals realize their sexual preference is around 26 years. Many people hide their sexuality until they can no longer contain their desires. However, they may have married or had children by this point. Therefore, therapists need to facilitate couple's counselling and disclosure. It is important for therapists to consider fairness to partner and family of clients. In situations when boundaries between roles in the bedroom and roles in the rest of the relationship blurs, a "bleed-through" problem has occurred. Therapists need to help clients resolve distress and deal with any underlying problems that led to the initial bleed-through.

Practices of BDSM survive from some of the oldest textual records in the world, associated with rituals to the Goddess Inanna (Ishtar in Akkadian). Cuneiform texts dedicated to Inanna which incorporate domination rituals. In particular she points to ancient writings such as Inanna and Ebih (in which the Goddess dominates Ebih), and Hymn to Inanna describing cross-dressing transformations and rituals "imbued with pain and ecstasy, bringing about initation and journeys of altered states of consciousness; punishment, moaning, ecstasy, lament and song, participants exhausting themselves in weeping and grief."

During the ninth century BC, ritual flagellations were performed in Artemis Orthia, one of the most important religious areas of ancient Sparta, where the Cult of Orthia, a preolympic religion, was practiced. Here ritual flagellation called "diamastigosis" took place, in which young adolescent men were whipped in a ceremony overseen by the priestess. These are referred to by a number of ancient authors, including Pausanius (III, 16: 10-11).

One of the oldest graphical proofs of sadomasochistic activities is found in the Etruscan Tomb of the Whipping near Tarquinia, which dates to the fifth century BC. Inside the tomb there is fresco which portrays two men who flagellate a woman with a cane and a hand during an erotic situation. Another reference related to flagellation is to be found in the sixth book of the "Satires" of the ancient Roman Poet Juvenal (1st–2nd century A.D.), further reference can be found in Petronius's "Satyricon" where a delinquent is whipped for sexual arousal. Anecdotal narratives related to humans who have had themselves voluntary bound, flagellated or whipped as a substitute for sex or as part of foreplay reach back to the third and fourth century.

In Pompeii, a whip-mistress figure with wings is depicted on the wall of the Villa of Mysteries, as part of an initiation of a young woman into the Mysteries. The whip-mistress role drove the sacred initiation of ceremonial death and rebirth. The archaic Greek Aphrodite may too once have been armed with an implement, with archaeological evidence of armed Aphrodite known from a number of locations in Cythera, Acrocorinth and Sparta, and which may have been a whip.

The "Kama Sutra" of India describes four different kinds of hitting during lovemaking, the allowed regions of the human body to target and different kinds of joyful "cries of pain" practiced by bottoms. The collection of historic texts related to sensuous experiences explicitly emphasizes that impact play, biting and pinching during sexual activities should only be performed consensually since only some women consider such behavior to be joyful. From this perspective the Kama Sutra can be considered as one of the first written resources dealing with sadomasochistic activities and safety rules. Further texts with sadomasochistic connotation appear worldwide during the following centuries on a regular basis.

There are anecdotal reports of people willingly being bound or whipped, as a prelude to or substitute for sex, during the 14th century. The medieval phenomenon of courtly love in all of its slavish devotion and ambivalence has been suggested by some writers to be a precursor of BDSM. Some sources claim that BDSM as a distinct form of sexual behavior originated at the beginning of the 18th century when Western civilization began medically and legally categorizing sexual behavior (see Etymology).

Flagellation practiced within an erotic setting has been recorded from at least the 1590s evidenced by a John Davies epigram, and references to "flogging schools" in Thomas Shadwell's "The Virtuoso" (1676) and Tim Tell-Troth's "Knavery of Astrology" (1680). Visual evidence such as mezzotints and print media is also identified revealing scenes of flagellation, such as "The Cully Flaug'd" from the British Museum collection.

John Cleland's novel "Fanny Hill", published in 1749, incorporates a flagellation scene between the character's protagonist Fanny Hill and Mr Barville. A large number of flagellation publications followed, including "" (c. 1761), promoting the names of ladies offering the service in a lecture room with rods and cat o' nine tails.

Other sources give a broader definition, citing BDSM-like behavior in earlier times and other cultures, such as the medieval flagellates and the physical ordeal rituals of some Native American societies.

BDSM ideas and imagery have existed on the fringes of Western culture throughout the twentieth century. Robert Bienvenu attributes the origins of modern BDSM to three sources, which he names as "European Fetish" (from 1928), "American Fetish" (from 1934), and "Gay Leather" (from 1950). Another source are the sexual games played in brothels, which go back into the 19th century if not earlier. Charles Guyette was the first American to produce and distribute fetish related material (costumes, footwear, photography, props and accessories) in the US. His successor, Irving Klaw, produced commercial sexploitation film and photography with a BDSM theme (most notably with Bettie Page) and issued fetish comics (known then as "chapter serials") by the now-iconic artists John Willie, Gene Bilbrew, and Eric Stanton.

Stanton's model Bettie Page became at the same time one of the first successful models in the area of fetish photography and one of the most famous pin-up girls of American mainstream culture. Italian author and designer Guido Crepax was deeply influenced by him, coining the style and development of European adult comics in the second half of the twentieth century. The artists Helmut Newton and Robert Mapplethorpe are the most prominent examples of the increasing use of BDSM-related motives in modern photography and the public discussions still resulting from this.

Alfred Binet first coined the term erotic fetishism in his 1887 book, "Du fétichisme dans l’amour" Richard von Krafft-Ebing saw BDSM interests as the end of a continuum.

Leather has been a predominantly gay male term to refer to one fetish, but it can stand for many more. Members of the gay male leather community may wear leathers such as Motorcycle leathers, or may be attracted to men wearing leather. Leather and BDSM are seen as two parts of one whole. Much of the BDSM culture can be traced back to the gay male leather culture, which formalized itself out of the group of men who were soldiers returning home after World War II (1939–1945). WWII was the setting where countless homosexual men and women tasted the life among homosexual peers. Post-war, homosexual individuals congregated in larger cities such as New York, Chicago, San Francisco, and Los Angeles. They formed leather clubs and bike clubs, some were fraternal services. The establishment of Mr. Leather Contest and Mr. Drummer Contest were made around this time. This was the genesis of the gay male leather community. Many of the members were attracted to extreme forms of sexuality, for which peak expression was in the pre-AIDS 1970s. This subculture is epitomized by the "Leatherman's Handbook" by Larry Townsend, published in 1972, which describes in detail the practices and culture of gay male sadomasochists in the late 1960s and early 1970s.
In the early 1980s, lesbians also joined the leathermen as a recognizable element of the gay leather community. They also formed leather clubs, but there were some gender differences such as the absence of leatherwomen’s bar. In 1981, the publication of "Coming to Power" by lesbian-feminist group Samois led to a greater knowledge and acceptance of BDSM in the lesbian community. By the 1990s, the gay men's and women's leather communities were no longer underground and played an important role in the kink community.

Today the Leather Movement is generally seen as a part of the BDSM-culture instead of as a development deriving from gay subculture, even if a huge part of the BDSM-subculture was gay in the past. In the 1990s the so-called New Guard leather subculture evolved. This new orientation started to integrate psychological aspects into their play.

The San Francisco South of Market Leather History Alley consists of four works of art along Ringold Alley honoring leather culture; it opened in 2017. One of the works of art is metal bootprints along the curb which honor 28 people (including Steve McEachern, owner of the Catacombs, a gay and lesbian S/M fisting club, and Cynthia Slater, a founder of the Society of Janus, the second oldest BDSM organization in the United States) who were an important part of the leather communities of San Francisco.

In the late-eighties, the Internet provided a way of finding people with specialized interests around the world as well as on a local level, and communicating with them anonymously. This brought about an explosion of interest and knowledge of BDSM, particularly on the usenet group alt.sex.bondage. When that group became too cluttered with spam, the focus moved to soc.subculture.bondage-bdsm. With an increased focus on forms of social media, FetLife was formed, which advertises itself as "a social network for the BDSM and fetish community". It operates similarly to other social media sites, with the ability to make friends with other users, events, and pages of shared interests.

In addition to traditional sex shops, which sell sex paraphernalia, there has also been an explosive growth of online adult toy companies that specialize in leather/latex gear and BDSM toys. Once a very niche market, there are now very few sex toy companies that do not offer some sort of BDSM or fetish gear in their catalog. Kinky elements seem to have worked their way into "vanilla" markets. The former niche expanded to an important pillar of the business with adult accessories. Today practically all suppliers of sex toys do offer items which originally found usage in the BDSM subculture. Padded handcuffs, latex and leather garments, as well as more exotic items like soft whips for fondling and TENS for erotic electro stimulation can be found in catalogs aiming on classical vanilla target groups, indicating that former boundaries increasingly seem to shift.

During the last years the Internet also provides a central platform for networking among individuals who are interested in the subject. Besides countless private and commercial choices there is an increasing number of local networks and support groups emerging. These groups often offer comprehensive background and health related information for people who have been unwillingly outed as well as contact lists with information on psychologists, physicians and lawyers who are familiar with BDSM related topics.

Increasingly, universities across the western world are witnessing BDSM and kink education providing student clubs coming up. Some of these are even officially recognized, such as at USA's Stanford University, Harvard University, University of Chicago, UC Berkeley, University of Southern California, Kent State University, Cornell University, Princeton University, Columbia University's BDSM club Conversio Virium and Iowa State University's BDSM club Cuffs. Other universities in USA having BDSM clubs include Tufts University, MIT, Yale University, Brown University, University of Pennsylvania, University of Michigan, Northwestern University, California State University at Northridge, San Francisco State University and Washington University in St. Louis. In Canada, it is present in University of Victoria in British Columbia, McGill University, York University, University of Calgary, among others. In UK, BDSM clubs are at University of York, University of East Anglia, Birmingham University, Imperial College London, etc. 

Some of the universities in USA (like Indiana University and Michigan State University) also have professors who do research and take classes on BDSM.

Section 90 of the criminal code declares bodily injury (§§ 83-84) or the endangerment of physical security (§ 89) to not be subject to penalty in cases in which the victim has consented and the injury or endangerment does not offend moral sensibilities. Case law from the Austrian Supreme Court has consistently shown that bodily injury is only offensive to moral sensibilities, thus it is only punishable when a "serious injury" (a damage to health or an employment disability lasting more than 24 days) or the death of the "victim" results. A "light injury" is generally considered "permissible" when the "victim" has consented to it. In cases of threats to bodily well being the standard depends on the probability that an injury will actually occur. If serious injury or even death would be a likely result of a threat being carried out, then even the threat itself is considered punishable.

In 2004 a judge in Canada ruled that videos seized by the police featuring BDSM activities were not obscene, and did not constitute violence, but a "normal and acceptable" sexual activity between two consenting adults.

In 2011, the Supreme Court of Canada ruled in "R. v. J.A." that a person must have an active mind during the specific sexual activity in order to legally consent. The Court ruled that it is a criminal offence to perform a sexual act on an unconscious person—whether or not that person consented in advance.

According to § 194 the charge of insult (slander) can only be prosecuted if the defamed person chooses to press charges. False imprisonment can be charged if the victim—when applying an objective view—can be considered to be impaired in his or her rights of free movement. According to § 228 of the German criminal code a person inflicting a bodily injury on another person with that person's permission violates the law only in cases where the act can be considered to have violated good morals in spite of permission having been given. On 26 May 2004 the Criminal Panel No. 2 of the Bundesgerichtshof (German Federal Court) ruled that sado-masochistically motivated physical injuries are not per se indecent and thus subject to § 228.

Following cases in which sado-masochistic practices had been repeatedly used as pressure tactics against former partners in custody cases, the Appeals Court of Hamm ruled in February 2006 that sexual inclinations toward sado-masochism are no indication of a lack of capabilities for successful child-raising.

In Italian law BDSM is right on the border between crime and legality, and everything lies in the interpretation of the legal code by the judge. This concept is that anyone willingly causing "injury" to another person is to be punished. In this context though "injury" is legally defined as "anything causing a condition of illness", and "illness" is ill-defined itself in two different legal ways. The first is "any anatomical or functional alteration of the organism" (thus technically including little scratches and bruises too); The second is "a significant worsening of a previous condition relevant to organic and relational processes, requiring any kind of therapy". This could make it somewhat risky to play with someone as later the "victim" may call foul play citing even an insignificant mark as evidence against the partner. Also any injury requiring over 20 days of medical care must be denounced by the professional medic who discovers it, leading to automatic indictment of the person who caused it.

In September 2010 a Swedish court acquitted a 32-year-old man of assault for engaging in consensual BDSM play with a 16-year-old woman (the age of consent in Sweden is 15). Norway's legal system has likewise taken a similar position, that safe and consensual BDSM play should not be subject to criminal prosecution. This parallels the stance of the mental health professions in the Nordic countries which have removed sadomasochism from their respective lists of psychiatric illnesses.

The age of consent in Switzerland is 16 years which also applies for BDSM play. Minors (i.e. those under 16) are not subject to punishment for BDSM play as long as the age difference between them is less than three years. Certain practices however require granting consent for light injuries with only those over 18 permitted to give consent. On 1 April 2002 Articles 135 and 197 of the Swiss Criminal Code were tightened to make ownership of ""objects or demonstrations [...] which depict sexual acts with violent content"" a punishable offense. This law amounts to a general criminalization of sado-masochism since nearly every sado-masochist will have some kind of media which fulfills this criterion. Critics also object to the wording of the law which puts sado-masochists in the same category as pedophiles and pederasts.

In British law, consent is an absolute defence to common assault, but not necessarily to actual bodily harm, where courts may decide that consent is not valid, as occurred in the case of R v Brown. Accordingly, consensual activities in the UK may not constitute "assault occasioning actual or grievous bodily harm" in law. The Spanner Trust states that this is defined as activities which have caused injury "of a lasting nature" but that only a slight duration or injury might be considered "lasting" in law. The decision contrasts with the later case of R v Wilson in which conviction for non-sexual consensual branding within a marriage was overturned, the appeal court ruling that R v Brown was not an authority in all cases of consensual injury and criticizing the decision to prosecute.

Following Operation Spanner the European Court of Human Rights ruled in January 1999 in Laskey, Jaggard and Brown v. United Kingdom that no violation of Article 8 occurred because the amount of physical or psychological harm that the law allows between any two people, even consenting adults, is to be determined by the jurisdiction the individuals live in, as it is the State's responsibility to balance the concerns of public health and well-being with the amount of control a State should be allowed to exercise over its citizens. In the Criminal Justice and Immigration Bill 2007, the British Government cited the Spanner case as justification for criminalizing images of consensual acts, as part of its proposed criminalization of possession of ""extreme pornography"". Another contrasting case was that of Stephen Lock in 2013, who was cleared of actual bodily harm on the grounds that the woman consented. In this case, the act was deemed to be sexual.

The United States Federal law does not list a specific criminal determination for consensual BDSM acts. Many BDSM practitioners cite the legal decision of People v. Jovanovic, 95 N.Y.2d 846 (2000), or the "Cybersex Torture Case", which was the first U.S. appellate decision to hold (in effect) that one does not commit assault if the victim consents. However, many individual states do criminalize specific BDSM actions within their state borders. Some states specifically address the idea of "consent to BDSM acts" within their assault laws, such as the state of New Jersey, which defines "simple assault" to be "a disorderly persons offense unless committed in a fight or scuffle "entered into by mutual consent", in which case it is a petty disorderly persons offense".

Oregon Ballot Measure 9 was a ballot measure in the U.S. state of Oregon in 1992, concerning sadism, masochism, gay rights, pedophilia, and public education, that drew widespread national attention. It would have added the following text to the Oregon Constitution:

It was defeated in 3 November 1992 general election with 638,527 votes in favor, 828,290 votes against.

The National Coalition for Sexual Freedom collects reports about punishment for sexual activities engaged in by consenting adults, and about its use in child custody cases.

Today the BDSM culture exists in most western countries. This offers BDSM practitioners the opportunity to discuss BDSM relevant topics and problems with like-minded people. This culture is often viewed as a subculture, mainly because BDSM is often still regarded as "unusual" by some of the public. Many people hide their leaning from society since they are afraid of the incomprehension and of social exclusion. It is commonly known in the BDSM culture that there are practitioners living on all continents, but there is no documented evidence for many countries (due to restrictive laws and censorship motivated by politics or religion) except their presence in online BDSM communities and dating sites.

In contrast to frameworks seeking to explain sadomasochism through psychological, psychoanalytic, medical or forensic approaches, which seek to categorize behaviour and desires and find a root "cause", Romana Byrne suggests that such practices can be seen as examples of "aesthetic sexuality", in which a founding physiological or psychological impulse is irrelevant. Rather, sadism and masochism may be practiced through choice and deliberation, driven by certain aesthetic goals tied to style, pleasure, and identity. These practices, in certain circumstances and contexts, can be compared with the creation of art.

One of the most commonly used symbols of the BDSM community is a derivation of a triskelion shape within a circle. Various forms of triskele have had many uses and many meanings in many cultures; its BDSM usage derives from the "Ring of O" in the classic book "Story of O". The BDSM Emblem Project claims copyright over one particular specified form of the triskelion symbol; other variants of the triskelion are free from such copyright claims.

The leather pride flag is a symbol for the leather subculture and also widely used within BDSM. In continental Europe, the "Ring of O" is widespread among BDSM practitioners.

The triskelion as a BDSM symbol can easily be perceived as the three separate parts of the acronym BDSM; which are BD, DS, and SM (Bondage & Discipline, Dominance & Submission, Sadism & Masochism). They are three separate items, that are normally associated together.

The BDSM rights flag, shown to the right, is intended to represent the belief that people whose sexuality or relationship preferences include BDSM practises deserve the same human rights as everyone else, and should not be discriminated against for pursuing BDSM with consenting adults.

The flag is inspired by the leather pride flag and BDSM emblem, but is specifically intended to represent the concept of BDSM rights and to be without the other symbols' restrictions against commercial use. It is designed to be recognisable by people familiar with either the leather pride flag or BDSM triskelion (or triskele) as "something to do with BDSM"; and to be distinctive whether reproduced in full colour, or in black and white (or another pair of colours).

BDSM and fetish items and styles have been spread widely in western societies' everyday life by different factors, such as avant-garde fashion, heavy metal, goth subculture, and science fiction TV series, and are often not consciously connected with their BDSM roots by many people. While it was mainly confined to the Punk and BDSM subcultures in the 1990s, it has since spread into wider parts of western societies.


Although it would be possible to establish certain elements related to BDSM in classical theater, not until the emergence of contemporary theatre would some plays have BDSM as the main theme. Exemplifying this are two works: one Austrian, one German, in which BDSM is not only incorporated, but integral to the storyline of the play.

Although examples of literature catering to BDSM and fetishistic tastes were created in earlier periods, BDSM literature as it exists today cannot be found much earlier than World War II.

The word Sadism originates from the works of Donatien Alphonse François, Marquis de Sade, and the word Masochism originates from Leopold von Sacher-Masoch, the author of "Venus in Furs". However, it is worth noting that the Marquis de Sade describes unconsented abuse in his works, such as in "Justine". "Venus in Furs" describes a consented domme-sub relationship.

A central work in modern BDSM literature is undoubtedly the "Story of O" (1954) by Anne Desclos under the pseudonym Pauline Réage.

Other notable works include "9½ Weeks" (1978) by Elizabeth McNeill, some works of the writer Anne Rice ("Exit to Eden", and her "Claiming of Sleeping Beauty" series of books), Jeanne de Berg ("L'Image" (1956) dedicated to Pauline Réage). Works from the Gor series by John Norman, and naturally all the works of Patrick Califia, Gloria Brame, the group Samois and many of the writer Georges Bataille (Histoire de l'oeil-Story of the Eye, Madame Edwarda, 1937), as well as Bob Flanagan: "Slave Sonnets" (1986), "Fuck Journal" (1987), "A Taste of Honey" (1990). A common part of many of the poems of Pablo Neruda is a reflection on feelings and sensations arising from the relations of EPE or erotic exchange of power. The "Fifty Shades" trilogy is a series of very popular erotic romance novels by E. L. James which involve BDSM; however the novels have been criticized for their inaccurate and harmful depiction of BDSM.

In the 21st century, a number of prestigious university press, such as of Duke University, Indiana University and University of Chicago, have published books on BDSM written by professors, thereby lending academic legitimacy to this once taboo topic.




</doc>
<doc id="4547" url="https://en.wikipedia.org/wiki?curid=4547" title="Bash (Unix shell)">
Bash (Unix shell)

Bash is a Unix shell and command language written by Brian Fox for the GNU Project as a free software replacement for the Bourne shell. First released in 1989, it has been distributed widely as the default login shell for most Linux distributions and Apple's macOS (formerly ). A version is also available for Windows 10. It is also the default user shell in Solaris 11. 

Bash is a command processor that typically runs in a text window where the user types commands that cause actions. Bash can also read and execute commands from a file, called a shell script. Like all Unix shells, it supports filename globbing (wildcard matching), piping, here documents, command substitution, variables, and control structures for condition-testing and iteration. The keywords, syntax and other basic features of the language are all copied from sh. Other features, e.g., history, are copied from csh and ksh. Bash is a POSIX-compliant shell, but with a number of extensions.

The shell's name is an acronym for "Bourne-again shell", a pun on the name of the Bourne shell that it replaces
and on the common term "born again".

A security hole in Bash dating from version 1.03 (August 1989), dubbed Shellshock, was discovered in early September 2014 and quickly led to a range of attacks across the Internet. Patches to fix the bugs were made available soon after the bugs were identified, but not all computers have been updated.

Brian Fox began coding Bash on January 10, 1988 after Richard Stallman became dissatisfied with the lack of progress being made by a prior developer. Stallman and the Free Software Foundation (FSF) considered a free shell that could run existing shell scripts so strategic to a completely free system built from BSD and GNU code that this was one of the few projects they funded themselves, with Fox undertaking the work as an employee of FSF. Fox released Bash as a beta, version .99, on June 8, 1989 and remained the primary maintainer until sometime between mid-1992 and mid-1994, when he was laid off from FSF and his responsibility was transitioned to another early contributor, Chet Ramey.

Since then, Bash has become by far the most popular shell among users of Linux, becoming the default interactive shell on that operating system's various distributions (although Almquist shell may be the default scripting shell) and on Apple's macOS. Bash has also been ported to Microsoft Windows and distributed with Cygwin and MinGW, to DOS by the DJGPP project, to Novell NetWare and to Android via various terminal emulation applications.

In September 2014, Stéphane Chazelas, a Unix/Linux specialist, discovered a security bug in the program. The bug, first disclosed on September 24, was named Shellshock and assigned the numbers CVE-2014-6271, CVE-2014-6277 and CVE-2014-7169. The bug was regarded as severe, since CGI scripts using Bash could be vulnerable, enabling arbitrary code execution. The bug was related to how Bash passes function definitions to subshells through environment variables.

The Bash command syntax is a superset of the Bourne shell command syntax. Bash can execute the vast majority of Bourne shell scripts without modification, with the exception of Bourne shell scripts stumbling into fringe syntax behavior interpreted differently in Bash or attempting to run a system command matching a newer Bash builtin, etc. Bash command syntax includes ideas drawn from the Korn shell (ksh) and the C shell (csh) such as command line editing, command history, the directory stack, the codice_1 and codice_2 variables, and POSIX command substitution syntax codice_3.

When a user presses the tab key within an interactive command-shell, Bash automatically uses command line completion, since beta version of 2.04, to match partly typed program names, filenames and variable names. The Bash command-line completion system is very flexible and customizable, and is often packaged with functions that complete arguments and filenames for specific programs and tasks.

Bash's syntax has many extensions lacking in the Bourne shell. Bash can perform integer calculations ("arithmetic evaluation") without spawning external processes. It uses the codice_4 command and the codice_5 variable syntax for this purpose. Its syntax simplifies I/O redirection. For example, it can redirect standard output (stdout) and standard error (stderr) at the same time using the codice_6 operator. This is simpler to type than the Bourne shell equivalent 'codice_7'. Bash supports process substitution using the codice_8 and codice_9syntax, which substitutes the output of (or input to) a command where a filename is normally used. (This is implemented through "/proc/fd/" unnamed pipes on systems that support that, or via temporary named pipes where necessary).

When using the 'function' keyword, Bash function declarations are not compatible with Bourne/Korn/POSIX scripts (the Korn shell has the same problem when using 'function'), but Bash accepts the same function declaration syntax as the Bourne and Korn shells, and is POSIX-conformant. Because of these and other differences, Bash shell scripts are rarely runnable under the Bourne or Korn shell interpreters unless deliberately written with that compatibility in mind, which is becoming less common as Linux becomes more widespread. But in POSIX mode, Bash conforms with POSIX more closely.

Bash supports here documents. Since version 2.05b Bash can redirect standard input (stdin) from a "here string" using the codice_10 operator.

Bash 3.0 supports in-process regular expression matching using a syntax reminiscent of Perl.

Bash 4.0, introduced in February 2009, support for associative arrays. Associative arrays allow a fake support for multi-dimensional (indexed) arrays, in a similar way to AWK. Bash 4.x has not been integrated in newer version of MacOS due to license restrictions.. Associative array example:

Brace expansion, also called alternation, is a feature copied from the C shell. It generates a set of alternative combinations. Generated results need not exist as files. The results of each expanded string are not sorted and left to right order is preserved:
Users should not use brace expansions in portable shell scripts, because the Bourne shell does not produce the same output.

When brace expansion is combined with wildcards, the braces are expanded first, and then the resulting wildcards are substituted normally. Hence, a listing of JPEG and PNG images in the current directory could be obtained using:

In addition to alternation, brace expansion can be used for sequential ranges between two integers or characters separated by double dots. Newer versions of Bash allow a third integer to specify the increment.

When brace expansion is combined with variable expansion the variable expansion is performed after the brace expansion, which in some cases may necessitate the use of the codice_11 built-in, thus:

When Bash starts, it executes the commands in a variety of dot files. Though similar to Bash shell script commands, which have execute permission enabled and an interpreter directive like codice_12, the initialization files used by Bash require neither.

Elements of Bash derive from the Bourne shell and csh. These allow limited startup file sharing with the Bourne shell and provide some startup features familiar to csh users.

The skeleton codice_13 below is compatible with the Bourne shell and gives semantics similar to csh for the codice_14 and codice_15. The codice_16 are tests to see if the "filename" exists and is readable, simply skipping the part after the codice_17 if it's not.

Some versions of Unix and Linux contain Bash system startup scripts, generally under the codice_18 directories. Bash calls these as part of its standard initialization, but other startup files can read them in a different order than the documented Bash startup sequence. The default content of the root user's files may also have issues, as well as the skeleton files the system provides to new user accounts upon setup. The startup scripts that launch the X window system may also do surprising things with the user's Bash startup scripts in an attempt to set up user-environment variables before launching the window manager. These issues can often be addressed using a codice_19 or codice_20 file to read the codice_21 — which provides the environment variables that Bash shell windows spawned from the window manager need, such as xterm or Gnome Terminal.

Invoking Bash with the codice_22 option or stating codice_23 in a script causes Bash to conform very closely to the POSIX 1003.2 standard.
Bash shell scripts intended for portability should at least take into account the Bourne shell it intends to replace. Bash has certain features that the traditional Bourne shell lacks. They include:


A "bashism" is a portion of bash code that does not run properly on other Unix shells.

Bash uses readline to provide keyboard shortcuts for command line editing using the default (Emacs) key bindings. Vi-bindings can be enabled by running codice_24.

The Bash shell has two modes of execution for commands: batch, and concurrent mode.

To execute commands in batch (i.e., in sequence) they must be separated by the character ";", or on separate lines:

in this example, when command1 is finished, command2 is executed.

You can also have a background execution of command1 using (symbol &) at the end of your execution command, and process will be executed in background returning inmmediatly control to your shell and allowing you to keep executing commands.
Or to have a concurrent execution of two command1 and command2, they must be executed in the Bash shell in the following way:

In this case command1 is executed in the background "&" symbol, returning immediately control to the shell that executes command2 in the foreground.

A process can be stopped and returned control to bash by typing while the process is running in the foreground.

A list of all processes, both in the background and stopped, can be achieved by running codice_25:
In the output, the number in brackets refers to the job id. The plus sign signifies the default process for codice_26 and codice_27. The text "Running" and "Stopped" refer to the Process state. The last string is the command that started the process. 

The state of a process can be changed using various commands. The codice_27 command brings a process to the foreground, while the codice_26 sets a stopped process running in the background. codice_26 and codice_27 can take a job id as their first argument, to specify the process to act on. Without one, they use the default process, identified by a plus sign in the output of codice_25. The codice_33 command can be used to end a process prematurely, by sending it a signal. The job id must be specified after a percent sign:

Bash supplies "conditional execution" command separators that make execution of a command contingent on the exit code set by a precedent command. For example:

Where "./do_something" is only executed if the "cd" (change directory) command was "successful" (returned an exit status of zero) and the "echo" command would only be executed if either the "cd" or the "./do_something" command return an "error" (non-zero exit status).

For all commands the exit status is stored in the special variable codice_34. Bash also supports and forms of conditional command evaluation.

An external command called "bashbug" reports Bash shell bugs. When the command is invoked, it brings up the user's default editor with a form to fill in. The form is mailed to the Bash maintainers (or optionally to other email addresses).




</doc>
<doc id="4548" url="https://en.wikipedia.org/wiki?curid=4548" title="Blizzard">
Blizzard

A blizzard is a severe snowstorm characterized by strong sustained winds of at least and lasting for a prolonged period of time—typically three hours or more. A ground blizzard is a weather condition where snow is not falling but loose snow on the ground is lifted and blown by strong winds. Blizzards can have an immense size, which can usually be larger than a few states in the United States.
In the United States, the National Weather Service defines a blizzard as a severe snow storm characterized by strong winds causing blowing snow that results in low visibilities. The difference between a blizzard and a snowstorm is the strength of the wind, not the amount of snow. To be a blizzard, a snow storm must have sustained winds or frequent gusts that are greater than or equal to with blowing or drifting snow which reduces visibility to or less and must last for a prolonged period of time—typically three hours or more.

While severe cold and large amounts of drifting snow may accompany blizzards, they are not required. Blizzards can bring whiteout conditions, and can paralyze regions for days at a time, particularly where snowfall is unusual or rare.

A severe blizzard has winds over , near zero visibility, and temperatures of or lower. In Antarctica, blizzards are associated with winds spilling over the edge of the ice plateau at an average velocity of .

Ground blizzard refers to a weather condition where loose snow or ice on the ground is lifted and blown by strong winds. The primary difference between a ground blizzard as opposed to a regular blizzard is that in a ground blizzard no precipitation is produced at the time, but rather all the precipitation is already present in the form of snow or ice at the surface.

The Australia Bureau of Meteorology describes a blizzard as, "Violent and very cold wind which is laden with snow, some part, at least, of which has been raised from snow covered ground."
The "Oxford English Dictionary" concludes the term "blizzard" is likely onomatopoeic, derived from the same sense as "blow, blast, blister, and bluster"; the first recorded use of it for weather dates to 1829, when it was defined as a "violent blow". It achieved its modern definition by 1859, when it was in use in the western United States. The term became common in the press during the harsh winter of 1880–81.

In the United States, storm systems powerful enough to cause blizzards usually form when the jet stream dips far to the south, allowing cold, dry polar air from the north to clash with warm, humid air moving up from the south.

When cold, moist air from the Pacific Ocean moves eastward to the Rocky Mountains and the Great Plains, and warmer, moist air moves north from the Gulf of Mexico, all that is needed is a movement of cold polar air moving south to form potential blizzard conditions that may extend from the Texas Panhandle to the Great Lakes and Midwest. A blizzard also may be formed when a cold front and warm front mix together and a blizzard forms at the border line.

Another storm system occurs when a cold core low over the Hudson Bay area in Canada is displaced southward over southeastern Canada, the Great Lakes, and New England. When the rapidly moving cold front collides with warmer air coming north from the Gulf of Mexico, strong surface winds, significant cold air advection, and extensive wintry precipitation occur.

Low pressure systems moving out of the Rocky Mountains onto the Great Plains, a broad expanse of flat land, much of it covered in prairie, steppe and grassland, can cause thunderstorms and rain to the south and heavy snows and strong winds to the north. With few trees or other obstructions to reduce wind and blowing, this part of the country is particularly vulnerable to blizzards with very low temperatures and whiteout conditions. In a true whiteout there is no visible horizon. People can become lost in their own front yards, when the door is only away, and they would have to feel their way back. Motorists have to stop their cars where they are, as the road is impossible to see.

A nor'easter is a macro-scale storm that occurs off the New England and Atlantic Canada coastlines. It gets its name from the direction the wind is coming from. The usage of the term in North America comes from the wind associated with many different types of storms some of which can form in the North Atlantic Ocean and some of which form as far south as the Gulf of Mexico. The term is most often used in the coastal areas of New England and Atlantic Canada. This type of storm has characteristics similar to a hurricane. More specifically it describes a low-pressure area whose center of rotation is just off the coast and whose leading winds in the left-forward quadrant rotate onto land from the northeast. High storm waves may sink ships at sea and cause coastal flooding and beach erosion. Notable nor'easters include The Great Blizzard of 1888, one of the worst blizzards in U.S. history. It dropped of snow and had sustained winds of more than that produced snowdrifts in excess of . Railroads were shut down and people were confined to their houses for up to a week. It killed 400 people, mostly in New York.

The 1972 Iran Blizzard, which caused 4,000 reported deaths, was the deadliest blizzard in recorded history. Dropping as much as of snow, it completely covered 200 villages. After a snowfall lasting nearly a week, an area the size of Wisconsin was entirely buried in snow.

The winter of 1880–1881 is widely considered the most severe winter ever known in parts of the United States. Many children—and their parents—learned of "The Snow Winter" through the children's book "The Long Winter" by Laura Ingalls Wilder, in which the author tells of her family's efforts to survive. The snow arrived in October 1880 and blizzard followed blizzard throughout the winter and into March 1881, leaving many areas snowbound throughout the entire winter. Accurate details in Wilder's novel include the blizzards' frequency and the deep cold, the Chicago and North Western Railway stopping trains until the spring thaw because the snow made the tracks impassable, the near-starvation of the townspeople, and the courage of her future husband Almanzo and another man, who ventured out on the open prairie in search of a cache of wheat that no one was even sure existed.

The October blizzard brought snowfalls so deep that two-story homes had snow up to the second floor windows. No one was prepared for the deep snow so early in the season and farmers all over the region were caught before their crops had even been harvested, their grain milled, or with their fuel supplies for the winter in place. By January the train service was almost entirely suspended from the region. Railroads hired scores of men to dig out the tracks but it was a wasted effort: As soon as they had finished shoveling a stretch of line, a new storm arrived, filling up the line and leaving their work useless.

There were no winter thaws and on February 2, 1881, a second massive blizzard struck that lasted for nine days. In the towns the streets were filled with solid drifts to the tops of the buildings and tunneling was needed to secure passage about town. Homes and barns were completely covered, compelling farmers to tunnel to reach and feed their stock.

When the snow finally melted in late spring of 1881, huge sections of the plains were flooded. Massive ice jams clogged the Missouri River and when they broke the downstream areas were ravaged. Most of the town of Yankton, in what is now South Dakota, was washed away when the river overflowed its banks.

The Storm of the Century, also known as the Great Blizzard of 1993, was a large cyclonic storm that formed over the Gulf of Mexico on March 12, 1993, and dissipated in the North Atlantic Ocean on March 15. It is unique for its intensity, massive size and wide-reaching effect. At its height, the storm stretched from Canada towards Central America, but its main impact was on the United States and Cuba. The cyclone moved through the Gulf of Mexico, and then through the Eastern United States before moving into Canada. Areas as far south as northern Alabama and Georgia received a dusting of snow and areas such as Birmingham, Alabama, received up to with hurricane-force wind gusts and record low barometric pressures. Between Louisiana and Cuba, hurricane-force winds produced high storm surges across northwestern Florida, which along with scattered tornadoes killed dozens of people. In the United States, the storm was responsible for the loss of electric power to over 10 million customers. It is purported to have been directly experienced by nearly 40 percent of the country's population at that time. A total of 310 people, including 10 from Cuba, perished during this storm. The storm cost $6 to $10 billion in damages.
















</doc>
<doc id="4550" url="https://en.wikipedia.org/wiki?curid=4550" title="Bikini">
Bikini

Bikini typically describes a women's simple two-piece swimsuit featuring two triangles of fabric on top, similar to a bra and covering the woman's breasts, and two triangles of fabric on the bottom, the front covering the pelvis but exposing the navel, and the back covering the buttocks. The size of the top and bottom can vary from full coverage of the breasts, pelvis, and buttocks, to very skimpy designs like a thong or G-string that cover only the areola and mons pubis, but expose the buttocks. 

Fashion designer Jacques Heim from Paris released a two-piece swimsuit design in 1946 that he named the Atome. Like swimsuits of the era, it covered the wearer's navel, and it failed to attract much attention. Parisian automotive engineer and designer Louis Réard introduced his design soon after. His skimpy design was risque, exposing the wearer's navel and much of her buttocks. No runway model would wear it, so he hired a nude dancer from the Casino de Paris to model it at a review of swimsuit fashions. He named the swimsuit after the Bikini Atoll, where the first public test of a nuclear bomb had taken place only four days before. 

Due to its controversial and revealing design, the bikini was accepted very slowly by the public. It gained increased exposure and acceptance as film stars like Brigitte Bardot, Raquel Welch, and Ursula Andress wore them and were photographed on public beaches and seen in film. In many countries the design was banned from beaches and other public places. 

The minimalist bikini design became common in most Western countries by the mid-1960s as both swimwear and underwear. By the late 20th century it was widely used as sportswear in beach volleyball and bodybuilding. There are a number of modern stylistic variations of the design used for marketing purposes and as industry classifications, including monokini, microkini, tankini, trikini, pubikini, and skirtini. A man's single-piece brief swimsuit may also be referred to as a bikini. Similarly, a variety of men's and women's underwear types are described as bikini underwear.

The bikini has gradually grown to gain wide acceptance in Western society. By the early 2000s, bikinis had become a US$811 million business annually, and boosted spin-off services such as bikini waxing and sun tanning.

While the two-piece swimsuit as a design existed in classical antiquity, the modern design first attracted public notice in Paris on July 5, 1946. French automotive engineer Louis Réard introduced a design he named the "bikini", adopting the name from the Bikini Atoll in the Pacific Ocean, which was the colonial name the Germans gave to the atoll, transliterated from the Marshallese name for the island, . 

Four days earlier, the United States had initiated its first peace-time nuclear weapons test at Bikini Atoll as part of Operation Crossroads. Réard hoped his swimsuit's revealing style would create an "explosive commercial and cultural reaction" similar to the explosion at Bikini Atoll.

By making an analogy with words like "bilingual" and "bilateral" containing the Latin prefix "bi-" (meaning "two" in Latin), the word "bikini" was first back-derived as consisting of two parts, ["bi" + "kini"] by Rudi Gernreich, who introduced the monokini in 1964. Later swimsuit designs like the tankini and trikini further cemented this derivation. Over time the ""–kini" family" (as dubbed by author William Safire), including the ""–ini" sisters" (as dubbed by designer Anne Cole), expanded into a variety of swimwear including the monokini (also known as a numokini or unikini), seekini, tankini, camikini, hikini (also hipkini), minikini, face-kini, burkini, and microkini. The "Language Report", compiled by lexicographer Susie Dent and published by the Oxford University Press (OUP) in 2003, considers lexicographic inventions like bandeaukini and camkini, two variants of the tankini, important to observe. Although "bikini" was originally a registered trademark of Réard, it has since become genericized.

Variations of the term are used to describe stylistic variations for promotional purposes and industry classifications, including monokini, microkini, tankini, trikini, pubikini, bandeaukini and skirtini. A man's brief swimsuit may also be referred to as a bikini. Similarly, a variety of men's and women's underwear types are described as bikini underwear.

Archaeologist James Mellaart described the earliest bikini-like costume in Çatalhöyük, Anatolia in the Chalcolithic era (around 5600 BC), where a mother goddess is depicted astride two leopards wearing a costume somewhat like a bikini. The two-piece swimsuit can be traced back to the Greco-Roman world, where bikini-like garments worn by women athletes are depicted on urns and paintings dating back to 1400 BC.

In "Coronation of the Winner," a mosaic in the floor of a Roman villa in Sicily that dates from the Diocletian period (286–305 AD), young women participate in weightlifting, discus throwing, and running ball games dressed in bikini-like garments (technically bandeaukinis in modern lexicon). The mosaic, found in the Sicilian Villa Romana del Casale, features ten maidens who have been anachronistically dubbed the "Bikini Girls". Other Roman archaeological finds depict the goddess Venus in a similar garment. In Pompeii, depictions of Venus wearing a bikini were discovered in the Casa della Venere, in the "tablinum" of the House of Julia Felix, and in an atrium garden of Via Dell'Abbondanza.

Swimming or bathing outdoors was discouraged in the Christian West, so there was little demand or need for swimming or bathing costumes until the 18th century. The bathing gown of the 18th century was a loose ankle-length full-sleeve chemise-type gown made of wool or flannel that retained coverage and modesty.

In 1907, Australian swimmer and performer Annette Kellerman was arrested on a Boston beach for wearing form-fitting sleeveless one-piece knitted swimming tights that covered her from neck to toe, a costume she adopted from England, although it became accepted swimsuit attire for women in parts of Europe by 1910. In 1913, designer Carl Jantzen made the first functional two-piece swimwear. Inspired by the introduction of females into Olympic swimming he designed a close-fitting costume with shorts for the bottom and short sleeves for the top.

During the 1920s and 1930s, people began to shift from "taking in the water" to "taking in the sun", at bathhouses and spas, and swimsuit designs shifted from functional considerations to incorporate more decorative features. Rayon was used in the 1920s in the manufacture of tight-fitting swimsuits, but its durability, especially when wet, proved problematic. Jersey and silk were also sometimes used. By the 1930s, manufacturers had lowered necklines in the back, removed sleeves, and tightened the sides. With the development of new clothing materials, particularly latex and nylon, swimsuits gradually began hugging the body through the 1930s, with shoulder straps that could be lowered for tanning.

Women's swimwear of the 1930s and 1940s incorporated increasing degrees of midriff exposure. The 1932 Hollywood film "Three on a Match" featured a midriff baring two piece bathing suit. Teen magazines of late 1940s and 1950s featured similar designs of midriff-baring suits and tops. However, midriff fashion was stated as only for beaches and informal events and considered indecent to be worn in public. Hollywood endorsed the new glamor in films like 1949's "Neptune's Daughter" in which Esther Williams wore provocatively named costumes such as "Double Entendre" and "Honey Child".

Wartime production during World War II required vast amounts of cotton, silk, nylon, wool, leather, and rubber. In 1942, the United States War Production Board issued Regulation L-85, cutting the use of natural fibers in clothing and mandating a 10% reduction in the amount of fabric in women's beachwear. To comply with the regulations, swimsuit manufacturers removed skirt panels and other attachments, while increasing production of the two-piece swimsuit with bare midriffs. At the same time, demand for all swimwear declined as there was not much interest in going to the beach, especially in Europe.

In post-World War II Europe, Western Europeans enjoyed their first war-free summer in many years. French designers sought to deliver fashions that matched the liberated mood of the people. Fabric was still in short supply, and in an endeavor to resurrect swimwear sales, two French designers – Jacques Heim and Louis Réard  – almost simultaneously launched new two-piece swimsuit designs in 1946. Heim launched a two-piece swimsuit design in Paris that he called the "atome", after the smallest known particle of matter. He announced that it was the "world's smallest bathing suit." Although briefer than the two-piece swimsuits of the 1930s, the bottom of Heim's new two-piece beach costume still covered the wearer's navel.

Soon after, Louis Réard created a competing two-piece swimsuit design, which he called the "bikini". He noticed that women at the beach rolled up the edges of their swimsuit bottoms and tops to improve their tan. He introduced his design at a swimsuit review held at the popular public pool, Piscine Molitor, four days after the first test of a nuclear weapon at the Bikini Atoll. The newspapers were full of news about it and Reard hoped for the same with his design. Réard's "bikini" undercut Heim's "atome" in its brevity. His design consisted of a two triangles of fabric forming a bra, and two triangular pieces of fabric covering the mons pubis and the buttocks connected by string. When he was unable to find a fashion model willing to showcase his revealing design, Réard hired Micheline Bernardini, a 19-year old nude dancer from the Casino de Paris. He announced that his swimsuit, with a total area of of cloth, was "smaller than the world’s smallest bathing suit.". Réard said that "like the [atom] bomb, the bikini is small and devastating". Fashion writer Diana Vreeland described the bikini as the "atom bomb of fashion". Bernardini received 50,000 fan letters, many of them from men.

Photographs of Bernardini and articles about the event were widely carried by the press. The "International Herald Tribune" alone ran nine stories on the event. French newspaper "Le Figaro" wrote, "People were craving the simple pleasures of the sea and the sun. For women, wearing a bikini signaled a kind of second liberation. There was really nothing sexual about this. It was instead a celebration of freedom and a return to the joys in life." 

Heim's "atome" was more in keeping with the sense of propriety of the 1940s, but Réard's design won the public's attention. Though Heim's design was the first worn on the beach and initially sold more swimsuits, it was Réard's description of the two-piece swimsuit as a "bikini" that stuck. As competing designs emerged, he declared in advertisements that a swimsuit could not be a genuine bikini "unless it could be pulled through a wedding ring." Modern bikinis were first made of cotton and jersey.

Despite the garment's initial success in France, women worldwide continued to wear traditional one-piece swimsuits. When his sales stalled, Réard went back to designing and selling orthodox knickers. In 1950, American swimsuit mogul Fred Cole, owner of mass market swimwear firm Cole of California, told "Time" that he had "little but scorn for France's famed Bikinis." Réard himself would later describe it as a "two-piece bathing suit which reveals everything about a girl except for her mother's maiden name." Fashion magazine "Modern Girl Magazine" in 1957 stated that "it is hardly necessary to waste words over the so-called bikini since it is inconceivable that any girl with tact and decency would ever wear such a thing".

In 1951, Eric Morley organized the "Festival Bikini Contest", a beauty contest and swimwear advertising opportunity at that year's Festival of Britain. The press, welcoming the spectacle, referred to it as "Miss World", a name Morley registered as a trademark. The winner was Kiki Håkansson of Sweden, who was crowned in a bikini. After the crowning, Håkansson was condemned by Pope Pius XII, while Spain and Ireland threatened to withdraw from the pageant. In 1952, bikinis were banned from the pageant and replaced by evening gowns. As a result of the controversy, the bikini was explicitly banned from many other beauty pageants worldwide. Though some regarded the bikini and beauty contests as bringing freedom to women, they were opposed by some feminists as well as religious and cultural groups who objected to the degree of exposure of the female body.

The bikini was banned on the French Atlantic coastline, Spain, Italy, Portugal and Australia, and was prohibited or discouraged in a number of US states. The United States Motion Picture Production Code, also known as the Hays Code, enforced from 1934, allowed two-piece gowns but prohibited the display of navels in Hollywood films. The National Legion of Decency, a Roman Catholic body guarding over American media content, also pressured Hollywood and foreign film producers to keep bikinis from being featured in Hollywood movies. As late as 1959, Anne Cole, one of the United States' largest swimsuit designers, said, "It's nothing more than a G-string. It's at the razor's edge of decency." The Hays Code was abandoned by the mid-1960s, and with it the prohibition of female navel exposure, as well as other restrictions. The influence of the National Legion of Decency also waned by the 1960s.

Increasingly common glamour shots of popular actresses and models on either side of the Atlantic played a large part in bringing the bikini into the mainstream. During the 1950s, Hollywood stars such as Ava Gardner, Rita Hayworth, Lana Turner, Elizabeth Taylor, Tina Louise, Marilyn Monroe, Esther Williams, and Betty Grable took advantage of the risqué publicity associated with the bikini by posing for photographs wearing them—pin-ups of Hayworth and Williams in costume were especially widely distributed in the United States. In 1950, Elvira Pagã walked at the Rio Carnival, Brazil in a golden bikini, starting the bikini tradition of the carnival.

In Europe, 17-year-old Brigitte Bardot wore scanty bikinis (by contemporary standards) in the French film "Manina, la fille sans voiles" ("Manina, the girl unveiled"). The promotion for the film, released in France in March 1953, drew more attention to Bardot's bikinis than to the film itself. By the time the film was released in the United States in 1958 it was re-titled "Manina, the Girl in the Bikini". Bardot was also photographed wearing a bikini on the beach during the 1957 Cannes Film Festival. Working with her husband and agent Roger Vadim she garnered significant attention with photographs of her wearing a bikini on every beach in the south of France.

Similar photographs were taken of Anita Ekberg and Sophia Loren, among others. According to "The Guardian", Bardot's photographs in particular turned Saint-Tropez into the beachwear capital of the world, with Bardot identified as the original Cannes bathing beauty. Bardot's photography helped to enhance the public profile of the festival, and Cannes in turn played a crucial role in her career.

Brian Hyland's novelty-song hit "Itsy Bitsy Teenie Weenie Yellow Polka Dot Bikini" became a "Billboard" No. 1 hit during the summer of 1960: the song tells a story about a young girl who is too shy to wear her new bikini on the beach, thinking it too risqué. "Playboy" first featured a bikini on its cover in 1962; the "Sports Illustrated Swimsuit Issue" debut two years later featured Babette March in a white bikini on the cover.

Ursula Andress, appearing as Honey Rider in the 1962 British James Bond film, "Dr. No", wore a white bikini, which became known as the "Dr. No bikini". It became one of the most famous bikinis of all time and an iconic moment in cinematic and fashion history. Andress said that she owed her career to that white bikini, remarking, "This bikini made me into a success. As a result of starring in "Dr. No" as the first Bond girl, I was given the freedom to take my pick of future roles and to become financially independent."

The bikini finally caught on, and by 1963, the movie "Beach Party", starring Annette Funicello and Frankie Avalon, led a wave of films that made the bikini a pop-culture symbol, though Funicello was barred from wearing Réard's bikini unlike the other young females in the films. In 1965, a woman told "Time" that it was "almost square" not to wear a bikini; the magazine wrote two years later that "65% of the young set had already gone over".

Raquel Welch's fur bikini in "One Million Years B.C." (1966) gave the world the most iconic bikini shot of all time and the poster image became an iconic moment in cinema history. Her deer skin bikini in "One Million Years B.C.", advertised as "mankind's first bikini", (1966) was later described as a "definitive look of the 1960s". Her role wearing the leather bikini raised Welch to a fashion icon and the photo of her in the bikini became a best-selling pinup poster.

Stretch nylon bikini briefs and bras complemented the adolescent boutique fashions of the 1960s, allowing those to be minimal. DuPont introduced lycra (DuPont's name for spandex) in the same decade. Spandex expanded the range of novelty fabrics available to designers which meant suits could be made to fit like a second skin without heavy linings. "The advent of Lycra allowed more women to wear a bikini," wrote Kelly Killoren Bensimon, a former model and author of "The Bikini Book", "It didn't sag, it didn't bag, and it concealed and revealed. It wasn't so much like lingerie anymore." Increased reliance on stretch fabric led to simplified construction. It allowed designers to create the string bikini, and allowed Rudi Gernreich to create the topless monokini. Alternative swimwear fabrics such as velvet, leather, and crocheted squares surfaced in the early '70s.

Réard's company folded in 1988, four years after his death. By the end of the century, the bikini had become the most popular beachwear around the globe. According to French fashion historian Olivier Saillard, this was due to "the power of women, and not the power of fashion". As he explains, "The emancipation of swimwear has always been linked to the emancipation of women", though one survey indicates 85% of all bikinis never touch the water. By 1988 the bikini made up nearly 20% of swimsuit sales, more than any other model in the US, though one-piece suits made a comeback during the 1980s and early 1990s.

In 1997, Miss Maryland Jamie Fox became the first contestant in 50 years to compete in a two-piece swimsuit at the Miss America Pageant. Actresses in action films like "Blue Crush" (2002) and "" (2003) made the two-piece "the millennial equivalent of the power suit", according to Gina Bellafonte of "The New York Times",

According to Beth Dincuff Charleston, research associate at the Costume Institute of the Metropolitan Museum of Art, "The bikini represents a social leap involving body consciousness, moral concerns, and sexual attitudes." By the early 2000s, bikinis had become a $811 million business annually, according to the NPD Group, a consumer and retail information company, and had boosted spin-off services like bikini waxing and the sun tanning industries.

Though child-sized bikinis appeared in the 1950s, in many European countries, swimsuits under size 11 are commonly not sold with a top part, but in the United States, Britain, and Canada, it has often been considered unacceptable for girls in late childhood (ages 7–11) to go topless. Several incidents of families being evicted from public pools due to their child being topless have been reported. In 2002, clothing retailer Abercrombie & Fitch came under criticism for selling child-sized thong bikinis and underwear.

The 1967 film "An Evening in Paris" is mostly remembered because it featured Bollywood actress Sharmila Tagore as the first Indian actress to wear a bikini on film. She also posed in a bikini for the glossy "Filmfare" magazine. The costume shocked a conservative Indian audience, but it also set in motion a trend carried forward by Zeenat Aman in "Heera Panna" (1973) and "Qurbani" (1980), Dimple Kapadia in "Bobby" (1973), and Parveen Babi in "Yeh Nazdeekiyan" (1982).

Indian women wear bikinis when they vacation abroad or in Goa without the family. Despite the conservative ideas prevalent in India, bikinis have become more popular. In summer, when women take up swimming, often in a public space, a lot of tankinis, shorts and single-piece swimsuits are sold. The maximum sales for bikinis happen in the winter, the honeymoon season.

By the end of the first decade of the 21st century, the Chinese bikini industry became a serious international threat for the Brazilian bikini industry. Huludao, Liaoning, China set the world record for the largest bikini parade in 2012, with 1,085 participants and a photo shoot involving 3,090 women.

For most parts of the Middle East, bikinis are either banned or is highly controversial. In 1966, In 1973, when Lebanese magazine "Ash-Shabaka" printed a bikini-clad woman on the cover they had to make a second version with only the face of the model. In 2011, Huda Naccache (Miss Earth 2011), when she posed for the cover of "Lilac" (based in Israel) became the first bikini-clad Arab model on the cover of an Arabic magazine.

While the name "bikini" was at first applied only to beachwear that revealed the wearer's navel, today the fashion industry considers any two-piece swimsuit a bikini. Modern bikini fashions are characterized by a simple, brief design: two triangles of fabric that form a bra and cover the woman's breasts and a third that forms a panty cut below the navel that covers the groin and the buttocks.

Bikinis can and have been made out of almost every possible clothing material, and the fabrics and other materials used to make bikinis are an essential element of their design. Modern bikinis were first made of cotton and jersey. DuPont's introduction of Lycra (spandex) in the 1960s completely changed how bikinis were designed and worn, as according to Kelly Killoren Bensimon, a former model and author of "The Bikini Book", "the advent of Lycra allowed more women to wear a bikini...it didn't sag, it didn't bag, and it concealed and revealed. It wasn't so much like lingerie anymore." Alternative swimwear fabrics such as velvet, leather, and crocheted squares surfaced in the early 1970s.

In a single fashion show in 1985, there were two-piece suits with cropped tank tops instead of the usual skimpy bandeaux, suits that resembled bikinis from the front and one-pieces from the back, suspender straps, ruffles, and deep navel-baring cutouts. Metal and stone jewelery pieces are now often used to dress up look and style according to tastes. To meet the fast pace of demands, some manufacturers now offer made-to-order bikinis ready in as few as seven minutes. The world's most expensive bikini was designed in February 2006 by Susan Rosen; containing of diamond, it was valued at £20 million.

There is a range of distinct bikini styles available — string bikinis, monokinis (topless or top and bottom connected), Trikinis (three pieces instead of two), tankinis (tank top, bikini bottom), camikinis (camisole top, bikini bottom), bandeaukini (bandeau top, bikini bottom), skirtini (bikini top, skirt bottom), "granny bikini" (bikini top, boy shorts bottom), hikinis (also hipkini), seekinis (transparent), minikinis, microkinis, miniminis, slingshots (or suspender bikinis), thong bottoms, tie-sides (a variety of string bikini) and teardrops.

Bikinis have become a major component of marketing various women's sports. It is an official uniform for beach volleyball and is widely worn in athletics and other sports. Sports bikinis have gained popularity since the 1990s. However, the trend has raised some criticism as an attempt to sell sex. Female swimmers do not normally wear bikinis in competitive swimming. The International Swimming Federation (FINA) voted to prohibit female swimmers from racing in bikinis in its meeting at Rome in 1960.

In 1994, the bikini became the official uniform of women's Olympic beach volleyball. In 1999, the International Volleyball Federation (FIVB) standardized beach volleyball uniforms, with the bikini becoming the required uniform for women. That regulation bottom is called a "bun-hugger", and players names are often written on the back of the bottom.

The uniform made its Olympic debut at Sydney's Bondi Beach in the 2000 Summer Olympics amid some criticism. It was the fifth largest television audience of all the sports at the 2000 Games. Much of the interest was because of the sex appeal of bikini-clad players along with their athletic ability. Bikini-clad dancers and cheerleaders entertain the audience during match breaks in many beach volleyball tournaments, including the Olympics. Even indoor volleyball costumes followed suit to become smaller and tighter.

However, the FIVB's mandating of the bikini ran into problems. Some sports officials consider it exploitative and impractical in colder weather. It also drew the ire of some athletes. At the 2006 Asian Games at Doha, Qatar, only one Muslim country – Iraq – fielded a team in the beach volleyball competition because of concerns that the uniform was inappropriate. They refused to wear bikinis. The weather during the evening games in 2012 London Olympics was so cold that the players sometimes had to wear shirts and leggings. Earlier in 2012, FIVB had announced it would allow shorts (maximum length above the knee) and sleeved tops at the games. Richard Baker, the federation spokesperson, said that "many of these countries have religious and cultural requirements so the uniform needed to be more flexible".

The bikini remains preferred by most players and corporate sponsors. US women's team has cited several advantages of bikini uniforms, such as comfort while playing on sand during hot weather. Competitors Natalie Cook and Holly McPeak support the bikini as a practical uniform for a sport played on sand during the heat of summer. Olympic gold medal winner Kerry Walsh said, "I love our uniforms." According to fellow gold medalist Misty May-Treanor and Walsh it does not restrict movement.

One feminist viewpoint sees the bikini uniform as objectification of women athletes. US beach volleyball player Gabrielle Reece described the bikini bottoms as uncomfortable with constant "yanking and fiddling." Many female beach volleyball players have suffered injuries by over-straining the abdominal muscles while many others have gone through augmentation mammoplasty to look appealing in their uniforms. Australian competitor Nicole Sanderson said about match break entertainment that "it's kind of disrespectful to the female players. I'm sure the male spectators love it, but I find it a little bit offensive."

Sports journalism expert Kimberly Bissell conducted a study on the camera angles used during the 2004 Summer Olympic Games beach volleyball games. Bissell found that 20% of the camera angles were focused on the women's chests, and 17% on their buttocks. Bissell theorized that the appearance of the players draws fans attention more than their actual athleticism.
Sports commentator Jeanne Moos commented, "Beach volleyball has now joined go-go girl dancing as perhaps the only two professions where a bikini is the required uniform." British Olympian Denise Johns argues that the regulation uniform is intended to be "sexy" and to attract attention. Rubén Acosta, president of the FIVB, says that it makes the game more appealing to spectators.

From the 1950s to mid-1970s, men's contest formats were often supplemented with women's beauty contests or bikini shows. The winners earned titles like Miss Body Beautiful, Miss Physical Fitness and Miss Americana, and also presented trophies to the winners of the men's contest. In the 1980s, the Ms Olympia competition started in the USA and in the UK the NABBA (National Amateur Body Building Association) renamed Miss Bikini International to Ms Universe. In 1986, the Ms Universe competition was divided into two sections – "physique" (for a more muscular physique) and "figure" (traditional feminine presentation in high heels). In November 2010 the IFBBF (International Federation of BodyBuilding & Fitness) introduced a women's bikini contest for women who do not wish to build their muscles to figure competition levels.

Costumes are regulation "posing trunks" (bikini briefs) for both men and women. Female bodybuilders in America are prohibited from wearing thongs or T-back swimsuits in contests filmed for television, though they are allowed to do so by certain fitness organizations in closed events. For men, the dress code specifies "swim trunks only (no shorts, cut-off pants, or Speedos)."

Women in athletics often wear bikinis of similar size as those worn in beach volleyball. Amy Acuff, a US high-jumper, wore a black leather bikini instead of a track suit at the 2000 Summer Olympics. Runner Florence Griffith-Joyner mixed bikini bottoms with one-legged tights at the 1988 Summer Olympics, earning her more attention than her record breaking performance in the women's 200 meters event. In the 2007 South Pacific Games, the rules were adjusted to allow players to wear less revealing shorts and cropped sports tops instead of bikinis. At the West Asian Games in 2006, organizers banned bikini-bottoms for female athletes and asked them to wear long shorts.

String bikinis and other revealing clothes are common in surfing, though most surfing bikinis are more robust with more coverage than sunning bikinis. "Surfing Magazine" printed a pictorial of Kymberly Herrin, "Playboy" Playmate March 1981, surfing in a revealing bikini, and eventually started an annual bikini issue. The Association of Surfing Professionals often pairs female surf meets with bikini contests, an issue that divides the female pro-surfing community into two parts. It has often been more profitable to win the bikini contest than the female surfing event.

In 1950, American swimsuit mogul Fred Cole, owner of Cole of California, told "Time" that bikinis were designed for "diminutive Gallic women", as because "French girls have short legs... swimsuits have to be hiked up at the sides to make their legs look longer." In 1961, "The New York Times" reported the opinion that the bikini is permissible for people are not "too fat or too thin". In the 1960s etiquette writer Emily Post decreed that "[A bikini] is for perfect figures only, and for the very young." In "The Bikini Book" by Kelly Killoren Bensimon, swimwear designer Norma Kamali says, "Anyone with a tummy" should not wear a bikini. Since then, a number of bikini designers including Malia Mills have encouraged women of all ages and body types to take up the style. The 1970s saw the rise of the lean ideal of female body and figures like Cheryl Tiegs. Her figure remained in vogue in the 21st century.

The fitness boom of the 1980s led to one of the biggest leaps in the evolution of the bikini. According to Mills, "The leg line became superhigh, the front was superlow, and the straps were superthin." Women's magazines used terms like "Bikini Belly", and workout programs were launched to develop a "bikini-worthy body". The tiny "fitness-bikinis" made of lycra were launched to cater to this hardbodied ideal. Movies like "Blue Crush" and TV reality shows like "Surf Girls" merged the concepts of bikini models and athletes together, further accentuating the toned body ideal. Some women, motivated by yearly Spring Break festivities that mark the start of the bikini season in North America, engage in eating disorders in an attempt to achieve the ideal bikini body.

In 1993, Suzy Menkes, then Fashion Editor of the "International Herald Tribune", suggested that women had begun to "revolt" against the "body ideal" and bikini "exposure." She wrote, "Significantly, on the beaches as on the streets, some of the youngest and prettiest women (who were once the only ones who dared to bare) seem to have decided that exposure is over." Nevertheless, professional beach volleyball player Gabrielle Reece, who competes in a bikini, claims that "confidence" alone can make a bikini sexy. One survey commissioned by Diet Chef, a UK home delivery service, reported by "The Today Show" and ridiculed by "More" magazine, showed that women should stop wearing bikinis by the age of 47.

Certain types of underwear are described as bikini underwear and designed for men and women. For women, bikini or bikini-style underwear is underwear that is similar in size and form to a regular bikini. It can refer to virtually any undergarment that provides less coverage to the midriff than lingerie, panties or knickers, especially suited to clothing such as crop tops. For men, bikini briefs are undergarments that are smaller and more revealing than men's classic briefs. Men's bikini briefs can be low- or high-side that are usually lower than true waist, often at hips, and usually have no access pouch or flap, legs bands at tops of thighs. String bikini briefs have front and rear sections that meet in the crotch but not at the waistband, with no fabric on the side of the legs.

Swimwear and underwear have similar design considerations, both being form-fitting garments. The main difference is that, unlike underwear, swimwear is open to public view. The swimsuit was, and is, following underwear styles, and at about the same time that attitudes towards the bikini began to change, underwear underwent a redesign towards a minimal, unboned design that emphasized comfort first.

As the swimsuit was evolving, the underwear started to change. Between 1900 and 1940, swimsuit lengths followed the changes in underwear designs. In the 1920s women started discarding the corset, while the Cadole company of Paris started developing something they called the "breast girdle". During the Great Depression, panties and bras became softly constructed and were made of various elasticized yarns making underwear fit like a second skin. By 1930s underwear styles for both women and men were influenced by the new brief models of swimwear from Europe. Although the waistband was still above the navel, the leg openings of the panty brief were cut in an arc to rise from the crotch to the hip joint. The brief served as a template for most all variations of panties for the rest of the century. Warner standardized the concept of Cup size in 1935. The first underwire bra was developed in 1938. Beginning in the late thirties skants, a type of skanty men's briefs, were introduced, featuring very high-cut leg openings and a lower rise to the waistband. Howard Hughes designed a push-up bra to be worn by Jane Russell in "The Outlaw" in 1943, although Russell stated in interviews that she never wore the 'contraption'. In 1950 Maidenform introduced the first official bust enhancing bra.

By the 1960s, the bikini swimsuit influenced panty styles and coincided with the cut of the new lower rise jeans and pants. In the seventies, with the emergence of skintight jeans, thong versions of the panty became mainstream, since the open, stringed back eliminated any tell-tale panty lines across the rear and hips. By the 1980s the design of the French-cut panty pushed the waistband back up to the natural waistline and the rise of the leg openings was nearly as high (French Cut panties come up to the waist, has a high cut leg, and usually are full in the rear). As with the bra and other type of lingerie, manufacturers of the last quarter of the century marketed panty styles that were designed primarily for their sexual allure. From this decade sexualization and eroticization of the male body was on the rise. The male body was celebrated through advertising campaigns for brands such as Calvin Klein, particularly by photographers Bruce Weber and Herb Ritts. Male bodies and men's undergarments were commodified and packaged for mass consumption, and swimwear and sportswear were influenced by sports photography and fitness. Over time, swimwear evolved from weighty wool to high-tech skin-tight garments, eventually cross-breeding with sportswear, underwear and exercise wear, resulting in the interchangeable fashions of the 1990s.

The term "men's bikini" is sometimes used to describe swim briefs. Men's bikinis can have high or low side panels, and string sides or tie sides. Most lack a button or flap front. Unlike swim briefs, bikinis are not designed for drag reduction and generally lack a visible waistband. Suits less than 1.5 inches wide at the hips are less common for sporting purposes and are most often worn for recreation, fashion, and sun tanning. The posing brief standard to bodybuilding competitions is an example of this style. Male punk rock musicians have performed on the stage wearing women's bikini briefs. The 2000 Bollywood film "Hera Pheri" shows men sunbathing in bikinis, who were mistakenly believed to be women from a distance.

Male bikini tops also exist and are often used as visual gags. A "mankini" is a type of sling swimsuit worn by men. The term is inspired by the word bikini. It was popularized by Sacha Baron Cohen when he donned one in the film "Borat".

Bikini waxing is the epilation of pubic hair beyond the bikini line by use of waxing. The bikini line delineates the part of a woman's pubic area to be covered by the bottom part of a bikini, which means any pubic hair visible beyond the boundaries of a swimsuit. Visible pubic hair is widely culturally disapproved, considered to be embarrassing, and often removed.

As popularity of bikini grew, the acceptability of pubic hair diminished. But, with certain styles of women's swimwear, pubic hair may become visible around the crotch area of a swimsuit. With the reduction in the size of swimsuits, especially since the advent of the bikini after 1945, the practice of bikini waxing has also become popular. The Brazilian style which became popular with the rise of thong bottoms.

Depending on the style of bikini-bottom and the amount of skin visible outside the bikini, pubic hair may be styled into several styles — American waxing (removal of pubic hair from the sides, top of the thighs, and under the navel), French waxing (leaves only a vertical strip in front), Brazilian waxing (removal of all hair in the pelvic area, particularly suitable for thong bottoms).

The tan lines created by the wearing of a bikini while tanning are known as a bikini tan. A 1969 innovation of tan-through swimwear uses fabric which is perforated with thousands of micro holes that are nearly invisible to the naked eye, but which let enough sunlight through to produce a line-free tan.

As bikinis leave most of the body exposed to potentially dangerous UV radiation, overexposure can cause sunburn, skin cancer, as well as other acute and chronic health effects on the skin, eyes, and immune system. As a result, medical organizations recommend that bikini wearers protect themselves from UV radiation by using broad-spectrum sunscreen, which has been shown to protect against sunburn, skin cancer, wrinkling and sagging skin. Certain sunscreen ingredients can cause harm if they penetrate the skin over time.




</doc>
<doc id="4551" url="https://en.wikipedia.org/wiki?curid=4551" title="Babur">
Babur

Babur (; 14 February 148326 December 1530), born Zahīr ud-Dīn Muhammad, was the ultimate founder and first Emperor of the Mughal dynasty in the Indian subcontinent. He was a direct descendant of Emperor Timur the Great (Tamerlane) from what is now Uzbekistan.

Babur was the eldest son of Umar Sheikh Mirza, governor of Farghana and great grandson of Timur the Great. He ascended the throne of Farghana in its capital Akhsikent in 1494 at the age of twelve and faced rebellion. He conquered Samarkand two years later, only to lose the vilayat of Fergana soon after. In his attempt to reconquer Fergana, he lost control of Samarkand. In 1501, his attempt to recapture both vilayats went in vain as he was defeated by Muhammad Shaybani Khan. In 1504, he conquered Kabul, which was under the rule of the infant heir of Ulugh Begh. Babur formed a partnership with Safavid ruler Ismail I and reconquered parts of Turkistan, including Samarkand, only to again lose it and the other newly conquered lands to the Sheybanids.

After losing Samarkand for the third time, Babur turned his attention to creating his empire in the north. At that time, the Indo-Gangetic Plain of the northern Indian Subcontinent was ruled by Ibrahim Lodi of the Afghan Lodi dynasty, whereas Rajputana was ruled by a Hindu Rajput Confederacy, led by Rana Sanga of Mewar. According to historical records and Baburnama (autobiography written by Babur himself) Daulat Khan Lodi invited him to attack on Delhi where Ibrahim Lodi was ruling at that time. He sent his ambassador to him to support him in his attack on Delhi. Also in 1524, Daulat Khan Lodi, a rebel of the Lodhi dynasty, invited Babur to overthrow Ibrahim and become ruler. Babur punished him for his mistake Ibrahim Lodi at the First Battle of Panipat in 1526 CE and founded the Mughal empire. However, he again faced opposition, this time from Rana Sanga of Mewar and Medini Rai,another rajput ruler in the battle of Chanderi who considered Babur a foreigner. The Rana was defeated in the Battle of Khanwa.

Babur married several times. Notable among his sons are Humayun, Kamran Mirza and Hindal Mirza. Babur died in 1530 and was succeeded by Humayun. According to Babur's wishes, he was buried in Bagh-e-Babur in Kabul, Afghanistan. Being a patrilineal descendant of Timur, Babur considered himself a Timurid and Chagatai Turkic. He is considered a national hero in Uzbekistan and Kyrgyzstan. Many of his poems also have become popular folk songs. He wrote his autobiography, "Baburnama", in Chaghatai Turkic and this was translated into Persian during Akbar's reign.

"Ẓahīr-ud-Dīn" is Arabic for "Defender of the Faith" (of Islam), and "Muhammad" honours the Islamic prophet.

The difficulty of pronouncing the name for his Central Asian Turco-Mongol army may have been responsible for the greater popularity of his nickname Babur, also variously spelled Baber, Babar, and Bābor. The name is generally taken in reference to the Persian "babr", meaning "tiger". The word repeatedly appears in Ferdowsi's "Shahnameh" and was borrowed into the Turkic languages of Central Asia. Thackston argues for an alternate derivation from the PIE word "beaver", pointing to similarities between the pronunciation "Bābor" and the Russian "bobr" (, "beaver").

Babur bore the royal titles "Badshah" and "al-ṣultānu 'l-ʿazam wa 'l-ḫāqān al-mukkarram pādshāh-e ġāzī". He and later Mughal emperors used the title of "mirza" and "gurkhan" as regalia.

Babur's memoirs form the main source for details of his life. They are known as the "Baburnama" and were written in Chaghatai Turkic, his mother-tongue, though, according to Dale, "his Turkic prose is highly Persianized in its sentence structure, morphology or word formation and vocabulary." "Baburnama" was translated into Persian during the rule of Babur's grandson Akbar.

Babur was born on 14 February 1483 in the city of Andijan, Andijan Province, Fergana Valley, contemporary Uzbekistan. He was the eldest son of Umar Sheikh Mirza, ruler of the Fergana Valley, the son of Abū Saʿīd Mirza (and grandson of Miran Shah, who was himself son of Timur) and his wife Qutlugh Nigar Khanum, daughter of Yunus Khan, the ruler of Moghulistan (and great-great grandson of Tughlugh Timur, the son of Esen Buqa I, who was the great-great-great grandson of Chaghatai Khan, the second-born son of Genghis Khan).

Babur hailed from the Barlas tribe, which was of Mongol origin and had embraced Turkic and Persian culture. They had also converted to Islam centuries earlier and resided in Turkestan and Khorasan. Aside from the Chaghatai language, Babur was equally fluent in Persian, the "lingua franca" of the Timurid elite.

Hence, Babur, though nominally a Mongol (or "Moghul" in Persian language), drew much of his support from the local Turkic and Iranian people of Central Asia, and his army was diverse in its ethnic makeup. It included Persians (known to Babur as "Sarts" and "Tajiks"), ethnic Afghans, Arabs, as well as Barlas and Chaghatayid Turko-Mongols from Central Asia.

In 1494, eleven years old Babur became the ruler of Fergana, in present-day Uzbekistan, after Umar Sheikh Mirza died "while tending pigeons in an ill-constructed dovecote that toppled into the ravine below the palace". During this time, two of his uncles from the neighbouring kingdoms, who were hostile to his father, and a group of nobles who wanted his younger brother Jahangir to be the ruler, threatened his succession to the throne. His uncles were relentless in their attempts to dislodge him from this position as well as from many of his other territorial possessions to come. Babur was able to secure his throne mainly because of help from his maternal grandmother, Aisan Daulat Begum, although there was also some luck involved.

Most territories around his kingdom were ruled by his relatives, who were descendants of either Timur or Genghis Khan, and were constantly in conflict. At that time, rival princes were fighting over the city of Samarkand to the west, which was ruled by his paternal cousin. Babur had a great ambition to capture the city. In 1497 he besieged Samarkand for seven months before eventually gaining control over it. He was fifteen years old and for him the campaign was a huge achievement. Babur was able to hold the city despite desertions in his army, but he later fell seriously ill. Meanwhile, a rebellion back home, approximately away, amongst nobles who favoured his brother, robbed him of Fergana. As he was marching to recover it, he lost Samarkand to a rival prince, leaving him with neither. He had held Samarkand for 100 days, and he considered this defeat as his biggest loss, obsessing over it even later in his life after his conquests in India.

In 1501, Babur laid siege to Samarkand once more, but was soon defeated by his most formidable rival, Muhammad Shaybani, Khan of the Uzbeks. Samarkand, his lifelong obsession, was lost again. He tried to reclaim Fergana but lost it too and escaping with a small band of followers, he wandered to the mountains of central Asia and took refuge with hill tribes. Thus, during the ten years since becoming the ruler of Fergana, Babur suffered many short-lived victories and was without shelter and in exile, aided by friends and peasants. He finally stayed in Tashkent, which was ruled by his maternal uncle. Babur wrote, "During my stay in Tashkent, I endured much poverty and humiliation. No country, or hope of one!" For three years Babur concentrated on building up a strong army, recruiting widely amongst the Tajiks of Badakhshan in particular. By 1502, he had resigned all hopes of recovering Fergana; he was left with nothing and was forced to try his luck someplace else.

Kabul was ruled by Ulugh Begh Mirza of the Arghun Dynasty, who died leaving only an infant as heir. The city was then claimed by Mukin Begh, who was considered to be a usurper and was opposed by the local populace. In 1504, Babur was able to cross the snowy Hindu Kush mountains and capture Kabul from the remaining Arghunids, who were forced to retreat to Kandahar. With this move, he gained a new kingdom, re-established his fortunes and would remain its ruler until 1526. In 1505, because of the low revenue generated by his new mountain kingdom, Babur began his first expedition to India; in his memoirs, he wrote, "My desire for Hindustan had been constant. It was in the month of Shaban, the Sun being in Aquarius, that we rode out of Kabul for Hindustan". It was a brief raid across the Khyber Pass.

In the same year, Babur united with Sultan Husayn Mirza Bayqarah of Herat, a fellow Timurid and distant relative, against their common enemy, the Uzbek Shaybani. However, this venture did not take place because Husayn Mirza died in 1506 and his two sons were reluctant to go to war. Babur instead stayed at Herat after being invited by the two Mirza brothers. It was then the cultural capital of the eastern Muslim world. Though he was disgusted by the vices and luxuries of the city, he marvelled at the intellectual abundance there, which he stated was "filled with learned and matched men". He became acquainted with the work of the Chagatai poet Mir Ali Shir Nava'i, who encouraged the use of Chagatai as a literary language. Nava'i's proficiency with the language, which he is credited with founding, may have influenced Babur in his decision to use it for his memoirs. He spent two months there before being forced to leave because of diminishing resources; it later was overrun by Shaybani and the Mirzas fled.

Babur became the only reigning ruler of the Timurid dynasty after the loss of Herat, and many princes sought refuge from him at Kabul because of Shaybani's invasion in the west. He thus assumed the title of "Padshah" (emperor) among the Timurids—though this title was insignificant since most of his ancestral lands were taken, Kabul itself was in danger and Shaybani continued to be a threat. Babur prevailed during a potential rebellion in Kabul, but two years later a revolt among some of his leading generals drove him out of Kabul. Escaping with very few companions, Babur soon returned to the city, capturing Kabul again and regaining the allegiance of the rebels. Meanwhile, Shaybani was defeated and killed by Ismail I, Shah of Shia Safavid Persia, in 1510.

Babur and the remaining Timurids used this opportunity to reconquer their ancestral territories. Over the following few years, Babur and Shah Ismail formed a partnership in an attempt to take over parts of Central Asia. In return for Ismail's assistance, Babur permitted the Safavids to act as a suzerain over him and his followers. Thus, in 1513, after leaving his brother Nasir Mirza to rule Kabul, he managed to take Samarkand for the third time; he also took Bokhara but lost both again to the Uzbeks. Shah Ismail reunited Babur with his sister Khānzāda, who had been imprisoned by and forced to marry the recently deceased Shaybani. Babur returned to Kabul after three years in 1514. The following 11 years of his rule mainly involved dealing with relatively insignificant rebellions from Afghan tribes, his nobles and relatives, in addition to conducting raids across the eastern mountains. Babur began to modernise and train his army despite it being, for him, relatively peaceful times.

Babur began relations with the Safavids when he met Ali Mirza Safavi at Samarkand; their good relations lasted even after Babur was approached by the Ottomans. The Safavids army led by Najm-e Sani massacred civilians in Central Asia and then sought the assistance of Babur, who advised the Safavids to withdraw. The Safavids, however, refused and were defeated during the Battle of Ghazdewan by the warlord Ubaydullah Khan.

Babur's early relations with the Ottomans were poor because the Ottoman Sultan Selim I provided his rival Ubaydullah Khan with powerful matchlocks and cannons. In 1507, when ordered to accept Selim I as his rightful suzerain, Babur refused and gathered Qizilbash servicemen in order to counter the forces of Ubaydullah Khan during the Battle of Ghazdewan. In 1513, Selim I reconciled with Babur (fearing that he would join the Safavids), dispatched Ustad Ali Quli the artilleryman and Mustafa Rumi the matchlock marksman, and many other Ottoman Turks, in order to assist Babur in his conquests; this particular assistance proved to be the basis of future Mughal-Ottoman relations. From them, he also adopted the tactic of using matchlocks and cannons in field (rather than only in sieges), which would give him an important advantage in India.

Babur still wanted to escape from the Uzbeks, and he chose India as a refuge instead of Badakhshan, which was to the north of Kabul. He wrote, "In the presence of such power and potency, we had to think of some place for ourselves and, at this crisis and in the crack of time there was, put a wider space between us and the strong foeman." After his third loss of Samarkand, Babur gave full attention to the conquest of North India, launching a campaign; he reached the Chenab River, now in Pakistan, in 1519. Until 1524, his aim was to only expand his rule to Punjab, mainly to fulfill the legacy of his ancestor Timur, since it used to be part of his empire. At the time parts of north India were under the rule of Ibrahim Lodi of the Lodi dynasty, but the empire was crumbling and there were many defectors. He received invitations from Daulat Khan Lodi, Governor of Punjab and Ala-ud-Din, uncle of Ibrahim. He sent an ambassador to Ibrahim, claiming himself the rightful heir to the throne, but the ambassador was detained at Lahore and released months later.

Babur started for Lahore, Punjab, in 1524 but found that Daulat Khan Lodi had been driven out by forces sent by Ibrahim Lodi. When Babur arrived at Lahore, the Lodi army marched out and his army was routed. In response, Babur burned Lahore for two days, then marched to Dipalpur, placing Alam Khan, another rebel uncle of Lodi, as governor. Alam Khan was quickly overthrown and fled to Kabul. In response, Babur supplied Alam Khan with troops who later joined up with Daulat Khan Lodi, and together with about 30,000 troops, they besieged Ibrahim Lodi at Delhi. He easily defeated and drove off Alam's army and Babur realised Lodi would not allow him to occupy the Punjab.

In November 1525 Babur got news at Peshawar that Daulat Khan Lodi had switched sides, and he drove out Ala-ud-Din. Babur then marched onto Lahore to confront Daulat Khan Lodi, only to see Daulat's army melt away at their approach. Daulat surrendered and was pardoned. Thus within three weeks of crossing the Indus River Babur had become the master of Punjab.

Babur marched on to Delhi via Sirhind. He reached Panipat on 20 April 1526 and there met Ibrahim Lodi's numerically superior army of about 100,000 soldiers and 100 elephants. In the battle that began on the following day, Babur used the tactic of "Tulugma", encircling Ibrahim Lodi's army and forcing it to face artillery fire directly, as well as frightening its war elephants. Ibrahim Lodi died during the battle, thus ending the Lodi dynasty.

Babur wrote in his memoirs about his victory:

After the battle, Babur occupied Delhi and Agra, took the throne of Lodi, and laid the foundation for the eventual rise of Mughal rule in India. However, before he became North India's ruler, he had to fend off challengers, such as Rana Sanga.

The Battle of Khanwa was fought between Babur and the Rajput ruler Rana Sanga on 17 March 1527. Rana Sanga wanted to overthrow Babur, whom he considered to be a foreigner ruling in India, and also to extend the Rajput territories by annexing Delhi and Agra. He was supported by Afghan chiefs who felt Babur had been deceptive by refusing to fulfill promises made to them. Upon receiving news of Rana Sangha's advance towards Agra, Babur took a defensive position at Khanwa (currently in the Indian state of Rajasthan), from where he hoped to launch a counterattack later. According to K.V. Krishna Rao, Babur won the battle because of his "superior generalship" and modern tactics: the battle was one of the first in India that featured cannons. Rao also notes that Rana Sanga faced "treachery" when the Hindu chief Silhadi joined Babur's army with a garrison of 6,000 soldiers.

This battle took place in the aftermath of the Battle of Khanwa. On receiving news that Rana Sanga had made preparations to renew the conflict with him, Babur decided to isolate the Rana by inflicting a military defeat on one of his staunchest allies, Medini Rai Khangar, who was the ruler of Malwa.

Upon reaching Chanderi, on January 20, 1528, Babur offered Shamsabad to Medini Rao in exchange for Chanderi as a peace overture, but the offer was rejected. The outer fortress of Chanderi was taken by Babur's army at night, and the next morning the upper fort was captured. Babur himself expressed surprise that the upper fort had fallen within an hour of the final assault. Medini Rai organized a Jauhar ceremony during which women and children within the fortress immolated themselves. A small number of soldiers also collected in Medini Rao's house and proceeded to kill each other in collective suicide. This sacrifice does not seem to have impressed Babur who does not express a word of admiration for the enemy in his autobiography.

There are no descriptions about Babur's physical appearance, except from the paintings in the translation of the "Baburnama" prepared during the reign of Akbar. In his autobiography, Babur claimed to be strong and physically fit, and claimed to have swum across every major river he encountered, including twice across the Ganges River in North India. Unlike his father, he had ascetic tendencies and did not have any great interest in women. In his first marriage, he was "bashful" towards Aisha Sultan Begum, later losing his affection for her. However, he acquired several more wives and concubines over the years, and as required for a prince, he was able to ensure the continuity of his line.
Babur's first wife, Aisha Sultan Begum, was his paternal cousin, the daughter of Sultan Ahmad Mirza, his father's brother. She was an infant when betrothed to Babur, who was himself five years old. They married eleven years later, . The couple had one daughter, Fakhr-un-Nissa, who died within a year in 1500. Three years later, after Babur's first defeat at Fergana, Aisha left him and returned to her father's household. In 1504, Babur married Zaynab Sultan Begum, who died childless within two years. In the period 1506–08, Babur married four women, Maham Begum (in 1506), Masuma Sultan Begum, Gulrukh Begum and Dildar Begum. Babur had four children by Maham Begum, of whom only one survived infancy. This was his eldest son and heir, Humayun. Masuma Sultan Begum died during childbirth; the year of her death is disputed (either 1508 or 1519). Gulrukh bore Babur two sons, Kamran and Askari, and Dildar Begum was the mother of Babur's youngest son, Hindal. Babur later married Mubaraka Yusufzai, a Pashtun woman of the Yusufzai tribe. Gulnar Aghacha and Nargul Aghacha were two Circassian slaves given to Babur as gifts by Tahmasp Shah Safavi, the Shah of Persia. They became "recognized ladies of the royal household."

During his rule in Kabul, when there was a time of relative peace, Babur pursued his interests in literature, art, music and gardening. Previously, he never drank alcohol and avoided it when he was in Herat. In Kabul, he first tasted it at the age of thirty. He then began to drink regularly, host wine parties and consume preparations made from opium. Though religion had a central place in his life, Babur also approvingly quoted a line of poetry by one of his contemporaries: "I am drunk, officer. Punish me when I am sober". He quit drinking for health reasons before the Battle of Khanwa, just two years before his death, and demanded that his court do the same. But he did not stop chewing narcotic preparations, and did not lose his sense of irony. He wrote, "Everyone regrets drinking and swears an oath (of abstinence); I swore the oath and regret that."

Babur died at the age of 47 on and was succeeded by his eldest son, Humayun. After death, his body was moved to Kabul, Afghanistan, where it lies in Bagh-e Babur (Babur Gardens).

It is generally agreed that, as a Timurid, Babur was not only significantly influenced by the Persian culture, but that his empire also gave rise to the expansion of the Persianate ethos in the Indian subcontinent.

For example, F. Lehmann states in the "Encyclopædia Iranica":
Although all applications of modern Central Asian ethnicities to people of Babur's time are anachronistic, Soviet and Uzbek sources regard Babur as an ethnic Uzbek. At the same time, during the Soviet Union Uzbek scholars were censored for idealising and praising Babur and other historical figures such as Ali-Shir Nava'i.

Babur is considered a national hero in Uzbekistan. On 14 February 2008, stamps in his name were issued in the country to commemorate his 525th birth anniversary. Many of Babur's poems have become popular Uzbek folk songs, especially by Sherali Jo'rayev. Some sources claim that Babur is a national hero in Kyrgyzstan too. In October 2005, Pakistan developed the Babur Cruise Missile, named in his honour.

One of the enduring features of Babur's life was that he left behind the lively and well-written autobiography known as "Baburnama". Quoting Henry Beveridge, Stanley Lane-Poole writes:
In his own words, "The cream of my testimony is this, do nothing against your brothers even though they may deserve it." Also, "The new year, the spring, the wine and the beloved are joyful. Babur make merry, for the world will not be there for you a second time."

Babri Masjid ("Babur's Mosque") in Ayodhya, India was constructed on the orders of Mir Baqi, one of Babur's generals who led forces sent to the region during his reign. In 2003, by the order of an Indian Court, the Archaeological Survey of India (ASI) was asked to conduct a more indepth study and an excavation to ascertain the type of structure that was beneath the mosque. The excavation was conducted from 12 March 2003 to 7 August 2003, resulting in 1360 discoveries. The ASI submitted its report to the Allahabad high court.

The summary of the ASI report indicated the presence of a 10th-century temple under the mosque. The ASI team said that, human activity at the site dates back to the 13th century BCE. The next few layers date back to the Shunga period (second-first century BCE) and the Kushan period. During the early medieval period (11–12th century CE), a huge but short-lived structure of nearly 50 metres north-south orientation was constructed. On the remains of this structure, another massive structure was constructed: this structure had at least three structural phases and three successive floors attached with it. The report concluded that it was over the top of this construction that the disputed structure was constructed during the early 16th century.





</doc>
<doc id="4552" url="https://en.wikipedia.org/wiki?curid=4552" title="Bernard of Clairvaux">
Bernard of Clairvaux

Bernard of Clairvaux, O.Cist (; 109020 August 1153) was a French abbot and a major leader in the reform of Benedictine monasticism that caused the formation of the Cistercian order.

"...[H]e was sent to found a new abbey at an isolated clearing in a glen known as the "Val d'Absinthe", about southeast of Bar-sur-Aube. According to tradition, Bernard founded the monastery on 25 June 1115, naming it "Claire Vallée", which evolved into "Clairvaux." There Bernard would preach an immediate faith, in which the intercessor was the Virgin Mary." In the year 1128, Bernard attended the Council of Troyes, at which he traced the outlines of the Rule of the Knights Templar, which soon became the ideal of Christian nobility.

On the death of Pope Honorius II on 13 February 1130, a schism broke out in the Church. King Louis VI of France convened a national council of the French bishops at Étampes in 1130, and Bernard was chosen to judge between the rivals for pope. By the end of 1131, the kingdoms of France, England, Germany, Portugal, Castile, and Aragon supported Innocent; however, most of Italy, southern France, and Sicily, with the Latin patriarchs of Constantinople, Antioch, and Jerusalem supported Anacletus. Bernard set out to convince these other regions to rally behind Innocent. 

In 1139, Bernard assisted at the Second Council of the Lateran. He subsequently denounced the teachings of Peter Abelard to the pope, who called a council at Sens in 1141 to settle the matter. Bernard soon saw one of his disciples elected Pope Eugene III. Having previously helped end the schism within the church, Bernard was now called upon to combat heresy. In June 1145, Bernard traveled in southern France and his preaching there helped strengthen support against heresy.

After the Christian defeat at the Siege of Edessa, the pope commissioned Bernard to preach the Second Crusade. The last years of Bernard's life were saddened by the failure of the crusaders, the entire responsibility for which was thrown upon him. Bernard died at the age of 63, after 40 years as a monk. He was the first Cistercian placed on the calendar of saints, and was canonized by Pope Alexander III on 18 January 1174. In 1830 Pope Pius VIII bestowed upon Bernard the title "Doctor of the Church".

Bernard's parents were Tescelin de Fontaine, lord of Fontaine-lès-Dijon, and , both members of the highest nobility of Burgundy. Bernard was the third of seven children, six of whom were sons. At the age of nine years, he was sent to a school at Châtillon-sur-Seine run by the secular canons of Saint-Vorles. Bernard had a great taste for literature and devoted himself for some time to poetry. His success in his studies won the admiration of his teachers. He wanted to excel in literature in order to take up the study of the Bible. He had a special devotion to the Virgin Mary, and he would later write several works about the Queen of Heaven.
Bernard would expand upon Anselm of Canterbury's role in transmuting the sacramentally ritual Christianity of the Early Middle Ages into a new, more personally held faith, with the life of Christ as a model and a new emphasis on the Virgin Mary. In opposition to the rational approach to divine understanding that the scholastics adopted, Bernard would preach an immediate faith, in which the intercessor was the Virgin Mary.

Bernard was only nineteen years of age when his mother died. During his youth, he did not escape trying temptations and around this time he thought of retiring from the world and living a life of solitude and prayer.

In 1098 Saint Robert of Molesme had founded Cîteaux Abbey, near Dijon, with the purpose of restoring the Rule of St Benedict in all its rigour. Returning to Molesme, he left the government of the new abbey to Saint Alberic of Cîteaux, who died in the year 1109. After the death of his mother, Bernard sought admission into the Cistercian order. At the age of 22, while Bernard was at prayer in a church, he felt the calling of God to enter the monastery of Cîteaux. In 1113 Saint Stephen Harding had just succeeded Saint Alberic as third Abbot of Cîteaux when Bernard and thirty other young noblemen of Burgundy sought admission into the monastery. Bernard's testimony was so irresistible that 30 of his friends, brothers, and relatives followed him into the monastic life.

The little community of reformed Benedictines at Cîteaux, which would have so profound an influence on Western monasticism, grew rapidly. Three years later, Bernard was sent with a band of twelve monks to found a new house at Vallée d'Absinthe, in the Diocese of Langres. This Bernard named "Claire Vallée", or "Clairvaux", on 25 June 1115, and the names of Bernard and Clairvaux would soon become inseparable. During the absence of the Bishop of Langres, Bernard was blessed as abbot by William of Champeaux, Bishop of Châlons-sur-Marne. From that moment a strong friendship sprang up between the abbot and the bishop, who was professor of theology at Notre Dame of Paris, and the founder of the Abbey of St. Victor, Paris.

The beginnings of Clairvaux Abbey were trying and painful. The regime was so austere that Bernard became ill, and only the influence of his friend William of Champeaux and the authority of the general chapter could make him mitigate the austerities. The monastery, however, made rapid progress. Disciples flocked to it in great numbers and put themselves under the direction of Bernard. The reputation of his holiness soon attracted 130 new monks, including his own father. His father and all his brothers entered Clairvaux to pursue religious life, leaving only Humbeline, his sister, in the secular world. She, with the consent of her husband, soon took the veil in the Benedictine nunnery of Jully-les-Nonnains. Gerard of Clairvaux, Bernard's older brother, became the cellarer of Citeaux. The abbey became too small for its members and it was necessary to send out bands to found new houses. In 1118 Trois-Fontaines Abbey was founded in the diocese of Châlons; in 1119 Fontenay Abbey in the Diocese of Autun; and in 1121 Foigny Abbey near Vervins, in the diocese of Laon. In addition to these victories, Bernard also had his trials. During an absence from Clairvaux, the Grand Prior of the Abbey of Cluny went to Clairvaux and enticed away Bernard's cousin, Robert of Châtillon. This was the occasion of the longest and most emotional of Bernard's letters.
In the year 1119, Bernard was present at the first general chapter of the order convoked by Stephen of Cîteaux. Though not yet 30 years old, Bernard was listened to with the greatest attention and respect, especially when he developed his thoughts upon the revival of the primitive spirit of regularity and fervour in all the monastic orders. It was this general chapter that gave definitive form to the constitutions of the order and the regulations of the "Charter of Charity", which Pope Callixtus II confirmed on 23 December 1119. In 1120, Bernard wrote his first work, ', and his homilies which he entitled '. The monks of the abbey of Cluny were unhappy to see Cîteaux take the lead role among the religious orders of the Roman Catholic Church. For this reason, the Black Monks attempted to make it appear that the rules of the new order were impracticable. At the solicitation of William of St. Thierry, Bernard defended the order by publishing his "Apology" which was divided into two parts. In the first part, he proved himself innocent of the charges of Cluny and in the second he gave his reasons for his counterattacks. He protested his profound esteem for the Benedictines of Cluny whom he declared he loved equally as well as the other religious orders. Peter the Venerable, abbot of Cluny, answered Bernard and assured him of his great admiration and sincere friendship. In the meantime Cluny established a reform, and Abbot Suger, the minister of Louis VI of France, was converted by the "Apology" of Bernard. He hastened to terminate his worldly life and restore discipline in his monastery. The zeal of Bernard extended to the bishops, the clergy, and lay people. Bernard's letter to the archbishop of Sens was seen as a real treatise, ""De Officiis Episcoporum."" About the same time he wrote his work on "Grace and Free Will".

In the year 1128 AD, Bernard participated in the Council of Troyes, which had been convoked by Pope Honorius II, and was presided over by Cardinal Matthew of Albano. The purpose of this council was to settle certain disputes of the bishops of Paris, and regulate other matters of the Church of France. The bishops made Bernard secretary of the council, and charged him with drawing up the synodal statutes. After the council, the bishop of Verdun was deposed. It was at this council that Bernard traced the outlines of the Rule of the Knights Templar who soon became the ideal of Christian nobility. Around this time, he praised them in his "Liber ad milites templi de laude novae militiae".

Again reproaches arose against Bernard and he was denounced, even in Rome. He was accused of being a monk who meddled with matters that did not concern him. Cardinal Harmeric, on behalf of the pope, wrote Bernard a sharp letter of remonstrance stating, "It is not fitting that noisy and troublesome frogs should come out of their marshes to trouble the Holy See and the cardinals."

Bernard answered the letter by saying that, if he had assisted at the council, it was because he had been dragged to it by force, replying:

This letter made a positive impression on Harmeric, and in the Vatican.

Bernard's influence was soon felt in provincial affairs. He defended the rights of the Church against the encroachments of kings and princes, and recalled to their duty Henri Sanglier, archbishop of Sens and Stephen of Senlis, bishop of Paris. On the death of Honorius II, which occurred on 14 February 1130, a schism broke out in the Church by the election of two popes, Pope Innocent II and Antipope Anacletus II. Innocent II, having been banished from Rome by Anacletus, took refuge in France. Louis VI convened a national council of the French bishops at Étampes, and Bernard, summoned there by consent of the bishops, was chosen to judge between the rival popes. He decided in favour of Innocent II. After the council of Étampes, Bernard spoke with King Henry I of England, also known as Henry Beauclerc, about Henry I's reservations regarding Pope Innocent II. Henry I was sceptical because most of the bishops of England supported Antipope Anacletus II; Bernard persuaded him to support Innocent. This caused the pope to be recognized by all the great powers. 

He then went with him into Italy and reconciled Pisa with Genoa, and Milan with the pope. The same year Bernard was again at the Council of Reims at the side of Innocent II. He then went to Aquitaine where he succeeded for the time in detaching William X, Duke of Aquitaine, from the cause of Anacletus.
Germany had decided to support Innocent through Norbert of Xanten, who was a friend of Bernard's. However, Innocent insisted on Bernard's company when he met with Lothair II, Holy Roman Emperor. Lothair II became Innocent's strongest ally among the nobility. Although the councils of Étampes, Wurzburg, Clermont, and Rheims all supported Innocent, large portions of the Christian world still supported Anacletus.

Bernard wrote to Gerard of Angouleme (a letter known as Letter 126), which questioned Gerard's reasons for supporting Anacletus. Bernard would later comment that Gerard was his most formidable opponent during the whole schism. After persuading Gerard, Bernard traveled to visit William X, Duke of Aquitaine. He was the hardest for Bernard to convince. He did not pledge allegiance to Innocent until 1135. After that, Bernard spent most of his time in Italy persuading the Italians to pledge allegiance to Innocent. He traveled to Sicily in 1137 to convince the king of Sicily to follow Innocent. The whole conflict ended when Anacletus died on 25 January 1138.

In 1132, Bernard accompanied Innocent II into Italy, and at Cluny the pope abolished the dues which Clairvaux used to pay to that abbey. This action gave rise to a quarrel between the White Monks and the Black Monks which lasted 20 years. In May of that year, the pope, supported by the army of Lothair III, entered Rome, but Lothair III, feeling himself too weak to resist the partisans of Anacletus, retired beyond the Alps, and Innocent sought refuge in Pisa in September 1133. Bernard had returned to France in June and was continuing the work of peacemaking which he had commenced in 1130. Towards the end of 1134, he made a second journey into Aquitaine, where William X had relapsed into schism. Bernard invited William to the Mass which he celebrated in the Church of La Couldre. At the Eucharist, he "admonished the Duke not to despise God as he did His servants". William yielded and the schism ended. Bernard went again to Italy, where Roger II of Sicily was endeavouring to withdraw the Pisans from their allegiance to Innocent. He recalled the city of Milan to obedience to the pope as they had followed the deposed Anselm V, Archbishop of Milan. For this, he was offered, and he refused, the archbishopric of Milan. He then returned to Clairvaux. Believing himself at last secure in his cloister, Bernard devoted himself with renewed vigour to the composition of the works which would win for him the title of "Doctor of the Church". He wrote at this time his sermons on the "Song of Songs". In 1137, he was again forced to leave his solitude by order of the pope to put an end to the quarrel between Lothair and Roger of Sicily. At the conference held at Palermo, Bernard succeeded in convincing Roger of the rights of Innocent II. He also silenced the final supporters who sustained the schism. Anacletus died of "grief and disappointment" in 1138, and with him the schism ended.

In 1139, Bernard assisted at the Second Council of the Lateran, in which the surviving adherents of the schism were definitively condemned. About the same time, Bernard was visited at Clairvaux by Saint Malachy, Primate of All Ireland, and a very close friendship formed between them. Malachy wanted to become a Cistercian, but the pope would not give his permission. Malachy would die at Clairvaux in 1148.

Towards the close of the 11th century, a spirit of independence flourished within schools of philosophy and theology. This led for a time to the exaltation of human reason and rationalism. The movement found an ardent and powerful advocate in Peter Abelard. Abelard's treatise on the Trinity had been condemned as heretical in 1121, and he was compelled to throw his own book into the fire. However, Abelard continued to develop his teachings, which were controversial in some quarters. Bernard, informed of this by William of St-Thierry, is said to have held a meeting with Abelard intending to persuade him to amend his writings, during which Abelard repented and promised to do so. But once out of Bernard's presence, he reneged. Bernard then denounced Abelard to the pope and cardinals of the Curia. Abelard sought a debate with Bernard, but Bernard initially declined, saying he did not feel matters of such importance should be settled by logical analyses. Bernard's letters to William of St-Thierry also express his apprehension about confronting the preeminent logician. Abelard continued to press for a public debate, and made his challenge widely known, making it hard for Bernard to decline. In 1141, at the urgings of Abelard, the archbishop of Sens called a council of bishops, where Abelard and Bernard were to put their respective cases so Abelard would have a chance to clear his name. Bernard lobbied the prelates on the evening before the debate, swaying many of them to his view. The next day, after Bernard made his opening statement, Abelard decided to retire without attempting to answer. The council found in favour of Bernard and their judgment was confirmed by the pope. Abelard submitted without resistance, and he retired to Cluny to live under the protection of Peter the Venerable, where he died two years later.

Bernard had occupied himself in sending bands of monks from his overcrowded monastery into Germany, Sweden, England, Ireland, Portugal, Switzerland, and Italy. Some of these, at the command of Innocent II, took possession of Tre Fontane Abbey, from which Eugene III would be chosen in 1145. Pope Innocent II died in the year 1143. His two successors, Pope Celestine II and Pope Lucius II, reigned only a short time, and then Bernard saw one of his disciples, Bernard of Pisa, and known thereafter as Eugene III, raised to the Chair of Saint Peter. Bernard sent him, at the pope's own request, various instructions which comprise the "Book of Considerations," the predominating idea of which is that the reformation of the Church ought to commence with the sanctity of the pope. Temporal matters are merely accessories; the principles according to Bernard's work were that piety and meditation were to precede action.

Having previously helped end the schism within the Church, Bernard was now called upon to combat heresy. Henry of Lausanne, a former Cluniac monk, had adopted the teachings of the Petrobrusians, followers of Peter of Bruys and spread them in a modified form after Peter's death. Henry of Lausanne's followers became known as Henricians. In June 1145, at the invitation of Cardinal Alberic of Ostia, Bernard traveled in southern France. His preaching, aided by his ascetic looks and simple attire, helped doom the new sects. Both the Henrician and the Petrobrusian faiths began to die out by the end of that year. Soon afterwards, Henry of Lausanne was arrested, brought before the bishop of Toulouse, and probably imprisoned for life. In a letter to the people of Toulouse, undoubtedly written at the end of 1146, Bernard calls upon them to extirpate the last remnants of the heresy. He also preached against Catharism.

News came at this time from the Holy Land that alarmed Christendom. Christians had been defeated at the Siege of Edessa and most of the county had fallen into the hands of the Seljuk Turks. The Kingdom of Jerusalem and the other Crusader states were threatened with similar disaster. Deputations of the bishops of Armenia solicited aid from the pope, and the King of France also sent ambassadors. In 1144 Eugene III commissioned Bernard to preach the Second Crusade and granted the same indulgences for it which Pope Urban II had accorded to the First Crusade.
There was at first virtually no popular enthusiasm for the crusade as there had been in 1095. Bernard found it expedient to dwell upon taking the cross as a potent means of gaining absolution for sin and attaining grace. On 31 March, with King Louis VII of France present, he preached to an enormous crowd in a field at Vézelay, making "the speech of his life". The full text has not survived, but a contemporary account says that "his voice rang out across the meadow like a celestial organ"

James Meeker Ludlow describes the scene romantically in his book "The Age of the Crusades":

When Bernard was finished the crowd enlisted en masse; they supposedly ran out of cloth to make crosses. Bernard is said to have flung off his own robe and began tearing it into strips to make more. Others followed his example and he and his helpers were supposedly still producing crosses as night fell. 

Unlike the First Crusade, the new venture attracted royalty, such as Eleanor of Aquitaine, Queen of France; Thierry of Alsace, Count of Flanders; Henry, the future Count of Champagne; Louis's brother Robert I of Dreux; Alphonse I of Toulouse; William II of Nevers; William de Warenne, 3rd Earl of Surrey; Hugh VII of Lusignan, Yves II, Count of Soissons; and numerous other nobles and bishops. But an even greater show of support came from the common people. Bernard wrote to the pope a few days afterwards, "Cities and castles are now empty. There is not left one man to seven women, and everywhere there are widows to still-living husbands."

Bernard then passed into Germany, and the reported miracles which multiplied almost at his every step undoubtedly contributed to the success of his mission. Conrad III of Germany and his nephew Frederick Barbarossa, received the cross from the hand of Bernard. Pope Eugenius came in person to France to encourage the enterprise. As in the First Crusade, the preaching led to attacks on Jews; a fanatical French monk named Radulphe was apparently inspiring massacres of Jews in the Rhineland, Cologne, Mainz, Worms, and Speyer, with Radulphe claiming Jews were not contributing financially to the rescue of the Holy Land. The archbishop of Cologne and the archbishop of Mainz were vehemently opposed to these attacks and asked Bernard to denounce them. This he did, but when the campaign continued, Bernard traveled from Flanders to Germany to deal with the problems in person. He then found Radulphe in Mainz and was able to silence him, returning him to his monastery.

The last years of Bernard's life were saddened by the failure of the Second Crusade he had preached, the entire responsibility for which was thrown upon him. Bernard considered it his duty to send an apology to the Pope and it is inserted in the second part of his ""Book of Considerations."" There he explains how the sins of the crusaders were the cause of their misfortune and failures. 

Moved by his burning words, many Christians embarked for the Holy Land, but the crusade ended in miserable failure.

The death of his contemporaries served as a warning to Bernard of his own approaching end. The first to die was Suger in 1152, of whom Bernard wrote to Eugene III, "If there is any precious vase adorning the palace of the King of Kings it is the soul of the venerable Suger". Conrad III and his son Henry died the same year. From the beginning of the year 1153, Bernard felt his death approaching. The passing of Pope Eugenius had struck the fatal blow by taking from him one whom he considered his greatest friend and consoler. Bernard died at age sixty-three on 20 August 1153, after forty years spent in the cloister. He was buried at the Clairvaux Abbey, but after its dissolution in 1792 by the French revolutionary government, his remains were transferred to Troyes Cathedral.

Bernard was named a Doctor of the Church in 1830. At the 800th anniversary of his death, Pope Pius XII issued an encyclical on Bernard, "Doctor Mellifluus", in which he labeled him "The Last of the Fathers." Bernard did not reject human philosophy which is genuine philosophy, which leads to God; he differentiates between different kinds of knowledge, the highest being theological. The central elements of Bernard's Mariology are how he explained the virginity of Mary, "the "Star of the Sea"", and her role as Mediatrix.

The first abbot of Clairvaux developed a rich theology of sacred space and music, writing extensively on both. 

Bernard, like Thomas Aquinas, denied the doctrine of the Immaculate Conception of Mary. John Calvin quotes Bernard several times in support of the doctrine of "Sola Fide", which Martin Luther described as the article upon which the church stands or falls. Calvin also quotes him in setting forth his doctrine of a forensic alien righteousness, or as it is commonly called imputed righteousness.

One day, to cool down his lustful temptation, Bernard threw himself into ice-cold water. Another time, while sleeping in an inn, a prostitute was introduced naked beside him, and he saved his chastity by running.

Many miracles were attributed to his intercession. One time he restored the power of speech to an old man that he might confess his sins before he died. Another time, an immense number of flies, that had infested the Church of Foigny, died instantly after the excommunication he made on them.

So great was his reputation that princes and Popes sought his advice, and even the enemies of the Church admired the holiness of his life and the greatness of his writings.

Bernard was instrumental in re-emphasizing the importance of "lectio divina" and contemplation on Scripture within the Cistercian order. Bernard had observed that when "lectio divina" was neglected monasticism suffered. Bernard considered "lectio divina" and contemplation guided by the Holy Spirit the keys to nourishing Christian spirituality.

Bernard "noted centuries ago: the people who are their own spiritual directors have fools for disciples."

Bernard's theology and Mariology continue to be of major importance, particularly within the Cistercian and Trappist orders. Bernard led to the foundation of 163 monasteries in different parts of Europe. At his death, they numbered 343. His influence led Alexander III to launch reforms that would lead to the establishment of canon law. He was the first Cistercian monk placed on the calendar of saints and was canonized by Alexander III 18 January 1174. Pope Pius VIII bestowed on him the title "Doctor of the Church". He is labeled the "Mellifluous Doctor" for his eloquence. Cistercians honour him as the founder of the order because of the widespread activity which he gave to the order.

Saint Bernard's "Prayer to the Shoulder Wound of Jesus" is often published in Catholic prayer books.

Bernard is Dante Alighieri's last guide, in "Divine Comedy", as he travels through the Empyrean. Dante's choice appears to be based on Bernard's contemplative mysticism, his devotion to Mary, and his reputation for eloquence.

He is also the attributed author of the poems often translated in English hymnals as "O Sacred Head, Now Wounded" and "Jesus the Very Thought of Thee".

The Couvent et Basilique Saint-Bernard, a collection of buildings dating from the 12th, 17th and 19th centuries, is dedicated to Bernard and stands in his birthplace of Fontaine-lès-Dijon.

The modern critical edition is "" (1957–1977), edited by Jean Leclercq.

Bernard's works include:


His sermons are also numerous:

Many letters, treatises, and other works, falsely attributed to him survive, and are now referred to as works by pseudo-Bernard. These include:






</doc>
<doc id="4554" url="https://en.wikipedia.org/wiki?curid=4554" title="Bishkek">
Bishkek

Bishkek (, "BISHKEK", بىشکەک; ; ), formerly Pishpek and Frunze, is the capital and largest city of Kyrgyzstan (Kyrgyz Republic). Bishkek is also the administrative center of the Chuy Region. The province surrounds the city, although the city itself is not part of the province, but rather a province-level unit of Kyrgyzstan.

In 1825 Khokand authorities established the fortress of "Pishpek" in order to control local caravan-routes and to collect tribute from Kyrgyz tribes. On 4 September 1860, with the approval of the Kyrgyz, Russian forces led by Colonel Zimmermann destroyed the fortress. 
In 1868 a Russian settlement was established on the site of the fortress under its original name, "Pishpek". It lay within the General Governorship of Russian Turkestan and its Semirechye Oblast.

In 1925 the Kara-Kirghiz Autonomous Oblast was established in Russian Turkestan, promoting Pishpek to its capital. In 1926 the Communist Party of the Soviet Union renamed the city as "Frunze", after the Bolshevik military leader Mikhail Frunze (1885–1925), who was born there. In 1936, the city of Frunze became the capital of the Kirghiz Soviet Socialist Republic, during the final stages of the national delimitation in the Soviet Union.

In 1991 the Kyrgyz parliament changed the capital's name to "Bishkek".

Bishkek is situated at an altitude of about , just off the northern fringe of the Kyrgyz Ala-Too range, an extension of the Tian Shan mountain range. These mountains rise to a height of and provide a spectacular backdrop to the city. North of the city, a fertile and gently undulating steppe extends far north into neighboring Kazakhstan. The Chui River drains most of the area. Bishkek is connected to the Turkestan-Siberia Railway by a spur line.

Bishkek is a city of wide boulevards and marble-faced public buildings combined with numerous Soviet-style apartment blocks surrounding interior courtyards. There are also thousands of smaller privately built houses – mostly outside the city center. Streets follow a grid pattern, with most flanked on both sides by narrow irrigation channels, watering innumerable trees to provide shade in the hot summers.

Originally a caravan rest stop (possibly founded by the Sogdians) on one of the branches of the Silk Road through the Tian Shan range, the location was fortified in 1825 by the Uzbek khan of Kokhand with a mud fort. In the last years of Kokhand rule, the Pishpek fortress was led by Atabek, the Datka.

In 1860, the fort was conquered and razed by the military forces of Colonel Zimmermann when Tsarist Russia annexed the area. Colonel Zimmermann rebuilt the town over the destroyed fort and put field Poruchik Titov as head of a new Russian garrison. The site was redeveloped from 1877 onward by the Russian government, which encouraged the settlement of Russian peasants by giving them fertile land to develop.

In 1926, the city became the capital of the newly established Kirghiz ASSR and was renamed "Frunze" after Mikhail Frunze, Lenin's close associate who was born in Bishkek and played key roles during the revolutions of 1905 and 1917 and during the Russian civil war of the early 1920s.

The early 1990s were tumultuous. In June 1990, a state of emergency was declared following severe ethnic riots in southern Kyrgyzstan that threatened to spread to the capital. The city was renamed Bishkek on 5 February 1991 and Kyrgyzstan achieved independence later that year during the breakup of the Soviet Union. Before independence, the majority of Bishkek's population were ethnic Russians. In 2004, Russians made up approximately 20% of the city's population, and about in 2011.

Today, Bishkek is a modern city with many restaurants and cafes, and with many second-hand European and Japanese cars and minibuses crowding its streets. However, streets and sidewalks have fallen into disrepair since the 1990s. At the same time, Bishkek still preserves its former Soviet feel with Soviet-period buildings and gardens prevailing over newer structures.

Bishkek is also the country's financial center, with all of the country's 21 commercial banks headquartered there. During the Soviet era, the city was home to a large number of industrial plants, but most have been shut down since 1991 or now operate on a much reduced scale. One of Bishkek's largest employment centers today is the Dordoy Bazaar open market, where many of the Chinese goods imported to CIS countries are sold.

Though the city is relatively young, the surrounding area has some sites of interest dating to prehistorical times. There are also sites from the Greco-Buddhist period, the period of Nestorian influence, the era of the Central Asian "khanates", and the Soviet period.

The central part of the city is laid out on a rectangular grid plan. The city's main street is the east–west Chui Avenue (Chuy Prospekti), named after the region's main river. In the Soviet era, it was called Lenin Avenue. Along or near it are many of the most important government buildings and universities. These include the Academy of Sciences compound. The westernmost section of the avenue is known as Deng Xiaoping Avenue.

The main north–south street is Yusup Abdrakhmanov Street, still commonly referred to by its old name, Sovietskaya Street. Its northern and southern sections are called, respectively, Yelebesov and Baityk Batyr Streets. Several major shopping centers are located along it, and in the north it provides access to Dordoy Bazaar.

Erkindik ("Freedom") Boulevard runs from north to south, from the main railroad station (Bishkek II) south of Chui Avenue to the museum quarter and sculpture park just north of Chui Avenue, and further north toward the Ministry of Foreign Affairs. In the past it was called Dzerzhinsky Boulevard, named after a Communist revolutionary, Felix Dzerzhinsky, and its northern continuation is still called Dzerzhinsky Street.

An important east–west street is Jibek Jolu ('Silk Road'). It runs parallel to Chui Avenue about north of it, and is part of the main east–west road of Chui Province. Both the eastern and western bus terminals are located along Jibek Jolu.

There is a Roman Catholic church located at ul. Vasiljeva 197 (near Rynok Bayat). It is the only Catholic cathedral in Kyrgyzstan.


The Dordoy Bazaar, just inside the bypass highway on the north-eastern edge of the city, is a major retail and wholesale market.

The Kyrgyz Ala-Too mountain range, some away, provides a spectacular backdrop to the city; the Ala Archa National Park is only a 30 to 45 minutes drive away.

Bishkek has both hot dry summer continental and mediterranean climate depending which isotherm would be used. (Köppen climate classification "Csa/Dsa" respectively). Average precipitation is around per year. Average daily high temperatures range from in January to about during July. The summer months are dominated by dry periods, punctuated by the occasional thunderstorm, which produces strong gusty winds and rare dust storms. The mountains to the south provide a natural boundary and protection from much of the damaging weather, as does the smaller mountain chain which runs north-west to south-east. In the winter months, sparse snow storms and frequent heavy fog are the dominating features. There are sometimes temperature inversions, during which the fog can last for days at a time.

Bishkek is the most populated city in Kyrgyzstan. Its population, estimated in 2015, was 937,400. From the foundation of the city to the mid-1990s, ethnic Russians and other peoples of European descent (Ukrainians, Germans) comprised the majority of the city's population. According to the 1970 census, the ethnic Kyrgyz were only 12.3%, while Europeans comprised more than 80% of Frunze population. Now Bishkek is a predominantly Kyrgyz city, with around 66% of its residents Kyrgyz, while European peoples make up less than 20% of the population. Despite this fact, Russian is the main language while Kyrgyz continues losing ground especially among the younger generations 

Emissions of air pollutants in Bishkek amounted to 14,400 tons in 2010. Among all cities in Kyrgyzstan, the level of air pollution in Bishkek is the highest, occasionally exceeding maximum allowable concentrations by several times, especially in the city center. For example, concentrations of formaldehyde occasionally exceed maximum allowable limits by a factor of four.

Responsibility for ambient air quality monitoring in Bishkek lies with the Kyrgyz State Agency of Hydrometeorology. There are seven air quality monitoring stations in Bishkek, measuring levels of sulfur dioxide, nitrogen oxides, formaldehyde, and ammonia.

Bishkek uses the Kyrgyzstan currency, the som. The som's value fluctuates regularly, but averaged around 61 som per U.S. dollar as of February 2015. The economy in Bishkek is primarily agricultural, and agricultural products are sometimes bartered in the outlying regions. The streets of Bishkek are regularly lined with produce vendors in a market style venue. In most of the downtown area there is a more urban cityscape with banks, stores, markets and malls. Sought after goods include hand-crafted artisan pieces, such as statues, carvings, paintings and many nature-based sculptures.

As with many cities in Post-Soviet states, housing in Bishkek has undergone extensive changes since the collapse of the Soviet Union. While housing was formerly distributed to citizens in the Soviet-era, housing in Bishkek has since become privatized.

Though single family houses are slowly becoming more popular, the majority of the residents live in Soviet-era apartments. Despite the Kyrgyz economy experiencing growth, increases in available housing has been slow with very little new construction. As a result of this growing prosperity and the lack of new formal housing, prices have been rising significantly – doubling from 2001 to 2002.

Those unable to afford the high price of housing within Bishkek, notably internal migrants from rural villages and small provincial towns often have to resort to informal squatter settlements on the outskirts of the city. These settlements are estimated to house 400,000 people or about 30 percent of Bishkek’s population. While many of the settlements have lacked basic necessities such as electricity and running water, recently there has been a push by the local government to provide these services.

Local government is administered by the Bishkek Mayor's Office. Askarbek Salymbekov was mayor until his resignation in August 2005, after which his deputy, Arstanbek Nogoev, took over the mayorship. Nogoev was in turn removed from his position in October 2007 through a decree of President Kurmanbek Bakiyev and replaced by businessman and former first deputy prime minister Daniar Usenov. In July 2008 former head of the Kyrgyz Railways Nariman Tuleyev was appointed mayor, who was dismissed by the interim government after 7 April 2010. From April 2010 to February 2011 Isa Omurkulov, also a former head of the Kyrgyz Railways, was an interim mayor, and from 4 February 2011 to 14 December 2013 he was re-elected the mayor of Bishkek. Kubanychbek Kulmatov was nominated for election by parliamentary group of Social Democratic Party of Kyrgyzstan in city kenesh, and he was elected as a new mayor on 15 January 2014, and stepped down on 9 February 2016 
The new major Albek Sabirbekovich Ibraimov was also nominated for election by parliamentary group of Social Democratic Party of Kyrgyzstan in city kenesh, and he was elected by Bishkek City Kenesh on 27 February 2016.

Bishkek city covers and is administered separately and not part of any region. Besides the city proper, one urban-type settlement and one village are administered by the city: Chong-Aryk and Orto-Say. The city is divided into 4 districts: Birinchi May, Lenin, Oktyabr and Sverdlov. Chong-Aryk and Orto-Say are part of Lenin District.

Bishkek is home to Spartak, the largest football stadium in Kyrgyzstan and the only one eligible to host international matches. Several Bishkek-based football teams play on this pitch, including six-time Kyrgyzstan League champions, Dordoi-Dynamo.

Bishkek hosted the 2014 IIHF Challenge Cup of Asia – Division I.

Educational institutions in Bishkek include:
In addition, the following international schools serve the expatriate community in Bishkek:

Public transportation includes buses, electric trolley buses, and public vans (known in Russian as "marshrutka"). The first bus and trolley bus services in Bishkek were introduced in 1934 and 1951, respectively.

Taxi cabs can be found throughout the city.

The city is considering designing and building a light rail system ().

There are two main bus stations in Bishkek. The smaller old Eastern Bus Station is primarily the terminal for minibuses to various destinations within or just beyond the eastern suburbs, such as Kant, Tokmok, Kemin, Issyk Ata, or the Korday border crossing.

Long-distance regular bus and minibus services to all parts of the country, as well as to Almaty (the largest city in neighboring Kazakhstan) and Kashgar, China, run mostly from the newer grand Western Bus Station; only a smaller number run from the Eastern Station.

The Dordoy Bazaar on the north-eastern outskirts of the city also contains makeshift terminals for frequent minibuses to suburban towns in all directions (from Sokuluk in the west to Tokmak in the east) and to some buses taking traders to Kazakhstan and Siberia.

, the Bishkek railway station sees only a few trains a day. It offers a popular three-day train service from Bishkek to Moscow.

There are also long-distance trains that leave for Siberia (Novosibirsk and Novokuznetsk), via Almaty, over the Turksib route, and to Yekaterinburg (Sverdlovsk) in the Urals, via Astana. These services are remarkably slow (over 48 hours to Yekaterinburg), due to long stops at the border and the indirect route (the trains first have to go west for more than a before they enter the main Turksib line and can continue to the east or north). For example, as of the fall of 2008, train No. 305 Bishkek-Yekaterinburg was scheduled to take 11 hours to reach the Shu junction—a distance of some by rail, and less than half of that by road.

The city is served by Manas International Airport (IATA code FRU), located approximately northwest of the city centre, and readily reachable by taxi.

In 2002, the United States obtained the right to use Manas International Airport as an air base for its military operations in Afghanistan and Iraq. Russia subsequently (2003) established an air base of its own (Kant Air Base) near Kant, some east of Bishkek. It is based at a facility that used to be home to a major Soviet military pilot training school; one of its students, Hosni Mubarak, later became president of Egypt.


Sister cities of Bishkek include:



</doc>
<doc id="4560" url="https://en.wikipedia.org/wiki?curid=4560" title="Braveheart">
Braveheart

Braveheart is a 1995 American epic war film directed by Mel Gibson, who stars as William Wallace, a late 13th-century Scottish warrior who led the Scots in the First War of Scottish Independence against King Edward I of England. The story is inspired by Blind Harry's epic poem "The Actes and Deidis of the Illustre and Vallyeant Campioun Schir William Wallace" and was adapted for the screen by Randall Wallace.

It grossed $210.4 million worldwide. "Braveheart" was nominated for ten Academy Awards at the 68th Academy Awards and won five: Best Picture, Best Director, Best Cinematography, Best Makeup, and Best Sound Editing. The film was criticised for inaccuracies regarding Wallace's title, love interests, and attire.

In 1280, King Edward "Longshanks" invades and conquers Scotland following the death of Alexander III of Scotland, who left no heir to the throne. Young William Wallace witnesses Longshanks' treachery, survives the deaths of his father and brother, and is taken abroad on a pilgrimage throughout Europe by his paternal Uncle Argyle, where he is educated. Years later, Longshanks grants his noblemen land and privileges in Scotland, including "Prima Nocte". Meanwhile, a grown Wallace returns to Scotland and falls in love with his childhood friend Murron MacClannough, and the two marry in secret. Wallace rescues Murron from being raped by English soldiers, but as she fights off their second attempt, Murron is captured and publicly executed. In retribution, Wallace leads his clan to slaughter the English garrison in his hometown and send the occupying garrison at Lanark back to England.

Longshanks orders his son Prince Edward to stop Wallace by any means necessary. Wallace rebels against the English, and as his legend spreads, hundreds of Scots from the surrounding clans join him. Wallace leads his army to victory at the Battle of Stirling Bridge and then destroys the city of York, killing Longshanks' nephew and sending his severed head to the king. Wallace seeks the assistance of Robert the Bruce, the son of nobleman Robert the Elder and a contender for the Scottish crown. Robert is dominated by his father, who wishes to secure the throne for his son by submitting to the English. Worried by the threat of the rebellion, Longshanks sends his son's wife Isabella of France to try to negotiate with Wallace as a distraction for the landing of another invasion force in Scotland.

After meeting him in person, Isabella becomes enamored of Wallace. Warned of the coming invasion by Isabella, Wallace implores the Scottish nobility to take immediate action to counter the threat and take back the country. Leading the English army himself, Longshanks confronts the Scots at Falkirk where noblemen Lochlan and Mornay, having been bribed by Longshanks, betray Wallace, causing the Scots to lose the battle. As Wallace charges toward the departing Longshanks on horseback, he is intercepted by one of the king's lancers, who turns out to be Robert the Bruce, but filled with remorse, Bruce gets Wallace to safety before the English can capture him. Wallace kills Lochlan and Mornay for their betrayal, and wages a guerrilla war against the English for the next seven years, assisted by Isabella, with whom he eventually has an affair. Robert sets up a meeting with Wallace in Edinburgh, but Robert's father has conspired with other nobles to capture and hand over Wallace to the English. Learning of his treachery, Robert disowns his father. Isabella exacts revenge on the now terminally ill Longshanks by telling him that his bloodline will be destroyed upon his death as she is now pregnant with Wallace's child.

In London, Wallace is brought before an English magistrate, tried for high treason, and condemned to public torture and beheading. Even whilst being hanged, drawn and quartered, Wallace refuses to submit to the king. As cries for mercy come from the watching crowd deeply moved by the Scotsman's valor, the magistrate offers him one final chance, asking him only to utter the word, "Mercy", and be granted a quick death. Wallace instead shouts, "Freedom!", and the judge orders his death. Moments before being decapitated, Wallace sees a vision of Murron in the crowd, smiling at him.

In 1314, Robert, now Scotland's king, leads a Scottish army before a ceremonial line of English troops on the fields of Bannockburn, where he is to formally accept English rule. As he begins to ride toward the English, he stops and invokes Wallace's memory, imploring his men to fight with him as they did with Wallace. Robert then leads his army into battle against the stunned English, winning the Scots their freedom.

Gibson's production company, Icon Productions, had difficulty raising enough money even if he were to star in the film. Warner Bros. was willing to fund the project on the condition that Gibson sign for another "Lethal Weapon" sequel, which he refused. Paramount Pictures only agreed to American and Canadian distribution of "Braveheart" after 20th Century Fox partnered for international rights. The production budget has been estimated by IMDb at US$72 million.

While the crew spent six weeks shooting on location in Scotland, the major battle scenes were shot in Ireland using members of the Irish Army Reserve as extras. To lower costs, Gibson had the same extras, up to 1,600 in some scenes, portray both armies. The reservists had been given permission to grow beards and swapped their military uniforms for medieval garb.

"Braveheart" was shot in the anamorphic format with Panavision C- and E-Series lenses.

Gibson toned down the film's battle scenes to avoid an NC-17 rating from the MPAA; the final version was rated R for "brutal medieval warfare".

The score was composed and conducted by James Horner and performed by the London Symphony Orchestra. It is Horner's second of three collaborations with Mel Gibson as director. The score has gone on to be one of the most commercially successful soundtracks of all time. It received considerable acclaim from film critics and audiences and was nominated for a number of awards, including the Academy Award, Saturn Award, BAFTA Award, and Golden Globe Award.

On its opening weekend, "Braveheart" grossed $9,938,276 in the United States and $75.6 million in its box office run in the U.S. and Canada. Worldwide, the film grossed $210,409,945 and was the thirteenth highest-grossing film of 1995.

Roger Ebert gave the film 3.5 stars out of four, calling it "An action epic with the spirit of the Hollywood swordplay classics and the grungy ferocity of "The Road Warrior"."

The film's depiction of the Battle of Stirling Bridge was listed by CNN as one of the best battles in cinema history.

"Braveheart" holds a 76% approval rating at review aggregator Rotten Tomatoes, with an average score of 7.2/10, based on 72 reviews.

In a 2005 poll by British film magazine "Empire", "Braveheart" was No. 1 on their list of "The Top 10 Worst Pictures to Win Best Picture Oscar". Empire readers had previously voted "Braveheart" the best film of 1995.

The European premiere was on September 3, 1995 in Stirling.

In 1996, the year after the film was released, the annual three-day "Braveheart Conference" at Stirling Castle attracted fans of "Braveheart", increasing the conference's attendance to 167,000 from 66,000 in the previous year. In the following year, research on visitors to the Stirling area indicated that 55% of the visitors had seen "Braveheart". Of visitors from outside Scotland, 15% of those who saw "Braveheart" said it influenced their decision to visit the country. Of all visitors who saw "Braveheart", 39% said the film influenced in part their decision to visit Stirling, and 19% said the film was one of the main reasons for their visit. In the same year, a tourism report said that the ""Braveheart" effect" earned Scotland ₤7 million to ₤15 million in tourist revenue, and the report led to various national organizations encouraging international film productions to take place in Scotland.

The film generated huge interest in Scotland and in Scottish history, not only around the world, but also in Scotland itself. Fans came from all over the world to see the places in Scotland where William Wallace fought, also to the places in Scotland and Ireland used as locations in the film. At a "Braveheart" Convention in 1997, held in Stirling the day after the Scottish Devolution vote and attended by 200 delegates from around the world, "Braveheart" author Randall Wallace, Seoras Wallace of the Wallace Clan, Scottish historian David Ross and Bláithín FitzGerald from Ireland gave lectures on various aspects of the film. Several of the actors also attended including James Robinson (Young William), Andrew Weir (Young Hamish), Julie Austin (the young bride) and Mhairi Calvey (Young Murron).

"Braveheart" was nominated for many awards during the 1995 Oscar season, though it was not viewed by many as a major contender such as "Apollo 13", "", "Leaving Las Vegas", "Sense and Sensibility", and "The Usual Suspects". It wasn't until after the film won the Golden Globe Award for Best Director at the 53rd Golden Globe Awards that it was viewed as a serious Oscar contender. When the nominations were announced for the 68th Academy Awards, "Braveheart" received ten Academy Award nominations, and a month later, won five. In 2010, the "Independent Film & Television Alliance" selected the film as one of the 30 Most Significant Independent Films of the last 30 years


Lin Anderson, author of "Braveheart: From Hollywood To Holyrood", credits the film with playing a significant role in affecting the Scottish political landscape in the mid to late 1990s.

In 1997, a 12-ton sandstone statue depicting Mel Gibson as William Wallace in "Braveheart" was placed in the car park of the Wallace Monument near Stirling, Scotland. The statue, which was the work of Tom Church, a monumental mason from Brechin, included the word "Braveheart" on Wallace's shield. The installation became the cause of much controversy; one local resident stated that it was wrong to "desecrate the main memorial to Wallace with a lump of crap". In 1998, someone wielding a hammer vandalized the statue's face. After repairs were made, the statue was encased in a cage every night to prevent further vandalism. This only incited more calls for the statue to be removed, as it then appeared that the Gibson/Wallace figure was imprisoned. The statue was described as "among the most loathed pieces of public art in Scotland". In 2008, the statue was returned to its sculptor to make room for a new visitor centre being built at the foot of the Wallace Monument.

Randall Wallace, who wrote the screenplay, has acknowledged Blind Harry's 15th-century epic poem "The Acts and Deeds of Sir William Wallace, Knight of Elderslie" as a major inspiration for the film. In defending his script, Randall Wallace has said, "Is Blind Harry true? I don't know. I know that it spoke to my heart and that's what matters to me, that it spoke to my heart." Blind Harry's poem is not regarded as historically accurate, and although some incidents in the film that are not historically accurate are taken from Blind Harry (e.g. the hanging of Scottish nobles at the start), there are large parts that are based neither on history nor Blind Harry (e.g. Wallace's affair with Princess Isabella).

Elizabeth Ewan describes "Braveheart" as a film that "almost totally sacrifices historical accuracy for epic adventure". The "brave heart" refers in Scottish history to that of Robert the Bruce, and an attribution by William Edmondstoune Aytoun, in his poem "Heart of Bruce", to Sir James the Good Douglas: "Pass thee first, thou dauntless heart, As thou wert wont of yore!", prior to Douglas' demise at the Battle of Teba in Andalusia. It has been described as one of the most historically inaccurate modern films.

Sharon Krossa noted that the film contains numerous historical errors, beginning with the wearing of belted plaid by Wallace and his men. In that period "no Scots ... wore belted plaids (let alone kilts of any kind)." Moreover, when Highlanders finally did begin wearing the belted plaid, it was not "in the rather bizarre style depicted in the film". She compares the inaccuracy to "a film about Colonial America showing the colonial men wearing 20th century business suits, but with the jackets worn back-to-front instead of the right way around." In a previous essay about the film, she wrote, "The events aren't accurate, the dates aren't accurate, the characters aren't accurate, the names aren't accurate, the clothes aren't accurate—in short, just about nothing is accurate." The belted plaid ("feileadh mór léine") was not introduced until the 16th century. Peter Traquair has referred to Wallace's "farcical representation as a wild and hairy highlander painted with woad (1,000 years too late) running amok in a tartan kilt (500 years too early)." 

Irish historian Seán Duffy remarked "the battle of Stirling Bridge could have done with a bridge." 

In 2009, the film was second on a list of "most historically inaccurate movies" in "The Times". In the humorous non-fictional historiography "An Utterly Impartial History of Britain" (2007), author John O'Farrell notes that "Braveheart" could not have been more historically inaccurate, even if a "Plasticine dog" had been inserted in the film and the title changed to “"William Wallace and Gromit"”.

In the DVD audio commentary of "Braveheart", Mel Gibson acknowledges many of the historical inaccuracies but defends his choices as director, noting that the way events were portrayed in the film was much more "cinematically compelling" than the historical fact or conventional mythos.

Edward Longshanks, King of England, is shown invoking "Jus primae noctis", allowing the Lord of a medieval estate to take the virginity of his serfs' maiden daughters on their wedding nights. Critical medieval scholarship regards this supposed right as a myth, "the simple reason why we are dealing with a myth here rests in the surprising fact that practically all writers who make any such claims have never been able or willing to cite any trustworthy source, if they have any."

The film suggests Scotland had been under English occupation for some time, at least during Wallace’s childhood, and in the run-up to the Battle of Falkirk Wallace says to the younger Bruce, “[W]e'll have what none of us have ever had before, a country of our own.” In fact Scotland had been invaded by England only the year before Wallace's rebellion; prior to the death of King Alexander III it had been a fully separate kingdom.

As John Shelton Lawrence and Robert Jewett write, "Because [William] Wallace is one of Scotland's most important national heroes and because he lived in the very distant past, much that is believed about him is probably the stuff of legend. But there is a factual strand that historians agree to", summarized from Scots scholar Matt Ewart:
A.E. Christa Canitz writes about the historical William Wallace further: "[He] was a younger son of the Scottish gentry, usually accompanied by his own chaplain, well-educated, and eventually, having been appointed Guardian of the Kingdom of Scotland, engaged in diplomatic correspondence with the Hanseatic cities of Lübeck and Hamburg". She finds that in "Braveheart", "any hint of his descent from the lowland gentry (i.e., the lesser nobility) is erased, and he is presented as an economically and politically marginalized Highlander and 'a farmer'—as one with the common peasant, and with a strong spiritual connection to the land which he is destined to liberate."

Colin McArthur writes that "Braveheart" "constructs Wallace as a kind of modern, nationalist guerrilla leader in a period half a millennium before the appearance of nationalism on the historical stage as a concept under which disparate classes and interests might be mobilised within a nation state." Writing about "Braveheart"s "omissions of verified historical facts", McArthur notes that Wallace made "overtures to Edward I seeking less severe treatment after his defeat at Falkirk", as well as "the well-documented fact of Wallace's having resorted to conscription and his willingness to hang those who refused to serve." Canitz posits that depicting "such lack of class solidarity" as the conscriptions and related hangings "would contaminate the movie's image of Wallace as the morally irreproachable "primus inter pares" among his peasant fighters."

Isabella of France is shown having an affair with Wallace after the Battle of Falkirk. She later tells Edward I she is pregnant, implying that her son, Edward III, was a product of the affair. In reality, Isabella was three years old and living in France at the time of the Battle of Falkirk, was not married to Edward II until he was already king, and Edward III was born seven years after Wallace died.

Robert the Bruce did change sides between the Scots loyalists and the English more than once in the earlier stages of the Wars of Scottish Independence, but he never betrayed Wallace directly, and he probably did not fight on the English side at the Battle of Falkirk (although this claim does appear in a few medieval sources). Later, the Battle of Bannockburn was not a spontaneous battle; he had already been fighting a guerrilla campaign against the English for eight years. His title before becoming king was Earl of Carrick, not Earl of Bruce.

The actual Edward I was ruthless and temperamental, but the film exaggerates his character for effect. Edward enjoyed poetry and harp music, was a devoted and loving husband to his wife Eleanor of Castile, and as a religious man he gave generously to charity. The film's scene where he scoffs cynically at Isabella for distributing gold to the poor after Wallace refuses it as a bribe would have been unlikely. Also, Edward died on campaign two years after Wallace's execution, not in bed at his home.

The depiction of the future Edward II as an effeminate homosexual drew accusations of homophobia against Gibson. The actual Edward II, who fathered five children by two different women, was rumoured to have had sexual affairs with men, including Piers Gaveston, on whom the Prince's male lover Phillip was loosely based.

Gibson defended his depiction of Prince Edward as weak and ineffectual, saying:
In response to Longshanks's murder of the Prince's male lover Phillip, Gibson replied: "The fact that King Edward throws this character out a window has nothing to do with him being gay ... He's terrible to his son, to everybody."
Gibson asserted that the reason Longshanks kills his son's lover is because the king is a "psychopath". Gibson expressed bewilderment that some filmgoers would laugh at this murder.

"MacGregors from the next glen" joining Wallace shortly after the action at Lanark is dubious, since it is questionable whether Clan Gregor existed at that stage, and when they did emerge their traditional home was Glen Orchy, some distance from Lanark.

Wallace did win an important victory at the Battle of Stirling Bridge, but the version in "Braveheart" is highly inaccurate, as it was filmed without a bridge (and without Andrew Moray, joint commander of the Scots army, who was fatally injured in the battle). Later, Wallace did carry out a large-scale raid into the north of England, but he did not get as far south as York, nor did he kill Longshanks' nephew. (However this was not as wide of the mark as Blind Harry, who has Wallace making it as far south as St. Albans, and only refraining from attacking London after the English queen came out to meet him.) Edward's nephew John of Brittany did take part in the Wars of Scottish Independence, but he was not killed at York.

The "Irish conscripts" at the Battle of Falkirk are also unhistorical; there were no Irish troops at Falkirk (although many of the English army were actually Welsh), and it is anachronistic to refer to conscripts in the Middle Ages (although there were feudal levies).

The two-handed long swords used by Gibson in the film were not in wide use in the period. A one-handed sword and shield would be more accurate.

Sections of the English media accused the film of harbouring Anglophobia. "The Economist" called it "xenophobic", and John Sutherland writing in "The Guardian" stated that: ""Braveheart" gave full rein to a toxic Anglophobia".

In "The Times", MacArthur said "the political effects are truly pernicious. It’s a xenophobic film." Ian Burrell of "The Independent" has noted, "The "Braveheart" phenomenon, a Hollywood-inspired rise in Scottish nationalism, has been linked to a rise in anti-English prejudice".

Braveheart was released on DVD on August 29 2000. It was released on Blu-ray which is part of the Paramount Sapphire Series on September 1 2009. It was released on 4K UHD Blu-Ray, which is part of the 4K upgrade of the Paramount Sapphire series, on May 15 2018. 

On February 9, 2018, a sequel titled "Robert the Bruce" was announced. The film will lead directly on from "Braveheart" and follow the widow Moira, portrayed by Anna Hutchison, and her family (portrayed by Gabriel Bateman and Talitha Bateman), who save Robert the Bruce, with Angus Macfadyen reprising his role from "Braveheart". The cast will also include Jared Harris, Patrick Fugit, Zach McGowan, Emma Kenney, Diarmaid Murtagh, Seoras Wallace, Shane Coffey, Kevin McNally and Melora Walters. Richard Gray will direct the film, with Macfadyen and Eric Belgau writing the script. Helmer Gray, Macfadyen, Hutchison, Kim Barnard, Nick Farnell, Cameron Nuggent and Andrew Curry will produce the film.



</doc>
<doc id="4561" url="https://en.wikipedia.org/wiki?curid=4561" title="Brian Aldiss">
Brian Aldiss

Brian Wilson Aldiss, OBE (; 18 August 1925 – 19 August 2017) was an English writer and anthologies editor, best known for science fiction novels and short stories. His byline reads either Brian W. Aldiss or simply Brian Aldiss, except for occasional pseudonyms during the mid-1960s.

Greatly influenced by science fiction pioneer H. G. Wells, Aldiss was a vice-president of the international H. G. Wells Society. He was (with Harry Harrison) co-president of the Birmingham Science Fiction Group. Aldiss was named a Grand Master by the Science Fiction Writers of America in 2000 and inducted by the Science Fiction Hall of Fame in 2004. He received two Hugo Awards, one Nebula Award, and one John W. Campbell Memorial Award. He wrote the short story "Super-Toys Last All Summer Long" (1969), the basis for the Stanley Kubrick-developed Steven Spielberg film "A.I. Artificial Intelligence" (2001). Aldiss was associated with the British New Wave of science fiction.

Aldiss was born on 18 August 1925, above his paternal grandfather's draper's shop in Dereham, Norfolk. When Aldiss's grandfather died, his father, Bill (the younger of two sons), sold his share in the shop and the family left Dereham. Aldiss's mother, Dot, was the daughter of a builder. He had an older sister who was stillborn, and a younger sister. As a 3-year-old, Aldiss started to write stories which his mother would bind and put on a shelf. At the age of 6, he went to Framlingham College but moved to Devon and was sent to board at West Buckland School in Devon in 1939 after the outbreak of the war. As a child he discovered the pulp magazine "Astounding Science Fiction", and read all the novels by H. G. Wells and Robert Heinlein, and later Philip K. Dick. In 1943, during the Second World War, he joined the Royal Signals and saw action in Burma.

His Army experience inspired the Horatio Stubbs second and third books, "A Soldier Erect" and "A Rude Awakening", respectively.

After the war, he worked as a bookseller in Oxford. He also wrote a number of short pieces for a booksellers' trade journal about life in a fictitious bookshop, which attracted the attention of Charles Monteith, an editor at the publisher Faber and Faber. As a result, Faber and Faber published Aldiss' first book, "The Brightfount Diaries" (1955), a 200-page novel in diary form about the life of a sales assistant in a bookshop.

About this time he also began to write science fiction for various magazines. According to ISFDB, his first speculative fiction in print was the short story "Criminal Record", published by John Carnell in the July 1954 issue of "Science Fantasy". Several of his stories appeared in 1955, including three in monthly issues of "New Worlds", a more important magazine also edited by Carnell.

In 1954, "The Observer" newspaper ran a competition for a short story set in the year 2500. Aldiss' story "Not For An Age" was ranked third following a reader vote.

"The Brightfount Diaries" had been a minor success, and Faber asked Aldiss if he had any more writing they could look at with a view to publishing. Aldiss confessed to being a science fiction author, to the delight of the publishers, who had a number of science fiction fans in high places, and so his first science fiction book was published, a collection of short stories entitled "Space, Time and Nathaniel" (Faber, 1957). By this time, his earnings from writing matched his wages in the bookshop, and he made the decision to become a full-time writer.

Aldiss led the voting for Most Promising New Author of 1958 at the next year's Worldcon, but finished behind "no award". He was elected president of the British Science Fiction Association in 1960. He was the literary editor of the "Oxford Mail" newspaper from 1958 to 1969. Around 1964, he and long-time collaborator Harry Harrison started the first ever journal of science fiction criticism, "Science Fiction Horizons", which during its brief span of two issues published articles and reviews by such authors as James Blish, and featured a discussion among Aldiss, C. S. Lewis, and Kingsley Amis in the first issue and an interview with William S. Burroughs in the second. In 1967 Algis Budrys listed Aldiss, J. G. Ballard, Roger Zelazny, and Samuel R. Delany as "an earthshaking new kind of" writers, and leaders of the New Wave.
Besides his own writings, he had great success as an anthologist. For Faber he edited "Introducing SF", a collection of stories typifying various themes of science fiction, and "Best Fantasy Stories". In 1961, he edited an anthology of reprinted short science fiction for the British paperback publisher Penguin Books under the title "Penguin Science Fiction". This was remarkably successful, went into numerous reprints, and was followed up by two further anthologies: "More Penguin Science Fiction" (1963) and "Yet More Penguin Science Fiction" (1964). The later anthologies enjoyed the same success as the first, and all three were eventually published together as "The Penguin Science Fiction Omnibus" (1973), which also went into a number of reprints. In the 1970s, he produced several large collections of classic grand-scale science fiction, under the titles "Space Opera" (1974), "Space Odysseys" (1975), "Galactic Empires" (1976), "Evil Earths" (1976), and "Perilous Planets" (1978) which were quite successful. Around this time, he edited a large-format volume "Science Fiction Art" (1975), with selections of artwork from the magazines and pulps.

In response to the results from the planetary probes of the 1960s and 1970s, which showed that Venus was completely unlike the hot, tropical jungle usually depicted in science fiction, Aldiss and Harrison edited an anthology "Farewell, Fantastic Venus!", reprinting stories based on the pre-probe ideas of Venus. He also edited, with Harrison, a series of anthologies "The Year's Best Science Fiction" (Nos. 1–9, 1968–1976).

Aldiss invented a form of extremely short story called the "mini-saga". "The Daily Telegraph" hosted a competition for the best mini-saga for several years, and Aldiss was the judge. He has edited several anthologies of the best mini-sagas.
Aldiss travelled to Yugoslavia, where he met fans in Ljubljana, Slovenia and published a travel book about Yugoslavia entitled "Cities and Stones" (1966), his only work in the genre. He published an alternative-history fantasy story, "The Day of the Doomed King" (1968), about Serbian kings in the Middle Ages, and wrote a novel called "The Malacia Tapestry", about an alternative Dalmatia.

In addition to a highly successful career as a writer, Aldiss was an accomplished artist. His first solo exhibition, "The Other Hemisphere", was held in Oxford, August–September 2010, and the exhibition's centrepiece "Metropolis" (see figure) has since been released as a limited edition fine art print.(The exhibition title denotes the writer/artist's notion, "words streaming from one side of his brain inspiring images in what he calls 'the other hemisphere'.")

In 1948, Aldiss married Olive Fortescue, secretary to the owner of Sanders' bookseller's in Oxford, where he had worked since 1947. He had two children from his first marriage: Clive in 1955 and Caroline Wendy in 1957, but the marriage "finally collapsed" in 1959 and dissolved in 1965.

In 1965, he married his second wife, Margaret Christie Manson (daughter of John Alexander Christie Manson, an aeronautical engineer), a Scottish woman and secretary to the editor of the "Oxford Mail"; Aldiss was 40, and she 31. They lived in Oxford and had two children together, Tim and Charlotte. She died in 1997.

Aldiss died on 19 August 2017, the day after his 92nd birthday.

He was elected a Fellow of the Royal Society of Literature in 1990.

Aldiss was the "Permanent Special Guest" at the annual International Conference on the Fantastic in the Arts (ICFA) from 1989 through 2008. He was also the Guest of Honor at the conventions in 1986 and 1999.

The Science Fiction Writers of America made him its 18th SFWA Grand Master in 2000 and the Science Fiction and Fantasy Hall of Fame inducted him in 2004.

He was awarded the title of Officer of the Order of the British Empire (OBE) for services to literature in the 2005 Birthday Honours list.

In January 2007 he appeared on "Desert Island Discs". His choice of record to 'save' was "Old Rivers" sung by Walter Brennan, his choice of book was John Heilpern's biography of John Osborne, and his luxury a banjo. The full selection of eight favourite records is on the BBC website.

On 1 July 2008 he was awarded an honorary doctorate by the University of Liverpool in recognition of his contribution to literature. The Brian W Aldiss Archive at the University holds manuscripts from the period 1943–1995.

In 2013, Aldiss was recipient of the World Fantasy Convention Award at the World Fantasy Convention in Brighton, England.

Aldiss sat on the Council of the Society of Authors.

He won two Hugo awards: in 1962 for the "Hothouse" series; and in 1987 for "Trillion Year Spree". Aldiss also won a Nebula award in 1965 for "The Saliva Tree: And Other Strange Growths".

Aldiss was the author of over 80 books and 300 short stories. he also wrote several volumes of poetry.







</doc>
<doc id="4563" url="https://en.wikipedia.org/wiki?curid=4563" title="Battle of Jutland">
Battle of Jutland

The Battle of Jutland (, the Battle of Skagerrak) was a naval battle fought by the British Royal Navy's Grand Fleet under Admiral Sir John Jellicoe, against the Imperial German Navy's High Seas Fleet under Vice-Admiral Reinhard Scheer during the First World War. The battle unfolded in extensive manoeuvring and three main engagements (the battlecruiser action, the fleet action and the night action), from 31 May to 1 June 1916, off the North Sea coast of Denmark's Jutland Peninsula. It was the largest naval battle and the only full-scale clash of battleships in that war. Jutland was the third fleet action between steel battleships, following the smaller but more decisive battles of the Yellow Sea (1904) and Tsushima (1905) during the Russo-Japanese War. Jutland was the last major battle in world history fought primarily by battleships.

Germany's High Seas Fleet intended to lure out, trap, and destroy a portion of the Grand Fleet, as the German naval force was insufficient to openly engage the entire British fleet. This formed part of a larger strategy to break the British blockade of Germany and to allow German naval vessels access to the Atlantic. Meanwhile, Great Britain's Royal Navy pursued a strategy of engaging and destroying the High Seas Fleet, thereby keeping German naval forces contained and away from Britain and her shipping lanes.

The Germans planned to use Vice-Admiral Franz Hipper's fast scouting group of five modern battlecruisers to lure Vice-Admiral Sir David Beatty's battlecruiser squadrons into the path of the main German fleet. They stationed submarines in advance across the likely routes of the British ships. However, the British learned from signal intercepts that a major fleet operation was likely, so on 30 May Jellicoe sailed with the Grand Fleet to rendezvous with Beatty, passing over the locations of the German submarine picket lines while they were unprepared. The German plan had been delayed, causing further problems for their submarines, which had reached the limit of their endurance at sea.

On the afternoon of 31 May, Beatty encountered Hipper's battlecruiser force long before the Germans had expected. In a running battle, Hipper successfully drew the British vanguard into the path of the High Seas Fleet. By the time Beatty sighted the larger force and turned back towards the British main fleet, he had lost two battlecruisers from a force of six battlecruisers and four powerful battleships – though he had sped ahead of his battleships of 5th Battle Squadron earlier in the day, effectively losing them as an integral component for much of this opening action against the five ships commanded by Hipper. Beatty's withdrawal at the sight of the High Seas Fleet, which the British had not known were in the open sea, would reverse the course of the battle by drawing the German fleet in pursuit towards the British Grand Fleet. Between 18:30, when the sun was lowering on the western horizon, back-lighting the German forces, and nightfall at about 20:30, the two fleets – totalling 250 ships between them – directly engaged twice.

Fourteen British and eleven German ships sank, with great loss of life. After sunset, and throughout the night, Jellicoe manoeuvred to cut the Germans off from their base, hoping to continue the battle the next morning, but under the cover of darkness Scheer broke through the British light forces forming the rearguard of the Grand Fleet and returned to port.

Both sides claimed victory. The British lost more ships and twice as many sailors but succeeded in containing the German fleet. However, the British press criticised the Grand Fleet's failure to force a decisive outcome, while Scheer's plan of destroying a substantial portion of the British fleet also failed. Finally, the British strategy of denying Germany access to both the United Kingdom and the Atlantic did succeed, which was the British long-term goal. The Germans' "fleet in being" continued to pose a threat, requiring the British to keep their battleships concentrated in the North Sea, but the battle reinforced the German policy of avoiding all fleet-to-fleet contact. At the end of 1916, after further unsuccessful attempts to reduce the Royal Navy's numerical advantage, the German Navy accepted that its surface ships had been successfully contained, subsequently turning its efforts and resources to unrestricted submarine warfare and the destruction of Allied and neutral shipping, which - along with the Zimmermann Telegram - by April 1917 triggered the United States of America's declaration of war on Germany.

Subsequent reviews commissioned by the Royal Navy generated strong disagreement between supporters of Jellicoe and Beatty concerning the two admirals' performance in the battle. Debate over their performance and the significance of the battle continues to this day.

With 16 dreadnought-type battleships, compared with the Royal Navy's 28, the German High Seas Fleet stood little chance of winning a head-to-head clash. The Germans therefore adopted a divide-and-conquer strategy. They would stage raids into the North Sea and bombard the English coast, with the aim of luring out small British squadrons and pickets, which could then be destroyed by superior forces or submarines.

In January 1916, Admiral von Pohl, commander of the German fleet, fell ill. He was replaced by Scheer, who believed that the fleet had been used too defensively, had better ships and men than the British, and ought to take the war to them. According to Scheer, the German naval strategy should be:

On 25 April 1916, a decision was made by the German admiralty to halt indiscriminate attacks by submarine on merchant shipping. This followed protests from neutral countries, notably the United States, that their nationals had been the victims of attacks. Germany agreed that future attacks would only take place in accord with internationally agreed prize rules, which required an attacker to give a warning and allow the crews of vessels time to escape, and not to attack neutral vessels at all. Scheer believed that it would not be possible to continue attacks on these terms, which took away the advantage of secret approach by submarines and left them vulnerable to even relatively small guns on the target ships. Instead, he set about deploying the submarine fleet against military vessels.

It was hoped that, following a successful German submarine attack, fast British escorts, such as destroyers, would be tied down by anti-submarine operations. If the Germans could catch the British in the expected locations, good prospects were thought to exist of at least partially redressing the balance of forces between the fleets. "After the British sortied in response to the raiding attack force", the Royal Navy's centuries-old instincts for aggressive action could be exploited to draw its weakened units towards the main German fleet under Scheer. The hope was that Scheer would thus be able to ambush a section of the British fleet and destroy it.

A plan was devised to station submarines offshore from British naval bases, and then stage some action that would draw out the British ships to the waiting submarines. The battlecruiser had been damaged in a previous engagement, but was due to be repaired by mid May, so an operation was scheduled for 17 May 1916. At the start of May, difficulties with condensers were discovered on ships of the third battleship squadron, so the operation was put back to 23 May. Ten submarines—, , , , , , , , , and —were given orders first to patrol in the central North Sea between 17 and 22 May, and then to take up waiting positions. "U-43" and "U-44" were stationed in the Pentland Firth, which the Grand Fleet was likely to cross leaving Scapa Flow, while the remainder proceeded to the Firth of Forth, awaiting battlecruisers departing Rosyth. Each boat had an allocated area, within which it could move around as necessary to avoid detection, but was instructed to keep within it. During the initial North Sea patrol the boats were instructed to sail only north–south so that any enemy who chanced to encounter one would believe it was departing or returning from operations on the west coast (which required them to pass around the north of Britain). Once at their final positions, the boats were under strict orders to avoid premature detection that might give away the operation. It was arranged that a coded signal would be transmitted to alert the submarines exactly when the operation commenced: "Take into account the enemy's forces may be putting to sea".

Additionally, "UB-27" was sent out on 20 May with instructions to work its way into the Firth of Forth past May Island. "U-46" was ordered to patrol the coast of Sunderland, which had been chosen for the diversionary attack, but because of engine problems it was unable to leave port and "U-47" was diverted to this task. On 13 May, "U-72" was sent to lay mines in the Firth of Forth; on the 23rd, "U-74" departed to lay mines in the Moray Firth; and on the 24th, "U-75" was dispatched similarly west of the Orkney Islands. "UB-21" and "UB-22" were sent to patrol the Humber, where (incorrect) reports had suggested the presence of British warships. "U-22", "U-46" and "U-67" were positioned north of Terschelling to protect against intervention by British light forces stationed at Harwich.

On 22 May 1916, it was discovered that "Seydlitz" was still not watertight after repairs and would not now be ready until the 29th. The ambush submarines were now on station and experiencing difficulties of their own: visibility near the coast was frequently poor due to fog, and sea conditions were either so calm the slightest ripple, as from the periscope, could give away their position, or so rough as to make it very hard to keep the vessel at a steady depth. The British had become aware of unusual submarine activity, and had begun counter patrols that forced the submarines out of position. "UB-27" passed Bell Rock on the night of 23 May on its way into the Firth of Forth as planned, but was halted by engine trouble. After repairs it continued to approach, following behind merchant vessels, and reached Largo Bay on 25 May. There the boat became entangled in nets that fouled one of the propellers, forcing it to abandon the operation and return home. "U-74" was detected by four armed trawlers on 27 May and sunk south-east of Peterhead. "U-75" laid its mines off the Orkney Islands, which, although they played no part in the battle, were responsible later for sinking the cruiser carrying Lord Kitchener (head of the army) on a mission to Russia on 5 June. "U-72" was forced to abandon its mission without laying any mines when an oil leak meant it was leaving a visible surface trail astern.

The Germans maintained a fleet of Zeppelins that they used for aerial reconnaissance and occasional bombing raids. The planned raid on Sunderland intended to use Zeppelins to watch out for the British fleet approaching from the north, which might otherwise surprise the raiders.

By 28 May, strong north-easterly winds meant that it would not be possible to send out the Zeppelins, so the raid again had to be postponed. The submarines could only stay on station until 1 June before their supplies would be exhausted and they had to return, so a decision had to be made quickly about the raid.

It was decided to use an alternative plan, abandoning the attack on Sunderland but instead sending a patrol of battlecruisers to the Skagerrak, where it was likely they would encounter merchant ships carrying British cargo and British cruiser patrols. It was felt this could be done without air support, because the action would now be much closer to Germany, relying instead on cruiser and torpedo boat patrols for reconnaissance.

Orders for the alternative plan were issued on 28 May, although it was still hoped that last-minute improvements in the weather would allow the original plan to go ahead. The German fleet assembled in the Jade River and at Wilhelmshaven and was instructed to raise steam and be ready for action from midnight on 28 May.

By 14:00 on 30 May, the wind was still too strong and the final decision was made to use the alternative plan. The coded signal "31 May G.G.2490" was transmitted to the ships of the fleet to inform them the Skagerrak attack would start on 31 May. The pre-arranged signal to the waiting submarines was transmitted throughout the day from the E-Dienst radio station at Brugge, and the U-boat tender "Arcona" anchored at Emden. Only two of the waiting submarines, and , received the order.

Unfortunately for the German plan, the British had obtained a copy of the main German codebook from the light cruiser , which had been boarded by the Russian Navy after the ship ran aground in Russian territorial waters in 1914. German naval radio communications could therefore often be quickly deciphered, and the British Admiralty usually knew about German activities.

The British Admiralty's Room 40 maintained direction finding and interception of German naval signals. It had intercepted and decrypted a German signal on 28 May that provided "ample evidence that the German fleet was stirring in the North Sea." Further signals were intercepted, and although they were not decrypted it was clear that a major operation was likely. At 11:00 on 30 May, Jellicoe was warned that the German fleet seemed prepared to sail the following morning. By 17:00, the Admiralty had intercepted the signal from Scheer, "31 May G.G.2490", making it clear something significant was imminent.

Not knowing the Germans' objective, Jellicoe and his staff decided to position the fleet to head off any attempt by the Germans to enter the North Atlantic, or the Baltic through the Skagerrak, by taking up a position off Norway where they could possibly cut off any German raid into the shipping lanes of the Atlantic, or prevent the Germans from heading into the Baltic. A position further west was unnecessary, as that area of the North Sea could be patrolled by air using blimps and scouting aircraft.
Consequently, Admiral Jellicoe led the sixteen dreadnought battleships of the 1st and 4th Battle Squadrons of the Grand Fleet and three battlecruisers of the 3rd Battlecruiser Squadron eastwards out of Scapa Flow at 22:30 on 30 May. He was to meet the 2nd Battle Squadron of eight dreadnought battleships commanded by Vice-Admiral Martyn Jerram coming from Cromarty. Hipper's raiding force did not leave the Outer Jade Roads until 01:00 on 31 May, heading west of Heligoland Island following a cleared channel through the minefields, heading north at . The main German fleet of sixteen dreadnought battleships of 1st and 3rd Battle Squadrons left the Jade at 02:30, being joined off Heligoland at 04:00 by the six pre-dreadnoughts of the 2nd Battle Squadron coming from the Elbe River. Beatty's faster force of six ships of the 1st and 2nd Battlecruiser Squadrons plus the 5th Battle Squadron of four fast battleships left the Firth of Forth on the next day; Jellicoe intended to rendezvous with him west of the mouth of the Skagerrak off the coast of Jutland and wait for the Germans to appear or for their intentions to become clear. The planned position would give him the widest range of responses to likely German moves.

The principle of concentration of force was fundamental to the fleet tactics of this time (as in earlier periods). Tactical doctrine called for a fleet approaching battle to be in a compact formation of parallel columns, allowing relatively easy manoeuvring, and giving shortened sight lines within the formation, which simplified the passing of the signals necessary for command and control.

A fleet formed in several short columns could change its heading faster than one formed in a single long column. Since most command signals were made with flags or signal lamps between ships, the flagship was usually placed at the head of the centre column so that its signals might be more easily seen by the many ships of the formation. Wireless telegraphy was in use, though security (radio direction finding), encryption, and the limitation of the radio sets made their extensive use more problematic. Command and control of such huge fleets remained difficult.

Thus, it might take a very long time for a signal from the flagship to be relayed to the entire formation. It was usually necessary for a signal to be confirmed by each ship before it could be relayed to other ships, and an order for a fleet movement would have to be received and acknowledged by every ship before it could be executed. In a large single-column formation, a signal could take 10 minutes or more to be passed from one end of the line to the other, whereas in a formation of parallel columns, visibility across the diagonals was often better (and always shorter) than in a single long column, and the diagonals gave signal "redundancy", increasing the probability that a message would be quickly seen and correctly interpreted.

However, before battle was joined the heavy units of the fleet would, if possible, deploy into a single column. To form the battle line in the correct orientation relative to the enemy, the commanding admiral had to know the enemy fleet's distance, bearing, heading, and speed. It was the task of the scouting forces, consisting primarily of battlecruisers and cruisers, to find the enemy and report this information in sufficient time, and, if possible, to deny the enemy's scouting forces the opportunity of obtaining the equivalent information.

Ideally, the battle line would cross the intended path of the enemy column so that the maximum number of guns could be brought to bear, while the enemy could fire only with the forward guns of the leading ships, a manoeuvre known as "crossing the T". Admiral Tōgō, commander of the Japanese battleship fleet, had achieved this against Admiral Zinovy Rozhestvensky's Russian battleships in 1905 at the Battle of Tsushima, with devastating results. Jellicoe achieved this twice in one hour against the High Seas Fleet at Jutland, but on both occasions, Scheer managed to turn away and disengage, thereby avoiding a decisive action. 

Within the existing technological limits, a trade-off had to be made between the weight and size of guns, the weight of armour protecting the ship, and the maximum speed. Battleships sacrificed speed for armour and heavy naval guns ( or larger). British battlecruisers sacrificed weight of armour for greater speed, while their German counterparts were armed with lighter guns and heavier armour. These weight savings allowed them to escape danger or catch other ships. Generally, the larger guns mounted on British ships allowed an engagement at greater range. In theory, a lightly armoured ship could stay out of range of a slower opponent while still scoring hits. The fast pace of development in the pre-war years meant that every few years, a new generation of ships rendered its predecessors obsolete. Thus, fairly young ships could still be obsolete compared to the newest ships, and fare badly in an engagement against them.

Admiral John Fisher, responsible for reconstruction of the British fleet in the pre-war period, favoured large guns, oil fuel, and speed. Admiral Tirpitz, responsible for the German fleet, favoured ship survivability and chose to sacrifice some gun size for improved armour. The German battlecruiser had belt armour equivalent in thickness—though not as comprehensive—to the British battleship , significantly better than on the British battlecruisers such as "Tiger". German ships had better internal subdivision and had fewer doors and other weak points in their bulkheads, but with the disadvantage that space for crew was greatly reduced. As they were designed only for sorties in the North Sea they did not need to be as habitable as the British vessels and their crews could live in barracks ashore when in harbour.

Warships of the period were armed with guns firing projectiles of varying weights, bearing high explosive warheads. The sum total of weight of all the projectiles fired by all the ship's broadside guns is referred to as "weight of broadside". At Jutland, the total of the British ships' weight of broadside was , while the German fleet's total was . This does not take into consideration the ability of some ships and their crews to fire more or less rapidly than others, which would increase or decrease amount of fire that one combatant was able to bring to bear on their opponent for any length of time.

Jellicoe's Grand Fleet was split into two sections. The dreadnought Battle Fleet, with which he sailed, formed the main force and was composed of 24 battleships and three battlecruisers. The battleships were formed into three squadrons of eight ships, further subdivided into divisions of four, each led by a flag officer. Accompanying them were eight armoured cruisers (classified by the Royal Navy since 1913 as "cruisers"), eight light cruisers, four scout cruisers, 51 destroyers, and one destroyer-minelayer.
The Grand Fleet sailed without three of its battleships: in refit at Invergordon, dry-docked at Rosyth and in refit at Devonport. The brand new was left behind; with only three weeks in service, her untrained crew was judged unready for battle.

British reconnaissance was provided by the Battlecruiser Fleet under David Beatty: six battlecruisers, four fast s, 14 light cruisers and 27 destroyers. Air scouting was provided by the attachment of the seaplane tender , one of the first aircraft carriers in history to participate in a naval engagement.

The German High Seas Fleet under Scheer was also split into a main force and a separate reconnaissance force. Scheer's main battle fleet was composed of 16 battleships and six pre-dreadnought battleships arranged in an identical manner to the British. With them were six light cruisers and 31 torpedo-boats, (the latter being roughly equivalent to a British destroyer).

The German scouting force, commanded by Franz Hipper, consisted of five battlecruisers, five light cruisers and 30 torpedo-boats. The Germans had no equivalent to "Engadine" and no heavier-than-air aircraft to operate with the fleet but had the Imperial German Naval Airship Service's force of rigid airships available to patrol the North Sea.

All of the battleships and battlecruisers on both sides carried torpedoes of various sizes, as did the lighter craft. The British battleships carried three or four underwater torpedo tubes. The battlecruisers carried from two to five. All were either 18-inch or 21-inch diameter. The German battleships carried five or six underwater torpedo tubes in three sizes from 18 to 21 inch and the battlecruisers carried four or five tubes.

The German battle fleet was hampered by the slow speed and relatively poor armament of the six pre-dreadnoughts of II Squadron, which limited maximum fleet speed to , compared to maximum British fleet speed of . On the British side, the eight armoured cruisers were deficient in both speed and armour protection. Both of these obsolete squadrons were notably vulnerable to attacks by more modern enemy ships.

The route of the British battlecruiser fleet took it through the patrol sector allocated to "U-32". After receiving the order to commence the operation, the U-boat moved to a position east of May Island at dawn on 31 May. At 03:40, it sighted the cruisers and leaving the Forth at . It launched one torpedo at the leading cruiser at a range of , but its periscope jammed 'up', giving away the position of the submarine as it manoeuvred to fire a second. The lead cruiser turned away to dodge the torpedo, while the second turned towards the submarine, attempting to ram. "U-32" crash dived, and on raising its periscope at 04:10 saw two battlecruisers (the 2nd Battlecruiser Squadron) heading south-east. They were too far away to attack, but "Kapitänleutnant" von Spiegel reported the sighting of two battleships and two cruisers to Germany.

"U-66" was also supposed to be patrolling off the Firth of Forth but had been forced north to a position off Peterhead by patrolling British vessels. This now brought it into contact with the 2nd Battle Squadron, coming from the Moray Firth. At 05:00, it had to crash dive when the cruiser appeared from the mist heading toward it. It was followed by another cruiser, , and eight battleships. "U-66" got within of the battleships preparing to fire, but was forced to dive by an approaching destroyer and missed the opportunity. At 06:35, it reported eight battleships and cruisers heading north.

The courses reported by both submarines were incorrect, because they reflected one leg of a zigzag being used by British ships to avoid submarines. Taken with a wireless intercept of more ships leaving Scapa Flow earlier in the night, they created the impression in the German High Command that the British fleet, whatever it was doing, was split into separate sections moving apart, which was precisely as the Germans wished to meet it.

Jellicoe's ships proceeded to their rendezvous undamaged and undiscovered. However, he was now misled by an Admiralty intelligence report advising that the German main battle fleet was still in port. The Director of Operations Division, Rear Admiral Thomas Jackson, had asked the intelligence division, Room 40, for the current location of German call sign DK, used by Admiral Scheer. They had replied that it was currently transmitting from Wilhelmshaven. It was known to the intelligence staff that Scheer deliberately used a different call sign when at sea, but no one asked for this information or explained the reason behind the query – to locate the German fleet.

The German battlecruisers cleared the minefields surrounding the Amrum swept channel by 09:00. They then proceeded north-west, passing west of the Horn's Reef lightship heading for the Little Fisher Bank at the mouth of the Skagerrak. The High Seas Fleet followed some behind. The battlecruisers were in line ahead, with the four cruisers of the II scouting group plus supporting torpedo boats ranged in an arc ahead and to either side. The IX torpedo boat flotilla formed close support immediately surrounding the battlecruisers. The High Seas Fleet similarly adopted a line-ahead formation, with close screening by torpedo boats to either side and a further screen of five cruisers surrounding the column away. The wind had finally moderated so that Zeppelins could be used, and by 11:30 five had been sent out: "L14" to the Skagerrak, "L23" east of Noss Head in the Pentland Firth, "L21" off Peterhead, "L9" off Sunderland, and "L16" east of Flamborough Head. Visibility, however, was still bad, with clouds down to .

By around 14:00, Beatty's ships were proceeding eastward at roughly the same latitude as Hipper's squadron, which was heading north. Had the courses remained unchanged, Beatty would have passed between the two German fleets, south of the battlecruisers and north of the High Seas Fleet at around 16:30, possibly trapping his ships just as the German plan envisioned. His orders were to stop his scouting patrol when he reached a point east of Britain and then turn north to meet Jellicoe, which he did at this time. Beatty's ships were divided into three columns, with the two battlecruiser squadrons leading in parallel lines apart. The 5th Battle Squadron was stationed to the north-west, on the side furthest away from any expected enemy contact, while a screen of cruisers and destroyers was spread south-east of the battlecruisers. After the turn, the 5th Battle Squadron was now leading the British ships in the westernmost column, and Beatty's squadron was centre and rearmost, with the 2nd BCS to the west.

At 14:20 on 31 May, despite heavy haze and scuds of fog giving poor visibility, scouts from Beatty's force reported enemy ships to the south-east; the British light units, investigating a neutral Danish steamer ("N J Fjord"), which was stopped between the two fleets, had found two German destroyers engaged on the same mission ( and ). The first shots of the battle were fired at 14:28 when and of the British 1st Light Cruiser Squadron opened on the German torpedo boats, which withdrew toward their approaching light cruisers. At 14:36, the Germans scored the first hit of the battle when , of Rear-Admiral Friedrich Boedicker's Scouting Group II, hit her British counterpart "Galatea" at extreme range.

Beatty began to move his battlecruisers and supporting forces south-eastwards and then east to cut the German ships off from their base and ordered "Engadine" to launch a seaplane to try to get more information about the size and location of the German forces. This was the first time in history that a carrier-based aeroplane was used for reconnaissance in naval combat. "Engadine"s aircraft did locate and report some German light cruisers just before 15:30 and came under anti-aircraft gunfire but attempts to relay reports from the aeroplane failed.

Unfortunately for Beatty, his initial course changes at 14:32 were not received by Sir Hugh Evan-Thomas's 5th Battle Squadron (the distance being too great to read his flags), because the battlecruiser —the last ship in his column—was no longer in a position where she could relay signals by searchlight to Evan-Thomas, as she had previously been ordered to do. Whereas before the north turn, "Tiger" had been the closest ship to Evan-Thomas, she was now further away than Beatty in "Lion". Matters were aggravated because Evan-Thomas had not been briefed regarding standing orders within Beatty's squadron, as his squadron normally operated with the Grand Fleet. Fleet ships were expected to obey movement orders precisely and not deviate from them. Beatty's standing instructions expected his officers to use their initiative and keep station with the flagship. As a result, the four s—which were the fastest and most heavily armed in the world at that time—remained on the previous course for several minutes, ending up behind rather than five. Beatty also had the opportunity during the previous hours to concentrate his forces, and no reason not to do so, whereas he steamed ahead at full speed, faster than the battleships could manage. Dividing the force had serious consequences for the British, costing them what would have been an overwhelming advantage in ships and firepower during the first half-hour of the coming battle.

With visibility favouring the Germans, Hipper's battlecruisers at 15:22, steaming approximately north-west, sighted Beatty's squadron at a range of about , while Beatty's forces did not identify Hipper's battlecruisers until 15:30. (position 1 on map). At 15:45, Hipper turned south-east to lead Beatty toward Scheer, who was south-east with the main force of the High Seas Fleet.

Beatty's conduct during the next 15 minutes has received a great deal of criticism, as his ships out-ranged and outnumbered the German squadron, yet he held his fire for over 10 minutes with the German ships in range. He also failed to use the time available to rearrange his battlecruisers into a fighting formation, with the result that they were still manoeuvring when the battle started.

At 15:48, with the opposing forces roughly parallel at , with the British to the south-west of the Germans (i.e., on the right side), Hipper opened fire, followed by the British ships as their guns came to bear upon targets (position 2). Thus began the opening phase of the battlecruiser action, known as the "Run to the South", in which the British chased the Germans, and Hipper intentionally led Beatty toward Scheer. During the first minutes of the ensuing battle, all the British ships except "Princess Royal" fired far over their German opponents, due to adverse visibility conditions, before finally getting the range. Only "Lion" and "Princess Royal" had settled into formation, so the other four ships were hampered in aiming by their own turning. Beatty was to windward of Hipper, and therefore funnel and gun smoke from his own ships tended to obscure his targets, while Hipper's smoke blew clear. Also, the eastern sky was overcast and the grey German ships were indistinct and difficult to range.

Beatty had ordered his ships to engage in a line, one British ship engaging with one German and his flagship doubling on the German flagship . However, due to another mistake with signalling by flag, and possibly because "Queen Mary" and "Tiger" were unable to see the German lead ship because of smoke, the second German ship, "Derfflinger", was left un-engaged and free to fire without disruption. drew fire from two of Beatty's battlecruisers, but still fired with great accuracy during this time, hitting "Tiger" 9 times in the first 12 minutes. The Germans drew first blood. Aided by superior visibility, Hipper's five battlecruisers quickly registered hits on three of the six British battlecruisers. Seven minutes passed before the British managed to score their first hit.

The first near-kill of the Run to the South occurred at 16:00, when a shell from "Lützow" wrecked the "Q" turret amidships on Beatty's flagship "Lion". Dozens of crewmen were instantly killed, but far larger destruction was averted when the mortally wounded turret commander – Major Francis Harvey of the Royal Marines – promptly ordered the magazine doors shut and the magazine flooded. This prevented a magazine explosion at 16:28, when a flash fire ignited ready cordite charges beneath the turret and killed everyone in the chambers outside "Q" magazine. "Lion" was saved. was not so lucky; at 16:02, just 14 minutes into the gunnery exchange, she was hit aft by three shells from , causing damage sufficient to knock her out of line and detonating "X" magazine aft. Soon after, despite the near-maximum range, "Von der Tann" put another shell on "Indefatigable"s "A" turret forward. The plunging shells probably pierced the thin upper armour, and seconds later "Indefatigable" was ripped apart by another magazine explosion, sinking immediately with her crew of 1,019 officers and men, leaving only two survivors. (position 3).

Hipper's position deteriorated somewhat by 16:15 as the 5th Battle Squadron finally came into range, so that he had to contend with gunfire from the four battleships astern as well as Beatty's five remaining battlecruisers to starboard. But he knew his baiting mission was close to completion, as his force was rapidly closing with Scheer's main body. At 16:08, the lead battleship of the 5th Battle Squadron, , caught up with Hipper and opened fire at extreme range, scoring a hit on "Von der Tann" within 60 seconds. Still, it was 16:15 before all the battleships of the 5th were able to fully engage at long range.

At 16:25, the battlecruiser action intensified again when was hit by what may have been a combined salvo from "Derfflinger" and ; she disintegrated when both forward magazines exploded, sinking with all but nine of her 1,275 man crew lost. (position 4). Commander von Hase, the first gunnery officer aboard "Derfflingler", noted:

During the Run to the South, from 15:48 to 16:54, the German battlecruisers made an estimated total of forty-two hits on the British battlecruisers (nine on "Lion", six on "Princess Royal", seven on "Queen Mary", 14 on "Tiger", one on "New Zealand", five on "Indefatigable"), and two more on the battleship "Barham", compared with only eleven hits by the British battlecruisers (four on "Lützow", four on "Seydlitz", two on "Moltke", one on "von der Tann"), and six hits by the battleships (one on "Seydlitz", four on "Moltke", one on "von der Tann").

Shortly after 16:26, a salvo struck on or around , which was obscured by spray and smoke from shell bursts. A signalman promptly leapt on to the bridge of "Lion" and announced ""Princess Royal"s blown up, Sir." Beatty famously turned to his flag captain, saying "Chatfield, there seems to be something wrong with our bloody ships today." (In popular legend, Beatty also immediately ordered his ships to "turn two points to port", i.e., two points nearer the enemy, but there is no official record of any such command or course change.) "Princess Royal", as it turned out, was still afloat after the spray cleared.

At 16:30, Scheer's leading battleships sighted the distant battlecruiser action; soon after, of Beatty's 2nd Light Cruiser Squadron led by Commodore William Goodenough sighted the main body of Scheer's High Seas Fleet, dodging numerous heavy-calibre salvos to report in detail the German strength: 16 dreadnoughts with six older battleships. This was the first news that Beatty and Jellicoe had that Scheer and his battle fleet were even at sea. Simultaneously, an all-out destroyer action raged in the space between the opposing battlecruiser forces, as British and German destroyers fought with each other and attempted to torpedo the larger enemy ships. Each side fired many torpedoes, but both battlecruiser forces turned away from the attacks and all escaped harm except "Seydlitz", which was hit forward at 16:57 by a torpedo fired by the British destroyer . Though taking on water, "Seydlitz" maintained speed. The destroyer , under the command of Captain Barry Bingham, led the British attacks. The British disabled the German torpedo boat , which the Germans soon abandoned and sank, and "Petard" then torpedoed and sank , her second score of the day. and rescued the crews of their sunken sister ships. But "Nestor" and another British destroyer – – were immobilised by shell hits, and were later sunk by Scheer's passing dreadnoughts. Bingham was rescued, and awarded the Victoria Cross for his leadership in the destroyer action.

As soon as he himself sighted the vanguard of Scheer's distant battleship line away, at 16:40, Beatty turned his battlecruiser force 180°, heading north to draw the Germans toward Jellicoe. (position 5). Beatty's withdrawal toward Jellicoe is called the "Run to the North", in which the tables turned and the Germans chased the British. Because Beatty once again failed to signal his intentions adequately, the battleships of the 5th Battle Squadron – which were too far behind to read his flags – found themselves passing the battlecruisers on an opposing course and heading directly toward the approaching main body of the High Seas Fleet. At 16:48, at extreme range, Scheer's leading battleships opened fire.

Meanwhile, at 16:47, having received Goodenough's signal and knowing that Beatty was now leading the German battle fleet north to him, Jellicoe signalled to his own forces that the fleet action they had waited so long for was finally imminent; at 16:51, by radio, he informed the Admiralty so in London.

The difficulties of the 5th Battle Squadron were compounded when Beatty gave the order to Evan-Thomas to "turn in succession" (rather than "turn together") at 16:48 as the battleships passed him. Evan-Thomas acknowledged the signal, but Lieutenant-Commander Ralph Seymour, Beatty's flag lieutenant, aggravated the situation when he did not haul down the flags (to execute the signal) for some minutes. At 16:55, when the 5BS had moved within range of the enemy battleships, Evan-Thomas issued his own flag command warning his squadron to expect sudden manoeuvres and to follow his lead, before starting to turn on his own initiative. The order to turn in succession would have resulted in all four ships turning in the same patch of sea as they reached it one by one, giving the High Seas Fleet repeated opportunity with ample time to find the proper range. However, the captain of the trailing ship () turned early, mitigating the adverse results.

For the next hour, the 5th Battle Squadron acted as Beatty's rearguard, drawing fire from all the German ships within range, while by 17:10 Beatty had deliberately eased his own squadron out of range of Hipper's now-superior battlecruiser force. Since visibility and firepower now favoured the Germans, there was no incentive for Beatty to risk further battlecruiser losses when his own gunnery could not be effective. Illustrating the imbalance, Beatty's battlecruisers did not score any hits on the Germans in this phase until 17:45, but they had rapidly received five more before he opened the range (four on "Lion", of which three were by "Lützow", and one on "Tiger" by "Seydlitz"). Now the only targets the Germans could reach, the ships of the 5th Battle Squadron, received simultaneous fire from Hipper's battlecruisers to the east (which HMS "Barham" and engaged) and Scheer's leading battleships to the south-east (which and engaged). Three took hits: "Barham" (four by "Derfflinger"), "Warspite" (two by "Seydlitz"), and "Malaya" (seven by the German battleships). Only "Valiant" was unscathed.

The four battleships were far better suited to take this sort of pounding than the battlecruisers, and none were lost, though "Malaya" suffered heavy damage, an ammunition fire, and heavy crew casualties. At the same time, the fire of the four British ships was accurate and effective. As the two British squadrons headed north at top speed, eagerly chased by the entire German fleet, the 5th Battle Squadron scored 13 hits on the enemy battlecruisers (four on "Lützow", three on "Derfflinger", six on "Seydlitz") and five on battleships (although only one, on , did any serious damage). (position 6).

Jellicoe was now aware that full fleet engagement was nearing, but had insufficient information on the position and course of the Germans. To assist Beatty, early in the battle at about 16:05, Jellicoe had ordered Rear-Admiral Horace Hood's 3rd Battlecruiser Squadron to speed ahead to find and support Beatty's force, and Hood was now racing SSE well in advance of Jellicoe's northern force. Rear-Admiral Arbuthnot's 1st Cruiser Squadron patrolled the van of Jellicoe's main battleship force as it advanced steadily to the south-east.

At 17:33, the armoured cruiser of Arbuthnot's squadron, on the far southwest flank of Jellicoe's force, came within view of , which was about ahead of Beatty with the 3rd Light Cruiser Squadron, establishing the first visual link between the converging bodies of the Grand Fleet. At 17:38, the scout cruiser , screening Hood's oncoming battlecruisers, was intercepted by the van of the German scouting forces under Rear-Admiral Boedicker.

Heavily outnumbered by Boedicker's four light cruisers, "Chester" was pounded before being relieved by Hood's heavy units, which swung westward for that purpose. Hood's flagship disabled the light cruiser shortly after 17:56. "Wiesbaden" became a sitting target for most of the British fleet during the next hour, but remained afloat and fired some torpedoes at the passing enemy battleships from long range. Meanwhile, Boedicker's other ships turned toward Hipper and Scheer in the mistaken belief that Hood was leading a larger force of British capital ships from the north and east. A chaotic destroyer action in mist and smoke ensued as German torpedo boats attempted to blunt the arrival of this new formation, but Hood's battlecruisers dodged all the torpedoes fired at them. In this action, after leading a torpedo counter-attack, the British destroyer was disabled, but continued to return fire at numerous passing enemy ships for the next hour.

In the meantime, Beatty and Evan-Thomas had resumed their engagement with Hipper's battlecruisers, this time with the visual conditions to their advantage. With several of his ships damaged, Hipper turned back toward Scheer at around 18:00, just as Beatty's flagship "Lion" was finally sighted from Jellicoe's flagship "Iron Duke". Jellicoe twice demanded the latest position of the German battlefleet from Beatty, who could not see the German battleships and failed to respond to the question until 18:14. Meanwhile, Jellicoe received confused sighting reports of varying accuracy and limited usefulness from light cruisers and battleships on the starboard (southern) flank of his force.

Jellicoe was in a worrying position. He needed to know the location of the German fleet to judge when and how to deploy his battleships from their cruising formation (six columns of four ships each) into a single battle line. The deployment could be on either the westernmost or the easternmost column, and had to be carried out before the Germans arrived; but early deployment could mean losing any chance of a decisive encounter. Deploying to the west would bring his fleet closer to Scheer, gaining valuable time as dusk approached, but the Germans might arrive before the manoeuvre was complete. Deploying to the east would take the force away from Scheer, but Jellicoe's ships might be able to cross the "T", and visibility would strongly favour British gunnery – Scheer's forces would be silhouetted against the setting sun to the west, while the Grand Fleet would be indistinct against the dark skies to the north and east, and would be hidden by reflection of the low sunlight off intervening haze and smoke. Deployment would take twenty irreplaceable minutes, and the fleets were closing at full speed. In one of the most critical and difficult tactical command decisions of the entire war, Jellicoe ordered deployment to the east at 18:15.

Meanwhile, Hipper had rejoined Scheer, and the combined High Seas Fleet was heading north, directly toward Jellicoe. Scheer had no indication that Jellicoe was at sea, let alone that he was bearing down from the north-west, and was distracted by the intervention of Hood's ships to his north and east. Beatty's four surviving battlecruisers were now crossing the van of the British dreadnoughts to join Hood's three battlecruisers; at this time, Arbuthnot's flagship, the armoured cruiser , and her squadron-mate both charged across Beatty's bows, and "Lion" narrowly avoided a collision with "Warrior". Nearby, numerous British light cruisers and destroyers on the south-western flank of the deploying battleships were also crossing each other's courses in attempts to reach their proper stations, often barely escaping collisions, and under fire from some of the approaching German ships. This period of peril and heavy traffic attending the merger and deployment of the British forces later became known as "Windy Corner".

Arbuthnot was attracted by the drifting hull of the crippled "Wiesbaden". With "Warrior", "Defence" closed in for the kill, only to blunder right into the gun sights of Hipper's and Scheer's oncoming capital ships. "Defence" was deluged by heavy-calibre gunfire from many German battleships, which detonated her magazines in a spectacular explosion viewed by most of the deploying Grand Fleet. She sank with all hands (903 officers and men). "Warrior" was also hit badly, but was spared destruction by a mishap to the nearby battleship "Warspite". "Warspite" had her steering gear overheat and jam under heavy load at high speed as the 5th Battle Squadron made a turn to the north at 18:19. Steaming at top speed in wide circles, "Warspite" appeared as a juicy target to the German dreadnoughts and took 13 hits, inadvertently drawing fire from the hapless "Warrior". "Warspite" was brought back under control and survived the onslaught, but was badly damaged, had to reduce speed, and withdrew northward; later (at 21:07), she was ordered back to port by Evan-Thomas. "Warspite" went on to a long and illustrious career, serving also in World War II. "Warrior", on the other hand, was abandoned and sank the next day after her crew was taken off at 08:25 on 1 June by "Engadine", which towed the sinking armoured cruiser during the night.
As "Defence" sank and "Warspite" circled, at about 18:19, Hipper moved within range of Hood's 3rd Battlecruiser Squadron, but was still also within range of Beatty's ships. At first, visibility favoured the British: hit "Derfflinger" three times and "Seydlitz" once, while "Lützow" quickly took 10 hits from "Lion", and "Invincible", including two below-waterline hits forward by "Invincible" that would ultimately doom Hipper's flagship. But at 18:30, "Invincible" abruptly appeared as a clear target before "Lützow" and "Derfflinger". The two German ships then fired three salvoes each at "Invincible", and sank her in 90 seconds. A shell from the third salvo struck "Invincible"s Q-turret amidships, detonating the magazines below and causing her to blow up and sink. All but six of her crew of 1,032 officers and men, including Rear-Admiral Hood, were killed. Of the remaining British battlecruisers, only "Princess Royal" received heavy-calibre hits at this time (two by the battleship "Markgraf"). "Lützow", flooding forward and unable to communicate by radio, was now out of action and began to attempt to withdraw; therefore Hipper left his flagship and transferred to the torpedo boat , hoping to board one of the other battlecruisers later.

By 18:30, the main battle fleet action was joined for the first time, with Jellicoe effectively "crossing Scheer's T". The officers on the lead German battleships, and Scheer himself, were taken completely by surprise when they emerged from drifting clouds of smoky mist to suddenly find themselves facing the massed firepower of the entire Grand Fleet main battle line, which they did not know was even at sea. Jellicoe's flagship "Iron Duke" quickly scored seven hits on the lead German dreadnought, but in this brief exchange, which lasted only minutes, as few as 10 of the Grand Fleet's 24 dreadnoughts actually opened fire. The Germans were hampered by poor visibility, in addition to being in an unfavourable tactical position, just as Jellicoe had intended. Realising he was heading into a death trap, Scheer ordered his fleet to turn and disengage at 18:33. Under a pall of smoke and mist, Scheer's forces succeeded in disengaging by an expertly executed 180° turn in unison ("battle about turn to starboard", German "Gefechtskehrtwendung nach Steuerbord"), which was a well-practised emergency manoeuvre of the High Seas Fleet. Scheer declared:

Conscious of the risks to his capital ships posed by torpedoes, Jellicoe did not chase directly but headed south, determined to keep the High Seas Fleet west of him. Starting at 18:40, battleships at the rear of Jellicoe's line were in fact sighting and avoiding torpedoes, and at 18:54 was hit by a torpedo (probably from the disabled "Wiesbaden"), which reduced her speed to . Meanwhile, Scheer, knowing that it was not yet dark enough to escape and that his fleet would suffer terribly in a stern chase, doubled back to the east at 18:55. In his memoirs he wrote, "the manoeuvre would be bound to surprise the enemy, to upset his plans for the rest of the day, and if the blow fell heavily it would facilitate the breaking loose at night." But the turn to the east took his ships, again, directly towards Jellicoe's fully deployed battle line.

Simultaneously, the disabled British destroyer HMS "Shark" fought desperately against a group of four German torpedo boats and disabled with gunfire, but was eventually torpedoed and sunk at 19:02 by the German destroyer . "Shark"s Captain Loftus Jones was awarded the Victoria Cross for his heroism in continuing to fight against all odds.

Commodore Goodenough's 2nd Light Cruiser Squadron dodged the fire of German battleships for a second time to re-establish contact with the High Seas Fleet shortly after 19:00. By 19:15, Jellicoe had crossed Scheer's "T" again. This time his arc of fire was tighter and deadlier, causing severe damage to the German battleships, particularly Rear-Admiral Behncke's leading 3rd Squadron (SMS "König", , "Markgraf", and all being hit, along with of the 1st Squadron), while on the British side, only the battleship was hit (twice, by but with little damage done).

At 19:17, for the second time in less than an hour, Scheer turned his outnumbered and out-gunned fleet to the west using the "battle about turn" (German: "Gefechtskehrtwendung"), but this time it was executed only with difficulty, as the High Seas Fleet's lead squadrons began to lose formation under concentrated gunfire. To deter a British chase, Scheer ordered a major torpedo attack by his destroyers and a potentially sacrificial charge by Scouting Group I's four remaining battlecruisers. Hipper was still aboard the torpedo boat "G39" and was unable to command his squadron for this attack. Therefore, , under Captain Hartog, led the already badly damaged German battlecruisers directly into "the greatest concentration of naval gunfire any fleet commander had ever faced", at ranges down to .

In what became known as the "death ride", all the battlecruisers except were hit and further damaged, as 18 of the British battleships fired at them simultaneously. "Derfflinger" had two main gun turrets destroyed. The crews of Scouting Group I suffered heavy casualties, but survived the pounding and veered away with the other battlecruisers once Scheer was out of trouble and the German destroyers were moving in to attack. In this brief but intense portion of the engagement, from about 19:05 to about 19:30, the Germans sustained a total of 37 heavy hits while inflicting only two; "Derfflinger" alone received 14.

While his battlecruisers drew the fire of the British fleet, Scheer slipped away, laying smoke screens. Meanwhile, from about 19:16 to about 19:40, the British battleships were also engaging Scheer's torpedo boats, which executed several waves of torpedo attacks to cover his withdrawal. Jellicoe's ships turned away from the attacks and successfully evaded all 31 of the torpedoes launched at them – though, in several cases, only barely – and sank the German destroyer , attributed to a salvo from "Iron Duke". British light forces also sank "V48", which had previously been disabled by HMS "Shark". This action, and the turn away, cost the British critical time and range in the last hour of daylight – as Scheer intended, allowing him to get his heavy ships out of immediate danger.

The last major exchanges between capital ships in this battle took place just after sunset, from about 20:19 to about 20:35, as the surviving British battlecruisers caught up with their German counterparts, which were briefly relieved by Rear-Admiral Mauve's obsolete pre-dreadnoughts (the German 2nd Squadron). The British received one heavy hit on "Princess Royal" but scored five more on "Seydlitz" and three on other German ships. As twilight faded to night and exchanged a few final shots with , neither side could have imagined that the only encounter between British and German dreadnoughts in the entire war was already concluded. 

At 21:00, Jellicoe, conscious of the Grand Fleet's deficiencies in night fighting, decided to try to avoid a major engagement until early dawn. He placed a screen of cruisers and destroyers behind his battle fleet to patrol the rear as he headed south to guard Scheer's expected escape route. In reality, Scheer opted to cross Jellicoe's wake and escape via Horns Reef. Luckily for Scheer, most of the light forces in Jellicoe's rearguard failed to report the seven separate encounters with the German fleet during the night; the very few radio reports that were sent to the British flagship were never received, possibly because the Germans were jamming British frequencies. Many of the destroyers failed to make the most of their opportunities to attack discovered ships, despite Jellicoe's expectations that the destroyer forces would, if necessary, be able to block the path of the German fleet.

Jellicoe and his commanders did not understand that the furious gunfire and explosions to the north (seen and heard for hours by all the British battleships) indicated that the German heavy ships were breaking through the screen astern of the British fleet. Instead, it was believed that the fighting was the result of night attacks by German destroyers. The most powerful British ships of all (the 15-inch-guns of the 5th Battle Squadron) directly observed German battleships crossing astern of them in action with British light forces, at ranges of or less, and gunners on HMS "Malaya" made ready to fire, but her captain declined, deferring to the authority of Rear-Admiral Evan-Thomas – and neither commander reported the sightings to Jellicoe, assuming that he could see for himself and that revealing the fleet's position by radio signals or gunfire was unwise.

While the nature of Scheer's escape, and Jellicoe's inaction, indicate the overall German superiority in night fighting, the results of the night action were no more clear-cut than were those of the battle as a whole. In the first of many surprise encounters by darkened ships at point-blank range, "Southampton", Commodore Goodenough's flagship, which had scouted so proficiently, was heavily damaged in action with a German Scouting Group composed of light cruisers, but managed to torpedo , which went down at 22:23 with all hands (320 officers and men).
From 23:20 to approximately 02:15, several British destroyer flotillas launched torpedo attacks on the German battle fleet in a series of violent and chaotic engagements at extremely short range (often under ). At the cost of five destroyers sunk and some others damaged, they managed to torpedo the light cruiser , which sank several hours later, and the pre-dreadnought , which blew up and sank with all hands (839 officers and men) at 03:10 during the last wave of attacks before dawn. Three of the British destroyers collided in the chaos, and the German battleship rammed the British destroyer , blowing away most of the British ship's superstructure merely with the muzzle blast of its big guns, which could not be aimed low enough to hit the ship. "Nassau" was left with a hole in her side, reducing her maximum speed to , while the removed plating was left lying on "Spitfire"s deck. "Spitfire" survived and made it back to port. Another German cruiser, , was accidentally rammed by the dreadnought and abandoned, sinking early the next day. Of the British destroyers, , , , and were lost during the night fighting.

Just after midnight on 1 June, and other German battleships sank of the ill-fated 1st Cruiser Squadron, which had blundered into the German battle line. Deployed as part of a screening force several miles ahead of the main force of the Grand Fleet, "Black Prince" had lost contact in the darkness and took a position near what she thought was the British line. The Germans soon identified the new addition to their line and opened fire. Overwhelmed by point-blank gunfire, "Black Prince" blew up, (857 officers and men – all hands – were lost), as her squadron leader "Defence" had done hours earlier. Lost in the darkness, the battlecruisers and had similar point-blank encounters with the British battle line and were recognised, but were spared the fate of "Black Prince" when the captains of the British ships, again, declined to open fire, reluctant to reveal their fleet's position.

At 01:45, the sinking battlecruiser "Lützow" – fatally damaged by "Invincible" during the main action – was torpedoed by the destroyer on orders of "Lützow"s Captain Viktor von Harder after the surviving crew of 1,150 transferred to destroyers that came alongside. At 02:15, the German torpedo boat suddenly had its bow blown off; "V2" and "V6" came alongside and took off the remaining crew, and the "V2" then sank the hulk. Since there was no enemy nearby, it was assumed that she had hit a mine or had been torpedoed by a submarine.

At 02:15, five British ships of the 13th Destroyer Flotilla under Captain James Uchtred Farie regrouped and headed south. At 02:25, they sighted the rear of the German line. inquired of the leader as to whether he thought they were British or German ships. Answering that he thought they were German, Farie then veered off to the east and away from the German line. All but "Moresby" in the rear followed, as through the gloom she sighted what she thought were four pre-dreadnought battleships away. She hoisted a flag signal indicating that the enemy was to the west and then closed to firing range, letting off a torpedo set for high running at 02:37, then veering off to rejoin her flotilla. The four pre-dreadnought battleships were in fact two pre-dreadnoughts, "Schleswig-Holstein" and , and the battlecruisers "Von der Tann" and "Derfflinger". "Von der Tann" sighted the torpedo and was forced to steer sharply to starboard to avoid it as it passed close to her bows. "Moresby" rejoined "Champion" convinced she had scored a hit.

Finally, at 05:20, as Scheer's fleet was safely on its way home, the battleship struck a British mine on her starboard side, killing one man and wounding ten, but was able to make port. "Seydlitz", critically damaged and very nearly sinking, barely survived the return voyage: after grounding and taking on even more water on the evening of 1 June, she had to be assisted stern first into port, where she dropped anchor at 07:30 on the morning of 2 June.

The Germans were helped in their escape by the failure of the British Admiralty in London to pass on seven critical radio intercepts obtained by naval intelligence indicating the true position, course and intentions of the High Seas Fleet during the night. One message was transmitted to Jellicoe at 23:15 that accurately reported the German fleet's course and speed as of 21:14. However, the erroneous signal from earlier in the day that reported the German fleet still in port, and an intelligence signal received at 22:45 giving another unlikely position for the German fleet, had reduced his confidence in intelligence reports. Had the other messages been forwarded, which confirmed the information received at 23:15, or had British ships reported accurately sightings and engagements with German destroyers, cruisers and battleships, then Jellicoe could have altered course to intercept Scheer at the Horns Reef. The unsent intercepted messages had been duly filed by the junior officer left on duty that night, who failed to appreciate their significance. By the time Jellicoe finally learned of Scheer's whereabouts at 04:15, the German fleet was too far away to catch and it was clear that the battle could no longer be resumed.

At midday on 2 June, German authorities released a press statement claiming a victory, including the destruction of a battleship, two battlecruisers, two armoured cruisers, a light cruiser, a submarine and several destroyers, for the loss of "Pommern" and "Wiesbaden". News that "Lützow", "Elbing" and "Rostock" had been scuttled was withheld, on the grounds this information would not be known to the enemy. The victory of the Skagerrak was celebrated in the press, children were given a holiday and the nation celebrated. The Kaiser announced a new chapter in world history. Post-war, the official German history hailed the battle as a victory and it continued to be celebrated until after World War II.

In Britain, the first official news came from German wireless broadcasts. Ships began to arrive in port, their crews sending messages to friends and relatives both of their survival and the loss of some 6,000 others. The authorities considered suppressing the news, but it had already spread widely. Some crews coming ashore found rumours had already reported them dead to relatives, while others were jeered for the defeat they had suffered. At 19:00 on 2 June, the Admiralty released a statement based on information from Jellicoe containing the bare news of losses on each side. The following day British newspapers reported a German victory. The "Daily Mirror" described the German Director of the Naval Department telling the "Reichstag": "The result of the fighting is a significant success for our forces against a much stronger adversary". The British population was shocked that the long anticipated battle had been a victory for Germany. On 3 June, the Admiralty issued a further statement expanding on German losses, and another the following day with exaggerated claims. However, on 7 June the German admission of the losses of "Lützow" and "Rostock" started to redress the sense of the battle as a loss. International perception of the battle began to change towards a qualified British victory, the German attempt to change the balance of power in the North Sea having been repulsed. In July, bad news from the "Somme campaign" swept concern over Jutland from the British consciousness.

At Jutland, the Germans, with a 99-strong fleet, sank of British ships, while a 151-strong British fleet sank of German ships. The British lost 6,094 seamen; the Germans 2,551. Several other ships were badly damaged, such as and .

As of the summer of 1916, the High Seas Fleet's strategy was to whittle away the numerical advantage of the Royal Navy by bringing its full strength to bear against isolated squadrons of enemy capital ships whilst declining to be drawn into a general fleet battle until it had achieved something resembling parity in heavy ships. In tactical terms, the High Seas Fleet had clearly inflicted significantly greater losses on the Grand Fleet than it had suffered itself at Jutland and the Germans never had any intention of attempting to hold the site of the battle, so some historians support the German claim of victory at Jutland.

However, Scheer seems to have quickly realised that further battles with a similar rate of attrition would exhaust the High Seas Fleet long before it reduced the Grand Fleet. Further, after the 19 August advance was nearly intercepted by the Grand Fleet, he no longer believed that it would be possible to trap a single squadron of Royal Navy warships without having the Grand Fleet intervene before he could return to port. Therefore, the High Seas Fleet abandoned its forays into the North Sea and turned its attention to the Baltic for most of 1917 whilst Scheer switched tactics against Britain to unrestricted submarine warfare in the Atlantic.

At a strategic level, the outcome has been the subject of a huge amount of literature with no clear consensus. The battle was widely viewed as indecisive in the immediate aftermath and this view remains influential.

Despite numerical superiority, the British had been disappointed in their hopes for a decisive victory comparable to Trafalgar and the objective of the influential strategic doctrines of Alfred Mahan. The High Seas Fleet survived as a fleet in being. Most of its losses were made good within a month – even "Seydlitz", the most badly damaged ship to survive the battle, was repaired by October and officially back in service by November. However, the Germans had failed in their objective of destroying a substantial portion of the British Fleet, and no progress had been made towards the goal of allowing the High Seas Fleet to operate in the Atlantic Ocean.

Subsequently, there has been considerable support for the view of Jutland as a strategic victory for the British. While the British had not destroyed the German fleet and had lost more ships than their enemy, the Germans had retreated to harbour; at the end of the battle the British were in command of the area.

The German fleet would only sortie into the North Sea thrice more, with a raid on 19 August, one in October 1916 and another in April 1918. All three were unopposed by capital ships and quickly aborted as neither side were prepared to take the risks of mines and submarines.

Apart from these three abortive operations the High Seas Fleet – unwilling to risk another encounter with the British fleet – confined its activities to the Baltic Sea for the remainder of the war. Jellicoe issued an order prohibiting the Grand Fleet from steaming south of the line of Horns Reef owing to the threat of mines and U-boats. A German naval expert, writing publicly about Jutland in November 1918, commented, "Our Fleet losses were severe. On 1 June 1916, it was clear to every thinking person that this battle must, and would be, the last one".

There is also significant support for viewing the battle as a German tactical victory, due to the much higher losses sustained by the British. The Germans declared a great victory immediately afterwards, while the British by contrast had only reported short and simple results. In response to public outrage, the First Lord of the Admiralty Arthur Balfour asked Winston Churchill to write a second report that was more positive and detailed.
At the end of the battle, the British had maintained their numerical superiority and had 23 dreadnoughts ready and four battlecruisers still able to fight, while the Germans had only 10 dreadnoughts. One month after the battle, the Grand Fleet was stronger than it had been before sailing to Jutland. "Warspite" was dry docked at Rosyth, returning to the fleet on 22 July, while "Malaya" was repaired in the floating dock at Invergordon, returning to duty on 11 July. "Barham" was docked for a month at Devonport before undergoing speed trials and returning to Scapa on 8 July. "Princess Royal" stayed initially at Rosyth but transferred to dry dock at Portsmouth before returning to duty at Rosyth 21 July. "Tiger" was dry docked at Rosyth and ready for service 2 July. "Queen Elizabeth", "Emperor of India" and , which had been undergoing maintenance at the time of the battle, returned to the fleet immediately, followed shortly after by "Resolution" and "Ramillies". "Lion" initially remained ready for sea duty despite the damaged turret, then underwent a month's repairs in July when Q turret was removed temporarily and replaced in September.

A third view, presented in a number of recent evaluations, is that Jutland, the last major fleet action between battleships, illustrated the irrelevance of battleship fleets following the development of the submarine, mine and torpedo. In this view, the most important consequence of Jutland was the decision of the Germans to engage in unrestricted submarine warfare. Although large numbers of battleships were constructed in the decades between the wars, it has been argued that this outcome reflected the social dominance among naval decision-makers of battleship advocates who constrained technological choices to fit traditional paradigms of fleet action. Battleships played a relatively minor role in World War II, in which the submarine and aircraft carrier emerged as the dominant offensive weapons of naval warfare.

The official British Admiralty examination of the Grand Fleet's performance recognised two main problems:

German armour-piercing shells were far more effective than the British ones, which often failed to penetrate heavy armour. The issue particularly concerned shells striking at oblique angles, which became increasingly the case at long range. Germany had adopted trinitrotoluene (TNT) as the explosive filler for artillery shells in 1902, while the United Kingdom was still using a picric acid mixture (Lyddite). The shock of impact of a shell against armour often prematurely detonated Lyddite in advance of fuze function while TNT detonation could be delayed until after the shell had penetrated and the fuze had functioned in the vulnerable area behind the armour plate.

The issue of poorly performing shells had been known to Jellicoe, who as Third Sea Lord from 1908 to 1910 had ordered new shells to be designed. However, the matter had not been followed through after his posting to sea and new shells had never been thoroughly tested. Beatty discovered the problem at a party aboard "Lion" a short time after the battle, when a Swedish Naval officer was present. He had recently visited Berlin, where the German navy had scoffed at how British shells had broken up on their ships' armour. The question of shell effectiveness had also been raised after the Battle of Dogger Bank, but no action had been taken. Hipper later commented, "It was nothing but the poor quality of their bursting charges which saved us from disaster."

Admiral Dreyer, writing later about the battle, during which he had been captain of the British flagship "Iron Duke", estimated that effective shells as later introduced would have led to the sinking of six more German capital ships, based upon the actual number of hits achieved in the battle. The system of testing shells, which remained in use up to 1944, meant that, statistically, a batch of shells of which 70% were faulty stood an even chance of being accepted. Indeed, even shells that failed this relatively mild test had still been issued to ships. Analysis of the test results afterwards by the Ordnance Board suggested the likelihood that 30–70% of shells would not have passed the standard penetration test specified by the Admiralty.

Efforts to replace the shells were initially resisted by the Admiralty, and action was not taken until Jellicoe became First Sea Lord in December 1916. As an initial response, the worst of the existing shells were withdrawn from ships in early 1917 and replaced from reserve supplies. New shells were designed, but did not arrive until April 1918, and were never used in action.

British battlecruisers were designed to chase and destroy enemy cruisers from out of the range of those ships. They were not designed to be ships of the line and exchange broadsides with the enemy. One German and three British battlecruisers were sunk—but none were destroyed by enemy shells penetrating the belt armour and detonating the magazines. Each of the British battlecruisers was penetrated through a turret roof and her magazines ignited by flash fires passing through the turret and shell-handling rooms. "Lützow" sustained 24 hits and her flooding could not be contained. She was eventually sunk by her escorts' torpedoes after most of her crew had been safely removed (though six trapped stokers died when the ship was scuttled). "Derfflinger" and "Seydlitz" sustained 22 hits each but reached port (although in "Seydlitz"'s case only just).

Jellicoe and Beatty, as well as other senior officers, gave an impression that the loss of the battlecruisers was caused by weak armour, despite reports by two committees and earlier statements by Jellicoe and other senior officers that Cordite and its management were to blame. This led to calls for armour to be increased, and an additional was placed over the relatively thin decks above magazines. To compensate for the increase in weight, ships had to carry correspondingly less fuel, water and other supplies. Whether or not thin deck armour was a potential weakness of British ships, the battle provided no evidence that it was the case. At least amongst the surviving ships, no enemy shell was found to have penetrated deck armour anywhere. The design of the new battlecruiser (which had started building at the time of the battle) was altered to give her of additional armour.

British and German propellant charges differed in packaging, handling, and chemistry. The British propellant was of two types, MK1 and MD. The Mark 1 cordite had a formula of 37% nitrocellulose, 58% nitroglycerine, and 5% petroleum jelly. It was a good propellant but burned hot and caused an erosion problem in gun barrels. The petroleum jelly served as both a lubricant and a stabiliser. Cordite MD was developed to reduce barrel wear, its formula being 65% nitrocellulose, 30% nitroglycerine, and 5% petroleum jelly. While cordite MD solved the gun-barrel erosion issue, it did nothing to improve its storage properties, which were poor. Cordite was very sensitive to variations of temperature, and acid propagation/cordite deterioration would take place at a very rapid rate. Cordite MD also shed micro-dust particles of nitrocellulose and iron pyrite. While cordite propellant was manageable, it required a vigilant gunnery officer, strict cordite lot control, and frequent testing of the cordite lots in the ships' magazines.

British cordite propellant (when uncased and exposed in the silk bag) tended to burn violently, causing uncontrollable "flash fires" when ignited by nearby shell hits. In 1945, a test was conducted by the U.S.N. Bureau of Ordnance (Bulletin of Ordnance Information, No.245, pp. 54–60) testing the sensitivity of cordite to then-current U.S. Naval propellant powders against a measurable and repeatable flash source. It found that cordite would ignite at 530 mm/22" from the flash, the current U.S. powder at 120 mm, /5", and the U.S. flashless powder at 25 mm./1"/

This meant that about 75 times the propellant would immediately ignite when exposed to flash, as compared to the U.S. powder. British ships had inadequate protection against these flash fires. German propellant ("RP C/12", handled in brass cartridge cases) was less vulnerable and less volatile in composition. German propellants were not that different in composition from cordite—with one major exception: centralite. This was symmetrical Diethyl Diphenyl Urea, which served as a stabiliser that was superior to the petroleum jelly used in British practice. It stored better and burned but did not explode. Stored and used in brass cases, it proved much less sensitive to flash. RP C/12 was composed of 64.13% nitrocellulose, 29.77% nitroglycerine, 5.75% centralite, 0.25% magnesium oxide and 0.10% graphite.

The Royal Navy Battle Cruiser Fleet had also emphasised speed in ammunition handling over established safety protocol. In practice drills, cordite could not be supplied to the guns rapidly enough through the hoists and hatches. To bring up the propellant in good time to load for the next broadside, many safety doors were kept open that should have been shut to safeguard against flash fires. Bags of cordite were also stocked and kept locally, creating a total breakdown of safety design features. By staging charges in the chambers between the gun turret and magazine, the Royal Navy enhanced their rate of fire but left their ships vulnerable to chain reaction ammunition fires and magazine explosions. This 'bad safety habit' carried over into real battle practices. Furthermore, the doctrine of a high rate of fire also led to the decision in 1913 to increase the supply of shells and cordite held on the British ships by 50%, for fear of running out of ammunition. When this exceeded the capacity of the ships' magazines, cordite was stored in insecure places.

The British cordite charges were stored two silk bags to a metal cylindrical container, with a 16-oz gunpowder igniter charge, which was covered with a thick paper wad, four charges being used on each projectile. The gun crews were removing the charges from their containers and removing the paper covering over the gunpowder igniter charges. The effect of having eight loads at the ready was to have of exposed explosive, with each charge leaking small amounts of gunpowder from the igniter bags. In effect, the gun crews had laid an explosive train from the turret to the magazines, and one shell hit to a battlecruiser turret was enough to end a ship.

A diving expedition during the summer of 2003 provided corroboration of this practice. It examined the wrecks of "Invincible", "Queen Mary", "Defence", and "Lützow" to investigate the cause of the British ships' tendency to suffer from internal explosions. From this evidence, a major part of the blame may be laid on lax handling of the cordite propellant for the shells of the main guns. The wreck of the "Queen Mary" revealed cordite containers stacked in the working chamber of the X turret instead of the magazine.

There was a further difference in the propellant itself. While the German "RP C/12" burned when exposed to fire, it did not explode, as opposed to cordite. "RP C/12" was extensively studied by the British and, after World War I, would form the basis of the later Cordite SC.

The memoirs of Alexander Grant, Gunner on "Lion", suggest that some British officers were aware of the dangers of careless handling of cordite:

Grant had already introduced measures onboard "Lion" to limit the number of cartridges kept outside the magazine and to ensure doors were kept closed, probably contributing to her survival.

On 5 June 1916, the First Lord of the Admiralty advised Cabinet Members that the three battlecruisers had been lost due to unsafe cordite management.

On 22 November 1916, following detailed interviews of the survivors of the destroyed battlecruisers, the Third Sea Lord, Rear Admiral Tudor, issued a report detailing the stacking of charges by the gun crews in the handling rooms to speed up loading of the guns.

After the battle, the B.C.F. Gunnery Committee issued a report (at the command of Admiral David Beatty) advocating immediate changes in flash protection and charge handling. It reported, among other things, that:

The United States Navy in 1939 had quantities of Cordite N, a Canadian propellant that was much improved, yet its Bureau of Ordnance objected strongly to its use onboard U.S. warships, considering it unsuitable as a naval propellant due to its inclusion of nitroglycerin.

British gunnery control systems, based on Dreyer tables, were well in advance of the German ones, as demonstrated by the proportion of main calibre hits made on the German fleet. Because of its demonstrated advantages, it was installed on ships progressively as the war went on, had been fitted to a majority of British capital ships by May, 1916, and had been installed on the main guns of all but two of the Grand Fleet's capital ships. The Royal Navy used centralised fire-control systems on their capital ships, directed from a point high up on the ship where the fall of shells could best be seen, utilising a director sight for both training and elevating the guns. In contrast, the German battlecruisers controlled the fire of turrets using a training-only director, which also did not fire the guns at once. The rest of the German capital ships were without even this innovation. German range-finding equipment was generally superior to the British FT24, as its operators were trained to a higher standard due to the complexity of the Zeiss range finders. Their stereoscopic design meant that in certain conditions they could range on a target enshrouded by smoke. The German equipment was not superior in range to the British Barr & Stroud rangefinder found in the newest British capital ships, and, unlike the British range finders, the German range takers had to be replaced as often as every thirty minutes, as their eyesight became impaired, affecting the ranges provided to their gunnery equipment.

The results of the battle confirmed the value of firing guns by centralised director. The battle prompted the Royal Navy to install director firing systems in cruisers and destroyers, where it had not thus far been used, and for secondary armament on battleships.

German ships were considered to have been quicker in determining the correct range to targets, thus obtaining an early advantage. The British used a 'bracket system', whereby a salvo was fired at the best-guess range and, depending where it landed, the range was progressively corrected up or down until successive shots were landing in front of and behind the enemy. The Germans used a 'ladder system', whereby an initial volley of three shots at different ranges was used, with the centre shot at the best-guess range. The ladder system allowed the gunners to get ranging information from the three shots more quickly than the bracket system, which required waiting between shots to see how the last had landed. British ships adopted the German system.

It was determined that range finders of the sort issued to most British ships were not adequate at long range and did not perform as well as the range finders on some of the most modern ships. In 1917, range finders of base lengths of were introduced on the battleships to improve accuracy.

Throughout the battle, British ships experienced difficulties with communications, whereas the Germans did not suffer such problems. The British preferred signalling using ship-to-ship flag and lamp signals, avoiding wireless, whereas the Germans used wireless successfully. One conclusion drawn was that flag signals were not a satisfactory way to control the fleet. Experience using lamps, particularly at night when issuing challenges to other ships, demonstrated this was an excellent way to advertise your precise location to an enemy, inviting a reply by gunfire. Recognition signals by lamp, once seen, could also easily be copied in future engagements.

British ships both failed to report engagements with the enemy but also, in the case of cruisers and destroyers, failed to actively seek out the enemy. A culture had arisen within the fleet of not acting without orders, which could prove fatal when any circumstances prevented orders being sent or received. Commanders failed to engage the enemy because they believed other, more senior officers must also be aware of the enemy nearby, and would have given orders to act if this was expected. Wireless, the most direct way to pass messages across the fleet (although it was being jammed by German ships), was avoided either for perceived reasons of not giving away the presence of ships or for fear of cluttering up the airwaves with unnecessary reports.

Naval operations were governed by standing orders issued to all the ships. These attempted to set out what ships should do in all circumstances, particularly in situations where ships would have to react without referring to higher authority, or when communications failed. A number of changes were introduced as a result of experience gained in the battle.

A new signal was introduced instructing squadron commanders to act independently as they thought best while still supporting the main fleet, particularly for use when circumstances would make it difficult to send detailed orders. The description stressed that this was not intended to be the only time commanders might take independent action, but was intended to make plain times when they definitely should. Similarly, instructions on what to do if the fleet was instructed to take evasive action against torpedoes were amended. Commanders were given discretion that if their part of the fleet was not under immediate attack, they should continue engaging the enemy rather than turning away with the rest of the fleet. In this battle, when the fleet turned away from Scheer's destroyer attack covering his retreat, not all the British ships had been affected, and could have continued to engage the enemy.

A number of opportunities to attack enemy ships by torpedo had presented themselves but had been missed. All ships, not just the destroyers armed principally with torpedoes but also battleships, were reminded that they carried torpedoes intended to be used whenever an opportunity arose. Destroyers were instructed to close the enemy fleet to fire torpedoes as soon as engagements between the main ships on either side would keep enemy guns busy directed at larger targets. Destroyers should also be ready to immediately engage enemy destroyers if they should launch an attack, endeavouring to disrupt their chances of launching torpedoes and keep them away from the main fleet.

To add some flexibility when deploying for attack, a new signal was provided for deploying the fleet to the centre, rather than as previously only either to left or right of the standard closed-up formation for travelling. The fast and powerful 5th Battle Squadron was moved to the front of the cruising formation so it would have the option of deploying left or right depending upon the enemy position. In the event of engagements at night, although the fleet still preferred to avoid night fighting, a destroyer and cruiser squadron would be specifically detailed to seek out the enemy and launch destroyer attacks.

At the time, Jellicoe was criticised for his caution and for allowing Scheer to escape. Beatty, in particular, was convinced that Jellicoe had missed a tremendous opportunity to annihilate the High Seas Fleet and win what would amount to another Trafalgar. Jellicoe was promoted away from active command to become First Sea Lord, the professional head of the Royal Navy, while Beatty replaced him as commander of the Grand Fleet.

The controversy raged within the navy and in public for about a decade after the war. Criticism focused on Jellicoe's decision at 19:15. Scheer had ordered his cruisers and destroyers forward in a torpedo attack to cover the turning away of his battleships. Jellicoe chose to turn to the south-east, and so keep out of range of the torpedoes. Supporters of Jellicoe, including the historian Cyril Falls, pointed to the folly of risking defeat in battle when one already has command of the sea. Jellicoe himself, in a letter to the Admiralty seventeen months before the battle, said that he intended to turn his fleet away from any mass torpedo attack (that being the universally accepted proper tactical response to such attacks, practised by all the major navies of the world). He said that, in the event of a fleet engagement in which the enemy turned away, he would assume they intended to draw him over mines or submarines, and he would decline to be so drawn. The Admiralty approved this plan and expressed full confidence in Jellicoe at the time (October 1914).

The stakes were high, the pressure on Jellicoe immense, and his caution certainly understandable. His judgement might have been that even 90% odds in favour were not good enough to bet the British Empire. The former First Lord of the Admiralty Winston Churchill said of the battle that Jellicoe "was the only man on either side who could have lost the war in an afternoon."

The criticism of Jellicoe also fails to sufficiently credit Scheer, who was determined to preserve his fleet by avoiding the full British battle line, and who showed great skill in effecting his escape.

On the other hand, some of Jellicoe's supporters condemned the actions of Beatty for the British failure to achieve a complete victory. Although Beatty was undeniably brave, his mismanagement of the initial encounter with Hipper's squadron and the High Seas Fleet cost considerable advantage in the first hours of the battle. His most glaring failure was in not providing Jellicoe with periodic information on the position, course, and speed of the High Seas Fleet. Beatty, aboard the battlecruiser "Lion", left behind the four fast battleships of the 5th Battle Squadron – the most powerful warships in the world at the time – engaging with six ships when better control would have given him 10 against Hipper's five. Though Beatty's larger guns out-ranged Hipper's guns by thousands of yards, Beatty held his fire for 10 minutes and closed the German squadron until within range of the Germans' superior gunnery, under lighting conditions that favoured the Germans. Most of the British losses in tonnage occurred in Beatty's force.

The total loss of life was 9,823 men, of which the British losses were 6,784 and German losses were 3,039. Counted among the British losses are 2 members of the Royal Australian Navy, and 1 member of the Royal Canadian Navy. 6 Australian nationals serving in the Royal Navy were also killed.

113,300 tons sunk:

62,300 tons sunk:

The Victoria Cross is the highest military decoration awarded for valour "in the face of the enemy" to members of the British Empire armed forces. The Ordre pour le Mérite was the Kingdom of Prussia and consequently the German Empire's highest military order until the end of the First World War.



In the years following the battle the wrecks were slowly discovered. was found by the Royal Navy minesweeper in 1919. After the Second World War some of the wrecks seem to have been commercially salvaged. For instance, the Hydrographic Office record for SMS "Lützow" (No.32344) shows that salvage operations were taking place on the wreck in 1960.

During 2000–2016 a series of diving and marine survey expeditions involving veteran shipwreck historian and archaeologist Innes McCartney has located all of the wrecks sunk in the battle. It was discovered that over 60% of them had suffered from metal theft. In 2003 McCartney led a detailed survey of the wrecks for the Channel 4 documentary "Clash of the Dreadnoughts". The film examined the last minutes of the lost ships and revealed for the first time how both 'P' and 'Q' turrets of had been blasted out of the ship and tossed into the sea before she broke in half. This was followed by the Channel 4 documentary "Jutland: WWI's Greatest Sea Battle", broadcast in May 2016, which showed how several of the major losses at Jutland had actually occurred and just how accurate the "Harper Record" actually was.

On the 90th anniversary of the battle, in 2006, the UK Ministry of Defence belatedly announced that the 14 British vessels lost in the battle were being designated as "protected places" under the Protection of Military Remains Act 1986. This legislation only affects British ships and citizens and in practical terms offers no real protection from non-British salvors of the wreck sites. In May 2016 a number of British newspapers named the Dutch salvage company "Friendship Offshore" as one of the main salvors of the Jutland wrecks in recent years and depicted leaked photographs revealing the extent of their activities on the wreck of .

The last surviving veteran of the battle, Henry Allingham, a British RAF (originally RNAS) airman, died on 18 July 2009, aged 113, by which time he was the oldest documented man in the world and one of the last surviving veterans of the whole war. Also among the combatants was the then 20-year-old Prince Albert, second in the line to the British throne, who would reign as King George VI of the United Kingdom from 1936 until his death in 1952. He served as a junior officer in the Royal Navy.

In 2013, one ship from the battle survives and is still afloat, the light cruiser . Decommissioned in 2011, she is docked at the Royal Naval Reserve depot in Belfast, Northern Ireland.

The Battle of Jutland was annually celebrated as a great victory by the right wing in Weimar Germany. This victory was used to repress the memory of the German navy's initiation of the German Revolution of 1918–1919, as well as the memory of the defeat in World War I in general. (The celebrations of the Battle of Tannenberg played a similar role.) This is especially true for the city of Wilhelmshaven, where wreath-laying ceremonies and torch-lit parades were performed until the end of the 1960s.
In 1916 Contreadmiral Friedrich von Kühlwetter (1865-1931) wrote a detailed analysis of the battle and published it in a book under the title "Skagerrak" (first anonymously published), which was reprinted in large numbers until after WWII and had a huge influence in keeping the battle in public memory amongst Germans as it was not tainted by the ideology of the Third Reich. Kühlwetter built the School for Naval Officers at Mürwik near Flensburg, where he is still remembered.

In May 2016, the 100th-anniversary commemoration of the Battle of Jutland was held. On 29 May, a commemorative service was held at St Mary's Church, Wimbledon, where the ensign from HMS "Inflexible" is on permanent display. On 31 May, the main service was held at St Magnus Cathedral in Orkney, attended by the British prime minister, David Cameron, and the German president, Joachim Gauck, along with Princess Anne and Vice Admiral Sir Tim Laurence.






[[Category:Conflicts in 1916]]
[[Category:1916 in Denmark]]
[[Category:1916 in Germany]]
[[Category:1916 in the United Kingdom]]
[[Category:Naval battles of World War I involving Australia]]
[[Category:Naval battles of World War I involving Canada]]
[[Category:Naval battles of World War I involving Germany]]
[[Category:Naval battles of World War I involving the United Kingdom]]
[[Category:North Sea operations of World War I]]
[[Category:Protected Wrecks of the United Kingdom]]
[[Category:Military history of the North Sea]]
[[Category:Battle of Jutland| ]]
[[Category:May 1916 events]]
[[Category:June 1916 events]]
[[Category:Germany–United Kingdom military relations]]

</doc>
<doc id="4565" url="https://en.wikipedia.org/wiki?curid=4565" title="Bambara language">
Bambara language

The Bambara (Bamana) language, Bamanankan, is a lingua franca and national language of Mali spoken by perhaps 15 million people, natively by 5 million Bambara people and about 10 million second-language users. It is estimated that about 80 percent of the population of Mali speak Bambara as a first or second language. It has a subject–object–verb clause structure and two lexical tones.

Bambara is a variety of a group of closely related languages called Manding, whose native speakers trace their cultural history to the medieval Mali Empire. Varieties of Manding are generally considered (among native speakers) to be mutually intelligible – dependent on exposure or familiarity with dialects between speakers – and spoken by 30 to 40 million people in the countries Burkina Faso, Senegal, Guinea-Bissau, Guinea, Liberia, Ivory Coast and the Gambia. Manding is part of the larger Mandé family of languages.

It uses seven vowels a, e, ɛ, i, o, ɔ and u, each of which can be nasalized, pharyngealized and murmured, giving a total number of 21 vowels(the letters approximate their equivalents). 
Writing with the latin alphabet began during the French occupation, and the first orthography was introduced in 1967. Literacy is limited, especially in rural areas. Although written literature is only slowly evolving (due to the predominance of French as the "language of the educated"), there exists a wealth of oral literature, which is often tales of kings and heroes. This oral literature is mainly tradited by the griots ("Jeliw" in Bambara) who are a mixture of storytellers, praise singers, and human history books who have studied the trade of singing and reciting for many years. Many of their songs are very old and are said to date back to the old empire of Mali.

Bambara is spoken throughout Mali as a lingua franca. The language is most widely spoken in the areas east, south, and north of Bamako, where native speakers and/or those that identify as members of the Bambara ethnic group are most densely populated. These regions are also usually considered to be the historical geographical origin of Bambara people, particularly Ségou, after diverging from other Manding groups.

The main dialect is Standard Bamara, which has significant influence from Maninkakan. Bambara has many local dialects: Kaarta, Tambacounda (west); Beledugu, Bananba, Mesekele (north); Jitumu, Jamaladugu, Segu (center); Cakadugu, Keleyadugu, Jalakadougu, Kurulamini, Banimɔncɛ, Cɛmala, Cɛndugu, Baninkɔ, Shɛndugu, Ganadugu (south); Kala, Kuruma, Saro, dialects to the northeast of Mopti (especially Bɔrɛ); Zegedugu, Bɛndugu, Bakɔkan, Jɔnka (southeast).,

Since 1967, Bambara has mostly been written in the Latin script, using some additional phonetic characters. The vowels are "a, e, ɛ" (formerly "è"), "i, o, ɔ" (formerly "ò"), "u"; accents can be used to indicate tonality. The former digraph "ny" is now written "ɲ" when it designates a palatal nasal glide; the "ny" spelling is kept for the combination of a nasal vowel with a subsequent oral palatal glide. Following the 1966 Bamako spelling conventions, a nasal velar glide "ŋ" is written as "ŋ", although in early publications it was often transcribed as "ng" or "nk".

The N'Ko () alphabet is a script devised by Solomana Kante in 1949 as a writing system for the Manding languages of West Africa; N’Ko means 'I say' in all Manding languages. Kante created N’Ko in response to what he felt were beliefs that Africans were a "cultureless people" since prior to this time there had been no indigenous African writing system for his language. N'ko came first into use in Kankan, Guinea as a Maninka alphabet and disseminated from there into other Manding-speaking parts of West Africa. N'ko and the Arabic script are still in use for Bambara, although the Latin script is much more common.

Each consonant represents a single sound. Although, there are some exceptions:

Like Turkish and Japanese, it is an agglutinative language, meaning that morphemes are glued together to form a word.

The basic sentence structure is Subject Object Verb. Take the phrase, "n t'a don" (I don't know [it]). "n" is the subject (me), "a" is the object (it), and "[ka] don" is the verb ([to] know). The "t'" is from the present tense marker "té." "té" is the negative present tense marker and "bé" is the affirmative present tense marker. Therefore, "n b'a don" would mean "I know it".

Bambara is an SOV language and has two (mid/standard and high) tones; e.g. "sa" 'die' vs. "sá" 'snake.' The typical argument structure of the language consists of a subject, followed by an aspectival auxiliary, followed by the direct object, and finally a transitive verb. Naturally, if the verb is intransitive, the direct object is absent.

Bambara does not inflect for gender. Gender for a noun can be specified by adding an adjective, "-cɛ" or "-kɛ" for male and "-muso" for female. The plural is formed by attaching a vocalic suffix "-u", most often with a low tone (in the orthography, "-w") to nouns or adjectives.

Bambara uses postpositions in much the same manner as languages like English and French use prepositions. These postpositions are found after the verb and are used to express direction, location, and in some cases, possession.

In urban areas, many Bambara conjunctions have been replaced in everyday use by French borrowings that often mark code-switches. The Bamako dialect makes use of sentences like: "N taara Kita mais il n'y avait personne là-bas." : "I went to Kita [Bambara] but there was no one there [French]." The sentence in Bambara alone would be "N taara Kita nka mɔgɔsi tuntɛ yen." The French proposition "est-ce que" is also used in Bambara; however, it is pronounced more slowly and as three syllables, .

Bambara uses many French loan words. For example, some people might say:
"I ka kulosi ye jauni ye": "Your skirt is yellow" (using a derivation of the French word for yellow, jaune.)

However, one could also say:
"I ka kulosi ye neremuguman ye", also meaning "your skirt is yellow." The original Bambara word for yellow comes from ""neremugu"," being flour ("mugu") made from néré (locust bean), a seed from a long seed pod. Neremugu is often used in sauces in Southern Mali.

Most French loan words are suffixed with the sound 'i'; this is particularly common when using French words which have a meaning not traditionally found in Mali. For example, the Bambara word for snow is "niegei", based on the French word for snow "neige". As there has never been snow in Mali, there has not been a traditional meaning for the word and thus no unique word in Bambara to describe it.

Malian artists such as Oumou Sangaré, Sidiki Diabaté, Rokia Traoré, Ali Farka Touré, Salif Keita, Habib Koité, and the married duo Amadou & Mariam often sing in Bambara. Aïda of the band Métisse often sings in Dioula, as does Mory Kante, born in Guinea to a Malian mother; his most famous song to date is "Yeke Yeke" (Alpha Blondy). Lyrics in Bambara occur on Stevie Wonder's soundtrack "Journey through the Secret Life of Plants".
Tiken Jah Fakoly (reggae) often sings in Dioula and French.

Additionally, in 2010, Spanish rock group Dover released its 7th studio album I Ka Kené, with the majority of lyrics in the language. American rapper Nas also released a track titled "Sabali" in 2010, which featured Damian Marley. "Sabali" is a Bambara word that means patience.

Bambara is one of several languages designated by Mali as a national language.







</doc>
<doc id="4566" url="https://en.wikipedia.org/wiki?curid=4566" title="Baku">
Baku

Baku ( , ; , ) is the capital and largest city of Azerbaijan, as well as the largest city on the Caspian Sea and of the Caucasus region, with a population of 2,374,000. Baku is located below sea level, which makes it the lowest lying national capital in the world and also the largest city in the world located below sea level. It is located on the southern shore of the Absheron Peninsula, alongside the Bay of Baku. At the beginning of 2009, Baku's urban population was estimated at just over two million people. Officially, about 25 percent of all inhabitants of the country live in Baku's metropolitan area. Baku is the sole metropolis in Azerbaijan.

Baku is divided into twelve administrative districts (raions) and 48 townships. Among these are the townships on the islands of the Baku Archipelago, and the town of Oil Rocks built on stilts in the Caspian Sea, away from Baku. The Inner City of Baku, along with the Shirvanshah's Palace and Maiden Tower, were inscribed as a UNESCO World Heritage Site in 2000. According to the Lonely Planet's ranking, Baku is also among the world's top ten destinations for urban nightlife.

The city is the scientific, cultural, and industrial center of Azerbaijan. Many sizeable Azerbaijani institutions have their headquarters there. The Baku International Sea Trade Port is capable of handling two million tons of general and dry bulk cargoes per year. In recent years, Baku has become an important venue for international events. It hosted the 57th Eurovision Song Contest in 2012, the 2015 European Games, 4th Islamic Solidarity Games, the F1 Azerbaijan Grand Prix since 2016, and will host UEFA Euro 2020. The city is bidding for Expo 2025 against Yekaterinburg, Russia and Osaka, Japan.

The city is renowned for its harsh winds, which is reflected in its nickname, the "City of Winds".

Baku is derived from the Persian name of the city باد-کوبه "Bād-kube", meaning "Wind-pounded city", in which "bād" means "wind" and "kube" is rooted in the verb کوبیدن "kubidan", "to pound", thus referring to a place where wind is strong and pounding. Indeed, the city is renowned for its fierce winter snow storms and harsh winds. This is also reflected in the city's nickname as the "City of Winds". A less probable folk etymology explains the name as deriving from "Baghkuy", meaning "God's town". "Baga" (now "بغ" "bagh") and "kuy" are the Old Persian words for "god" and "town" respectively; the name "Baghkuy" may be compared with "Baghdād" ("God-given") in which "dād" is the Old Persian word for "give". Arabic sources refer to the city as "Baku", "Bakukh", "Bakuya", and "Bakuye", all of which seem to come from a Persian name.

During Soviet rule, the city was spelled in Cyrillic as "Баку" in Russian and «Бакы» in Azerbaijani. Nowadays, when Azerbaijan is using the Latin alphabet, it is spelled as "Bakı".

Around 100,000 years ago, the territory of modern Baku and Absheron was savanna with rich flora and fauna. Traces of human settlement go back to the Stone age. From the Bronze age there have been rock carvings discovered near Bayil, and a bronze figure of a small fish discovered in the territory of the Old City. These have led some to suggest the existence of a Bronze Age settlement within the city's territory. Near Nardaran, in a place called Umid Gaya, a prehistoric observatory was discovered, where on the rock the images of sun and various constellations are carved together with a primitive astronomic table. Further archeological excavations revealed various prehistoric settlements, native temples, statues and other artifacts within the territory of the modern city and around it.

In the 1st century CE, the Romans organized two Caucasian campaigns and reached Baku. Near the city, in Gobustan, Roman inscriptions dating from 84–96 CE were discovered. This is one of the earliest written evidences for Baku.

Baku was the realm of the Shirvanshahs during the 8th century CE. The city frequently came under assault of the Khazars and (starting from the 10th century) the Rus. Shirvanshah Akhsitan I built a navy in Baku and successfully repelled another Rus assault in 1170. After a devastating earthquake struck Shamakhi, the capital of Shirvan, Shirvanshah’s court moved to Baku in 1191.
The Shirvan era greatly influenced Baku and the remainder of what is present-day Azerbaijan. Between the 12th and 14th centuries, massive fortifications were undertaken in Baku and the surrounding towns. The Maiden Tower, the Ramana Tower, the Nardaran Fortress, the Shagan Castle, the Mardakan Castle, the Round Castle and also the famous Sabayil Castle on the island of the Bay of Baku was built during this period. The city walls of Baku were also rebuilt and strengthened.

By the early 16th century Baku's wealth and strategic position attracted the focus of its larger neighbors; in the previous two centuries, it was under the rule of the in Iran-centred Kara Koyunlu and Ak Koyunlu. The fall of the Ak Koyunlu brought the city immediately into the sphere of the newly formed Iranian Safavid dynasty, led by king ("shah") Ismail I (r. 1501–1524). Ismail I laid siege to Baku in 1501 and captured it; he allowed the Shirvanshahs to remain in power, under Safavid suzerainty. His successor, king Tahmasp I (r. 1524–1576), completely removed the Shirvanshahs from power, and made Baku a part of the Shirvan province. Baku remained as an integral part of his empire and the successive Iranian dynasties to come for the next centuries, until the irrevocable cession in the first half of the 19th century. The House of Shirvan, who ruled Baku since the 9th century, was extinguished in the course of the Safavid rule.

At this time the city was enclosed within the lines of strong walls, which were washed by the sea on one side and protected by a wide trench on land. The Ottomans briefly gained control over Baku as a result of the Ottoman-Safavid War of 1578–1590; by 1607, it was again put under Iranian control. In 1604 the Baku fortress was destroyed by Shah Abbas I (r. 1588-1629).

In the wake of the demise of the Safavids, the Russians took advantage of the situation and invaded; the Safavids were forced to cede Baku to Russia for a few years. By 1730, the situation had deteriorated for the Russians; the successes of Nader Shah (r. 1736–1747) forced them to make an agreement near Ganja on 10 March 1735, ceding the city and all other conquered territories in the Caucasus back to Iran.

The eruption of instability following Nader Shah's death gave rise to the various Caucasian khanates. The semi-autonomous Persian-ruled Baku Khanate was one of these. It was ruled by Mirza Muhammed Khan but soon became a dependency of the much stronger Quba Khanate. During this time, the population of Baku was small (approximately 5,000), and the economy was ruined as a result of constant warfare.

From the late 18th century, Imperial Russia switched to a more aggressive geopolitical stance towards its two neighbors and rivals to the south, namely Iran and the Ottoman Empire. In the spring of 1796, by Catherine II’s order, General Valerian Zubov’s troops started a large campaign against Qajar Persia. Zubov had sent 13,000 men to capture Baku, and it was overrun subsequently without any resistance. On 13 June 1796, a Russian flotilla entered Baku Bay, and a garrison of Russian troops was placed inside the city. Later, however, Pavel I ordered the cessation of the campaign and the withdrawal of Russian forces following his predecessor, Catherine the Great's death. In March 1797, the tsarist troops left Baku and the city became part of Qajar Iran again.

In 1813, following the Russo-Persian War of 1804–1813, Qajar Iran was forced to sign the Treaty of Gulistan with Russia, which provided for the irrevocable cession of Baku and most of Iran's territories in the North Caucasus and South Caucasus to Russia. During the next and final bout of hostilities between the two, the Russo-Persian War of 1826–1828, Baku was briefly recaptured by the Iranians. However, militarily superior, the Russians ended this war in a victory as well, and the resulting Treaty of Turkmenchay made its inclusion into the Russian Empire definite.
When Baku was occupied by the Russian troops during the war of 1804–13, nearly the entire population of some 8,000 people was ethnic Tat.

Drilling for oil began in the mid-1800s, with the first oil well drilled in the Bibi-Heybat suburb of Baku in 1846. It was mechanically drilled, though a number of hand-dug wells predate it. Large-scale oil exploration started in 1872 when Russian imperial authorities auctioned the parcels of oil-rich land around Baku to private investors. The pioneer of oil extracting from the bottom of the sea was Polish geologist Witold Zglenicki. Soon after that Swiss, British, French, Belgian, German, Swedish and American investors appeared in Baku. Among them were the firms of the Nobel brothers together with the family von Börtzell-Szuch (Carl Knut Börtzell, who also owned the Livadia Palace) and the Rothschild family. An industrial oil belt, better known as Black City, was established near Baku.

Professor A. V. Williams Jackson of Columbia University wrote in his work "From Constantinople to the Home of Omar Khayyam" (1911):
By the beginning of the 20th century, half of the oil sold in international markets was being extracted in Baku. The oil boom contributed to the massive growth of Baku. Between 1856 and 1910 Baku's population grew at a faster rate than that of London, Paris or New York.

In 1917, after the October revolution and amidst the turmoil of World War I and the breakup of the Russian Empire, Baku came under the control of the Baku Commune, which was led by veteran Bolshevik Stepan Shahumyan. Seeking to capitalize on the existing inter-ethnic conflicts, by spring 1918, Bolsheviks inspired and condoned civil warfare in and around Baku. During the infamous March Days, Bolsheviks and Dashnaks seeking to establish control over the Baku streets, were faced with armed Azerbaijani groups. The Azerbaijanis suffered a crushing defeat by the united forces of the Baku Soviet and were massacred by Dashnak teams in what was called March Days. An estimated 3–12,000 Azerbaijanis were killed in their own capital. After the massacre, on 28 May 1918, the Azerbaijani faction of the Transcaucasian Sejm proclaimed the independence of the Azerbaijan Democratic Republic (ADR) in Ganja, thereby becoming the first Muslim-majority democratic and secular republic. The newly independent Azerbaijani republic, being unable to defend the independence of the country on their own, asked the Ottoman Empire for military support in accordance with clause 4 of the treaty between the two countries. Shortly after, Azerbaijani forces, with support of the Ottoman Army of Islam led by Nuru Pasha, started their advance onto Baku, eventually capturing the city from the loose coalition of Bolsheviks, Esers, Dashnaks, Mensheviks and British forces under the command of General Lionel Dunsterville on 15 September 1918.

After the Battle of Baku, the Azerbaijani irregular troops, with the tacit support of the Turkish command, conducted four days of pillaging and killing of 10–30,000 of the Armenian residents of Baku. This pogrom was known as the September Days. Shortly after this Baku was proclaimed the new capital of the Azerbaijan Democratic Republic.

With Turkey having lost the war by October 1918 they conducted the Armistice of Mudros with the British which meant Baku was to be evacuated. Headed by General William Thomson, British troops of 5,000 soldiers, including parts of Dunsterforce, arrived in Baku on 17 November. Thomson declared himself military governor of Baku and implemented Martial law on the capital until "the civil power would be strong enough to release the forces from the responsibility to maintain the public order". British forces left before the end of 1919 having felt they had done so.

The independence of the Azerbaijani republic was significant but a short-lived chapter. On 28 April 1920, the 11th Red Army invaded Baku and reinstalled the Bolsheviks, making Baku the capital of the Azerbaijan Soviet Socialist Republic.

The city underwent many major changes. As a result, Baku played a great role in many branches of the Soviet life. Since about 1921, the city was headed by the Baku City Executive Committee, commonly known in Russian as "Bakgorispolkom". Together with the Baku Party Committee (known as the "Baksovet"), it developed the economic significance of the Caspian metropolis. From 1922 to 1930, Baku was the venue for one of the major Trade fairs of the Soviet Union, serving as a commercial bridgehead to Iran and the Middle East.

Baku's growing importance as a major energy hub remained in sight of the major powers. During World War II and the Nazi German invasion of the southwestern Soviet Union, Baku had become of vital strategic importance. In fact, capturing the oil fields of Baku was one of the ultimate goals of Operation Edelweiss, carried out between May and November 1942. However, the German Army's closest approach to Baku was no closer than some northwest of Baku in November 1942, falling far short of the city's capture before being driven back during the Soviet Operation Little Saturn in mid-December 1942.

After the dissolution of the Soviet Union, Baku embarked on a process of restructuring on a scale unseen in its history. Thousands of buildings from the Soviet period were demolished to make way for a green belt on its shores; parks and gardens were built on the land reclaimed by filling up the beaches of the Baku Bay. Improvements were made in the general cleaning, maintenance, and garbage collection, and these services are now at Western European standards. The city is growing dynamically and developing at full speed on an east-west axis along the shores of the Caspian Sea. Sustainability has become a key factor in future urban development.

Baku is situated on the western coast of Caspian Sea. In the vicinity of the city there are a number of mud volcanoes (Keyraki, Bogkh-bogkha, Lokbatan and others) and salt lakes (Boyukshor, Khodasan and so on).

Baku has a temperate semi-arid climate (Köppen climate classification: "BSk") with hot and humid summers, cool and occasionally wet winters, and strong winds all year long. However, unlike many other cities with such climate features, Baku does not see extremely hot summers. This is largely because of its northerly latitude and the fact that it is located on a peninsula on the shore of the Caspian Sea. Baku and the Absheron Peninsula on which it is situated, is the most arid part of Azerbaijan (precipitation here is around or less than a year). The majority of the light annual precipitation occurs in seasons other than summer, but none of these seasons are particularly wet.
During Soviet times, Baku with its long hours of sunshine and dry healthy climate, was a vacation destination where citizens could enjoy beaches or relax in now-dilapidated spa complexes overlooking the Caspian Sea. The city's past as a Soviet industrial center has left it as one of the most polluted cities in the world.

At the same time Baku is noted as a very windy city throughout the year, hence the city's nickname the "City of Winds", and gale-force winds, the cold northern wind "khazri" and the warm southern wind "gilavar" are typical here in all seasons. Indeed, the city is renowned for its fierce winter snow storms and harsh winds.
The speed of the "khazri" sometimes reaches 144 kph (89 mph), which can cause damage to crops, trees and roof tiles.

The daily mean temperature in July and August averages , and there is very little rainfall during that season. During summer the "khazri" sweeps through, bringing desired coolness. Winter is cool and occasionally wet, with the daily mean temperature in January and February averaging . During winter the "khazri" sweeps through, driven by polar air masses; temperatures on the coast frequently drop below freezing and make it feel bitterly cold. Winter snow storms are occasional; snow usually melts within a few days after each snowfall.

Today, Baku is divided into 12 "rayonlar (sub-rayons)" (administrative districts) and 5 settlements of city type. 
Until 1988 Baku had very large Russian, Armenian, and Jewish populations which contributed to cultural diversity and added in various ways (music, literature, architecture and progressive outlook) to Baku's history. With the onset of the Karabakh War and the pogrom against Armenians starting in January 1990, the city's large Armenian population was expelled.
Under Communism, the Soviets took over the majority of Jewish property in Baku and Kuba. After the collapse of the Soviet Union, Azerbaijani President Heydar Aliyev returned several synagogues and a Jewish college, nationalized by the Soviets, to the Jewish community. He encouraged the restoration of these buildings and is well liked by the Jews of Azerbaijan. Renovation has begun on seven of the original 11 synagogues, including the Gilah synagogue, built in 1896, and the large Kruei Synagogue.

Today the vast majority of the population of Baku are ethnic Azerbaijanis (more than 90%). When Baku was occupied by the Russian troops during the war of 1804–13, nearly the entire population of some 8,000 people was ethnic Tat. The intensive growth of the population started in the middle of the 19th century when Baku was a small town with a population of about 7,000 people. The population increased again from about 13,000 in the 1860s to 112,000 in 1897 and 215,000 in 1913, making Baku the largest city in the Caucasus region.

Baku has been a cosmopolitan city at certain times during its history, meaning ethnic Azerbaijanis did not constitute the majority of population. In 2003 Baku additionally had 153,400 internally displaced persons and 93,400 refugees.

The urban landscape of Baku is shaped by many communities. The religion with the largest community of followers is Islam. The majority of the Muslims are Shia Muslims, and the Republic of Azerbaijan has the second highest Shia population percentage in the world after Iran. The city's notable mosques include Juma Mosque, Bibi-Heybat Mosque, Muhammad Mosque and Taza Pir Mosque.

There are some other faiths practiced among the different ethnic groups within the country. By article 48 of its Constitution, Azerbaijan is a secular state and ensures religious freedom. Religious minorities include Russian Orthodox Christians, Catholic Levantines, Georgian Orthodox Christians, Lutherans, Ashkenazi Jews and Sufi Muslims.

Zoroastrianism, although extinct in the city as well as in the rest of the country by the present time, had a long history in Azerbaijan and the Zoroastrian New Year (Nowruz) continues to be the main holiday in the city as well as in the rest of Azerbaijan.

Baku's largest industry is petroleum, and its petroleum exports make it a large contributor to Azerbaijan's balance of payments. The existence of petroleum has been known since the 8th century. In the 10th century, the Arabian traveler, Marudee, reported that both white and black oil were being extracted naturally from Baku. By the 15th century, oil for lamps was obtained from hand-dug surface wells.
Commercial exploitation began in 1872, and by the beginning of the 20th century the Baku oil fields were the largest in the world. Towards the end of the 20th century much of the onshore petroleum had been exhausted, and drilling had extended into the sea offshore. By the end of the 19th century skilled workers and specialists flocked to Baku. By 1900 the city had more than 3,000 oil wells, of which 2,000 were producing oil at industrial levels. Baku ranked as one of the largest centres for the production of oil industry equipment before World War II. The World War II Battle of Stalingrad was fought to determine who would have control of the Baku oil fields. Fifty years before the battle, Baku produced half of the world's oil supply.

Currently the oil economy of Baku is undergoing a resurgence, with the development of the massive Azeri-Chirag-Guneshli field (Shallow water Gunashli by SOCAR, deeper areas by a consortium led by BP), development of the Shah Deniz gas field, the expansion of the Sangachal Terminal and the construction of the BTC Pipeline.

The Baku Stock Exchange is Azerbaijan's largest stock exchange, and largest in the Caucasian region by market capitalization. A relatively large number of transnational companies are headquartered in Baku. One of the more prominent institutions headquartered in Baku is the International Bank of Azerbaijan, which employs over 1,000 people. International banks with branches in Baku include HSBC, Société Générale and Credit Suisse.
Baku is one of the most important tourist destinations in the Caucasus, with hotels in the city earning 7 million euros in 2009. Many sizable world hotel chains have a presence in the city. Baku has many popular tourist and entertainment spots, such as the downtown Fountains Square, the One and Thousand Nights Beach, Shikhov Beach and Oil Rocks. Baku's vicinities feature Yanar Dag, an ever-blazing spot of natural gas. On 2 September 2010, with the inauguration of National Flag Square, Baku became home to the world's tallest flagpole, according to the Guinness Book of Records. However, on 24 May 2011 Baku lost this record by just to the city of Dushanbe in Tajikistan. As of October 2017, the Flag Pole is dismantled and the National Flag Square closed with fences.

Baku has several shopping malls; the most famous city center malls are Park Bulvar, Genclik Mall, Metro Park, 28 MALL, Aygun city and AF MALL. The retail areas contain shops from chain stores up to high-end boutiques.

The city is listed 48th in the 2011 list of the most expensive cities in the world conducted by the Mercer Human Resource Consulting. Its Nizami Street is one of the most expensive streets in the world.

The city has many amenities that offer a wide range of cultural activities, drawing both from a rich local dramatic portfolio and an international repertoire. It also boasts many museums such as Baku Museum of Modern Art and Azerbaijan State Museum of History, most notably featuring historical artifacts and art. Many of the city's cultural sites were celebrated in 2009 when Baku was designated an Islamic Culture Capital. Baku was also chosen to host the Eurovision Dance Contest 2010. In 2007 the Heydar Aliyev Cultural Center was opened.

Among Baku's prestigious cultural venues are Azerbaijan State Philharmonic Hall, Azerbaijan State Academic Opera and Ballet Theatre. The main movie theatre is Azerbaijan Cinema.
Festivals include the Baku International Film Festival, Baku International Jazz Festival, Novruz Festival, "Gül Bayramı" (Flower Festival) and the National Theater Festival. International and local exhibitions are presented at the Baku Expo Center.

, the city along with Ganja and Lankaran participates in Earth Hour movement.

Baku has wildly varying architecture, ranging from the Old City core to modern buildings and the spacious layout of the Baku port. Many of the city's most impressive buildings were built during the early 20th century, when architectural elements of the European styles were combined in eclectic style. Baku has an original and unique appearance, earning it a reputation as the 'Paris of the East'.

The Hamam tradition in Baku is interesting. There are a number of ancient hamams in Baku dating back to the 12th, 14th and 18th centuries. Hamams play a very important role in the architectural appearance of Baku.

Teze Bey is the most popular hamam (traditional bath) in Baku. It was built in 1886 in the center of Baku and in 2003 it was fully restored and modernized. Along with its modern amenities, Teze Bey features a swimming pool and architectural details inspired by Oriental, Russian and Finnish baths.

Gum Hamam was discovered during archaeological excavations underneath the sand; hence the name: Gum hamam (sand bath). It was built sometime during the 12th–14th centuries.

In ancient times Bairamali Hamam was called “Bey Hamam”. The original structure was built sometime during the 12th–14th centuries and was reconstructed in 1881.

Agha Mikayil Hamam was constructed in the 18th century by Haji Agha Mikayil on Kichik Gala Street in the Old City (icherisheher). It is still operating in its ancient setting. The Hamam is open to women on Mondays and Fridays and to men on the other days of the week.

Late modern and postmodern architecture began to appear in the early 2000s. With economic development, old buildings such as Atlant House were razed to make way for new ones. Buildings with all-glass shells have appeared around the city, the most prominent examples being the Azerbaijan Tower, Heydar Aliyev Cultural Center, Flame Towers, Baku Crystal Hall, Baku White City and SOCAR Tower. These projects also caught the attention of international media as notable programmes such as Discovery Channel's Extreme Engineering did pieces focusing in on changes to the city.

The Old City of Baku, also known as the Walled City of Baku, refers to the ancient Baku settlement. Most of the walls and towers, strengthened after the Russian conquest in 1806, survived. This section is picturesque, with its maze of narrow alleys and ancient buildings: the cobbled streets past the Palace of the Shirvanshahs, two caravansaries, the baths and the Juma Mosque (which used to house the Azerbaijan National Carpet and Arts Museum but is now a mosque again). The old town core also has dozens of small mosques, often without any particular sign to distinguish them as such.

In 2003, UNESCO placed the Inner City on the List of World Heritage in Danger, citing damage from a November 2000 earthquake, poor conservation as well as "dubious" restoration efforts.

The music scene in Baku can be traced back to ancient times and villages of Baku, generally revered as the fountainhead of meykhana and mugham in the Azerbaijan.

In recent years, the success of Azerbaijani performers such as AySel, Farid Mammadov, Sabina Babayeva, Safura and Elnur Hüseynov in the Eurovision Song Contest has significantly boosted the profile of Baku's music scene, prompting international attention. Following the victory of Azerbaijan's representative Eldar & Nigar at the Eurovision Song Contest 2011, Baku hosted the Eurovision Song Contest 2012.

2005 was a landmark in the development of Azerbaijani jazz in the city. It has been home to legendary jazz musicians like Vagif Mustafazadeh, Aziza Mustafa Zadeh, Rafig Babayev and Rain Sultanov. Among Baku's prominent annual fairs and festivals is Baku International Jazz Festival, which features some of the world's most identifiable jazz names.

Baku also has a thriving International Center of Mugham, which is located in Baku Boulevard, Gulustan Palace and Buta Palace, one of the principal performing arts centers and music venues in the city.

The majority of Azerbaijan's media companies (including television, newspaper and radio, such as, Azad Azerbaijan TV, Ictimai TV, Lider TV and Region TV) are headquartered in Baku. The films "The World Is Not Enough" and "The Diamond Arm" are set in the city, while "Amphibian Man" includes several scenes filmed in Old City.

Out of the city's radio stations, "Ictimai Radio", "Radio Antenn", "Burc FM", "Avto FM", "ASAN Radio" and "Lider FM Jazz" are some of the more influential competitors with large national audiences.

Some of the most influential Baku newspapers include the daily "Azadliq", "Zaman" (The Time), "Bakinskiy Rabochiy" (The Baku Worker), "Echo" and the English-language "Baku Today".

Baku is also featured in the video game "Battlefield 4".

Baku boasts a vibrant nightlife. Many clubs that are open until dawn can be found throughout the city. Clubs with an eastern flavor provide special treats from the cuisine of Azerbaijan along with local music. Western-style clubs target younger, more energetic crowds. Most of the public houses and bars are located near Fountains Square and are usually open until the early hours of the morning.

Baku is home to restaurants catering to every cuisine and occasion. Restaurants range from luxurious and expensive to ordinary and affordable.

In the Lonely Planet "1000 Ultimate Experiences", Baku placed 8th among the top 10 party cities in the world.

Baku has large sections of greenery either preserved by the National Government or designated as green zones. The city, however, continues to lack a green belt development as economic activity pours into the capital, resulting in massive housing projects along the suburbs.

Baku Boulevard is a pedestrian promenade that runs parallel to Baku's seafront. The boulevard contains an amusement park, yacht club, musical fountain, statues and monuments. The park is popular with dog-walkers and joggers, and is convenient for tourists. It is adjacent to the newly built International Center of Mugham and the musical fountain.

Other prominent parks and gardens include Heydar Aliyev Park, Samad Vurgun Park, Narimanov Park, Alley of Honor and the Fountains Square. The Martyrs' Lane, formerly the Kirov Park, is dedicated to the memory of those who lost their lives during the Nagorno-Karabakh War and also to the 137 people killed on Black January.

 Baku hosts a Formula One race on the Baku City Circuit. The first was the 2016 European Grand Prix.

The city will also host three group games and one quarter-final of the UEFA Euro 2020 European Football Championship.

Since 2002, Baku has hosted 36 major sporting events and selected to host the 2015 European Games. Baku is also to host the fourth edition of the Islamic Solidarity Games in 2017.

Baku is also one of world's leading chess centres, having produced famous grandmasters like Teimour Radjabov, Vugar Gashimov, Garry Kasparov, Shahriyar Mammadyarov and Rauf Mammadov, as well as the arbiter Faik Hasanov. The city also annually hosts the international tournaments such as Baku Chess Grand Prix, President's Cup, Baku Open and currently bidding to host 42nd Chess Olympiad in 2014.

First class sporting facilities were built for the indoor games, including the Palace of Hand Games and Heydar Aliyev Sports and Exhibition Complex. It hosted many sporting events, including FIFA U-17 Women's World Cup, Rhythmic Gymnastics European Championships in 2007 and 2009, 2005 World Rhythmic Gymnastics Championships, 2007 FILA Wrestling World Championships and 2010 European Wrestling Championships, 2011 World Amateur Boxing Championships, 2009 Women's Challenge Cup and European Taekwondo Championships in 2007. Since 2011 the city annually hosts WTA tennis event called Baku Cup.

The Synergy Baku Cycling Project participates in the Tour d'Azerbaïdjan a 2.2 multi-stage bicycle race on the UCI Europe Tour.

Baku made a bid to host the 2016 Summer Olympics and 2020 Summer Olympics, but failed to become a Candidate City both times.

The largest sport hub in the city is Baku Olympic Stadium with 68,700 seating capacity whose construction was completed in 2015. The city's three main football clubs are Neftchi Baku, Inter Baku and FC Baku of whom first has eight Premier League titles making Neftchi the most successful Azerbaijani football club. Baku also has several football clubs in the premier and regional leagues, including AZAL and Ravan in Premier League. The city's second largest stadium, Tofiq Bahramov Stadium hosts a number of domestic and international competitions and was the main sport centre of the city for a long period until the construction of Baku Olympic Stadium.

In the Azerbaijan Women's Volleyball Super League, Baku is represented by Rabita Baku, Azerrail Baku, Lokomotiv Baku and Azeryol Baku.

Throughout history the transport system of Baku used the now-defunct horsecars, trams and narrow gauge railways. , 1,000 black cabs are ordered by Baku Taxi Company, and as part of a programme originally announced by the Transport Ministry of Azerbaijan, there is a plan to introduce London cabs into Baku. The move was part of £16 million agreement between Manganese Bronze subsidiary LTI Limited and Baku Taxi Company.

Local rail transport includes the Baku Funicular and the Baku Metro, a rapid-transit system notable for its art, murals, mosaics and ornate chandeliers. Baku Metro was opened in November 1967 and includes 3 lines and 25 stations at present; 170 million people used Baku Metro over the past five years. In 2008, the Chief of the Baku Metro, Taghi Ahmadov, announced plans to construct 41 new stations over the next 17 years. These will serve the new bus complex as well as the international airport.

BakuCard is a single Smart Card for payment on all types of city transport. The intercity buses and metro use this type of card-based fare-payment system.

Baku's Central Railway Station is the terminus for national and international rail links to the city. The Kars–Tbilisi–Baku railway, which will directly connect Turkey, Georgia and Azerbaijan, began to be constructed in 2007 and is scheduled for completion in 2015. The completed branch will connect Baku with Tbilisi in Georgia, and from there trains will continue to Akhalkalaki, and Kars in Turkey.
Sea transport is vital for Baku, as the city is practically surrounded by the Caspian Sea to the east. Shipping services operate regularly from Baku across the Caspian Sea to Turkmenbashi (formerly Krasnovodsk) in Turkmenistan and to Bandar Anzali and Bandar Nowshar in Iran. The commuter ferries, along with the high-speed catamaran "Seabus" ("Deniz Avtobusu"), also form the main connection between the city and the Absheron peninsula.

The Baku Port was founded in 1902 and since then has been the largest Caspian Sea port. It has six facilities: the main cargo terminal, the container terminal, the ferry terminal, the oil terminal, the passenger terminal and the port fleet terminal. The port's throughput capacity reaches 15 million tons of liquid bulk and up to 10 million tons of dry cargoes.
Beginning in 2010, the Baku International Sea Trade Port is being reconstructed. The construction will take place in three stages and will be completed by 2016. The estimated costs are 400 Million US$. From April to November the Baku Port is accessible to ships loading cargoes for direct voyages from Western European and Mediterranean ports. The State Road M-1 and the European route E60 are the two main motorway connections between Europe and Azerbaijan. The motorway network around Baku is well developed and is constantly being extended.
The Heydar Aliyev International Airport is the only commercial airport serving Baku. The new Baku Cargo Terminal was officially opened in March 2005. It was constructed to be a major cargo hub in the CIS countries and is actually now one of the biggest and most technically advanced in the region. There are also several smaller military airbases near Baku, such as Baku Kala Air Base, intended for private aircraft, helicopters and charters.

Baku hosts many universities, junior colleges and vocational schools. Baku State University, the first established university in Azerbaijan was opened in 1919 by the government of the Azerbaijan Democratic Republic. In the early years of the Soviet era, Baku already had Azerbaijan State Oil Academy, Azerbaijan Medical University and Azerbaijan State Economic University. In the post-WWII period, a few more universities were established such as Azerbaijan Technical University, Azerbaijan University of Languages and the Azerbaijan Architecture and Construction University. After 1991 when Azerbaijan gained independence from the Soviet Union, the fall of communism led to the development of a number of private institutions, including Qafqaz University and Khazar University which are currently considered the most prestigious academic institutions. Apart from the private universities, the government established the Academy of Public Administration, the Azerbaijan Diplomatic Academy and various military academies. The largest universities according to the student population are Baku State University and Azerbaijan State Economic University. In addition, there are the Baku Music Academy and the Azerbaijan National Conservatoire in Baku established in the early 1920s. Publicly run kindergartens and elementary schools (years 1 through 11) are operated by local wards or municipal offices.

The Azerbaijan National Academy of Sciences, the main state research organization in Azerbaijan is locating in Baku as well. Moreover, Baku has numerous libraries, many of which contain vast collections of historic documents from the Roman, Byzantine, Ottoman and Soviet periods, as well as from other civilisations of the past. The most important libraries in terms of historic document collections include the Nizami Museum of Azerbaijan Literature, the National Library of Azerbaijan, the Mirza Alakbar Central Library, the Samad Vurgun Library and the Baku Presidential Library.

The city has many public and private hospitals, clinics and laboratories within its bounds and numerous medical research centers. Many of these facilities have high technology equipment, which has contributed to the recent upsurge in "medical tourism" to Baku, particularly from post-Soviet countries such as Georgia and Moldova, whose governments send lower-income patients to the city for inexpensive high-tech medical treatments and operations.

Because of its intermittent periods of great prosperity as well as being the largest city in the Caucasus and one of the most ethnically and culturally diverse in the Soviet Union, Baku prides itself on having produced a disproportionate number of notable figures in the sciences, arts and other fields. Some of the houses they resided in display commemorative plaques. Some of the many prestigious residents include: Academy Award winners Rustam Ibrahimbeyov and Vladimir Menshov, one of the founders and head of the Soviet space program Kerim Kerimov, Nobel Prize winner and physicist Lev Landau and famous musicians such as Gara Garayev, Uzeyir Hajibeyov, Muslim Magomayev, Vagif Mustafazadeh and Alim Qasimov. World-famous cellist Mstislav Rostropovich was born and raised in Baku, as was world-famous chess player, Garry Kasparov.
Baku is twinned with:


Partnership relations also exist at different levels with: Berlin, Paris, Vienna, Tbilisi, Astana, Minsk, Moscow, Volgograd, Kizlyar, Tashkent and Chengdu.




</doc>
<doc id="4567" url="https://en.wikipedia.org/wiki?curid=4567" title="Balalaika">
Balalaika

The balalaika (, ) is a Russian stringed musical instrument with a characteristic triangular wooden, hollow body and three strings. Two strings are usually tuned to the same note and the third string is a perfect fourth higher. The higher-pitched balalaikas are used to play melodies and chords. The instrument generally has a short sustain, necessitating rapid strumming or plucking when it is used to play melodies. Balalaikas are often used for Russian folk music and dancing.

The balalaika "family of instruments" includes instruments of various sizes, from the highest-pitched to the lowest: the piccolo balalaika, prima balalaika, secunda balalaika, alto balalaika, bass balalaika, and contrabass balalaika. There are balalaika orchestras which consist solely of different balalaikas; these ensembles typically play Classical music that has been arranged for balalaikas. The prima balalaika is the most common; the piccolo is rare. There have also been "descant" and "tenor" balalaikas, but these are considered obsolete. All have three-sided bodies; spruce, evergreen, or fir tops; and backs made of three to nine wooden sections (usually maple). They are typically strung with three strings, and the necks are fretted.

The prima balalaika, secunda and alto are played either with the fingers or a plectrum (pick), depending on the music being played, and the bass and contrabass (equipped with extension legs that rest on the floor) are played with leather plectra. The rare piccolo instrument is usually played with a pick.

The earliest mention of the term "balalaika" dates back to a 1688 Russian document. The term "balabaika" was used in Ukrainian language document from 18th century. According to one theory, the term was loaned to Russian, where – in literary language – it first appeared in "Elysei", a 1771 poem by V. Maykov.

The most common solo instrument is the prima, which is tuned E–E–A (thus the two lower strings are tuned to the same pitch). Sometimes the balalaika is tuned "guitar style" by folk musicians to G–B–D (mimicking the three highest strings of the Russian guitar), whereby it is easier to play for Russian guitar players, although classically trained balalaika purists avoid this tuning. It can also be tuned to E–A–D, like its cousin, the domra, to make it easier for those trained on the domra to play the instrument, and still have a balalaika sound. The folk (pre-Andreev) tuning D–F–A was very popular, as this makes it easier to play certain riffs.

The balalaika has been made the following sizes:

Factory-made six-string prima balalaikas with three sets of double courses are also common. These have three double courses similar to the stringing of the mandolin and often use a "guitar" tuning. 

Four string alto balalaikas are also encountered and are used in the orchestra of the Piatnistky Folk Choir.

The piccolo, prima, d secunda balalaikas were originally strung with gut with the thinnest melody string made of stainless steel. Today, nylon strings are commonly used in place of gut.

Amateur and/or souvenir-style prima balalaikas usually have a total of 16 frets, while in professional orchesta-like ones that number raises to 24.

An important part of balalaika technique is the use of the left thumb to fret notes on the lower string, particularly on the prima, where it is used to form chords. Traditionally, the side of the index finger of the right hand is used to sound notes on the prima, while a plectrum is used on the larger sizes.

Because of the large size of the contrabass's strings, it is not uncommon to see players using plectra made from a leather shoe or boot heel. The bass balalaika and contrabass balalaika rest on the ground, on a wooden or metal pin that is drilled into one of its corners.

The exact origins of the balalaika are unknown.

However, it is commonly agreed that it descended from the domra, an instrument from the Caucasus region of Russia. There is also similarity to the Kazakh dombra, which has 2 strings, and the Mongolian topshur.

Early representations of the balalaika show it with anywhere from two to six strings, which resembles certain Central Asian instruments. Similarly, frets on earlier balalaikas were made of animal gut and tied to the neck so that they could be moved around by the player at will (as is the case with the modern saz, which allows for the playing distinctive to Turkish and Central Asian music).

The first known document mentioning the instrument dates back to 1688. A guard's logbook from the Moscow Kremlin records that two commoners were stopped from playing the Balalika whilst drunk. Further documents from 1700 and 1714 also mention the instrument. In the early 18th century the term appeared in Ukrainian documents, where it sounded like "Balabaika". Balalaika appeared in "Elysei", a 1771 poem by V. Maikov. In the 19th century, the balalaika evolved into a triangular instrument with a neck that was substantially shorter than that of its Asian counterparts. It was popular as a village instrument for centuries, particularly with the "skomorokhs", sort of free-lance musical jesters whose tunes ridiculed the Tsar, the Russian Orthodox Church, and Russian society in general.

In the 1880s, Vasily Vasilievich Andreyev, who was then a professional violinist in the music salons of St Petersburg, developed what became the standardized balalaika, with the assistance of violin maker V. Ivanov. The instrument began to be used in his concert performances. A few years later, St. Petersburg craftsman Paserbsky further refined the instruments by adding a fully chromatic set of frets and also a number of balalaikas in orchestral sizes with the tunings now found in modern instruments. Andreyev patented the design and arranged numerous traditional Russian folk melodies for the orchestra. He also composed a body of concert pieces for the instrument.

The end result of Andreyev's labours was the establishment of an orchestral folk tradition in Tsarist Russia, which later grew into a movement within the Soviet Union. The balalaika orchestra in its full form consists of balalaikas, domras, gusli, bayan, Vladimir Shepherd's Horns, garmoshkas and several types of percussion instruments.

With the establishment of the Soviet system and the entrenchment of a proletarian cultural direction, the culture of the working classes (which included that of village labourers) was actively supported by the Soviet establishment. The concept of the balalaika orchestra was adopted wholeheartedly by the Soviet government as something distinctively proletarian (that is, from the working classes) and was also deemed progressive. Significant amounts of energy and time were devoted to support and foster formal study of the balalaika, from which highly skilled ensemble groups such as the Osipov State Russian Folk Orchestra emerged. Balalaika virtuosi such as Boris Feoktistov and Pavel Necheporenko became stars both inside and outside the Soviet Union. The movement was so powerful that even the renowned Red Army Choir, which initially used a normal symphonic orchestra, changed its instrumentation, replacing violins, violas, and violoncellos with orchestral balalaikas and domras.

Often musicians perform solo on the balalaika. In particular, Alexey Arkhipovsky is well known for his solo performances. In particular, he was invited to play at the opening ceremony of the second semi final of the Eurovision Song Contest 2009 in Moscow because the organizers wanted to give a "more Russian appearance" to the contest.

Through the 20th century, interest in Russian folk instruments grew outside of Russia, likely as a result of western tours by Andreyev and other balalaika virtuosi early in the century. Significant balalaika associations are found in Washington, D.C., Los Angeles, New York; Atlanta and Seattle.





</doc>
<doc id="4568" url="https://en.wikipedia.org/wiki?curid=4568" title="Bank of China Tower (Hong Kong)">
Bank of China Tower (Hong Kong)

The Bank of China Tower (abbreviated BOC Tower) is one of the most recognisable skyscrapers in Central, Hong Kong. Located at 1 Garden Road, the tower houses the headquarters of the Bank of China (Hong Kong) Limited.

Designed by I. M. Pei and L.C Pei of I.M Pei and Partners, the building is high with two masts reaching high.
It was the tallest building in Hong Kong and Asia from 1989 to 1992, and it was the first supertall skyscraper outside the United States, the first to break the 305 m (1,000 ft) mark. It is now the fourth tallest skyscraper in Hong Kong, after International Commerce Centre, Two International Finance Centre and Central Plaza.

The site on which the building is constructed was formerly the location of Murray House. After its brick-by-brick relocation to Stanley, the site was sold by the Government for "only HK$1 billion" in August 1982 amidst growing concern over the future of Hong Kong in the run-up to the transfer of sovereignty.

The building was initially built by the Hong Kong Branch of the Bank of China; its Garden Road entrance continues to display the name "Bank of China", rather than BOCHK. The top four and the bottom 19 storeys are used by the Bank, while the other floors are leased out. Ownership has since been transferred to BOCHK, although the Bank of China has leased back several floors for use by its own operations in Hong Kong.

The Government had apparently given preferential treatment to Chinese companies, and was again criticised for the apparent preferential treatment to the BOCHK.

The price paid was half the amount of the 6,250 m² Admiralty II plot, for which the MTR Corporation paid HK$1.82 billion in cash. The BOC would make initial payment of $60 million, with the rest payable over 13 years at 6% interest. The announcement of the sale was also poorly handled, and a dive in business confidence ensued. The Hang Seng Index fell 80 points, and the HK$ lost 1.5% of its value the next day.

The tower was built by Japanese contractor Kumagai Gumi. Superstructure work began in May 1986.

The tower is a steel-frame structure. The spray-on fireproofing material applied to the steel structure, a product called Monokote MK-5, was a source of controversy as it contains asbestos. At the time, the use of asbestos was only partially banned in Hong Kong.

The Tiananmen Square protests of 1989 interrupted publicity surrounding the building's design and construction. A press conference scheduled for 24 May 1989, two weeks before the massacre, was intended to show off the building's "designer socialist furnishings", but was called off as the student demonstrations in Peking escalated. The public relations firm that organised the conference explained to the "South China Morning Post" that "under the circumstances, it has been decided to stop any publicity to do with the Bank of China."

Once developed, gross floor area was expected to be 100,000 m². The original project was intended for completion on the auspicious date of 8 August 1988. However, owing to project delays, groundbreaking took place in March 1985, almost two years late. It was topped out in 1989, and occupied on 15 June 1990.

Designed by Pritzker Prize-winning architect I. M. Pei, the building is high with two masts reaching high. The 72-storey building is located near Central MTR station. This was the tallest building in Hong Kong and Asia from 1990 to 1992, the first building outside the United States to break the 305 m (1,000 ft) mark, and the first composite space frame high-rise building. That also means it was the tallest outside the United States from its completion year, 1990. It is now the fourth tallest skyscraper in Hong Kong, after International Commerce Centre, Two International Finance Centre and Central Plaza.

A small observation deck on the 43rd floor of the building is open to the public.

The structural expressionism adopted in the design of this building resembles growing bamboo shoots, symbolising livelihood and prosperity. The whole structure is supported by the four steel columns at the corners of the building, with the triangular frameworks transferring the weight of the structure onto these four columns. It is covered with glass curtain walls.

While its distinctive look makes it one of Hong Kong's most identifiable landmarks today, it was the source of some controversy at one time, as the bank is the only major building in Hong Kong to have bypassed the convention of consulting with feng shui masters on matters of design prior to construction.

The building has been criticised by some practitioners of feng shui for its sharp edges and its negative symbolism by the numerous 'X' shapes in its original design, though Pei modified the design to some degree before construction following this feedback. The building's profile from some angles resembles that of a meat cleaver and it is sometimes referred to as a "vertical knife". This earned it the nickname “一把刀”(Yaat Baa Dou) in Cantonese, literally meaning 'One Knife'. 

The Bank Of China Tower can be accessed by the Mass Transit Railway (MTR) by walking through Chater Garden from Central Station Exit J2.





</doc>
<doc id="4569" url="https://en.wikipedia.org/wiki?curid=4569" title="Blind Lemon Jefferson">
Blind Lemon Jefferson

Lemon Henry "Blind Lemon" Jefferson (September 24, 1893 – December 19, 1929) was an American blues and gospel singer, songwriter, and musician. He was one of the most popular blues singers of the 1920s and has been called the "Father of the Texas Blues".

Jefferson's performances were distinctive because of his high-pitched voice and the originality of his guitar playing. His recordings sold well, but he was not a strong influence on younger blues singers of his generation, who could not imitate him as easily as they could other commercially successful artists. Later blues and rock and roll musicians, however, did attempt to imitate both his songs and his musical style.

Jefferson was born blind (or possibly partially blind), near Coutchman, Texas. He was the youngest of seven (or possibly eight) children born to Alex and Clarissa Jefferson, who were African-American sharecroppers. Disputes regarding the date of his birth derive from contradictory census records and draft registration records. By 1900, the family was farming southeast of Streetman, Texas. Jefferson's birth date was recorded as September 1893 in the 1900 census. The 1910 census, taken in May, before his birthday, confirms his year of birth as 1893 and indicated that the family was farming northwest of Wortham, near his birthplace.

In his 1917 draft registration, Jefferson gave his birth date as October 26, 1894, stating that he lived in Dallas, Texas, and had been blind since birth. In the 1920 census, he is recorded as having returned to Freestone County and was living with his half-brother, Kit Banks, on a farm between Wortham and Streetman.

Jefferson began playing the guitar in his early teens and soon after he began performing at picnics and parties. He became a street musician, playing in East Texas towns in front of barbershops and on street corners. According to his cousin Alec Jefferson, quoted in the notes for "Blind Lemon Jefferson, Classic Sides":

In the early 1910s, Jefferson began traveling frequently to Dallas, where he met and played with the blues musician Lead Belly. Jefferson was one of the earliest and most prominent figures in the blues movement developing in the Deep Ellum section of Dallas. It is likely that he moved to Deep Ellum on a more permanent basis by 1917, where he met Aaron Thibeaux Walker, also known as T-Bone Walker. Jefferson taught Walker the basics of playing blues guitar in exchange for Walker's occasional services as a guide. By the early 1920s, Jefferson was earning enough money for his musical performances to support a wife and, possibly, a child. However, firm evidence of his marriage and children has not been found.

Prior to Jefferson, few artists had recorded solo voice and blues guitar, the first of which were the vocalist Sara Martin and the guitarist Sylvester Weaver, who recorded "Longing for Daddy Blues", probably on October 24, 1923. The first self-accompanied solo performer of a self-composed blues song was Lee Morse, whose "Mail Man Blues" was recorded on October 7, 1924. Jefferson's music is uninhibited and represented the classic sounds of everyday life, from a honky-tonk to a country picnic, to street corner blues, to work in the burgeoning oil fields (a reflection of his interest in mechanical objects and processes).

Jefferson did what few had ever done before him – he became a successful solo guitarist and male vocalist in the commercial recording world. Unlike many artists who were "discovered" and recorded in their normal venues, Jefferson was taken to Chicago, Illinois, in December 1925 or January 1926 to record his first tracks. Uncharacteristically, his first two recordings from this session were gospel songs ("I Want to Be Like Jesus in My Heart" and "All I Want Is That Pure Religion"), released under the name Deacon L. J. Bates. A second recording session was held in March 1926. His first releases under his own name, "Booster Blues" and "Dry Southern Blues", were hits. Their popularity led to the release of the other two songs from that session, "Got the Blues" and "Long Lonesome Blues", which became a runaway success, with sales in six figures. He recorded about 100 tracks between 1926 and 1929; 43 records were issued, all but one for Paramount Records. Paramount's studio techniques and quality were poor, and the recordings were released with poor sound quality. In May 1926, Paramount re-recorded Jefferson performing his hits "Got the Blues" and "Long Lonesome Blues" in the superior facilities at Marsh Laboratories, and subsequent releases used those versions. Both versions appear on compilation albums.

Largely because of the popularity of artists such as Jefferson and his contemporaries Blind Blake and Ma Rainey, Paramount became the leading recording company for the blues in the 1920s. Jefferson's earnings reputedly enabled him to buy a car and employ chauffeurs (this information has been disputed); he was given a Ford car "worth over $700" by Mayo Williams, Paramount's connection with the black community. This was a common compensation for recording rights in that market. Jefferson is known to have done an unusual amount of traveling for the time in the American South, which is reflected in the difficulty of placing his music in a single regional category.

Jefferson's "old-fashioned" sound and confident musicianship made it easy to market him. His skillful guitar playing and impressive vocal range opened the door for a new generation of male solo blues performers, such as Furry Lewis, Charlie Patton, and Barbecue Bob. He stuck to no musical conventions, varying his riffs and rhythm and singing complex and expressive lyrics in a manner exceptional at the time for a "simple country blues singer." According to the North Carolina musician Walter Davis, Jefferson played on the streets in Johnson City, Tennessee, during the early 1920s, at which time Davis and the entertainer Clarence Greene learned the art of blues guitar.

Jefferson was reputedly unhappy with his royalties (although Williams said that Jefferson had a bank account containing as much as $1500). In 1927, when Williams moved to Okeh Records, he took Jefferson with him, and Okeh quickly recorded and released Jefferson's "Matchbox Blues", backed with "Black Snake Moan". It was his only Okeh recording, probably because of contractual obligations with Paramount. Jefferson's two songs released on Okeh have considerably better sound quality than his Paramount records at the time. When he returned to Paramount a few months later, "Matchbox Blues" had already become such a hit that Paramount re-recorded and released two new versions, with the producer Arthur Laibly. In 1927, Jefferson recorded another of his classic songs, the haunting "See That My Grave Is Kept Clean" (again using the pseudonym Deacon L. J. Bates), and two other uncharacteristically spiritual songs, "He Arose from the Dead" and "Where Shall I Be". "See That My Grave Is Kept Clean" was so successful that it was re-recorded and re-released in 1928.

Jefferson died in Chicago at 10:00 a.m. on December 19, 1929, of what his death certificate said was "probably acute myocarditis". For many years, rumors circulated that a jealous lover had poisoned his coffee, but a more likely explanation is that he died of a heart attack after becoming disoriented during a snowstorm. Some have said that he died of a heart attack after being attacked by a dog in the middle of the night. The book "Tolbert's Texas" claimed that he was killed while being robbed of a large royalty payment by a guide escorting him to Union Station to catch a train home to Texas. Paramount Records paid for the return of his body to Texas by train, accompanied by the pianist William Ezell.

Jefferson was buried at Wortham Negro Cemetery (later Wortham Black Cemetery). Far from being kept clean, his grave was unmarked until 1967, when a Texas historical marker was erected in the general area of his plot, the precise location of which is unknown. By 1996, the cemetery and marker were in poor condition, and a new granite headstone was erected in 1997. In 2007, the cemetery's name was changed to Blind Lemon Memorial Cemetery, and his gravesite is kept clean by a cemetery committee in Wortham, Texas.

Jefferson had an intricate and fast style of guitar playing and a particularly high-pitched voice. He was a founder of the Texas blues sound and an important influence on other blues singers and guitarists, including Lead Belly and Lightnin' Hopkins.

He was the author of many songs covered by later musicians, including the classic "See That My Grave Is Kept Clean". Another of his songs, "Matchbox Blues", was recorded more than 30 years later by the Beatles, in a rockabilly version credited to Carl Perkins, who did not credit Jefferson on his 1955 recording.

The Rock and Roll Hall of Fame selected Jefferson's 1927 recording of "Matchbox Blues" as one of the 500 songs that shaped rock and roll. Jefferson was among the inaugural class of blues musicians inducted into the Blues Hall of Fame in 1980.








</doc>
<doc id="4571" url="https://en.wikipedia.org/wiki?curid=4571" title="Baku (mythology)">
Baku (mythology)

The Japanese term "baku" has two current meanings, referring to both the traditional dream-devouring creature and to the Malayan tapir. In recent years, there have been changes in how the baku is depicted.

The traditional Japanese nightmare-devouring baku originates in Chinese folklore and was familiar in Japan as early as the Muromachi period (14th-15th century). Hori Tadao has described the dream-eating abilities attributed to the traditional baku and relates them to other preventatives against nightmare such as amulets. Kaii-Yōkai Denshō Database, citing a 1957 paper, and Mizuki also describe the dream-devouring capacities of the traditional baku.

An early 17th-century Japanese manuscript, the "Sankai Ibutsu" (), describes the baku as a shy, Chinese mythical chimera with an elephant’s trunk, rhinoceros' eyes, an ox's tail, and a tiger's paws, which protected against pestilence and evil, although eating nightmares was not included among its abilities. However, in a 1791 Japanese wood-block illustration, a specifically dream-destroying baku is depicted with an elephant’s head, tusks, and trunk, with horns and tiger’s claws. The elephant’s head, trunk, and tusks are characteristic of baku portrayed in classical era (pre-Meiji) Japanese wood-block prints (see illustration) and in shrine, temple, and netsuke carvings.

Writing in the Meiji period, Lafcadio Hearn (1902) described a baku with very similar attributes that was also able to devour nightmares.
Legend has it, that a person who wakes up from a bad dream can call out to baku. A child having a nightmare in Japan will wake up and repeat three times, "Baku-san, come eat my dream." Legends say that the baku will come into the child’s room and devour the bad dream, allowing the child to go back to sleep peacefully. However, calling to the baku must be done sparingly, because if he remains hungry after eating one’s nightmare, he may also devour their hopes and desires as well, leaving them to live an empty life. The baku can also be summoned for protection from bad dreams prior to falling asleep at night. In the 1910s, it was common for Japanese children to keep a baku talisman at their bedside.





</doc>
