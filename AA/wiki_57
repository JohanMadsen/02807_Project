<doc id="7593" url="https://en.wikipedia.org/wiki?curid=7593" title="Calculator">
Calculator

An electronic calculator is typically a portable electronic device used to perform calculations, ranging from basic arithmetic to complex mathematics.

The first solid state electronic calculator was created in the early 1960s.

The pocket sized devices became available in the 1970s, especially after the first microprocessor, the Intel 4004, developed by Intel for the Japanese calculator company Busicom. They later became used commonly within the petroleum industry (oil and gas).

Modern electronic calculators vary: from cheap, give-away, credit-card-sized models to sturdy desktop models with built-in printers. They became popular in the mid-1970s (as integrated circuits made their size and cost small). By the end of that decade, calculator prices had reduced to a point where a basic calculator was affordable to most and they became common in schools.

Computer operating systems as far back as early Unix have included interactive calculator programs such as dc and hoc, and calculator functions are included in almost all personal digital assistant (PDA) type devices (save a few dedicated address book and dictionary devices).

In addition to general purpose calculators, there are those designed for specific markets. For example, there are scientific calculators which include trigonometric and statistical calculations. Some calculators even have the ability to do computer algebra. Graphing calculators can be used to graph functions defined on the real line, or higher-dimensional Euclidean space. , basic calculators cost little, but the scientific and graphing models tend to cost more.

In 1986, calculators still represented an estimated 41% of the world's general-purpose hardware capacity to compute information. By 2007, this diminished to less than 0.05%.

Electronic calculators contain a keyboard with buttons for digits and arithmetical operations; some even contain "00" and "000" buttons to make larger or smaller numbers easier to enter. Most basic calculators assign only one digit or operation on each button; however, in more specific calculators, a button can perform multi-function working with key combinations.

Calculators usually have liquid-crystal displays (LCD) as output in place of historical light-emitting diode (LED) displays and vacuum fluorescent displays (VFD); details are provided in the section "Technical improvements".

Large-sized figures are often used to improve readability; while using decimal separator (usually a point rather than a comma) instead of or in addition to vulgar fractions. Various symbols for function commands may also be shown on the display. Fractions such as are displayed as decimal approximations, for example rounded to . Also, some fractions (such as , which is ; to 14 significant figures) can be difficult to recognize in decimal form; as a result, many scientific calculators are able to work in vulgar fractions or mixed numbers.

Calculators also have the ability to store numbers into computer memory. Basic calculators usually store only one number at a time; more specific types are able to store many numbers represented in variables. The variables can also be used for constructing formulas. Some models have the ability to extend memory capacity to store more numbers; the extended memory address is termed an array index.

Power sources of calculators are: batteries, solar cells or mains electricity (for old models), turning on with a switch or button. Some models even have no turn-off button but they provide some way to put off (for example, leaving no operation for a moment, covering solar cell exposure, or closing their lid). Crank-powered calculators were also common in the early computer era.

The following keys are common to most pocket calculators. While the arrangement of the digits is standard, the positions of other keys vary from model to model; the illustration is an example.

In general, a basic electronic calculator consists of the following components:


Clock rate of a processor chip refers to the frequency at which the central processing unit (CPU) is running. It is used as an indicator of the processor's speed, and is measured in "clock cycles per second" or the SI unit hertz (Hz). For basic calculators, the speed can vary from a few hundred hertz to the kilohertz range.
A basic explanation as to how calculations are performed in a simple four-function calculator:

To perform the calculation , one presses keys in the following sequence on most calculators:     .

Other functions are usually performed using repeated additions or subtractions.

Most pocket calculators do all their calculations in BCD rather than a floating-point representation. BCD is common in electronic systems where a numeric value is to be displayed, especially in systems consisting solely of digital logic, and not containing a microprocessor. By employing BCD, the manipulation of numerical data for display can be greatly simplified by treating each digit as a separate single sub-circuit. This matches much more closely the physical reality of display hardware—a designer might choose to use a series of separate identical seven-segment displays to build a metering circuit, for example. If the numeric quantity were stored and manipulated as pure binary, interfacing to such a display would require complex circuitry. Therefore, in cases where the calculations are relatively simple, working throughout with BCD can lead to a simpler overall system than converting to and from binary.

The same argument applies when hardware of this type uses an embedded microcontroller or other small processor. Often, smaller code results when representing numbers internally in BCD format, since a conversion from or to binary representation can be expensive on such limited processors. For these applications, some small processors feature BCD arithmetic modes, which assist when writing routines that manipulate BCD quantities.

Where calculators have added functions (such as square root, or trigonometric functions), software algorithms are required to produce high precision results. Sometimes significant design effort is needed to fit all the desired functions in the limited memory space available in the calculator chip, with acceptable calculation time.

The fundamental difference between a calculator and computer is that a computer can be programmed in a way that allows the program to take different branches according to intermediate results, while calculators are pre-designed with specific functions (such as addition, multiplication, and logarithms) built in. The distinction is not clear-cut: some devices classed as programmable calculators have programming functions, sometimes with support for programming languages (such as RPL or TI-BASIC).

For instance, instead of a hardware multiplier, a calculator might implement floating point mathematics with code in read-only memory (ROM), and compute trigonometric functions with the CORDIC algorithm because CORDIC does not require much multiplication. Bit serial logic designs are more common in calculators whereas bit parallel designs dominate general-purpose computers, because a bit serial design minimizes chip complexity, but takes many more clock cycles. This distinction blurs with high-end calculators, which use processor chips associated with computer and embedded systems design, more so the Z80, MC68000, and ARM architectures, and some custom designs specialized for the calculator market.

The first known tools used to aid arithmetic calculations were: bones (used to tally items), pebbles, and counting boards, and the abacus, known to have been used by Sumerians and Egyptians before 2000 BC. Except for the Antikythera mechanism (an "out of the time" astronomical device), development of computing tools arrived near the start of the 17th century: the geometric-military compass (by Galileo), logarithms and Napier bones (by Napier), and the slide rule (by Edmund Gunter).
In 1642, the Renaissance saw the invention of the mechanical calculator (by Wilhelm Schickard and several decades later Blaise Pascal), a device that was at times somewhat over-promoted as being able to perform all four arithmetic operations with minimal human intervention. Pascal's Calculator could add and subtract two numbers directly and thus, if the tedium could be borne, multiply and divide by repetition. Schickard's machine, constructed several decades earlier, used a clever set of mechanised multiplication tables to ease the process of multiplication and division with the adding machine as a means of completing this operation. (Because they were different inventions with different aims a debate about whether Pascal or Schickard should be credited as the "inventor" of the adding machine (or calculating machine) is probably pointless.) Schickard and Pascal were followed by Gottfried Leibniz who spent forty years designing a four-operation mechanical calculator, the stepped reckoner, inventing in the process his leibniz wheel, but who couldn't design a fully operational machine. There were also five unsuccessful attempts to design a calculating clock in the 17th century.
The 18th century saw the arrival of some interesting improvements, first by Poleni with the first fully functional calculating clock and four-operation machine, but these machines were almost always "one of the kind". Luigi Torchi invented the first direct multiplication machine in 1834: this was also the second key-driven machine in the world, following that of James White (1822). It was not until the 19th century and the Industrial Revolution that real developments began to occur. Although machines capable of performing all four arithmetic functions existed prior to the 19th century, the refinement of manufacturing and fabrication processes during the eve of the industrial revolution made large scale production of more compact and modern units possible. The Arithmometer, invented in 1820 as a four-operation mechanical calculator, was released to production in 1851 as an adding machine and became the first commercially successful unit; forty years later, by 1890, about 2,500 arithmometers had been sold plus a few hundreds more from two arithmometer clone makers (Burkhardt, Germany, 1878 and Layton, UK, 1883) and Felt and Tarrant, the only other competitor in true commercial production, had sold 100 comptometers.

It wasn't until 1902 that the familiar push-button user interface was developed, with the introduction of the Dalton Adding Machine, developed by James L. Dalton in the United States.

In 1921, Edith Clarke invented the "Clarke calculator", a simple graph-based calculator for solving line equations involving hyperbolic functions. This allowed electrical engineers to simplify calculations for inductance and capacitance in power transmission lines.

The Curta calculator was developed in 1948 and, although costly, became popular for its portability. This purely mechanical hand-held device could do addition, subtraction, multiplication and division. By the early 1970s electronic pocket calculators ended manufacture of mechanical calculators, although the Curta remains a popular collectable item.

The first mainframe computers, using firstly vacuum tubes and later transistors in the logic circuits, appeared in the 1940s and 1950s. This technology was to provide a stepping stone to the development of electronic calculators.

The Casio Computer Company, in Japan, released the Model "14-A" calculator in 1957, which was the world's first all-electric (relatively) compact calculator. It did not use electronic logic but was based on relay technology, and was built into a desk.
In October 1961, the world's first "all-electronic desktop" calculator, the British Bell Punch/Sumlock Comptometer ANITA (A New Inspiration To Arithmetic/Accounting) was announced. This machine used vacuum tubes, cold-cathode tubes and Dekatrons in its circuits, with 12 cold-cathode "Nixie" tubes for its display. Two models were displayed, the Mk VII for continental Europe and the Mk VIII for Britain and the rest of the world, both for delivery from early 1962. The Mk VII was a slightly earlier design with a more complicated mode of multiplication, and was soon dropped in favour of the simpler Mark VIII. The ANITA had a full keyboard, similar to mechanical comptometers of the time, a feature that was unique to it and the later Sharp CS-10A among electronic calculators. The ANITA weighed roughly due to its large tube system. Bell Punch had been producing key-driven mechanical calculators of the comptometer type under the names "Plus" and "Sumlock", and had realised in the mid-1950s that the future of calculators lay in electronics. They employed the young graduate Norbert Kitz, who had worked on the early British Pilot ACE computer project, to lead the development. The ANITA sold well since it was the only electronic desktop calculator available, and was silent and quick.

The tube technology of the ANITA was superseded in June 1963 by the U.S. manufactured Friden EC-130, which had an all-transistor design, a stack of four 13-digit numbers displayed on a cathode ray tube (CRT), and introduced Reverse Polish Notation (RPN) to the calculator market for a price of $2200, which was about three times the cost of an electromechanical calculator of the time. Like Bell Punch, Friden was a manufacturer of mechanical calculators that had decided that the future lay in electronics. In 1964 more all-transistor electronic calculators were introduced: Sharp introduced the CS-10A, which weighed and cost 500,000 yen ($), and Industria Macchine Elettroniche of Italy introduced the IME 84, to which several extra keyboard and display units could be connected so that several people could make use of it (but apparently not at the same time).

There followed a series of electronic calculator models from these and other manufacturers, including Canon, Mathatronics, Olivetti, SCM (Smith-Corona-Marchant), Sony, Toshiba, and Wang. The early calculators used hundreds of germanium transistors, which were cheaper than silicon transistors, on multiple circuit boards. Display types used were CRT, cold-cathode Nixie tubes, and filament lamps. Memory technology was usually based on the delay line memory or the magnetic core memory, though the Toshiba "Toscal" BC-1411 appears to have used an early form of dynamic RAM built from discrete components. Already there was a desire for smaller and less power-hungry machines.

The Olivetti Programma 101 was introduced in late 1965; it was a stored program machine which could read and write magnetic cards and displayed results on its built-in printer. Memory, implemented by an acoustic delay line, could be partitioned between program steps, constants, and data registers. Programming allowed conditional testing and programs could also be overlaid by reading from magnetic cards. It is regarded as the first personal computer produced by a company (that is, a desktop electronic calculating machine programmable by non-specialists for personal use). The Olivetti Programma 101 won many industrial design awards.
Another calculator introduced in 1965 was Bulgaria's ELKA 6521, developed by the Central Institute for Calculation Technologies and built at the Elektronika factory in Sofia. The name derives from "ELektronen KAlkulator", and it weighed around . It is the first calculator in the world which includes the square root function. Later that same year were released the ELKA 22 (with a luminescent display) and the ELKA 25, with an in-built printer. Several other models were developed until the first pocket model, the ELKA 101, was released in 1974. The writing on it was in Roman script, and it was exported to western countries.

The "Monroe Epic" programmable calculator came on the market in 1967. A large, printing, desk-top unit, with an attached floor-standing logic tower, it could be programmed to perform many computer-like functions. However, the only "branch" instruction was an implied unconditional branch (GOTO) at the end of the operation stack, returning the program to its starting instruction. Thus, it was not possible to include any conditional branch (IF-THEN-ELSE) logic. During this era, the absence of the conditional branch was sometimes used to distinguish a programmable calculator from a computer.

The first handheld calculator was a prototype called "Cal Tech", whose development was led by Jack Kilby at Texas Instruments in 1967. It could add, multiply, subtract, and divide, and its output device was a paper tape.

The electronic calculators of the mid-1960s were large and heavy desktop machines due to their use of hundreds of transistors on several circuit boards with a large power consumption that required an AC power supply. There were great efforts to put the logic required for a calculator into fewer and fewer integrated circuits (chips) and calculator electronics was one of the leading edges of semiconductor development. U.S. semiconductor manufacturers led the world in large scale integration (LSI) semiconductor development, squeezing more and more functions into individual integrated circuits. This led to alliances between Japanese calculator manufacturers and U.S. semiconductor companies: Canon Inc. with Texas Instruments, Hayakawa Electric (later renamed Sharp Corporation) with North-American Rockwell Microelectronics (later renamed Rockwell International), Busicom with Mostek and Intel, and General Instrument with Sanyo.

By 1970, a calculator could be made using just a few chips of low power consumption, allowing portable models powered from rechargeable batteries. The first portable calculators appeared in Japan in 1970, and were soon marketed around the world. These included the Sanyo ICC-0081 "Mini Calculator", the Canon Pocketronic, and the Sharp QT-8B "micro Compet". The Canon Pocketronic was a development of the "Cal-Tech" project which had been started at Texas Instruments in 1965 as a research project to produce a portable calculator. The Pocketronic has no traditional display; numerical output is on thermal paper tape. As a result of the "Cal-Tech" project, Texas Instruments was granted master patents on portable calculators.

Sharp put in great efforts in size and power reduction and introduced in January 1971 the Sharp EL-8, also marketed as the Facit 1111, which was close to being a pocket calculator. It weighed 1.59 pounds (721 grams), had a vacuum fluorescent display, rechargeable NiCad batteries, and initially sold for US $395.

However, the efforts in integrated circuit development culminated in the introduction in early 1971 of the first "calculator on a chip", the MK6010 by Mostek, followed by Texas Instruments later in the year. Although these early hand-held calculators were very costly, these advances in electronics, together with developments in display technology (such as the vacuum fluorescent display, LED, and LCD), led within a few years to the cheap pocket calculator available to all.

In 1971 Pico Electronics. and General Instrument also introduced their first collaboration in ICs, a full single chip calculator IC for the Monroe Royal Digital III calculator. Pico was a spinout by five GI design engineers whose vision was to create single chip calculator ICs. Pico and GI went on to have significant success in the burgeoning handheld calculator market.

The first truly pocket-sized electronic calculator was the Busicom LE-120A "HANDY", which was marketed early in 1971. Made in Japan, this was also the first calculator to use an LED display, the first hand-held calculator to use a single integrated circuit (then proclaimed as a "calculator on a chip"), the Mostek MK6010, and the first electronic calculator to run off replaceable batteries. Using four AA-size cells the LE-120A measures .

The first European-made pocket-sized calculator, DB 800 is made in May 1971 by Digitron in Buje, Croatia (former Yugoslavia) with four functions and an eight-digit display and special characters for a negative number and a warning that the calculation has too many digits to display.

The first American-made pocket-sized calculator, the Bowmar 901B (popularly termed "The Bowmar Brain"), measuring , came out in the Autumn of 1971, with four functions and an eight-digit red LED display, for $240, while in August 1972 the four-function Sinclair Executive became the first slimline pocket calculator measuring and weighing . It retailed for around £79 ($). By the end of the decade, similar calculators were priced less than £5 ($).

The first Soviet Union made pocket-sized calculator, the "Elektronika B3-04" was developed by the end of 1973 and sold at the start of 1974.

One of the first low-cost calculators was the Sinclair Cambridge, launched in August 1973. It retailed for £29.95 ($), or £5 ($) less in kit form. The Sinclair calculators were successful because they were far cheaper than the competition; however, their design led to slow and inaccurate computations of transcendental functions.

Meanwhile, Hewlett-Packard (HP) had been developing a pocket calculator. Launched in early 1972, it was unlike the other basic four-function pocket calculators then available in that it was the first pocket calculator with "scientific" functions that could replace a slide rule. The $395 HP-35, along with nearly all later HP engineering calculators, used reverse Polish notation (RPN), also called postfix notation. A calculation like "8 plus 5" is, using RPN, performed by pressing , , , and ; instead of the algebraic infix notation: , , , . It had 35 buttons and was based on Mostek Mk6020 chip.

The first Soviet "scientific" pocket-sized calculator the "B3-18" was completed by the end of 1975.

In 1973, Texas Instruments (TI) introduced the SR-10, ("SR" signifying slide rule) an "algebraic entry" pocket calculator using scientific notation for $150. Shortly after the SR-11 featured an added key for entering Pi (π). It was followed the next year by the SR-50 which added log and trig functions to compete with the HP-35, and in 1977 the mass-marketed TI-30 line which is still produced.

In 1978 a new company, Calculated Industries arose which focused on specialized markets. Their first calculator, the Loan Arranger (1978) was a pocket calculator marketed to the Real Estate industry with preprogrammed functions to simplify the process of calculating payments and future values. In 1985, CI launched a calculator for the construction industry called the Construction Master which came preprogrammed with common construction calculations (such as angles, stairs, roofing math, pitch, rise, run, and feet-inch fraction conversions). This would be the first in a line of construction related calculators.

The first desktop "programmable calculators" were produced in the mid-1960s by Mathatronics and Casio (AL-1000). These machines were very heavy and costly. The first programmable pocket calculator was the HP-65, in 1974; it had a capacity of 100 instructions, and could store and retrieve programs with a built-in magnetic card reader. Two years later the HP-25C introduced "continuous memory", i.e., programs and data were retained in CMOS memory during power-off. In 1979, HP released the first "alphanumeric", programmable, "expandable" calculator, the HP-41C. It could be expanded with random access memory (RAM, for memory) and read-only memory (ROM, for software) modules, and peripherals like bar code readers, microcassette and floppy disk drives, paper-roll thermal printers, and miscellaneous communication interfaces (RS-232, HP-IL, HP-IB).

The first Soviet programmable desktop calculator ISKRA 123, powered by the power grid, was released at the start of the 1970s. The first Soviet pocket battery-powered programmable calculator, Elektronika "B3-21", was developed by the end of 1976 and released at the start of 1977. The successor of B3-21, the Elektronika B3-34 wasn't backward compatible with B3-21, even if it kept the reverse Polish notation (RPN). Thus B3-34 defined a new command set, which later was used in a series of later programmable Soviet calculators. Despite very limited abilities (98 bytes of instruction memory and about 19 stack and addressable registers), people managed to write all kinds of programs for them, including adventure games and libraries of calculus-related functions for engineers. Hundreds, perhaps thousands, of programs were written for these machines, from practical scientific and business software, which were used in real-life offices and labs, to fun games for children. The Elektronika MK-52 calculator (using the extended B3-34 command set, and featuring internal EEPROM memory for storing programs and external interface for EEPROM cards and other periphery) was used in Soviet spacecraft program (for Soyuz TM-7 flight) as a backup of the board computer.

This series of calculators was also noted for a large number of highly counter-intuitive mysterious undocumented features, somewhat similar to "synthetic programming" of the American HP-41, which were exploited by applying normal arithmetic operations to error messages, jumping to nonexistent addresses and other methods. A number of respected monthly publications, including the popular science magazine "Nauka i Zhizn" ("Наука и жизнь", "Science and Life"), featured special columns, dedicated to optimization methods for calculator programmers and updates on undocumented features for hackers, which grew into a whole esoteric science with many branches, named "yeggogology" ("еггогология"). The error messages on those calculators appear as a Russian word "YEGGOG" ("ЕГГОГ") which, unsurprisingly, is translated to "Error".

A similar hacker culture in the USA revolved around the HP-41, which was also noted for a large number of undocumented features and was much more powerful than B3-34.

Through the 1970s the hand-held electronic calculator underwent rapid development. The red LED and blue/green vacuum fluorescent displays consumed a lot of power and the calculators either had a short battery life (often measured in hours, so rechargeable nickel-cadmium batteries were common) or were large so that they could take larger, higher capacity batteries. In the early 1970s liquid-crystal displays (LCDs) were in their infancy and there was a great deal of concern that they only had a short operating lifetime. Busicom introduced the Busicom "LE-120A "HANDY"" calculator, the first pocket-sized calculator and the first with an LED display, and announced the Busicom "LC" with LCD. However, there were problems with this display and the calculator never went on sale. The first successful calculators with LCDs were manufactured by Rockwell International and sold from 1972 by other companies under such names as: Dataking "LC-800", Harden "DT/12", Ibico "086", Lloyds "40", Lloyds "100", Prismatic "500" (a.k.a. "P500"), Rapid Data "Rapidman 1208LC". The LCDs were an early form using the "Dynamic Scattering Mode DSM" with the numbers appearing as bright against a dark background. To present a high-contrast display these models illuminated the LCD using a filament lamp and solid plastic light guide, which negated the low power consumption of the display. These models appear to have been sold only for a year or two.

A more successful series of calculators using a reflective DSM-LCD was launched in 1972 by Sharp Inc with the Sharp "EL-805", which was a slim pocket calculator. This, and another few similar models, used Sharp's "Calculator On Substrate" (COS) technology. An extension of one glass plate needed for the liquid crystal display was used as a substrate to mount the needed chips based on a new hybrid technology. The COS technology may have been too costly since it was only used in a few models before Sharp reverted to conventional circuit boards.
In the mid-1970s the first calculators appeared with field-effect, "twisted nematic" (TN) LCDs with dark numerals against a grey background, though the early ones often had a yellow filter over them to cut out damaging ultraviolet rays. The advantage of LCDs is that they are passive light modulators reflecting light, which require much less power than light-emitting displays such as LEDs or VFDs. This led the way to the first credit-card-sized calculators, such as the Casio "Mini Card LC-78" of 1978, which could run for months of normal use on button cells.

There were also improvements to the electronics inside the calculators. All of the logic functions of a calculator had been squeezed into the first "calculator on a chip" integrated circuits (ICs) in 1971, but this was leading edge technology of the time and yields were low and costs were high. Many calculators continued to use two or more ICs, especially the scientific and the programmable ones, into the late 1970s.

The power consumption of the integrated circuits was also reduced, especially with the introduction of CMOS technology. Appearing in the Sharp "EL-801" in 1972, the transistors in the logic cells of CMOS ICs only used any appreciable power when they changed state. The LED and VFD displays often required added driver transistors or ICs, whereas the LCDs were more amenable to being driven directly by the calculator IC itself.

With this low power consumption came the possibility of using solar cells as the power source, realised around 1978 by calculators such as the Royal "Solar 1", Sharp "EL-8026", and Teal "Photon".

At the start of the 1970s, hand-held electronic calculators were very costly, at two or three weeks' wages, and so were a luxury item. The high price was due to their construction requiring many mechanical and electronic components which were costly to produce, and production runs that were too small to exploit economies of scale. Many firms saw that there were good profits to be made in the calculator business with the margin on such high prices. However, the cost of calculators fell as components and their production methods improved, and the effect of economies of scale was felt.

By 1976, the cost of the cheapest four-function pocket calculator had dropped to a few dollars, about 1/20th of the cost five years before. The results of this were that the pocket calculator was affordable, and that it was now difficult for the manufacturers to make a profit from calculators, leading to many firms dropping out of the business or closing down. The firms that survived making calculators tended to be those with high outputs of higher quality calculators, or producing high-specification scientific and programmable calculators.

The first calculator capable of symbolic computing was the HP-28C, released in 1987. It could, for example, solve quadratic equations symbolically. The first graphing calculator was the Casio fx-7000G released in 1985.

The two leading manufacturers, HP and TI, released increasingly feature-laden calculators during the 1980s and 1990s. At the turn of the millennium, the line between a graphing calculator and a handheld computer was not always clear, as some very advanced calculators such as the TI-89, the Voyage 200 and HP-49G could differentiate and integrate functions, solve differential equations, run word processing and PIM software, and connect by wire or IR to other calculators/computers.

The HP 12c financial calculator is still produced. It was introduced in 1981 and is still being made with few changes. The HP 12c featured the reverse Polish notation mode of data entry. In 2003 several new models were released, including an improved version of the HP 12c, the "HP 12c platinum edition" which added more memory, more built-in functions, and the addition of the algebraic mode of data entry.

Calculated Industries competed with the HP 12c in the mortgage and real estate markets by differentiating the key labeling; changing the “I”, “PV”, “FV” to easier labeling terms such as "Int", "Term", "Pmt", and not using the reverse Polish notation. However, CI's more successful calculators involved a line of construction calculators, which evolved and expanded in the 1990s to present. According to Mark Bollman, a mathematics and calculator historian and associate professor of mathematics at Albion College, the "Construction Master is the first in a long and profitable line of CI construction calculators" which carried them through the 1980s, 1990s, and to the present.

Personal computers often come with a calculator utility program that emulates the appearance and functions of a calculator, using the graphical user interface to portray a calculator. One such example is Windows Calculator. Most personal data assistants (PDAs) and smartphones also have such a feature.

In most countries, students use calculators for schoolwork. There was some initial resistance to the idea out of fear that basic or elementary arithmetic skills would suffer. There remains disagreement about the importance of the ability to perform calculations "in the head", with some curricula restricting calculator use until a certain level of proficiency has been obtained, while others concentrate more on teaching estimation methods and problem-solving. Research suggests that inadequate guidance in the use of calculating tools can restrict the kind of mathematical thinking that students engage in. Others have argued that calculator use can even cause core mathematical skills to atrophy, or that such use can prevent understanding of advanced algebraic concepts. In December 2011 the UK's Minister of State for Schools, Nick Gibb, voiced concern that children can become "too dependent" on the use of calculators. As a result, the use of calculators is to be included as part of a review of the Curriculum. In the United States, many math educators and boards of education enthusiastically endorsed the National Council of Teachers of Mathematics (NCTM) standards and actively promoted the use of classroom calculators from kindergarten through high school.






</doc>
<doc id="7594" url="https://en.wikipedia.org/wiki?curid=7594" title="Cash register">
Cash register

A cash register, also referred to as a till in the United Kingdom and other Commonwealth countries, is a mechanical or electronic device for registering and calculating transactions at a point of sale. It is usually attached to a drawer for storing cash and other valuables. The cash register is also usually attached to a printer, that can print out receipts for record keeping purposes.

An early mechanical cash register was invented by James Ritty and John Birch following the American Civil War. James was the owner of a saloon in Dayton, Ohio, USA, and wanted to stop employees from pilfering his profits. The Ritty Model I was invented in 1879 after seeing a tool that counted the revolutions of the propeller on a steamship. With the help of James' brother John Ritty, they patented it in 1883. It was called "Ritty's Incorruptible Cashier" and it was invented for the purpose to stop cashiers of pilfering and eliminating employee theft or embezzlement.

Early mechanical registers were entirely mechanical, without receipts. The employee was required to ring up every transaction on the register, and when the total key was pushed, the drawer opened and a bell would ring, alerting the manager to a sale taking place. Those original machines were nothing but simple adding machines.

Since the registration is done with the process of returning change, according to Bill Bryson odd pricing came about because by charging odd amounts like 49 and 99 cents (or 45 and 95 cents when nickels are more used than pennies), the cashier very probably had to open the till for the penny change and thus announce the sale.
Shortly after the patent, Ritty became overwhelmed with the responsibilities of running two businesses, so he sold all of his interests in the cash register business to Jacob H. Eckert of Cincinnati, a china and glassware salesman, who formed the National Manufacturing Company. In 1884 Eckert sold the company to John H. Patterson, who renamed the company the National Cash Register Company and improved the cash register by adding a paper roll to record sales transactions, thereby creating the journal for internal bookkeeping purposes, and the receipt for external bookkeeping purposes. The original purpose of the receipt was enhanced fraud protection. The business owner could read the receipts to ensure that cashiers charged customers the correct amount for each transaction and did not embezzle the cash drawer. It also prevents a customer from defrauding the business by falsely claiming receipt of a lesser amount of change or a transaction that never happened in the first place. The first evidence of an actual cash register was used in Coalton, Ohio, at the old mining company.

In 1906, while working at the National Cash Register company, inventor Charles F. Kettering designed a cash register with an electric motor.
A leading designer, builder, manufacturer, seller and exporter of cash registers from the 1950s until the 1970s was London-based (and later Brighton-based) Gross Cash Registers Ltd., founded by brothers Sam and Henry Gross. Their cash registers were particularly popular around the time of decimalisation in Britain in early 1971, Henry having designed one of the few known models of cash register which could switch currencies from £sd to £p so that retailers could easily change from one to the other on or after Decimal Day. Sweda also had decimal-ready registers where the retailer used a special key on Decimal Day for the conversion.

In some jurisdictions the law also requires customers to collect the receipt and keep it at least for a short while after leaving the shop, again to check that the shop records sales, so that it cannot evade sales taxes.

Often cash registers are attached to scales, barcode scanners, checkstands, and debit card or credit card terminals. Increasingly, dedicated cash registers are being replaced with general purpose computers with POS software. Cash registers use bitmap characters for printing.

Today, point of sale systems scan the barcode (usually EAN or UPC) for each item, retrieve the price from a database, calculate deductions for items on sale (or, in British retail terminology, "special offer", "multibuy" or "buy one, get one free"), calculate the sales tax or VAT, calculate differential rates for preferred customers, actualize inventory, time and date stamp the transaction, record the transaction in detail including each item purchased, record the method of payment, keep totals for each product or type of product sold as well as total sales for specified periods, and do other tasks as well. These POS terminals will often also identify the cashier on the receipt, and carry additional information or offers.

Currently, many cash registers are individual computers. They may be running traditionally in-house software or general purpose software such as DOS. Many of the newer ones have touch screens. They may be connected to computerized point of sale networks using any type of protocol. Such systems may be accessed remotely for the purpose of obtaining records or troubleshooting. Many businesses also use tablet computers as cash registers, utilizing the sale system as downloadable app-software.

Cash registers include a key labeled "No Sale", abbreviated "NS" on many modern electronic cash registers. Its function is to open the drawer, printing a receipt stating "No Sale" and recording in the register log that the register was opened. Some cash registers require a numeric password or physical key to be used when attempting to open the till.

A cash register's drawer can only be opened by an instruction from the cash register except when using special keys, generally held by the owner and some employees (e.g. manager). This reduces the amount of contact most employees have with cash and other valuables. It also reduces risks of an employee taking money from the drawer without a record and the owner's consent, such as when a customer does not expressly ask for a receipt but still has to be given change (cash is more easily checked against recorded sales than inventory).

A cash drawer is usually a compartment underneath a cash register in which the cash from transactions is kept. The drawer typically contains a removable till. The till is usually a plastic or wooden tray divided into compartments used to store each denomination of bank notes and coins separately in order to make counting easier. The removable till allows money to be removed from the sales floor to a more secure location for counting and creating bank deposits. Some modern cash drawers are individual units separate from the rest of the cash register.

A cash drawer is usually of strong construction and may be integral with the register or a separate piece that the register sits atop. It slides in and out of its lockable box and is secured by a spring-loaded catch. When a transaction that involves cash is completed, the register sends an electrical impulse to a solenoid to release the catch and open the drawer.
Cash drawers that are integral to a stand-alone register often have a manual release catch underneath to open the drawer in the event of a power failure. More advanced cash drawers have eliminated the manual release in favor of a cylinder lock, requiring a key to manually open the drawer. The cylinder lock usually has several positions: locked, unlocked, online (will open if an impulse is given), and release. The release position is an intermittent position with a spring to push the cylinder back to the unlocked position. In the "locked" position, the drawer will remain latched even when an electric impulse is sent to the solenoid.

Due to the increasing number of notes and varieties of notes, many cash drawers are designed to store notes upright & facing forward, instead of the traditional flat & facing upright position. This enables faster access to each note and allows more varieties of notes to be stored. Sometimes the cashier will even divide the notes without any physical divider at all. Some cash drawers are flip top in design, where they flip open instead of sliding out like an ordinary drawer, resembling a cashbox instead.

Registers will typically feature a numerical pad, QWERTY or custom keyboard, touch screen interface, or a combination of these input methods for the cashier to enter products and fees by hand and access information necessary to complete the sale. For older registers as well as at restaurants and other establishments that do not sell barcoded items, the manual input may be the only method of interacting with the register. While customization was previously limited to larger chains that could afford to have physical keyboards custom-built for their needs, the customization of register inputs is now more widespread with the use of touch screens that can display a variety of point of sale software.

Modern cash registers may be connected to a handheld or stationary barcode reader so that a customer's purchases can be more rapidly scanned than would be possible by keying numbers into the register by hand. The use of scanners should also help prevent errors that result from manually entering the product's barcode or pricing. At grocers, the register's scanner may be combined with a scale for measuring product that is sold by weight.

Cashiers are often required to provide a receipt to the customer after a purchase has been made. Registers typically use thermal printers to print receipts, although older dot matrix printers are still in use at some retailers. Alternatively, retailers can forgo issuing paper receipts in some jurisdictions by instead asking the customer for an email to which their receipt can be sent. The receipts of larger retailers tend to include unique barcodes or other information identifying the transaction so that the receipt can be scanned to facilitate returns or other customer services.

In stores that use electronic article surveillance, a pad or other surface will be attached to the register that deactivates security devices embedded in or attached to the items being purchased. This will prevent a customer's purchase from setting off security alarms at the store's exit.

Some corporations and supermarkets have introduced self-checkout machines, where the customer is trusted to scan the barcodes (or manually identify uncoded items like fruit), and place the items into a bagging area. The bag is weighed, and the machine halts the checkout when the weight of something in the bag does not match the weight in the inventory database. Normally, an employee is watching over several such checkouts to prevent theft or exploitation of the machines' weaknesses (for example, intentional misidentification of expensive produce or dry goods). Payment on these machines is accepted by debit card/credit card, or cash via coin slot and bank note scanner. Store employees are also needed to authorize "age-restricted" purchases, such as alcohol, solvents or knives, which can either be done remotely by the employee observing the self-checkout, or by means of a "store login" which the operator has to enter.




</doc>
<doc id="7595" url="https://en.wikipedia.org/wiki?curid=7595" title="Chronometer">
Chronometer

Chronometer or chronoscope may refer to:



</doc>
<doc id="7597" url="https://en.wikipedia.org/wiki?curid=7597" title="Processor design">
Processor design

Processor design is the design engineering task of creating a processor, a component of computer hardware. It is a subfield of computer engineering (design, development and implementation) and electronics engineering (fabrication). The design process involves choosing an instruction set and a certain execution paradigm (e.g. VLIW or RISC) and results in a microarchitecture, which might be described in e.g. VHDL or Verilog. For microprocessor design, this description is then manufactured employing some of the various semiconductor device fabrication processes, resulting in a die which is bonded onto a chip carrier. This chip carrier is then soldered onto, or inserted into a socket on, a printed circuit board (PCB).

The mode of operation of any processor is the execution of lists of instructions. Instructions typically include those to compute or manipulate data values using registers, change or retrieve values in read/write memory, perform relational tests between data values and to control program flow.

CPU design focuses on six main areas:

CPUs designed for high-performance markets might require custom designs for each of these items to achieve frequency, power-dissipation, and chip-area goals whereas CPUs designed for lower performance markets might lessen the implementation burden by acquiring some of these items by purchasing them as intellectual property. Control logic implementation techniques (logic synthesis using CAD tools) can be used to implement datapaths, register files, and clocks. Common logic styles used in CPU design include unstructured random logic, finite-state machines, microprogramming (common from 1965 to 1985), and Programmable logic arrays (common in the 1980s, no longer common).

Device types used to implement the logic include:

A CPU design project generally has these major tasks:

Re-designing a CPU core to a smaller die-area helps to shrink everything (a "photomask shrink"), resulting in the same number of transistors on a smaller die. It improves performance (smaller transistors switch faster), reduces power (smaller wires have less parasitic capacitance) and reduces cost (more CPUs fit on the same wafer of silicon). Releasing a CPU on the same size die, but with a smaller CPU core, keeps the cost about the same but allows higher levels of integration within one very-large-scale integration chip (additional cache, multiple CPUs or other components), improving performance and reducing overall system cost.

As with most complex electronic designs, the logic verification effort (proving that the design does
not have bugs) now dominates the project schedule of a CPU.

Key CPU architectural innovations include index register, cache, virtual memory, instruction pipelining, superscalar, CISC, RISC, virtual machine, emulators, microprogram, and stack.

A variety of have been proposed,
including reconfigurable logic, clockless CPUs, computational RAM, and optical computing.

Benchmarking is a way of testing CPU speed. Examples include SPECint and SPECfp, developed by Standard Performance Evaluation Corporation, and ConsumerMark developed by the Embedded Microprocessor Benchmark Consortium EEMBC.

Some of the commonly used metrics include:

There may be tradeoffs in optimizing some of these metrics. In particular, many design techniques that make a CPU run faster make the "performance per watt", "performance per dollar", and "deterministic response" much worse, and vice versa.

There are several different markets in which CPUs are used. Since each of these markets differ in their requirements for CPUs, the devices designed for one market are in most cases inappropriate for the other markets.

The vast majority of revenues generated from CPU sales is for general purpose computing, that is, desktop, laptop, and server computers commonly used in businesses and homes. In this market, the Intel IA-32 and the 64-bit version x86-64 architecture dominate the market, with its rivals PowerPC and SPARC maintaining much smaller customer bases. Yearly, hundreds of millions of IA-32 architecture CPUs are used by this market. A growing percentage of these processors are for mobile implementations such as netbooks and laptops.

Since these devices are used to run countless different types of programs, these CPU designs are not specifically targeted at one type of application or one function. The demands of being able to run a wide range of programs efficiently has made these CPU designs among the more advanced technically, along with some disadvantages of being relatively costly, and having high power consumption.

In 1984, most high-performance CPUs required four to five years to develop.

Scientific computing is a much smaller niche market (in revenue and units shipped). It is used in government research labs and universities. Before 1990, CPU design was often done for this market, but mass market CPUs organized into large clusters have proven to be more affordable. The main remaining area of active hardware design and research for scientific computing is for high-speed data transmission systems to connect mass market CPUs.

As measured by units shipped, most CPUs are embedded in other machinery, such as telephones, clocks, appliances, vehicles, and infrastructure. Embedded processors sell in the volume of many billions of units per year, however, mostly at much lower price points than that of the general purpose processors.

These single-function devices differ from the more familiar general-purpose CPUs in several ways:

The embedded CPU family with the largest number of total units shipped is the 8051, averaging nearly a billion units per year. The 8051 is widely used because it is very inexpensive. The design time is now roughly zero, because it is widely available as commercial intellectual property. It is now often embedded as a small part of a larger system on a chip. The silicon cost of an 8051 is now as low as US$0.001, because some implementations use as few as 2,200 logic gates and take 0.0127 square millimeters of silicon.

As of 2009, more CPUs are produced using the ARM architecture instruction set than any other 32-bit instruction set.
The ARM architecture and the first ARM chip were designed in about one and a half years and 5 human years of work time.

The 32-bit Parallax Propeller microcontroller architecture and the first chip were designed by two people in about 10 human years of work time.

The 8-bit AVR architecture and first AVR microcontroller was conceived and designed by two students at the Norwegian Institute of Technology.

The 8-bit 6502 architecture and the first MOS Technology 6502 chip were designed in 13 months by a group of about 9 people.

The 32 bit Berkeley RISC I and RISC II architecture and the first chips were mostly designed by a series of students as part of a four quarter sequence of graduate courses.
This design became the basis of the commercial SPARC processor design.

For about a decade, every student taking the 6.004 class at MIT was part of a team—each team had one semester to design and build a simple 8 bit CPU out of 7400 series integrated circuits.
One team of 4 students designed and built a simple 32 bit CPU during that semester.

Some undergraduate courses require a team of 2 to 5 students to design, implement, and test a simple CPU in a FPGA in a single 15-week semester.

The MultiTitan CPU was designed with 2.5 man years of effort, which was considered "relatively little design effort" at the time.
24 people contributed to the 3.5 year MultiTitan research project, which included designing and building a prototype CPU.

For embedded systems, the highest performance levels are often not needed or desired due to the power consumption requirements. This allows for the use of processors which can be totally implemented by logic synthesis techniques. These synthesized processors can be implemented in a much shorter amount of time, giving quicker time-to-market.




</doc>
<doc id="7598" url="https://en.wikipedia.org/wiki?curid=7598" title="Carinatae">
Carinatae

Carinatae is the group of all birds and their extinct relatives to possess a keel, or "carina", on the underside of the breastbone used to anchor large flight muscles.

Traditionally, Carinatae were defined as all birds whose sternum (breast bone) has a keel ("carina"). The keel is a strong median ridge running down the length of the sternum. This is an important area for the attachment of flight muscles. Thus, all flying birds have a pronounced keel. Ratites, all of which are flightless, lack a strong keel. Thus, living birds were divided into carinates (keeled) and ratites (from "ratis", "raft", referring to the flatness of the sternum). The difficulty with this scheme phylogenetically was that some flightless birds, without strong keels, are descended directly from ordinary flying birds possessing one. Examples include the kakapo, a flightless parrot, and the dodo, a columbiform (the pigeon family). None of these birds is a ratite. Thus, this supposedly distinctive feature was easy to use, but had nothing to do with actual phylogenic relationship.

Beginning in the 1980s, Carinatae was given several phylogenetic definitions. The first was as a node-based clade uniting "Ichthyornis" with modern birds. However, in many analyses, this definition would be synonymous with the more widely used name Ornithurae. An alternate definition was provided in 2001, naming Carinatae an apomorphy-based clade defined by the presence of a keeled sternum.

The most primitive known bird relative with a keeled breastbone is "Confuciusornis". While some specimens of this stem-bird have flat breastbones, some show a small ridge that could have supported a cartilaginous keel.


</doc>
<doc id="7599" url="https://en.wikipedia.org/wiki?curid=7599" title="Cocktail">
Cocktail

When used to refer to any generic alcoholic mixed drink, cocktail may mean any beverage that contains three or more ingredients if at least one of those ingredients contains alcohol.

A cocktail more specifically may mean a beverage with at least three flavors, one of which is alcohol. More specifically still, it must contain alcohol, a sugar, and a bitter/citrus. When a mixed drink contains only a distilled spirit and a mixer, such as soda or fruit juice, it is a highball; many of the International Bartenders Association Official Cocktails are highballs. When a mixed drink contains only a distilled spirit and a liqueur, it is a duo and when it adds a mixer, it is a trio. Additional ingredients may be sugar, honey, milk, cream, and various herbs.
The origin of the word "cocktail" is disputed. The first recorded use of "cocktail" not referring to a horse is found in "The Morning Post and Gazetteer" in London, England, March 20, 1798:

<poem>
Mr. Pitt,
two petit vers of "L'huile de Venus"
Ditto, one of "perfeit amour"
Ditto, "cock-tail" (vulgarly called ginger)
</poem>

"The Oxford English Dictionary" cites the word as originating in the U.S. The first recorded use of "cocktail" as a beverage (possibly non-alcoholic) in the United States appears in "The Farmer's Cabinet", April 28, 1803:

The first definition of "cocktail" known to be an alcoholic beverage appeared in "The Balance and Columbian Repository" (Hudson, New York) May 13, 1806; editor Harry Croswell answered the question, "What is a cocktail?":

Etymologist Anatoly Liberman endorses as "highly probable" the theory advanced by Låftman (1946), which Liberman summarizes as follows:

In his book "Imbibe!" (2007) David Wondrich also speculates that "cocktail" is a reference to a practice for perking up an old horse by means of a ginger suppository so that the animal would "cock its tail up and be frisky."

Several authors have theorized that "cocktail" may be a corruption of "cock ale".

There is a lack of clarity on the origins of cocktails. Traditionally cocktails were a mixture of spirits, sugar, water, and bitters. But by the 1860s, a cocktail frequently included a liqueur.

The first publication of a bartenders' guide which included cocktail recipes was in 1862 – "How to Mix Drinks; or, The Bon Vivant's Companion", by "Professor" Jerry Thomas. In addition to recipes for punches, sours, slings, cobblers, shrubs, toddies, flips, and a variety of other mixed drinks were 10 recipes for "cocktails". A key ingredient differentiating cocktails from other drinks in this compendium was the use of bitters. Mixed drinks popular today that conform to this original meaning of "cocktail" include the Old Fashioned whiskey cocktail, the Sazerac cocktail, and the Manhattan cocktail.
The ingredients listed (spirits, sugar, water, and bitters) match the ingredients of an Old Fashioned, which originated as a term used by late 19th century bar patrons to distinguish cocktails made the "old-fashioned" way from newer, more complex cocktails.

The term highball appears during the 1890s to distinguish a drink composed only of a distilled spirit and a mixer.

The first "cocktail party" ever thrown was allegedly by Mrs. Julius S. Walsh Jr. of St. Louis, Missouri, in May 1917. Mrs. Walsh invited 50 guests to her home at noon on a Sunday. The party lasted an hour, until lunch was served at 1 pm. The site of this first cocktail party still stands. In 1924, the Roman Catholic Archdiocese of St. Louis bought the Walsh mansion at 4510 Lindell Boulevard, and it has served as the local archbishop's residence ever since.

During Prohibition in the United States (1919–1933), when alcoholic beverages were illegal, cocktails were still consumed illegally in establishments known as speakeasies. The quality of liquor available during Prohibition was much worse than previously. There was a shift from whiskey to gin, which does not require aging and is therefore easier to produce illicitly. Honey, fruit juices, and other flavorings served to mask the foul taste of the inferior liquors. Sweet cocktails were easier to drink quickly, an important consideration when the establishment might be raided at any moment.

Cocktails became less popular in the late 1960s and through the 1970s, until resurging in the 1980s with vodka often substituting the original gin in drinks such as the martini. Traditional cocktails began to make a comeback in the 2000s, and by the mid-2000s there was a renaissance of cocktail culture in a style typically referred to as mixology that draws on traditional cocktails for inspiration but utilizes novel ingredients and often complex flavors.

Lists
Devices for producing and imbibing
Media



</doc>
<doc id="7601" url="https://en.wikipedia.org/wiki?curid=7601" title="Coptic Orthodox Church of Alexandria">
Coptic Orthodox Church of Alexandria

The Coptic Orthodox Church of Alexandria (Coptic: Ϯⲉⲕ̀ⲕⲗⲏⲥⲓⲁ ̀ⲛⲣⲉⲙ̀ⲛⲭⲏⲙⲓ ⲛⲟⲣⲑⲟⲇⲟⲝⲟⲥ, "ti.eklyseya en.remenkimi en.orthodoxos", literally: the Egyptian Orthodox Church) is an Oriental Orthodox Christian church based in Egypt, Northeast Africa and the Middle East. The head of the Church and the See of Alexandria is the Patriarch of Alexandria on the Holy See of Saint Mark, who also carries the title of Coptic Pope. The See of Alexandria is titular, and today the Coptic Pope presides from Saint Mark's Coptic Orthodox Cathedral in the Abbassia District in Cairo. The church follows the Alexandrian Rite for its liturgy, prayer and devotional patrimony. With 18–22 million members worldwide, whereof about 15 to 18 million are in Egypt (see Demographics section below), it is the country's largest Christian church.

According to its tradition, the Coptic Church was established by Saint Mark, an apostle and evangelist, during the middle of the 1st century (c. 42 AD). Due to disputes concerning the nature of Christ, it split from the rest of the Christendom after the Council of Chalcedon in AD 451, resulting in a rivalry with the Byzantine Orthodox Church. In the 4-7th centuries the Coptic Church gradually expanded due to the Christianization of the Aksumite empire and of two of the three Nubian kingdoms, Nobatia and Alodia, while the third Nubian kingdom, Makuria, recognized the Coptic patriarch after initially being aligned to the Byzantine Orthodox Church. 

After 639 A.D. Egypt was ruled by its Islamic conquerors from Arabia, and the treatment of the Coptic Christians ranged from tolerance to open persecution. In the 12th century, the church relocated its seat from Alexandria to Cairo. The same century also saw the Copts became a minority in their own country. During the 14th and 15th centuries, Nubian Christianity was supplanted by Islam, while the Ethiopian was granted autocephaly in 1959, which was extended to the Eritrean Tewahedo Church in 1998. Since the Arab Spring in 2011, however, the Copts have been suffering increased religion-based discrimination and violence.

Egypt is identified in the Bible as the place of refuge that the Holy Family sought in its flight from Judea: When he [Joseph] arose, he took the young Child and His mother by night and departed for Egypt, and was there until the death of Herod the Great, that it might be fulfilled which was spoken by the Lord through the prophet, saying, "Out of Egypt I called My Son" (Matthew 2:12–23).

The Egyptian Church is traditionally believed to be founded by St Mark at around AD 42, regards itself as the subject of many prophecies in the Old Testament. Isaiah the prophet, in Chapter 19, Verse 19 says "In that day there will be an altar to the LORD in the midst of the land of Egypt, and a pillar to the LORD at its border."

The first Christians in Egypt were common people who spoke Egyptian Coptic. There were also Alexandrian Jewish people such as Theophilus, whom Saint Luke the Evangelist addresses in the introductory chapter of his gospel. When the church was founded by Saint Mark during the reign of the Roman emperor Nero, a great multitude of native Egyptians (as opposed to Greeks or Jews) embraced the Christian faith.

Christianity spread throughout Egypt within half a century of Saint Mark's arrival in Alexandria, as is clear from the New Testament writings found in Bahnasa, in Middle Egypt, which date around the year AD 200, and a fragment of the Gospel of John, written in Coptic, which was found in Upper Egypt and can be dated to the first half of the 2nd century. In the 2nd century, Christianity began to spread to the rural areas, and scriptures were translated into the local languages, namely Coptic. 

The Coptic language is a universal language used in Coptic churches in every country. It is derived from ancient Egyptian and uses Greek letters. Many of the hymns in the liturgy are in Coptic and have been passed down for several thousand years. The language is used to preserve Egypt's original language, which was banned by the Arab invaders, who ordered Arabic to be used instead. Some examples of these hymns are Epooro, Ekesmar'oot, Tai Shori, and many more.

The Catechetical School of Alexandria is the oldest catechetical school in the world. St. Jerome records that the Christian School of Alexandria was founded by Saint Mark himself. Around AD 190 under the leadership of the scholar Pantanaeus, the school of Alexandria became an important institution of religious learning, where students were taught by scholars such as Athenagoras, Clement, Didymus, and the native Egyptian Origen, who was considered the father of theology and who was also active in the field of commentary and comparative Biblical studies. Origen wrote over 6,000 commentaries of the Bible in addition to his famous Hexapla.

Many scholars such as Jerome visited the school of Alexandria to exchange ideas and to communicate directly with its scholars. The scope of this school was not limited to theological subjects; science, mathematics and humanities were also taught there. The question-and-answer method of commentary began there, and 15 centuries before Braille, wood-carving techniques were in use there by blind scholars to read and write.

The Theological college of the catechetical school was re-established in 1893. The new school currently has campuses in Ireland, Cairo, New Jersey, and Los Angeles, where Coptic priests-to-be and other qualified men and women are taught among other subjects Christian theology, history, the Coptic language and art – including chanting, music, iconography, and tapestry.

Many Egyptian Christians went to the desert during the 3rd century, and remained there to pray and work and dedicate their lives to seclusion and worship of God. This was the beginning of the monastic movement, which was organized by Anthony the Great, Saint Paul of Thebes, the world's first anchorite, Saint Macarius the Great and Saint Pachomius the Cenobite in the 4th century.

Christian monasticism was born in Egypt and was instrumental in the formation of the Coptic Orthodox Church character of submission, simplicity and humility, thanks to the teachings and writings of the Great Fathers of Egypt's Deserts. By the end of the 5th century, there were hundreds of monasteries, and thousands of cells and caves scattered throughout the Egyptian desert. A great number of these monasteries are still flourishing and have new vocations to this day.

All Christian monasticism stems, either directly or indirectly, from the Egyptian example: Saint Basil the Great Archbishop of Caesaria of Cappadocia, founder and organizer of the monastic movement in Asia Minor, visited Egypt around AD 357 and his rule is followed by the Eastern Orthodox Churches; Saint Jerome who translated the Bible into Latin, came to Egypt, while en route to Jerusalem, around AD 400 and left details of his experiences in his letters; Benedict founded the Benedictine Order in the 6th century on the model of Saint Pachomius, but in a stricter form. Countless pilgrims have visited the "Desert Fathers" to emulate their spiritual, disciplined lives.

In the 4th century, an Alexandrian presbyter named Arius began a theological dispute about the nature of Christ that spread throughout the Christian world and is now known as Arianism. The Ecumenical Council of Nicea AD 325 was convened by Constantine after the Pope Alexander I of Alexandria request to hold a Council to respond to heresies, under the presidency of Saint Hosius of Cordova to resolve the dispute and eventually led to the formulation of the Symbol of Faith, also known as the Nicene Creed. The Creed, which is now recited throughout the Christian world, was based largely on the teaching put forth by a man who eventually would become Pope Saint Athanasius of Alexandria, the chief opponent of Arius.

In the year AD 381, Pope Timothy I of Alexandria presided over the second ecumenical council known as the Ecumenical Council of Constantinople, to judge Macedonius, who denied the Divinity of the Holy Spirit. This council completed the Nicene Creed with this confirmation of the divinity of the Holy Spirit:

We believe in the Holy Spirit, the Lord, the Giver of Life, who proceeds from the Father, who with the Father through the Son is worshiped and glorified who spoke by the Prophets and in One, Holy, Catholic, and Apostolic church. We confess one Baptism for the remission of sins and we look for the resurrection of the dead and the life of the coming age, Amen.

Another theological dispute in the 5th century occurred over the teachings of Nestorius, the Patriarch of Constantinople who taught that God the Word was not hypostatically joined with human nature, but rather dwelt in the man Jesus. As a consequence of this, he denied the title "Mother of God" "(Theotokos)" to the Virgin Mary, declaring her instead to be "Mother of Christ" "Christotokos".

When reports of this reached the Apostolic Throne of Saint Mark, Pope Saint Cyril I of Alexandria acted quickly to correct this breach with orthodoxy, requesting that Nestorius repent. When he would not, the Synod of Alexandria met in an emergency session and a unanimous agreement was reached. Pope Cyril I of Alexandria, supported by the entire See, sent a letter to Nestorius known as "The Third Epistle of Saint Cyril to Nestorius." This epistle drew heavily on the established Patristic Constitutions and contained the most famous article of Alexandrian Orthodoxy: "The Twelve Anathemas of Saint Cyril." In these anathemas, Cyril excommunicated anyone who followed the teachings of Nestorius. For example, "Anyone who dares to deny the Holy Virgin the title "Theotokos" is Anathema!" Nestorius however, still would not repent and so this led to the convening of the First Ecumenical Council of Ephesus (431), over which Cyril presided.

The Council confirmed the teachings of Saint Athanasius and confirmed the title of Mary as "Mother of God". It also clearly stated that anyone who separated Christ into two hypostases was anathema, as Cyril had said that there is "One Nature [and One Hypostasis] for God the Word Incarnate" ("Mia Physis tou Theou Logou Sesarkōmenē"). Also, the introduction to the creed was formulated as follows:

We magnify you O Mother of the True Light and we glorify you O saint and Mother of God "(Theotokos)" for you have borne unto us the Saviour of the world. Glory to you O our Master and King: Christ, the pride of the Apostles, the crown of the martyrs, the rejoicing of the righteous, firmness of the churches and the forgiveness of sins. We proclaim the Holy Trinity in One Godhead: we worship Him, we glorify Him, Lord have mercy, Lord have mercy, Lord bless us, Amen. [not dissimilar to the "Axion Estin" Chant still used in Orthodoxy]

When in AD 451, Emperor Marcian attempted to heal divisions in the Church, the response of Pope Dioscorus – the Pope of Alexandria who was later exiled – was that the emperor should not intervene in the affairs of the Church. It was at Chalcedon that the emperor, through the Imperial delegates, enforced harsh disciplinary measures against Pope Dioscorus in response to his boldness. In 449, Pope Dioscorus headed the 2nd Council of Ephesus, called the "Robber Council" by Chalcedonian historians. It held to the Miaphysite formula which upheld the Christology of "One Incarnate Nature of God the Word" (Greek: μία φύσις Θεοῦ Λόγου σεσαρκωμένη ("mia physis Theou Logou sesarkōmenē")), and upheld the heretic Eutyches claiming he was orthodox.

The Council of Chalcedon summoned Dioscorus three times to appear at the council, after which he was deposed. The Council of Chalcedon further deposed him for his support of Eutyches, but not necessarily for Eutychian Monophysitism. Dioscorus appealed to the conciliar fathers to allow for a more Miaphysite interpretation of Christology at the council, but was denied. Following his being deposed, the Coptic Church and its faithful felt unfairly underrepresented at the council and oppressed politically by the Byzantine Empire. After the Byzantines appointed Proterius of Alexandria as Patriarch to represent the Chalcedonian Church, the Coptic Church appointed their own Patriarch Timothy Aelurus and broke from the Chalcedonian communion.

The Council of Chalcedon, from the perspective of the Alexandrine Christology, has deviated from the approved Cyrillian terminology and declared that Christ was one hypostasis in two natures. However, in the Nicene-Constantinopolitan Creed, "Christ was conceived of the Holy Spirit and of the Virgin Mary," thus the foundation of the definition according to the Non-Chalcedonian adherents, according to the Christology of Cyril of Alexandria is valid. There is a change in the Non-Chalcedonian definition here, as the Nicene creed clearly uses the terms "of", rather than "in."

In terms of Christology, the Oriental Orthodox (Non-Chalcedonians) understanding is that Christ is "One Nature—the Logos Incarnate," "of" the full humanity and full divinity. The Chalcedonians' understanding is that Christ is "recognized in" two natures, full humanity and full divinity. Oriental Orthodoxy contends that such a formulation is no different from what the Nestorians teach. This is the doctrinal perception that makes the apparent difference which separated the Oriental Orthodox from the Eastern Orthodox.

The Council's findings were rejected by many of the Christians on the fringes of the Byzantine Empire, including Egyptians, Syriacs, Armenians, and others.

From that point onward, Alexandria would have two patriarchs: the non-Chalcedonian native Egyptian one, now known as the Coptic Pope of Alexandria and Patriarch of All Africa on the Holy Apostolic See of St. Mark, and the Melkite or Imperial Patriarch, now known as the Greek Orthodox Patriarch of Alexandria.

Almost the entire Egyptian population rejected the terms of the Council of Chalcedon and remained faithful to the native Egyptian Church (now known as the Coptic Orthodox Church of Alexandria). Those who supported the Chalcedonian definition remained in communion with the other leading imperial churches of Rome and Constantinople. The non-Chalcedonian party became what is today called the Oriental Orthodox Church.

The Coptic Orthodox Church of Alexandria regards itself as having been misunderstood at the Council of Chalcedon. There was an opinion in the Church that viewed that perhaps the Council understood the Church of Alexandria correctly, but wanted to curtail the existing power of the Alexandrine Hierarch, especially after the events that happened several years before at Constantinople from Pope Theophilus of Alexandria towards Patriarch John Chrysostom and the unfortunate turnouts of the Second Council of Ephesus in AD 449, where Eutichus misled Pope Dioscorus and the Council in confessing the Orthodox Faith in writing and then renouncing it after the Council, which in turn, had upset Rome, especially that the Tome which was sent was not read during the Council sessions.

To make things even worse, the Tome of Pope Leo of Rome was, according to the Alexandria School of Theology, particularly in regards to the definition of Christology, considered influenced by Nestorian heretical teachings. So, due to the above-mentioned, especially in the consecutive sequences of events, the Hierarchs of Alexandria were considered holding too much of power from one hand, and on the other hand, due to the conflict of the Schools of Theology, there would be an impasse and a scapegoat, i.e. Pope Dioscorus. The Tome of Leo has been widely criticized (surprisingly by Roman Catholic and Eastern Orthodox scholars) in the past 50 years as a much less than perfect orthodox theological doctrine.

It is also to be noted that by anathemizing Pope Leo because of the tone and content of his tome, as per Alexandrine Theology perception, Pope Dioscorus was found guilty of doing so without due process; in other words, the Tome of Leo was not a subject of heresy in the first place, but it was a question of questioning the reasons behind not having it either acknowledged or read at the Second Council of Ephesus in AD 449. Pope Dioscorus of Alexandria was never labeled as heretic by the council's canons.

Copts also believe that the Pope of Alexandria was forcibly prevented from attending the third congregation of the council from which he was ousted, apparently the result of a conspiracy tailored by the Roman delegates.

Before the current positive era of Eastern and Oriental Orthodox dialogues, Chalcedonians sometimes used to call the non-Chalcedonians "Monophysites", though the Coptic Orthodox Church in reality regards Monophysitism as a heresy. The Chalcedonian doctrine in turn came to be known as "Dyophysite".

A term that comes closer to Coptic Orthodoxy is Miaphysite, which refers to a conjoined nature for Christ, both human and divine, united indivisibly in the Incarnate Logos. The Coptic Orthodox Church of Alexandria believes that Christ is perfect in His divinity, and He is perfect in His humanity, but His divinity and His humanity were united in one nature called "the nature of the incarnate word", which was reiterated by Saint Cyril of Alexandria.

Copts, thus, believe in two natures "human" and "divine" that are united in one hypostasis "without mingling, without confusion, and without alteration". These two natures "did not separate for a moment or the twinkling of an eye" (Coptic Liturgy of Saint Basil of Caesarea).

Prior to Chalcedon, the Imperial Church's main division stemmed from Nestorianism, eventually leading the Church of the East to declare its independence in 424. After the Council of Chalcedon in 451, the Coptic Church and its hierarchy felt suspicious of what they believed were Nestorian elements within the Chalcedonian Church. As a result, the anti-Chalcedon partisan, Timotheos Aelurus, consigned himself to depose the Chalcedonian Pope of Alexandria, Proterius of Alexandria, and to set himself up as the Pope of Alexandria in opposition to the entire Chalcedonian Church of the Byzantine Empire. Copts suffered under the rule of the Byzantine Empire. The Melkite Patriarchs, appointed by the emperors as both spiritual leaders and civil governors, massacred those Egyptians they considered heretics. Many were tortured and martyred in attempts to force their acceptance of the Chalcedonian terms, but the Egyptians remained loyal to the Cyrillian Miaphysitism. One of the most renowned Egyptian saints of the period is Saint Samuel the Confessor.

The Muslim invasion of Egypt took place in AD 639. Despite the political upheaval, the Egyptian population remained mainly Christian. However, gradual conversions to Islam over the centuries had changed Egypt from a Christian to a largely Muslim country by the end of the 12th century. Egypt's Umayyad rulers taxed Christians at a higher rate than Muslims, driving merchants towards Islam and undermining the economic base of the Coptic Church. Although the Coptic Church did not disappear, the Umayyad tax policies made it difficult for the church to retain the Egyptian elites.

The Church suffered greatly under the many regimes of Islamic rule. Sometime during the 2nd Millennium AD, the leadership of the Church, including the Pope, moved from Alexandria to Cairo.

The position of Copts began to improve early in the 19th century under the stability and tolerance of the Muhammad Ali Dynasty. The Coptic community ceased to be regarded by the state as an administrative unit. In 1855 the jizya tax was abolished. Shortly thereafter, the Copts started to serve in the Egyptian army.

Towards the end of the 19th century, the Coptic Church underwent phases of new development. In 1853, Pope Cyril IV established the first modern Coptic schools, including the first Egyptian school for girls. He also founded a printing press, which was only the second national press in the country. The Pope established very friendly relations with other denominations, to the extent that when the Greek Patriarch in Egypt had to absent himself from the country for a long period of time, he left his Church under the guidance of the Coptic Patriarch.

The Theological College of the School of Alexandria was reestablished in 1893. It began its new history with five students, one of whom was later to become its dean. Today it has campuses in Alexandria and Cairo, and in various dioceses throughout Egypt, as well as outside Egypt. It has campuses in New Jersey, Los Angeles, Sydney, Melbourne, and London, where potential clergymen and other qualified men and women study many subjects, including theology, church history, missionary studies, and the Coptic language.

On 17 March 2012, the Coptic Orthodox Pope, Pope Shenouda III died, leaving many Copts mourning and worrying as tensions rose with Muslims. Pope Shenouda III constantly met with Muslim leaders in order to create peace. Many were worried about Muslims controlling Egypt as the Muslim Brotherhood won 70% of the parliamentary elections.

On 4 November 2012, Bishop Tawadros was chosen as the 118th Pope. In a ritual filled with prayer, chants and incense at Abbasiya cathedral in Cairo, the 60-year-old bishop's name was picked by a blindfolded child from a glass bowl in which the names of two other candidates had also been placed. The enthronement was scheduled on 18 November 2012.

In 1959, the Ethiopian Orthodox Tewahedo Church was granted its first own Patriarch by Pope Cyril VI. Furthermore, the Eritrean Orthodox Tewahedo Church similarly became independent of the Ethiopian Orthodox Tewahedo Church in 1994, when four bishops were consecrated by Pope Shenouda III of Alexandria to form the basis of a local Holy Synod of the Eritrean Church. In 1998, the Eritrean Orthodox Tewahedo Church gained its autocephelacy from the Coptic Orthodox Church when its first Patriarch was enthroned by Pope Shenouda III of Alexandria.

These three churches remain in full communion with each other and with the other Oriental Orthodox churches. The Ethiopian Orthodox Tewahedo Church and the Eritrean Orthodox Tewahedo Church do acknowledge the Honorary Supremacy of the Coptic Orthodox Patriarch of Alexandria, since the Church of Alexandria is technically their Mother Church. Upon their selection, both Patriarchs (Ethiopian & Eritrean) must receive the approval and communion from the Holy Synod of the Apostolic See of Alexandria before their enthronement.

Since the 1980s theologians from the Oriental (non-Chalcedonian) Orthodox and Eastern (Chalcedonian) Orthodox churches have been meeting in a bid to resolve theological differences, and have concluded that many of the differences are caused by the two groups using different terminology to describe the same thing (see Agreed Official Statements on Christology with the Eastern Orthodox Churches).

In the summer of 2001, the Coptic Orthodox and Greek Orthodox Patriarchates of Alexandria agreed to mutually recognize baptisms performed in each other's churches, making re-baptisms unnecessary, and to recognize the sacrament of marriage as celebrated by the other. Previously, if a Coptic Orthodox and Greek Orthodox wanted to get married, the marriage had to be performed twice, once in each church, for it to be recognized by both. Now it can be done in only one church and be recognized by both.

According to Christian Tradition and Canon Law, the Coptic Orthodox Church of Alexandria only ordains men to the priesthood and episcopate, and if they wish to be married, they must be married before they are ordained. In this respect they follow the same practices as all other Oriental Orthodox Churches, aswell as all of Eastern Orthodox Churches.

Traditionally, the Coptic language was used in church services, and the scriptures were written in the Coptic alphabet. However, due to the Arabisation of Egypt, service in churches started to witness increased use of Arabic, while preaching is done entirely in Arabic. Native languages are used, in conjunction with Coptic, during services outside Egypt.

The liturgical calendar of the Coptic Orthodox Church is the Coptic calendar (also called the Alexandrian Calendar). This calendar is based on the Egyptian calendar of Ancient Egypt. Coptic Orthodox Christians celebrate Christmas on 29 Koiak, which corresponds to 7 January in the Gregorian Calendar and 25 December in the Julian Calendar. Coptic Christmas was adopted as an official national holiday in Egypt in 2002.

In Tahrir Square, Cairo, on Wednesday 2 February 2011, Coptic Christians joined hands to provide a protective cordon around their Muslim neighbors during salat (prayers) in the midst of the 2011 Egyptian Revolution.

There are about 20 million Coptic Orthodox Christians in the world. Between 15 and 18 million of them are found in Egypt under the jurisdiction of the Coptic Orthodox Church of Alexandria.<ref name="The world factbook/Egypt/"></ref> Since Egypt does not have an official government census on religion, estimates of the size of Egypt's Christian population vary from the estimated low previous government figures of 6 to 7 million to the 12 million reported by some Christian leaders in 2008. The actual numbers may be in the 15 to 18 million range, out of an Egyptian population of more than 90 million. However, in 2011, the Pew Research Center announced that Copts in Egypt constitute 4.5% of the population, while the Catholic Holy See puts Copts at 6 to 8%. Then in 2017 government owned news Al Ahram estimated the percentage of Copts at 10 to 15%. However, in 2012 after the Egyptian revolution the Muslim Brotherhood was drafting new constitution then Major-General Abu Bakr al-Guindi claimed that the lower figures support a downward trend in the percentage of Copts in Egypt, as recorded in consecutive estimated Egyptian censuses since a 1927 high where Egyptian Christians formed 8.3% of the population. This decline has been explained by Major-General Abu Bakr al-Guindi, head of Egypt's Central Agency for Public Mobilization and Statistics, as being the result of Copts having the highest emigration rate, the lowest birthrate and the highest income level in the country. Although many Copts rejected this statement and population count by Abu Barker al-Guindi claiming they have been undercounted as most Copts reside in Upper Egypt.

There are also significant numbers in the diaspora outside Africa in countries such as the United States, Canada, Australia, France, and Germany. The number of Coptic Orthodox Christians in the diaspora is roughly 2 million.

In addition, there are between 350,000 and 400,000 native African adherents in East, Central and South Africa, most in Sudan, whose population is less than 200,000. Although under the jurisdiction of the Coptic Orthodox Church, these adherents are not considered Copts, since they are not ethnic Egyptians.

Some accounts regard members of the Ethiopian Orthodox Tewahedo Church (roughly 45 million), the Eritrean Orthodox Tewahedo Church (roughly 2.5 million), as members of the Coptic Orthodox Church. This is however a misnomer, since both the Ethiopian and the Eritrean Churches, although daughter churches of the Church of Alexandria, are currently autocephalous churches.

While Copts have cited instances of persecution throughout their history, Human Rights Watch has noted "growing religious intolerance" and sectarian violence against Coptic Christians in recent years, and a failure by the Egyptian government to effectively investigate properly and prosecute those responsible. Over a hundred Egyptian copts have been killed in sectarian clashes from 2011 to 2017, and many homes and businesses destroyed. In just one province (Minya), 77 cases of sectarian attacks on Copts between 2011 and 2016 have been documented by the Egyptian Initiative for Personal Rights. The abduction and disappearance of Coptic Christian women and girls also remains a serious ongoing problem.

Besides Egypt, the Church of Alexandria has jurisdiction over Pentapolis, Libya, Nubia, Sudan, Ethiopia, Eritrea and all Africa.

Both the Patriarchate of Addis Ababa and all Ethiopia, and the Patriarchate of Asmara and all Eritrea do acknowledge the supremacy of honor and dignity of the Pope and Patriarch of Alexandria on the basis that both Patriarchates were established by the Throne of Alexandria and that they have their roots in the Apostolic Church of Alexandria, and acknowledge that Saint Mark the Apostlic is the founder of their Churches through the heritage and Apostolic evangelization of the Fathers of Alexandria.

In other words, the Patriarchates of Ethiopia and Eritrea are daughter Churches of the Holy Apostolic Patriarchate of Alexandria.

In addition to the above, the countries of Uganda, Kenya, Tanzania, Zambia, Zimbabwe, the Congo, Cameroon, Nigeria, Ghana, Botswana, Malawi, Angola, Namibia and South Africa are under the jurisdiction and the evangelization of the Throne of Alexandria. It is still expanding in the continent of Africa.

Ethiopia received Christianity next to Jerusalem, through Jesus's own apostle, only a year after Jesus was crucified (Acts 8: 26–39). Christianity became a national religion of Ethiopia, under the dominion of the Church of Alexandria, in the 4th century. The first bishop of Ethiopia, Saint Frumentius, was consecrated as Bishop of Axum by Pope Athanasius of Alexandria in AD 328. From then on, until 1959, the Pope of Alexandria, as Patriarch of All Africa, always named an Egyptian (a Copt) to be the Archbishop of the Ethiopian Church. On 13 July 1948, the Coptic Church of Alexandria and the Ethiopian Orthodox Tewahedo Church reached an agreement concerning the relationship between the two churches. In 1950, the Ethiopian Orthodox Tewahedo Church was granted autocephaly by Pope Joseph II of Alexandria, head of the Coptic Orthodox Church. Five Ethiopian bishops were immediately consecrated by the Pope of Alexandria and Patriarch of All Africa, and were empowered to elect a new Patriarch for their church. This promotion was completed when Joseph II consecrated the first Ethiopian-born Archbishop, Abuna Basilios, as head of the Ethiopian Church on 14 January 1951. In 1959, Pope Cyril VI of Alexandria crowned Abuna Basilios as the first Patriarch of Ethiopia.

Patriarch Basilios died in 1971, and was succeeded on the same year by Abuna Theophilos. With the fall of Emperor Haile Selassie I of Ethiopia in 1974, the new Marxist government arrested Abuna Theophilos and secretly executed him in 1979. The Ethiopian government then ordered the Ethiopian Church to elect Abuna Takla Haymanot as Patriarch of Ethiopia. The Coptic Orthodox Church refused to recognize the election and enthronement of Abuna Takla Haymanot on the grounds that the Synod of the Ethiopian Church had not removed Abuna Theophilos, and that the Ethiopian government had not publicly acknowledged his death, and he was thus still legitimate Patriarch of Ethiopia. Formal relations between the two churches were halted, although they remained in communion with each other.

After the death of Abuna Takla Haymanot in 1988, Abune Merkorios who had close ties to the Derg (Communist) government was elected Patriarch of Ethiopia. Following the fall of the Derg regime in 1991, Abune Merkorios abdicated under public and governmental pressure and went to exile in the United States. The newly elected Patriarch, Abune Paulos was officially recognized by the Coptic Orthodox Church of Alexandria in 1992 as the legitimate Patriarch of Ethiopia. Formal relations between the Coptic Church of Alexandria and the Ethiopian Orthodox Tewahedo Church were resumed on 13 July 2007. Abune Paulos died in August 2012.

Following the independence of Eritrea from Ethiopia in 1993, the newly independent Eritrean government appealed to Pope Shenouda III of Alexandria for Eritrean Orthodox autocephaly. In 1994, Pope Shenouda ordained Abune Phillipos as first Archbishop of Eritrea. The Eritrean Orthodox Tewahedo Church obtained autocephaly on 7 May 1998, and Abune Phillipos was subsequently consecrated as first Patriarch of Eritrea. The two churches remain in full communion with each other and with the other Oriental Orthodox Churches, although the Coptic Orthodox Church of Alexandria, along with the Ethiopian Orthodox Tewahedo Church does not recognize the deposition of the third Patriarch of Eritrea, Abune Antonios.

There are several and institutions outside Egypt, including churches and institutions in:




The patriarch of Alexandria was originally known merely as bishop of Alexandria. However, this title continued to evolve as the Church grew under Theophilus and his nephew and successor Cyril ( 376–444), and especially in the 5th century when the Church developed its hierarchy.

The bishop of Alexandria, being the successor of the first bishop in Roman Egypt consecrated by Saint Mark, was honored by the other bishops as first among equals "primus inter pares". Under the sixth canon of the Council of Nicaea, Cyril was raised to prelate or chief bishop at the head of the episcopates of Egypt, Libya, and the Pentapolis without the existence of intermediate archbishops as existed in other ecclesiastic provinces. He had the privilege of choosing and consecrating bishops.

The title of "pope" has been attributed to the Patriarch of Alexandria since the episcopate of Heraclas, the 13th Patriarch of Alexandria. All the clergy of Alexandria and Lower Egypt honored him with the title "papas", which means "father" as the archbishop and metropolitan having authority over all bishops, within the Egyptian province, who are under his jurisdiction. Alexandria, while the ecclesiastical and provincial capital, also had the distinction as being the place where Saint Mark was martyred.

The title "Patriarch" originally referred to a clan leader or head of a familial lineage. Ecclesiastically it means a bishop of high rank and was originally used as a title for the bishops of Rome, Constantinople, Jerusalem, Antioch, and Alexandria. For the Coptic patriarch, this title was "Patriarch of Alexandria and all Africa on the Holy Apostolic Throne of Saint Mark the Evangelist," that is "of Egypt". The title of "Patriarch" was first used around the time of the Third Ecumenical Council of Ephesus, convened in AD 431, and ratified at Chalcedon in AD 451.

It is to be noted that only the Patriarch of Alexandria has the double title of "Pope" and "Patriarch" among the Eastern Orthodox and Oriental Orthodox ecumenical church heads.

The Coptic Orthodox patriarchate of Alexandria is governed by its Holy Synod, which is headed by the Patriarch of Alexandria. Under his authority are the metropolitan archbishops, metropolitan bishops, diocesan bishops, patriarchal exarchs, missionary bishops, auxiliary bishops, suffragan bishops, assistant bishops, chorbishops and the patriarchal vicars for the Church of Alexandria. They are organized as follows:







</doc>
<doc id="7602" url="https://en.wikipedia.org/wiki?curid=7602" title="The Family International">
The Family International

The Family International (TFI) is a cult that started in 1968 in Huntington Beach, California, USA. It was originally called Teens for Christ and later gained notoriety as The Children of God (COG). It was later renamed and reorganized as The Family of Love, which was eventually shortened to The Family. It is currently called The Family International.

TFI initially spread a message of salvation, apocalypticism, spiritual "revolution and happiness" and distrust of the outside world, which the members called "The System". In 1976, it began a method of evangelism called Flirty Fishing, that used sex to "show God's love and mercy" and win converts, resulting in controversy. TFI's founder and prophetic leader, David Berg (who was first called "Moses David" in the Texas press), gave himself the titles of "King", "The Last Endtime Prophet", "Moses", and "David". He communicated with his followers via "Mo Letters"—letters of instruction and counsel on myriad spiritual and practical subjects—until his death in late 1994. After his death, his widow Karen Zerby became the leader of TFI, taking the titles of "Queen" and "Prophetess". She married Steve Kelly, an assistant of Berg's whom Berg had handpicked as her "consort". Kelly took the title of "King Peter" and became the face of TFI, speaking in public more often than either David Berg or Karen Zerby.

Members of The Children of God (COG) founded communes, first called colonies (now referred to as homes), in various cities. They would proselytize in the streets and distribute pamphlets. Leaders within COG were referred to as "The Chain".

The founder of the movement, David Brandt Berg (1919–1994), was a former Christian and Missionary Alliance pastor.

Berg communicated with his followers by writing letters. He published nearly 3,000 letters over a period of 24 years, referred to as the "Mo Letters". In a letter written in January 1972, Berg stated that he was God's prophet for the contemporary world, attempting to further solidify his spiritual authority within the group. Berg's letters also contained public acknowledgement of his own failings and weaknesses.

By 1972, COG had 130 communities around the world.

The Children of God was abolished in February 1978. Berg reorganized the movement amid reports of serious misconduct, financial mismanagement, The Chain's abuse of authority, and disagreements within it about the continued use of Flirty Fishing. One-eighth of the total membership left the movement. Those who remained became part of a reorganized movement called the Family of Love, and later, The Family. The majority of the group's beliefs remained the same.

The Family of Love era was characterized by international expansion.

In 1976, before the dissolution of The Children of God, David Berg had introduced a new proselytizing method called Flirty Fishing (or FFing), which encouraged female members to "show God's love" through sexual relationships with potential converts. Flirty Fishing was practiced by members of Berg's inner circle starting in 1973, and was introduced to the general membership in 1976 and became common practice within the group. In some areas flirty fishers used escort agencies to meet potential converts. According to TFI "over 100,000 received God's gift of salvation through Jesus, and some chose to live the life of a disciple and missionary" as a result of Flirty Fishing. Researcher Bill Bainbridge obtained data from TFI suggesting that, from 1974 until 1987, members had sexual contact with 223,989 people while practicing Flirty Fishing.

In March 1989, TF issued a statement that, in "early 1985", an urgent memorandum had been sent to all members "reminding them that any such activities [adult–child sexual contact] are "strictly forbidden" within our group" (emphasis in original), and such activities were grounds for immediate excommunication from the group. In January 2005, Claire Borowik, a spokesperson for TFI, stated that:[d]ue to the fact that our current zero-tolerance policy regarding sexual interaction between adults and underage minors was not in our literature published before 1986, we came to the realization that during a transitional stage of our movement, from 1978 until 1986, there were cases when some minors were subject to sexually inappropriate advances ... This was corrected officially in 1986, when any contact between an adult and minor (any person under 21 years of age) was declared an excommunicable offense.

After Berg's death in October 1994, Karen Zerby (known in the group as Mama Maria, Queen Maria, Maria David, or Maria Fontaine), assumed leadership of the group.

In February 1995, the group introduced the "Love Charter", which defined the rights and responsibilities of Charter Members and Homes. The Charter also included the "Fundamental Family Rules", a summary of rules and guidelines from past TF publications which were still in effect.

In the 1994–95 British court case, the Rt. Hon. Lord Justice Alan Ward ruled that the group, including some of its top leaders, had in the past engaged in abusive sexual practices involving minors and had also used severe corporal punishment and sequestration of minors. He found that by 1995 TF had abandoned these practices and concluded that they were a safe environment for children. Nevertheless, he did require that the group cease all corporal punishment of children in the United Kingdom and denounce any of Berg's writings that were "responsible for children in TF having been subjected to sexually inappropriate behaviour".

The Love Charter is The Family's set governing document that entails each member's rights, responsibilities and requirements, while the "Missionary Member Statutes" and "Fellow Member Statutes" were written for the governance of TFI's Missionary member and Fellow Member circles, respectively. FD Homes were reviewed every six months against a published set of criteria. The Love Charter increased the number of single family homes as well as homes that relied on jobs such as self-employment.

TFI's recent teachings center around beliefs they term the "new [spiritual] weapons". TFI members believe that they are soldiers in the spiritual war of good versus evil for the souls and hearts of men.

These include angels, departed humans, other religious and mythical figures, and even celebrities; for example the goddess Aphrodite, the Snowman, Merlin, the Sphinx, Elvis, Marilyn Monroe, Audrey Hepburn, Richard Nixon, and Winston Churchill.

TFI believes that the Biblical passage "I will give you the keys of the kingdom of heaven, and whatsoever you bind on earth will be bound in heaven, and whatsoever you loose on earth will be loosed in heaven" (), refers to an increased spiritual authority given to Peter and the early disciples. According to TFI beliefs, this refers to keys that were hidden and unused in the centuries that followed, but were revealed again through Karen Zerby as additional power for praying and obtaining miracles. TFI members call on the various Keys of the Kingdom for extra effect during prayer. The Keys, like most TFI beliefs, were digested in comic-book magazines to help teach them to children. These beliefs are still generally held and practiced, even after the "reboot" documents of 2010.

This is a term TFI members use to describe their intimate, sexual relationship with Jesus. TFI describes its "Loving Jesus" teaching as a radical form of bridal theology. They believe the church of followers is Christ's bride, called to love and serve him with wifely fervor. But they take bridal theology further, encouraging members to imagine Jesus is joining them during sexual intercourse and masturbation. Male members are cautioned to visualize themselves as women, in order to avoid a homosexual relationship with Jesus. Many TFI publications, and spirit messages claimed to be from Jesus himself, elaborate this intimate, sexual relation they believe Jesus desires and needs. TFI imagines itself as his special "bride" in graphic poetry, guided visualizations, artwork, and songs. Some TFI literature is not brought into conservative countries for fear it may be classified at customs as pornography. The literature outlining this view of Jesus and his desire for a sexual relationship with believers was edited for younger teens, then further edited for children.

Second-generation adults (known as "SGAs") are adults born or reared in TFI.

Anti-TFI sentiment has been publicly expressed by some who have left the group; examples include sisters Celeste Jones, Kristina Jones, and Juliana Buhring, who wrote a book on their lives in TFI.

TFI members are expected to respect legal and civil authorities where they live. Members have typically cooperated with appointed authorities, even during the police and social-service raids of their communities in the early 1990s.

The group has been criticized by the press and the anti-cult movement. In 1971, an organization called FREECOG was founded by concerned parents and others, including deprogrammer Ted Patrick, to "free" members of the COG from their involvement in the group. Academics were divided, with some categorizing TFI as a "new religious movement", and others, such as Benjamin Beit-Hallahmi and John Huxley, labeling the group a "cult".







</doc>
<doc id="7603" url="https://en.wikipedia.org/wiki?curid=7603" title="CIT">
CIT

CIT or cit may refer to:





</doc>
<doc id="7604" url="https://en.wikipedia.org/wiki?curid=7604" title="Code of Hammurabi">
Code of Hammurabi

The Code of Hammurabi is a well-preserved Babylonian code of law of ancient Mesopotamia, dated back to about 1754 BC (Middle Chronology). It is one of the oldest deciphered writings of significant length in the world. The sixth Babylonian king, Hammurabi, enacted the code. A partial copy exists on a 2.25 metre (7.5 ft) stone stele. It consists of 282 laws, with scaled punishments, adjusting "an eye for an eye, a tooth for a tooth" ("lex talionis") as graded depending on social status, of slave versus free man or woman.

Nearly half of the code deals with matters of contract, establishing, for example, the wages to be paid to an ox driver or a surgeon. Other provisions set the terms of a transaction, establishing the liability of a builder for a house that collapses, for example, or property that is damaged while left in the care of another. A third of the code addresses issues concerning household and family relationships such as inheritance, divorce, paternity, and reproductive behaviour. Only one provision appears to impose obligations on an official; this provision establishes that a judge who reaches an incorrect decision is to be fined and removed from the bench permanently. A few provisions address issues related to military service.

The code was discovered by modern archaeologists in 1901, and its "editio princeps" translation published in 1902 by Jean-Vincent Scheil. This nearly complete example of the code is carved into a basalt stele in the shape of a huge index finger, tall. The code is inscribed in the Akkadian language, using cuneiform script carved into the stele. 

It is currently on display in the Louvre, with replicas in numerous institutions, including the Oriental Institute at the University of Chicago, the Northwestern Pritzker School of Law in Chicago, the Clendening History of Medicine Library & Museum at the University of Kansas Medical Center, the library of the Theological University of the Reformed Churches in the Netherlands, the Pergamon Museum of Berlin, the Arts Faculty of the University of Leuven in Belgium, the National Museum of Iran in Tehran, and the Department of Anthropology, National Museum of Natural History, Smithsonian Institution, the University Museum at the University of Pennsylvania, and Museum of the Bible in Washington, DC.

Hammurabi ruled for nearly 42 years, from about 1792 to 1749 BC according to the Middle chronology. In the preface to the law, he states, "Anu and Bel called by name me, Hammurabi, the exalted prince, who feared God, to bring about the rule of righteousness in the land, to destroy the wicked and the evil-doers; so that the strong should not harm the weak; so that I should rule over the black-headed people like Shamash, and enlighten the land, to further the well-being of mankind." On the stone slab are 44 columns and 28 paragraphs that contained 282 laws. Some of these laws follow along the rules of "an eye for an eye".

It had been taken as plunder by the Elamite king Shutruk-Nahhunte in the 12th century BC and was taken to Susa in Elam (located in the present-day Khuzestan Province of Iran) where it was no longer available to the Babylonian people. However, when Cyrus the Great brought both Babylon and Susa under the rule of his Persian Empire, and placed copies of the document in the Library of Sippar, the text became available for all the peoples of the vast Persian Empire to view.

In 1901, Egyptologist Gustave Jéquier, a member of an expedition headed by Jacques de Morgan, found the stele containing the Code of Hammurabi during archaeological excavations at the ancient site of Susa in Khuzestan.

The Code of Hammurabi was one of the only sets of laws in the ancient Near East and also one of the first forms of law. The code of laws was arranged in orderly groups, so that all who read the laws would know what was required of them. Earlier collections of laws include the Code of Ur-Nammu, king of Ur (c. 2050 BC), the Laws of Eshnunna (c. 1930 BC) and the codex of Lipit-Ishtar of Isin (c. 1870 BC), while later ones include the Hittite laws, the Assyrian laws, and Mosaic Law.
These codes come from similar cultures in a relatively small geographical area, and they have passages which resemble each other.
The Code of Hammurabi is the longest surviving text from the Old Babylonian period. The code has been seen as an early example of a fundamental law, regulating a government — i.e., a primitive constitution. The code is also one of the earliest examples of the idea of presumption of innocence, and it also suggests that both the accused and accuser have the opportunity to provide evidence. The occasional nature of many provisions suggests that the code may be better understood as a codification of Hammurabi's supplementary judicial decisions, and that, by memorializing his wisdom and justice, its purpose may have been the self-glorification of Hammurabi rather than a modern legal code or constitution. However, its copying in subsequent generations indicates that it was used as a model of legal and judicial reasoning.

While the Code of Hammurabi was trying to achieve equality, biases still existed towards those categorized in the lower end of the social spectrum and some of the punishments and justice could be gruesome. The magnitude of criminal penalties often was based on the identity and gender of both the person committing the crime and the victim. The Code issues justice following the three classes of Babylonian society: property owners, freed men, and slaves.

Punishments for someone assaulting someone from a lower class were far lighter than if they had assaulted someone of equal or higher status. For example, if a doctor killed a rich patient, he would have his hands cut off, but if he killed a slave, only financial restitution was required. Women could also receive punishments that their male counterparts would not, as men were permitted to have affairs with their servants and slaves, whereas married women would be harshly punished for committing adultery.

Various copies of portions of the Code of Hammurabi have been found on baked clay tablets, some possibly older than the celebrated basalt stele now in the Louvre. The Prologue of the Code of Hammurabi (the first 305 inscribed squares on the stele) is on such a tablet, also at the Louvre (Inv #AO 10237). Some gaps in the list of benefits bestowed on cities recently annexed by Hammurabi may imply that it is older than the famous stele (currently dated to the early 18th century BC). Likewise, the Museum of the Ancient Orient, part of the Istanbul Archaeology Museums, also has a "Code of Hammurabi" clay tablet, dated to 1790 BC, in (Room 5, Inv # Ni 2358).

In July 2010, archaeologists reported that a fragmentary Akkadian cuneiform tablet was discovered at Tel Hazor, Israel, containing a c. 1700 BC text that was said to be partly parallel to portions of the Hammurabi code. The Hazor law code fragments are currently being prepared for publication by a team from the Hebrew University of Jerusalem.

The laws covered such subjects as:

One of the best known laws from Hammurabi's code was:

Hammurabi had many other punishments, as well. If a son strikes his father, his hands shall be hewn off. Translations vary.

Adultery

Ex. Law # 129: "If the wife of a man has been caught lying with another man, they shall bind them and throw them into the waters. If the owner of the wife would save his wife then in turn the king could save his servant." 



</doc>
<doc id="7605" url="https://en.wikipedia.org/wiki?curid=7605" title="Rum and Coke">
Rum and Coke

Rum and Coke, or a Cuba Libre (; , "Free Cuba"), is a highball cocktail consisting of cola, rum, and traditionally lime juice on ice. Traditionally, the cola ingredient is Coca-Cola ("Coke"), and the alcohol is a Cuban light rum such as Bacardi. However, the drink may be made with various types of rums and cola brands, and lime juice may or may not be included.

The cocktail originated in the early 20th century in Cuba, after the country won independence in the Spanish–American War. It subsequently became popular across Cuba, the United States, and other countries. Its simple recipe and inexpensive, ubiquitous ingredients have made it one of the world's most popular alcoholic drinks. Drink critics often consider the drink mediocre, but it has been noted for its historical significance. 

The drink was created in Cuba in the early 1900s, but its exact origins are not known with certainty. It became popular shortly after 1900, when bottled Coca-Cola was first imported into Cuba from the United States. It is associated with the heavy U.S. presence in Cuba following the Spanish–American War of 1898; the drink's traditional name, "Cuba Libre" (Free Cuba), was the slogan of the Cuban independence movement. The Cuba Libre is sometimes said to have been created during the Spanish–American War. However, this predates the first distribution of Coca-Cola to Cuba in 1900. A drink called a "Cuba Libre" was indeed known in 1898, but this was a mix of water and brown sugar.

Fausto Rodriguez, a Bacardi advertising executive, claimed to have been present when the drink was first poured, and produced a notarized affidavit to that effect in 1965. According to Rodriguez, this took place in August 1900, when he was a 14-year-old messenger working for a member of the U.S. Army Signal Corps in Havana. One day at a local bar, Rodriguez's employer ordered Bacardi rum mixed with Coca-Cola. This intrigued a nearby group of American soldiers, who ordered a round for themselves, giving birth to a popular new drink. Bacardi published Rodriguez's affidavit in a "Life" magazine ad in 1966. However, his status as a Bacardi executive has led some commenters to doubt the veracity of his story. Another story states that the drink was first created in 1902 at Havana's Restaurant La Florida to celebrate the anniversary of Cuban independence.

The drink became a staple in Cuba. It caught on due to the pervasiveness of its ingredients. Havana was already known for its iced drinks in the 19th century, as it was one of the few warm-weather cities that had abundant stores of ice shipped down from colder regions. Bacardi and other Cuban rums also boomed after independence brought in large numbers of foreign tourists and investors, as well as new opportunities for exporting alcohol. Light rums such as Bacardi became favored for cocktails, as they were considered to mix better than harsher dark rums. Coca-Cola had been a common mixer in the United States ever since it was first bottled in 1886, and it became a ubiquitous drink in many countries after it was first exported in 1900.

Rum and Coke quickly spread from Cuba to the United States. In the early 20th century the cocktail, like Coca-Cola itself, was most popular in the Southern United States. During the Prohibition era from 1922–1933, Coca-Cola became a favored mixer for disguising the taste of low-quality rums, as well as other liquors. In 1921 H. L. Mencken jokingly wrote of a South Carolina variant called the "jump stiddy", which consisted of Coca-Cola mixed with denatured alcohol drained from automobile radiators. After Prohibition, rum and Coke became prevalent in the Northern and Western U.S. as well, and in high-brow as well as low-brow circles.

Rum and Coke achieved a new level of popularity during World War II. Starting in 1940, the United States established a series of outposts among the British West Indies to defend against the German Navy. Their presence created cross-cultural demand, with American servicemen and the locals developing tastes for each other's products. In particular, American military personnel took to Caribbean rum due to its inexpensiveness, while Coca-Cola became especially prevalent in the islands thanks to the company shipping it out with the military. Within the United States, imported rum became increasingly popular, as government quotas for industrial alcohol reduced the output of American distillers of domestic liquors. In 1943, Lord Invader's Calypso song "Rum and Coca-Cola" drew further attention to the drink in Trinidad. In 1945, the Andrews Sisters had a major hit with a version of the song (plagiarized by Morey Amsterdam), which further boosted the drink's popularity.

During the Cuban Revolution in 1959, Bacardi fled to Puerto Rico. In 1960, the U.S. placed an embargo against Cuba which prohibited the importation of Cuban products, while Cuba likewise banned the importation of American products. With Cuban-made rum unavailable in the U.S. and Coca-Cola unavailable in Cuba, it became impossible to make a rum and Coke with its traditional ingredients in either country.

The rum and Coke is one of the most popular drinks in the world. Bacardi estimates that it is the second most popular alcoholic drink. Its popularity derives from the ubiquity and low cost of the main ingredients, and the fact that it is very easy to make. As it can be made with any quantity or style of rum, it is simple to prepare and difficult to ruin.

Drink critics often have a low opinion of the cocktail. Writer Wayne Curtis called it "a drink of inspired blandness," while Jason Wilson of "The Washington Post" called it "a lazy person's drink." Troy Patterson of "Slate" called it "the classic mediocre Caribbean-American highball," which "became a classic despite not being especially good." Charles A. Coulombe considers it a historically important drink, writing that the Cuba Libre is "a potent symbol of a changing world order – the marriage of rum, lubricant of the old colonial empires, and Coca-Cola, icon of modern American global capitalism," and that it "seems to reflect perfectly the historical elements of the modern world."

Recipes vary somewhat in measures and additional ingredients, but the main ingredients are always rum and cola. The International Bartenders Association recipe calls for 5 centiliters of light rum, 12 cl of cola, and 1 cl of fresh lime juice on ice. However, any amount and proportion of rum and cola may be used. Additionally, while light rum is traditional, dark rums and other varieties are also common. Different colas are also often used; in fact in Cuba, Coca-Cola has not been imported since the U.S. embargo of 1960, so the domestic TuKola is used in Cuba Libres.

Lime is traditionally included, though it is often left out, especially when the order is for just "rum and Coke". Some early recipes called for lime juice to be mixed in, though others included lime only as a garnish. Other early recipes called for additional ingredients such as gin and bitters. Some sources consider lime essential for a drink to be a true Cuba Libre, which they distinguish from a mere rum and Coke. However, lime is frequently included even in orders for "rum and Coke".

When aged añejo rum is used, the drink is sometimes called a Cubata. Some modern recipes inspired by older ones include additional ingredients such as bitters. Some call for other colas such as Mexican Coke (which uses cane sugar instead of high-fructose corn syrup) or Moxie. More elaborate variants with additional ingredients include the Cinema Highball, which uses rum infused with buttered popcorn and mixed with cola. Another is the Mandeville Cocktail, which includes light and dark rum, cola, and citrus juice along with Pernod absynthe and grenadine.


</doc>
<doc id="7607" url="https://en.wikipedia.org/wiki?curid=7607" title="Collagen helix">
Collagen helix

In collagen, the collagen helix, or type-2 helix, is a major shape in secondary structure. It consists of a triple helix made of the repetitious amino acid sequence glycine - X - Y, where X and Y are frequently proline or hydroxyproline.A collagen triple helix has 3.3 residues per turn.
Each of the three chains is stabilized by the steric repulsion due to the pyrrolidine rings of proline and hydroxyproline residues. The pyrrolidine rings keep out of each other’s way when the polypeptide chain assumes this extended helical form, which is much more open than the tightly coiled form of the alpha helix.
The three chains are hydrogen bonded to each other. The hydrogen bond donors are the peptide NH groups of glycine residues. The hydrogen bond acceptors are the CO groups of residues on the other chains. The OH group of hydroxyproline also participates in hydrogen bonding. The rise of the collagen helix (superhelix) is 2.9 Å (0.29 nm) per residue.


</doc>
<doc id="7609" url="https://en.wikipedia.org/wiki?curid=7609" title="Cosmic censorship hypothesis">
Cosmic censorship hypothesis

The weak and the strong cosmic censorship hypotheses are two mathematical conjectures about the structure of gravitational singularities arising in general relativity.

Singularities that arise in the solutions of Einstein's equations are typically hidden within event horizons, and therefore cannot be observed from the rest of spacetime. Singularities that are not so hidden are called "naked". The weak cosmic censorship hypothesis was conceived by Roger Penrose in 1969 and posits that no naked singularities, other than the Big Bang singularity, exist in the universe.

Since the physical behavior of singularities is unknown, if singularities can be observed from the rest of spacetime, causality may break down, and physics may lose its predictive power. The issue cannot be avoided, since according to the Penrose-Hawking singularity theorems, singularities are inevitable in physically reasonable situations. Still, in the absence of naked singularities, the universe, as described by the general theory of relativity, is deterministic —it is possible to predict the entire evolution of the universe (possibly excluding some finite regions of space hidden inside event horizons of singularities), knowing only its condition at a certain moment of time (more precisely, everywhere on a spacelike three-dimensional hypersurface, called the Cauchy surface). Failure of the cosmic censorship hypothesis leads to the failure of determinism, because it is yet impossible to predict the behavior of spacetime in the causal future of a singularity. Cosmic censorship is not merely a problem of formal interest; some form of it is assumed whenever black hole event horizons are mentioned.

The hypothesis was first formulated by Roger Penrose in 1969, and it is not stated in a completely formal way. In a sense it is more of a research program proposal: part of the research is to find a proper formal statement that is physically reasonable and that can be proved to be true or false (and that is sufficiently general to be interesting). Because the statement is not a strictly formal one, there is sufficient latitude for (at least) two independent formulations, a weak form, and a strong form.

The weak and the strong cosmic censorship hypothesis are two conjectures concerned with the global geometry of spacetimes.

The weak cosmic censorship hypothesis asserts there can be no singularity visible from future null infinity. In other words, singularities need to be hidden from an observer at infinity by the event horizon of a black hole. Mathematically, the conjecture states that, for generic initial data, the maximal Cauchy development possesses a complete future null infinity.

The strong cosmic censorship hypothesis asserts that, generically, general relativity is a deterministic theory, in the same sense that classical mechanics is a deterministic theory. In other words, the classical fate of all observers should be predictable from the initial data. Mathematically, the conjecture states that the maximal Cauchy development of generic compact or asymptotically flat initial data is locally inextendible as a regular Lorentzian manifold.

The two conjectures are mathematically independent, as there exist spacetimes for which weak cosmic censorship is valid but strong cosmic censorship is violated and, conversely, there exist spacetimes for which weak cosmic censorship is violated but strong cosmic censorship is valid.

The Kerr metric, corresponding to a black hole of mass formula_1 and angular momentum formula_2, can be used to derive the effective potential for particle orbits restricted to the equator (as defined by rotation). This potential looks like:
where formula_4 is the coordinate radius, formula_5 and formula_6 are the test-particle's conserved energy and angular momentum respectively (constructed from the Killing vectors).

To preserve "cosmic censorship", the black hole is restricted to the case of formula_7. For there to exist an event horizon around the singularity, the requirement formula_7 must be satisfied. This amounts to the angular momentum of the black hole being constrained to below a critical value, outside of which the horizon would disappear. 

The following thought experiment is reproduced from Hartle's "Gravity":

Imagine specifically trying to violate the censorship conjecture. This could be done by somehow imparting an angular momentum upon the black hole, making it exceed the critical value (assume it starts infinitesimally below it). This could be done by sending a particle of angular momentum formula_9. Because this particle has angular momentum, it can only be captured by the black hole if the maximum potential of the black hole is less than formula_10.

Solving the above effective potential equation for the maximum under the given conditions results in a maximum potential of exactly formula_10! Testing other values shows that no particle with enough angular momentum to violate the censorship conjecture would be able to enter the black hole, because they have too much angular momentum to fall in.

There are a number of difficulties in formalizing the hypothesis:


In 1991, John Preskill and Kip Thorne bet against Stephen Hawking that the hypothesis was false. Hawking conceded the bet in 1997, due to the discovery of the special situations just mentioned, which he characterized as "technicalities". Hawking later reformulated the bet to exclude those technicalities. The revised bet is still open, the prize being "clothing to cover the winner's nakedness".

An exact solution to the scalar-Einstein equations formula_14 which forms a counterexample to many formulations of the 
cosmic censorship hypothesis was found by Mark D. Roberts in 1985:
where formula_16 is a constant.





</doc>
<doc id="7610" url="https://en.wikipedia.org/wiki?curid=7610" title="Catholic (term)">
Catholic (term)

The word catholic (with lowercase "c"; derived via Late Latin "catholicus", from the Greek adjective ("katholikos"), meaning "universal") comes from the Greek phrase ("katholou"), meaning "on the whole", "according to the whole" or "in general", and is a combination of the Greek words meaning "about" and meaning "whole". The term Catholic (usually written with uppercase "C" in English) was first used to describe the Christian Church in the early 2nd century to emphasize its universal scope. In the context of Christian ecclesiology, it has a rich history and several usages. 

The word in English can mean either "of the Roman Catholic faith" or "relating to the historic doctrine and practice of the Western Church". Many Christians use it to refer more broadly to the whole Christian Church or to all believers in Jesus Christ regardless of denominational affiliation; it can also more narrowly refer to Catholicity, which encompasses several historic churches sharing major beliefs. "Catholicos", the title used for the head of some churches in Eastern Christian traditions, is derived from the same linguistic origin.

In non-ecclesiastical use, it derives its English meaning directly from its root, and is currently used to mean the following:

The term has been incorporated into the name of the largest Christian communion, the Roman Catholic Church (also called the Catholic Church). All of the three main branches of Christianity in the East (Eastern Orthodox Church, Oriental Orthodox Church and Church of the East) had always identified themselves as "Catholic" in accordance with Apostolic traditions and the Nicene Creed. Anglicans, Lutherans, and some Methodists also believe that their churches are "Catholic" in the sense that they too are in continuity with the original universal church founded by the Apostles. However, each church defines the scope of the "Catholic Church" differently. For instance, the Roman Catholic, Eastern Orthodox, Oriental Orthodox churches, and Church of the East, each maintain that their own denomination is identical with the original universal church, from which all other denominations broke away.

Distinguishing beliefs of Catholicity, the beliefs of most Christians who call themselves "Catholic", include the episcopal polity, that bishops are considered the highest order of ministers within the Christian religion, as well as the Nicene Creed of AD 381. In particular, along with unity, sanctity, and apostolicity, catholicity is considered one of Four Marks of the Church, found the line of the Nicene Creed: "I believe in one holy catholic and apostolic Church."

During the medieval and modern times, additional distinctions arose regarding the use of the terms "Western Catholic" and "Eastern Catholic". Before the East–West Schism, those terms had just the basic geographical meanings, since only one undivided Catholicity existed, uniting the Latin speaking Christians of West and the Greek speaking Christians of the East. After the split of 1054 terminology became much more complicated, resulting in the creation of parallel and confronting terminological systems.

The Greek adjective "katholikos", the origin of the term "catholic" means "universal". Directly from the Greek, or via Late Latin "catholicus", the term "catholic" entered many other languages, becoming the base for the creation of various theological terms such as "catholicism" and "catholicity" (Late Latin "catholicismus", "catholicitas").

The term "catholicism" is the English form of Late Latin "catholicismus", an abstract noun based on the adjective "catholic". The Modern Greek equivalent ("") is back-formed and usually refers to the Catholic Church.

The terms "catholic", "catholicism" and "catholicity" is closely related to the use of the term "Catholic Church". The earliest evidence of the use of that term is the "Letter to the Smyrnaeans" that Ignatius of Antioch wrote in about 108 to Christians in Smyrna. Exhorting Christians to remain closely united with their bishop, he wrote: "Wherever the bishop shall appear, there let the multitude [of the people] also be; even as, wherever Jesus Christ is, there is the Catholic Church."

From the second half of the second century, the word "catholic" began to be used to mean "orthodox" (non-heretical), "because Catholics claimed to teach the whole truth, and to represent the whole Church, while heresy arose out of the exaggeration of some one truth and was essentially partial and local". In 380, Emperor Theodosius I limited use of the term "Catholic Christian" exclusively to those who followed the same faith as Pope Damasus I of Rome and Pope Peter of Alexandria. Numerous other early writers including Cyril of Jerusalem (c. 315–386), Augustine of Hippo (354–430) further developed the use of the term "catholic" in relation to Christianity.

The earliest recorded evidence of the use of the term "Catholic Church" is the "Letter to the Smyrnaeans" that Ignatius of Antioch wrote in about 107 to Christians in Smyrna. Exhorting Christians to remain closely united with their bishop, he wrote: "Wherever the bishop shall appear, there let the multitude [of the people] also be; even as, wherever Jesus Christ is, there is the Catholic Church."

Of the meaning for Ignatius of this phrase J.H. Srawley wrote:
This is the earliest occurrence in Christian literature of the phrase 'the Catholic Church' (ἡ καθολικὴ ἐκκλησία). The original sense of the word is 'universal'. Thus Justin Martyr ("Dial". 82) speaks of the 'universal or general resurrection', using the words ἡ καθολικὴ ἀνάστασις. Similarly here the Church universal is contrasted with the particular Church of Smyrna. Ignatius means by the Catholic Church 'the aggregate of all the Christian congregations' (Swete, "Apostles Creed", p. 76). So too the letter of the Church of Smyrna is addressed to all the congregations of the Holy Catholic Church in every place. And this primitive sense of 'universal' the word has never lost, although in the latter part of the second century it began to receive the secondary sense of 'orthodox' as opposed to 'heretical'. Thus it is used in an early Canon of Scripture, the Muratorian fragment ("circa" 170 A.D.), which refers to certain heretical writings as 'not received in the Catholic Church'. So too Cyril of Jerusalem, in the fourth century, says that the Church is called Catholic not only 'because it is spread throughout the world', but also 'because it teaches completely and without defect all the doctrines which ought to come to the knowledge of men'. This secondary sense arose out of the original meaning because Catholics claimed to teach the whole truth, and to represent the whole Church, while heresy arose out of the exaggeration of some one truth and was essentially partial and local.

By "Catholic Church" Ignatius designated the universal church. Ignatius considered that certain heretics of his time, who disavowed that Jesus was a material being who actually suffered and died, saying instead that "he only seemed to suffer" (Smyrnaeans, 2), were not really Christians.

The term is also used in the "Martyrdom of Polycarp" (155) and in the Muratorian fragment (about 177).

As mentioned in the above quotation from J.H. Srawley, Cyril of Jerusalem (c. 315–386), who is venerated as a saint by the Roman Catholic Church, the Eastern Orthodox Church, and the Anglican Communion, distinguished what he called the "Catholic Church" from other groups who could also refer to themselves as an ἐκκλησία (assembly or church):
Since the word Ecclesia is applied to different things (as also it is written of the multitude in the theatre of the Ephesians, "And when he had thus spoken, he dismissed the Assembly" (Acts 19:14), and since one might properly and truly say that there is a "Church of evil doers", I mean the meetings of the heretics, the Marcionists and Manichees, and the rest, for this cause the Faith has securely delivered to you now the Article, "And in one Holy Catholic Church"; that you may avoid their wretched meetings, and ever abide with the Holy Church Catholic in which you were regenerated. And if ever you are sojourning in cities, inquire not simply where the Lord's House is (for the other sects of the profane also attempt to call their own dens houses of the Lord), nor merely where the Church is, but where is the Catholic Church. For this is the peculiar name of this Holy Church, the mother of us all, which is the spouse of our Lord Jesus Christ, the Only-begotten Son of God(Catechetical Lectures, XVIII, 26).

Theodosius I, Emperor from 379 to 395, declared "Catholic" Christianity the official religion of the Roman Empire, declaring in the Edict of Thessalonica of 27 February 380:

It is our desire that all the various nations which are subject to our clemency and moderation, should continue the profession of that religion which was delivered to the Romans by the divine Apostle Peter, as it has been preserved by faithful tradition and which is now professed by the Pontiff Damasus and by Peter, Bishop of Alexandria, a man of apostolic holiness. According to the apostolic teaching and the doctrine of the Gospel, let us believe in the one Deity of the Father, Son and Holy Spirit, in equal majesty and in a holy Trinity. We authorize the followers of this law to assume the title "Catholic" Christians; but as for the others, since in our judgment they are foolish madmen, we decree that they shall be branded with the ignominious name of heretics, and shall not presume to give their conventicles the name of churches. They will suffer in the first place the chastisement of the divine condemnation, and in the second the punishment which our authority, in accordance with the will of heaven, will decide to inflict. Theodosian Code XVI.i.2

Only slightly later, Saint Augustine of Hippo (354–430) also used the term "Catholic" to distinguish the "true" church from heretical groups:
In the Catholic Church, there are many other things which most justly keep me in her bosom. The consent of peoples and nations keeps me in the Church; so does her authority, inaugurated by miracles, nourished by hope, enlarged by love, established by age. The succession of priests keeps me, beginning from the very seat of the Apostle Peter, to whom the Lord, after His resurrection, gave it in charge to feed His sheep (Jn 21:15–19), down to the present episcopate.
And so, lastly, does the very name of Catholic, which, not without reason, amid so many heresies, the Church has thus retained; so that, though all heretics wish to be called Catholics, yet when a stranger asks where the Catholic Church meets, no heretic will venture to point to his own chapel or house.
Such then in number and importance are the precious ties belonging to the Christian name which keep a believer in the Catholic Church, as it is right they should ... With you, where there is none of these things to attract or keep me... No one shall move me from the faith which binds my mind with ties so many and so strong to the Christian religion... For my part, I should not believe the gospel except as moved by the authority of the Catholic Church. —St. Augustine (354–430): "Against the Epistle of Manichaeus called Fundamental", chapter 4: Proofs of the Catholic Faith.

A contemporary of Augustine, St. Vincent of Lerins, wrote in 434 (under the pseudonym Peregrinus) a work known as the "Commonitoria" ("Memoranda"). While insisting that, like the human body, church doctrine develops while truly keeping its identity (sections 54-59, chapter XXIII), he stated:

During early centuries of Christian history, majority of Christians who followed doctrines represented in Nicene Creed were bound by one common and undivided Catholicity that was uniting the Latin speaking Christians of West and the Greek speaking Christians of the East. In those days, terms "eastern Catholic" and "western Catholic" had their basic geographical meanings, generally corresponding to existing linguistic distinctions between Greek East and Latin West. In spite of various and quite frequent theological and ecclesiastical disagreements between major Christian sees, common Catholicity was preserved until the great disputes that arose between 9th and 11th century. After the East–West Schism, the notion of common Catholicity was broken and each side started to develop its own terminological practice.

All major theological and ecclesiastical disputes in the Christian East or West have been commonly accompanied by attempts of arguing sides to deny each other the right to use the word "Catholic" as term of self-designation. After the acceptance of Filioque clause into the Nicene Creed by the Rome, Orthodox Christians in the East started to refer to adherents of Filioquism in the West just as "Latins" considering them no longer to be "Catholics".

The dominant view in the Eastern Orthodox Church, that all Western Christians who accepted Filioque interpolation and unorthodox Pneumatology ceased to be Catholics, was held and promoted by famous Eastern Orthodox canonist Theodore Balsamon who was patriarch of Antioch. He wrote in 1190:

On the other side of the widening rift, Eastern Orthodox were considered by western theologians to be "Schismatics". Relations between East and West were further estranged by the tragic events of the Massacre of the Latins in 1182 and Sack of Constantinople in 1204. Those bloody events were followed by several failed attempts to reach reconciliation (see: Second Council of Lyon, Council of Florence, Union of Brest, Union of Uzhhorod). During the late medieval and early modern period, terminology became much more complicated, resulting in the creation of parallel and confronting terminological systems that exist today in all of their complexity.

During the Early Modern period, a special term "Acatholic" was widely used in the West to mark all those who were considered to hold heretical theological views and irregular ecclesiastical practices. In the time of Counter-Reformation the term "Acatholic" was used by zealous members of the Catholic Church to designate Protestants as well as Eastern Orthodox Christians. The term was considered to be so insulting that the Council of the Serbian Orthodox Church, held in Temeswar in 1790, decided to send an official plea to emperor Leopold II, begging him to ban the use of the term "Acatholic".

The Augsburg Confession found within the Book of Concord, a compendium of belief of the Lutheran Churches, teaches that "the faith as confessed by Luther and his followers is nothing new, but the true catholic faith, and that their churches represent the true catholic or universal church". When the Lutherans presented the Augsburg Confession to Charles V, Holy Roman Emperor in 1530, they believe to have "showed that each article of faith and practice was true first of all to Holy Scripture, and then also to the teaching of the church fathers and the councils".

The term "Catholic" is commonly associated with the whole of the church led by the Roman Pontiff, the Catholic Church. Other Christian churches that use the description "Catholic" include the Eastern Orthodox Church and other churches that believe in the historic episcopate (bishops), such as the Anglican Communion. Many of those who apply the term "Catholic Church" to all Christians object to the use of the term to designate what they view as only one church within what they understand as the "whole" Catholic Church. In the English language, the first known use of the term is in Andrew of Wyntoun's "Orygynale Cronykil of Scotland", "He was a constant Catholic/All Lollard he hated and heretic."

The Catholic Church, led by the Pope in Rome, usually distinguishes itself from other churches by calling itself the "Catholic", however has also used the description "Roman Catholic". Even apart from documents drawn up jointly with other churches, it has sometimes, in view of the central position it attributes to the See of Rome, adopted the adjective "Roman" for the whole church, Eastern as well as Western, as in the papal encyclicals "Divini illius Magistri" and "Humani generis". Another example is its self-description as "the holy Catholic Apostolic Roman Church" (or, by separating each adjective, as the "Holy, Catholic, Apostolic and Roman Church") in the 24 April 1870 Dogmatic Constitution on the Catholic Faith of the First Vatican Council. In all of these documents it also refers to itself both simply as the Catholic Church and by other names. The Eastern Catholic Churches, while united with Rome in the faith, have their own traditions and laws, differing from those of the Latin Rite and those of other Eastern Catholic Churches.

The contemporary Catholic Church has always considered itself to be the historic Catholic Church, and consider all others as "non-Catholics". This practice is an application of the belief that not all who claim to be Christians are part of the Catholic Church, as Ignatius of Antioch, the earliest known writer to use the term "Catholic Church", considered that certain heretics who called themselves Christians only seemed to be such.
Regarding the relations with Eastern Christians, Pope Benedict XVI stated his wish to restore full unity with the Orthodox. The Roman Catholic Church considers that almost all of the ancient theological differences have been satisfactorily addressed (the Filioque clause, the nature of purgatory, etc.), and has declared that differences in traditional customs, observances and discipline are no obstacle to unity.

Recent historic ecumenical efforts on the part of the Catholic Church have focused on healing the rupture between the Western ("Catholic") and the Eastern ("Orthodox") churches. Pope John Paul II often spoke of his great desire that the Catholic Church "once again breathe with both lungs", thus emphasizing that the Roman Catholic Church seeks to restore full communion with the separated Eastern churches.

All of the three main branches of Christianity in the East (Eastern Orthodox Church, Oriental Orthodox Church and Church of the East) are continuing to identify themselves as "Catholic" in accordance with Apostolic traditions and the Nicene Creed. The Eastern Orthodox Church firmly upholds the ancient doctrines of Eastern Orthodox Catholicity and commonly uses the term "Catholic", as in the title of "The Longer Catechism of the Orthodox, Catholic, Eastern Church". So does the Coptic Orthodox Church that belongs to Oriental Orthodoxy and considers its communion to be "the True Church of the Lord Jesus Christ". Non of the Eastern Churches, Orthodox or Oriental, have any intention to abandon ancient traditions of their own Catholicity.

Most Reformation and post-Reformation churches use the term "Catholic" (often with a lower-case "c") to refer to the belief that all Christians are part of one Church regardless of denominational divisions; e.g., Chapter XXV of the Westminster Confession of Faith refers to the "catholic or universal Church". It is in line with this interpretation, which applies the word "catholic" (universal) to no one denomination, that they understand the phrase "one holy catholic and apostolic Church" in the Nicene Creed, the phrase "the Catholic faith" in the Athanasian Creed and the phrase "holy catholic church" in the Apostles' Creed.

The term is used also to mean those Christian churches that maintain that their episcopate can be traced unbrokenly back to the apostles and consider themselves part of a "catholic" (universal) body of believers. Among those who regard themselves as "Catholic" but not "Roman Catholic" are Anglicans and Lutherans, who stress that they are both Reformed and Catholic. The Old Catholic Church and the various groups classified as Independent Catholic Churches also lay claim to the description "Catholic". Traditionalist Catholics, even if they may not be in communion with Rome, consider themselves to be not only Catholics but the "true" Roman Catholics.

Some use the term "Catholic" to distinguish their own position from a Calvinist or Puritan form of Reformed-Protestantism. These include a faction of Anglicans often also called Anglo-Catholics, 19th century Neo-Lutherans, 20th century High Church Lutherans or evangelical-Catholics and others.

Methodists and Presbyterians believe their denominations owe their origins to the Apostles and the early church, but do not claim descent from ancient church structures such as the episcopate. However, both of these churches hold that they are a part of the catholic (universal) church. According to "Harper's New Monthly Magazine": As such, according to one viewpoint, for those who "belong to the Church," the term Methodist Catholic, or Presbyterian Catholic, or Baptist Catholic, is as proper as the term Roman Catholic. It simply means that body of Christian believers over the world who agree in their religious views, and accept the same ecclesiastical forms.

Some Independent Catholics accept that, among bishops, that of Rome is "primus inter pares", and hold that conciliarism is a necessary check against ultramontanism. They are however, by definition, not recognised by the Catholic Church.

Some Protestant churches avoid using the term completely, to the extent among many Lutherans of reciting the Creed with the word "Christian" in place of "catholic". The Orthodox churches share some of the concerns about Roman Catholic papal claims, but disagree with some Protestants about the nature of the church as one body.


</doc>
<doc id="7611" url="https://en.wikipedia.org/wiki?curid=7611" title="Crystal Eastman">
Crystal Eastman

Crystal Catherine Eastman (June 25, 1881 – July 8, 1928) was an American lawyer, antimilitarist, feminist, socialist, and journalist. She is best remembered as a leader in the fight for women's suffrage, as a co-founder and co-editor with her brother Max Eastman of the radical arts and politics magazine "The Liberator," co-founder of the Women's International League for Peace and Freedom, and co-founder in 1920 of the American Civil Liberties Union. In 2000 she was inducted into the National Women's Hall of Fame in Seneca Falls, New York.

Crystal Eastman was born in Marlborough, Massachusetts, on June 25, 1881, the third of four children. Her oldest brother, Morgan, was born in 1878 and died in 1884 at age seven. The second brother, Anstice Ford Eastman, who became a general surgeon, was born in 1878 and died in 1937. Max was the youngest, born in 1882. In 1883 their parents, Samuel Elijah Eastman and Annis Bertha Ford, moved the family to Canandaigua, New York. In 1889, their mother became one of the first women ordained as a Protestant minister in America when she became a minister of the Congregational Church. Her father was also a Congregational minister, and the two served as pastors at the church of Thomas K. Beecher near Elmira. This part of New York was in the so-called "Burnt Over District." During the Second Great Awakening earlier in the 19th century, its frontier had been a center of evangelizing and much religious excitement, which resulted in the founding of the Shakers and Mormonism. During the antebellum period, some were inspired by religious ideals to support such progressive social causes as abolitionism and the Underground Railroad.

Crystal and her brother Max Eastman were influenced by this progressive tradition. Their parents were friendly with the writer Mark Twain. From this association young Crystal also became acquainted with him.
She was the sister of the socialist activist Max Eastman, with whom she was quite close throughout her life. The two lived together for several years on 11th Street in Greenwich Village among other radical activists. The group, including Ida Rauh, Inez Milholland, Floyd Dell, and Doris Stevens, also spent summers and weekends in Croton-on-Hudson.

Eastman graduated from Vassar College in 1903 and received an M.A. in sociology (a relatively new field) from Columbia University in 1904. Gaining her law degree from New York University Law School, she graduated second in the class of 1907.

Social work pioneer and journal editor Paul Kellogg offered Eastman her first job, investigating labor conditions for The Pittsburgh Survey sponsored by the Russell Sage Foundation. Her report, "Work Accidents and the Law" (1910), became a classic and resulted in the first workers' compensation law, which she drafted while serving on a New York state commission.

She continued to campaign for occupational safety and health while working as an investigating attorney for the U.S. Commission on Industrial Relations during Woodrow Wilson's presidency. She was at one time called the "most dangerous woman in America," due to her free-love idealism and outspoken nature.

During a brief marriage to Wallace J. Benedict which ended in divorce, Eastman moved to Milwaukee and managed the unsuccessful 1912 Wisconsin suffrage campaign.

When she returned east in 1913, she joined Alice Paul, Lucy Burns, and others in founding the militant Congressional Union, which became the National Woman's Party. After the passage of the 19th Amendment gave women the right to vote in 1920, Eastman and Paul wrote the Equal Rights Amendment, first introduced in 1923. One of the few socialists to endorse the ERA, she warned that protective legislation for women would mean only discrimination against women. Eastman claimed that one could assess the importance of the ERA by the intensity of the opposition to it, but she felt that it was still a struggle worth fighting. She also delivered the speech, "Now We Can Begin" following the ratification of the Nineteenth Amendment, outlining the work that needed to be done in the political and economic spheres to achieve gender equality.

During World War I, Eastman was one of the founders of the Woman's Peace Party, soon joined by Jane Addams, Lillian D. Wald, and others. She served as president of the New York branch. Renamed the Women's International League for Peace and Freedom in 1921, it remains the oldest extant women's peace organization. Eastman also became executive director of the American Union Against Militarism, which lobbied against America's entrance into the European war and more successfully against war with Mexico in 1916, sought to remove profiteering from arms manufacturing, and campaigned against conscription, imperial adventures and military intervention.

When the United States entered World War I, Eastman organized with Roger Baldwin and Norman Thomas the National Civil Liberties Bureau to protect conscientious objectors, or in her words: "To maintain something over here that will be worth coming back to when the weary war is over." The NCLB grew into the American Civil Liberties Union, with Baldwin at the head and Eastman functioning as attorney-in-charge. Eastman is credited as a founding member of the ACLU, but her role as founder of the NCLB may have been largely ignored by posterity due to her personal differences with Baldwin.

In 1916 Eastman married the British editor and antiwar activist Walter Fuller, who had come to the United States to direct his sisters’ singing of folksongs. They had two children, Jeffrey and Annis. They worked together as activists until the end of the war; then he worked as the managing editor of "The Freeman" until 1922, when he returned to England. He died in 1927, nine months before Crystal, ending his career editing "Radio Times" for the BBC.

After Max Eastman's periodical "The Masses" was forced to close by government censorship in 1917, he and Crystal co-founded a radical journal of politics, art, and literature, "The Liberator", early in 1918. She and Max co-edited it until they put it in the hands of faithful friends in 1922.

After the war, Eastman organized the First Feminist Congress in 1919.

She traveled by ship to London to be with her husband at times. In New York, her activities led to her being blacklisted during the Red Scare of 1919-1920. She struggled to find paying work.

During the 1920s her only paid work was as a columnist for feminist journals, notably "Equal Rights" and "Time and Tide". Eastman claimed that "life was a big battle for the complete feminist," but she was convinced that the complete feminist would someday achieve total victory.

Crystal Eastman died on July 8, 1928, of nephritis. Her friends were entrusted with her two children, then orphans, to rear them until adulthood.

Eastman has been called one of the United States' most neglected leaders, because, although she wrote pioneering legislation and created long-lasting political organizations, she disappeared from history for fifty years. Freda Kirchwey, then editor of "The Nation", wrote at the time of her death: "When she spoke to people—whether it was to a small committee or a swarming crowd—hearts beat faster. She was for thousands a symbol of what the free woman might be."

Her speech "Now We Can Begin", given in 1920, is listed as #83 in American Rhetoric's Top 100 Speeches of the 20th Century (listed by rank).

In 2000 Eastman was inducted into the (American) National Women's Hall of Fame in Seneca Falls, New York.

In 2018 "The Socialist", the official publication of the Socialist Party USA, published the article "Remembering Socialist Feminist Crystal Eastman" by Lisa Petriello, which was written "on the 90th-year anniversary of her [Eastman's] death to bring her life and legacy once again to the public eye."

Eastman's papers are housed at Harvard University.

The Library of Congress has the following publications by Eastman in its collection, much of them published posthumously:




</doc>
<doc id="7612" url="https://en.wikipedia.org/wiki?curid=7612" title="Christopher Alexander">
Christopher Alexander

Christopher Wolfgang Alexander (born 4 October 1936 in Vienna, Austria) is a widely influential architect and design theorist, and currently emeritus professor at the University of California, Berkeley. His theories about the nature of human-centered design have affected fields beyond architecture, including urban design, software, sociology and others. Alexander has designed and personally built over 100 buildings, both as an architect and a general contractor.

In software, Alexander is regarded as the father of the pattern language movement. The first wiki—the technology behind Wikipedia—led directly from Alexander's work, according to its creator, Ward Cunningham. Alexander's work has also influenced the development of agile software development.

In architecture, Alexander's work is used by a number of different contemporary architectural communities of practice, including the New Urbanist movement, to help people to reclaim control over their own built environment. However, Alexander is controversial among some mainstream architects and critics, in part because his work is often harshly critical of much of contemporary architectural theory and practice.

Alexander is known for many books on the design and building process, including "Notes on the Synthesis of Form, A City is Not a Tree" (first published as a paper and recently re-published in book form), "The Timeless Way of Building, A New Theory of Urban Design," and "The Oregon Experiment." More recently he published the four-volume "The Nature of Order: An Essay on the Art of Building and the Nature of the Universe," about his newer theories of "morphogenetic" processes, and "The Battle for the Life and Beauty of the Earth", about the implementation of his theories in a large building project in Japan. All his works are developed or accumulated from his previous works, so his works should be read as a whole rather than fragmented pieces. His life's work or the best of his works is The Nature of Order on which he spent about 30 years, and the very first version of The Nature of Order was done in 1981, one year before this famous debate with Peter Eisenman in Harvard.

Alexander is perhaps best known for his 1977 book "A Pattern Language," a perennial seller some four decades after publication. Reasoning that users are more sensitive to their needs than any architect could be, he produced and validated (in collaboration with his students Sara Ishikawa, Murray Silverstein, Max Jacobson, Ingrid King, and Shlomo Angel) a "pattern language" to empower anyone to design and build at any scale.

As a young child Alexander emigrated in fall 1938 with his parents from Austria to England, when his parents were forced to flee the Nazi regime. He spent much of his childhood in Chichester and Oxford, England, where he began his education in the sciences. He moved from England to the United States in 1958 to study at Harvard University and Massachusetts Institute of Technology. He moved to Berkeley, California in 1963 to accept an appointment as Professor of Architecture, a position he would hold for almost 40 years. In 2002, after his retirement, Alexander moved to Arundel, England, where he continues to write, teach and build. Alexander is married to Margaret Moore Alexander, and he has two daughters, Sophie and Lily, by his former wife Pamela.

Alexander attended Oundle School, England. In 1954, he was awarded the top open scholarship to Trinity College, Cambridge University in chemistry and physics, and went on to read mathematics. He earned a Bachelor's degree in Architecture and a Master's degree in Mathematics. He took his doctorate at Harvard (the first Ph.D. in Architecture ever awarded at Harvard University), and was elected fellow at Harvard. During the same period he worked at MIT in transportation theory and computer science, and worked at Harvard in cognition and cognitive studies.

Alexander was elected to the Society of Fellows, Harvard University 1961-64; awarded the First Medal for Research by the American Institute of Architects, 1972; elected member of the Swedish Royal Academy, 1980; winner of the Best Building in Japan award, 1985; winner of the ACSA (Association of Collegiate Schools of Architecture) Distinguished Professor Award, 1986 and 1987; invited to present the Louis Kahn Memorial Lecture, 1992; awarded the Seaside Prize, 1994; elected a Fellow of the American Academy of Arts and Sciences, 1996; one of the two inaugural recipients of the Athena Award, given by the Congress for the New Urbanism (CNU), 2006;. awarded ("in absentia") the Vincent Scully Prize by the National Building Museum, 2009; awarded the lifetime achievement award by the Urban Design Group, 2011; winner of the Global Award for Sustainable Architecture, 2014.

"The Timeless Way of Building" (1979) described the perfection of use to which buildings could aspire:

"A Pattern Language: Towns, Buildings, Construction" (1977) described a practical architectural system in a form that a theoretical mathematician or computer scientist might call a generative grammar.

The work originated from an observation that many medieval cities are attractive and harmonious. The authors said that this occurs because they were built to local regulations that required specific features, but freed the architect to adapt them to particular situations.

The book provides rules and pictures, and leaves decisions to be taken from the precise environment of the project. It describes exact methods for constructing practical, safe and attractive designs at every scale, from entire regions, through cities, neighborhoods, gardens, buildings, rooms, built-in furniture, and fixtures down to the level of doorknobs.

A notable value is that the architectural system consists only of classic patterns tested in the real world and reviewed by multiple architects for beauty and practicality.

The book includes all needed surveying and structural calculations, and a novel simplified building system that copes with regional shortages of wood and steel, uses easily stored inexpensive materials, and produces long-lasting classic buildings with small amounts of materials, design and labor. It first has users prototype a structure on-site in temporary materials. Once accepted, these are finished by filling them with very-low-density concrete. It uses vaulted construction to build as high as three stories, permitting very high densities.

This book's method was adopted by the University of Oregon, as described in "The Oregon Experiment" (1975), and remains the official planning instrument. It has also been adopted in part by some cities as a building code.

The idea of a pattern language appears to apply to any complex engineering task, and has been applied to some of them. It has been especially influential in software engineering where patterns have been used to document collective knowledge in the field.

"A New Theory of Urban Design" (1987) coincided with a renewal of interest in urbanism among architects, but stood apart from most other expressions of this by assuming a distinctly anti-masterplanning stance. An account of a design studio conducted with Berkeley students on a site in San Francisco, it shows how convincing urban networks can be generated by requiring individual actors to respect only "local" rules, in relation to neighbours. A vastly undervalued part of the Alexander canon, "A New Theory" is important in understanding the generative processes which give rise to the shanty towns latterly championed by Stewart Brand, Robert Neuwirth, and the Prince of Wales. There have been critical reconstructions of Alexander's design studio based on the theories put forward in "A New Theory of Urban Design".

"The Nature of Order: An Essay on the Art of Building and the Nature of the Universe" (2003–04), which includes The "Phenomenon of Life", "The Process of Creating Life", "A Vision of a Living World" and "The Luminous Ground", is Alexander's most comprehensive and elaborate work. In it, he puts forth a new theory about the nature of space and describes how this theory influences thinking about architecture, building, planning, and the way in which we view the world in general. The mostly static patterns from "A Pattern Language" have been amended by more dynamic sequences, which describe how to work towards patterns (which can roughly be seen as the end result of sequences). Sequences, like patterns, promise to be tools of wider scope than building (just as his theory of space goes beyond architecture).

The online publication "Katarxis 3" (September 2004) includes several essays by Christopher Alexander, as well as the legendary debate between Alexander and Peter Eisenman from 1982.

Alexander's latest book, "The Battle for the Life and Beauty of the Earth: A Struggle Between Two World-Systems" (2012), is the story of the largest project he and his colleagues had ever tackled, the construction of a new High School/College campus in Japan. He also uses the project to connect with themes in his four-volume series. He contrasts his approach, (System A) with the construction processes endemic in the US and Japanese economies (System B). As Alexander describes it, System A is focused on enhancing the life/spirit of spaces within given constraints (land, budget, client needs, etc.) (drawings are sketches - decisions on placing buildings, materials used, finish and such are made in the field as construction proceeds, with adjustments as needed to meet overall budget); System B ignores, and tends to diminish or destroy that quality (architect responsible for drawings which the builder uses to build structures at the lowest possible cost). In the last few chapters he describes "centers" as a way of thinking about the connections among spaces, and about what brings more wholeness and life to a space.

Among Alexander's most notable built works are the Eishin Campus near Tokyo (the building process of which is outlined in his 2012 book "The Battle for the Life and Beauty of the Earth"); the West Dean Visitors Centre in West Sussex, England; the Julian Street Inn (a homeless shelter) in San Jose, California (both described in "Nature of Order"); the Sala House and the Martinez House (experimental houses in Albany and Martinez, California made of lightweight concrete); the low-cost housing in Mexicali, Mexico (described in "The Production of Houses"); and several private houses (described and illustrated in "The Nature of Order"). Alexander's built work is characterized by a special quality (which he used to call "the quality without a name", but named "wholeness" in "Nature of Order") that relates to human beings and induces feelings of belonging to the place and structure. This quality is found in the most loved traditional and historic buildings and urban spaces, and is precisely what Alexander has tried to capture with his sophisticated mathematical design theories. Paradoxically, achieving this connective human quality has also moved his buildings away from the abstract imageability valued in contemporary architecture, and this is one reason why his buildings are under-appreciated at present.

His former student and colleague Michael Mehaffy wrote an introductory essay on Alexander's built work in the online publication "Katarxis 3", which includes a gallery of Alexander's major built projects through September 2004.

In addition to his lengthy teaching career as a Professor at UC Berkeley (during which a number of international students began to appreciate and apply his methods), Alexander was a key faculty member at both The Prince of Wales's Summer Schools in Civil Architecture (1990–1994) and The Prince's Foundation for the Built Environment. He also initiated the process which led to the international Building Beauty post-graduate school for architecture, which launched in Sorrento, Italy for the 2017-18 academic year.

Alexander's work has widely influenced architects; among those who acknowledge his influence are Sarah Susanka, Andres Duany, and Witold Rybczynski. Robert Campbell, the Pulitzer Prize-winning architecture critic for the "Boston Globe", stated that Alexander "has had an enormous critical influence on my life and work, and I think that's true of a whole generation of people."

Architecture critic Peter Buchanan, in an essay for "The Architectural Review"s 2012 campaign "The Big Rethink", argues that Alexander's work as reflected in "A Pattern Language" is "thoroughly subversive and forward looking rather than regressive, as so many misunderstand it to be." He continues:
Many urban development projects continue to incorporate Alexander's ideas. For example, in the UK the developers Living Villages have been highly influenced by Alexander's work and used "A Pattern Language" as the basis for the design of The Wintles in Bishops Castle, Shropshire. Sarah Susanka's "Not So Big House" movement adapts and popularizes Alexander's patterns and outlook.

Alexander's "Notes on the Synthesis of Form" was said to be required reading for researchers in computer science throughout the 1960s. It had an influence in the 1960s and 1970s on programming language design, modular programming, object-oriented programming, software engineering and other design methodologies. Alexander's mathematical concepts and orientation were similar to Edsger Dijkstra's influential "A Discipline of Programming".

The greatest influence of "A Pattern Language" in computer science is the design patterns movement. Alexander's philosophy of incremental, organic, coherent design also influenced the extreme programming movement. The Wiki was invented to allow the Hillside Group to work on programming design patterns. More recently the "deep geometrical structures" as discussed in "The Nature of Order" have been cited as having importance for object-oriented programming, particularly in C++.

Will Wright wrote that Alexander's work was influential in the origin of the "SimCity" computer games, and in his later game "Spore".

Alexander has often led his own software research, such as the 1996 Gatemaker project with Greg Bryant.

Alexander discovered and conceived a recursive structure, so called wholeness, which is defined mathematically, exists in space and matter physically, and reflects in our minds and cognition psychologically. He had his idea of wholeness back to early 1980s when he finished his very first version of "The Nature of Order". In fact, his idea of wholeness or degree of wholeness relying a recursive structure of centers resemble in spirit to Google's PageRank.

The fourth volume of "The Nature of Order" approaches religious questions from a scientific and philosophical rather than mystical direction. In it, Alexander describes deep ties between the nature of matter, human perception of the universe, and the geometries people construct in buildings, cities, and artifacts. He suggests a crucial link between traditional practices and beliefs, and recent scientific advances. Despite his leanings toward Deism, and his naturalistic and anthropological approach to religion, Alexander maintains that he is a practicing member of the Catholic Church, believing it to embody a great deal of accumulated human truth within its rituals.

The life's work of Alexander is dedicated to turn design from unselfconscious behavior to selfconscious behavior, so called design science. In his very first book "Notes on the Synthesis of Forms", he has set what he wanted to do. He was inspired by traditional buildings, and tried to derive some 253 patterns for architectural design. Later on, he further distills 15 geometric properties to characterize living structure in "The Nature of Order". The design principles are differentiation and adaptation.

In his classic ‘A City is Not a Tree’, he already had some primary ideas of complex networks, although he used semilattice rather than complex networks. In his first book ‘Notes on the synthesis of forms’ on page 65, he illustrated something that is in fact community or community structure in complex networks, a recent topic emerged around 2004: Community structure.

Alexander's published works include:


Unpublished:





</doc>
<doc id="7614" url="https://en.wikipedia.org/wiki?curid=7614" title="Clabbers">
Clabbers

Clabbers is a game played by tournament Scrabble players for fun, or occasionally at Scrabble variant tournaments. The name derives from the fact that the words CLABBERS and SCRABBLE form an anagram pair.

The rules are identical to those of Scrabble, except that valid plays are only required to form anagrams of acceptable words; in other words, the letters in a word do not need to be placed in the correct order. If a word is challenged, the player who played the word must then name an acceptable word that anagrams to the tiles played.

Because the number of "words" that can be formed is vastly larger than in standard English, the board usually ends up tightly packed in places, and necessarily quite empty in others. Game scores will often be much higher than in standard Scrabble, due to the relative ease of making high-scoring overlap plays and easier access to premium squares.

The Internet Scrabble Club offers the ability to play Clabbers online.

Horizontal words from top to bottom (# denotes words that exist in the Collins English Dictionary but not the TWL). Some of the words below have multiple anagrams:


Vertical words from left to right




</doc>
<doc id="7616" url="https://en.wikipedia.org/wiki?curid=7616" title="Canopus (disambiguation)">
Canopus (disambiguation)

Canopus may refer to:



</doc>
<doc id="7617" url="https://en.wikipedia.org/wiki?curid=7617" title="Corum Jhaelen Irsei">
Corum Jhaelen Irsei

Corum Jhaelen Irsei ("the Prince in the Scarlet Robe") is the name of a fictional fantasy hero in a series of two trilogies written by author Michael Moorcock.

Corum is the last survivor of the Vadhagh race and an incarnation aspect of the Eternal Champion, a being that exists in all worlds to ensure there is "Cosmic Balance".

This trilogy consists of "The Knight of the Swords" (1971), "The Queen of the Swords" (1971), and "The King of the Swords" (1971). In the United Kingdom it has been collected as an omnibus edition titled "Corum", "Swords of Corum" and most recently "Corum: The Prince in the Scarlet Robe" (vol. 30 of Orion's Fantasy Masterworks series). In the United States the first trilogy has been published as "Corum: The Coming of Chaos".

Corum is a Vadhagh, one of a race of long-lived beings with limited magical abilities dedicated to peaceful pursuits such as art and poetry. A group of "Mabden" (men) led by the savage Earl Glandyth-a-Krae raid the family castle and slaughter everyone with the exception of Corum, who escapes. Arming himself, Corum attacks and kills several of the Mabden before being captured and tortured. After having his left hand cut off and right eye put out, Corum escapes by moving into another plane of existence, becoming invisible to the Mabden. They depart and Corum is found by The Brown Man, a dweller of the forest of Laar able to see Corum while out of phase. The Brown Man takes Corum to a being called Arkyn, who treats his wounds and explains he has a higher purpose.

Travelling to Moidel's Castle, Corum encounters his future lover, the Margravine Rhalina, a mabden woman of the civilized land of Lwym an Esh. Having found out Corum's location by torturing and killing the Brown Man of Laar, Glandyth-a-Krae marshalled his allies to Moidel's Castle. Glandyth had kept Corum's former hand and eye as souvenirs, and showed them to Corum to provoke a reaction. Rhalina uses sorcery (a ship summoned from the depths of the ocean and manned by her drowned dead husband and crew) to ward off an attack by Glandyth-a-Krae. Determined to restore himself, Corum and Rhalina travel to the island of Shool, a near immortal and mad sorcerer. During the journey Corum observes a mysterious giant who trawls the ocean with a net. On arrival at the island Shool takes Rhalina hostage, and then provides Corum with two artifacts to replace his lost hand and eye: the Hand of Kwll and the Eye of Rhynn. The Eye of Rhynn allows Corum to see into an undead netherworld where the last beings killed by Corum exist until summoned by the Hand of Kwll.

Shool then explains that Corum's ill fortune has been caused by the Chaos God Arioch, the Knight of the Swords. When Arioch and his fellow Chaos Lords conquered the Fifteen Planes, the balance between the forces of Law and Chaos tipped in favor of Chaos, and their minions - such as Glandyth-a-Krae - embarked on a bloody rampage. Shool sends Corum to Arioch's fortress to steal the Heart of Arioch, which the sorcerer intends to use to attain greater power. Corum confronts Arioch, and learns Shool is nothing more than a pawn of the Chaos God. Arioch then ignores Corum, who discovers the location of the Heart. Corum is then attacked by Arioch, but the Hand of Kwll crushes the Heart and banishes the Chaos God forever. Before fading from existence, Arioch warns Corum that he has now earned the enmity of the Sword Rulers. Corum returns to the island to rescue Rhalina, and observes Shool has become a powerless moron, and is devoured by his own creations soon afterwards. Corum learns Arkyn is in fact a Lord of Law, and that this is the first step towards Law regaining control of the Fifteen Planes.

On another five planes, the forces of Chaos - led by Xiombarg, Queen of the Swords - reign supreme and are on the verge on eradicating the last resistance from the forces of Law. The avatars of the Bear and Dog gods plot with Earl Glandyth-a-Krae to murder Corum and return Arioch to the Fifteen Planes. Guided by Arkyn, Corum, Rhalina and companion Jhary-a-Conel cross the planes and encounter the King Without A Country, the last of his people who in turn is seeking the City in the Pyramid. The group locate the City, which is in fact a floating arsenal powered by advanced technology and inhabited by a people originally from Corum's world and his distant kin.

Besieged by the forces of Chaos, the City requires certain rare minerals to continue to power their weapons. Corum and Jhary attempt to locate the minerals and also encounter Xiombarg, who learns of Corum's identity. Corum slows Xiombarg's forces by defeating their leader, Prince Gaynor the Damned. Xiombarg is goaded into attacking the City directly in revenge for Arioch's banishment. Arkyn provides the minerals and confronts Xiombarg, who has manifested in a vulnerable state. As Arkyn banishes Xiombarg, Corum and his allies devastate the forces of Chaos. Glandyth-a-Krae, however, escapes, and seeks revenge.

A spell - determined to have been cast by the forces of Chaos - forces the inhabitants of Corum's plane to war with each other (including the City in the Pyramid). Desperate to stop the slaughter, Corum, Rhalina and Jhary-a-Conel travel to the last five planes, ruled by Mabelode, the King of the Swords. Rhalina is taken hostage by the forces of Chaos and Corum has several encounters with the forces of Chaos, including Earl Glandyth-a-Krae.

Corum also meets two other aspects of the Eternal Champion: Elric and Erekosë, with all three seeking the mystical city of Tanelorn for their own purposes. After a brief adventure in the "Vanishing Tower", the other heroes depart and Corum and Jhary arrive at their version of Tanelorn. Corum discovers one of the "Lost Gods", the being Kwll, who is imprisoned and cannot be freed until whole. Corum offers Kwll his hand, on the condition that he aid them against Mabelode. Kwll accepts the terms, but reneges on the bargain until persuaded to assist. Corum is also stripped of his artificial eye, which belongs to Rhynn - actually the mysterious giant Corum had previously encountered. Kwll transports Corum and Jhary to the court of Mabelode, with the pair fleeing with Rhalina when Kwll directly challenges the Chaos God.

In a final battle Corum avenged his family by killing Glandyth-a-Krae and decimating the last of Chaos' mortal forces. Kwll later located Corum and revealed that all the gods - of both Chaos and Law - have been slain in order to free humanity and allow it to shape its own destiny.

This trilogy consists of "The Bull and the Spear" (1973), "The Oak and the Ram" (1973), and "The Sword and the Stallion" (1974). It was titled "The Prince with the Silver Hand" in the United Kingdom and "The Chronicles of Corum" in the United States respectively. The previous trilogy hinted at a Celtic or proto-Celtic setting for the stories - the terms "mabden" (human beings) and "shefanhow" (demons) occurring in these books are both Cornish language words. The Silver Hand trilogy is more explicit in its Celtic connections, with overt borrowings from Celtic mythology.

Set eighty years after the defeat of the Sword Rulers, Corum has become despondent and alone since the death of his Mabden bride Rhalina. Plagued by voices at night, Corum believes he has gone insane until old friend Jhary-a-Conel advises Corum it is in fact a summons from another world. Listening to the voices allows Corum to pass to the other world, which is in fact the distant future. The descendants of Rhalina's folk, the Tuha-na-Cremm Croich (see: Crom Cruach), who call Corum "Corum Llew Ereint" (see: Lludd Llaw Eraint), face extinction by the Fhoi Myore (Fomorians). The Fhoi Myore, seven powerful but diseased and barely sentient giants, with the aid of their allies have conquered the land and plunged it into eternal winter. Allying himself with King Mannach, ruler of the Tuha-na-Cremm Croich, Corum falls in love with his daughter Medhbh (see: Medb).

Corum also hears the prophecy of a seeress, who claims Corum should fear a brother (who will apparently slay him), a harp and above all, beauty. Corum seeks the lost artifacts of the Tuha-na-Cremm Croich - a sacred Bull, a spear, an oak, a ram, a sword and a stallion - which will restore the land. Corum gains new allies, Goffanon (a blacksmith and diminutive giant, a member of the Sidhe race) and Goffanon's cousin and true giant Illbrec. They battle the Fhoi Myore, who themselves have allies: a returned Prince Gaynor, the wizard Calatin and his clone of Corum, the Brothers of the Pine, the undead Ghoolegh and a host of giant demonic dogs. After being instrumental in the death of two of the Fhoi Myore and restoring to his senses the enscorceled Amergin, the High King and Chief Druid of the Tuha-na-Cremm Croich, Corum and his allies fight a final battle in which all their foes are destroyed.

Corum decides not to return his own world, and is attacked by his clone, whom he defeats with the aid of a spell placed on his silver hand by Medhbh. Medhbh, however, attacks and wounds Corum, having been told by the being the Dagdah that their world must be free of all gods and demi-gods if they are to flourish as a people. Corum is then killed with his own sword by his animated silver hand, thereby fulfilling the prophecy.

"The Swords" trilogy:

"The Silver Hand" trilogy:

Additional appearances:

The August Derleth Award won by:


First Comics published "The Chronicles of Corum", a twelve issue limited series (Jan. 1986 - Dec. 1988) that adapted the "Swords Trilogy", and was followed by the four issue limited series "Corum: The Bull and the Spear" (Jan. - July (bi-monthly) 1989), which adapted the first book in the second trilogy.

Darcsyde Productions produced a supplement for use with Chaosium's "Stormbringer" (2001) role-playing game adapting the characters and settings from the "Corum" series for role-playing.

Gollancz have announced plans to release the entire Corum stories in both print and ebook form, commencing in 2013. The ebooks will be available via Gollancz's SF Gateway site.

Audiobooks

In 2016, produced dramatized audiobook versions of Corum.



</doc>
<doc id="7618" url="https://en.wikipedia.org/wiki?curid=7618" title="Cumberland (disambiguation)">
Cumberland (disambiguation)

Cumberland is one of the historic counties of England.

Cumberland may also refer to:


















</doc>
<doc id="7619" url="https://en.wikipedia.org/wiki?curid=7619" title="Capella (disambiguation)">
Capella (disambiguation)

Capella is a bright star in the constellation of Auriga.

Capella may also refer to:






</doc>
<doc id="7622" url="https://en.wikipedia.org/wiki?curid=7622" title="Complex instruction set computer">
Complex instruction set computer

A complex instruction set computer (CISC ) is a computer in which single instructions can execute several low-level operations (such as a load from memory, an arithmetic operation, and a memory store) or are capable of multi-step operations or addressing modes within single instructions. The term was retroactively coined in contrast to reduced instruction set computer (RISC) and has therefore become something of an umbrella term for everything that is not RISC, from large and complex mainframe computers to simplistic microcontrollers where memory load and store operations are not separated from arithmetic instructions. A modern RISC processor can therefore be much more complex than, say, a modern microcontroller using a CISC-labeled instruction set, especially in terms of electronic circuit complexity, but also in terms of the number of instructions or the complexity of their encoding patterns. The only typical differentiating characteristic is that most RISC designs use uniform instruction length for almost all instructions, and employ strictly separate load/store-instructions.

Examples of instruction set architectures that have been retroactively labeled CISC are System/360 through z/Architecture, the PDP-11 and VAX architectures, Data General Nova and many others. Well known microprocessors and microcontrollers that have also been labeled CISC in many academic publications include the Motorola 6800, 6809 and 68000-families; the Intel 8080, iAPX432 and x86-family; the Zilog Z80, Z8 and Z8000-families; the National Semiconductor 32016 and NS320xx-line; the MOS Technology 6502-family; the Intel 8051-family; and others.

Some designs have been regarded as borderline cases by some writers. For instance, the Microchip Technology PIC has been labeled RISC in some circles and CISC in others. The 6502 and 6809 have both been described as "RISC-like", although they have complex addressing modes as well as arithmetic instructions that operate on memory, contrary to the RISC-principles.

Before the RISC philosophy became prominent, many computer architects tried to bridge the so-called semantic gap, i.e., to design instruction sets that directly support high-level programming constructs such as procedure calls, loop control, and complex addressing modes, allowing data structure and array accesses to be combined into single instructions. Instructions are also typically highly encoded in order to further enhance the code density. The compact nature of such instruction sets results in smaller program sizes and fewer (slow) main memory accesses, which at the time (early 1960s and onwards) resulted in a tremendous saving on the cost of computer memory and disc storage, as well as faster execution. It also meant good programming productivity even in assembly language, as high level languages such as Fortran or Algol were not always available or appropriate. Indeed, microprocessors in this category are sometimes still programmed in assembly language for certain types of critical applications.

In the 1970s, analysis of high-level languages indicated some complex machine language implementations and it was determined that new instructions could improve performance. Some instructions were added that were never intended to be used in assembly language but fit well with compiled high-level languages. Compilers were updated to take advantage of these instructions. The benefits of semantically rich instructions with compact encodings can be seen in modern processors as well, particularly in the high-performance segment where caches are a central component (as opposed to most embedded systems). This is because these fast, but complex and expensive, memories are inherently limited in size, making compact code beneficial. Of course, the fundamental reason they are needed is that main memories (i.e., dynamic RAM today) remain slow compared to a (high-performance) CPU core.

While many designs achieved the aim of higher throughput at lower cost and also allowed high-level language constructs to be expressed by fewer instructions, it was observed that this was not "always" the case. For instance, low-end versions of complex architectures (i.e. using less hardware) could lead to situations where it was possible to improve performance by "not" using a complex instruction (such as a procedure call or enter instruction), but instead using a sequence of simpler instructions.

One reason for this was that architects (microcode writers) sometimes "over-designed" assembly language instructions, including features which could not be implemented efficiently on the basic hardware available. There could, for instance, be "side effects" (above conventional flags), such as the setting of a register or memory location that was perhaps seldom used; if this was done via ordinary (non duplicated) internal buses, or even the external bus, it would demand extra cycles every time, and thus be quite inefficient.

Even in balanced high-performance designs, highly encoded and (relatively) high-level instructions could be complicated to decode and execute efficiently within a limited transistor budget. Such architectures therefore required a great deal of work on the part of the processor designer in cases where a simpler, but (typically) slower, solution based on decode tables and/or microcode sequencing is not appropriate. At a time when transistors and other components were a limited resource, this also left fewer components and less opportunity for other types of performance optimizations.

The circuitry that performs the actions defined by the microcode in many (but not all) CISC processors is, in itself, a processor which in many ways is reminiscent in structure to very early CPU designs. In the early 1970s, this gave rise to ideas to return to simpler processor designs in order to make it more feasible to cope without ("then" relatively large and expensive) ROM tables and/or PLA structures for sequencing and/or decoding. The first (retroactively) RISC-"labeled" processor (IBM 801 IBM's Watson Research Center, mid-1970s) was a tightly pipelined simple machine originally intended to be used as an internal microcode kernel, or engine, in CISC designs, but also became the processor that introduced the RISC idea to a somewhat larger public. Simplicity and regularity also in the visible instruction set would make it easier to implement overlapping processor stages (pipelining) at the machine code level (i.e. the level seen by compilers). However, pipelining at that level was already used in some high performance CISC "supercomputers" in order to reduce the instruction cycle time (despite the complications of implementing within the limited component count and wiring complexity feasible at the time). Internal microcode execution in CISC processors, on the other hand, could be more or less pipelined depending on the particular design, and therefore more or less akin to the basic structure of RISC processors.

In a more modern context, the complex variable-length encoding used by some of the typical CISC architectures makes it complicated, but still feasible, to build a superscalar implementation of a CISC programming model "directly"; the in-order superscalar original Pentium and the out-of-order superscalar Cyrix 6x86 are well known examples of this. The frequent memory accesses for operands of a typical CISC machine may limit the instruction level parallelism that can be extracted from the code, although this is strongly mediated by the fast cache structures used in modern designs, as well as by other measures. Due to inherently compact and semantically rich instructions, the average amount of work performed per machine code unit (i.e. per byte or bit) is higher for a CISC than a RISC processor, which may give it a significant advantage in a modern cache based implementation.

Transistors for logic, PLAs, and microcode are no longer scarce resources; only large high-speed cache memories are limited by the maximum number of transistors today. Although complex, the transistor count of CISC decoders do not grow exponentially like the total number of transistors per processor (the majority typically used for caches). Together with better tools and enhanced technologies, this has led to new implementations of highly encoded and variable length designs without load-store limitations (i.e. non-RISC). This governs re-implementations of older architectures such as the ubiquitous x86 (see below) as well as new designs for microcontrollers for embedded systems, and similar uses. The superscalar complexity in the case of modern x86 was solved by converting instructions into one or more micro-operations and dynamically issuing those micro-operations, i.e. indirect and dynamic superscalar execution; the Pentium Pro and AMD K5 are early examples of this. It allows a fairly simple superscalar design to be located after the (fairly complex) decoders (and buffers), giving, so to speak, the best of both worlds in many respects. This technique is also used in IBM z196 and later z/Architecture microprocessors.

The terms CISC and RISC have become less meaningful with the continued evolution of both CISC and RISC designs and implementations. The first highly (or tightly) pipelined x86 implementations, the 486 designs from Intel, AMD, Cyrix, and IBM, supported every instruction that their predecessors did, but achieved "maximum efficiency" only on a fairly simple x86 subset that was only a little more than a typical RISC instruction set (i.e. without typical RISC "load-store" limitations). The Intel P5 Pentium generation was a superscalar version of these principles. However, modern x86 processors also (typically) decode and split instructions into dynamic sequences of internally buffered micro-operations, which not only helps execute a larger subset of instructions in a pipelined (overlapping) fashion, but also facilitates more advanced extraction of parallelism out of the code stream, for even higher performance.

Contrary to popular simplifications (present also in some academic texts), not all CISCs are microcoded or have "complex" instructions. As CISC became a catch-all term meaning anything that's not a load-store (RISC) architecture, it's not the number of instructions, nor the complexity of the implementation or of the instructions themselves, that define CISC, but the fact that arithmetic instructions also perform memory accesses. Compared to a small 8-bit CISC processor, a RISC floating-point instruction is complex. CISC does not even need to have complex addressing modes; 32 or 64-bit RISC processors may well have more complex addressing modes than small 8-bit CISC processors.

A PDP-10, a PDP-8, an Intel 80386, an Intel 4004, a Motorola 68000, a System z mainframe, a Burroughs B5000, a VAX, a Zilog Z80000, and a MOS Technology 6502 all vary wildly in the number, sizes, and formats of instructions, the number, types, and sizes of registers, and the available data types. Some have hardware support for operations like scanning for a substring, arbitrary-precision BCD arithmetic, or transcendental functions, while others have only 8-bit addition and subtraction. But they are all in the CISC category because they have "load-operate" instructions that load and/or store memory contents within the same instructions that perform the actual calculations. For instance, the PDP-8, having only 8 fixed-length instructions and no microcode at all, is a CISC because of "how" the instructions work, PowerPC, which has over 230 instructions (more than some VAXes), and complex internals like register renaming and a reorder buffer, is a RISC, while Minimal CISC has 8 instructions, but is clearly a CISC because it combines memory access and computation in the same instructions.





</doc>
<doc id="7624" url="https://en.wikipedia.org/wiki?curid=7624" title="CISC">
CISC

CISC may refer to:



</doc>
<doc id="7626" url="https://en.wikipedia.org/wiki?curid=7626" title="Cetacea">
Cetacea

Cetacea () are a widely distributed and diverse clade of aquatic mammals that today consists of whales, dolphins, and porpoises. Cetaceans are carnivorous and finned. Most species live in the sea, some in rivers. The name is derived from the Latin "cetus" "whale", itself from the Greek κῆτος "kētos" "huge fish".

There are around 89 extant species, which are divided into two groups or parvorders, the Odontoceti or toothed whales, a group of more than 70 species that includes the dolphins, porpoises, belugas, narwhals, sperm and beaked whales, and the Mysticeti or baleen whales, of which there are now 15 species. The extinct ancestors of modern whales are the Archaeoceti.

While cetaceans were historically thought to have descended from mesonychids, molecular evidence supports them as a relative of Artiodactyls (even-toed ungulates). Cetaceans belong to the order Cetartiodactyla (formed by combining Cetacea + Artiodactyla) and their closest living relatives are hippopotamuses and other hoofed mammals (camels, pigs, and ruminants), having diverged about 50 million years ago.

Cetaceans range in size from the and Maui's dolphin to the and blue whale, which is also the largest animal ever known to have existed. Several species exhibit sexual dimorphism. They have streamlined bodies and two (external) limbs that are modified into flippers. Though not as flexible or agile as seals, cetaceans can swim very quickly, with the killer whale able to travel at in short bursts and the fin whale able to cruise at . Dolphins are able to make very tight turns while swimming at high speeds. The hindlimbs of cetaceans are internal, and are thought to be vestigial. Baleen whales have short hairs on their mouth, unlike the toothed whales. Cetaceans have well-developed senses—their eyesight and hearing are adapted for both air and water, and baleen whales have a tactile system in their vibrissae. They have a layer of fat, or blubber, under the skin to maintain body heat in cold water. Some species are well adapted for diving to great depths.

Although cetaceans are widespread, most species prefer the colder waters of the Northern and Southern Hemispheres. They spend their lives in the water, having to mate, give birth, molt or escape from predators, like killer whales, underwater. This has drastically affected their anatomy to be able to do so. They feed largely on fish and marine invertebrates; but a few, like the killer whale, feed on large mammals and birds, such as penguins and seals. Some baleen whales (mainly gray whales and right whales) are specialised for feeding on benthic creatures. Male cetaceans typically mate with more than one female (polygyny), although the degree of polygyny varies with the species. Cetaceans are not known to have pair bonds. Male cetacean strategies for reproductive success vary between herding females, defending potential mates from other males, or whale song which attracts mates. Calves are typically born in the fall and winter months, and females bear almost all the responsibility for raising them. Mothers of some species fast and nurse their young for a relatively short period of time, which is more typical of baleen whales as their main food source (invertebrates) aren't found in their breeding and calving grounds (tropics). Cetaceans produce a number of vocalizations, notably the clicks and whistles of dolphins and the moaning songs of the humpback whale.

The meat, blubber and oil of cetaceans have traditionally been used by indigenous peoples of the Arctic. Cetaceans have been depicted in various cultures worldwide. Dolphins are commonly kept in captivity and are even sometimes trained to perform tricks and tasks, other cetaceans aren't as often kept in captivity (with usually unsuccessful attempts). Cetaceans have been relentlessly hunted by commercial industries for their products, although this is now forbidden by international law. The baiji (Chinese river dolphin) has become "Possibly Extinct" in the past century, while the vaquita and Yangtze finless porpoise are ranked Critically Endangered by the International Union for Conservation of Nature. Besides hunting, cetaceans also face threats from accidental trapping, marine pollution, and ongoing climate change.

The two parvorders, baleen whales (Mysticeti) and toothed whales (Odontoceti), are thought to have diverged around thirty-four million years ago.

Baleen whales have bristles made of keratin instead of teeth. The bristles filter krill and other small invertebrates from seawater. Grey whales feed on bottom-dwelling mollusks. Rorqual family (balaenopterids) use throat pleats to expand their mouths to take in food and sieve out the water. Balaenids (right whales and bowhead whales) have massive heads that can make up 40% of their body mass. Most mysticetes prefer the food-rich colder waters of the Northern and Southern Hemispheres, migrating to the Equator to give birth. During this process, they are capable of fasting for several months, relying on their fat reserves.

The parvorder of Odontocetes – the toothed whales – include sperm whales, beaked whales, killer whales, dolphins and porpoises. Generally the teeth are designed for catching fish, squid or other marine invertebrates, not for chewing them, so prey is swallowed whole. Teeth are shaped like cones (dolphins and sperm whales), spades (porpoises), pegs (belugas), tusks (narwhals) or variable (beaked whale males). Female beaked whales' teeth are hidden in the gums and are not visible, and most male beaked whales have only two short tusks. Narwhals have vestigial teeth other than their tusk, which is present on males and 15% of females and has millions of nerves to sense water temperature, pressure and salinity. A few toothed whales, such as some killer whales, feed on mammals, such as pinnipeds and other whales. 

Toothed whales have well-developed senses – their eyesight and hearing are adapted for both air and water, and they have advanced sonar capabilities using their melon. Their hearing is so well-adapted for both air and water that some blind specimens can survive. Some species, such as sperm whales, are well adapted for diving to great depths. Several species of toothed whales show sexual dimorphism, in which the males differ from the females, usually for purposes of sexual display or aggression. 

Cetacean bodies are generally similar to that of fish, which can be attributed to their lifestyle and the habitat conditions. Their body is well-adapted to their habitat, although they share essential characteristics with other higher mammals (Eutheria).

They have a streamlined shape, and their forelimbs are flippers. Almost all have a dorsal fin on their backs that can take on many forms depending on the species. A few species, such as the beluga whale, lack them. Both the flipper and the fin are for stabilization and steering in the water.

The male genitals and mammary glands of females are sunken into the body.

The body is wrapped in a thick layer of fat, known as blubber, used for thermal insulation and gives cetaceans their smooth, streamlined body shape. In larger species, it can reach a thickness up to half a meter (1.6 ft).

Sexual dimorphism evolved in many toothed whales. Sperm whales, narwhals, many members of the beaked whale family, several species of the porpoise family, killer whales, pilot whales, eastern spinner dolphins and northern right whale dolphins show this characteristic. Males in these species developed external features absent in females that are advantageous in combat or display. For example, male sperm whales are up to 63% percent larger than females, and many beaked whales possess tusks used in competition among males.

They have a cartilaginous fluke at the end of their tails that is used for propulsion. The fluke is set horizontally on the body, unlike fish, which have vertical tails.

Hind legs are not present in cetaceans, nor are any other external body attachments such as a pinna and hair.

Whales have an elongated head, especially baleen whales, due to the wide overhanging jaw. Bowhead whale plates can be long. Their nostril(s) make up the blowhole, with one in toothed whales and two in baleen whales.

The nostrils are located on top of the head above the eyes so that the rest of the body can remain submerged while surfacing for air. The back of the skull is significantly shortened and deformed. By shifting the nostrils to the top of the head, the nasal passages extend perpendicularly through the skull. The teeth or baleen in the upper jaw sit exclusively on the maxilla. The braincase is concentrated through the nasal passage to the front and is correspondingly higher, with individual cranial bones that overlap.

In toothed whales, connective tissue exists in the melon as a head buckle. This is filled with air sacs and fat that aid in buoyancy and biosonar. The sperm whale has a particularly pronounced melon; this is called the spermaceti organ and contains the eponymous spermaceti, hence the name "sperm whale". Even the long tusk of the narwhal is a vice-formed tooth. In many toothed whales, the depression in their skull is due to the formation of a large melon and multiple, asymmetric air bags.

River dolphins, unlike most other cetaceans, can turn their head 90°. Other cetaceans have fused neck vertebrae and are unable to turn their head at all.

The baleen of baleen whales consists of long, fibrous strands of keratin. Located in place of the teeth, it has the appearance of a huge fringe and is used to sieve the water for plankton and krill.

The neocortex of many cetaceans is home to elongated spindle neurons that, prior to 2007, were known only in hominids. In humans, these cells are thought to be involved in social conduct, emotions, judgment and theory of mind. Cetacean spindle neurons are found in areas of the brain homologous to where they are found in humans, suggesting they perform a similar function.

Brain size was previously considered a major indicator of intelligence. Since most of the brain is used for maintaining bodily functions, greater ratios of brain to body mass may increase the amount of brain mass available for cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately two-thirds or three-quarter exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such an analysis provides an encephalization quotient that can be used as an indication of animal intelligence. Sperm whales have the largest brain mass of any animal on earth, averaging and in mature males. The brain to body mass ratio in some odontocetes, such as belugas and narwhals, is second only to humans. In some whales, however, it is less than half that of humans: 0.9% versus 2.1%. The sperm whale ("Physeter macrocephalus") is the largest of all toothed predatory animals and possesses the largest brain.

The cetacean skeleton is largely made up of cortical bone, which stabilizes the animal in the water. For this reason, the usual terrestrial compact bones, which are finely woven cancellous bone, are replaced with lighter and more elastic material. In many places, bone elements are replaced by cartilage and even fat, thereby improving their hydrostatic qualities. The ear and the muzzle contain a bone shape that is exclusive to cetaceans with a high density, resembling porcelain. This conducts sound better than other bones, thus aiding biosonar.

The number of vertebrae that make up the spine varies by species, ranging from forty to ninety-three. The cervical spine, found in all mammals, consists of seven vertebrae which, however, are reduced or fused. This gives stability during swimming at the expense of mobility. The fins are carried by the thoracic vertebrae, ranging from nine to seventeen individual vertebrae. The sternum is cartilaginous. The last two to three pairs of ribs are not connected and hang freely in the body wall. The stable lumbar and tail include the other vertebrae. Below the caudal vertebrae is the chevron bone.

The front limbs are paddle-shaped with shortened arms and elongated finger bones, to support movement. They are connected by cartilage. The second and third fingers display a proliferation of the finger members, a so-called hyperphalangy. The shoulder joint is the only functional joint in all cetaceans except for the Amazon river dolphin. The collarbone is completely absent.

Cetaceans have powerful hearts. Blood oxygen is distributed effectively throughout the body. They are warm-blooded, i.e., they hold a nearly constant body temperature.

Cetaceans have lungs, meaning they breathe air. An individual can last without a breath from a few minutes to over two hours depending on the species. Cetacea are deliberate breathers who must be awake to inhale and exhale. When stale air, warmed from the lungs, is exhaled, it condenses as it meets colder external air. As with a terrestrial mammal breathing out on a cold day, a small cloud of 'steam' appears. This is called the 'spout' and varies across species in shape, angle and height. Species can be identified at a distance using this characteristic.

The structure of the respiratory and circulatory systems is of particular importance for the life of marine mammals. The oxygen balance is effective. Each breath can replace up to 90% of the total lung volume. For land mammals, in comparison, this value is usually about 15%. During inhalation, about twice as much oxygen is absorbed by the lung tissue as in a land mammal. As with all mammals, the oxygen is stored in the blood and the lungs, but in cetaceans, it is also stored in various tissues, mainly in the muscles. The muscle pigment, myoglobin, provides an effective bond. This additional oxygen storage is vital for deep diving, since beyond a depth around , the lung tissue is almost completely compressed by the water pressure.

The stomach consists of three chambers. The first region is formed by a loose gland and a muscular forestomach (missing in beaked whales), which is then followed by the main stomach and the pylorus. Both are equipped with glands to help digestion. A bowel adjoins the stomachs, whose individual sections can only be distinguished histologically. The liver is large and separate from the gall bladder.

The kidneys are long and flattened. The salt concentration in cetacean blood is lower than that in seawater, requiring kidneys to excrete salt. This allows the animals to drink seawater.

Cetacean eyes are set on the sides rather than the front of the head. This means only species with pointed 'beaks' (such as dolphins) have good binocular vision forward and downward. Tear glands secrete greasy tears, which protect the eyes from the salt in the water. The lens is almost spherical, which is most efficient at focusing the minimal light that reaches deep water. Cetaceans make up for their generally poor vision (except dolphins) with excellent hearing.

At least one species, the tucuxi or Guiana dolphin, is able to use electroreception to sense prey.

The external ear has lost the pinna (visible ear), but still retains a narrow external auditory meatus. To register sounds, instead, the posterior part of the mandible has a thin lateral wall (the pan bone) fronting a concavity that houses a fat pad. The pad passes anteriorly into the greatly enlarged mandibular foramen to reach in under the teeth and posteriorly to reach the thin lateral wall of the ectotympanic. The ectotympanic offers a reduced attachment area for the tympanic membrane. The connection between this auditory complex and the rest of the skull is reduced—to a single, small cartilage in oceanic dolphins.

In odontocetes, the complex is surrounded by spongy tissue filled with air spaces, while in mysticetes, it is integrated into the skull as with land mammals. In odontocetes, the tympanic membrane (or ligament) has the shape of a folded-in umbrella that stretches from the ectotympanic ring and narrows off to the malleus (quite unlike the flat, circular membrane found in land mammals.) In mysticetes, it also forms a large protrusion (known as the "glove finger"), which stretches into the external meatus and the stapes are larger than in odontocetes. In some small sperm whales, the malleus is fused with the ectotympanic.

The ear ossicles are pachyosteosclerotic (dense and compact) and differently shaped from land mammals (other aquatic mammals, such as sirenians and earless seals, have also lost their pinnae). T semicircular canals are much smaller relative to body size than in other mammals.

The auditory bulla is separated from the skull and composed of two compact and dense bones (the periotic and tympanic) referred to as the tympanoperiotic complex. This complex is located in a cavity in the middle ear, which, in the Mysticeti, is divided by a bony projection and compressed between the exoccipital and squamosal, but in the odontoceti, is large and completely surrounds the bulla (hence called "peribullar"), which is, therefore, not connected to the skull except in physeterids. In the Odontoceti, the cavity is filled with a dense foam in which the bulla hangs suspended in five or more sets of ligaments. The pterygoid and peribullar sinuses that form the cavity tend to be more developed in shallow water and riverine species than in pelagic Mysticeti. In Odontoceti, the composite auditory structure is thought to serve as an acoustic isolator, analogous to the lamellar construction found in the temporal bone in bats.

Cetaceans use sound to communicate, using groans, moans, whistles, clicks or the 'singing' of the humpback whale.

Odontoceti are generally capable of echolocation. They can discern the size, shape, surface characteristics, distance and movement of an object. They can search for, chase and catch fast-swimming prey in total darkness. Most Odontoceti can distinguish between prey and nonprey (such as humans or boats); captive Odontoceti can be trained to distinguish between, for example, balls of different sizes or shapes.

Mysticeti have exceptionally thin, wide basilar membranes in their cochleae without stiffening agents, making their ears adapted for processing low to infrasonic frequencies. Echolocation clicks also contain characteristic details unique to each animal, which may suggest that toothed whales can discern between their own click and that of others.

The initial karyotype includes a set of chromosomes from 2n = 44. They have four pairs of telocentric chromosomes (whose centromeres sit at one of the telomeres), two to four pairs of subtelocentric and one or two large pairs of submetacentric chromosomes. The remaining chromosomes are metacentric—the centromere is approximately in the middle—and are rather small. Sperm whales, beaked whales and right whales converge to a reduction in the number of chromosomes to 2n = 42.

Cetaceans are found in all oceans. River dolphin species live exclusively in fresh water. While many marine species, such as the blue whale, the humpback whale and the killer whale, have a distribution area that includes nearly the entire ocean, some species occur only locally or in broken populations. These include the vacquita, which inhabits a small part of the Gulf of California and Hector's dolphin, which lives in some coastal waters in New Zealand. Both species prefer deeper marine areas and species that live frequently or exclusively in coastal and shallow water areas.

Many species inhabit specific latitudes, often in tropical or subtropical waters, such as Bryde's whale or Risso's dolphin. Others are found only in a specific body of water. The southern right whale dolphin and the hourglass dolphin live only in the Southern Ocean. The narwhal and the beluga live only in the Arctic Ocean. Sowerby's beaked whale and the Clymene dolphin exist only in the Atlantic and the Pacific white-sided dolphin and the northern straight dolphin live only in the North Pacific.

Cosmopolitan species may be found in the Pacific, Atlantic and Indian Oceans. However, northern and southern populations become genetically separated over time. In some species, this separation leads eventually to a divergence of the species, such as produced the southern right whale, North Pacific right whale and North Atlantic right whale. Migratory species' reproductive sites often lie in the tropics and their feeding grounds in polar regions.

Thirty-two species are found in European waters, including twenty-five toothed and seven baleen species.

Conscious breathing cetaceans sleep but cannot afford to be unconscious for long, because they may drown. While knowledge of sleep in wild cetaceans is limited, toothed cetaceans in captivity have been recorded to exhibit unihemispheric slow-wave sleep (USWS), which means they sleep with one side of their brain at a time, so that they may swim, breathe consciously and avoid both predators and social contact during their period of rest.

A 2008 study found that sperm whales sleep in vertical postures just under the surface in passive shallow 'drift-dives', generally during the day, during which whales do not respond to passing vessels unless they are in contact, leading to the suggestion that whales possibly sleep during such dives.

While diving, the animals reduce their oxygen consumption by lowering the heart activity and blood circulation; individual organs receive no oxygen during this time. Some rorquals can dive for up to 40 minutes, sperm whales between 60 and 90 minutes and bottlenose whales for two hours. Diving depths average about . Species such as sperm whales can dive to , although more commonly .

Most whales are social animals, although a few species live in pairs or are solitary. A group, known as a pod, usually consists of ten to fifty animals, but on occasion, such as mass availability of food or during mating season, groups may encompass more than one thousand individuals. Inter-species socialization can occur.

Pods have a fixed hierarchy, with the priority positions determined by biting, pushing or ramming. The behavior in the group is aggressive only in situations of stress such as lack of food, but usually it is peaceful. Contact swimming, mutual fondling and nudging are common. The playful behavior of the animals, which is manifested in air jumps, somersaults, surfing, or fin hitting, occurs more often than not in smaller cetaceans, such as dolphins and porpoises.

Males in some baleen species communicate via whale song, sequences of high pitched sounds. These "songs" can be heard for hundreds of kilometers. Each population generally shares a distinct song, which evolves over time. Sometimes, an individual can be identified by its distinctive vocals, such as the 52-hertz whale that sings at a higher frequency than other whales. Some individuals are capable of generating over 600 distinct sounds. In baleen species such as humpbacks, blues and fins, male-specific song is believed to be used to attract and display fitness to females.

Pod groups also hunt, often with other species. Many species of dolphins accompany large tunas on hunting expeditions, following large schools of fish. The killer whale hunts in pods and targets belugas and even larger whales. Humpback whales, among others, form in collaboration bubble carpets to herd krill or plankton into bait balls before lunging at them.

Cetacea are known to teach, learn, cooperate, scheme and grieve.

Smaller cetaceans, such as dolphins and porpoises, engage in complex play behavior, including such things as producing stable underwater toroidal air-core vortex rings or "bubble rings". The two main methods of bubble ring production are rapid puffing of air into the water and allowing it to rise to the surface, forming a ring, or swimming repeatedly in a circle and then stopping to inject air into the helical vortex currents thus formed. They also appear to enjoy biting the vortex rings, so that they burst into many separate bubbles and then rise quickly to the surface. Whales produce bubble nets to aid in herding prey.

Larger whales are also thought to engage in play. The southern right whale elevates its tail fluke above the water, remaining in the same position for a considerable time. This is known as "sailing". It appears to be a form of play and is most commonly seen off the coast of Argentina and South Africa. Humpback whales also display this behaviour.

Self-awareness appears to be a sign of abstract thinking. Self-awareness, although not well-defined, is believed to be a precursor to more advanced processes such as metacognitive reasoning (thinking about thinking) that humans exploit. Cetaceans appear to possess self-awareness. The most widely used test for self-awareness in animals is the mirror test, in which a temporary dye is placed on an animal's body and the animal is then presented with a mirror. Researchers then explore whether the animal shows signs of self-recognition.

Critics claim that the results of these tests are susceptible to the Clever Hans effect. This test is much less definitive than when used for primates. Primates can touch the mark or the mirror, while cetaceans cannot, making their alleged self-recognition behavior less certain. Skeptics argue that behaviors said to identify self-awareness resemble existing social behaviors, so researchers could be misinterpreting self-awareness for social responses. Advocates counter that the behaviors are different from normal responses to another individual. Cetaceans show less definitive behavior of self-awareness, because they have no pointing ability.

In 1995, Marten and Psarakos used video to test dolphin self-awareness. They showed dolphins real-time footage of themselves, recorded footage and another dolphin. They concluded that their evidence suggested self-awareness rather than social behavior. While this particular study has not been replicated, dolphins later "passed" the mirror test.

Most cetaceans sexually mature at seven to 10 years. An exception to this is the La Plata dolphin, which is sexually mature at two years, but lives only to about 20. The sperm whale reaches sexual maturity within about 20 years and a lifespan between 50 and 100 years.

For most species, reproduction is seasonal. Ovulation coincides with male fertility. This cycle is usually coupled with seasonal movements that can be observed in many species. Most toothed whales have no fixed bonds. In many species, females choose several partners during a season. Baleen whales are largely monogamous within each reproductive period.

Gestation ranges from 9 to 16 months. Duration is not necessarily a function of size. Porpoises and blue whales gestate for about 11 months. During gestation, the embryo is fed by a special nutritive tissue, the placenta.

Cetaceans usually bear one calf. In the case of twins, one usually dies, because the mother cannot produce sufficient milk for both. The fetus is positioned for a tail-first delivery, so that the risk of drowning during delivery is minimal. After birth, the mother carries the infant to the surface for its first breath. At birth they are about one-third of their adult length and tend to be independently active, comparable to terrestrial mammals.

Like other placental mammals, cetaceans give birth to well-developed calves and nurse them with milk from their mammary glands. When suckling, the mother actively splashes milk into the mouth of the calf, using the muscles of her mammary glands, as the calf has no lips. This milk usually has a high fat content, ranging from 16 to 46%, causing the calf to increase rapidly in size and weight.

In many small cetaceans, suckling lasts for about four months. In large species, it lasts for over a year and involves a strong bond between mother and offspring.

The mother is solely responsible for brooding. In some species, so-called "aunts" occasionally suckle the young.

This reproductive strategy provides a few offspring that have a high survival rate.

Among cetaceans, whales are distinguished by an unusual longevity compared to other higher mammals. Some species, such as the bowhead whale ("Balaena mysticetus"), can reach over 200 years. Based on the annual rings of the bony otic capsule, the age of the oldest known specimen is a male determined to be 211 years at the time of death.

Upon death, whale carcasses fall to the deep ocean and provide a substantial habitat for marine life. Evidence of whale falls in present-day and fossil records shows that deep-sea whale falls support a rich assemblage of creatures, with a global diversity of 407 species, comparable to other neritic biodiversity hotspots, such as cold seeps and hydrothermal vents.

Deterioration of whale carcasses happens through three stages. Initially, organisms such as sharks and hagfish scavenge the soft tissues at a rapid rate over a period of months and as long as two years. This is followed by the colonization of bones and surrounding sediments (which contain organic matter) by enrichment opportunists, such as crustaceans and polychaetes, throughout a period of years. Finally, sulfophilic bacteria reduce the bones releasing hydrogen sulfide enabling the growth of chemoautotrophic organisms, which in turn, support organisms such as mussels, clams, limpets and sea snails. This stage may last for decades and supports a rich assemblage of species, averaging 185 per site.

Brucellosis affects almost all mammals. It is distributed worldwide, while fishing and pollution have caused porpoise population density pockets, which risks further infection and disease spreading. "Brucella ceti", most prevalent in dolphins, has been shown to cause chronic disease, increasing the chance of failed birth and miscarriages, male infertility, neurobrucellosis, cardiopathies, bone and skin lesions, strandings and death. Until 2008, no case had ever been reported in porpoises, but isolated populations have an increased risk and consequentially a high mortality rate.

Molecular biology and immunology show that cetaceans are phylogenetically closely related with the even-toed ungulates (Artiodactyla). Whales direct lineage began in the early Eocene, more than 50 million years ago, with early artiodactyls. Fossil discoveries at the beginning of the 21st century confirmed this.

Most molecular biological evidence suggests that hippos are the closest living relatives. Common anatomical features include similarities in the morphology of the posterior molars, and the bony ring on the temporal bone (bulla) and the involucre, a skull feature that was previously associated only with cetaceans. The fossil record, however, does not support this relationship, because the hippo lineage dates back only about 15 million years. The most striking common feature is the talus, a bone in the upper ankle. Early cetaceans, archaeocetes, show double castors, which only occur in even-toed ungulates. Corresponding findings are from Tethys Sea deposits in northern India and Pakistan. The Tethys Sea was a shallow sea between the Asian continent and northward-bound Indian plate.

Mysticetes evolved baleen around 25 million years ago and lost their teeth.

The direct ancestors of today's cetaceans are probably found within the Dorudontidae whose most famous member, "Dorudon atrox", lived at the same time as "Basilosaurus". Both groups had already developed the typical anatomical features of today's whales, such as hearing. Life in the water for a formerly terrestrial creature required significant adjustments such as the fixed bulla, which replaces the mammalian eardrum, as well as sound-conducting elements for submerged directional hearing. Their wrists were stiffened and probably contributed to the typical build of flippers. The hind legs existed, however, but were significantly reduced in size and with a vestigial pelvis connection.

The fossil record traces the gradual transition from terrestrial to aquatic life. The regression of the hind limbs allowed greater flexibility of the spine. This made it possible for whales to move around with the vertical tail hitting the water. The front legs transformed into flippers, costing them their mobility on land.

One of the oldest members of ancient cetaceans (Archaeoceti) is "Pakicetus" from the Middle Eocene. This is an animal the size of a wolf, whose skeleton is known only partially. It had functioning legs and lived near the shore. This suggests the animal could still move on land. The long snout had carnivorous dentition.

The transition from land to sea dates to about 49 million years ago, with the "Ambulocetus" ("running whale"), discovered in Pakistan. It was up to long. The limbs of this archaeocete were adapted to swimming, but terrestrial locomotion was still possible. It probably crawled like a seal or crocodile. The snout was elongated with overhead nostrils and eyes. The tail was strong and supported movement through water. "Ambulocetus" probably lived in mangroves in brackish water and fed in the riparian zone as a predator of fish and other vertebrates.

Dating from about 45 million years ago are species such as "Indocetus", "Kutchicetus", "Rodhocetus" and "Andrewsiphius", all of which were adapted to life in water. The hind limbs of these species were regressed and their body shapes resemble modern whales. Protocetidae family member "Rodhocetus" is considered the first to be fully aquatic. The body was streamlined and delicate with extended hand and foot bones. The merged pelvic lumbar spine was present, making it possible to support the floating movement of the tail. It was likely a good swimmer, but could probably move only clumsily on land, much like a modern seal.

Since the late Eocene, about 40 million years ago, cetaceans populated the subtropical oceans and no longer emerged on land. An example is the 18-m-long "Basilosaurus", sometimes referred to as "Zeuglodon". The transition from land to water was completed in about 10 million years. The Wadi Al-Hitan ("Whale Valley") in Egypt contains numerous skeletons of "Basilosaurus", as well as other marine vertebrates.

The two parvorders are Baleen whales (Mysticeti) which owe their name to their baleen, and toothed whales (Odontoceti), which have teeth shaped like cones, spades, pegs or tusks, and can perceive their environment through biosonar.

The terms whale and dolphin are informal:
The term Great Whales covers those currently regulated by the International Whaling Commission:
the Odontoceti family Physeteridae (sperm whales); and the Mysticeti families Balaenidae (right and bowhead whales), Eschrichtiidae (grey whales), and some of the Balaenopteridae (Minke, Bryde's, Sei, Blue and Fin; not Eden's and Omura's whales).

†Recently extinct

The primary threats to cetaceans come from people, both directly from whaling or drive hunting and indirect threats from fishing and pollution.

Whaling is the practice of hunting whales, mainly baleen and sperm whales. This activity has gone on since the Stone Age.

In the Middle Ages, reasons for whaling included their meat, oil usable as fuel and the jawbone, which was used in house construction. At the end of the Middle Ages, early whaling fleets aimed at baleen whales, such as bowheads. In the 16th and 17th centuries, the Dutch fleet had about 300 whaling ships with 18,000 crewmen.

In the 18th and 19th centuries, baleen whales especially were hunted for their baleen, which was used as a replacement for wood, or in products requiring strength and flexibility such as corsets and crinoline skirts. In addition, the spermaceti found in the sperm whale was used as a machine lubricant and the ambergris as a material for pharmaceutical and perfume industries. In the second half of the 19th century, the explosive harpoon was invented, leading to a massive increase in the catch size.

Large ships were used as "mother" ships for the whale handlers. In the first half of the 20th century, whales were of great importance as a supplier of raw materials. Whales were intensively hunted during this time; in the 1930s, 30,000 whales were killed. This increased to over 40,000 animals per year up to the 1960s, when stocks of large baleen whales collapsed.

Most hunted whales are now threatened, with some great whale populations exploited to the brink of extinction. Atlantic and Korean gray whale populations were completely eradicated and the North Atlantic right whale population fell to some 300-600. The blue whale population is estimated to be around 14,000.

The first efforts to protect whales came in 1931. Some particularly endangered species, such as the humpback whale (which then numbered about 100 animals), were placed under international protection and the first protected areas were established. In 1946, the International Whaling Commission (IWC) was established, to monitor and secure whale stocks. Whaling for commercial purposes was prohibited worldwide by this organization from 1985 to 2005.

The stocks of species such as humpback and blue whales have recovered, though they are still threatened. The United States Congress passed the Marine Mammal Protection Act of 1972 sustain the marine mammal population. It prohibits the taking of marine mammals except for several hundred per year taken in Alaska. Japanese whaling ships are allowed to hunt whales of different species for ostensibly scientific purposes. 

Aboriginal whaling is still permitted. About 1,200 pilot whales were taken in the Faroe Islands in 2017., and about 900 narwhals and 800 belugas per year are taken in Alaska, Canada, Greenland, and Siberia. About 150 minke are taken in Greenland per year, 120 gray whales in Siberia and 50 bowheads in Alaska, as aboriginal whaling, besides the 600 minke taken commercially by Norway, 300 minke and 100 sei taken by Japan and up to 100 fin whales taken by Iceland. Iceland and Norway do not recognize the ban and operate commercial whaling. Norway and Japan are committed to ending the ban.

Dolphins and other smaller cetaceans are sometimes hunted in an activity known as dolphin drive hunting. This is accomplished by driving a pod together with boats, usually into a bay or onto a beach. Their escape is prevented by closing off the route to the ocean with other boats or nets. Dolphins are hunted this way in several places around the world, including the Solomon Islands, the Faroe Islands, Peru and Japan (the most well-known practitioner). Dolphins are mostly hunted for their meat, though some end up in dolphinaria. Despite the controversy thousands of dolphins are caught in drive hunts each year.

Dolphin pods often reside near large tuna shoals. This is known to fishermen, who look for dolphins to catch tuna. Dolphins are much easier to spot from a distance than tuna, since they regularly breathe. The fishermen pull their nets hundreds of meters wide in a circle around the dolphin groups, in the expectation that they will net a tuna shoal. When the nets are pulled together, the dolphins become entangled under water and drown. Line fisheries in larger rivers are threats to river dolphins.

A greater threat than by-catch for small cetaceans is targeted hunting. In Southeast Asia, they are sold as fish-replacement to locals, since the region's edible fish promise higher revenues from exports. In the Mediterranean, small cetaceans are targeted to ease pressure on edible fish.

A stranding is when a cetacean leaves the water to lie on a beach. In some cases, groups of whales strand together. The best known are mass strandings of pilot whales and sperm whales. Stranded cetaceans usually die, because their as much as body weight compresses their lungs or breaks their ribs. Smaller whales can die of heatstroke because of their thermal insulation.

The causes are not clear. Possible reasons for mass beachings are:

Since 2000, whale strandings frequently occurred following military sonar testing. In December 2001, the US Navy admitted partial responsibility for the beaching and the deaths of several marine mammals in March 2000. The coauthor of the interim report stated that animals killed by active sonar of some Navy ships were injured. Generally, underwater noise, which is still on the increase, is increasingly tied to strandings; because it impairs communication and sense of direction.

Climate change influences the major wind systems and ocean currents, which also lead to cetacean strandings. Researchers studying strandings on the Tasmanian coast from 1920–2002 found that greater strandings occurred at certain time intervals. Years with increased strandings were associated with severe storms, which initiated cold water flows close to the coast. In nutrient-rich, cold water, cetaceans expect large prey animals, so they follow the cold water currents into shallower waters, where the risk is higher for strandings. Whales and dolphins who live in pods may accompany sick or debilitated pod members into shallow water, stranding them at low tide. Once stranded, large whales are crushed by their own body weight, if they cannot quickly return to the water. In addition, body temperature regulation is compromised.

Heavy metals, residues of many plant and insect venoms and plastic waste flotsam are not biodegradable. Sometimes, cetaceans consume these hazardous materials, mistaking them for food items. As a result, the animals are more susceptible to disease and have fewer offspring.

Damage to the ozone layer reduces plankton reproduction because of its resulting radiation. This shrinks the food supply for many marine animals, but the filter-feeding baleen whales are most impacted. Even the Nekton is, in addition to intensive exploitation, damaged by the radiation.

Food supplies are also reduced long-term by ocean acidification due to increased absorption of increased atmospheric carbon dioxide. The CO reacts with water to form carbonic acid, which reduces the construction of the calcium carbonate skeletons of food supplies for zooplankton that baleen whales depend on.

The military and resource extraction industries operate strong sonar and blasting operations. Marine seismic surveys use loud, low-frequency sound that show what is lying underneath the Earth's surface. Vessel traffic also increases noise in the oceans. Such noise can disrupt cetacean behavior such as their use of biosonar for orientation and communication. Severe instances can panic them, driving them to the surface. This leads to bubbles in blood gases and can cause decompression sickness. Naval exercises with sonar regularly results in fallen cetaceans that wash up with fatal decompression. Sounds can be disruptive at distances of more than . Damage varies across frequency and species.

In Aristotle's time, the 4th century BCE, whales were regarded as fish due to their superficial similarity. Aristotle, however, observed many physiological and anatomical similarities with the terrestrial vertebrates, such as blood (circulation), lungs, uterus and fin anatomy. His detailed descriptions were assimilated by the Romans, but mixed with a more accurate knowledge of the dolphins, as mentioned by Pliny the Elder in his "Natural history". In the art of this and subsequent periods, dolphins are portrayed with a high-arched head (typical of porpoises) and a long snout. The harbour porpoise was one of the most accessible species for early cetologists; because it could be seen close to land, inhabiting shallow coastal areas of Europe. Much of the findings that apply to all cetaceans were first discovered in porpoises. One of the first anatomical descriptions of the airways of a harbor porpoise dates from 1671 by John Ray. It nevertheless referred to the porpoise as a fish.

In the 10th edition of Systema Naturae (1758), Swedish biologist and taxonomist Carl Linnaeus asserted that cetaceans were mammals and not fish. His groundbreaking binomial system formed the basis of modern whale classification.

Cetaceans play a role in human culture.

Stone Age petroglyphs, such as those in Roddoy and Reppa (Norway), depict them. Whale bones were used for many purposes. In the Neolithic settlement of Skara Brae on Orkney sauce pans were made from whale vertebrae.

The whale was first mentioned in ancient Greece by Homer. There, it is called Ketos, a term that initially included all large marine animals. From this was derived the Roman word for whale, Cetus. Other names were phálaina (Aristotle, Latin form of ballaena) for the female and, with an ironic characteristic style, musculus (Mouse) for the male. North Sea whales were called Physeter, which was meant for the sperm whale "Physter macrocephalus". Whales are described in particular by Aristotle, Pliny and Ambrose. All mention both live birth and suckling. Pliny describes the problems associated with the lungs with spray tubes and Ambrose claimed that large whales would take their young into their mouth to protect them.

In the Bible especially, the leviathan plays a role as a sea monster. The essence, which features a giant crocodile or a dragon and a whale, was created according to the Bible by God and should again be destroyed by him. In the Book of Job, the leviathan is described in more detail.

In Jonah there is a more recognizable description of a whale alongside the prophet Jonah, who, on his flight from the city of Nineveh is swallowed by a whale.

Dolphins are mentioned far more often than whales. Aristotle discusses the sacred animals of the Greeks in his "Historia Animalium" and gives details of their role as aquatic animals. The Greeks admired the dolphin as a "king of the aquatic animals" and referred to them erroneously as fish. Its intelligence was apparent both in its ability to escape from fishnets and in its collaboration with fishermen.

River dolphins are known from the Ganges and – erroneously – the Nile. In the latter case it was equated with sharks and catfish. Supposedly they attacked even crocodiles.

Dolphins appear in Greek mythology. Because of their intelligence, they rescued multiple people from drowning. They were said to love music – probably not least because of their own song – they saved, in the legends, famous musicians such as Arion of Lesbos from Methymna or Kairanos from Miletus. Because of their mental faculties, dolphins were considered for the god Dionysus.

Dolphins belong to the domain of Poseidon and led him to his wife Amphitrite. Dolphins are associated with other gods, such as Apollo, Dionysus and Aphrodite. The Greeks paid tribute to both whales and dolphins with their own constellation. The constellation of the Whale (Ketos, lat. Cetus) is located south of the Dolphin (Delphi, lat. Delphinus) north of the zodiac.

Ancient art often included dolphin representations, including the Cretan Minoans. Later they appeared on reliefs, gems, lamps, coins, mosaics and gravestones. A particularly popular representation is that of Arion or the Taras (mythology) riding on a dolphin. In early Christian art, the dolphin is a popular motif, at times used as a symbol of Christ.

St. Brendan described in his travel story "Navigatio Sancti Brendani" an encounter with a whale, between the years 565–573. He described how he and his companions entered a treeless island, which turned out to be a giant whale, which he called Jasconicus. He met this whale seven years later and rested on his back.

Most descriptions of large whales from this time until the whaling era, beginning in the 17th century, were of beached whales, which resembled no other animal. This was particularly true for the sperm whale, the most frequently stranded in larger groups. Raymond Gilmore documented seventeen sperm whales in the estuary of the Elbe from 1723 to 1959 and thirty-one animals on the coast of Great Britain in 1784. In 1827, a blue whale beached itself off the coast of Ostend. Whales were used as attractions in museums and traveling exhibitions.
Whalers in the 17–19th centuries depicted whales in drawings and recounted tales of their occupation. Although they knew that whales were harmless giants, they described battles with harpooned animals. These included descriptions of sea monsters, including huge whales, sharks, sea snakes, giant squid and octopuses.

Among the first whalers who described their experiences on whaling trips was Captain William Scoresby from Great Britain, who published the book "Northern Whale Fishery", describing the hunt for northern baleen whales. This was followed by Thomas Beale, a British surgeon, in his book "Some observations on the natural history of the sperm whale" in 1835; and Frederick Debell Bennett's "The tale of a whale hunt" in 1840. Whales were described in narrative literature and paintings, most famously in the novels "Moby Dick" by Herman Melville and "20,000 Leagues Under the Sea" by Jules Verne. In the 1882 children's book "Adventures of Pinocchio" by Carlo Collodi, the wooden figure Pinocchio and his creator Geppetto were swallowed by a whale.

Baleen was used to make vessel components such as the bottom of a bucket in the Scottish National Museum. The Norse crafted ornamented plates from baleen, sometimes interpreted as ironing boards.

In the Canadian Arctic (east coast) in Punuk and Thule culture (1000–1600 C.E.), I baleen was used to construct houses in place of wood as roof support for winter houses, with half of the building buried under the ground. The actual roof was probably made of animal skins that were covered with soil and moss.

In the 20th century perceptions of cetaceans changed. They transformed from monsters into objects of wonder. As science revealed them to be intelligent and peaceful animals. Hunting was replaced by whale and dolphin tourism. This change is reflected in films and novels. For example, the protagonist of the series Flipper was a bottle-nose dolphin. The TV series SeaQuest DSV (1993–1996), the movies Free Willy, and the book series The Hitchhiker's Guide to the Galaxy by Douglas Adams are examples.

The study of whale song also produced a popular album, Songs of the Humpback Whale.

Whales and dolphins have been kept in captivity for use in education, research and entertainment since the 19th century.

Beluga whales were the first whales to be kept in captivity. Other species were too rare, too shy or too big. The first was shown at Barnum's Museum in New York City in 1861. For most of the 20th century, Canada was the predominant source. They were taken from the St. Lawrence River estuary until the late 1960s, after which they were predominantly taken from the Churchill River estuary until capture was banned in 1992. Russia then became the largest provider. Belugas are caught in the Amur Darya delta and their eastern coast and are transported domestically to aquaria or dolphinaria in Moscow, St. Petersburg and Sochi, or exported to countries such as Canada. They have not been domesticated.

As of 2006, 30 belugas lived in Canada and 28 in the United States. 42 deaths in captivity had been reported. A single specimen can reportedly fetch up to US$100,000 (UK£64,160). The beluga's popularity is due to its unique color and its facial expressions. The latter is possible because while most cetacean "smiles" are fixed, the extra movement afforded by the beluga's unfused cervical vertebrae allows a greater range of apparent expression.

The killer whale's intelligence, trainability, striking appearance, playfulness in captivity and sheer size have made it a popular exhibit at aquaria and aquatic theme parks. From 1976 to 1997, fifty-five whales were taken from the wild in Iceland, nineteen from Japan and three from Argentina. These figures exclude animals that died during capture. Live captures fell dramatically in the 1990s and by 1999, about 40% of the forty-eight animals on display in the world were captive-born.

Organizations such as World Animal Protection and the Whale and Dolphin Conservation Society campaign against the practice of keeping them in captivity.

In captivity, they often develop pathologies, such as the dorsal fin collapse seen in 60–90% of captive males. Captives have reduced life expectancy, on average only living into their 20s, although some live longer, including several over 30 years old and two, Corky II and Lolita, in their mid-40s. In the wild, females who survive infancy live 46 years on average and up to 70–80 years. Wild males who survive infancy live 31 years on average and can reach 50–60 years.

Captivity usually bears little resemblance to wild habitat and captive whales' social groups are foreign to those found in the wild. Critics claim captive life is stressful due to these factors and the requirement to perform circus tricks that are not part of wild killer whale behavior. Wild killer whales may travel up to in a day and critics say the animals are too big and intelligent to be suitable for captivity. Captives occasionally act aggressively towards themselves, their tankmates, or humans, which critics say is a result of stress. Killer whales are well known for their performances in shows, but the number of orcas kept in captivity is small, especially when compared to the number of bottlenose dolphins, with only forty-four captive orcas being held in aquaria as of 2012.

Each country has its own tank requirements; in the US, the minimum enclosure size is set by the Code of Federal Regulations, 9 CFR E § 3.104, under the "Specifications for the Humane Handling, Care, Treatment and Transportation of Marine Mammals".
Aggression among captive killer whales is common. They attack each other and their trainers as well. In 2013, SeaWorld's treatment of killer whales in captivity was the basis of the movie "Blackfish", which documents the history of Tilikum, a killer whale at SeaWorld Orlando, who had been involved in the deaths of three people. The film was a sensation, leading the company to announce in 2016 that it would phase out its killer whale program after various unsuccessful attempts to restore its reputation and stock price.

Dolphins and porpoises are kept in captivity. Bottlenose dolphins are the most common, as they are relatively easy to train, have a long lifespan in captivity and have a friendly appearance. Bottlenose dolphins live in captivity across the world, though exact numbers are hard to determine. Other species kept in captivity are spotted dolphins, false killer whales and common dolphins, Commerson's dolphins, as well as rough-toothed dolphins, but all in much lower numbers. There are also fewer than ten pilot whales, Amazon river dolphins, Risso's dolphins, spinner dolphins, or tucuxi in captivity. Two unusual and rare hybrid dolphins, known as wolphins, are kept at Sea Life Park in Hawaii, which is a cross between a bottlenose dolphin and a false killer whale. Also, two common/bottlenose hybrids reside in captivity at Discovery Cove and SeaWorld San Diego.

In repeated attempts in the 1960s and 1970s, narwhals kept in captivity died within months. A breeding pair of pygmy right whales were retained in a netted area. They were eventually released in South Africa. In 1971, SeaWorld captured a California gray whale calf in Mexico at Scammon's Lagoon. The calf, later named Gigi, was separated from her mother using a form of lasso attached to her flukes. Gigi was displayed at SeaWorld San Diego for a year. She was then released with a radio beacon affixed to her back; however, contact was lost after three weeks. Gigi was the first captive baleen whale. JJ, another gray whale calf, was kept at SeaWorld San Diego. JJ was an orphaned calf that beached itself in April 1997 and was transported two miles to SeaWorld. The calf was a popular attraction and behaved normally, despite separation from his mother. A year later, the then whale though smaller than average, was too big to keep in captivity, and was released on April 1, 1998. A captive Amazon river dolphin housed at Acuario de Valencia is the only trained river dolphin in captivity.



</doc>
<doc id="7627" url="https://en.wikipedia.org/wiki?curid=7627" title="The Canterbury Tales">
The Canterbury Tales

The Canterbury Tales () is a collection of 24 stories that runs to over 17,000 lines written in Middle English by Geoffrey Chaucer between 1387 and 1400. In 1386, Chaucer became Controller of Customs and Justice of Peace and, in 1389, Clerk of the King's work. It was during these years that Chaucer began working on his most famous text, "The Canterbury Tales". The tales (mostly written in verse, although some are in prose) are presented as part of a story-telling contest by a group of pilgrims as they travel together on a journey from London to Canterbury to visit the shrine of Saint Thomas Becket at Canterbury Cathedral. The prize for this contest is a free meal at the Tabard Inn at Southwark on their return.

After a long list of works written earlier in his career, including "Troilus and Criseyde", "House of Fame", and "Parliament of Fowls", "The Canterbury Tales" is near-unanimously seen as Chaucer's "magnum opus". He uses the tales and descriptions of its characters to paint an ironic and critical portrait of English society at the time, and particularly of the Church. Chaucer's use of such a wide range of classes and types of people was without precedent in English. Although the characters are fictional, they still offer a variety of insights into customs and practices of the time. Often, such insight leads to a variety of discussions and disagreements among people in the 14th century. For example, although various social classes are represented in these stories and all of the pilgrims are on a spiritual quest, it is apparent that they are more concerned with worldly things than spiritual. Structurally, the collection resembles Giovanni Boccaccio's "The Decameron", which Chaucer may have read during his first diplomatic mission to Italy in 1372.

It has been suggested that the greatest contribution of "The Canterbury Tales" to English literature was the popularisation of the English vernacular in mainstream literature, as opposed to French, Italian or Latin. English had, however, been used as a literary language centuries before Chaucer's time, and several of Chaucer's contemporaries—John Gower, William Langland, the Pearl Poet, and Julian of Norwich—also wrote major literary works in English. It is unclear to what extent Chaucer was seminal in this evolution of literary preference.

While Chaucer clearly states the addressees of many of his poems, the intended audience of "The Canterbury Tales" is more difficult to determine. Chaucer was a courtier, leading some to believe that he was mainly a court poet who wrote exclusively for nobility.

"The Canterbury Tales" is generally thought to have been incomplete at the end of Chaucer's life. In the General Prologue, some 30 pilgrims are introduced. According to the Prologue, Chaucer's intention was to write four stories from the perspective of each pilgrim, two each on the way to and from their ultimate destination, St. Thomas Becket's shrine (making for a total of about 120 stories). Although perhaps incomplete, "The Canterbury Tales" is revered as one of the most important works in English literature. It is also open to a wide range of interpretations.

The question of whether "The Canterbury Tales" is a finished work has not been answered to date. There are 84 manuscripts and four incunabula (printed before c 1540) editions of the work, dating from the late medieval and early Renaissance periods, more than for any other vernacular literary text with the exception of "The Prick of Conscience". This is taken as evidence of the "Tales"' popularity during the century after Chaucer's death. Fifty-five of these manuscripts are thought to have been originally complete, while 28 are so fragmentary that it is difficult to ascertain whether they were copied individually or as part of a set. The "Tales" vary in both minor and major ways from manuscript to manuscript; many of the minor variations are due to copyists' errors, while it is suggested that in other cases Chaucer both added to his work and revised it as it was being copied and possibly as it was being distributed. Determining the text of the work is complicated by the question of the narrator's voice which Chaucer made part of his literary structure. 

Even the oldest surviving manuscripts of the "Tales" are not Chaucer's originals. The very oldest is probably MS Peniarth 392 D (called "Hengwrt"), written by a scribe shortly after Chaucer's death. The most beautiful, on the other hand, is the Ellesmere Manuscript, a manuscript handwritten by one person with illustrations by several illustrators; the tales are put in an order that many later editors have followed for centuries. The first version of "The Canterbury Tales" to be published in print was William Caxton's 1476 edition. Only 10 copies of this edition are known to exist, including one held by the British Library and one held by the Folger Shakespeare Library. 

In 2004, Linne Mooney claimed that she was able to identify the scrivener who worked for Chaucer as an Adam Pinkhurst. Mooney, then a professor at the University of Maine and a visiting fellow at Corpus Christi College, Cambridge, said she could match Pinkhurst's signature, on an oath he signed, to his handwriting on a copy of "The Canterbury Tales" that might have been transcribed from Chaucer's working copy.

In the absence of consensus as to whether or not a complete version of the "Tales" exists, there is also no general agreement regarding the order in which Chaucer intended the stories to be placed.

Textual and manuscript clues have been adduced to support the two most popular modern methods of ordering the tales. Some scholarly editions divide the "Tales" into ten "Fragments". The tales that make up a Fragment are closely related and contain internal indications of their order of presentation, usually with one character speaking to and then stepping aside for another character. However, between Fragments, the connection is less obvious. Consequently, there are several possible orders; the one most frequently seen in modern editions follows the numbering of the Fragments (ultimately based on the Ellesmere order). Victorians frequently used the nine "Groups", which was the order used by Walter William Skeat whose edition "Chaucer: Complete Works" was used by Oxford University Press for most of the twentieth century, but this order is now seldom followed.

An alternative ordering (seen in an early manuscript containing "The Canterbury Tales", the early-fifteenth century Harley MS. 7334) places Fragment VIII before VI. Fragments I and II almost always follow each other, just as VI and VII, IX and X do in the oldest manuscripts. Fragments IV and V, by contrast, vary in location from manuscript to manuscript.

Chaucer wrote in late Middle English, which has clear differences from Modern English. From philological research, we know certain facts about the pronunciation of English during the time of Chaucer. Chaucer pronounced "-e" at the end of words, so that "care" was , not as in Modern English. Other silent letters were also pronounced, so that the word "knight" was , with both the "k" and the "gh" pronounced, not . In some cases, vowel letters in Middle English were pronounced very differently from Modern English, because the Great Vowel Shift had not yet happened. For instance, the long "e" in "wepyng" "weeping" was pronounced as , as in modern German or Italian, not as . Below is an IPA transcription of the opening lines of "The Merchant's Prologue":

Although no manuscript exists in Chaucer's own hand, two were copied around the time of his death by Adam Pinkhurst, a scribe with whom he seems to have worked closely before, giving a high degree of confidence that Chaucer himself wrote the "Tales". Because the final "-e" sound was lost soon after Chaucer's time, scribes did not accurately copy it, and this gave scholars the impression that Chaucer himself was inconsistent in using it. It has now been established, however, that "-e" was an important part of Chaucer's grammar, and helped to distinguish singular adjectives from plural and subjunctive verbs from indicative.

No other work prior to Chaucer's is known to have set a collection of tales within the framework of pilgrims on a pilgrimage. It is obvious, however, that Chaucer borrowed portions, sometimes very large portions, of his stories from earlier stories, and that his work was influenced by the general state of the literary world in which he lived. Storytelling was the main entertainment in England at the time, and storytelling contests had been around for hundreds of years. In 14th-century England the English Pui was a group with an appointed leader who would judge the songs of the group. The winner received a crown and, as with the winner of "The Canterbury Tales", a free dinner. It was common for pilgrims on a pilgrimage to have a chosen "master of ceremonies" to guide them and organise the journey. Harold Bloom suggests that the structure is mostly original, but inspired by the "pilgrim" figures of Dante and Virgil in "The Divine Comedy". New research suggests that the General Prologue, in which the innkeeper and host Harry Bailey introduces each pilgrim, is a pastiche of the historical Harry Bailey's surviving 1381 poll-tax account of Southwark's inhabitants.

"The Decameron" by Giovanni Boccaccio contains more parallels to "The Canterbury Tales" than any other work. Like the "Tales", it features a number of narrators who tell stories along a journey they have undertaken (to flee from the Black Death). It ends with an apology by Boccaccio, much like Chaucer's Retraction to the "Tales". A quarter of the tales in "The Canterbury Tales" parallel a tale in the "Decameron", although most of them have closer parallels in other stories. Some scholars thus find it unlikely that Chaucer had a copy of the work on hand, surmising instead that he must have merely read the "Decameron" at some point, while a new study claims he had a copy of the "Decameron" and used it extensively as he began work on his own collection. Each of the tales has its own set of sources that have been suggested by scholars, but a few sources are used frequently over several tales. They include poetry by Ovid, the Bible in one of the many vulgate versions in which it was available at the time (the exact one is difficult to determine), and the works of Petrarch and Dante. Chaucer was the first author to use the work of these last two, both Italians. Boethius' "Consolation of Philosophy" appears in several tales, as the works of John Gower do. Gower was a known friend to Chaucer. A full list is impossible to outline in little space, but Chaucer also, lastly, seems to have borrowed from numerous religious encyclopaedias and liturgical writings, such as John Bromyard's "Summa praedicantium", a preacher's handbook, and Jerome's "Adversus Jovinianum". Many scholars say there is a good possibility Chaucer met Petrarch or Boccaccio.

"The Canterbury Tales" is a collection of stories built around a frame narrative or frame tale, a common and already long established genre of its period. Chaucer's "Tales" differs from most other story "collections" in this genre chiefly in its intense variation. Most story collections focused on a theme, usually a religious one. Even in the "Decameron", storytellers are encouraged to stick to the theme decided on for the day. The idea of a pilgrimage to get such a diverse collection of people together for literary purposes was also unprecedented, though "the association of pilgrims and storytelling was a familiar one". Introducing a competition among the tales encourages the reader to compare the tales in all their variety, and allows Chaucer to showcase the breadth of his skill in different genres and literary forms.

While the structure of the "Tales" is largely linear, with one story following another, it is also much more than that. In the "General Prologue", Chaucer describes not the tales to be told, but the people who will tell them, making it clear that structure will depend on the characters rather than a general theme or moral. This idea is reinforced when the Miller interrupts to tell his tale after the Knight has finished his. Having the Knight go first gives one the idea that all will tell their stories by class, with the Monk following the Knight. However, the Miller's interruption makes it clear that this structure will be abandoned in favour of a free and open exchange of stories among all classes present. General themes and points of view arise as the characters tell their tales, which are responded to by other characters in their own tales, sometimes after a long lapse in which the theme has not been addressed.

Lastly, Chaucer does not pay much attention to the progress of the trip, to the time passing as the pilgrims travel, or to specific locations along the way to Canterbury. His writing of the story seems focused primarily on the stories being told, and not on the pilgrimage itself.

The variety of Chaucer's tales shows the breadth of his skill and his familiarity with many literary forms, linguistic styles, and rhetorical devices. Medieval schools of rhetoric at the time encouraged such diversity, dividing literature (as Virgil suggests) into high, middle, and low styles as measured by the density of rhetorical forms and vocabulary. Another popular method of division came from St. Augustine, who focused more on audience response and less on subject matter (a Virgilian concern). Augustine divided literature into "majestic persuades", "temperate pleases", and "subdued teaches". Writers were encouraged to write in a way that kept in mind the speaker, subject, audience, purpose, manner, and occasion. Chaucer moves freely between all of these styles, showing favouritism to none. He not only considers the readers of his work as an audience, but the other pilgrims within the story as well, creating a multi-layered rhetorical puzzle of ambiguities. Thus Chaucer's work far surpasses the ability of any single medieval theory to uncover.

With this, Chaucer avoids targeting any specific audience or social class of readers, focusing instead on the characters of the story and writing their tales with a skill proportional to their social status and learning. However, even the lowest characters, such as the Miller, show surprising rhetorical ability, although their subject matter is more lowbrow. Vocabulary also plays an important part, as those of the higher classes refer to a woman as a "lady", while the lower classes use the word "wenche", with no exceptions. At times the same word will mean entirely different things between classes. The word "pitee", for example, is a noble concept to the upper classes, while in the "Merchant's Tale" it refers to sexual intercourse. Again, however, tales such as the "Nun's Priest's Tale" show surprising skill with words among the lower classes of the group, while the "Knight's Tale" is at times extremely simple.

Chaucer uses the same meter throughout almost all of his tales, with the exception of "Sir Thopas" and his prose tales. It is a decasyllable line, probably borrowed from French and Italian forms, with riding rhyme and, occasionally, a caesura in the middle of a line. His meter would later develop into the heroic meter of the 15th and 16th centuries and is an ancestor of iambic pentameter. He avoids allowing couplets to become too prominent in the poem, and four of the tales (the Man of Law's, Clerk's, Prioress', and Second Nun's) use rhyme royal.

"The Canterbury Tales" was written during a turbulent time in English history. The Catholic Church was in the midst of the Western Schism and, though it was still the only Christian authority in Europe, was the subject of heavy controversy. Lollardy, an early English religious movement led by John Wycliffe, is mentioned in the "Tales", which also mention a specific incident involving pardoners (sellers of indulgences, which were believed to relieve the temporal punishment due for sins that were already forgiven in the Sacrament of Confession) who nefariously claimed to be collecting for St. Mary Rouncesval hospital in England. "The Canterbury Tales" is among the first English literary works to mention paper, a relatively new invention that allowed dissemination of the written word never before seen in England. Political clashes, such as the 1381 Peasants' Revolt and clashes ending in the deposing of King Richard II, further reveal the complex turmoil surrounding Chaucer in the time of the "Tales"' writing. Many of his close friends were executed and he himself moved to Kent to get away from events in London.

While some readers look to interpret the characters of "The Canterbury Tales" as historical figures, other readers choose to interpret its significance in less literal terms. After analysis of Chaucer's diction and historical context, his work appears to develop a critique of society during his lifetime. Within a number of his descriptions, his comments can appear complimentary in nature, but through clever language, the statements are ultimately critical of the pilgrim's actions. It is unclear whether Chaucer would intend for the reader to link his characters with actual persons. Instead, it appears that Chaucer creates fictional characters to be general representations of people in such fields of work. With an understanding of medieval society, one can detect subtle satire at work.

The "Tales" reflect diverse views of the Church in Chaucer's England. After the Black Death, many Europeans began to question the authority of the established Church. Some turned to lollardy, while others chose less extreme paths, starting new monastic orders or smaller movements exposing church corruption in the behaviour of the clergy, false church relics or abuse of indulgences. Several characters in the "Tales" are religious figures, and the very setting of the pilgrimage to Canterbury is religious (although the prologue comments ironically on its merely seasonal attractions), making religion a significant theme of the work.

Two characters, the Pardoner and the Summoner, whose roles apply the Church's secular power, are both portrayed as deeply corrupt, greedy, and abusive. Pardoners in Chaucer's day were those people from whom one bought Church "indulgences" for forgiveness of sins, who were guilty of abusing their office for their own gain. Chaucer's Pardoner openly admits the corruption of his practice while hawking his wares. Summoners were Church officers who brought sinners to the Church court for possible excommunication and other penalties. Corrupt summoners would write false citations and frighten people into bribing them to protect their interests. Chaucer's Summoner is portrayed as guilty of the very kinds of sins for which he is threatening to bring others to court, and is hinted as having a corrupt relationship with the Pardoner. In The Friar's Tale, one of the characters is a summoner who is shown to be working on the side of the devil, not God.

Churchmen of various kinds are represented by the Monk, the Prioress, the Nun's Priest, and the Second Nun. Monastic orders, which originated from a desire to follow an ascetic lifestyle separated from the world, had by Chaucer's time become increasingly entangled in worldly matters. Monasteries frequently controlled huge tracts of land on which they made significant sums of money, while peasants worked in their employ. The Second Nun is an example of what a Nun was expected to be: her tale is about a woman whose chaste example brings people into the church. The Monk and the Prioress, on the other hand, while not as corrupt as the Summoner or Pardoner, fall far short of the ideal for their orders. Both are expensively dressed, show signs of lives of luxury and flirtatiousness and show a lack of spiritual depth. The Prioress's Tale is an account of Jews murdering a deeply pious and innocent Christian boy, a blood libel against Jews that became a part of English literary tradition. The story did not originate in the works of Chaucer and was well known in the 14th century.

Pilgrimage was a very prominent feature of medieval society. The ultimate pilgrimage destination was Jerusalem, but within England Canterbury was a popular destination. Pilgrims would journey to cathedrals that preserved relics of saints, believing that such relics held miraculous powers. Saint Thomas Becket, Archbishop of Canterbury, had been murdered in Canterbury Cathedral by knights of Henry II during a disagreement between Church and Crown. Miracle stories connected to his remains sprang up soon after his death, and the cathedral became a popular pilgrimage destination. The pilgrimage in the work ties all of the stories together and may be considered a representation of Christians' striving for heaven, despite weaknesses, disagreement, and diversity of opinion.

The upper class or nobility, represented chiefly by the Knight and his Squire, was in Chaucer's time steeped in a culture of chivalry and courtliness. Nobles were expected to be powerful warriors who could be ruthless on the battlefield yet mannerly in the King's Court and Christian in their actions. Knights were expected to form a strong social bond with the men who fought alongside them, but an even stronger bond with a woman whom they idealised to strengthen their fighting ability. Though the aim of chivalry was to noble action, its conflicting values often degenerated into violence. Church leaders frequently tried to place restrictions on jousts and tournaments, which at times ended in the death of the loser. The Knight's Tale shows how the brotherly love of two fellow knights turns into a deadly feud at the sight of a woman whom both idealise. To win her, both are willing to fight to the death. Chivalry was on the decline in Chaucer's day, and it is possible that The Knight's Tale was intended to show its flaws, although this is disputed. Chaucer himself had fought in the Hundred Years' War under Edward III, who heavily emphasised chivalry during his reign. Two tales, "Sir Topas" and "The Tale of Melibee" are told by Chaucer himself, who is travelling with the pilgrims in his own story. Both tales seem to focus on the ill-effects of chivalry—the first making fun of chivalric rules and the second warning against violence.

The "Tales" constantly reflect the conflict between classes. For example, the division of the three estates: the characters are all divided into three distinct classes, the classes being "those who pray" (the clergy), "those who fight" (the nobility), and "those who work" (the commoners and peasantry). Most of the tales are interlinked by common themes, and some "quit" (reply to or retaliate against) other tales. Convention is followed when the Knight begins the game with a tale, as he represents the highest social class in the group. But when he is followed by the Miller, who represents a lower class, it sets the stage for the "Tales" to reflect both a respect for and a disregard for upper class rules. Helen Cooper, as well as Mikhail Bakhtin and Derek Brewer, call this opposition "the ordered and the grotesque, Lent and Carnival, officially approved culture and its riotous, and high-spirited underside." Several works of the time contained the same opposition.

Chaucer's characters each express different—sometimes vastly different—views of reality, creating an atmosphere of testing, empathy, and relativism. As Helen Cooper says, "Different genres give different readings of the world: the fabliau scarcely notices the operations of God, the saint's life focuses on those at the expense of physical reality, tracts and sermons insist on prudential or orthodox morality, romances privilege human emotion." The sheer number of varying persons and stories renders the "Tales" as a set unable to arrive at any definite truth or reality.

The concept of liminality figures prominently within "The Canterbury Tales". A liminal space, which can be both geographical as well as metaphorical or spiritual, is the transitional or transformational space between a “real” (secure, known, limited) world and an unknown or imaginary space of both risk and possibility. The notion of a pilgrimage is itself a liminal experience, because it centres on travel between destinations and because pilgrims undertake it hoping to become more holy in the process. Thus, the structure of "The Canterbury Tales" itself is liminal; it not only covers the distance between London and Canterbury, but the majority of the tales refer to places entirely outside the geography of the pilgrimage. Jean Jost summarises the function of liminality in "The Canterbury Tales",

"Both appropriately and ironically in this raucous and subversive liminal space, a ragtag assembly gather together and tell their equally unconventional tales. In this unruly place, the rules of tale telling are established, themselves to be both disordered and broken; here the tales of game and earnest, solas and sentence, will be set and interrupted. Here the sacred and profane adventure begins, but does not end. Here, the condition of peril is as prominent as that of protection. The act of pilgrimaging itself consists of moving from one urban space, through liminal rural space, to the next urban space with an ever fluctuating series of events and narratives punctuating those spaces. The goal of pilgrimage may well be a religious or spiritual space at its conclusion, and reflect a psychological progression of the spirit, in yet another kind of emotional space."

Liminality is also evident in the individual tales. An obvious instance of this is the Friar’s Tale in which the yeoman devil is a liminal figure because of his transitory nature and function; it is his purpose to issue souls from their current existence to hell, an entirely different one. The Franklin’s Tale is a Breton Lai tale, which takes the tale into a liminal space by invoking not only the interaction of the supernatural and the mortal, but also the relation between the present and the imagined past.

It is sometimes argued that the greatest contribution that this work made to English literature was in popularising the literary use of the vernacular English, rather than French or Latin. English had, however, been used as a literary language for centuries before Chaucer's life, and several of Chaucer's contemporaries—John Gower, William Langland, and the Pearl Poet—also wrote major literary works in English. It is unclear to what extent Chaucer was responsible for starting a trend rather than simply being part of it. It is interesting to note that, although Chaucer had a powerful influence in poetic and artistic terms, which can be seen in the great number of forgeries and mistaken attributions (such as "The Floure and the Leafe", which was translated by John Dryden), modern English spelling and orthography owe much more to the innovations made by the Court of Chancery in the decades during and after his lifetime.

While Chaucer clearly states the addressees of many of his poems (the "Book of the Duchess" is believed to have been written for John of Gaunt on the occasion of his wife's death in 1368), the intended audience of "The Canterbury Tales" is more difficult to determine. Chaucer was a courtier, leading some to believe that he was mainly a court poet who wrote exclusively for the nobility. He is referred to as a noble translator and poet by Eustache Deschamps and by his contemporary John Gower. It has been suggested that the poem was intended to be read aloud, which is probable as this was a common activity at the time. However, it also seems to have been intended for private reading as well, since Chaucer frequently refers to himself as the writer, rather than the speaker, of the work. Determining the intended audience directly from the text is even more difficult, since the audience is part of the story. This makes it difficult to tell when Chaucer is writing to the fictional pilgrim audience or the actual reader.

Chaucer's works may have been distributed in some form during his lifetime in part or in whole. Scholars speculate that manuscripts were circulated among his friends, but likely remained unknown to most people until after his death. However, the speed with which copyists strove to write complete versions of his tale in manuscript form shows that Chaucer was a famous and respected poet in his own day. The Hengwrt and Ellesmere manuscripts are examples of the care taken to distribute the work. More manuscript copies of the poem exist than for any other poem of its day except "The Prick of Conscience", causing some scholars to give it the medieval equivalent of bestseller status. Even the most elegant of the illustrated manuscripts, however, is not nearly as highly decorated as the work of authors of more respectable works such as John Lydgate's religious and historical literature.

John Lydgate and Thomas Occleve were among the first critics of Chaucer's "Tales", praising the poet as the greatest English poet of all time and the first to show what the language was truly capable of poetically. This sentiment was universally agreed upon by later critics into the mid-15th century. Glosses included in "The Canterbury Tales" manuscripts of the time praised him highly for his skill with "sentence" and rhetoric, the two pillars by which medieval critics judged poetry. The most respected of the tales was at this time the Knight's, as it was full of both.

The incompleteness of the "Tales" led several medieval authors to write additions and supplements to the tales to make them more complete. Some of the oldest existing manuscripts of the tales include new or modified tales, showing that even early on, such additions were being created. These emendations included various expansions of the "Cook's Tale", which Chaucer never finished, "The Plowman's Tale", "The Tale of Gamelyn", the "Siege of Thebes", and the "Tale of Beryn".

The "Tale of Beryn", written by an anonymous author in the 15th century, is preceded by a lengthy prologue in which the pilgrims arrive at Canterbury and their activities there are described. While the rest of the pilgrims disperse throughout the town, the Pardoner seeks the affections of Kate the barmaid, but faces problems dealing with the man in her life and the innkeeper Harry Bailey. As the pilgrims turn back home, the Merchant restarts the storytelling with "Tale of Beryn". In this tale, a young man named Beryn travels from Rome to Egypt to seek his fortune only to be cheated by other businessmen there. He is then aided by a local man in getting his revenge. The tale comes from the French tale "Bérinus" and exists in a single early manuscript of the tales, although it was printed along with the tales in a 1721 edition by John Urry.

John Lydgate wrote "The Siege of Thebes" in about 1420. Like the "Tale of Beryn", it is preceded by a prologue in which the pilgrims arrive in Canterbury. Lydgate places himself among the pilgrims as one of them and describes how he was a part of Chaucer's trip and heard the stories. He characterises himself as a monk and tells a long story about the history of Thebes before the events of the "Knight's Tale". John Lydgate's tale was popular early on and exists in old manuscripts both on its own and as part of the "Tales". It was first printed as early as 1561 by John Stow, and several editions for centuries after followed suit.

There are actually two versions of "The Plowman's Tale", both of which are influenced by the story "Piers Plowman", a work written during Chaucer's lifetime. Chaucer describes a Plowman in the "General Prologue" of his tales, but never gives him his own tale. One tale, written by Thomas Occleve, describes the miracle of the Virgin and the Sleeveless Garment. Another tale features a pelican and a griffin debating church corruption, with the pelican taking a position of protest akin to John Wycliffe's ideas.

"The Tale of Gamelyn" was included in an early manuscript version of the tales, Harley 7334, which is notorious for being one of the lower-quality early manuscripts in terms of editor error and alteration. It is now widely rejected by scholars as an authentic Chaucerian tale, although some scholars think he may have intended to rewrite the story as a tale for the Yeoman. Dates for its authorship vary from 1340 to 1370.

Many literary works (both fiction and non-fiction alike) have used a similar frame narrative to "The Canterbury Tales" as an homage. Science-fiction writer Dan Simmons wrote his Hugo Award winning novel "Hyperion" based on an extra-planetary group of pilgrims. Evolutionary biologist Richard Dawkins used "The Canterbury Tales" as a structure for his 2004 non-fiction book about evolution titled "". His animal pilgrims are on their way to find the common ancestor, each telling a tale about evolution.

Henry Dudeney's book "The Canterbury Puzzles" contains a part reputedly lost from what modern readers know as Chaucer's tales.

Historical-mystery novelist P.C. Doherty wrote a series of novels based on "The Canterbury Tales", making use of both the story frame and Chaucer's characters.

Canadian author Angie Abdou translates "The Canterbury Tales" to a cross section of people, all snow-sports enthusiasts but from different social backgrounds, converging on a remote back-country ski cabin in British Columbia in the 2011 novel "The Canterbury Trail".

"The Two Noble Kinsmen", by William Shakespeare and John Fletcher, a retelling of "The Knight's Tale", was first performed in 1613 or 1614 and published in 1634. In 1961, Erik Chisholm completed his opera, "The Canterbury Tales". The opera is in three acts: The Wyf of Bath’s Tale, The Pardoner’s Tale and The Nun’s Priest’s Tale. Nevill Coghill's modern English version formed the basis of a musical version that was first staged in 1964.

"A Canterbury Tale", a 1944 film jointly written and directed by Michael Powell and Emeric Pressburger, is loosely based on the narrative frame of Chaucer's tales. The movie opens with a group of medieval pilgrims journeying through the Kentish countryside as a narrator speaks the opening lines of the "General Prologue". The scene then makes a now-famous transition to the time of World War II. From that point on, the film follows a group of strangers, each with his or her own story and in need of some kind of redemption, who are making their way to Canterbury together. The film's main story takes place in an imaginary town in Kent and ends with the main characters arriving at Canterbury Cathedral, bells pealing and Chaucer's words again resounding. "A Canterbury Tale" is recognised as one of the Powell-Pressburger team's most poetic and artful films. It was produced as wartime propaganda, using Chaucer's poetry, referring to the famous pilgrimage, and offering photography of Kent to remind the public of what made Britain worth fighting for. In one scene a local historian lectures an audience of British soldiers about the pilgrims of Chaucer's time and the vibrant history of England.

Pier Paolo Pasolini's 1972 film "The Canterbury Tales" features several of the tales, some of which keep close to the original tale and some of which are embellished. The "Cook's Tale", for instance, which is incomplete in the original version, is expanded into a full story, and the "Friar's Tale" extends the scene in which the Summoner is dragged down to hell. The film includes these two tales as well as the "Miller's Tale", the "Summoner's Tale", the "Wife of Bath's Tale", and the "Merchant's Tale".

On April 26, 1986, American radio personality Garrison Keillor opened "The News from Lake Wobegon" portion of the first live TV broadcast of his "A Prairie Home Companion" radio show with a reading of the original Middle English text of the General Prologue. He commented, "Although those words were written more than 600 years ago, they still describe spring."

Television adaptations include Alan Plater's 1975 re-telling of the stories in a series of plays for BBC2: "Trinity Tales". In 2003, BBC again featured modern re-tellings of selected tales.

[https://www.toevolution.com/file/view/2176/the-canterbury-tales-by-geoffrey-chaucer-download-pdf-online-reading-summary The Canterbury Tales by Geoffrey Chaucer [Download-PDF-Online Reading-Summary<nowiki>]</nowiki>]






</doc>
<doc id="7628" url="https://en.wikipedia.org/wiki?curid=7628" title="Christine de Pizan">
Christine de Pizan

Christine de Pizan (also seen as de Pisan; ; 1364 – c. 1430) was an Italian late medieval author. She is best remembered for defending women in "The Book of the City of Ladies" and "The Treasure of the City of Ladies". Pizan was a prominent moralist and political thinker in medieval France. Pizan’s patrons included Louis of Orleans, Philip the Bold and John the Fearless. She served as a court writer during the reign of Charles VI. Her books of advice to princesses, princes and knights remained in print until the 16th century. 

In recent decades, Pizan's work has been returned to prominence by the efforts of scholars such as Charity Cannon Willard, Earl Jeffrey Richards and Simone de Beauvoir. 
Christine de Pizan was born 1364 in Venice, Italy. She was the daughter of Tommaso di Benvenuto da Pizzano. Her father became known as Thomas de Pizan, named for the family's origins in the town of Pizzano, south east of Bologna. Her father worked as physician, court astrologer, and Councillor of the Republic of Venice. Thomas de Pizan accepted an appointment to the court of Charles V of France as the king's astrologer and in 1368 Pizan moved to Paris. In 1379 Pizan married the notary and royal secretary Etienne du Castel. 

She had three children. Her daughter became a nun at the Dominican Abbey in Poissy in 1397 as a companion to the king's daughter Marie of Valois. Pizan’s husband died of the plague in 1389, her father had died the year before. Pizan was left to support her mother and her children. When she tried to collect money from her husband's estate, she faced complicated lawsuits regarding the recovery of salary due her husband. On 4 June 1389, in a judgment concerning a lawsuit filed against her by the archbishop of Sens and François Chanteprime, councillors of the king, Christine was styled "damoiselle" and widow of "Estienne du Castel".

In order to support herself and her family, Christine turned to writing. By 1393, she was writing love ballads, which caught the attention of wealthy patrons within the court. Pizan became a prolific writer. Her involvement in the production of her books and her skilful use of patronage in turbulent political times has earned her the title of the first professional women of letters in Europe. Although Italian by birth, Pizan expressed fervent nationalism for France. Affective and financially she attached to the royal family of France. She gifted or dedicated her early ballades to members of the royal family, such as Isabeau of Bavaria, Louis I, Duke of Orléans and Marie of Berry. Of Queen Isabeau she wrote in 1402 "High, excellent crowned Queen of France, very redoubtable princess, powerful lady, born at a lucky hour". 

France was ruled by Charles VI who experienced a series of mental breakdowns, causing a crisis of leadership for the French monarchy. He was often absent from court and could eventually only make decisions with the approval of a royal council. Queen Isabeau was nominally in charge of governance when her husband was absent from court, but could not extinguish the quarrel between members of the royal family. In the past, Blanche of Castile had played a central role in the stability of the royal court and had acted as regent of France. Pizan published a series of works on the virtues of women, referencing Queen Blanche and dedicating them to Queen Isabeau. 

Pizan believed that France had been founded by the descendants of the Trojans and that its governance by the royal family adhered to the Aristotelian ideal. In 1400 Pizan published "L'Épistre de Othéa a Hector" ("Letter of Othea to Hector"). When first published, the book was dedicated to Louis of Orléans, the brother of Charles VI, who was at court seen as potential regent of France. In "L'Épistre de Othéa a Hector" Hector of Troy is tutored in statecraft and the political virtues by the goddess of wisdom Othéa. Pizan produced richly illustrated luxury editions of "L'Épistre de Othéa a Hector" in 1400. Between 1408 and 1415 Pizan produced further editions of the book. Throughout her career she produced rededicated editions of the book with customised prologues for patrons, including an edition for Philip the Bold in 1403, and editions for Jean of Berry and Henry IV of England in 1404. Patronage changed in the late Middle Ages. Texts were still produced and circulated as continuous roll manuscripts, but were increasingly replaced by the bound codex. Members of royal family became patrons of writers by commissioning books. As materials became cheaper a book trade developed, so writers and bookmakers produced books for the French nobility, who could afford to establish their own libraries. Pizan thus had no single patron who consistently supported her financially and became associated with the royal court and the different fractions of the royal family - the Burgundy, Orleans and Berry – each having their own respective courts. Throughout her career Pizan undertook concurrent paid projects for individual patrons and subsequently published these works for dissemination among the nobility of France. 

In 1402 Pizan became involved in a renowned literary controversy, the "Querelle du Roman de la Rose". Pizan instigated this debate by questioning the literary merits of Jean de Meun's popular "Romance of the Rose". "Romance of the Rose" satirizes the conventions of courtly love while critically depicting women as nothing more than seducers. In the midst of the Hundred Years’ War between French and English kings, Pizan published the dream allegory "Le Chemin de long estude" in 1403. In the first person narrative she and Cumaean Sibyl travel together and witness a debate on the state of the world between the four allegories - Wealth, Nobility, Chivalry and Wisdom. Pizan suggests that justice could be brought to earth by a single monarch who had the necessary qualities. 

In 1404 Pizan chronicled the life of Charles V, portraying him as the ideal king and political leader, in "Le Livre des fais et bonnes meurs du sage roy Charles V". The chronical had been commissioned by Philip the Bold and in the chronicle Pizan passed judgement on the state of the royal court. When praising the efforts of Charles V in studying Latin, Pizan lamented that her contemporaries had to resort to strangers to read the law to them. Before the book was completed, Philip the Bold died, and Pizan offered the book to Jean of Berry in 1405, finding a new royal patron. She was paid 100 livre for the book by Philip's successor John the Fearless in 1406 and would receive payments from his court for books until 1412.

In 1405 Pizan published "Le Livre de la cité des dames" ("The Book of the City of Ladies") and "Le Livre des trois vertus" ("Book of Three Virtues", known as "The Treasure of the City of Ladies"). In "Le Livre de la cité des dames" Pizan presented intellectual and royal female leaders, such as Queen Zenobia. Pizan dedicated "Le Livre des trois vertus" to the dauphine Margaret of Nevers, advising the young princess on what she had to learn. As Queen Isabeau’s oldest son Louis of Guyenne came of age Pizan addressed three works to him with the intention of promoting wise and effective government. The earliest of the three works has been lost. In "Livre du Corps de policie" ("The Book of the Body Politic"), published in 1407 and dedicated to the dauphin, Pizan set out a political treatises which analysed and described the customs and governments of late medieval European societies. Pizan favoured hereditary monarchies, arguing in reference to Italian city-states that were governed by princes or trades, that "such governance is not profitable at all for the common good". Pizan also devoted several chapters to the duties of a king as military leader and she described in detail the role of the military class in society.

France was at the verge of all out civil war since 1405. In 1407 John I of Burgundy, also known as John the Fearless, plunged France into a crisis when he had Louis of Orléans assassinated. The Duke of Burgundy fled Paris when his complicity in the assignation got known, but was appointed regent of France on behalf of Charles VI in late 1408 after his military victory in the Battle of Othee. It is not certain who commissioned Pizan to write a treatise on military warfare, but in 1410 Pizan published the manual on chivalry, entitled "Livre des fais d'armes et de chevalerie" ("The Book of Feats of Arms and of Chivalry"). Pizan received 200 livre from the royal treasury in early 1411 for the book. In the preface Pizan explained that she published the manual in French so that it could be read by practitioners of war not well versed in Latin. The book opened with a discussion of the just war theory advanced by Honoré Bonet. Pizan also referenced classical writers on military warfare, such as Vegetius, Frontinus and Valerius Maximus. Pizan discussed contemporary matters relating to what she termed the "Laws of War", such as capital punishment, the payment of troops, as well as the treatment of noncombatants and prisoners of war. Pizan opposed trial by combat, but articulated the medieval belief that God is the lord and governor of battle and that wars are the proper execution of justice. Nevertheless she acknowledged that in a war "many great wrongs, extortions, and grievous deeds are committed, as well as raping, killings, forced executions, and arsons". Pizan limited the right to wage war to sovereign kings because as head of states they were responsible for the welfare of their subjects. In 1411 the royal court published an edict prohibiting nobles from raising an army.

After civil war had broken out in France, Pizan in 1413 offered guidance to the young dauphin on how to govern well, publishing "Livre de la paix" ("The Book of Peace"). "Livre de la paix" would be Pizan’s last major work and contained detailed formulations of her thoughts on good governance. The period was marked by pouts of civil war and failed attempts to bring John the Fearless to justice for assassinating his cousin. Pizan addressed Louis of Guyenne directly, encouraging him to continue the quest for peace in France. She argued that “Every kingdom divided in itself will be made desolate, and every city and house divided against itself will not stand”. Pizan was acquainted with William of Tignonville, an ambassador to the royal court, and referenced Tignonville’s speeches on the Armagnac–Burgundian Civil War. Pizan’s drew a utopian vision of a just ruler, who could take advice from those older or wiser. In arguing that peace and justice were possible on earth as well as in heaven, Pizan was influenced by Dante, who she had referenced in "Le Chemin de long estude". Pizan encouraged the dauphin to deserve respect, by administering justice promptly and living by worthy example. Pizan urged young princes to make themselves available to their subjects, avoid anger and cruelty, to act liberally, clement and truthful. Pizan’s interpretation of the virtuous Christian prince built on the advice to rulers by St Benedict, Peter Abelard and Cicero.
In 1414 Pizan presented Queen Isabeau with a lavishly decorated collection of her works (now known as "British Library Harley 4431"). The bound book contained 30 of Pizan's writings and 130 miniatures. She had been asked by the queen to produce the book. Noted for its quality miniature illuminations, Pizan herself and her past royal patrons were depicted. As a mark of ownership and authorship the opening frontispiece depicted Queen Isabeau being presented with the book by Pizan. 

In 1418 Pizan published a consolation for women who had lost family members in the Battle of Agincourt under the title "Epistre de la prison de vie Humaine" ("Letter Concerning the Prison of Human Life"). In it Pizan did not express any optimism or hope that peace could be found on earth. Instead she expressed the view that the soul was trapped in the body and imprisoned in hell. The previous year she had presented the "Epistre de la prison de vie Humaine" to Marie of Berry, the administrator of the Duchy of Bourbon whose husband was held in English captivity.

Historians assume that Pizan spent the last ten years of her life in the Dominican Convent of Poissy because of the civil war and the occupation of Paris by the English. Away from the royal court her literary activity dried up. However, in 1429, after Joan of Arc's military victory over the English, Pizan published the poem "Ditié de Jehanne d'Arc" ("The Tale of Joan of Arc"). Published just a few days after the coronation of Charles VII, Pizan expressed renewed optimism. She cast Arc as the fulfilment of prophecies by Merlin, Cumaean Sibyl and Saint Bede, helping Charles VII to fulfil the predictions of Charlemagne.

Pizan is believed to have died in 1430, before Arc was trialled and executed by the English. After her death the political crisis in France was resolved when Queen Isabeau’s only surviving son Charles VII and John the Fearless’ successor as Duke of Burgundy, Philip the Good, signed the Peace of Arras in 1435.

Pizan produced a large number of vernacular works, in both prose and verse. Her works include political treatises, mirrors for princes, epistles, and poetry.

Pizan’s book "Le Dit de la Rose" ("The Tale of the Rose") was published in 1402 as a direct attack on Jean de Meun’s extremely popular book "Romance of the Rose" which characterised women as seducers. Pizan claimed that Meun’s views were misogynistic, vulgar, immoral, and slanderous to women. The exchange between the two authors involved them sending each other their treatises, defending their respective views. At the height of the exchange Pizan published "Querelle du Roman de la Rose" ("Letters on the Debate of the Rose"). In this particular apologetic response, Pizan belittles her own writing style, employing a rhetorical strategy by writing against the grain of her meaning, also known as antiphrasis.

By 1405 Pizan had completed her most famous literary works, "The Book of the City of Ladies" ("Le Livre de la cité des dames") and "The Treasure of the City of Ladies" ("Le Livre des trois vertus"). The first of these shows the importance of women's past contributions to society, and the second strives to teach women of all estates how to cultivate useful qualities.

In "The Book of the City of Ladies" Pizan created a symbolic city in which women are appreciated and defended. She constructed three allegorical figures – Reason, Justice, and Rectitude – in the common pattern of literature in that era, when many books and poetry utilized stock allegorical figures to express ideas or emotions. She enters into a dialogue, a movement between question and answer, with these allegorical figures that is from a completely female perspective. Together, they create a forum to speak on issues of consequence to all women. Only female voices, examples and opinions provide evidence within this text. Through Lady Reason in particular Pizan argues that stereotypes of women can be sustained only if women are prevented from entering into the conversation.

In "City of Ladies" Pizan deliberated on the debate whether the virtues of men and women differ, a frequently debated topic in late medieval Europe, particularly in the context of Aristotelian virtue ethics and his views on women. Pizan repeatedly used the theological argument that men and women are created in God's image and both have souls capable of embracing God's goodness. Among the inhabitants of the "City of Ladies" are female saints, women from the Old Testament and virtuous women from the pagan antiquity as portrait by Giovanni Boccaccio.

In "The Treasure of the City of Ladies" Pizan addressed the "community" of women with the stated objective of instructing them in the means of achieving virtue. She took the position that all women were capable of humility, diligence and moral rectitude, and that duly educated all women could become worthy residents of the imaginary "City of Ladies". Drawing on her own life, Pizan advised women on how to navigate the perils of early 15th century French society. With reference to Augustine of Hippo and other saints Pizan offered advice on how the noble lady could achieve the love of God. Pizan speaks through the allegorical figures of God's daughters - Reason, Rectitude and Justice - who represent the Three Virtues most important to women's success. Through secular examples of these three virtues, Pizan urged women to discover meaning and achieve worthy acts in their lives. Pizan argued that women's success depends on their ability to manage and mediate by speaking and writing effectively.

Pizan specifically sought out other women to collaborate in the creation of her work. She makes special mention of a manuscript illustrator we know only as Anastasia, whom she described as the most talented of her day.

Pizan published 41 known pieces of poetry and prose in her lifetime and she gained fame across Europe as the first professional woman writer. She achieved such credibility that royalty commissioned her prose and contemporary intellectuals kept copies of her works in their libraries.

After her death in 1430 Pizan's influence was acknowledged by a variety of authors and her writings remained popular. Her book "Le Livre de la cité des dames" remained in print. Portuguese and Dutch editions of it exist from the 15th century, and French editions were still being printed in 1536. In 1521 "The Book of the City of Ladies" was published in English. Pizan's "Le Livre des trois vertus" ("The Treasure of the City of Ladies") became an important reference point for royal women in the 15th and 16th century. Anne of France, who acted as regent of France, used it as a basis for her 1504 book of "Enseignemens", written for her daughter Suzanne Duchess of Bourbon, who as agnatic heir to the Bourbon lands became co-regent. Pizan's advice to princesses was translated and circulated as manuscript or printed book among the royal families of France and Portugal. The "City of Ladies" was acknowledged and referenced by 16th century French women writers, including Anne de Beaujeu, Gabrielle de Bourbon, Marguerite de Navarre and Georgette de Montenay.

Pizan's political writings received some attention too. "Livre de la paix" was referenced by the humanist Gabriel Naudé and Pizan was given large entries in encyclopedias by Denis Diderot, Louis Moréri and Prosper Marchand. In 1470 Jean V de Bueil reproduced Pizan's detailed accounts of the armies and material needed to defend a castle or town against a siege in "Le Jouvence". "Livre des fais d'armes et de chevalerie" was published in its entirety by the book printer Antoine Vérard in 1488, but Vérard claimed that it was his translation of Vegetius. Philippe Le Noir authored an abridged version of Pizan's book in 1527 under the title "L'Arbre des Batailles et fleur de chevalerie" ("The tree of battles and flower of chivalry"). 

"Livre des fais d'armes et de chevalerie" was translated into English by William Caxton for Henry VII in 1489 and was published under the title "The Book of Feats of Arms and of Chivalry" as print one year later, attributing Pizan as author. English editions of "The Book of the City of Ladies" and "Livre du corps de policie" ("The Book of the Body Politic") were printed in 1521 without referencing Pizan as the author. Elizabeth I had in her court library copies of "The Book of the City of Ladies", "L'Épistre de Othéa a Hector" ("Letter of Othea to Hector") and "The Book of Feats of Arms and of Chivalry". Among the possessions of the English queen were tapestries with scenes from the "City of Ladies". However, when in the early 19th century Raimond Thomassy published an overview of Pizan’s political writings, he noted that modern editions of these writings were not published and that as a political theorist Pizan was descending into obscurity. 
While Pizan’s mixture of classical philosophy and humanistic ideals was in line with the style of other popular authors at the time, her outspoken defence of women was an anomaly. In her works she vindicated women against popular misogynist texts, such as Ovid’s "Art of Love", Jean de Meun’s "Romance of the Rose" and Matheolus’s "Lamentations". Her activism has drawn the fascination of modern feminists. Simone de Beauvoir wrote in 1949 that "Épître au Dieu d'Amour" was "the first time we see a woman take up her pen in defence of her sex".
The 1979 artwork "The Dinner Party" features a place setting for Christine de Pizan. In the 1980s Sandra Hindman published a study of the political events reverenced in the illuminations of Pizan’s published works. 





</doc>
<doc id="7630" url="https://en.wikipedia.org/wiki?curid=7630" title="Catharism">
Catharism

Catharism (; from the Greek: , "katharoi", "the pure [ones]") was a Christian dualist or Gnostic revival movement that thrived in some areas of Southern Europe, particularly what is now northern Italy and southern France, between the 12th and 14th centuries. The followers were known as Cathars and are now mainly remembered for a prolonged period of persecution by the Catholic Church which did not recognise their belief as Christian. Catharism appeared in Europe in the Languedoc region of France in the 11th century and this is when the name first appears. The adherents were also sometimes known as Albigensians after the city Albi in southern France where the movement first took hold. The beliefs are believed to have been brought from Persia or the Byzantine Empire.

Cathar beliefs varied between communities, because Catharism was initially taught by ascetic leaders who had set few guidelines. The Catholic Church denounced its practices including the "Consolamentum" ritual, by which Cathar individuals were baptized and raised to the status of "perfect".

Catharism may have had its roots in the Paulician movement in Armenia and eastern Byzantine Anatolia and certainly in the Bogomils of the First Bulgarian Empire, who were influenced by the Paulicians resettled in Thrace (Philipopolis) by the Byzantines. Though the term "Cathar" () has been used for centuries to identify the movement, whether the movement identified itself with this name is debatable. In Cathar texts, the terms "Good Men" ("Bons Hommes"), "Good Women" ("Bonnes Femmes"), or "Good Christians" ("Bons Chrétiens") are the common terms of self-identification.

The idea of two gods or principles, one good and the other evil, was central to Cathar beliefs. The good God was the God of the New Testament and the creator of the spiritual realm, contrasted with the evil Old Testament God—the creator of the physical world whom many Cathars, and particularly their persecutors, identified as Satan. All visible matter, including the human body, was created by this evil god; matter was therefore tainted with sin. This was antithetical to the monotheistic Catholic Church, whose fundamental principle was that there was only one God, who created all things visible and invisible. Cathars thought human spirits were the genderless spirits of angels trapped within the physical creation of the evil god, destined to be reincarnated until they achieved salvation through the consolamentum.

From the beginning of his reign, Pope Innocent III attempted to end Catharism by sending missionaries and by persuading the local authorities to act against them. In 1208, Innocent's papal legate Pierre de Castelnau was murdered while returning to Rome after excommunicating Count Raymond VI of Toulouse, who, in his view, was too lenient with the Cathars. Pope Innocent III then abandoned the option of sending Catholic missionaries and jurists, declared Pierre de Castelnau a martyr and launched the Albigensian Crusade which all but ended Catharism.

The origins of the Cathars' beliefs are unclear, but most theories agree they came from the Byzantine Empire, mostly by the trade routes and spread from the First Bulgarian Empire to the Netherlands. The name of Bulgarians ("Bougres") was also applied to the Albigensians, and they maintained an association with the similar Christian movement of the Bogomils ("Friends of God") of Thrace. "That there was a substantial transmission of ritual and ideas from Bogomilism to Catharism is beyond reasonable doubt." Their doctrines have numerous resemblances to those of the Bogomils and the Paulicians, who influenced them, as well as the earlier Marcionites, who were found in the same areas as the Paulicians, the Manicheans and the Christian Gnostics of the first few centuries AD, although, as many scholars, most notably Mark Pegg, have pointed out, it would be erroneous to extrapolate direct, historical connections based on theoretical similarities perceived by modern scholars.

St John Damascene, writing in the 8th century AD, also notes of an earlier sect called the "Cathari", in his book "On Heresies", taken from the epitome provided by Epiphanius of Salamis in his "Panarion". He says of them: "They absolutely reject those who marry a second time, and reject the possibility of penance [that is, forgiveness of sins after baptism]". These are probably the same Cathari (actually Novations) who are mentioned in Canon 8 of the First Ecumenical Council of Nicaea in the year 325, which states "...[I]f those called Cathari come over [to the faith], let them first make profession that they are willing to communicate [share full communion] with the twice-married, and grant pardon to those who have lapsed..."

It is likely that we have only a partial view of their beliefs, because the writings of the Cathars were mostly destroyed because of the doctrinal threat perceived by the Papacy; much of our existing knowledge of the Cathars is derived from their opponents. Conclusions about Cathar ideology continue to be debated with commentators regularly accusing their opponents of speculation, distortion and bias. There are a few texts from the Cathars themselves which were preserved by their opponents (the "Rituel Cathare de Lyon") which give a glimpse of the inner workings of their faith, but these still leave many questions unanswered. One large text which has survived, "The Book of Two Principles" ("Liber de duobus principiis"), elaborates the principles of dualistic theology from the point of view of some of the Albanenses Cathars.

It is now generally agreed by most scholars that identifiable historical Catharism did not emerge until at least 1143, when the first confirmed report of a group espousing similar beliefs is reported being active at Cologne by the cleric Eberwin of Steinfeld. A landmark in the "institutional history" of the Cathars was the Council, held in 1167 at Saint-Félix-Lauragais, attended by many local figures and also by the Bogomil "papa" Nicetas, the Cathar bishop of (northern) France and a leader of the Cathars of Lombardy.

The Cathars were largely local, Western European/Latin Christian phenomena, springing up in the Rhineland cities (particularly Cologne) in the mid-12th century, northern France around the same time, and particularly the Languedoc—and the northern Italian cities in the mid-late 12th century. In the Languedoc and northern Italy, the Cathars attained their greatest popularity, surviving in the Languedoc, in much reduced form, up to around 1325 and in the Italian cities until the Inquisitions of the fourteenth century finally extirpated them.

Cathars, in general, formed an anti-sacerdotal party in opposition to the Catholic Church, protesting against what they perceived to be the moral, spiritual and political corruption of the Church.

When Bishop Fulk of Toulouse, a key leader of the anti-Cathar persecutions, excoriated the Languedoc Knights for not pursuing the heretics more diligently, he received the reply:
In contrast to the Catholic Church, the Cathars had but one central rite, the Consolamentum, or Consolation. This involved a brief spiritual ceremony to remove all sin from the believer and to induct him or her into the next higher level as a perfect.

Many believers would receive the Consolamentum as death drew near, performing the ritual of liberation at a moment when the heavy obligations of purity required of Perfecti would be temporally short. Some of those who received the sacrament of the consolamentum upon their death-beds may thereafter have shunned further food or drink and, more often and in addition, expose themselves to extreme cold, in order to speed death. This has been termed the "endura". It was claimed by some of the Catholic writers that when a Cathar, after receiving the Consolamentum, began to show signs of recovery he or she would be smothered in order to ensure his or her entry into paradise. Other than at such moments of "extremis", little evidence exists to suggest this was a common Cathar practice.

The Cathars also refused the Catholic Sacrament of the eucharist saying that it could not possibly be the body of Christ. They also refused to partake in the practice of Baptism by water. The following two quotes are taken from the Catholic Inquisitor Bernard Gui's experiences with the Cathar practices and beliefs:
Some believe that the Catharist conception of Jesus resembled nontrinitarian modalistic monarchianism (Sabellianism) in the West and adoptionism in the East.

Bernard of Clairvaux's biographer and other sources accuse some Cathars of Arianism, and some scholars see Cathar Christology as having traces of earlier Arian roots. According to some of their contemporary enemies Cathars did not accept the normative Trinitarian understanding of Jesus, but considered him the human form of an angel similar to Docetic Christology. Zoé Oldenbourg (2000) compared the Cathars to "Western Buddhists" because she considered that their view of the doctrine of "resurrection" taught by Jesus was, in fact, similar to the Buddhist doctrine of reincarnation. The Cathars taught that to regain angelic status one had to renounce the material self completely. Until one was prepared to do so, he/she would be stuck in a cycle of reincarnation, condemned to live on the corrupt Earth.

The alleged sacred texts of the Cathars besides the "New Testament", include "The Gospel of the Secret Supper, or John's Interrogation" and "The Book of the Two Principles".

Killing was abhorrent to the Cathars. Consequently, abstention from all animal food (sometimes exempting fish) was enjoined of the Perfecti. The Perfecti avoided eating anything considered to be a by-product of sexual reproduction. War and capital punishment were also condemned—an abnormality in Medieval Europe. In a world where few could read, their rejection of oath-taking marked them as social outcasts.

To the Cathars, reproduction was a moral evil to be avoided, as it continued the chain of reincarnation and suffering in the material world. It was claimed by their opponents that, given this loathing for procreation, they generally resorted to sodomy. Such was the situation that a charge of heresy leveled against a suspected Cathar was usually dismissed if the accused could show he was legally married.

It has been alleged that the Cathar Church of the Languedoc had a relatively flat structure, distinguishing between the baptised "perfecti" (a term they did not use; instead, "bonhommes") and ordinary unbaptised believers ("credentes"). By about 1140, liturgy and a system of doctrine had been established. They created a number of bishoprics, first at Albi around 1165 and after the 1167 Council at Saint-Félix-Lauragais sites at Toulouse, Carcassonne, and Agen, so that four bishoprics were in existence by 1200. In about 1225, during a lull in the Albigensian Crusade, the bishopric of Razès was added. Bishops were supported by their two assistants: a "filius maior" (typically the successor) and a "filius minor", who were further assisted by deacons. The "perfecti" were the spiritual elite, highly respected by many of the local people, leading a life of austerity and charity. In the apostolic fashion they ministered to the people and travelled in pairs.

Catharism has been seen as giving women the greatest opportunities for independent action since women were found as being believers as well as Perfecti, who were able to administer the sacrament of the "consolamentum".

Cathars believed that one would be repeatedly reincarnated until one commits to the self-denial of the material world. A man could be reincarnated as a woman and vice versa, thereby rendering gender meaningless. The spirit was of utmost importance to the Cathars and was described as being immaterial and sexless. Because of this belief, the Cathars saw women as equally capable of being spiritual leaders, which undermined the very concept of gender as held by the Catholic Church.

Women accused of being heretics in early medieval Christianity included those labeled Gnostics, Cathars, and Beguines, as well as several other groups that were sometimes "tortured and executed". Cathars, like the Gnostics who preceded them, assigned more importance to the role of Mary Magdalene in the spread of early Christianity than the Church previously did. Her vital role as a teacher contributed to the Cathar belief that women could serve as spiritual leaders. Women were found to be included in the Perfecti in significant numbers, with numerous receiving the "consolamentum" after being widowed. Having reverence for the Gospel of John, the Cathars saw Mary Magdalene as perhaps even more important than Saint Peter, the founder of the Church.

The Cathar movement proved successful in gaining female followers because of its proto-feminist teachings along with the general feeling of exclusion from the Catholic church. Catharism attracted numerous women with the promise of a leadership role that the Catholic Church did not allow. Catharism let women become a perfect of the faith, a position of far more prestige than anything the Catholic Church offered. These female perfects were required to adhere to a strict and ascetic lifestyle, but were still able to have their own houses. Although many women found something attractive in Catharism, not all found its teachings convincing. A notable example is Hildegard of Bingen, who in 1163 gave a widely renowned sermon against the Cathars in Cologne. During this speech, Hildegard announced a state of eternal punishment and damnation to all those who accepted Cathar beliefs.

While women perfects rarely traveled to preach the faith, they still played a vital role in the spreading of the Catharism by establishing group homes for women. Though it was extremely uncommon, there were isolated cases of female Cathars leaving their homes to spread the faith. In Cathar communal homes (ostals), women were educated in the faith, and these women would go on to bear children who would then also become believers. Through this pattern the faith grew exponentially through the efforts of women as each generation passed. Among some groups of Cathars there were more women than there were men.

Despite women having an instrumental role in the growing of the faith, misogyny was not completely absent from the Cathar movement. Some seemingly misogynistic Cathar beliefs include that one's last incarnation had to be experienced as a man to break the cycle. This belief was inspired by later French Cathars, which taught that women must be reborn as men in order to achieve salvation. Another one was that the sexual allure of women impeded man's ability to reject the material world. Toward the end of the Cathar movement, Catharism became more misogynistic and started the practice of excluding women perfects. However, the influence of these type of misogynistic beliefs and practices remained limited (Later Italian perfects still included women.)

In 1147, Pope Eugene III sent a legate to the Cathar district in order to arrest the progress of the Cathars. The few isolated successes of Bernard of Clairvaux could not obscure the poor results of this mission, which clearly showed the power of the sect in the Languedoc at that period. The missions of Cardinal Peter of St. Chrysogonus to Toulouse and the Toulousain in 1178, and of Henry of Marcy, cardinal-bishop of Albano, in 1180–81, obtained merely momentary successes. Henry's armed expedition, which took the stronghold at Lavaur, did not extinguish the movement.

Decisions of Catholic Church councils—in particular, those of the Council of Tours (1163) and of the Third Council of the Lateran (1179)—had scarcely more effect upon the Cathars. When Pope Innocent III came to power in 1198, he was resolved to deal with them.

At first Innocent tried peaceful conversion, and sent a number of legates into the Cathar regions. They had to contend not only with the Cathars, the nobles who protected them, and the people who respected them, but also with many of the bishops of the region, who resented the considerable authority the Pope had conferred upon his legates. In 1204, Innocent III suspended a number of bishops in Occitania; in 1205 he appointed a new and vigorous bishop of Toulouse, the former troubadour Foulques. In 1206 Diego of Osma and his canon, the future Saint Dominic, began a programme of conversion in Languedoc; as part of this, Catholic-Cathar public debates were held at Verfeil, Servian, Pamiers, Montréal and elsewhere.

Saint Dominic met and debated with the Cathars in 1203 during his mission to the Languedoc. He concluded that only preachers who displayed real sanctity, humility and asceticism could win over convinced Cathar believers. The institutional Church as a general rule did not possess these spiritual warrants. His conviction led eventually to the establishment of the Dominican Order in 1216. The order was to live up to the terms of his famous rebuke, "Zeal must be met by zeal, humility by humility, false sanctity by real sanctity, preaching falsehood by preaching truth." However, even St. Dominic managed only a few converts among the Cathari.

In January 1208 the papal legate, Pierre de Castelnau—a Cistercian monk, theologian and canon lawyer—was sent to meet the ruler of the area, Raymond VI, Count of Toulouse. Known for excommunicating noblemen who protected the Cathars, Castelnau excommunicated Raymond for abetting heresy following an allegedly fierce argument during which Raymond supposedly threatened Castelnau with violence. Shortly thereafter, Castelnau was murdered as he returned to Rome, allegedly by a knight in the service of Count Raymond. His body was returned and laid to rest in the Abbey at Saint Gilles.

As soon as he heard of the murder, the Pope ordered the legates to preach a crusade against the Cathars and wrote a letter to Philip Augustus, King of France, appealing for his intervention—or an intervention led by his son, Louis. This was not the first appeal but some see the murder of the legate as a turning point in papal policy. Others claim it as a fortuitous event in allowing the Pope to excite popular opinion and to renew his pleas for intervention in the south. The chronicler of the crusade which followed, Peter of Vaux de Cernay, portrays the sequence of events in such a way that, having failed in his effort to peaceably demonstrate the errors of Catharism, the Pope then called a formal crusade, appointing a series of leaders to head the assault.

The French King refused to lead the crusade himself, and could not spare his son to do so either—despite his victory against John, King of England, there were still pressing issues with Flanders and the empire and the threat of an Angevin revival. Philip did however sanction the participation of some of his more bellicose and ambitious—some might say dangerous—barons, notably Simon de Montfort and Bouchard de Marly. There followed twenty years of war against the Cathars and their allies in the Languedoc: the Albigensian Crusade.

This war pitted the nobles of France against those of the Languedoc. The widespread northern enthusiasm for the Crusade was partially inspired by a papal decree permitting the confiscation of lands owned by Cathars and their supporters. This not only angered the lords of the south but also the French King, who was at least nominally the suzerain of the lords whose lands were now open to despoliation and seizure. Philip Augustus wrote to Pope Innocent in strong terms to point this out—but the Pope did not change his policy. As the Languedoc was supposedly teeming with Cathars and Cathar sympathisers, this made the region a target for northern French noblemen looking to acquire new fiefs. The barons of the north headed south to do battle.

Their first target was the lands of the Trencavel, powerful lords of Carcassonne, Béziers, Albi and the Razes. Little was done to form a regional coalition and the crusading army was able to take Carcassonne, the Trencavel capital, incarcerating Raymond Roger Trencavel in his own citadel where he died within three months; champions of the Occitan cause claimed that he was murdered. Simon de Montfort was granted the Trencavel lands by the Pope and did homage for them to the King of France, thus incurring the enmity of Peter II of Aragon who had held aloof from the conflict, even acting as a mediator at the time of the siege of Carcassonne. The remainder of the first of the two Cathar wars now focused on Simon's attempt to hold on to his gains through winters where he was faced, with only a small force of confederates operating from the main winter camp at Fanjeaux, with the desertion of local lords who had sworn fealty to him out of necessity—and attempts to enlarge his newfound domains in the summer when his forces were greatly augmented by reinforcements from France, Germany and elsewhere.

Summer campaigns saw him not only retake, sometimes with brutal reprisals, what he had lost in the "close" season, but also seek to widen his sphere of operation—and we see him in action in the Aveyron at St. Antonin and on the banks of the Rhone at Beaucaire. Simon's greatest triumph was the victory against superior numbers at the Battle of Muret—a battle which saw not only the defeat of Raymond of Toulouse and his Occitan allies—but also the death of Peter of Aragon—and the effective end of the ambitions of the house of Aragon/Barcelona in the Languedoc. This was in the medium and longer term of much greater significance to the royal house of France than it was to de Montfort—and with the battle of Bouvines was to secure the position of Philip Augustus vis a vis England and the Empire. The Battle of Muret was a massive step in the creation of the unified French kingdom and the country we know today—although Edward III, the Black Prince and Henry V would threaten later to shake these foundations.

The crusader army came under the command, both spiritually and militarily, of the papal legate Arnaud-Amaury, Abbot of Cîteaux. In the first significant engagement of the war, the town of Béziers was besieged on 22 July 1209. The Catholic inhabitants of the city were granted the freedom to leave unharmed, but many refused and opted to stay and fight alongside the Cathars.

The Cathars spent much of 1209 fending off the crusaders. The Béziers army attempted a sortie but was quickly defeated, then pursued by the crusaders back through the gates and into the city. Arnaud-Amaury, the Cistercian abbot-commander, is supposed to have been asked how to tell Cathars from Catholics. His reply, recalled by Caesarius of Heisterbach, a fellow Cistercian, thirty years later was ""Caedite eos. Novit enim Dominus qui sunt eius""—"Kill them all, the Lord will recognise His own". The doors of the church of St Mary Magdalene were broken down and the refugees dragged out and slaughtered. Reportedly at least 7,000 innocent men, women and children were killed there by Catholic forces. Elsewhere in the town, many more thousands were mutilated and killed. Prisoners were blinded, dragged behind horses, and used for target practice. What remained of the city was razed by fire. Arnaud-Amaury wrote to Pope Innocent III, "Today your Holiness, twenty thousand heretics were put to the sword, regardless of rank, age, or sex." "The permanent population of Béziers at that time was then probably no more than 5,000, but local refugees seeking shelter within the city walls could conceivably have increased the number to 20,000."

After the success of his siege of Carcassonne, which followed the Massacre at Béziers in 1209, Simon de Montfort was designated as leader of the Crusader army. Prominent opponents of the Crusaders were Raymond Roger Trencavel, viscount of Carcassonne, and his feudal overlord Peter II, the king of Aragon, who held fiefdoms and had a number of vassals in the region. Peter died fighting against the crusade on 12 September 1213 at the Battle of Muret. Simon de Montfort was killed on 25 June 1218 after maintaining a siege of Toulouse for nine months.

The official war ended in the Treaty of Paris (1229), by which the king of France dispossessed the house of Toulouse of the greater part of its fiefs, and that of the Trencavels (Viscounts of Béziers and Carcassonne) of the whole of their fiefs. The independence of the princes of the Languedoc was at an end. But in spite of the wholesale massacre of Cathars during the war, Catharism was not yet extinguished and Catholic forces would continue to pursue Cathars.

In 1215, the bishops of the Catholic Church met at the Fourth Council of the Lateran under Pope Innocent III; part of the agenda was combating the Cathar heresy.

The Inquisition was established in 1233 to uproot the remaining Cathars. Operating in the south at Toulouse, Albi, Carcassonne and other towns during the whole of the 13th century, and a great part of the 14th, it succeeded in crushing Catharism as a popular movement and driving its remaining adherents underground. Cathars who refused to recant were hanged, or burnt at the stake.

On Friday May 13, 1239, 183 men and women convinced of Catharism were burned at the stake on the orders of Robert le Bougre. Mount Guimar was already denounced as a place of heresy by the letter of the bishop of Liège to Pope Lucius II in 1144. St. Augustine, bishop of Hippo, had expelled from the city a Fortunatus who had fled Africa in 392; he is a Fortunatus who is reported as a monk from Africa and protected by the lord of Widomarum. 

From May 1243 to March 1244, the Cathar fortress of Montségur was besieged by the troops of the seneschal of Carcassonne and the archbishop of Narbonne. On 16 March 1244, a large and symbolically important massacre took place, where over 200 Cathar Perfects were burnt in an enormous pyre at the "prat dels cremats" ("field of the burned") near the foot of the castle. Moreover, the Church decreed lesser chastisements against laymen suspected of sympathy with Cathars, at the 1235 Council of Narbonne.
A popular though as yet unsubstantiated theory holds that a small party of Cathar Perfects escaped from the fortress before the massacre at "prat dels cremats". It is widely held in the Cathar region to this day that the escapees took with them "le trésor cathar". What this treasure consisted of has been a matter of considerable speculation: claims range from sacred Gnostic texts to the Cathars' accumulated wealth, which might have included the Holy Grail (see the Section on Historical Scholarship, below).

Hunted by the Inquisition and deserted by the nobles of their districts, the Cathars became more and more scattered fugitives: meeting surreptitiously in forests and mountain wilds. Later insurrections broke out under the leadership of Roger-Bernard II, Count of Foix, Aimery III of Narbonne and Bernard Délicieux, a Franciscan friar later prosecuted for his adherence to another heretical movement, that of the Spiritual Franciscans at the beginning of the 14th century. But by this time the Inquisition had grown very powerful. Consequently, many presumed to be Cathars were summoned to appear before it. Precise indications of this are found in the registers of the Inquisitors, Bernard of Caux, Jean de St Pierre, Geoffroy d'Ablis, and others. The parfaits it was said only rarely recanted, and hundreds were burnt. Repentant lay believers were punished, but their lives were spared as long as they did not relapse. Having recanted, they were obliged to sew yellow crosses onto their outdoor clothing and to live apart from other Catholics, at least for a while.

After several decades of harassment and re-proselytising, and, perhaps even more important, the systematic destruction of their religious texts, the sect was exhausted and could find no more adepts. The leader of a Cathar revival in the Pyrenean foothills, Peire Autier was captured and executed in April 1310 in Toulouse. After 1330, the records of the Inquisition contain very few proceedings against Cathars. The last known Cathar perfectus in the Languedoc, Guillaume Bélibaste, was executed in the autumn of 1321.

From the mid-12th century onwards, Italian Catharism came under increasing pressure from the Pope and the Inquisition, "spelling the beginning of the end". Other movements, such as the Waldensians and the pantheistic Brethren of the Free Spirit, which suffered persecution in the same area, survived in remote areas and in small numbers into the 14th and 15th centuries. Some Waldensian ideas were absorbed into early Protestant sects, such as the Hussites, Lollards, and the Moravian Church (Herrnhuters of Germany). Cathars were in no way Protestant, and very few if any Protestants consider them as their forerunners (as opposed to groups like Waldensians, Hussites, Lollards and Arnoldists).

After the suppression of Catharism, the descendants of Cathars were at times required to live outside towns and their defences. They thus retained a certain Cathar identity, despite having returned to the Catholic religion.

Any use of the term "Cathar" to refer to people after the suppression of Catharism in the 14th century is a cultural or ancestral reference, and has no religious implication. Nevertheless, interest in the Cathars, their history, legacy and beliefs continues.

The term "Pays Cathare", French meaning "Cathar Country" is used to highlight the Cathar heritage and history of the region where Catharism was traditionally strongest. This area is centred around fortresses such as Montségur and Carcassonne; also the French département of the Aude uses the title "Pays Cathare" in tourist brochures. These areas have ruins from the wars against the Cathars which are still visible today.

Some criticise the promotion of the identity of "Pays Cathare" as an exaggeration for tourist purposes. Many of the promoted Cathar castles were not built by Cathars but by local lords and later many of them were rebuilt and extended for strategic purposes. Good examples of these are the magnificent castles of Queribus and Peyrepertuse which are both perched on the side of precipitous drops on the last folds of the Corbieres mountains. They were for several hundred years frontier fortresses belonging to the French crown and most of what is still there dates from a post-Cathar era. Many consider the County of Foix to be the actual historical centre of Catharism.

In an effort to find the few remaining heretics in and around the village of Montaillou, Jacques Fournier, Bishop of Pamiers, future Pope Benedict XII, had those suspected of heresy interrogated in the presence of scribes who recorded their conversations. The late 13th- to early 14th-century document, discovered in the Vatican archives in the 1960s and edited by Jean Duvernoy, is the basis for Emmanuel Le Roy Ladurie's work "Montaillou: The Promised Land of Error".

The publication of the early scholarly book "Crusade against the Grail" by the young German Otto Rahn in the 1930s rekindled interest in the connection between the Cathars and the Holy Grail, especially in Germany. Rahn was convinced that the 13th-century work "Parzival" by Wolfram von Eschenbach was a veiled account of the Cathars. The philosopher and Nazi government official Alfred Rosenberg speaks favourably of the Cathars in "The Myth of the Twentieth Century".

Academic books in English first appeared at the beginning of the millennium: for example, Malcolm Lambert's "The Cathars" and Malcolm Barber's "The Cathars".

Starting in the 1990s and continuing to the present day, historians like R.I Moore have radically challenged the extent to which Catharism, as an institutionalized religion, actually existed. Building on the work of French historians such as Monique Zerner and Uwe Brunn, Moore's "The War on Heresy" argues that Catharism was "contrived from the resources of [the] well-stocked imaginations" of churchmen, "with occasional reinforcement from miscellaneous and independent manifestations of local anticlericalism or apostolic enthusiasm". In short, Moore claims that the men and women persecuted as Cathars were not the followers of a secret religion imported from the East, instead they were part of a broader spiritual revival taking place in the later twelfth and early thirteenth century. Moore's work is indicative of a larger historiographical trend towards examination of how heresy was constructed by the Church.

The principal legacy of the Cathar movement is in the poems and songs of the Cathar troubadors, though this artistic legacy is only a smaller part of the wider Occitan linguistic and artistic heritage. Recent artistic projects concentrating on the Cathar element in Provençal and troubador art include commercial recording projects by Thomas Binkley, electric hurdy-gurdy artist Valentin Clastrier and his CD Heresie dedicated to the church at Cathars, La Nef, and Jordi Savall.

The Cathars have been depicted or re-interpreted in popular books, video games, and films such as "The Holy Blood and the Holy Grail", "The Bone Clocks", the History Channel tv series "Knightfall," Damsels in Distress "Labyrinth", "The Winter Ghosts", "The Apocalypse Fire", "", Paulo Coelho's Brida, Bernard Cornwell's "The Grail Quest" series and Theodore Roszak's "Flicker".

Catharism, along with other Christian movements including Fraticelli, Waldensianism, and Lollardy, is featured in the grand strategy game "Crusader Kings II", which is notable as being the only Catholic heresy in-game that allows female priests; it also grants the option of absolute cognatic succession laws (such as absolute primogeniture) and the appointment of female generals and councilors.


Notes
Bibliography



</doc>
<doc id="7632" url="https://en.wikipedia.org/wiki?curid=7632" title="Cerebrospinal fluid">
Cerebrospinal fluid

Cerebrospinal fluid (CSF) is a clear, colorless body fluid found in the brain and spinal cord. It is produced by the specialised ependymal cells in the choroid plexuses of the ventricles of the brain, and absorbed in the arachnoid granulations. There is about 125mL of CSF at any one time, and about 500mL is generated every day. CSF acts as a cushion or buffer for the brain, providing basic mechanical and immunological protection to the brain inside the skull. CSF also serves a vital function in cerebral autoregulation of cerebral blood flow.

CSF occupies the subarachnoid space (between the arachnoid mater and the pia mater) and the ventricular system around and inside the brain and spinal cord. It fills the ventricles of the brain, cisterns, and sulci, as well as the central canal of the spinal cord. There is also a connection from the subarachnoid space to the bony labyrinth of the inner ear via the perilymphatic duct where the perilymph is continuous with the cerebrospinal fluid.

A sample of CSF can be taken via lumbar puncture. This can reveal the intracranial pressure, as well as indicate diseases including infections of the brain or its surrounding meninges. Although noted by Hippocrates, it was only in the 18th century that Emanuel Swedenborg is credited with its rediscovery, and as late as 1914 that Harvey W. Cushing demonstrated CSF was secreted by the choroid plexus.

There is about 125-150 mL of CSF at any one time. This CSF circulates within the ventricular system of the brain. The ventricles are a series of cavities filled with CSF. The majority of CSF is produced from within the two lateral ventricles. From here, CSF passes through the interventricular foramina to the third ventricle, then the cerebral aqueduct to the fourth ventricle. From the fourth ventricle, the fluid passes into the subarachnoid space through four openings – the central canal of the spinal cord, the median aperture, and the two lateral apertures. CSF is present within the subarachnoid space, which covers the brain, spinal cord, and stretches below the end of the spinal cord to the sacrum. There is a connection from the subarachnoid space to the bony labyrinth of the inner ear making the cerebrospinal fluid continuous with the perilymph in 93% of people.

CSF moves in a single outward direction from the ventricles, but multidirectionally in the subarachnoid space. Fluid movement is pulsatile, matching the pressure waves generated in blood vessels by the beating of the heart. Some authors dispute this, posing that there is no unidirectional CSF circulation, but cardiac cycle-dependent bi-directional systolic-diastolic to-and-fro cranio-spinal CSF movements.

CSF is derived from blood plasma and is largely similar to it, except that CSF is nearly protein-free compared with plasma and has some different electrolyte levels. Due to the way it is produced, CSF has a higher chloride level than plasma, and an equivalent sodium level.

CSF contains approximately 0.3% plasma proteins, or approximately 15 to 40 mg/dL, depending on sampling site. In general, globular proteins and albumin are in lower concentration in ventricular CSF compared to lumbar or cisternal fluid. This continuous flow into the venous system dilutes the concentration of larger, lipid-insoluble molecules penetrating the brain and CSF. CSF is normally free of red blood cells, and at most contains only a few white blood cells. Any white blood cell count higher than this constitutes pleocytosis.

At around the third week of development, the embryo is a three-layered disc, covered with ectoderm, mesoderm and endoderm. A tube-like formation develops in the midline, called the notochord. The notochord releases extracellular molecules that affect the transformation of the overlying ectoderm into nervous tissue. The neural tube, forming from the ectoderm, contains CSF prior to the development of the choroid plexuses. The open neuropores of the neural tube close after the first month of development, and CSF pressure gradually increases.

As the brain develops, by the fourth week of embryological development three swellings have formed within the embryo around the canal, near where the head will develop. These swellings represent different components of the central nervous system: the prosencephalon, mesencephalon and rhombencephalon. Subarachnoid spaces are first evident around the 32nd day of development near the rhombencephalon; circulation is visible from the 41st day. At this time, the first choroid plexus can be seen, found in the fourth ventricle, although the time at which they first secrete CSF is not yet known.

The developing forebrain surrounds the neural cord. As the forebrain develops, the neural cord within it becomes a ventricle, ultimately forming the lateral ventricles. Along the inner surface of both ventricles, the ventricular wall remains thin, and a choroid plexus develops, producing and releasing CSF. CSF quickly fills the neural canal. Arachnoid villi are formed around the 35th week of development, with aracnhoid granulations noted around the 39th, and continuing developing until 18 months of age.

The subcommissural organ secretes SCO-spondin, which forms Reissner's fiber within CSF assisting movement through the cerebral aqueduct. It is present in early intra-uterine life but disappears during early development.

CSF serves several purposes:


The brain produces roughly 500 mL of cerebrospinal fluid per day, at a rate of about 25 mL an hour. This transcellular fluid is constantly reabsorbed, so that only 125–150 mL is present at any one time.

Most (about two-thirds to 80%) of CSF is produced by the choroid plexus. The choroid plexus is a network of blood vessels present within sections of the four ventricles of the brain. It is present throughout the ventricular system except for the cerebral aqueduct, frontal horn of the lateral ventricle, and occipital horn of the lateral ventricle. CSF is also produced by the single layer of column-shaped ependymal cells which line the ventricles; by the lining surrounding the subarachnoid space; and a small amount directly from the tiny spaces surrounding blood vessels around the brain.

CSF is produced by the choroid plexus in two steps. Firstly, a filtered form of plasma moves from fenestrated capillaries in the choroid plexus into an interstitial space, with movement guided by a difference in pressure between the blood in the capillaries and the interstitial fluid. This fluid then needs to pass through the epithelium cells lining the choroid plexus into the ventricles, an active process requiring the transport of sodium, potassium and chloride that draws water into CSF by creating osmotic pressure. Unlike blood passing from the capillaries into the choroid plexus, the epithelial cells lining the choroid plexus contain tight junctions between cells, which act to prevent most substances flowing freely into CSF.

Water and carbon dioxide from the interstitial fluid diffuse into the epithelial cells. Within these cells, carbonic anhydrase converts the substances into bicarbonate and hydrogen ions. These are exchanged for sodium and chloride on the cell surface facing the interstitium. Sodium, chloride, bicarbonate and potassium are then actively secreted into the ventricular lumen. This creates osmotic pressure and draws water into CSF, facilitated by aquaporins. Chloride, with a negative charge, moves with the positively charged sodium, to maintain electroneutrality. Potassium and bicarbonate are also transported out of CSF. As a result, CSF contains a higher concentration of sodium and chloride than blood plasma, but less potassium, calcium and glucose and protein. Choroid plexuses also secrete growth factors, vitamins B1, 12 C, folate, beta-2 microglobulin, arginine vasopressin and nitrous oxide into CSF. A Na-K-Cl cotransporter and Na/K ATPase found on the surface of the choroid endothelium, appears to play a role in regulating CSF secretion and composition.

Orešković and Klarica hypothesise that CSF is not primarily produced by the choroid plexus, but is being permanently produced inside the entire CSF system, as a consequence of water filtration through the capillary walls into the interstitial fluid of the surrounding brain tissue, regulated by AQP-4.

There are circadian variations in CSF secretion, with the mechanisms not fully understood, but potentially relating to differences in the activation of the autonomic nervous system over the course of the day.

CSF returns to the vascular system by entering the dural venous sinuses via arachnoid granulations. These are outpouchings of the arachnoid mater into the venous sinuses around the brain, with valves to ensure one-way drainage. This occurs because of a pressure difference between the arachnoid mater and venous sinuses. CSF has also been seen to drain into lymphatic vessels, particularly those surrounding the nose via drainage along the olfactory nerve through the cribriform plate. The pathway and extent are currently not known, but may involve CSF flow along some cranial nerves and be more prominent in the neonate. CSF turns over at a rate of three to four times a day. CSF has also been seen to be reabsorbed through the sheathes of cranial and spinal nerve sheathes, and through the ependyma.

The composition and rate of CSF generation are influenced by hormones and the content and pressure of blood and CSF. For example, when CSF pressure is higher, there is less of a pressure difference between the capillary blood in choroid plexuses and CSF, decreasing the rate at which fluids move into the choroid plexus and CSF generation. The autonomic nervous system influences choroid plexus CSF secretion, with activation of the sympathetic nervous system increasing secretion and the parasympathetic nervous system decreasing it. Changes in the pH of the blood can affect the activity of carbonic anhydrase, and some drugs (such as frusemide, acting on the Na-K-Cl cotransporter) have the potential to impact membrane channels.

CSF pressure, as measured by lumbar puncture, is 10–18 cmHO (8–15 mmHg or 1.1–2 kPa) with the patient lying on the side and 20–30 cmHO (16–24 mmHg or 2.1–3.2 kPa) with the patient sitting up. In newborns, CSF pressure ranges from 8 to 10 cmHO (4.4–7.3 mmHg or 0.78–0.98 kPa). Most variations are due to coughing or internal compression of jugular veins in the neck. When lying down, the CSF pressure as estimated by lumbar puncture is similar to the intracranial pressure.

Hydrocephalus is an abnormal accumulation of CSF in the ventricles of the brain. Hydrocephalus can occur because of obstruction of the passage of CSF, such as from an infection, injury, mass, or congenital abnormality. Hydrocephalus without obstruction associated with normal CSF pressure may also occur. Symptoms can include problems with gait and coordination, urinary incontinence, nausea and vomiting, and progressively impaired thinking. In infants, hydrocephalus can cause an enlarged head, as the bones of the skull have not yet fused, seizures, irritability and drowsiness. A CT scan or MRI scan may reveal enlargement of one or both lateral ventricles, or causative masses or lesions, and lumbar puncture may be used to demonstrate and in some circumstances relieve high intracranial pressure. Hydrocephalus is usually treated through the insertion of a shunt, which diverts fluid to another part of the body, such as a ventriculo-peritoneal shunt.

Idiopathic intracranial hypertension is a condition of unknown cause characterised by a rise in CSF pressure. It is associated with headaches, double vision, difficulties seeing, and a swollen optic disc. It can occur in association with the use of Vitamin A and tetracycline antibiotics, or without any identifiable cause at all, particularly in younger obese women. Management may include ceasing any known causes, a carbonic anhydrase inhibitor such as acetazolamide, repeated drainage via lumbar puncture, or the insertion of a shunt such as a ventriculoperitoneal shunt.

CSF can leak from the dura as a result of different causes such as physical trauma or a lumbar puncture, or from no known cause when it is termed spontaneous cerebrospinal fluid leak. It is usually associated with intracranial hypotension: low CSF pressure. It can cause headaches, made worse by standing, moving and coughing, as the low CSF pressure causes the brain to "sag" downwards and put pressure on its lower structures. If a leak is identified, a beta-2 transferrin test of the leaking fluid, when positive, is highly specific and sensitive for the detection for CSF leakage. Medical imaging such as CT scans and MRI scans can be used to investigate for a presumed CSF leak when no obvious leak is found but low CSF pressure is identified. Caffeine, given either orally or intravenously, often offers symptomatic relief. Treatment of an identified leak may include injection of a person's blood into the epidural space (an epidural blood patch), spinal surgery, or fibrin glue.

CSF can be tested for the diagnosis of a variety of neurological diseases, usually obtained by a procedure called lumbar puncture. Lumbar puncture is carried out under sterile conditions by inserting a needle into the subarachnoid space, usually between the third and fourth lumbar vertebrae. CSF is extracted through the needle, and tested. About one third of people experience a headache after lumbar puncture, and pain or discomfort at the needle entry site is common. Rarer complications may include bruising, meningitis or ongoing post lumbar-puncture leakage of CSF.

Testing often including observing the colour of the fluid, measuring CSF pressure, and counting and identifying white and red blood cells within the fluid; measuring protein and glucose levels; and culturing the fluid. The presence of red blood cells and xanthochromia may indicate subarachnoid hemorrhage; whereas central nervous system infections such as meningitis, may be indicated by elevated white blood cell levels. A CSF culture may yield the microorganism that has caused the infection, or PCR may be used to identify a viral cause. Investigations to the total type and nature of proteins reveal point to specific diseases, including multiple sclerosis, paraneoplastic syndromes, systemic lupus erythematosus, neurosarcoidosis, cerebral angiitis; and specific antibodies such as Aquaporin 4 may be tested for to assist in the diagnosis of autoimmune conditions. A lumbar puncture that drains CSF may also be used as part of treatment for some conditions, including idiopathic intracranial hypertension and normal pressure hydrocephalus.

Lumbar puncture can also be performed to measure the intracranial pressure, which might be increased in certain types of hydrocephalus. However, a lumbar puncture should never be performed if increased intracranial pressure is suspected due to certain situations such as a tumour, because it can lead to fatal brain herniation.

Some anaesthetics and chemotherapy are injected intrathecally into the subarachnoid space, where they spread around CSF, meaning substances that cannot cross the blood-brain barrier can still be active throughout the central nervous system. Baricity refers to the density of a substance compared to the density of human cerebrospinal fluid and is used in general anesthesia to determine the manner in which a particular drug will spread in the intrathecal space.

Various comments by ancient physicians have been read as referring to CSF. Hippocrates discussed "water" surrounding the brain when describing congenital hydrocephalus, and Galen referred to "excremental liquid" in the ventricles of the brain, which he believed was purged into the nose. But for some 16 intervening centuries of ongoing anatomical study, CSF remained unmentioned in the literature. This is perhaps because of the prevailing autopsy technique, which involved cutting off the head, thereby removing evidence of CSF before the brain was examined.

The modern rediscovery of CSF is now credited to Emanuel Swedenborg. In a manuscript written between 1741 and 1744, unpublished in his lifetime, Swedenborg referred to CSF as "spirituous lymph" secreted from the roof of the fourth ventricle down to the medulla oblongata and spinal cord. This manuscript was eventually published in translation in 1887.

Albrecht von Haller, a Swiss physician and physiologist, made note in his 1747 book on physiology that the "water" in the brain was secreted into the ventricles and absorbed in the veins, and when secreted in excess, could lead to hydrocephalus. Francois Magendie studied the properties of CSF by vivisection. He discovered the foramen Magendie, the opening in the roof of the fourth ventricle, but mistakenly believed that CSF was secreted by the pia mater.

Thomas Willis (noted as the discoverer of the circle of Willis) made note of the fact that the consistency of CSF is altered in meningitis. In 1869 Gustav Schwalbe proposed that CSF drainage could occur via lymphatic vessels. 

In 1891, W. Essex Wynter began treating tubercular meningitis by tapping the subarachnoid space, and Heinrich Quincke began to popularize lumbar puncture, which he advocated for both diagnostic and therapeutic purposes. In 1912, a neurologist William Mestrezat gave the first accurate description of the chemical composition of CSF. In 1914, Harvey W. Cushing published conclusive evidence that CSF is secreted by the choroid plexus.

During phylogenesis, CSF is present within the neuraxis before it circulates. The CSF of Teleostei fish is contained within the ventricles of the brains, but not in a nonexistent subarachnoid space. In mammals, where a subarachnoid space is present, CSF is present in it. Absorption of CSF is seen in amniotes and more complex species, and as species become progressively more complex, the system of absorption becomes progressively more enhanced, and the role of spinal epidural veins in absorption plays a progressively smaller and smaller role.

The amount of cerebrospinal fluid varies by size and species. In humans and other mammals, cerebrospinal fluid, produced, circulating, and reabsorbed in a similar manner to humans, and with a similar function, turns over at a rate of 3–5 times a day. Problems with CSF circulation leading to hydrocephalus occur in other animals.




</doc>
<doc id="7633" url="https://en.wikipedia.org/wiki?curid=7633" title="Cordial">
Cordial

Cordial may refer to:

Food and drink:

Other uses:


</doc>
<doc id="7635" url="https://en.wikipedia.org/wiki?curid=7635" title="Charles F. Hockett">
Charles F. Hockett

Charles Francis Hockett (January 17, 1916 – November 3, 2000) was an American linguist who developed many influential ideas in American structuralist linguistics. He represents the post-Bloomfieldian phase of structuralism often referred to as "distributionalism" or "taxonomic structuralism". His academic career spanned over half a century at Cornell and Rice universities.

At the age of 16, Hockett enrolled at Ohio State University in Columbus, Ohio where he received a Bachelor of Arts and Master of Arts in ancient history. While enrolled at Ohio State, Hockett became interested in the work of Leonard Bloomfield, a leading figure in the field of structural linguistics. Hockett continued his education at Yale University where he studied anthropology and linguistics and received his PhD in anthropology in 1939. While studying at Yale, Hockett studied with several other influential linguists such as Edward Sapir, George P. Murdock, and Benjamin Whorf. Hockett's dissertation was based on his fieldwork in Potawatomi; his paper on Potawatomi syntax was published in "Language" in 1939. In 1948 his dissertation was published as a series in the International Journal of American Linguistics. Following fieldwork in Kickapoo and Michoacán, Mexico, Hockett did two years of postdoctoral study with Leonard Bloomfield in Chicago and Michigan.

Hockett began his teaching career in 1946 as an assistant professor of linguistics in the Division of Modern Languages at Cornell University where he was responsible for directing the Chinese language program. In 1957, Hockett became a member of Cornell's anthropology department and continued to teach anthropology and linguistics until he retired to emeritus status in 1982. In 1986, he took up an adjunct post at Rice University in Houston, Texas, where he remained active until his death in 2000.

Charles Hockett held membership among many academic institutions such as the National Academy of Sciences the American Academy of Arts and Sciences, and the Society of Fellows at Harvard University. He served as president of both the Linguistic Society of America and the Linguistic Association of Canada and the United States.

In addition to making many contributions to the field of structural linguistics, Hockett also considered such things as Whorfian Theory, jokes, the nature of writing systems, slips of the tongue, and animal communication and their relativeness to speech.

Outside the realm of linguistics and anthropology, Hockett practiced musical performance and composition. Hockett composed a full-length opera called "The Love of Doña Rosita" which was based on a play by Federico García Lorca and premiered at Ithaca College by the Ithaca Opera.

Hockett and his wife Shirley were vital leaders in the development of the Cayuga Chamber Orchestra in Ithaca, New York. In appreciation of the Hocketts' hard work and dedication to the Ithaca community, Ithaca College established the Charles F. Hockett Music Scholarship, the Shirley and Chas Hockett Chamber Music Concert Series, and the Hockett Family Recital Hall.

In his "Note on Structure" he argues that linguistics can be seen as a game and as a science. A linguist as player has a freedom for experimentation on all the utterances of a language, but no criterion to compare his analysis with other linguists. Late in his career, he was known for his stinging criticism of Chomskyan linguistics.

After carefully examining the generative school's proposed innovations in Linguistics, Hockett decided that this approach was of little value. His book "The State of the Art" outlined his criticisms of the generative approach. In his paraphrase a key principle of the Chomskyean paradgim is that there are an infinite number of grammatical sentences in any particular language. 
The grammar of a language is a finite system that characterizes an infinite set of (well-formed) sentences. More specifically, the grammar of a language is a "well-defined system" by definition not more powerful than a universal Turing machine (and, in fact, surely a great deal weaker).

The crux of Hockett's rebuttal is that the set of grammatical sentences in a language is not infinite, but rather ill-defined. Hockett proposes that "no physical system is well-defined".

Later in "Where the tongue slips, there slip I" he writes as follows. 

It is currently fashionable to assume that, underlying the actual more or less bumbling speech behavior of any human being, there is a subtle and complicated but determinate linguistic "competence": a sentence-generating device whose design can only be roughly guessed at by any techniques so far available to us. This point of view makes linguistics very hard and very erudite, so that anyone who actually does discover facts about underlying "competence" is entitled to considerable kudos.

Within this popular frame of reference, a theory of "performance" -- of the "generation of speech" -- must take more or less the following form. If a sentence is to be uttered aloud, or even thought silently to oneself, it must first be built by the internal "competence" of the speaker, the functioning of which is by definition such that the sentence will be legal ("grammatical") in every respect. But that is not enough; the sentence as thus constructed must then be "performed", either overtly so that others may hear it, or covertly so that it is perceived only by the speaker himself. It is in this second step that blunders may appear. That which is generated by the speaker's internal "competence"is what the speaker "intends to say," and is the only real concern of linguistics: blunders in actually performed speech are instructions from elsewhere. Just if there are no such intrusions is what is performed an instance of "smooth speech".

I believe this view is unmitigated nonsense, unsupported by any empirical evidence of any sort. In its place, I propose the following.

"All" speech, smooth as well as blunderful, can be and must be accounted for essentially in terms of the three mechanisms we have listed: analogy, blending, and editing. An individual's language, at a given moment, is a set of habits--that is, of analogies, where different analogies are in conflict, one may appear as a constraint on the working of another. Speech actualizes habits--and changes the habits as it does so. Speech reflects awareness of norms; but norms are themselves entirely a matter of analogy (that is, of habit), not some different kind of thing. 

Despite his criticisms, Hockett always expressed gratitude to the generative school for seeing real problems in the preexisting approaches. 

There are many situations in which bracketing does not serve to disambiguate. As already noted, words that belong together cannot always be spoken together, and when they are not, bracketing is difficult or impossible. In the 1950s this drove some grammarians to drink and other to transformations, but both are only anodynes, not answers

One of Hockett’s most important contributions was his development of the design-feature approach to comparative linguistics. He attempted to distinguish the similarities and differences among animal communication systems and human language.

Hockett initially developed seven features, which were published in the 1959 paper “Animal ‘Languages’ and Human Language.” However, after many revisions, he settled on 13 design-features in the "Scientific American " "The Origin of Speech."

Hockett argued that while every communication system has some of the 13 design features, only human, spoken language has all 13 features. In turn, that differentiates human spoken language from animal communication and other human communication systems such as written language.


While Hockett believed that all communication systems, animal and human alike, share many of these features, only human language contains all 13 design features. Additionally, traditional transmission, and duality of patterning are key to human language.



Foraging honey bees communicate with other members of their hive when they have discovered a relevant source of pollen, nectar, or water. In an effort to convey information about the location and the distance of such resources, honeybees participate in a particular figure-eight dance known as the waggle dance.

In Hockett's "The Origin of Speech", he determined that the honeybee communication system of the waggle dance holds the following design features:


Gibbons are small apes in the family Hylobatidae. While they share the same kingdom, phylum, class, and order of humans and are relatively close to man, Hockett distinguishes between the gibbon communication system and human language by noting that gibbons are devoid of the last four design features.

Gibbons possess the first nine design features, but do not possess the last four (displacement, productivity, traditional transmission, and duality of patterning).


In a report published in 1968 with anthropologist and scientist Stuart A. Altmann, Hockett derived three more Design Features, bringing the total to 16. These are the additional three:


Cognitive scientist and linguist at the University of Sussex Larry Trask offered an alternative term and definition for number 14, Prevarication:

There has since been one more Feature added to the list, by Dr. William Taft Stuart, a director of the Undergraduate Studies program at the University of Maryland: College Park’s Anthropology school, part of the College of Behavioral and Social Sciences. His “extra” Feature is:

This follows the definition of Grammar and Syntax, as given by Merriam-Webster’s Dictionary:

Additionally, Dr. Stuart defends his postulation with references to famous linguist Noam Chomsky and University of New York psychologist Gary Marcus. Chomsky theorized that humans are unique in the animal world because of their ability to utilize Design Feature 5: Total Feedback, or recursive grammar. This includes being able to correct oneself and insert explanatory or even non sequitur statements into a sentence, without breaking stride, and keeping proper grammar throughout.

While there have been studies attempting to disprove Chomsky, Marcus states that, "An intriguing possibility is that the capacity to recognize recursion might be found only in species that can acquire new patterns of vocalization, for example, songbirds, humans and perhaps some cetaceans." This is in response to a study performed by psychologist Timothy Gentner of the University of California at San Diego. Gentner’s study found that starling songbirds use recursive grammar to identify “odd” statements within a given “song.” However, the study does not necessarily debunk Chomsky’s observation because it has not yet been proven that songbirds have the semantic ability to generalize from patterns.

There is also thought that symbolic thought is necessary for grammar-based speech, and thus Homo Erectus and all preceding “humans” would have been unable to comprehend modern speech. Rather, their utterances would have been halting and even quite confusing to us, 
today.

The University of Oxford: Phonetics Laboratory Faculty of Linguistics, Philology and Phonetics published the following chart, detailing how Hockett's (and Altmann's) Design Features fit into other forms of communication, in animals:





</doc>
<doc id="7638" url="https://en.wikipedia.org/wiki?curid=7638" title="Consilience">
Consilience

In science and history, consilience (also convergence of evidence or concordance of evidence) refers to the principle that evidence from independent, unrelated sources can "converge" on strong conclusions. That is, when multiple sources of evidence are in agreement, the conclusion can be very strong even when none of the individual sources of evidence is significantly so on its own. Most established scientific knowledge is supported by a convergence of evidence: if not, the evidence is comparatively weak, and there will not likely be a strong scientific consensus.

The principle is based on the unity of knowledge; measuring the same result by several different methods should lead to the same answer. For example, it should not matter whether one measures the distance between the Giza pyramid complex by laser rangefinding, by satellite imaging, or with a meter stick – in all three cases, the answer should be approximately the same. For the same reason, different dating methods in geochronology should concur, a result in chemistry should not contradict a result in geology, etc.

The word "consilience" was originally coined as the phrase "consilience of inductions" by William Whewell ("consilience" refers to a "jumping together" of knowledge). The word comes from Latin "com-" "together" and "-siliens" "jumping" (as in resilience).

Consilience requires the use of independent methods of measurement, meaning that the methods have few shared characteristics. That is, the mechanism by which the measurement is made is different; each method is dependent on an unrelated natural phenomenon. For example, the accuracy of laser rangefinding measurements is based on the scientific understanding of lasers, while satellite pictures and meter sticks rely on different phenomena. Because the methods are independent, when one of several methods is in error, it is very unlikely to be in error in the "same way" as any of the other methods, and a difference between the measurements will be observed. If the scientific understanding of the properties of lasers were inaccurate, then the laser measurement would be inaccurate but the others would not.

As a result, when several different methods agree, this is strong evidence that "none" of the methods are in error and the conclusion is correct. This is because of a greatly reduced likelihood of errors: for a consensus estimate from multiple measurements to be wrong, the errors would have to be similar for all samples and all methods of measurement, which is extremely unlikely. Random errors will tend to cancel out as more measurements are made, due to regression to the mean; systematic errors will be detected by differences between the measurements (and will also tend to cancel out since the direction of the error will still be random). This is how scientific theories reach high confidence – over time, they build up a large degree of evidence which converges on the same conclusion.

When results from different strong methods do appear to conflict, this is treated as a serious problem to be reconciled. For example, in the 19th century, the Sun appeared to be no more than 20 million years old, but the Earth appeared to be no less than 300 million years (resolved by the discovery of nuclear fusion and radioactivity, and the theory of quantum mechanics); or current attempts to resolve theoretical differences between quantum mechanics and general relativity.

Because of consilience, the strength of evidence for any particular conclusion is related to how many independent methods are supporting the conclusion, as well as how different these methods are. Those techniques with the fewest (or no) shared characteristics provide the strongest consilience and result in the strongest conclusions. This also means that confidence is usually strongest when considering evidence from different fields, because the techniques are usually very different.

For example, the theory of evolution is supported by a convergence of evidence from genetics, molecular biology, paleontology, geology, biogeography, comparative anatomy, comparative physiology, and many other fields. In fact, the evidence within each of these fields is itself a convergence providing evidence for the theory. (As a result, to disprove evolution, most or all of these independent lines of evidence would have to be found to be in error.) The strength of the evidence, considered together as a whole, results in the strong scientific consensus that the theory is correct. In a similar way, evidence about the history of the universe is drawn from astronomy, astrophysics, planetary geology, and physics.

Finding similar conclusions from multiple independent methods is also evidence for the reliability of the methods themselves, because consilience eliminates the possibility of all potential errors that do not affect all the methods equally. This is also used for the validation of new techniques through comparison with the consilient ones. If only partial consilience is observed, this allows for the detection of errors in methodology; any weaknesses in one technique can be compensated for by the strengths of the others. Alternatively, if using more than one or two techniques for every experiment is infeasible, some of the benefits of consilience may still be obtained if it is well-established that these techniques usually give the same result.

Consilience is important across all of science, including the social sciences, and is often used as an argument for scientific realism by philosophers of science. Each branch of science studies a subset of reality that depends on factors studied in other branches. Atomic physics underlies the workings of chemistry, which studies emergent properties that in turn are the basis of biology. Psychology is not separate from the study of properties emergent from the interaction of neurons and synapses. Sociology, economics, and anthropology are each, in turn, studies of properties emergent from the interaction of countless individual humans. The concept that all the different areas of research are studying one real, existing universe is an apparent explanation of why scientific knowledge determined in one field of inquiry has often helped in understanding other fields.

Consilience does not forbid deviations: in fact, since not all experiments are perfect, some deviations from established knowledge are expected. However, when the convergence is strong enough, then new evidence inconsistent with the previous conclusion is not usually enough to outweigh that convergence. Without an equally strong convergence on the new result, the weight of evidence will still favor the established result. This means that the new evidence is most likely to be wrong.

Science denialism (for example, AIDS denialism) is often based on a misunderstanding of this property of consilience. A denier may promote small gaps not yet accounted for by the consilient evidence, or small amounts of evidence contradicting a conclusion without accounting for the pre-existing strength resulting from consilience. More generally, to insist that all evidence converge precisely with no deviations would be naïve falsificationism, equivalent to considering a single contrary result to falsify a theory when another explanation, such as equipment malfunction or misinterpretation of results, is much more likely.

Historical evidence also converges in an analogous way. For example: if five ancient historians, none of whom knew each other, all claim that Julius Caesar seized power in Rome in 49 BCE, this is strong evidence in favor of that event occurring even if each individual historian is only partially reliable. By contrast, if the same historian had made the same claim five times in five different places (and no other types of evidence were available), the claim is much weaker because it originates from a single source. The evidence from the ancient historians could also converge with evidence from other fields, such as archaeology: for example, evidence that many senators fled Rome at the time, that the battles of Caesar’s civil war occurred, and so forth.

Consilience has also been discussed in reference to Holocaust denial. 
That is, individually the evidence may underdetermine the conclusion, but together they overdetermine it. A similar way to state this is that to ask for one "particular" piece of evidence in favor of a conclusion is a flawed question.

In addition to the sciences, consilience can be important to the arts, ethics and religion. Both artists and scientists have identified the importance of biology in the process of artistic innovation.

Consilience has its roots in the ancient Greek concept of an intrinsic orderliness that governs our cosmos, inherently comprehensible by logical process, a vision at odds with mystical views in many cultures that surrounded the Hellenes. The rational view was recovered during the high Middle Ages, separated from theology during the Renaissance and found its apogee in the Age of Enlightenment.

Whewell’s definition was that:
More recent descriptions include:

Although the concept of consilience in Whewell's sense was widely discussed by philosophers of science, the term was unfamiliar to the broader public until the end of the 20th century, when it was revived in "Consilience: The Unity of Knowledge," a 1998 book by the humanist biologist Edward Osborne Wilson, as an attempt to bridge the culture gap between the sciences and the humanities that was the subject of C. P. Snow's "The Two Cultures and the Scientific Revolution" (1959).

Wilson held that with the rise of the modern sciences, the sense of unity gradually was lost in the increasing fragmentation and specialization of knowledge in the last two centuries. He asserted that the sciences, humanities, and arts have a common goal: to give a purpose to understanding the details, to lend to all inquirers "a conviction, far deeper than a mere working proposition, that the world is orderly and can be explained by a small number of natural laws." Wilson's concept is a much broader notion of consilience than that of Whewell, who was merely pointing out that generalizations invented to account for one set of phenomena often account for others as well.

A parallel view lies in the term universology, which literally means "the science of the universe." Universology was first promoted for the study of the interconnecting principles and truths of all domains of knowledge by Stephen Pearl Andrews, a 19th-century utopian futurist and anarchist.




</doc>
<doc id="7642" url="https://en.wikipedia.org/wiki?curid=7642" title="Clarence Brown">
Clarence Brown

Clarence Leon Brown (May 10, 1890 – August 17, 1987) was an American film director.

Born in Clinton, Massachusetts, the son of Larkin Harry Brown (1866-1942) a cotton manufacturer and Katherine Ann Brown (née Gaw) (1865-1954), Brown moved to Tennessee when he was 11 years old. He attended Knoxville High School and the University of Tennessee, both in Knoxville, Tennessee, graduating from the university at the age of 19 with two degrees in engineering. An early fascination in automobiles led Brown to a job with the Stevens-Duryea Company, then to his own Brown Motor Car Company in Alabama. He later abandoned the car dealership after developing an interest in motion pictures around 1913. He was hired by the Peerless Studio at Fort Lee, New Jersey, and became an assistant to the French-born director Maurice Tourneur.

After serving in World War I, Brown was given his first co-directing credit (with Tourneur) for "The Great Redeemer" (1920). Later that year, he directed a major portion of "The Last of the Mohicans" after Tourneur was injured in a fall.

Brown moved to Universal in 1924, and then to MGM, where he stayed until the mid-1950s. At MGM he was one of the main directors of their female stars; he directed Joan Crawford six times and Greta Garbo seven.

He was nominated five times (see below) for the Academy Award as a director and once as a producer, but he never received an Oscar. However, he won Best Foreign Film for "Anna Karenina", starring Garbo at the 1935 Venice International Film Festival.

Brown's films gained a total of 38 Academy Award nominations and earned nine Oscars. Brown himself received six Academy Award nominations and in 1949, he won the British Academy Award for the film version of William Faulkner's "Intruder in the Dust.

In 1957, Brown was awarded The George Eastman Award, given by George Eastman House for distinguished contribution to the art of film. Brown retired a wealthy man due to his real estate investments, but refused to watch new movies, as he feared they might cause him to restart his career.
The Clarence Brown Theater, on the campus of the University of Tennessee, is named in his honor. He is tied with Robert Altman and Alfred Hitchcock for the most Academy Award nominations for best director without a single win.

Clarence Brown was married five times. Firstly in 1913 to Paul Herndon Pratt (1894-1966) which lasted from 1913 until their divorce in 1920. The couple produced a daughter Adrienne Brown (1917-2013). Secondly, Brown married Ona Wilson (1884-1960) which lasted from 1922 until their divorce in 1927. Thirdly, Clarence Brown married Mona Maris (1903-1991) from 1929 until their divorce in 1931. Fourthly, Clarence Brown married Alice Joyce (1890-1955) from 1933 until their divorce in 1945. Lastly, Clarence Brown married his last ever wife Marian Spies (1910-2004) from 1946 which lasted until his death in 1987.

Brown died at the Saint John's Health Center in Santa Monica, California from kidney failure on August 17, 1987, at the age of 97. He is interred at Forest Lawn Memorial Park in Glendale, California.

On February 8, 1960, Brown received a star on the Hollywood Walk of Fame at 1752 Vine Street, for his contributions to the motion pictures industry




</doc>
<doc id="7643" url="https://en.wikipedia.org/wiki?curid=7643" title="Conciliation">
Conciliation

Conciliation is an alternative dispute resolution (ADR) process whereby the parties to a dispute use a conciliator, who meets with the parties both separately and together in an attempt to resolve their differences. They do this by lowering tensions, improving communications, interpreting issues, encouraging parties to explore potential solutions and assisting parties in finding a mutually acceptable outcome.

Conciliation differs from arbitration in that the conciliation process, in and of itself, has no legal standing, and the conciliator usually has no authority to seek evidence or call witnesses, usually writes no decision, and makes no award.

Conciliation differs from mediation in that in conciliation, often the parties are in need of restoring or repairing a relationship, either personal or business.

A conciliator assists each of the parties to independently develop a list of all of their objectives (the outcomes which they desire to obtain from the conciliation). The conciliator then has each of the parties separately prioritize their own list from most to least important. He/She then goes back and forth between the parties and encourages them to "give" on the objectives one at a time, starting with the least important and working toward the most important for each party in turn. The parties rarely place the same priorities on all objectives, and usually have some objectives that are not listed by the other party. Thus the conciliator can quickly build a string of successes and help the parties create an atmosphere of trust which the conciliator can continue to develop.

Most successful conciliators are highly skilled negotiators. Some conciliators operate under the auspices of any one of several non-governmental entities, and for governmental agencies such as the Federal Mediation and Conciliation Service in the United States.

Historical conciliation is an applied conflict resolution approach that utilizes historical narratives to positively transform relations between societies in conflicts. Historical conciliation can utilize many different methodologies, including mediation, sustained dialogue, apologies, acknowledgement, support of public commemoration activities, and public diplomacy.

Historical conciliation is not an excavation of objective facts. The point of facilitating historical questions is not to discover all the facts in regard to who was right or wrong. Rather, the objective is to discover the complexity, ambiguity, and emotions surrounding both dominant and non-dominant cultural and individual narratives of history. It is also not a rewriting of history. The goal is not to create a combined narrative that everyone agrees upon. Instead, the aim is to create room for critical thinking and more inclusive understanding of the past and conceptions of “the other.”

Conflicts that are addressed through historical conciliation have their roots in conflicting identities of the people involved. Whether the identity at stake is their ethnicity, religion or culture, it requires a comprehensive approach that takes people’s needs, hopes, fears, and concerns into account.

Japanese law makes extensive use of in civil disputes. The most common forms are civil conciliation and domestic conciliation, both of which are managed under the auspice of the court system by one judge and two non-judge "conciliators."

Civil conciliation is a form of dispute resolution for small lawsuits, and provides a simpler and cheaper alternative to litigation. Depending on the nature of the case, non-judge experts (doctors, appraisers, actuaries, and so on) may be called by the court as conciliators to help decide the case.

Domestic conciliation is most commonly used to handle contentious divorces, but may apply to other domestic disputes such as the annulment of a marriage or acknowledgment of paternity. Parties in such cases are required to undergo conciliation proceedings and may only bring their case to court once conciliation has failed.


</doc>
<doc id="7645" url="https://en.wikipedia.org/wiki?curid=7645" title="Cyclone (programming language)">
Cyclone (programming language)

The Cyclone programming language is intended to be a safe dialect of the C language. Cyclone is designed to avoid buffer overflows and other vulnerabilities that are possible in C programs, without losing the power and convenience of C as a tool for system programming.

Cyclone development was started as a joint project of AT&T Labs Research and Greg Morrisett's group at Cornell in 2001. Version 1.0 was released on May 8, 2006.

Cyclone attempts to avoid some of the common pitfalls of C, while still maintaining its look and performance. To this end, Cyclone places the following limits on programs:

To maintain the tool set that C programmers are used to, Cyclone provides the following extensions:

For a better high-level introduction to Cyclone, the reasoning behind Cyclone and the source of these lists, see this paper.

Cyclone looks, in general, much like C, but it should be viewed as a C-like language.

Cyclone implements three kinds of pointer:
The purpose of introducing these new pointer types is to avoid common problems when using pointers. Take for instance a function, called codice_17 that takes a pointer to an int:

Although the person who wrote the function codice_17 could have inserted codice_1 checks, let us assume that for performance reasons they did not. Calling codice_20 will result in undefined behavior (typically, although not necessarily, a SIGSEGV signal being sent to the application). To avoid such problems, Cyclone introduces the codice_14 pointer type, which can never be codice_1. Thus, the "safe" version of codice_17 would be:

This tells the Cyclone compiler that the argument to codice_17 should never be codice_1, avoiding the aforementioned undefined behavior. The simple change of codice_13 to codice_14 saves the programmer from having to write codice_1 checks and the operating system from having to trap codice_1 pointer dereferences. This extra limit, however, can be a rather large stumbling block for most C programmers, who are used to being able to manipulate their pointers directly with arithmetic. Although this is desirable, it can lead to buffer overflows and other "off-by-one"-style mistakes. To avoid this, the codice_16 pointer type is delimited by a known bound, the size of the array. Although this adds overhead due to the extra information stored about the pointer, it improves safety and security. Take for instance a simple (and naïve) codice_31 function, written in C:

This function assumes that the string being passed in is terminated by NULL (codice_32). However, what would happen if codice_33 were passed to this string? This is perfectly legal in C, yet would cause codice_31 to iterate through memory not necessarily associated with the string codice_35. There are functions, such as codice_36 which can be used to avoid such problems, but these functions are not standard with every implementation of ANSI C. The Cyclone version of codice_31 is not so different from the C version:

Here, codice_31 bounds itself by the length of the array passed to it, thus not going over the actual length. Each of the kinds of pointer type can be safely cast to each of the others, and arrays and strings are automatically cast to codice_16 by the compiler. (Casting from codice_16 to codice_13 invokes a bounds check, and casting from codice_16 to codice_14 invokes both a codice_1 check and a bounds check. Casting from codice_13 to codice_16 results in no checks whatsoever; the resulting codice_16 pointer has a size of 1.)

Consider the following code, in C:

Function codice_48 allocates an array of chars codice_49 on the stack and returns a pointer to the start of codice_49. However the memory used on the stack for codice_49 is deallocated when the function returns, so the returned value cannot be used safely outside of the function. While gcc and other compilers will warn about such code, the following will typically compile without warnings:

gcc can produce warnings for such code as a side-effect of option -O2 or -O3, but there are no guarantees that all such errors will be detected.
Cyclone does regional analysis of each segment of code, preventing dangling pointers, such as the one returned from this version of codice_48. All of the local variables in a given scope are considered to be part of the same region, separate from the heap or any other local region. Thus, when analyzing codice_48, the Cyclone compiler would see that codice_54 is a pointer into the local stack, and would report an error.




Presentations:


</doc>
<doc id="7646" url="https://en.wikipedia.org/wiki?curid=7646" title="Cognitivism">
Cognitivism

Cognitivism may refer to:



</doc>
<doc id="7647" url="https://en.wikipedia.org/wiki?curid=7647" title="Counter (digital)">
Counter (digital)

In digital logic and computing, a counter is a device which stores (and sometimes displays) the number of times a particular event or process has occurred, often in relationship to a clock signal. The most common type is a sequential digital logic circuit with an input line called the "clock" and multiple output lines. The values on the output lines represent a number in the binary or BCD number system. Each pulse applied to the clock input increments or decrements the number in the counter.

A counter circuit is usually constructed of a number of flip-flops connected in cascade. Counters are a very widely used component in digital circuits, and are manufactured as separate integrated circuits and also incorporated as parts of larger integrated circuits.

In electronics, counters can be implemented quite easily using register-type circuits such as the flip-flop, and a wide variety of classified into:

Each is useful for different applications. Usually, counter circuits are digital in nature, and count in natural binary. Many types of counter circuits are available as digital building blocks, for example a number of chips in the 4500 series implement different counters.

Occasionally there are advantages to using a counting sequence other than the natural binary sequence—such as the binary coded decimal counter, a linear-feedback shift register counter, or a Gray-code counter.

Counters are useful for digital clocks and timers, and in oven timers, VCR clocks, etc.

An asynchronous (ripple) counter is a single d-type flip-flop, with its J (data) input fed from its own inverted output. This circuit can store one bit, and hence can count from zero to one before it overflows (starts over from 0). This counter will increment once for every clock cycle and takes two clock cycles to overflow, so every cycle it will alternate between a transition from 0 to 1 and a transition from 1 to 0. Notice that this creates a new clock with a 50% duty cycle at exactly half the frequency of the input clock. If this output is then used as the clock signal for a similarly arranged D flip-flop (remembering to invert the output to the input), one will get another 1 bit counter that counts half as fast. Putting them together yields a two-bit counter:
You can continue to add additional flip-flops, always inverting the output to its own input, and using the output from the previous flip-flop as the clock signal. The result is called a ripple counter, which can count to where "n" is the number of bits (flip-flop stages) in the counter. Ripple counters suffer from unstable outputs as the overflows "ripple" from stage to stage, but they do find frequent application as dividers for clock signals, where the instantaneous count is unimportant, but the division ratio overall is (to clarify this, a 1-bit counter is exactly equivalent to a divide by two circuit; the output frequency is exactly half that of the input when fed with a regular train of clock pulses).

The use of flip-flop outputs as clocks leads to timing skew between the count data bits, making this ripple technique incompatible with normal synchronous circuit design styles.

In synchronous counters, the clock inputs of all the flip-flops are connected together and are triggered by the input pulses. Thus, all the flip-flops change state simultaneously (in parallel). The circuit below is a 4-bit synchronous counter. The J and K inputs of FF0 are connected to HIGH. FF1 has its J and K inputs connected to the output of FF0, and the J and K inputs of FF2 are connected to the output of an AND gate that is fed by the outputs of FF0 and FF1.
A simple way of implementing the logic for each bit of an ascending counter (which is what is depicted in the adjacent image) is for each bit to toggle when all of the less significant bits are at a logic high state. For example, bit 1 toggles when bit 0 is logic high; bit 2 toggles when both bit 1 and bit 0 are logic high; bit 3 toggles when bit 2, bit 1 and bit 0 are all high; and so on.

Synchronous counters can also be implemented with hardware finite-state machines, which are more complex but allow for smoother, more stable transitions.

A decade counter is one that counts in decimal digits, rather than binary. A decade counter may have each (that is, it may count in binary-coded decimal, as the 7490 integrated circuit did) or other binary encodings. "A decade counter is a binary counter that is designed to count to 1010 (decimal 10). An ordinary four-stage counter can be easily modified to a decade counter by adding a NAND gate as in the schematic to the right. Notice that FF2 and FF4 provide the inputs to the NAND gate. The NAND gate outputs are connected to the CLR input of each of the FFs." 
A decade counter is one that counts in decimal digits, rather than binary. It counts from 0 to 9 and then resets to zero. The counter output can be set to zero by pulsing the reset line low. The count then increments on each clock pulse until it reaches 1001 (decimal 9). When it increments to 1010 (decimal 10) both inputs of the NAND gate go high. The result is that the NAND output goes low, and resets the counter to zero. D going low can be a CARRY OUT signal, indicating that there has been a count of ten.

A ring counter is a circular shift register which is initiated such that only one of its flip-flops is the state one while others are in their zero states.

A ring counter is a shift register (a cascade connection of flip-flops) with the output of the last one connected to the input of the first, that is, in a ring. Typically, a pattern consisting of a single bit is circulated so the state repeats every n clock cycles if n flip-flops are used.

A Johnson counter (or switch-tail ring counter, twisted ring counter, walking ring counter, or Möbius counter) is a modified ring counter, where the output from the last stage is inverted and fed back as input to the first stage. The register cycles through a sequence of bit-patterns, whose length is equal to twice the length of the shift register, continuing indefinitely. These counters find specialist applications, including those similar to the decade counter, digital-to-analog conversion, etc. They can be implemented easily using D- or JK-type flip-flops.in this process of the counter

In computability theory, a counter is considered a type of memory. A counter stores a single natural number (initially zero) and can be arbitrarily long. A counter is usually considered in conjunction with a finite-state machine (FSM), which can perform the following operations on the counter:


The following machines are listed in order of power, with each one being strictly more powerful than the one below it:

For the first and last, it doesn't matter whether the FSM is a deterministic finite automaton or a nondeterministic finite automaton. They have the same power. The first two and the last one are levels of the Chomsky hierarchy.

The first machine, an FSM plus two counters, is equivalent in power to a Turing machine. See the article on counter machines for a proof.

A web counter or hit counter is a computer software program that indicates the number of visitors, or hits, a particular webpage has received. Once set up, these counters will be incremented by one every time the web page is accessed in a web browser.

The number is usually displayed as an inline digital image or in plain text or on a physical counter such as a mechanical counter. Images may be presented in a variety of fonts, or styles; the classic example is the wheels of an odometer.

"Web counter" was popular in the mid to late 1990s and early 2000s, later replaced by more detailed and complete web traffic measures.

Many automation systems use PC and laptops to monitor different parameters of machines and production data. Counters may count parameters such as the number of pieces produced, the production batch number, and measurements of the amounts of material used.

Long before electronics became common, mechanical devices were used to count events. These are known as tally counters. They typically consist of a series of disks mounted on an axle, with the digits zero through nine marked on their edge. The right most disk moves one increment with each event. Each disk except the left-most has a protrusion that, after the completion of one revolution, moves the next disk to the left one increment. Such counters were used as odometers for bicycles and cars and in tape recorders, fuel dispensers, in production machinery as well as in other machinery. One of the largest manufacturers was the Veeder-Root company, and their name was often used for this type of counter.

Hand held tally counters are used mainly for stocktaking and for counting people attending events.

Electromechanical counters were used to accumulate totals in tabulating machines that pioneered the data processing industry.



</doc>
<doc id="7649" url="https://en.wikipedia.org/wiki?curid=7649" title="Cervical mucus method">
Cervical mucus method

Cervical mucus method may refer to a specific method of fertility awareness or natural family planning:



</doc>
<doc id="7651" url="https://en.wikipedia.org/wiki?curid=7651" title="Coleridge (disambiguation)">
Coleridge (disambiguation)

Samuel Taylor Coleridge (1772–1834) was an English poet, literary critic, philosopher and theologian.

Coleridge may also refer to:




</doc>
<doc id="7655" url="https://en.wikipedia.org/wiki?curid=7655" title="Clay Mathematics Institute">
Clay Mathematics Institute

The Clay Mathematics Institute (CMI) is a private, non-profit foundation, based in Peterborough, New Hampshire, United States. CMI's scientific activities are managed from the President's office in Oxford, United Kingdom. The institute is "dedicated to increasing and disseminating mathematical knowledge." It gives out various awards and sponsorships to promising mathematicians. The institute was founded in 1998 through the sponsorship of Boston businessman Landon T. Clay. Harvard mathematician Arthur Jaffe was the first president of CMI. 

While the institute is best known for its Millennium Prize Problems, it carries out a wide range of activities, including a postdoctoral program (ten Clay Research Fellows are supported currently), conferences, workshops, and summer schools.

The institute is run according to a standard structure comprising a scientific advisory committee that decides on grant-awarding and research proposals, and a board of directors that oversees and approves the committee's decisions. , the board is made up of members of the Clay family, whereas the advisory committee is composed of leading authorities in mathematics, namely Sir Andrew Wiles, Michael Hopkins, Carlos Kenig, Andrei Okounkov, and Simon Donaldson. Nicholas Woodhouse is the current president of CMI.

The institute is best known for establishing the Millennium Prize Problems on May 24, 2000. These seven problems are considered by CMI to be "important classic questions that have resisted solution over the years." For each problem, the first person to solve it will be awarded $1,000,000 by the CMI. In announcing the prize, CMI drew a parallel to Hilbert's problems, which were proposed in 1900, and had a substantial impact on 20th century mathematics. Of the initial 23 Hilbert problems, most of which have been solved, only the Riemann hypothesis (formulated in 1859) is included in the seven Millennium Prize Problems.

For each problem, the Institute had a professional mathematician write up an official statement of the problem, which will be the main standard by which a given solution will be measured against. The seven problems are:


Some of the mathematicians who were involved in the selection and presentation of the seven problems were Atiyah, Bombieri, Connes, Deligne, Fefferman, Milnor, Mumford, Wiles, and Witten.

In recognition of major breakthroughs in mathematical research, the institute has an annual prize — the Clay Research Award. Its recipients to date are Ian Agol, Manindra Agrawal, Yves Benoist, Manjul Bhargava, Danny Calegari, Alain Connes, Nils Dencker, Alex Eskin, David Gabai, Ben Green, Larry Guth, Christopher Hacon, Richard S. Hamilton, Michael Harris, Jeremy Kahn, Nets Katz, Laurent Lafforgue, Gérard Laumon, Vladimir Markovic, James McKernan, Maryam Mirzakhani, Ngô Bảo Châu, Rahul Pandharipande, Jonathan Pila, Jean-François Quint, Peter Scholze, Oded Schramm, Stanislav Smirnov, Terence Tao, Clifford Taubes, Richard Taylor, Claire Voisin, Jean-Loup Waldspurger, Andrew Wiles, and Edward Witten.

Besides the Millennium Prize Problems, the Clay Mathematics Institute supports mathematics via the awarding of research fellowships (which range from two to five years, and are aimed at younger mathematicians), as well as shorter-term scholarships for programs, individual research, and book writing. The institute also has a yearly Clay Research Award, recognizing major breakthroughs in mathematical research. Finally, the institute organizes a number of summer schools, conferences, workshops, public lectures, and outreach activities aimed primarily at junior mathematicians (from the high school to postdoctoral level). CMI publications are available in PDF form at most six months after they appear in print.

The episode of the television series "Elementary" entitled "Solve for X" (Season 2, Episode 2) mentions the Clay Mathematics Institute in reference to their involvement in the P versus NP problem.




</doc>
<doc id="7659" url="https://en.wikipedia.org/wiki?curid=7659" title="Cerebral arteriovenous malformation">
Cerebral arteriovenous malformation

A cerebral arteriovenous malformation (cerebral AVM, CAVM, cAVM) is an abnormal connection between the arteries and veins in the brain—specifically, an arteriovenous malformation in the cerebrum.

The most frequently observed problems, related to an AVM, are headaches and seizures, backaches, neckaches and eventual nausea, as the coagulated blood makes its way down to be dissolved in the individual's spinal fluid. It is supposed that 15% of the population, at detection, have no symptoms at all. Other common symptoms are a pulsing noise in the head, progressive weakness and numbness and vision changes as well as debilitating, excruciating pain.

In serious cases, the blood vessels rupture and there is bleeding within the brain (intracranial hemorrhage). Nevertheless, in more than half of patients with AVM, hemorrhage is the first symptom. Symptoms due to bleeding include loss of consciousness, sudden and severe headache, nausea, vomiting, incontinence, and blurred vision, amongst others. Impairments caused by local brain tissue damage on the bleed site are also possible, including seizure, one-sided weakness (hemiparesis), a loss of touch sensation on one side of the body and deficits in language processing (aphasia). Ruptured AVMs are responsible for considerable mortality and morbidity.

AVMs in certain critical locations may stop the circulation of the cerebrospinal fluid, causing accumulation of the fluid within the skull and giving rise to a clinical condition called hydrocephalus. A stiff neck can occur as the result of increased pressure within the skull and irritation of the meninges.

AVMs are an abnormal connection between the arteries and veins in the human brain. Arteriovenous malformations are most commonly of prenatal origin. The cause of AVMs remains unknown. In a normal brain oxygen enriched blood from the heart travels in sequence through smaller blood vessels going from arteries, to arterioles and then capillaries. Oxygen is removed in the latter vessel to be used by the brain. After the oxygen is removed blood reaches venules and later veins which will take it back to the heart and lungs. On the other hand, when there is an AVM blood goes directly from arteries to veins through the abnormal vessels disrupting the normal circulation of blood.

An AVM diagnosis is established by neuroimaging studies after a complete neurological and physical examination. Three main techniques are used to visualize the brain and search for AVM: computed tomography (CT), magnetic resonance imaging (MRI), and cerebral angiography. A CT scan of the head is usually performed first when the subject is symptomatic. It can suggest the approximate site of the bleed. MRI is more sensitive than CT in the diagnosis of AVMs and provides better information about the exact location of the malformation. More detailed pictures of the tangle of blood vessels that compose an AVM can be obtained by using radioactive agents injected into the blood stream. If a CT is used in conjunctiangiogram, this is called a computerized tomography angiogram; while, if MRI is used it is called magnetic resonance angiogram. The best images of an AVM are obtained through cerebral angiography. This procedure involves using a catheter, threaded through an artery up to the head, to deliver a contrast agent into the AVM. As the contrast agent flows through the AVM structure, a sequence of X-ray images are obtained.

A common method of grading cerebral AVMs is the Spetzler-Martin (SM) grade. This system was designed to assess the patient's risk of neurological deficit after open surgical resection (surgical morbidity), based on characteristics of the AVM itself. Based on this system, AVMs may be classified as grades 1 - 5. This system was not intended to characterize risk of hemorrhage.
"Eloquent cortex" is a name used by neurologists for areas of cortex that, if removed will result in loss of sensory processing or linguistic ability, minor paralysis, or paralysis.

The risk of post-surgical neurological deficit (difficulty with language, motor weakness, vision loss) increases with increasing Spetzler-Martin grade.

A limitation of the Spetzler-Martin Grading system is that it does not include the following factors: Patient age, hemorrhage, diffuseness of nidus, and arterial supply. In 2010 a new supplemented Spetzler-Martin system (SM-supp, Lawton-Young) was devised adding these variables to the SM system. Under this new system AVMs are classified from grades 1 - 10. It has since been determined to have greater predictive accuracy that Spetzler-Martin grades alone.
Treatment depends on the location and size of the AVM and whether there is bleeding or not.

The treatment in the case of sudden bleeding is focused on restoration of vital function. Anticonvulsant medications such as phenytoin are often used to control seizure; medications or procedures may be employed to relieve intracranial pressure. Eventually, curative treatment may be required to prevent recurrent hemorrhage. However, any type of intervention may also carry a risk of creating a neurological deficit.

Preventive treatment of as yet unruptured brain AVMs has been controversial, as several studies suggested favorable long-term outcome for unruptured AVM patients not undergoing intervention. The NIH-funded longitudinal ARUBA study ("A Randomized trial of Unruptured Brain AVMs) compares the risk of stroke and death in patients with preventive AVM eradication versus those followed without intervention. Interim results suggest that fewer strokes occur as long as patients with unruptured AVM do not undergo intervention. Because of the higher than expected event rate in the interventional arm of the ARUBA study, NIH/NINDS stopped patient enrollment in April 2013, while continuing to follow all participants to determine whether the difference in stroke and death in the two arms changes over time.

Surgical elimination of the blood vessels involved is the preferred curative treatment for many types of AVM. Surgery is performed by a neurosurgeon who temporarily removes part of the skull (craniotomy), separates the AVM from surrounding brain tissue, and resects the abnormal vessels. While surgery can result in an immediate, complete removal of the AVM, risks exist depending on the size and the location of the malformation. The AVM must be resected en bloc, for partial resection will likely cause severe hemorrhage. The preferred treatment of Spetzler-Martin grade 1 and 2 AVMs in young, healthy patients is surgical resection due to the relatively small risk of neurological damage compared to the high lifetime risk of hemorrhage. Grade 3 AVMs may or may not be amenable to surgery. Grade 4 and 5 AVMs are not usually surgically treated.

Radiosurgery has been widely used on small AVMs with considerable success. The Gamma Knife is an apparatus used to precisely apply a controlled radiation dosage to the volume of the brain occupied by the AVM. While this treatment does not require an incision and craniotomy (with their own inherent risks), three or more years may pass before the complete effects are known, during which time patients are at risk of bleeding. Complete obliteration of the AVM may or may not occur after several years, and repeat treatment may be needed. Radiosurgery is itself not without risk. In one large study, nine percent of patients had transient neurological symptoms, including headache, after radiosurgery for AVM. However, most symptoms resolved, and the long-term rate of neurological symptoms was 3.8%.

Embolization is performed by interventional neuroradiologists and the occlusion of blood vessels most commonly is obtained with Ethylene-vinyl alcohol copolymer (Onyx) or N-butyl cyanoacrylate (NBCA). These substances are introduced by a radiographically guided catheter, and block vessels responsible for blood flow into the AVM. Embolization is frequently used as an adjunct to either surgery or radiation treatment. Embolization reduces the size of the AVM and during surgery it reduces the risk of bleeding. However, embolization alone may completely obliterate some AVMs. In high flow intranidal fistulas balloons can also be used to reduce the flow so that embolization can be done safely.

The main risk is intracranial hemorrhage. This risk is difficult to quantify since many patients with asymptomatic AVMs will never come to medical attention. Small AVMs tend to bleed more often than do larger ones, the opposite of cerebral aneurysms. If a rupture or bleeding incident occurs, the blood may penetrate either into the brain tissue (cerebral hemorrhage) or into the subarachnoid space, which is located between the sheaths (meninges) surrounding the brain (subarachnoid hemorrhage). Bleeding may also extend into the ventricular system (intraventricular hemorrhage). Cerebral hemorrhage appears to be most common.

One long-term study (mean follow up greater than 20 years) of over 150 symptomatic AVMs (either presenting with bleeding or seizures) found the risk of cerebral hemorrhage to be approximately 4% per year, slightly higher than the 2-3% seen in other studies. A simple, rough approximation of a patient's lifetime bleeding risk is 105 - (patient age in years), assuming a 3% bleed risk annually. For example, a healthy 30-year-old patient would have approximately a 75% lifetime risk of at least one bleeding event. Ruptured AVMs are a significant source or morbidity and mortality; post rupture, as many as 29% of patients will die, and only 55% will be able to live independently.

The annual new detection rate incidence of AVMs is approximately 1 per 100,000 a year. The point prevalence in adults is approximately 18 per 100,000. AVMs are more common in males than females, although in females pregnancy may start or worsen symptoms due the increase in blood flow and volume it usually brings. There is a significant preponderance (15-20%) of AVM in patients with hereditary hemorrhagic telangiectasia (Osler-Weber-Rendu syndrome).

No randomized, controlled clinical trial has established a survival benefit for treating patients (either with open surgery or radiosurgery) with AVMs that have not yet bled.



</doc>
<doc id="7660" url="https://en.wikipedia.org/wiki?curid=7660" title="Comparative method">
Comparative method

In linguistics, the comparative method is a technique for studying the development of languages by performing a feature-by-feature comparison of two or more languages with common descent from a shared ancestor, in order to extrapolate back to infer the properties of that ancestor. The comparative method may be contrasted with the method of internal reconstruction, in which the internal development of a single language is inferred by the analysis of features within that language. Ordinarily both methods are used together to reconstruct prehistoric phases of languages, to fill in gaps in the historical record of a language, to discover the development of phonological, morphological, and other linguistic systems, and to confirm or refute hypothesised relationships between languages.

The comparative method was developed over the 19th century. Key contributions were made by the Danish scholars Rasmus Rask and Karl Verner and the German scholar Jacob Grimm. The first linguist to offer reconstructed forms from a proto-language was August Schleicher, in his "Compendium der vergleichenden Grammatik der indogermanischen Sprachen", originally published in 1861. Here is Schleicher's explanation of why he offered reconstructed forms:
In the present work an attempt is made to set forth the inferred Indo-European original language side by side with its really existent derived languages. Besides the advantages offered by such a plan, in setting immediately before the eyes of the student the final results of the investigation in a more concrete form, and thereby rendering easier his insight into the nature of particular Indo-European languages, there is, I think, another of no less importance gained by it, namely that it shows the baselessness of the assumption that the non-Indian Indo-European languages were derived from Old-Indian (Sanskrit).

The comparative method aims to prove that two or more historically attested languages descend from a single proto-language by comparing lists of cognate terms. From them, regular sound correspondences between the languages are established, and a sequence of regular sound changes can then be postulated, which allows the reconstruction of a proto-language. Relation is deemed certain only if at least a partial reconstruction of the common ancestor is feasible, and if regular sound correspondences can be established—with chance similarities ruled out.

"Descent" is defined as transmission across the generations: children learn a language from the parents' generation and after being influenced by their peers transmit it to the next generation, and so on. For example, a continuous chain of speakers across the centuries links Vulgar Latin to all of its modern descendants.

Two languages are "genetically related" if they descended from the same ancestor language. For example, Italian and French both come from Latin and therefore belong to the same family, the Romance languages. Having a large component of vocabulary from a certain origin is not sufficient to establish relatedness: for example, as a result of heavy borrowing from Arabic into Persian, Modern Persian in fact takes more of its vocabulary from Arabic than from its direct ancestor, Proto-Indo-Iranian, but Persian remains a member of the Indo-Iranian family and is not considered "related" to Arabic.

However, it is possible for languages to have different degrees of relatedness. English, for example, is related both to German and to Russian, but is more closely related to the former than to the latter. Although all three languages share a common ancestor, Proto-Indo-European, English and German also share a more recent common ancestor, Proto-Germanic, while Russian does not. Therefore, English and German are considered to belong to a different subgroup, the Germanic languages.

"Shared retentions" from the parent language are not sufficient evidence of a sub-group. For example, German and Russian both retain from Proto-Indo-European a contrast between the dative case and the accusative case, which English has lost. However, this similarity between German and Russian is not evidence that German is more closely related to Russian than to English; it only means that the "innovation" in question—the loss of the accusative/dative distinction—happened more recently in English than the divergence of English from German. The division of related languages into sub-groups is more certainly accomplished by finding "shared linguistic innovations" differentiating them from the parent language, rather than shared features retained from the parent language.

Languages have been compared since antiquity. For example, in the 1st century BC the Romans were aware of the similarities between Greek and Latin, which they explained mythologically, as the result of Rome being a Greek colony speaking a debased dialect. In the 9th or 10th century AD, Yehuda Ibn Quraysh compared the phonology and morphology of Hebrew, Aramaic, and Arabic, but attributed this resemblance to the Biblical story of Babel, with Abraham, Isaac and Joseph retaining Adam's language, with other languages at various removes becoming more altered from the original Hebrew.

In publications of 1647 and 1654, Marcus van Boxhorn first described a rigid methodology for historical linguistic comparisons and proposed the existence of an Indo-European proto-language (which he called "Scythian") unrelated to Hebrew, but ancestral to Germanic, Greek, Romance, Persian, Sanskrit, Slavic, Celtic and Baltic languages. The Scythian theory was further developed by Andreas Jäger (1686) and William Wotton (1713), who made early forays to reconstruct this primitive common language. In 1710 and 1723 Lambert ten Kate first formulated the regularity of sound laws, introducing among others, the term root vowel.

Another early systematic attempt to prove the relationship between two languages on the basis of similarity of grammar and lexicon was made by the Hungarian János Sajnovics in 1770, when he attempted to demonstrate the relationship between Sami and Hungarian (work that was later extended to the whole Finno-Ugric language family in 1799 by his countryman Samuel Gyarmathi), But the origin of modern historical linguistics is often traced back to Sir William Jones, an English philologist living in India, who in 1786 made his famous 
The Sanscrit language, whatever be its antiquity, is of a wonderful structure; more perfect than the Greek, more copious than the Latin, and more exquisitely refined than either, yet bearing to both of them a stronger affinity, both in the roots of verbs and the forms of grammar, than could possibly have been produced by accident; so strong indeed, that no philologer could examine them all three, without believing them to have sprung from some common source, which, perhaps, no longer exists. There is a similar reason, though not quite so forcible, for supposing that both the Gothick and the Celtick, though blended with a very different idiom, had the same origin with the Sanscrit; and the old Persian might be added to the same family.

The comparative method developed out of attempts to reconstruct the proto-language mentioned by Jones, which he did not name, but which subsequent linguists labelled Proto-Indo-European (PIE). The first professional comparison between the Indo-European languages known then was made by the German linguist Franz Bopp in 1816. Though he did not attempt a reconstruction, he demonstrated that Greek, Latin and Sanskrit shared a common structure and a common lexicon. Friedrich Schlegel in 1808 first stated the importance of using the eldest possible form of a language when trying to prove its relationships; in 1818, Rasmus Christian Rask developed the principle of regular sound-changes to explain his observations of similarities between individual words in the Germanic languages and their cognates in Greek and Jacob Grimm—better known for his "Fairy Tales"—in "Deutsche Grammatik" (published 1819–1837 in four volumes) made use of the comparative method in attempting to show the development of the Germanic languages from a common origin, the first systematic study of diachronic language change.

Both Rask and Grimm were unable to explain apparent exceptions to the sound laws that they had discovered. Although Hermann Grassmann explained one of these anomalies with the publication of Grassmann's law in 1862, Karl Verner made a methodological breakthrough in 1875 when he identified a pattern now known as Verner's law, the first sound-law based on comparative evidence showing that a phonological change in one phoneme could depend on other factors within the same word (such as the neighbouring phonemes and the position of the accent), now called "conditioning environments".

Similar discoveries made by the "Junggrammatiker" (usually translated as "Neogrammarians") at the University of Leipzig in the late 19th century led them to conclude that all sound changes were ultimately regular, resulting in the famous statement by Karl Brugmann and Hermann Osthoff in 1878 that "sound laws have no exceptions". This idea is fundamental to the modern comparative method, since the method necessarily assumes regular correspondences between sounds in related languages, and consequently regular sound changes from the proto-language. This "Neogrammarian Hypothesis" led to application of the comparative method to reconstruct Proto-Indo-European, with Indo-European being at that time by far the most well-studied language family. Linguists working with other families soon followed suit, and the comparative method quickly became the established method for uncovering linguistic relationships.

There is no fixed set of steps to be followed in the application of the comparative method, but some steps are suggested by Lyle Campbell and Terry Crowley, both authors of introductory texts in historical linguistics. The abbreviated summary below is based on their concepts of how to proceed.

This step involves making lists of words that are likely cognates among the languages being compared. If there is a regularly recurring match between the phonetic structure of basic words with similar meanings a genetic kinship can probably be established. For example, looking at the Polynesian family linguists might come up with a list similar to the following (a list actually used by them would be much longer):

Borrowings or false cognates can skew or obscure the correct data. For example, English "taboo" () is like the six Polynesian forms due to borrowing from Tongan into English, and not because of a genetic similarity. This problem can usually be overcome by using basic vocabulary—such as kinship terms, numbers, body parts, pronouns, and other basic terms. Nonetheless, even basic vocabulary can be sometimes borrowed. Finnish, for example, borrowed the word for "mother", "äiti", from Gothic "aiþei".; English borrowed the pronouns "they", "them", and "their(s)" from Norse; and Thai (along with various other East Asian languages) borrowed its numbers from Chinese. An extreme case is represented by Pirahã, a Muran language of South America, which, it is controversially claimed, borrowed all its pronouns from Nheengatu.

The next step involves determining the regular sound-correspondences exhibited by the lists of potential cognates. For example, in the Polynesian data above, it is apparent that words that contain "t" in most of the languages listed have cognates in Hawaiian with "k" in the same position. This is visible in multiple cognate sets: the words glossed as 'one', 'three', 'man', and 'taboo' all show this relationship. This situation is termed a "regular correspondence" between "k" in Hawaiian and "t" in the other Polynesian languages. Similarly, in those data a regular correspondence can be seen between Hawaiian and Rapanui "h", Tongan and Samoan "f", Maori "ɸ", and Rarotongan "ʔ".

Mere phonetic similarity, as between English "day" and Latin "dies" (both with the same meaning), has no probative value. English initial "d-" does not "regularly" match —it is not possible to assemble a large set of English and Latin non-borrowed cognates such that English "d" repeatedly and consistently corresponds to Latin "d" at the beginning of a word—and whatever sporadic matches can be observed are due either to chance (as in the above example) or to borrowing (for example, Latin "diabolus" and English "devil"—both ultimately of Greek origin). English and Latin "do" exhibit a regular correspondence of "t-" : "d-" (where the notation "A : B" means "A corresponds to B"); for example,

If there are many regular correspondence sets of this kind (the more the better), then a common origin becomes a virtual certainty, particularly if some of the correspondences are non-trivial or unusual.

During the late 18th to late 19th century, two major developments improved the method's effectiveness.

First, it was found that many sound changes are conditioned by a specific "context". For example, in both Greek and Sanskrit, an aspirated stop evolved into an unaspirated one, but only if a second aspirate occurred later in the same word; this is Grassmann's law, first described for Sanskrit by Sanskrit grammarian Pāṇini and promulgated by Hermann Grassmann in 1863.

Second, it was found that sometimes sound changes occurred in contexts that were later lost. For instance, in Sanskrit velars ("k"-like sounds) were replaced by palatals ("ch"-like sounds) whenever the following vowel was "*i" or "*e". Subsequent to this change, all instances of "*e" were replaced by "a". The situation would have been unreconstructable, had not the original distribution of "e" and "a" been recoverable from the evidence of other Indo-European languages. For instance, the Latin suffix "que", "and", preserves the original "*e" vowel that caused the consonant shift in Sanskrit:

Verner's Law, discovered by Karl Verner 1875, provides a similar case: the voicing of consonants in Germanic languages underwent a change that was determined by the position of the old Indo-European accent. Following the change, the accent shifted to initial position. Verner solved the puzzle by comparing the Germanic voicing pattern with Greek and Sanskrit accent patterns.

This stage of the comparative method, therefore, involves examining the correspondence sets discovered in step 2 and seeing which of them apply only in certain contexts. If two (or more) sets apply in complementary distribution, they can be assumed to reflect a single original phoneme: "some sound changes, particularly conditioned sound changes, can result in a proto-sound being associated with more than one correspondence set".

For example, the following potential cognate list can be established for Romance languages, which descend from Latin:

They evidence two correspondence sets, "k : k" and "k : :

Since French "" only occurs before "a" where the other languages also have "a", while French "k" occurs elsewhere, the difference is due to different environments (being before an "a" conditions the change) and the sets are complementary. They can therefore be assumed to reflect a single proto-phoneme (in this case "*k", spelled |c| in Latin). The original Latin words are "corpus", "crudus", "catena" and "captiare", all with an initial k-sound. If more evidence along these lines were given, one might conclude an alteration of the original k took place because of a different environment.

A more complex case involves consonant clusters in Proto-Algonquian. The Algonquianist Leonard Bloomfield used the reflexes of the clusters in four of the daughter languages to reconstruct the following correspondence sets:

Although all five correspondence sets overlap with one another in various places, they are not in complementary distribution, and so Bloomfield recognized that a different cluster must be reconstructed for each set; his reconstructions were, respectively, "*hk", "*xk", "*čk" (=), "*šk" (=), and "çk" (where "x" and "ç" are arbitrary symbols, not attempts to guess the phonetic value of the proto-phonemes).

Typology assists in deciding what reconstruction best fits the data. For example, the voicing of voiceless stops between vowels is common, but not the devoicing of voiced stops in that environment. If a correspondence "-t-" : "-d-" between vowels is found in two languages, the proto-phoneme is more likely to be "*-t-", with a development to the voiced form in the second language. The opposite reconstruction would represent a rare type.

However, unusual sound changes do occur. The Proto-Indo-European word for "two", for example, is reconstructed as "*dwō", which is reflected in Classical Armenian as "erku". Several other cognates demonstrate a regular change "*dw-" → "erk-" in Armenian. Similarly, in Bearlake, a dialect of the Athabaskan language of Slavey, there has been a sound change of Proto-Athabaskan "*ts" → Bearlake '. It is very unlikely that "*dw-" changed directly into "erk-" and "*ts" into ', but instead they probably went through several intermediate steps to arrive at the later forms. It is not phonetic similarity which matters when utilizing the comparative method, but regular sound correspondences.

By the principle of economy, the reconstruction of a proto-phoneme should require as few sound changes as possible to arrive at the modern reflexes in the daughter languages. For example, Algonquian languages exhibit the following correspondence set:

The simplest reconstruction for this set would be either "*m" or "*b". Both "*m" → "b" and "*b" → "m" are likely. Because "m" occurs in five of the languages, and "b" in only one, if "*b" is reconstructed, then it is necessary to assume five separate changes of "*b" → "m", whereas if "*m" is reconstructed, it is only necessary to assume a single change of "*m" → "b". "*m" would be most economical. (This argument assumes that the languages other than Arapaho are at least partly independent of each other. If they all formed a common subgroup, the development "*b" → "m" would only have to be assumed to have occurred once.)

In the final step, the linguist checks to see how the proto-phonemes fit the known typological constraints. For example, a hypothetical system,
has only one voiced stop, "*b", and although it has an alveolar and a velar nasal, "*n" and "*ŋ", there is no corresponding labial nasal. However, languages generally (though not always) tend to maintain symmetry in their phonemic inventories. In this case, the linguist might attempt to investigate the possibilities that what was earlier reconstructed as "*b" is in fact "*m", or that the "*n" and "*ŋ" are in fact "*d" and "*g".

Even a symmetrical system can be typologically suspicious. For example, the traditional Proto-Indo-European stop inventory is:

An earlier voiceless aspirated row was removed on grounds of insufficient evidence. Since the mid-20th century, a number of linguists have argued that this phonology is implausible; that it is extremely unlikely for a language to have a voiced aspirated (breathy voice) series without a corresponding voiceless aspirated series. Thomas Gamkrelidze and Vyacheslav Ivanov provided a potential solution, arguing that the series traditionally reconstructed as plain voiced should in fact be reconstructed as glottalized—either implosive or ejective . The plain voiceless and voiced aspirated series would thus be replaced by just voiceless and voiced, with aspiration being a non-distinctive quality of both. This example of the application of linguistic typology to linguistic reconstruction has become known as the Glottalic Theory. It has a large number of proponents but is not generally accepted. As an alternative, the voiceless aspirated row was restored.

The reconstruction of proto-sounds logically precedes the reconstruction of grammatical morphemes (word-forming affixes and inflectional endings), patterns of declension and conjugation, and so on. The full reconstruction of an unrecorded protolanguage is an open-ended task.

The limitations of the comparative method were recognized by the very linguists who developed it, but it is still seen as a valuable tool. In the case of Indo-European, the method seemed to at least partially validate the centuries-old search for an Ursprache, the original language. These others were presumed ordered in a family tree, becoming the tree model of the neogrammarians.

The archaeologists followed suit, attempting to find archaeological evidence of a culture or cultures that could be presumed to have spoken a proto-language, such as Vere Gordon Childe's "The Aryans: a study of Indo-European origins", 1926. Childe was a philologist turned archaeologist. These views culminated in the "Siedlungsarchaologie", or "settlement-archaeology", of Gustaf Kossinna, becoming known as "Kossinna's Law". He asserted that cultures represent ethnic groups, including their languages. It was rejected as a law in the post–World War II era. The fall of Kossinna's Law removed the temporal and spatial framework previously applied to many proto-languages. Fox concludes:
The Comparative Method "as such" is not, in fact, historical; it provides evidence of linguistic relationships to which we may give a historical interpretation. ...[Our increased knowledge about the historical processes involved] has probably made historical linguists less prone to equate the idealizations required by the method with historical reality. ...Provided we keep [the interpretation of the results and the method itself] apart, the Comparative Method can continue to be used in the reconstruction of earlier stages of languages.

Proto-languages can be verified in many historical instances, such as Latin. Although no longer a law, settlement-archaeology is known to be essentially valid for some cultures that straddle history and prehistory, such as the Celtic Iron Age (mainly Celtic) and Mycenaean civilization (mainly Greek). None of these models can be or have been completely rejected, and yet none alone are sufficient.

The foundation of the comparative method, and of comparative linguistics in general, is the Neogrammarians' fundamental assumption that "sound laws have no exceptions". When it was initially proposed, critics of the Neogrammarians proposed an alternate position, summarized by the maxim "each word has its own history". Several types of change do in fact alter words in non-regular ways. Unless identified, they may hide or distort laws and cause false perceptions of relationship.

All languages borrow words from other languages in various contexts. They are likely to have followed the laws of the languages from which they were borrowed rather than the laws of the borrowing language.

Borrowing on a larger scale occurs in areal diffusion, when features are adopted by contiguous languages over a geographical area. The borrowing may be phonological, morphological or lexical. A false proto-language over the area may be reconstructed for them or may be taken to be a third language serving as a source of diffused features.

Several areal features and other influences may converge to form a sprachbund, a wider region sharing features that appear to be related but are diffusional. For instance, the Mainland Southeast Asia linguistic area suggested several false classifications of such languages as Chinese, Thai and Vietnamese before it was recognized.

Sporadic changes, such as irregular inflections, compounding, and abbreviation, do not follow any laws. For example, the Spanish words "palabra" ('word'), "peligro" ('danger') and "milagro" ('miracle') should have been "parabla", "periglo", "miraglo" by regular sound changes from the Latin "parabŏla", "perīcŭlum" and "mīrācŭlum", but the "r" and "l" changed places by sporadic metathesis.

Analogy is the sporadic change of a feature to be like another feature in the same or a different language. It may affect a single word or be generalized to an entire class of features, such as a verb paradigm. For example, the Russian word for "nine", by regular sound changes from Proto-Slavic, should have been , but is in fact . It is believed that the initial ' changed to ' under influence of the word for "ten" in Russian, .

Students of contemporary language changes, such as William Labov, note that even a systematic sound change is at first applied in an unsystematic fashion, with the percentage of its occurrence in a person's speech dependent on various social factors. The sound change gradually spreads, a process known as lexical diffusion. While not invalidating the Neogrammarians' axiom that "sound laws have no exceptions", their gradual application shows that they do not always apply to all lexical items at the same time. Hock notes, "While it probably is true in the long run every word has its own history, it is not justified to conclude as some linguists have, that therefore the Neogrammarian position on the nature of linguistic change is falsified."

The comparative method is used to construct a tree model (German "Stammbaum") of language evolution, in which daughter languages are seen as branching from the proto-language, gradually growing more distant from it through accumulated phonological, morpho-syntactic, and lexical changes.

The tree model features nodes that are presumed to be distinct proto-languages existing independently in distinct regions during distinct historical times. The reconstruction of unattested proto-languages lends itself to that illusion: they cannot be verified and the linguist is free to select whatever definite times and places for them seem best. Right from the outset of Indo-European studies, however, Thomas Young said:It is not, however, very easy to say what the definition should be that should constitute a separate language, but it seems most natural to call those languages distinct, of which the one cannot be understood by common persons in the habit of speaking the other ... Still, however, it may remain doubtfull whether the Danes and the Swedes could not, in general, understand each other tolerably well ... nor is it possible to say if the twenty ways of pronouncing the sounds, belonging to the Chinese characters, ought or ought not to be considered as so many languages or dialects... But, ... the languages so nearly allied must stand next to each other in a systematic order…

The assumption of uniformity in a proto-language, implicit in the comparative method, is problematic. Even in small language communities there are always dialect differences, whether based on area, gender, class, or other factors. The Pirahã language of Brazil is spoken by only several hundred people, but it has at least two different dialects, one spoken by men and one by women. Campbell points out:
It is not so much that the comparative method 'assumes' no variation; rather, it is just that there is nothing built into the comparative method which would allow it to address variation directly...This assumption of uniformity is a reasonable idealization; it does no more damage to the understanding of the language than, say, modern reference grammars do which concentrate on a language's general structure, typically leaving out consideration of regional or social variation.

Different dialects, as they evolve into separate languages, remain in contact with one another and influence each other. Even after they are considered distinct, languages near to one another continue to influence each other, often sharing grammatical, phonological, and lexical innovations. A change in one language of a family may spread to neighboring languages; and multiple waves of change are communicated like waves across language and dialect boundaries, each with its own randomly delimited range. If a language is divided into an inventory of features, each with its own time and range (isoglosses), they do not all coincide. History and prehistory may not offer a time and place for a distinct coincidence, as may be the case for proto-Italic, in which case the proto-language is only a concept. However, Hock observes:

The discovery in the late nineteenth century that isoglosses can cut across well-established linguistic boundaries at first created considerable attention and controversy. And it became fashionable to oppose a wave theory to a tree theory... Today, however, it is quite evident that the phenomena referred to by these two terms are complementary aspects of linguistic change...

The reconstruction of unknown proto-languages is inherently subjective. In the Proto-Algonquian example above, the choice of "*m" as the parent phoneme is only "likely", not "certain". It is conceivable that a Proto-Algonquian language with "*b" in those positions split into two branches, one which preserved "*b" and one which changed it to "*m" instead; and while the first branch only developed into Arapaho, the second spread out wider and developed into all the other Algonquian tribes. It is also possible that the nearest common ancestor of the Algonquian languages used some other sound instead, such as "*p", which eventually mutated to "*b" in one branch and to "*m" in the other. While examples of strikingly complicated and even circular developments are indeed known to have occurred (such as PIE "*t" > Pre-Proto-Germanic "*þ" > PG "*ð" > Proto-West-Germanic "*d" > Old High German "t" in "fater" > Modern German "Vater"), in the absence of any evidence or other reason to postulate a more complicated development, the preference of a simpler explanation is justified by the principle of parsimony, also known as Occam's razor. Since reconstruction involves many of these choices, some linguists prefer to view the reconstructed features as abstract representations of sound correspondences, rather than as objects with a historical time and place.

The existence of proto-languages and the validity of the comparative method is verifiable in cases where the reconstruction can be matched to a known language, which may only be known as a shadow in the loanwords of another language. For example, Finnic languages such as Finnish have borrowed many words from an early stage of Germanic, and the shape of the loans matches the forms that have been reconstructed for Proto-Germanic. Finnish "kuningas" 'king' and "kaunis" 'beautiful' match the Germanic reconstructions *"kuningaz" and *"skauniz" (> German "König" 'king', "schön" 'beautiful').

The Wave model was developed in the 1870s as an alternative to the tree model, in order to represent the historical patterns of language diversification. Both the tree-based and the wave-based representations are compatible with the Comparative Method.

By contrast, some approaches are incompatible with the Comparative method, including glottochronology and mass lexical comparison. Most historical linguists consider these to be flawed and unreliable.




</doc>
<doc id="7661" url="https://en.wikipedia.org/wiki?curid=7661" title="Council of Constance">
Council of Constance

The Council of Constance is the 15th-century ecumenical council recognized by the Catholic Church, held from 1414 to 1418 in the Bishopric of Constance. The council ended the Western Schism by deposing or accepting the resignation of the remaining papal claimants and by electing Pope Martin V.

The council also condemned Jan Hus as a heretic and facilitated his execution by the civil authority. It also ruled on issues of national sovereignty, the rights of pagans and just war, in response to a conflict between the Grand Duchy of Lithuania, Kingdom of Poland and the Order of the Teutonic Knights. The council is important for its relationship to ecclesial conciliarism and Papal supremacy.

The council's main purpose was to end the Papal schism which had resulted from the confusion following the Avignon Papacy. Pope Gregory XI's return to Rome in 1377, followed by his death (in 1378) and the controversial election of his successor, Pope Urban VI, resulted in the defection of a number of cardinals and the election of a rival pope based at Avignon in 1378. After thirty years of schism, the rival courts convened the Council of Pisa seeking to resolve the situation by deposing the two claimant popes and electing a new one. The council claimed that in such a situation, a council of bishops had greater authority than just one bishop, even if he were the bishop of Rome. Though the elected Antipope Alexander V and his successor, Antipope John XXIII (not to be confused with the 20th-century Pope John XXIII), gained widespread support, especially at the cost of the Avignon antipope, the schism remained, now involving not two but three claimants: Gregory XII at Rome, Benedict XIII at Avignon, and John XXIII.

Therefore, many voices, including Sigismund, King of the Romans and of Hungary (and later Holy Roman Emperor), pressed for another council to resolve the issue. That council was called by John XXIII and was held from 16 November 1414 to 22 April 1418 in Constance, Germany. According to Joseph McCabe, the council was attended by roughly 29 cardinals, 100 "learned doctors of law and divinity", 134 abbots, and 183 bishops and archbishops.

Sigismund arrived on Christmas Eve 1414 and exercised a profound and continuous influence on the course of the council in his capacity of imperial protector of the church. An innovation at the council was that instead of voting as individuals, the bishops voted in national blocs. The vote by nations was in great measure the initiative of the English, German, and French members. The legality of this measure, in imitation of the "nations" of the universities, was more than questionable, but during February 1415 it carried and thenceforth was accepted in practice, though never authorized by any formal decree of the council. The four "nations" consisted of England, France, Italy, and Germany, with Poles, Hungarians, Danes, and Scandinavians counted with the Germans. While the Italian representatives made up half of those in attendance, they were equal in influence to the English who sent twenty deputies and three bishops.

Many members of the new assembly (comparatively few bishops, but many doctors of theology and of canon and civil law, procurators of bishops, deputies of universities, cathedral chapters, provosts, etc., agents and representatives of princes, etc.) strongly favored the voluntary abdication of all three popes, as did King Sigismund.

Although the Italian bishops who had accompanied John XXIII in large numbers supported his legitimacy, he grew increasingly more suspicious of the council. Partly in response to a fierce anonymous attack on his character from an Italian source, on 2 March 1415 he promised to resign. However, on 20 March he secretly fled the city and took refuge at Schaffhausen in territory of his friend Frederick, Duke of Austria-Tyrol.

The famous decree "Haec Sancta Synodus," which gave primacy to the authority of the council and thus became a source for ecclesial conciliarism, was promulgated in the fifth session, 6 April 1415:

"Haec Sancta Synodus" marks the high-water mark of the Conciliar movement of reform. This decree, however, is not considered valid by the Magisterium of the Catholic Church, since it was never approved by Pope Gregory XII or his successors, and was passed by the council in a session before his confirmation. The church declared the first sessions of the Council of Constance an invalid and illicit assembly of bishops, gathered under the authority of John XXIII.

The acts of the council were not made public until 1442, at the behest of the Council of Basel; they were printed in 1500. The creation of a book on how to die was ordered by the council, and thus written in 1415 under the title "Ars moriendi".

With the support of King Sigismund, enthroned before the high altar of the cathedral of Constance, the Council of Constance recommended that all three papal claimants abdicate, and that another be chosen. In part because of the constant presence of the King, other rulers demanded that they have a say in who would be pope.

Gregory XII then sent representatives to Constance, whom he granted full powers to summon, open, and preside over an Ecumenical Council; he also empowered them to present his resignation to the Papacy. This would pave the way for the end of the Western Schism.

The legates were received by King Sigismund and by the assembled Bishops, and the King yielded the presidency of the proceedings to the papal legates, Cardinal Giovanni Dominici of Ragusa and Prince Carlo Malatesta. On 4 July 1415 the Bull of Gregory XII which appointed Dominici and Malatesta as his proxies at the council was formally read before the assembled Bishops. The cardinal then read a decree of Gregory XII which convoked the council and authorized its succeeding acts. Thereupon, the Bishops voted to accept the summons. Prince Malatesta immediately informed the council that he was empowered by a commission from Pope Gregory XII to resign the Papal Throne on the Pontiff's behalf. He asked the council whether they would prefer to receive the abdication at that point or at a later date. The Bishops voted to receive the Papal abdication immediately. Thereupon the commission by Gregory XII authorizing his proxy to resign the Papacy on his behalf was read and Malatesta, acting in the name of Gregory XII, pronounced the resignation of the papacy by Gregory XII and handed a written copy of the resignation to the assembly.

Former Pope Gregory XII was then created titular Cardinal Bishop of Porto and Santa Ruffina by the council, with rank immediately below the Pope (which made him the highest-ranking person in the church, since, due to his abdication, the See of Peter in Rome was vacant). Gregory XII's cardinals were accepted as true cardinals by the council, but the members of the council delayed electing a new pope for fear that a new pope would restrict further discussion of pressing issues in the church.

By the time the anti-popes were all deposed and the new Pope, Martin V, was elected, two years had passed since Gregory XII's abdication, and Gregory was already dead. The council took great care to protect the legitimacy of the succession, ratified all his acts, and a new pontiff was chosen. The new pope, Martin V, elected November 1417, soon asserted the absolute authority of the papal office.

A second goal of the council was to continue the reforms begun at the Council of Pisa (1409). The reforms were largely directed against John Wycliffe, mentioned in the opening session and condemned in the eighth on 4 May 1415, and Jan Hus, along with their followers. [Hus, summoned to Constance under a letter of safe conduct, was found guilty of heresy by the council and turned over to the secular court. "This holy synod of Constance, seeing that God's church has nothing more that it can do, relinquishes Jan Hus to the judgment of the secular authority and decrees that he is to be relinquished to the secular court." (Council of Constance-Session 15 – 6 July 1415). The secular court sentenced him to the stake.

Jerome of Prague, a supporter of Hus, came to Constance to offer assistance but was similarly arrested, judged, found guilty of heresy and turned over to the same secular court, with the same outcome as Hus. Poggio Bracciolini attended the council and related the unfairness of the process against Jerome.

Paweł Włodkowic and the other Polish representatives to the Council of Constance publicly defended Hus.

In 1411, the First Peace of Thorn ended the Polish–Lithuanian–Teutonic War, in which the Teutonic Knights fought the Kingdom of Poland and Grand Duchy of Lithuania. However, the peace was not stable and further conflicts arose regarding demarcation of the Samogitian borders. The tensions erupted into the brief Hunger War in summer 1414. It was concluded that the disputes would be mediated by the Council of Constance.

The Polish-Lithuanian position was defended by Paulus Vladimiri, rector of the Jagiellonian University, who challenged legality of the Teutonic crusade. He argued that a forced conversion was incompatible with free will, which was an essential component of a genuine conversion. Therefore, the Knights could only wage a defensive war if pagans violated natural rights of the Christians. Vladimiri further stipulated that infidels had rights which had to be respected, and neither the Pope nor the Holy Roman Emperor had the authority to violate them. Lithuanians also brought a group of Samogitian representatives to testify of atrocities committed by the Knights.

The Dominican theologian John of Falkenberg proved to be the fiercest opponent of the Poles. In his "Liber de doctrina", Falkenberg argued that "the Emperor has the right to slay even peaceful infidels simply because they are pagans." The Poles deserve death for defending infidels, and should be exterminated even more than the infidels; they should be deprived of their sovereignty and reduced to slavery." In "Satira", he attacked Polish-Lithuanian King Jogaila, calling him a "mad dog" unworthy to be king. Falkenberg was condemned and imprisoned for such libel, but was not officially accused of heresy. Other opponents included Grand Master's proctor Peter Wormditt, Dominic of San Gimignano, John Urbach, Ardecino de Porta of Novara, and Bishop of Ciudad Rodrigo Andrew Escobar. They argued that the Knights were perfectly justified in their crusade as it was a sacred duty of Christians to spread the true faith. Cardinal Pierre d'Ailly published an independent opinion that attempted to somewhat balance both Polish and Teutonic positions.

The council did not make any political decisions. It established the Diocese of Samogitia, with its seat in Medininkai and subordinated to Lithuanian dioceses, and appointed Matthias of Trakai as the first bishop. Pope Martin V appointed Polish-Lithuanian King Jogaila and Lithuanian Grand Duke Vytautas as vicars general in Pskov and Veliky Novgorod in recognition of their Catholicism. After another round of futile negotiations, the Gollub War broke out in 1422. It ended with the Treaty of Melno. Polish-Lithuanian-Teutonic wars continued for another hundred years.




</doc>
<doc id="7662" url="https://en.wikipedia.org/wiki?curid=7662" title="Churches Uniting in Christ">
Churches Uniting in Christ

Churches Uniting in Christ (CUIC) is an ecumenical organization that brings together ten mainline American denominations (including both predominantly white and predominantly black churches), and was inaugurated on January 20, 2002 in Memphis, Tennessee on the balcony of the Lorraine Motel. It is the successor organization to the Consultation on Church Union.

CUIC is the successor organization to the Consultation on Church Union (COCU), which had been founded in 1962. The original task of COCU was to negotiate a consensus between its nine (originally four) member communions (it also included three "advisory participant" churches). However, it never succeeded in this goal, despite making progress on several ecumenical fronts. At COCU's 18th plenary meeting in St. Louis, Missouri (January 1999), CUIC was proposed as a new relationship among the nine member communions. Each member communion voted to join CUIC over the next few years.

Heads of communion from each member of COCU (as well as the ELCA, a partner in mission and dialogue) inaugurated the group on the day before Martin Luther King, Jr. Day in 2002 at the motel where he was killed. This particular location highlighted the group's focus on racism as a major dividing factor between and among churches.

The Coordinating Council of CUIC created several task forces: Racial and Social Justice, Ministry, Young Adult and Local and Regional Ecumenism. Each task force represented an important part of early CUIC work. Local ecumenical liturgies were encouraged, and excitement initially built around "pilot programs" in Denver, Los Angeles, and Memphis. The Racial and Social Justice task force created gatherings and discussions on racial justice. The Ministry task force received much of the attention from church structures, however. The group had been given a mandate to complete work on reconciliation by 2007, and in 2003 began working on a document entitled "Mutual Recognition and Mutual Reconciliation of Ministries."

One of the most difficult issues concerning recognition and reconciliation of ministries was that of the historic episcopate. This was one of the issues that defeated proposals for union by COCU as well. The group approached this problem through dialogue, soliciting information from each member communion on the particularities of their theology and ecclesiology in order to come to a mutually acceptable conclusion.

CUIC released the seventh and final draft of the MRMRM document in June 2005. Much work was done in 2006 on this document, which focused on "Episkope," the oversight of ministry. The work culminated in a consultation on episkope in St. Louis in October 2006 involving the heads of communion of the members of CUIC. At this consultation, the MRMRM document was met with resistance, and concern was raised in particular that CUIC was focusing too narrowly on reconciliation of ministries and "not taking seriously our commitment to working on those issues of systemic racism that remain at the heart of our continuing and separated life as churches here in the United States."

The nine churches which inaugurated CUIC in 2002 were joined by the Moravian Church, Northern Province. The Moravians had been partners in mission and dialogue since 2002, but joined as a member communion after the October 2006 consultation on Episkope.

In 2007, the African Methodist Episcopal Zion Church and the African Methodist Episcopal Church withdrew from CUIC. Neither body sent representatives to the CUIC plenary on January 11–14, 2008, though the AME Council of Bishops never voted to suspend membership officially. They felt the other churches were not doing enough to counter the history of racial injustice between black and white churches. In response to this, the remaining churches in CUIC decided in 2008 to suspend their work while they seek reconciliation with these churches. This work began with a group of representatives who revisited the 1999 document "Call to Christian Commitment and Action to Combat Racism," which is available on the current CUIC website. This also meant eliminating the position of Director as well as the suspension of the work of the CUIC task forces. As of 2012, CUIC no longer has physical offices, opting instead for a virtual office and storing the archives of both CUIC and COCU at Princeton Seminary's Henry Luce III Library.

The African Methodist Episcopal Church resumed its participation by the February 2010 plenary meeting, where CUIC moved to refocus on its eight marks of commitment and a shared concern for racial justice as a major dividing factor facing ecumenism. Although the African Methodist Episcopal Zion Church has not rejoined the group, efforts have continued to bring this communion back into membership. The Rev. Staccato Powell, an AMEZ pastor, preached at the 2011 CUIC plenary in Ft. Lauderdale, Florida as a part of these reconciliation efforts. Combating racism has again become a priority of CUIC. Concerns over the historic episcopate have been sidelined since 2008, though they may re-emerge. The group's focus on mutual reconciliation of ministries has been revisited in the light of racism and the impact that racism may have on exchanging ministers between denominations. Therefore, the coordinating council of CUIC created a consultation on race and ministry while also choosing to partner with the Samuel Dewitt Proctor Conference, a social justice organization involved in African American faith communities.

The purpose of CUIC has always been unity (as reflected in their current slogan, "reconciling the baptized, seeking unity with justice"). This reflects one of the core scripture passages in the ecumenical movement, Jesus' prayer in John 17:21, "That they all may be one". CUIC has approached this goal of unity in various ways throughout its history.

Racism has been a primary focus of CUIC since 2002 (and, indeed, a primary focus of COCU alongside other forms of exclusion and prejudice, such as sexism and ableism). According to Dan Krutz, former president of CUIC, "Overcoming racism has been a focal point of CUIC since its beginning... Racism may be the biggest sin that divides churches." Even before the absence of the AME and AMEZ churches at the January 2011 plenary, some in CUIC had noticed the lack of commitment to racial reconciliation. Since 2008, however, racism has become an even more pressing concern. This has led CUIC to address issues of racism in the public sphere, including the killing of Trayvon Martin and the recovery from the 2010 Haiti earthquake.

According to their website, one of the reasons for transitioning from COCU to CUIC is so that member churches "stop 'consulting' and start living their unity in Christ more fully." This means that each member communion in CUIC agrees to abide by the eight Marks of Commitment, which are summarized as follows:








</doc>
<doc id="7663" url="https://en.wikipedia.org/wiki?curid=7663" title="Canadian Unitarian Council">
Canadian Unitarian Council

Canadian Unitarian Council () (CUC) formed on May 14, 1961 to be the national organization for Canadians who belong to the Unitarian Universalist Association (UUA) (the UUA formed a day later, on May 15, 1961). Until 2002, almost all member congregations of the CUC were also members of the UUA, and most services to CUC member congregations were provided by the UUA. However, after an agreement between the CUC and the UUA, most services since 2002 have been provided by the CUC to its own member congregations, with the UUA continuing to provide ministerial settlement services. Some Canadian congregations have continued to be members of both the CUC and UUA, while others are members of only the CUC.

The CUC is currently the only national body for Unitarian Universalist congregations in Canada, and is a member of the International Council of Unitarians and Universalists.

The CUC is made up of 46 member congregations and emerging groups, who are the legal owners of the organization, and who are, for governance and service delivery, divided into four regions: "BC" (British Columbia), "Western" (Alberta to Thunder Bay), "Central" (between Thunder Bay and Kingston), and "Eastern" (Kingston, Ottawa and everything east of that). However, for youth ministry, the "Central" and "Eastern" regions are combined to form a youth region known as "QuOM" (Quebec, Ontario and the Maritimes), giving the youth only three regions for their activities. The organization as a whole is governed by the CUC Board of Trusties (Board), whose mandate it is to govern in the best interests of the CUC's owners. The Board is made up of 8 members who are elected by congregational delegates at the CUC's Annual General Meeting. This consists of two Trustees from each region, who are eligible to serve a maximum of two three-year terms. Board meetings also include Official Observers to the Board, who participate without a vote and represent UU Youth and Ministers.

As members of the CUC, congregations and emerging groups are served by volunteer Service Consultants, Congregational Networks, and a series of other committees. There are two directors of regional services, one for the Western two regions, and one for the Eastern two regions. The Director of Lifespan Learning oversees development of religious exploration programming and youth and young adults are served by a Youth and Young Adult Ministry Development staff person.

Policies and business of the CUC are determined at the Annual Conference and Meeting (ACM), consisting of the Annual Conference, in which workshops are held, and the Annual General Meeting, in which business matters and plenary meetings are performed. The ACM features two addresses, a Keynote and a Confluence Lecture. The Confluence Lecture is comparable to the UUA's Ware Lecture in prestige. In early days this event simply consisted of the Annual General Meeting component as the Annual Conference component was not added to much later. And starting in 2017 the conference portion will only tack place every second year. Past ACMs have been held in the following locations:

^Not an ACM, but an "Annual General Meeting" and "Symposium", and unlike ACMs it was organized by the CUC and the Unitarian Universalist Ministers of Canada instead of a local congregation.<br>#Not a keynote presenter or lecturer, rather a symposium "Provocateur".<br>*Upcoming locations
The CUC does not have a central creed in which members are required to believe, but they have found it useful to articulate their common values in what has become known as The Principles and Sources of our Religious Faith, which are currently based on the UUA's Principles and Purposes. The CUC had a task force whose mandate was to consider revising them.

The principles and sources as published in church literature and on the CUC website:

The CUC formed on May 14, 1961 to be the national organization for Canadians within the about to form, UUA (they formed a day later on May 15, 1961). And until 2002, almost all member congregations of the CUC were also members of the UUA and most services to CUC member congregations were provided by the UUA. However, after an agreement between the UUA and the CUC, since 2002 most services have been provided by the CUC to its own member congregations, with the UUA continuing to provide ministerial settlement services. And also since 2002, some Canadian congregations have continued to be members of both the UUA and CUC while others are members of only the CUC.

The Canadian Unitarian Universalist youth of the day disapproved of the 2002 change in relationship between the CUC and UUA. It is quite evident in the words of this statement, which was adopted by the attendees of the 2001 youth conference held at the Unitarian Church of Montreal: "We the youth of Canada are deeply concerned about the direction the CUC seems to be taking. As stewards of our faith, adults have a responsibility to take into consideration the concerns of youth. We are opposed to making this massive jump in our evolutionary progress."

While the name of the organization is the Canadian Unitarian Council, the CUC includes congregations with Unitarian, Universalist, Unitarian Universalist, and Universalist Unitarian in their names. Changing the name of the CUC has occasionally been debated, but there have been no successful motions. To recognize this diversity, some members of the CUC abbreviate Unitarian Universalist as U*U (and playfully read it as "You star, you"). Note, not all CUC members like this playful reading and so when these people write the abbreviation they leave out the star(*), just writing UU instead.




</doc>
<doc id="7668" url="https://en.wikipedia.org/wiki?curid=7668" title="Charles Mingus">
Charles Mingus

Charles Mingus Jr. (April 22, 1922 – January 5, 1979) was an American jazz double bassist, pianist, composer and bandleader. A major proponent of collective improvisation, he is considered to be one of the greatest jazz musicians and composers in history, with a career spanning three decades and collaborations with other jazz legends such as Louis Armstrong, Charlie Parker, Dizzy Gillespie, Dannie Richmond, and Herbie Hancock.

Mingus' compositions continue to be played by contemporary musicians ranging from the repertory bands Mingus Big Band, Mingus Dynasty, and Mingus Orchestra, to the high school students who play the charts and compete in the Charles Mingus High School Competition.

Charles Mingus was born in Nogales, Arizona. His father, Charles Mingus Sr., was a sergeant in the U.S. Army. Mingus was largely raised in the Watts area of Los Angeles. His maternal grandfather was a Chinese British subject from Hong Kong, and his maternal grandmother was an African-American from the southern United States. Mingus was the third great-grandson of the family's founding patriarch who was, by most accounts, a German immigrant. His ancestors included German American, African American, Native American.

In Mingus's autobiography "Beneath the Underdog" his mother was described as "the daughter of an Englishman and a Chinese woman", and his father was the son "of a black farm worker and a Swedish woman". Charles Mingus Sr. claims to have been raised by his mother and her husband as a white person until he was fourteen, when his mother revealed to her family that the child's true father was a black slave, after which he had to run away from his family and live on his own. The autobiography doesn't confirm whether Charles Mingus Sr. or Mingus himself believed this story was true, or whether it was merely an embellished version of the Mingus family's lineage.

His mother allowed only church-related music in their home, but Mingus developed an early love for other music, especially Duke Ellington. He studied trombone, and later cello, although he was unable to follow the cello professionally because, at the time, it was nearly impossible for a black musician to make a career of classical music, and the cello was not yet accepted as a jazz instrument. Despite this, Mingus was still attached to the cello; as he studied bass with Red Callender in the late 1930s, Callender even commented that the cello was still Mingus's main instrument. In "Beneath the Underdog", Mingus states that he did not actually start learning bass until Buddy Collette accepted him into his swing band under the stipulation that he be the band's bass player.

Due to a poor education, the young Mingus could not read musical notation quickly enough to join the local youth orchestra. This had a serious impact on his early musical experiences, leaving him feeling ostracized from the classical music world. These early experiences, in addition to his lifelong confrontations with racism, were reflected in his music, which often focused on themes of racism, discrimination and (in)justice.

Much of the cello technique he learned was applicable to double bass when he took up the instrument in high school. He studied for five years with Herman Reinshagen, principal bassist of the New York Philharmonic, and compositional techniques with Lloyd Reese. Throughout much of his career, he played a bass made in 1927 by the German maker Ernst Heinrich Roth.

Beginning in his teen years, Mingus was writing quite advanced pieces; many are similar to Third Stream because they incorporate elements of classical music. A number of them were recorded in 1960 with conductor Gunther Schuller, and released as "Pre-Bird", referring to Charlie "Bird" Parker; Mingus was one of many musicians whose perspectives on music were altered by Parker into "pre- and post-Bird" eras.

Mingus gained a reputation as a bass prodigy. His first major professional job was playing with former Ellington clarinetist Barney Bigard. He toured with Louis Armstrong in 1943, and by early 1945 was recording in Los Angeles in a band led by Russell Jacquet, which also included Teddy Edwards, Maurice Simon, Bill Davis, and Chico Hamilton, and in May that year, in Hollywood, again with Teddy Edwards, in a band led by Howard McGhee.

He then played with Lionel Hampton's band in the late 1940s; Hampton performed and recorded several of Mingus's pieces. A popular trio of Mingus, Red Norvo and Tal Farlow in 1950 and 1951 received considerable acclaim, but Mingus's race caused problems with club owners and he left the group. Mingus was briefly a member of Ellington's band in 1953, as a substitute for bassist Wendell Marshall. Mingus's notorious temper led to him being one of the few musicians personally fired by Ellington (Bubber Miley and drummer Bobby Durham are among the others), after an on-stage fight between Mingus and Juan Tizol.

Also in the early 1950s, before attaining commercial recognition as a bandleader, Mingus played gigs with Charlie Parker, whose compositions and improvisations greatly inspired and influenced him. Mingus considered Parker the greatest genius and innovator in jazz history, but he had a love-hate relationship with Parker's legacy. Mingus blamed the Parker mythology for a derivative crop of pretenders to Parker's throne. He was also conflicted and sometimes disgusted by Parker's self-destructive habits and the romanticized lure of drug addiction they offered to other jazz musicians. In response to the many sax players who imitated Parker, Mingus titled a song, "If Charlie Parker were a Gunslinger, There'd be a Whole Lot of Dead Copycats" (released on "Mingus Dynasty" as "Gunslinging Bird").

Mingus was married four times, to Jeanne Gross, Lucille (Celia) Germanis, Judy Starkey, and Susan Graham Ungaro.

In 1952 Mingus co-founded Debut Records with Max Roach so he could conduct his recording career as he saw fit. The name originated from his desire to document unrecorded young musicians. Despite this, the best-known recording the company issued was of the most prominent figures in bebop. On May 15, 1953, Mingus joined Dizzy Gillespie, Parker, Bud Powell, and Roach for a concert at Massey Hall in Toronto, which is the last recorded documentation of Gillespie and Parker playing together. After the event, Mingus chose to overdub his barely audible bass part back in New York; the original version was issued later. The two 10" albums of the Massey Hall concert (one featured the trio of Powell, Mingus and Roach) were among Debut Records' earliest releases. Mingus may have objected to the way the major record companies treated musicians, but Gillespie once commented that he did not receive any royalties "for years and years" for his Massey Hall appearance. The records, however, are often regarded as among the finest live jazz recordings.

One story has it that Mingus was involved in a notorious incident while playing a 1955 club date billed as a "reunion" with Parker, Powell, and Roach. Powell, who suffered from alcoholism and mental illness (possibly exacerbated by a severe police beating and electroshock treatments), had to be helped from the stage, unable to play or speak coherently. As Powell's incapacitation became apparent, Parker stood in one spot at a microphone, chanting "Bud Powell...Bud Powell..." as if beseeching Powell's return. Allegedly, Parker continued this incantation for several minutes after Powell's departure, to his own amusement and Mingus's exasperation. Mingus took another microphone and announced to the crowd, "Ladies and Gentleman, please don't associate me with any of this. This is not jazz. These are sick people." This was Parker's last public performance; about a week later he died after years of substance abuse.

Mingus often worked with a mid-sized ensemble (around 8–10 members) of rotating musicians known as the Jazz Workshop. Mingus broke new ground, constantly demanding that his musicians be able to explore and develop their perceptions on the spot. Those who joined the Workshop (or Sweatshops as they were colorfully dubbed by the musicians) included Pepper Adams, Jaki Byard, Booker Ervin, John Handy, Jimmy Knepper, Charles McPherson and Horace Parlan. Mingus shaped these musicians into a cohesive improvisational machine that in many ways anticipated free jazz. Some musicians dubbed the workshop a "university" for jazz.

The decade that followed is generally regarded as Mingus's most productive and fertile period. Over a ten-year period, made 30 records for a number of labels (Atlantic, Candid, Columbia, Impulse and others), a pace perhaps unmatched by any other musicians except Ellington. 

Mingus had already recorded around ten albums as a bandleader, but 1956 was a breakthrough year for him, with the release of "Pithecanthropus Erectus", arguably his first major work as both a bandleader and composer. Like Ellington, Mingus wrote songs with specific musicians in mind, and his band for "Erectus" included adventurous musicians: piano player Mal Waldron, alto saxophonist Jackie McLean and the Sonny Rollins-influenced tenor of J. R. Monterose. The title song is a ten-minute tone poem, depicting the rise of man from his hominid roots ("Pithecanthropus erectus") to an eventual downfall. A section of the piece was free improvisation, free of structure or theme.

Another album from this period, "The Clown" (1957 also on Atlantic Records), the title track of which features narration by humorist Jean Shepherd, was the first to feature drummer Dannie Richmond, who remained his preferred drummer until Mingus's death in 1979. The two men formed one of the most impressive and versatile rhythm sections in jazz. Both were accomplished performers seeking to stretch the boundaries of their music while staying true to its roots. When joined by pianist Jaki Byard, they were dubbed "The Almighty Three".

In 1959 Mingus and his jazz workshop musicians recorded one of his best-known albums, "Mingus Ah Um". Even in a year of standout masterpieces, including Dave Brubeck's "Time Out", Miles Davis's "Kind of Blue", John Coltrane's "Giant Steps", and Ornette Coleman's prophetic "The Shape of Jazz to Come", this was a major achievement, featuring such classic Mingus compositions as "Goodbye Pork Pie Hat" (an elegy to Lester Young) and the vocal-less version of "Fables of Faubus" (a protest against segregationist Arkansas governor Orval E. Faubus that features double-time sections). Also during 1959, Mingus recorded the album "Blues & Roots", which was released the following year. As Mingus explained in his liner notes: "I was born swinging and clapped my hands in church as a little boy, but I've grown up and I like to do things other than just swing. But blues can do more than just swing."

Mingus witnessed Ornette Coleman's legendary—and controversial—1960 appearances at New York City's Five Spot jazz club. He initially expressed rather mixed feelings for Coleman's innovative music: "...if the free-form guys could play the same tune twice, then I would say they were playing something...Most of the time they use their fingers on the saxophone and they don't even know what's going to come out. They're experimenting." That same year, however, Mingus formed a quartet with Richmond, trumpeter Ted Curson and multi-instrumentalist Eric Dolphy. This ensemble featured the same instruments as Coleman's quartet, and is often regarded as Mingus rising to the challenging new standard established by Coleman. The quartet recorded on both "Charles Mingus Presents Charles Mingus" and "Mingus". The former also features the version of "Fables of Faubus" with lyrics, aptly titled "Original Faubus Fables".

Only one misstep occurred in this era: 1962's "Town Hall Concert". An ambitious program, it was plagued with troubles from its inception. Mingus's vision, now known as "Epitaph", was finally realized by conductor Gunther Schuller in a concert in 1989, 10 years after Mingus's death.

In 1963, Mingus released "The Black Saint and the Sinner Lady", a sprawling, multi-section masterpiece, described as "one of the greatest achievements in orchestration by any composer in jazz history." The album was also unique in that Mingus asked his psychotherapist, Dr. Edmund Pollock, to provide notes for the record.

Mingus also released "Mingus Plays Piano", an unaccompanied album featuring some fully improvised pieces, in 1963.

In addition, 1963 saw the release of "Mingus Mingus Mingus Mingus Mingus", an album praised by critic Nat Hentoff.

In 1964 Mingus put together one of his best-known groups, a sextet including Dannie Richmond, Jaki Byard, Eric Dolphy, trumpeter Johnny Coles, and tenor saxophonist Clifford Jordan. The group was recorded frequently during its short existence; Coles fell ill and left during a European tour. Dolphy stayed in Europe after the tour ended, and died suddenly in Berlin on June 28, 1964. 1964 was also the year that Mingus met his future wife, Sue Graham Ungaro. The couple were married in 1966 by Allen Ginsberg. Facing financial hardship, Mingus was evicted from his New York home in 1966.

Mingus's pace slowed somewhat in the late 1960s and early 1970s. In 1974, after his 1970 sextet with Charles McPherson, Eddie Preston and Bobby Jones disbanded, he formed a quintet with Richmond, pianist Don Pullen, trumpeter Jack Walrath and saxophonist George Adams. They recorded two well-received albums, "Changes One" and "Changes Two". Mingus also played with Charles McPherson in many of his groups during this time. "Cumbia and Jazz Fusion" in 1976 sought to blend Colombian music (the "Cumbia" of the title) with more traditional jazz forms. In 1971, Mingus taught for a semester at the University at Buffalo, The State University of New York as the Slee Professor of Music.

By the mid-1970s, Mingus was suffering from amyotrophic lateral sclerosis (ALS). His once formidable bass technique declined until he could no longer play the instrument. He continued composing, however, and supervised a number of recordings before his death. At the time of his death, he was working with Joni Mitchell on an album eventually titled "Mingus", which included lyrics added by Mitchell to his compositions, including "Goodbye Pork Pie Hat". The album featured the talents of Wayne Shorter, Herbie Hancock, and another influential bassist and composer, Jaco Pastorius.

Mingus died, aged 56, in Cuernavaca, Mexico, where he had traveled for treatment and convalescence. His ashes were scattered in the Ganges River.

His compositions retained the hot and soulful feel of hard bop, drawing heavily from black gospel music and blues, while sometimes containing elements of Third Stream, free jazz, and classical music. He once cited Duke Ellington and church as his main influences.

Mingus espoused collective improvisation, similar to the old New Orleans jazz parades, paying particular attention to how each band member interacted with the group as a whole. In creating his bands, he looked not only at the skills of the available musicians, but also their personalities. Many musicians passed through his bands and later went on to impressive careers. He recruited talented and sometimes little-known artists, whom he utilized to assemble unconventional instrumental configurations. As a performer, Mingus was a pioneer in double bass technique, widely recognized as one of the instrument's most proficient players.

Because of his brilliant writing for midsize ensembles, and his catering to and emphasizing the strengths of the musicians in his groups, Mingus is often considered the heir of Duke Ellington, for whom he expressed great admiration and collaborated on the record "Money Jungle". Indeed, Dizzy Gillespie had once claimed Mingus reminded him "of a young Duke", citing their shared "organizational genius."

Nearly as well known as his ambitious music was Mingus's often fearsome temperament, which earned him the nickname "The Angry Man of Jazz". His refusal to compromise his musical integrity led to many onstage eruptions, exhortations to musicians, and dismissals. Although respected for his musical talents, Mingus was sometimes feared for his occasionally violent onstage temper, which was at times directed at members of his band and other times aimed at the audience. He was physically large, prone to obesity (especially in his later years), and was by all accounts often intimidating and frightening when expressing anger or displeasure.

For example, when confronted with a nightclub audience talking and clinking ice in their glasses while he performed, Mingus stopped his band and loudly chastised the audience, stating: "Isaac Stern doesn't have to put up with this shit." Another time, Mingus reportedly destroyed a $20,000 bass in response to audience heckling at New York's Five Spot.

Guitarist and singer Jackie Paris was a first-hand witness to Mingus's irascibility. Paris recalls his time in the Jazz Workshop: "He chased everybody off the stand except [drummer] Paul Motian and me... The three of us just wailed on the blues for about an hour and a half before he called the other cats back."

On October 12, 1962, Mingus punched Jimmy Knepper in the mouth while the two men were working together at Mingus' apartment on a score for his upcoming concert at The Town Hall in New York, and Knepper refused to take on more work. Mingus' blow broke off a crowned tooth and its underlying stub. According to Knepper, this ruined his embouchure and resulted in the permanent loss of the top octave of his range on the trombone – a significant handicap for any professional trombonist. This attack temporarily ended their working relationship, and Knepper was unable to perform at the concert. Charged with assault, Mingus appeared in court in January 1963 and was given a suspended sentence. Knepper did again work with Mingus in 1977 and played extensively with the Mingus Dynasty, formed after Mingus' death in 1979.

In addition to bouts of ill temper, Mingus was prone to clinical depression and tended to have brief periods of extreme creative activity intermixed with fairly long periods of greatly decreased output.

In 1966, Mingus was evicted from his apartment at 5 Great Jones Street in New York City for nonpayment of rent, captured in the 1968 documentary film "", directed by Thomas Reichman. The film also features Mingus performing in clubs and in the apartment, firing a .410 shotgun indoors, composing at the piano, playing with and taking care of his young daughter Caroline, and discussing love, art, politics, and the music school he had hoped to create.

Charles Mingus' music is currently being performed and reinterpreted by the Mingus Big Band, which in October 2008 began playing every Monday at Jazz Standard in New York City, and often tours the rest of the U.S. and Europe. The Mingus Big Band, the Mingus Orchestra, and the Mingus Dynasty band are managed by Jazz Workshop, Inc. and run by Mingus' widow Sue Graham Mingus.

Elvis Costello has written lyrics for a few Mingus pieces. He had once sung lyrics for one piece, "Invisible Lady", backed by the Mingus Big Band on the album, "Tonight at Noon: Three of Four Shades of Love".

"Epitaph" is considered one of Charles Mingus' masterpieces. The composition is 4,235 measures long, requires two hours to perform, and is one of the longest jazz pieces ever written. "Epitaph" was only completely discovered, by musicologist Andrew Homzy, during the cataloging process after Mingus' death. With the help of a grant from the Ford Foundation, the score and instrumental parts were copied, and the piece itself was premiered by a 30-piece orchestra, conducted by Gunther Schuller. This concert was produced by Mingus' widow, Sue Graham Mingus, at Alice Tully Hall on June 3, 1989, 10 years after Mingus' death. It was performed again at several concerts in 2007. The performance at Walt Disney Concert Hall is available on NPR. Hal Leonard published the complete score in 2008.

Mingus wrote the sprawling, exaggerated, quasi-autobiography, "Beneath the Underdog: His World as Composed by Mingus", throughout the 1960s, and it was published in 1971. Its "stream of consciousness" style covered several aspects of his life that had previously been off-record. In addition to his musical and intellectual proliferation, Mingus goes into great detail about his perhaps overstated sexual exploits. He claims to have had more than 31 affairs in the course of his life (including 26 prostitutes in one sitting). This does not include any of his five wives (he claims to have been married to two of them simultaneously). In addition, he asserts that he held a brief career as a pimp. This has never been confirmed.

Mingus's autobiography also serves as an insight into his psyche, as well as his attitudes about race and society. It includes accounts of abuse at the hands of his father from an early age, being bullied as a child, his removal from a white musician's union, and grappling with disapproval while married to white women and other examples of the hardship and prejudice.

The work of Charles Mingus has also received attention in academic spaces. According to Ashon Crawley, the musicianship of Charles Mingus provides a salient example of the power of music to unsettle the dualistic, categorical distinction of sacred from profane through otherwise epistemologies. Crawley offers a reading of Mingus that examines the deep, imbrication uniting Holiness-Pentecostal aesthetic practices and jazz. Mingus recognized the importance and impact of the midweek gathering of black folks at the Holiness-Pentecostal Church at 79th and Watts in Los Angeles that he'd attend with his stepmother or his friend Britt Woodman. Crawley goes on to argues that these visits were the impetus for the song "Wednesday Prayer Meeting." Emphasis is placed on the ethical demand of the prayer meeting felt and experienced that, according to Crawley, Mingus attempts to capture. In many ways, "Wednesday Night Prayer Meeting" was Mingus's memorial, homage, to black sociality. By exploring Mingus' homage to black Pentecostal aesthetics, Crawley expounds on how Mingus figured out that those Holiness-Pentecostal gatherings were the constant repetition of the ongoing, deep, intense mode of study, a kind of study wherein the aesthetic forms created could not be severed from the intellectual practice because they were one and also, but not, the same." Gunther Schuller has suggested that Mingus should be ranked among the most important American composers, jazz or otherwise. In 1988, a grant from the National Endowment for the Arts made possible the cataloging of Mingus compositions, which were then donated to the Music Division of the New York Public Library for public use. In 1993, The Library of Congress acquired Mingus's collected papers—including scores, sound recordings, correspondence and photos—in what they described as "the most important acquisition of a manuscript collection relating to jazz in the Library's history".

Considering the number of compositions that Charles Mingus wrote, his works have not been recorded as often as comparable jazz composers. The only Mingus tribute albums recorded during his lifetime were baritone saxophonist Pepper Adams's album, "Pepper Adams Plays the Compositions of Charlie Mingus", in 1963, and Joni Mitchell's album "Mingus", in 1979. Of all his works, his elegant elegy for Lester Young, "Goodbye Pork Pie Hat" (from "Mingus Ah Um") has probably had the most recordings. Besides recordings from the expected jazz artists, the song has also been recorded by musicians as disparate as Jeff Beck, Andy Summers, Eugene Chadbourne, and Bert Jansch and John Renbourn with and without Pentangle. Joni Mitchell sang a version with lyrics that she wrote for it.

Elvis Costello has recorded "Hora Decubitus" (from "Mingus Mingus Mingus Mingus Mingus") on "My Flame Burns Blue" (2006). "Better Git It in Your Soul" was covered by Davey Graham on his album "Folk, Blues, and Beyond." Trumpeter Ron Miles performs a version of "Pithecanthropus Erectus" on his CD "Witness." New York Ska Jazz Ensemble has done a cover of Mingus's "Haitian Fight Song", as have the British folk rock group Pentangle and others. Hal Willner's 1992 tribute album "Weird Nightmare: Meditations on Mingus" (Columbia Records) contains idiosyncratic renditions of Mingus's works involving numerous popular musicians including Chuck D, Keith Richards, Henry Rollins and Dr. John. The Italian band Quintorigo recorded an entire album devoted to Mingus's music, titled "Play Mingus".

Gunther Schuller's edition of Mingus's "Epitaph" which premiered at Lincoln Center in 1989 was subsequently released on Columbia/Sony Records.

One of the most elaborate tributes to Mingus came on September 29, 1969, at a festival honoring him. Duke Ellington performed "The Clown", with Duke reading Jean Shepherd's narration. It was long believed that no recording of this performance existed; however, one was discovered and premiered on July 11, 2013, by Dry River Jazz host Trevor Hodgkins for NPR member station KRWG-FM with re-airings on July 13, 2013, and July 26, 2014. Mingus's elegy for Duke, "Duke Ellington's Sound Of Love", was recorded by Kevin Mahogany on "Double Rainbow" (1993) and Anita Wardell on "Why Do You Cry?" (1995).





</doc>
<doc id="7669" url="https://en.wikipedia.org/wiki?curid=7669" title="Centimetre">
Centimetre

A centimetre (international spelling as used by the International Bureau of Weights and Measures; symbol cm) or centimeter (American spelling) is a unit of length in the metric system, equal to one hundredth of a metre, "centi" being the SI prefix for a factor of . The centimetre was the base unit of length in the now deprecated centimetre–gram–second (CGS) system of units.

Though for many physical quantities, SI prefixes for factors of 10—like "milli-" and "kilo-"—are often preferred by technicians, the centimetre remains a practical unit of length for many everyday measurements. A centimetre is approximately the width of the fingernail of an average adult person.

One 1 millilitre is defined as one cubic centimetre, under the SI system of units.

In addition to its use in the measurement of length, the centimetre is used:

For the purposes of compatibility with Chinese, Japanese and Korean (CJK) characters, Unicode has symbols for:

They are mostly used only with East Asian fixed-width CJK fonts, because they are equal in size to one Chinese character.



</doc>
<doc id="7670" url="https://en.wikipedia.org/wiki?curid=7670" title="Central Coast">
Central Coast

Central Coast may refer to:





</doc>
<doc id="7671" url="https://en.wikipedia.org/wiki?curid=7671" title="Committee on Data for Science and Technology">
Committee on Data for Science and Technology

The Committee on Data for Science and Technology (CODATA) was established in 1966 as an interdisciplinary committee of the International Council for Science. It seeks to improve the compilation, critical evaluation, storage, and retrieval of data of importance to science and technology.

CODATA sponsors the CODATA international conference every two years.

CODATA is best known for (and sometimes confused with) its Task Group on Fundamental Constants (TGFC). Established in 1969, its purpose is to periodically provide the international scientific and technological communities with an internationally accepted set of values of the fundamental physical constants and closely related conversion factors for use worldwide.

The first such CODATA set was published in 1973. Later versions are named based on the year of the data incorporated; the 1986 CODATA (published April 1987) used data up to 1 January 1986. All subsequent releases use data up to the "end" of the stated year, and are necessarily published a year or two later: 1998 (April 2000), 2002 (January 2005), 2006 (June 2008) and the sixth in 2010 (November 2012). The latest version is Version 7.0 called "2014 CODATA" published on 25 June 2015.

The CODATA recommended values of fundamental physical constants are published at the NIST Reference on Constants, Units, and Uncertainty.

Since 1998, the task group has produced a new version every four years, incorporating results published up to the end of the specified year.

In order to support the upcoming redefinition of the SI base units, expected to be adopted at the 26th General Conference on Weights and Measures in the fall of 2018, CODATA made a special release that was published in October 2017.
It incorporates all data up to 1 July 2017, and determines the final numerical values of "h", "e", "k", and "N" that are to be used for the new SI definitions.

The next regular version, with a closing date of 31 December 2018, will be used to produce the new 2018 CODATA values that will be made available by the time the revised SI should come into force, on 20 May 2019. This is necessary because the redefinitions have a significant (mostly beneficial) effect on the uncertainties and correlation coefficients reported by CODATA.





</doc>
<doc id="7672" url="https://en.wikipedia.org/wiki?curid=7672" title="Chuck Jones">
Chuck Jones

Charles Martin "Chuck" Jones (September 21, 1912 – February 22, 2002) was an American animator, filmmaker, cartoonist, author, and screenwriter, best known for his work with Warner Bros. Cartoons on the "Looney Tunes" and "Merrie Melodies" shorts. He wrote, produced, and/or directed many classic animated cartoon shorts starring Bugs Bunny, Daffy Duck, Wile E. Coyote and the Road Runner, Pepé Le Pew, Porky Pig, Michigan J. Frog, the Three Bears, and a slew of other Warner characters.

After his career at Warner Bros. ended in 1962, Jones started Sib Tower 12 Productions, and began producing cartoons for Metro-Goldwyn-Mayer, including a new series of "Tom and Jerry" shorts and the television adaptation of Dr. Seuss' "How the Grinch Stole Christmas!". He later started his own studio, Chuck Jones Enterprises, which created several one-shot specials, and periodically worked on "Looney Tunes" related works.

Jones was nominated for an Oscar eight times and won three times, receiving awards for the cartoons "For Scent-imental Reasons", "So Much for So Little", and "The Dot and the Line". He received an Honorary Academy Award in 1996 for his work in the animation industry. Film historian Leonard Maltin has praised Jones' work at Warner Bros., MGM and Chuck Jones Enterprises. He also said that the "feud" that there may have been between Jones and colleague Bob Clampett was mainly because they were so different from each other. In Jerry Beck's "The 50 Greatest Cartoons", ten of the entries were directed by Jones, with four out of the five top cartoons being Jones shorts.

Jones was born on September 21, 1912, in Spokane, Washington, the son of Mabel McQuiddy (Martin) and Charles Adams Jones. He later moved with his parents and three siblings to the Los Angeles, California area.

In his autobiography, "Chuck Amuck", Jones credits his artistic bent to circumstances surrounding his father, who was an unsuccessful businessman in California in the 1920s. His father, Jones recounts, would start every new business venture by purchasing new stationery and new pencils with the company name on them. When the business failed, his father would quietly turn the huge stacks of useless stationery and pencils over to his children, requiring them to use up all the material as fast as possible. Armed with an endless supply of high-quality paper and pencils, the children drew constantly. Later, in one art school class, the professor gravely informed the students that they each had 100,000 bad drawings in them that they must first get past before they could possibly draw anything worthwhile. Jones recounted years later that this pronouncement came as a great relief to him, as he was well past the 200,000 mark, having used up all that stationery. Jones and several of his siblings went on to artistic careers.

During his artistic education, he worked part-time as a janitor. After graduating from Chouinard Art Institute, Jones got a phone call from a friend named Fred Kopietz, who had been hired by the Ub Iwerks studio and offered him a job. He worked his way up in the animation industry, starting as a cel washer; "then I moved up to become a painter in black and white, some color. Then I went on to take animator's drawings and traced them onto the celluloid. Then I became what they call an in-betweener, which is the guy that does the drawing between the drawings the animator makes". While at Iwerks, he met a cel painter named Dorothy Webster, who later became his first wife.

Jones joined Leon Schlesinger Productions, the independent studio that produced "Looney Tunes" and "Merrie Melodies" for Warner Bros., in 1933 as an assistant animator. In 1935, he was promoted to animator, and assigned to work with new Schlesinger director Tex Avery. There was no room for the new Avery unit in Schlesinger's small studio, so Avery, Jones, and fellow animators Bob Clampett, Virgil Ross, and Sid Sutherland were moved into a small adjacent building they dubbed "Termite Terrace". When Clampett was promoted to director in 1937, Jones was assigned to his unit; the Clampett unit was briefly assigned to work with Jones' old employer, Ub Iwerks when Iwerks subcontracted four cartoons to Schlesinger in 1937. Jones became a director (or "supervisor", the original title for an animation director in the studio) himself in 1938 when Frank Tashlin left the studio. The following year Jones created his first major character, Sniffles, a cute Disney-style mouse, who went on to star in twelve Warner Bros. cartoons. 

He was actively involved in efforts to unionize the staff of Leon Schlesinger Studios. He was responsible for recruiting animators, layout men, and background people. Almost all animators joined, in reaction to salary cuts imposed by Leon Schlesinger. The Metro-Goldwyn-Mayer cartoon studio had already signed a union contract, encouraging their counterparts under Schlesinger. In a meeting with his staff, Schlesinger talked for a few minutes, then turned over the meeting to his attorney. His insulting manner had a unifying effect on the staff. Jones gave a pep talk at the union headquarters. As negotiations broke down, the staff decided to go on strike. Schlesinger locked them out of the studio for a few days, before agreeing to sign the contract. A Labor Management Committee was formed and Jones served as a moderator. Because of his role as a supervisor in the studio, he could not himself join the union. Jones created many of his lesser-known characters during this period, including Charlie Dog, Hubie and Bertie, and The Three Bears.
During World War II, Jones worked closely with Theodor Geisel, better known as Dr. Seuss, to create the "Private Snafu" series of Army educational cartoons (the character was created by director Frank Capra). Jones later collaborated with Seuss on animated adaptations of Seuss' books, including "How the Grinch Stole Christmas!" in 1966. Jones directed such shorts as "The Weakly Reporter", a 1944 short that related to shortages and rationing on the home front. During the same year, he directed "Hell-Bent for Election", a campaign film for Franklin D. Roosevelt.

Jones created characters through the late 1940s and the 1950s, which include Claude Cat, Marc Antony and Pussyfoot, Charlie Dog, Michigan J. Frog, and his four most popular creations, Marvin the Martian, Pepé Le Pew, Wile E. Coyote and The Road Runner. Jones and writer Michael Maltese collaborated on the Road Runner cartoons, "Duck Amuck", "One Froggy Evening", and "What's Opera, Doc?". Other staff at Unit A that Jones collaborated with include layout artist, background designer, co-director Maurice Noble; animator and co-director Abe Levitow; and animators Ken Harris and Ben Washam.

Jones remained at Warner Bros. throughout the 1950s, except for a brief period in 1953 when Warner closed the animation studio. During this interim, Jones found employment at Walt Disney Productions, where he teamed with Ward Kimball for a four-month period of uncredited work on "Sleeping Beauty" (1959). Upon the reopening of the Warner animation department, Jones was rehired and reunited with most of his unit.

In the early 1960s, Jones and his wife Dorothy wrote the screenplay for the animated feature "Gay Purr-ee". The finished film would feature the voices of Judy Garland, Robert Goulet and Red Buttons as cats in Paris, France. The feature was produced by UPA and directed by his former Warner Bros. collaborator, Abe Levitow.

Jones moonlighted to work on the film since he had an exclusive contract with Warner Bros. UPA completed the film and made it available for distribution in 1962; it was picked up by Warner Bros. When Warner Bros. discovered that Jones had violated his exclusive contract with them, they terminated him. Jones' former animation unit was laid off after completing the final cartoon in their pipeline, "The Iceman Ducketh", and the rest of the Warner Bros. Cartoons studio was closed in early 1963.

With business partner Les Goldman, Jones started an independent animation studio, Sib Tower 12 Productions, and brought on most of his unit from Warner Bros., including Maurice Noble and Michael Maltese. In 1963, Metro-Goldwyn-Mayer contracted with Sib Tower 12 to have Jones and his staff produce new "Tom and Jerry" cartoons as well as a television adaptation of all "Tom and Jerry" theatricals produced to that date. This included major editing, including writing out the African-American maid, Mammy Two-Shoes, and replacing her with one of Irish descent voiced by June Foray. In 1964, Sib Tower 12 was absorbed by MGM and was renamed MGM Animation/Visual Arts. His animated short film, "The Dot and the Line: A Romance in Lower Mathematics", won the 1965 Academy Award for Best Animated Short Film. Jones directed the classic animated short "The Bear That Wasn't".

As the "Tom and Jerry" series wound down (it was discontinued in 1967), Jones produced more for television. In 1966, he produced and directed the TV special "How the Grinch Stole Christmas!", featuring the voice and facial models based on the readings by Boris Karloff.

Jones continued to work on other TV specials such as "Horton Hears a Who!" (1970), but his main focus during this time was producing the feature film "The Phantom Tollbooth", which did lukewarm business when MGM released it in 1970. Jones co-directed 1969's "The Pogo Special Birthday Special", based on the Walt Kelly comic strip, and voiced the characters of Porky Pine and Bun Rab. It was at this point that he decided to start ST Incorporated.

MGM closed the animation division in 1970, and Jones once again started his own studio, Chuck Jones Enterprises. He produced a Saturday morning children's TV series for the American Broadcasting Company called "The Curiosity Shop" in 1971. In 1973, he produced an animated version of the George Selden book "The Cricket in Times Square" and would go on to produce two sequels.

Three of his works during this period were animated TV adaptations of short stories from Rudyard Kipling's "Mowgli's Brothers", "The White Seal" and "Rikki-Tikki-Tavi". During this period, Jones began to experiment with more realistically designed characters, most of which having larger eyes, leaner bodies, and altered proportions, such as those of the Looney Tunes characters.

Jones resumed working with Warner Bros. in 1976 with the animated TV adaptation of "The Carnival of the Animals" with Bugs Bunny and Daffy Duck. Jones also produced the 1979 film "The Bugs Bunny/Road Runner Movie" which was a compilation of Jones' best theatrical shorts; Jones produced new Road Runner shorts for "The Electric Company" series and "Bugs Bunny's Looney Christmas Tales" (1979), and even newer shorts were made for "Bugs Bunny's Bustin' Out All Over" (1980).

From 1977 to 1978, Jones wrote and drew the newspaper comic strip "Crawford" (also known as "Crawford & Morgan") for the Chicago Tribune-NY News Syndicate. In 2011 IDW Publishing collected Jones' strip as part of their Library of American Comic Strips.

In 1978, Jones' wife Dorothy died; three years later, he married Marian Dern, the writer of the comic strip "Rick O'Shay".

On December 11, 1975, shortly after the release of "Bugs Bunny Superstar", which prominently featured Bob Clampett, Jones wrote a letter to Tex Avery, accusing Clampett of taking credit for ideas that were not his, and for characters created by other directors (notably Jones's Sniffles and Friz Freleng's Yosemite Sam). Their correspondence was never published in the media. It was forwarded to Michael Barrier, who conducted the interview with Clampett and was distributed by Jones to multiple people concerned with animation over the years. Robert McKimson claimed in an interview that many animators but mostly Clampett contributed to the crazy personality of Bugs, while others like Chuck Jones concentrated more on the more calmed-down gags. As far as plagiarism is concerned, McKimson claimed the animators would always be looking at each other's sheets to see if they could borrow some punchlines and cracks.

Through the 1980s and 1990s, Jones was painting cartoon and parody art, sold through animation galleries by his daughter's company, Linda Jones Enterprises. Jones was the creative consultant and character designer for two Raggedy Ann animated specials and the first "Alvin and the Chipmunks" Christmas special "A Chipmunk Christmas". He made a cameo appearance in the 1984 film "Gremlins" and directed the Bugs Bunny/Daffy Duck animated sequences that bookend its sequel "" (1990). Jones directed animated sequences for various features such as a lengthy sequence in the 1992 film "Stay Tuned" and a shorter one seen at the start of the 1993 film "Mrs. Doubtfire". Also during the 1980s and 1990s Jones served on the advisory board of the National Student Film Institute.

Jones' final Looney Tunes cartoon was "From Hare to Eternity" in 1997, which starred Bugs Bunny and Yosemite Sam, with Greg Burson voicing Bugs. The cartoon was dedicated to Friz Freleng, who had died in 1995. Jones' final animation project was a series of 13 shorts starring a timber wolf character he had designed in the 1960s named Thomas Timber Wolf. The series was released online by Warner Bros. in 2000. From 2001 until 2004, Cartoon Network aired "The Chuck Jones Show" which features shorts directed by him. The show won the Annie Award for Outstanding Achievement in an Animated Special Project.

Jones died of heart failure on February 22, 2002. He was cremated and his ashes were scattered at sea. After his death, the Looney Tunes cartoon "Daffy Duck for President", based on the book that Jones had written and using Jones' style for the characters, originally scheduled to be released in 2000, was released in 2004 as part of of the "" DVD set.

Jones was a historical authority as well as a major contributor to the development of animation throughout the 20th century. He received an honorary degree from Oglethorpe University in 1993. For his contribution to the motion picture industry, Jones has a star on the Hollywood Walk of Fame at 7011 Hollywood Blvd.

Jones, whose work had been nominated eight times over his career for an Oscar (winning the award three times: "For Scent-imental Reasons", "So Much for So Little", and "The Dot and the Line"), received an Honorary Academy Award in 1996 by the board of governors of the Academy of Motion Picture Arts and Sciences, for "the creation of classic cartoons and cartoon characters whose animated lives have brought joy to our real ones for more than half a century." At that year's awards show, Robin Williams, a self-confessed "Jones-aholic," presented the honorary award to Jones, calling him "The Orson Welles of cartoons.", and the audience gave Jones a standing ovation as he walked onto the stage. For himself, a flattered Jones wryly remarked in his acceptance speech, "Well, what can I say in the face of such humiliating evidence? I stand guilty before the world of directing over three hundred cartoons in the last fifty or sixty years. Hopefully, this means you've forgiven me." He received the Lifetime Achievement Award at the World Festival of Animated Film – Animafest Zagreb in 1988.

Jones' life and legacy were celebrated January 12, 2012, with the official grand opening of "The Chuck Jones Experience" at Circus Circus Las Vegas. Many of Jones' family welcomed celebrities, animation aficionados and visitors to the new attraction when they opened the attraction in an appropriate and unconventional way. Among those in attendance were Jones' widow, Marian Jones; daughter Linda Clough; and grandchildren Craig, Todd and Valerie Kausen.







</doc>
<doc id="7673" url="https://en.wikipedia.org/wiki?curid=7673" title="Costume">
Costume

Costume is the distinctive style of dress of an individual or group that reflects their class, gender, profession, ethnicity, nationality, activity or epoch.

The term also was traditionally used to describe typical appropriate clothing for certain activities, such as riding costume, swimming costume, dance costume, and evening costume. Appropriate and acceptable costume is subject to changes in fashion and local cultural norms.

This general usage has gradually been replaced by the terms "dress", "attire" or "wear" and usage of "costume" has become more limited to unusual or out-of-date clothing and to attire intended to evoke a change in identity, such as theatrical, Halloween, and mascot costumes.

Before the advent of ready-to-wear apparel, clothing was made by hand. When made for commercial sale it was made, as late as the beginning of the 20th century, by "costumiers", often women who ran businesses that met the demand for complicated or intimate female costume, including millinery and corsetry.

Costume comes from the same Italian word, inherited via French, which means fashion or custom.

National costume or regional costume expresses local (or exiled) identity and emphasizes a culture's unique attributes. They are often a source of national pride. Examples include the Scottish kilt or Japanese kimono.

In Bhutan there is a traditional national dress prescribed for men and women, including the monarchy. These have been in vogue for thousands of years and have developed into a distinctive dress style. The dress worn by men is known as Gho which is a robe worn up to knee-length and is fastened at the waist by a band called the Kera. The front part of the dress which is formed like a pouch, in olden days was used to hold baskets of food and short dagger, but now it is used to keep cell phone, purse and the betel nut called "Doma". The dress worn by women consist of three pieces known as Kira, Tego and Wonju. The long dress which extends up to the ankle is Kira. The jacket worn above this is Tego which is provided with Wonju, the inner jacket. However, while visiting the Dzong or monastery a long scarf or stoll, called Kabney is worn by men across the shoulder, in colours appropriate to their ranks. Women also wear scarfs or stolls called Rachus, made of raw silk with embroidery, over their shoulder but not indicative of their rank.

"Costume" often refers to a particular style of clothing worn to portray the wearer as a character or type of character at a social event in a theatrical performance on the stage or in film or television. In combination with other aspects of stagecraft, theatrical costumes can help actors portray characters' and their contexts as well as communicate information about the historical period/era, geographic location and time of day, season or weather of the theatrical performance. Some stylized theatrical costumes, such as Harlequin and Pantaloon in the Commedia dell'arte, exaggerate an aspect of a character.

The wearing of costumes is an important part of holidays developed from religious festivals such as Mardi Gras (in the lead up to Easter), and Halloween (related to All Hallow's Eve). Mardi Gras costumes usually take the form of jesters and other fantasy characters; Halloween costumes traditionally take the form of supernatural creatures such as ghosts, vampires, pop-culture icons and angels. In modern times. Christmas costumes typically portray characters such as Santa Claus (developed from Saint Nicholas). In Australia, the United Kingdom and the United States the American version of a Santa suit and beard is popular; in the Netherlands, the costume of Zwarte Piet is customary. Easter costumes are associated with the Easter Bunny or other animal costumes.

In Judaism, a common practice is to dress up on Purim. During this holiday, Jews celebrate the change of their destiny. They were delivered from being the victims of an evil decree against them and were instead allowed by the King to destroy their enemies. A quote from the Book of Esther, which says: "On the contrary" () is the reason that wearing a costume has become customary for this holiday.

Buddhist religious festivals in Tibet, Bhutan, Mongolia and Lhasa and Sikkim in India perform the Cham dance, which is a popular dance form utilising masks and costumes.

Parades and processions provide opportunities for people to dress up in historical or imaginative costumes. For example, in 1879 the artist Hans Makart designed costumes and scenery to celebrate the wedding anniversary of the Austro-Hungarian Emperor and Empress and led the people of Vienna in a costume parade that became a regular event until the mid-twentieth century. Uncle Sam costumes are worn on Independence Day in the United States. The Lion Dance, which is part of Chinese New Year celebrations, is performed in costume. Some costumes, such as the ones used in the Dragon Dance, need teams of people to create the required effect.

Public sporting events such as fun runs also provide opportunities for wearing costumes, as do private masquerade balls and fancy dress parties.

Costumes are popularly employed at sporting events, during which fans dress as their team's representative mascot to show their support. Businesses use mascot costumes to bring in people to their business either by placing their mascot in the street by their business or sending their mascot out to sporting events, festivals, national celebrations, fairs, and parades. Mascots appear at organizations wanting to raise awareness of their work. Children's Book authors create mascots from the main character to present at their book signings. Animal costumes that are visually very similar to mascot costumes are also popular among the members of the furry fandom, where the costumes are referred to as fursuits and match one's animal persona, or "fursona".

Costumes also serve as an avenue for children to explore and role-play. For example, children may dress up as characters from history or fiction, such as pirates, princesses, cowboys, or superheroes. They may also dress in uniforms used in common jobs, such as nurses, police officers, or firefighters, or as zoo or farm animals. Young boys tend to prefer costumes that reinforce stereotypical ideas of being male, and young girls tend to prefer costumes that reinforce stereotypical ideas of being female.

Cosplay, a word of Japanese origin that in English is short for "costume play", is a performance art in which participants wear costumes and accessories to represent a specific character or idea that is usually always identified with a unique name (as opposed to a generic word). These costume wearers often interact to create a subculture centered on role play, so they can be seen most often in play groups, or at a gathering or convention. A significant number of these costumes are homemade and unique, and depend on the character, idea, or object the costume wearer is attempting to imitate or represent. The costumes themselves are often artistically judged to how well they represent the subject or object that the costume wearer is attempting to contrive.

Costume design is the envisioning of clothing and the overall appearance of a character or performer. Costume may refer to the style of dress particular to a nation, a class, or a period. In many cases, it may contribute to the fullness of the artistic, visual world that is unique to a particular theatrical or cinematic production. The most basic designs are produced to denote status, provide protection or modesty, or provide visual interest to a character. Costumes may be for, but not limited to, theater, cinema, or musical performances. Costume design should not be confused with costume coordination, which merely involves altering existing clothing, although both processes are used to create stage clothes.

The Costume Designers Guild's international membership includes motion picture, television, and commercial costume designers, assistant costume designers and costume illustrators, and totals over 750 members.

"The Costume Designer" is a quarterly magazine devoted to the costume design industry.

Notable costume designers include recipients of the Academy Award for Best Costume Design, Tony Award for Best Costume Design, and Drama Desk Award for Outstanding Costume Design. Edith Head and Orry-Kelly, both of whom were born late in 1897, were two of Hollywood's most notable costume designers.

In the 20th century, contemporary fabric stores offered commercial patterns that could be bought and used to make a costume from raw materials. Some companies also began producing catalogs with great numbers of patterns.

More recently, and particularly with the advent of the Internet, the DIY movement has ushered in a new era of DIY costumes and pattern sharing. POPSUGAR is one such example, with several hundred designs readily available. YouTube, Pinterest, Mashable also feature many DIY costumes.

Professional-grade costumes are typically designed and produced by artisan crafters, often specifically for a particular character or setting. Specialty shops may also include common costumes of this caliber.

Some high-end costumes may even be designed by the costume's wearer.

The costume industry includes vendors such the American company Spirit Halloween, which opens consumer-oriented stores seasonally with pre-made Halloween costumes.



</doc>
<doc id="7674" url="https://en.wikipedia.org/wiki?curid=7674" title="Cable car (railway)">
Cable car (railway)

A cable car (cable tram elsewhere, apart from North America) is a type of cable railway used for mass transit where rail cars are hauled by a continuously moving cable running at a constant speed. Individual cars stop and start by releasing and gripping this cable as required. Cable cars are distinct from funiculars, where the cars are permanently attached to the cable, and cable railways, which are similar to funiculars, but where the rail vehicles are attached and detached manually.
The first cable-operated railway, employing a moving rope that could be picked up or released by a grip on the cars was the Fawdon Wagonway in 1826, a Colliery railway line. The London and Blackwall Railway, which opened for passengers in east London, England, in 1840 used such a system. The rope available at the time proved too susceptible to wear and the system was abandoned in favour of steam locomotives after eight years. In America, the first cable car installation in operation probably was the West Side and Yonkers Patent Railway in New York City, as its first-ever elevated railway which ran from 1 July 1868 to 1870. The cable technology used in this elevated railway involved collar-equipped cables and claw-equipped cars, proving cumbersome. The line was closed and rebuilt, reopening with steam locomotives.

In 1869 P. G. T. Beauregard demonstrated a cable car at New Orleans and was issued .

Other cable cars to use grips were those of the Clay Street Hill Railroad, which later became part of the San Francisco cable car system. The building of this line was promoted by Andrew Smith Hallidie with design work by William Eppelsheimer, and it was first tested in 1873. The success of these grips ensured that this line became the model for other cable car transit systems, and this model is often known as the "Hallidie Cable Car".

In 1881 the Dunedin cable tramway system opened in Dunedin, New Zealand and became the first such system outside San Francisco. For Dunedin, George Smith Duncan further developed the Hallidie model, introducing the pull curve and the slot brake; the former was a way to pull cars through a curve, since Dunedin's curves were too sharp to allow coasting, while the latter forced a wedge down into the cable slot to stop the car. Both of these innovations were generally adopted by other cities, including San Francisco.

In Australia, the Melbourne cable tramway system operated from 1885 to 1940. It was one of the most extensive in the world with 1200 trams and trailers operating over 15 routes with 103 km (64 miles) of track. Sydney also had a couple of cable tram routes.

Cable cars rapidly spread to other cities, although the major attraction for most was the ability to displace horsecar (or mule-drawn) systems rather than the ability to climb hills. Many people at the time viewed horse-drawn transit as unnecessarily cruel, and the fact that a typical horse could work only four or five hours per day necessitated the maintenance of large stables of draft animals that had to be fed, housed, groomed, medicated and rested. Thus, for a period, economics worked in favour of cable cars even in relatively flat cities.

For example, the Chicago City Railway, also designed by Eppelsheimer, opened in Chicago in 1882 and went on to become the largest and most profitable cable car system. As with many cities, the problem in flat Chicago was not one of incline, but of transportation capacity. This caused a different approach to the combination of grip car and trailer. Rather than using a grip car and single trailer, as many cities did, or combining the grip and trailer into a single car, like San Francisco's "California Cars", Chicago used grip cars to pull trains of up to three trailers.

In 1883 the New York and Brooklyn Bridge Railway was opened, which had a most curious feature: though it was a cable car system, it used steam locomotives to get the cars into and out of the terminals. After 1896 the system was changed to one on which a motor car was added to each train to maneuver at the terminals, while en route, the trains were still propelled by the cable.
On 25 September 1883, a test of a cable car system was held by Liverpool United Tramways and Omnibus Company in Kirkdale, Liverpool. This would have been the first cable car system in Europe, but the company decided against implementing it. Instead, the distinction went to the 1884 route from Archway to Highgate, north London, which used a continuous cable and grip system on the 1 in 11 (9%) climb of Highgate Hill. The installation was not reliable and was replaced by electric traction in 1909. Other cable car systems were implemented in Europe, though, among which was the Glasgow District Subway, the first underground cable car system, in 1896. (London, England's first deep-level tube railway, the City & South London Railway, had earlier also been built for cable haulage but had been converted to electric traction before opening in 1890.) A few more cable car systems were built in the United Kingdom, Portugal, and France. European cities, having many more curves in their streets, were ultimately less suitable for cable cars than American cities.

Though some new cable car systems were still being built, by 1890 the cheaper to construct and simpler to operate electrically-powered trolley or tram started to become the norm, and eventually started to replace existing cable car systems. For a while hybrid cable/electric systems operated, for example in Chicago where electric cars had to be pulled by grip cars through the loop area, due to the lack of trolley wires there. Eventually, San Francisco became the only street-running manually operated system to survive—Dunedin, the second city with such cars, was also the second-last city to operate them, closing down in 1957.

In the last decades of the 20th-century, cable traction in general has seen a limited revival as automatic people movers, used in resort areas, airports (for example, Toronto Airport), huge hospital centers and some urban settings. While many of these systems involve cars permanently attached to the cable, the Minimetro system from Poma/Leitner Group and the Cable Liner system from DCC Doppelmayr Cable Car both have variants that allow the cars to be automatically decoupled from the cable under computer control, and can thus be considered a modern interpretation of the cable car.

The cable is itself powered by a stationary motor or engine situated in a cable house or power house. The speed at which it moves is relatively constant depending on the number of units gripping the cable at any given time.

The cable car begins moving when a clamping device attached to the car, called a "grip", applies pressure to ("grips") the moving cable. Conversely, the car is stopped by releasing pressure on the cable (with or without completely detaching) and applying the brakes. This gripping and releasing action may be manual, as was the case in all early cable car systems, or automatic, as is the case in some recent cable operated people mover type systems. Gripping must be an even and gradual process in order to avoid bringing the car to cable speed too quickly and unacceptably jarring the passengers.

In the case of manual systems, the grip resembles a very large pair of pliers, and considerable strength and skill are required to operate the car. As many early cable car operators discovered the hard way, if the grip is not applied properly, it can damage the cable, or even worse, become entangled in the cable. In the latter case, the cable car may not be able to stop and can wreak havoc along its route until the cable house realizes the mishap and halts the cable.

One apparent advantage of the cable car is its relative energy efficiency. This is due to the economy of centrally located power stations, and the ability of descending cars to transfer energy to ascending cars. However, this advantage is totally negated by the relatively large energy consumption required to simply move the cable over and under the numerous guide rollers and around the many sheaves. Approximately 95% of the tractive effort in the San Francisco system is expended in simply moving the four cables at 9.5 miles per hour. Electric cars with regenerative braking do offer the advantages, without the problem of moving a cable. In the case of steep grades, however, cable traction has the major advantage of not depending on adhesion between wheels and rails. There is also the obvious advantage that keeping the car gripped to the cable will also limit the downhill speed to that of the cable.

Because of the constant and relatively low speed, a cable car's potential to cause harm in an accident can be underestimated. Even with a cable car traveling at only 9 miles per hour, the mass of the cable car and the combined strength and speed of the cable can do quite a lot of damage in a collision.

A cable car is superficially similar to a funicular, but differs from such a system in that its cars are not permanently attached to the cable and can stop independently, whereas a funicular has cars that are permanently attached to the propulsion cable, which is itself stopped and started. A cable car cannot climb as steep a grade as a funicular, but many more cars can be operated with a single cable, making it more flexible, and allowing a higher capacity. During the rush hour on San Francisco's Market Street Railway, a car would leave the terminal every 15 seconds.

A few funicular railways operate in street traffic, and because of this operation are often incorrectly described as cable cars. Examples of such operation, and the consequent confusion, are:

Even more confusingly, a hybrid cable car/funicular line once existed in the form of the original Wellington Cable Car, in the New Zealand city of Wellington. This line had both a continuous loop haulage cable that the cars gripped using a cable car gripper, and a balance cable permanently attached to both cars over an undriven pulley at the top of the line. The descending car gripped the haulage cable and was pulled downhill, in turn pulling the ascending car (which remained ungripped) uphill by the balance cable. This line was rebuilt in 1979 and is now a standard funicular, although it retains its old cable car name.

The best known existing cable car system is the San Francisco cable car system in the city of San Francisco, California. San Francisco's cable cars constitute the oldest and largest such system in permanent operation, and it is the only one to still operate in the traditional manner with manually operated cars running in street traffic.

Several cities operate a modern version of the cable car system. These systems are fully automated and run on their own reserved right of way. They are commonly referred to as people movers, although that term is also applied to systems with other forms of propulsion, including funicular style cable propulsion.

These cities include:










8th St. Tunnel in use (1887–1956)


Information

Patents


</doc>
<doc id="7676" url="https://en.wikipedia.org/wiki?curid=7676" title="Creaky voice">
Creaky voice

In linguistics, creaky voice (sometimes called laryngealisation, pulse phonation, vocal fry, or glottal fry) is a special kind of phonation in which the arytenoid cartilages in the larynx are drawn together; as a result, the vocal folds are compressed rather tightly, becoming relatively slack and compact. They normally vibrate irregularly at 20–50 pulses per second, about two octaves below the frequency of normal voicing, and the airflow through the glottis is very slow. Although creaky voice may occur with very low pitch, as at the end of a long intonation unit, it can also occur with a higher pitch.

Creaky voice is prevalent as a peer-group affectation among young women in the United States. For example, researcher Ikuko Patricia Yuasa suggests that the tendency is a product of young women trying to infuse their speech with gravitas by means of reaching for the male register and found that "college-age Americans [...] perceive female creaky voice as hesitant, nonaggressive, and informal but also educated, urban-oriented, and upwardly mobile." However, according to a 2012 study in "PLOS ONE", young women using creaky voice are viewed as less competent, less educated, less trustworthy, less attractive and less employable. Some suggest that creaky voice can function as a marker of parentheticals in conversations; creaky voice may indicate that certain phrases, when uttered with creaky voice, contain less central information. 

In some languages, such as Jalapa Mazatec, creaky voice has a phonemic status; that is, the presence or absence of creaky voice can change the meaning of a word. In the International Phonetic Alphabet, creaky voice of a phone is represented by a diacritical tilde , for example . The Danish prosodic feature "stød" is an example of a form of laryngealisation that has a phonemic function.

A slight degree of laryngealisation, occurring in some Korean consonants for example, is called "stiff voice".




</doc>
<doc id="7677" url="https://en.wikipedia.org/wiki?curid=7677" title="Computer monitor">
Computer monitor

A computer monitor is an output device which displays information in pictorial form. A monitor usually comprises the display device, circuitry, casing, and power supply. The display device in modern monitors is typically a thin film transistor liquid crystal display (TFT-LCD) with LED backlighting having replaced cold-cathode fluorescent lamp (CCFL) backlighting. Older monitors used a cathode ray tube (CRT). Monitors are connected to the computer via VGA, Digital Visual Interface (DVI), HDMI, DisplayPort, Thunderbolt, low-voltage differential signaling (LVDS) or other proprietary connectors and signals.

Originally, computer monitors were used for data processing while television receivers were used for entertainment. From the 1980s onwards, computers (and their monitors) have been used for both data processing and entertainment, while televisions have implemented some computer functionality. The common aspect ratio of televisions, and computer monitors, has changed from 4:3 to 16:10, to 16:9.

Modern computer monitors are easily interchangeable with conventional television sets. However, as computer monitors do not necessarily include components such as a television tuner and speakers, it may not be possible to use a computer monitor as a television without external components.

Early electronic computers were fitted with a panel of light bulbs where the state of each particular bulb would indicate the on/off state of a particular register bit inside the computer. This allowed the engineers operating the computer to monitor the internal state of the machine, so this panel of lights came to be known as the 'monitor'. As early monitors were only capable of displaying a very limited amount of information and were very transient, they were rarely considered for program output. Instead, a line printer was the primary output device, while the monitor was limited to keeping track of the program's operation.

As technology developed engineers realized that the output of a CRT display was more flexible than a panel of light bulbs and eventually, by giving control of what was displayed in the program itself, the monitor itself became a powerful output device in its own right.

Computer monitors were formerly known as visual display units (VDU), but this term had mostly fallen out of use by the 1990s.

Multiple technologies have been used for computer monitors. Until the 21st century most used cathode ray tubes but they have largely been superseded by LCD monitors.

The first computer monitors used cathode ray tubes (CRTs). Prior to the advent of home computers in the late 1970s, it was common for a video display terminal (VDT) using a CRT to be physically integrated with a keyboard and other components of the system in a single large chassis. The display was monochrome and far less sharp and detailed than on a modern flat-panel monitor, necessitating the use of relatively large text and severely limiting the amount of information that could be displayed at one time. High-resolution CRT displays were developed for the specialized military, industrial and scientific applications but they were far too costly for general use.

Some of the earliest home computers (such as the TRS-80 and Commodore PET) were limited to monochrome CRT displays, but color display capability was already a standard feature of the pioneering Apple II, introduced in 1977, and the specialty of the more graphically sophisticated Atari 800, introduced in 1979. Either computer could be connected to the antenna terminals of an ordinary color TV set or used with a purpose-made CRT color monitor for optimum resolution and color quality. Lagging several years behind, in 1981 IBM introduced the Color Graphics Adapter, which could display four colors with a resolution of 320 x 200 pixels, or it could produce 640 x 200 pixels with two colors. In 1984 IBM introduced the Enhanced Graphics Adapter which was capable of producing 16 colors and had a resolution of 640 x 350.

By the end of the 1980s color CRT monitors that could clearly display 1024 x 768 pixels were widely available and increasingly affordable. During the following decade, maximum display resolutions gradually increased and prices continued to fall. CRT technology remained dominant in the PC monitor market into the new millennium partly because it was cheaper to produce and offered to view angles close to 180 degrees. CRTs still offer some image quality advantages over LCDs but improvements to the latter have made them much less obvious. The dynamic range of early LCD panels was very poor, and although text and other motionless graphics were sharper than on a CRT, an LCD characteristic known as pixel lag caused moving graphics to appear noticeably smeared and blurry.

There are multiple technologies that have been used to implement liquid crystal displays (LCD). Throughout the 1990s, the primary use of LCD technology as computer monitors was in laptops where the lower power consumption, lighter weight, and smaller physical size of LCD's justified the higher price versus a CRT. Commonly, the same laptop would be offered with an assortment of display options at increasing price points: (active or passive) monochrome, passive color, or active matrix color (TFT). As volume and manufacturing capability have improved, the monochrome and passive color technologies were dropped from most product lines.

TFT-LCD is a variant of LCD which is now the dominant technology used for computer monitors.

The first standalone LCDs appeared in the mid-1990s selling for high prices. As prices declined over a period of years they became more popular, and by 1997 were competing with CRT monitors. Among the first desktop LCD computer monitors was the Eizo L66 in the mid-1990s, the Apple Studio Display in 1998, and the Apple Cinema Display in 1999. In 2003, TFT-LCDs outsold CRTs for the first time, becoming the primary technology used for computer monitors. The main advantages of LCDs over CRT displays are that LCD's consume less power, take up much less space, and are considerably lighter. The now common active matrix TFT-LCD technology also has less flickering than CRTs, which reduces eye strain. On the other hand, CRT monitors have superior contrast, have a superior response time, are able to use multiple screen resolutions natively, and there is no discernible flicker if the refresh rate is set to a sufficiently high value. LCD monitors have now very high temporal accuracy and can be used for vision research.

High dynamic range (HDR) has been implemented into high-end LCD monitors to improve color accuracy. Since around the late 2000s, widescreen LCD monitors have become popular, in part due to television series, motion pictures and video games transitioning to high-definition (HD), which makes standard-width monitors unable to display them correctly as they either stretch or crop HD content. These types of monitors may also display it in the proper width, however they usually fill the extra space at the top and bottom of the image with black bars. Other advantages of widescreen monitors over standard-width monitors is that they make work more productive by displaying more of a user's documents and images, and allow displaying toolbars with documents. They also have a larger viewing area, with a typical widescreen monitor having a 16:9 aspect ratio, compared to the 4:3 aspect ratio of a typical standard-width monitor.

Organic light-emitting diode (OLED) monitors provide higher contrast and better viewing angles than LCDs but they require more power when displaying documents with white or bright backgrounds.

The performance of a monitor is measured by the following parameters:

On two-dimensional display devices such as computer monitors the display size or view able image size is the actual amount of screen space that is available to display a picture, video or working space, without obstruction from the case or other aspects of the unit's design. The main measurements for display devices are: width, height, total area and the diagonal.

The size of a display is usually by monitor manufacturers given by the diagonal, i.e. the distance between two opposite screen corners. This method of measurement is inherited from the method used for the first generation of CRT television, when picture tubes with circular faces were in common use. Being circular, it was the external diameter of the glass envelope that described their size. Since these circular tubes were used to display rectangular images, the diagonal measurement of the rectangular image was smaller than the diameter of the tube's face (due to the thickness of the glass). This method continued even when cathode ray tubes were manufactured as rounded rectangles; it had the advantage of being a single number specifying the size, and was not confusing when the aspect ratio was universally 4:3.

With the introduction of flat panel technology, the diagonal measurement became the actual diagonal of the visible display. This meant that an eighteen-inch LCD had a larger visible area than an eighteen-inch cathode ray tube.

The estimation of the monitor size by the distance between opposite corners does not take into account the display aspect ratio, so that for example a 16:9 widescreen display has less area, than a 4:3 screen. The 4:3 screen has dimensions of and area , while the widescreen is , .

Until about 2003, most computer monitors had a aspect ratio and some had . Between 2003 and 2006, monitors with and mostly (8:5) aspect ratios became commonly available, first in laptops and later also in standalone monitors. Reasons for this transition was productive uses for such monitors, i.e. besides widescreen computer game play and movie viewing, are the word processor display of two standard letter pages side by side, as well as CAD displays of large-size drawings and CAD application menus at the same time. In 2008 16:10 became the most common sold aspect ratio for LCD monitors and the same year 16:10 was the mainstream standard for laptops and notebook computers.

In 2010 the computer industry started to move over from to because 16:9 was chosen to be the standard high-definition television display size, and because they were cheaper to manufacture.

In 2011 non-widescreen displays with 4:3 aspect ratios were only being manufactured in small quantities. According to Samsung this was because the "Demand for the old 'Square monitors' has decreased rapidly over the last couple of years," and "I predict that by the end of 2011, production on all 4:3 or similar panels will be halted due to a lack of demand."

The resolution for computer monitors has increased over time. From 320x200 during the early 1980s, to 1024x768 during the late 1990s. Since 2009, the most commonly sold resolution for computer monitors is 1920x1080. Before 2013 top-end consumer LCD monitors were limited to 2560x1600 at , excluding Apple products and CRT monitors. Apple introduced 2880x1800 with Retina MacBook Pro at on June 12, 2012, and introduced a 5120x2880 Retina iMac at on October 16, 2014. By 2015 most major display manufacturers had released 3840x2160 resolution displays.

Most modern monitors will switch to a power-saving mode if no video-input signal is received. This allows modern operating systems to turn off a monitor after a specified period of inactivity. This also extends the monitor's service life.

Some monitors will also switch themselves off after a time period on standby.

Most modern laptops provide a method of screen dimming after periods of inactivity or when the battery is in use. This extends battery life and reduces wear.

Many monitors have other accessories (or connections for them) integrated. This places standard ports within easy reach and eliminates the need for another separate hub, camera, microphone, or set of speakers. These monitors have advanced microprocessors which contain codec information, Windows Interface drivers and other small software which help in proper functioning of these functions.

Some displays, especially newer LCD monitors, replace the traditional anti-glare matte finish with a glossy one. This increases color saturation and sharpness but reflections from lights and windows are very visible. Anti-reflective coatings are sometimes applied to help reduce reflections, although this only mitigates the effect.

In about 2009, NEC/Alienware together with Ostendo Technologies (based in Carlsbad, CA) were offering a curved (concave) monitor that allows better viewing angles near the edges, covering 75% of peripheral vision. This monitor had 2880x900 resolution, LED backlight and was marketed as suitable both for gaming and office work, while for $6499 it was rather expensive. While this particular monitor is no longer in production, most PC manufacturers now offer some sort of curved desktop display.

Narrow viewing angle screens are used in some security conscious applications.

Newer monitors are able to display a different image for each eye, often with the help of special glasses, giving the perception of depth. An autostereoscopic screen can generate 3D images without headgear.

These monitors use touching of the screen as an input method. Items can be selected or moved with a finger, and finger gestures may be used to convey commands. The screen will need frequent cleaning due to image degradation from fingerprints.

A combination of a monitor with a graphics tablet. Such devices are typically unresponsive to touch without the use of one or more special tools' pressure. Newer models however are now able to detect touch from any pressure and often have the ability to detect tilt and rotation as well.

Touch and tablet screens are used on LCDs as a substitute for the light pen, which can only work on CRTs.

Monitors that feature an aspect ratio of 21:9 as opposed to the more common 16:9.

Computer monitors are provided with a variety of methods for mounting them depending on the application and environment.

A desktop monitor is typically provided with a stand from the manufacturer which lifts the monitor up to a more ergonomic viewing height. The stand may be attached to the monitor using a proprietary method or may use, or be adaptable to, a Video Electronics Standards Association, VESA, standard mount. Using a VESA standard mount allows the monitor to be used with an after-market stand once the original stand is removed. Stands may be fixed or offer a variety of features such as height adjustment, horizontal swivel, and landscape or portrait screen orientation.

The Flat Display Mounting Interface (FDMI), also known as VESA Mounting Interface Standard (MIS) or colloquially as a VESA mount, is a family of standards defined by the Video Electronics Standards Association for mounting flat panel monitors, TVs, and other displays to stands or wall mounts. It is implemented on most modern flat-panel monitors and TVs.

For Computer Monitors, the VESA Mount typically consists of four threaded holes on the rear of the display that will mate with an adapter bracket.

Rack mount computer monitors are available in two styles and are intended to be mounted into a 19-inch rack:
A fixed rack mount monitor is mounted directly to the rack with the LCD visible at all times. The height of the unit is measured in rack units (RU) and 8U or 9U are most common to fit 17-inch or 19-inch LCDs. The front sides of the unit are provided with flanges to mount to the rack, providing appropriately spaced holes or slots for the rack mounting screws. A 19-inch diagonal LCD is the largest size that will fit within the rails of a 19-inch rack. Larger LCDs may be accommodated but are 'mount-on-rack' and extend forward of the rack. There are smaller display units, typically used in broadcast environments, which fit multiple smaller LCDs side by side into one rack mount.
A stowable rack mount monitor is 1U, 2U or 3U high and is mounted on rack slides allowing the display to be folded down and the unit slid into the rack for storage. The display is visible only when the display is pulled out of the rack and deployed. These units may include only a display or may be equipped with a keyboard creating a KVM (Keyboard Video Monitor). Most common are systems with a single LCD but there are systems providing two or three displays in a single rack mount system.

A panel mount computer monitor is intended for mounting into a flat surface with the front of the display unit protruding just slightly. They may also be mounted to the rear of the panel. A flange is provided around the LCD, sides, top and bottom, to allow mounting. This contrasts with a rack mount display where the flanges are only on the sides. The flanges will be provided with holes for thru-bolts or may have studs welded to the rear surface to secure the unit in the hole in the panel. Often a gasket is provided to provide a water-tight seal to the panel and the front of the LCD will be sealed to the back of the front panel to prevent water and dirt contamination.

An open frame monitor provides the LCD monitor and enough supporting structure to hold associated electronics and to minimally support the LCD. Provision will be made for attaching the unit to some external structure for support and protection. Open frame LCDs are intended to be built into some other piece of equipment. An arcade video game would be a good example with the display mounted inside the cabinet. There is usually an open frame display inside all end-use displays with the end-use display simply providing an attractive protective enclosure. Some rack mount LCD manufacturers will purchase desk-top displays, take them apart, and discard the outer plastic parts, keeping the inner open-frame LCD for inclusion into their product.

According to an NSA document leaked to Der Spiegel, the NSA sometimes swaps the monitor cables on targeted computers with a bugged monitor cable in order to allow the NSA to remotely see what's displayed on the targeted computer monitor.

Van Eck phreaking is the process of remotely displaying the contents of a CRT or LCD by detecting its electromagnetic emissions. It is named after Dutch computer researcher Wim van Eck, who in 1985 published the first paper on it, including proof of concept. Phreaking is the process of exploiting telephone networks, used here because of its connection to eavesdropping.



</doc>
<doc id="7681" url="https://en.wikipedia.org/wiki?curid=7681" title="ClearType">
ClearType

ClearType is Microsoft's implementation of subpixel rendering technology in rendering text in a font system. ClearType attempts to improve the appearance of text on certain types of computer display screens by sacrificing color fidelity for additional intensity variation. This trade-off is asserted to work well on LCD flat panel monitors.

ClearType was first announced at the November 1998 COMDEX exhibition. The technology was first introduced in software in January 2000 as an always-on feature of Microsoft Reader, which was released to the public in August 2000.

ClearType was significantly changed with the introduction of DirectWrite in Windows 7.

Word 2013 stopped using ClearType, because "There is a problem with ClearType: it depends critically on the color of the background pixels."

Computer displays where the positions of individual pixels are permanently fixed such as most modern flat panel displays can show saw-tooth edges when displaying small, high-contrast graphic elements, such as text. ClearType uses spatial "anti"-aliasing at the subpixel level to reduce visible artifacts on such displays when text is rendered, making the text appear "smoother" and less jagged. ClearType also uses very heavy font hinting to force the font to fit into the pixel grid. This increases edge contrast and readability of small fonts at the expense of font rendering fidelity and has been criticized by graphic designers for making different fonts look similar.

Like most other types of subpixel rendering, ClearType involves a compromise, sacrificing one aspect of image quality (color or "chrominance" detail) for another (light and dark or "luminance" detail). The compromise can improve text appearance when luminance detail is more important than chrominance.

Only user and system applications render the application of ClearType. ClearType does not alter other graphic display elements (including text already in bitmaps). For example, ClearType enhancement renders text on the screen in Microsoft Word, but text placed in a bitmapped image in a program such as Adobe Photoshop is not. In theory, the method (called "RGB Decimation" internally) can enhance the anti-aliasing of any digital image.

ClearType is not used when printing text. Most printers already use such small pixels that aliasing is rarely a problem, and they don't have the addressable fixed subpixels ClearType requires. Nor does ClearType affect text stored in files. ClearType only applies any processing to the text while it is being rendered onto the screen.

ClearType was invented in the Microsoft e-Books team by Bert Keely and Greg Hitchcock. It was then analyzed by researchers in the company, and signal processing expert John Platt designed an improved version of the algorithm. Dick Brass, a Vice President at Microsoft from 1997 to 2004, complained that the company was slow in moving ClearType to market in the portable computing field.

Normally, the software in a computer treats the computer’s display screen as a rectangular array of square, indivisible pixels, each of which has an intensity and color that are determined by the blending of three primary colors: red, green, and blue. However, actual display hardware usually implements each pixel as a group of three adjacent, independent "subpixels," each of which displays a different primary color. Thus, on a real computer display, each pixel is actually composed of separate red, green, and blue subpixels. For example, if a flat-panel display is examined under a magnifying glass, the pixels may appear as follows:

In the illustration above, there are nine pixels but 27 subpixels.

If the computer controlling the display knows the exact position and color of all the subpixels on the screen, it can take advantage of this to improve the apparent resolution in certain situations. If each pixel on the display actually contains three rectangular subpixels of red, green, and blue, in that fixed order, then things on the screen that are smaller than one full pixel in size can be rendered by lighting only one or two of the subpixels. For example, if a diagonal line with a width smaller than a full pixel must be rendered, then this can be done by lighting only the subpixels that the line actually touches. If the line passes through the leftmost portion of the pixel, only the red subpixel is lit; if it passes through the rightmost portion of the pixel, only the blue subpixel is lit. This effectively triples the horizontal resolution of the image at normal viewing distances; the drawback is that the line thus drawn will show color fringes (at some points it might look green, at other points it might look red or blue).

ClearType uses this method to improve the smoothness of text. When the elements of a type character are smaller than a full pixel, ClearType lights only the appropriate subpixels of each full pixel in order to more closely follow the outlines of that character. Text rendered with ClearType looks “smoother” than text rendered without it, provided that the pixel layout of the display screen exactly matches what ClearType expects.

The following picture shows a 4× enlargement of the word "Wikipedia" rendered using ClearType. The word was originally rendered using a Times New Roman 12 pt font.

In this magnified view, it becomes clear that, while the overall smoothness of the text seems to improve, there is also color fringing of the text.

An extreme close-up of a color display shows (a) text rendered without ClearType and (b) text rendered with ClearType. Note the changes in subpixel intensity that are used to increase effective resolution when ClearType is enabled without ClearType, all sub-pixels of a given pixel have the same intensity.

In the above lines of text, when the orange circle is shown, all the text in the frame is rendered using ClearType (RGB subpixel rendering); when the orange circle is absent all the text is rendered using normal (full pixel greyscale) anti-aliasing.

ClearType and similar technologies work on the theory that variations in intensity are more noticeable than variations in color.

In a MSDN article, Microsoft acknowledges that "[te]xt that is rendered with ClearType can also appear significantly different when viewed by individuals with varying levels of color sensitivity. Some individuals can detect slight differences in color better than others." This opinion is shared by font designer Thomas Phinney (Vice President of FontLab and formerly with Adobe Systems): "There is also considerable variation between individuals in their sensitivity to color fringing. Some people just notice it and are bothered by it a lot more than others." Software developer Melissa Elliot has written about finding ClearType rendering uncomfortable to read, saying that "instead of seeing black text, I see blue text, and rendered over it but offset by a pixel or two, I see orange text, and someone reached into a bag of purple pixel glitter and just tossed it on...I’m not the only person in the world with this problem, and yet, every time it comes up, people are quick to assure me it works for them as if that’s supposed to make me feel better."

Hinting expert Beat Stamm, who worked on ClearType at Microsoft, agrees that ClearType may look blurry at 96 dpi, which was a typical resolution for LCDs in 2008, but adds that higher resolution displays improve on this aspect: "WPF [Windows Presentation Foundation] uses method C [ClearType with fractional pixel positioning], but few display devices have a sufficiently high resolution to make the potential blur a moot point for everybody. . . . Some people are ok with the blur in Method C, some aren’t. Anecdotal evidence suggests that some people are fine with Method C when reading continuous text at 96 dpi (e.g. Times Reader, etc.) but not in UI scenarios. Many people are fine with the colors of ClearType, even at 96 dpi, but a few aren’t… To my eyes and at 96 dpi, Method C doesn’t read as well as Method A. It reads “blurrily” to me. Conversely, at 144 dpi, I don’t see a problem with Method C. It looks and reads just fine to me." One illustration of the potential problem is the following image:

In the above block of text, the same portion of text is shown in the upper half without and in the lower half with ClearType rendering (as opposed to Standard and ClearType in the previous image). This and the previous example with the orange circle demonstrate the blurring introduced.

A 2001 study, conducted by researchers from Clemson University and The University of Pennsylvania on "18 users who spent 60 minutes reading fiction from each of three different displays" found that "When reading from an LCD display, users preferred text rendered with ClearType™. ClearType also yielded higher readability judgments and lower ratings of mental fatigue." A 2002 study on 24 users conducted by the same researchers from Clemson University also found that "Participants were significantly more accurate at identifying words with ClearType™ than without ClearType™."

According to a 2006 study, at the University of Texas at Austin by Dillon et al., ClearType "may not be universally beneficial". The study notes that maximum benefit may be seen when the information worker is spending large proportions of their time reading text (which is not necessarily the case for the majority of computer users today). Additionally, over one third of the study participants experienced some disadvantage when using ClearType. Whether ClearType, or other rendering, should be used is very subjective and it must be the choice of the individual, with the report recommending "to allow users to disable [ClearType] if they find it produces effects other than improved performance".

Another 2007 empirical study, found that "while ClearType rendering does not improve text legibility, reading speed or comfort compared to perceptually-tuned grayscale rendering, subjects prefer text with moderate ClearType rendering to text with grayscale or higher-level ClearType contrast."

A 2007 survey, of the literature by Microsoft researcher Kevin Larson presented a different picture: "Peer-reviewed studies have consistently found that using ClearType boosts reading performance compared with other text-rendering systems. In a 2004 study, for instance, Lee Gugerty, a psychology professor at Clemson University, in South Carolina, measured a 17 percent improvement in word recognition accuracy with ClearType. Gugerty’s group also showed, in a sentence comprehension study, that ClearType boosted reading speed by 5 percent and comprehension by 2 percent. Similarly, in a study published in 2007, psychologist Andrew Dillon at the University of Texas at Austin found that when subjects were asked to scan a spreadsheet and pick out certain information, they did those tasks 7 percent faster with ClearType."

ClearType and allied technologies require display hardware with fixed pixels and subpixels. More precisely, the positions of the pixels and subpixels on the screen must be exactly known to the computer to which it is connected. This is the case for flat-panel displays, on which the positions of the pixels are permanently fixed by the design of the screen itself. Almost all flat panels have a perfectly rectangular array of square pixels, each of which contains three rectangular subpixels in the three primary colors, with the normal ordering being red, green, and blue, arranged in vertical bands. ClearType assumes this arrangement of pixels when rendering text.

ClearType does not work properly with flat-panel displays that are operated at resolutions other than their “native” resolutions, since only the native resolution corresponds exactly to the actual positions of pixels on the screen of the display.

If a display does not have the type of fixed pixels that ClearType expects, text rendered with ClearType enabled actually looks worse than type rendered without it. Some flat panels have unusual pixel arrangements, with the colors in a different order, or with the subpixels positioned differently (in three horizontal bands, or in other ways). ClearType needs to be manually tuned for use with such displays (see below).

ClearType will not work as intended on displays that have no fixed pixel positions, such as CRT displays, however it will still have some antialiasing effect and may be preferable to some users as compared to non-anti-aliased type.

Because ClearType utilizes the physical layout of the red, green and blue pigments of the LCD screen, it is sensitive to the orientation of the display.

ClearType in Windows XP currently supports the RGB and BGR sub pixel structures. Rotated displays, in which the subpixels are arranged vertically rather than horizontally, are "not" currently supported. Using ClearType on these display configurations will actually reduce the display quality. The best option for users of Windows XP having rotated LCD displays (Tablet PCs or swivel-stand LCD displays) is using regular anti-aliasing, or switching off font-smoothing altogether.

The software developer documentation for Windows CE states that ClearType for rotated screens is supported on that platform.

Vertical sub pixel structures are not supported in Windows XP.


ClearType is also an integrated component of the Windows Presentation Foundation text-rendering engine.

ClearType can be globally enabled or disabled for GDI applications. A control panel applet is available to let the users tune the GDI ClearType settings. The GDI implementation of ClearType does not support sub-pixel positioning.

Some versions of Microsoft Windows, as supplied, allow ClearType to be turned on or off, with no adjustment; other versions allow tuning of the ClearType parameters. A Microsoft ClearType tuner utility is available for free download for Windows versions lacking this facility. If ClearType is disabled in the operating system, applications with their own ClearType controls can still support it. Microsoft Reader (for e-books) has its own ClearType tuner.

All text in Windows Presentation Foundation is anti-aliased and rendered using ClearType. There are separate ClearType registry settings for GDI and WPF applications, but by default the WPF entries are absent, and the GDI values are used in their absence. WPF registry entries can be tuned using the instructions from the MSDN WPF Text Blog.

ClearType in WPF supports sub-pixel positioning, natural advance widths, Y-direction anti-aliasing and hardware acceleration. WPF supports aggressive caching of pre-rendered ClearType text in video memory. The extent to which this is supported is dependent on the video card. DirectX 10 cards will be able to cache the font glyphs in video memory, then perform the composition (assembling of character glyphs in the correct order, with the correct spacing), alpha blending (application of anti-aliasing), and RGB blending (ClearType's sub-pixel color calculations), entirely in hardware. This means that only the original glyphs need to be stored in video memory once per font (Microsoft estimates that this would require 2 MB of video memory per font), and other operations such as the display of anti-aliased text on top of other graphics including video can also be done with no computation effort on the part of the CPU. DirectX 9 cards will only be able to cache the alpha-blended glyphs in memory, thus requiring the CPU to handle glyph composition and alpha-blending before passing this to the video card. Caching these partially rendered glyphs requires significantly more memory (Microsoft estimates 5 MB per process). Cards that don't support DirectX 9 have no hardware-accelerated text rendering capabilities.

The font rendering engine in DirectWrite supports an improved version of ClearType, as demonstrated at PDC 2008. The improved version is sometimes called "Natural ClearType". The improvements have been confirmed by independent sources, such as Firefox developers; they were particularly noticeable for OpenType fonts in Compact Font Format (CFF).

Despite these improvements, Word 2013 stopped using ClearType. The reasons invoked are, in the words of Murray Sargent: "There is a problem with ClearType: it depends critically on the color of the background pixels. This isn’t a problem if you know a priori that those pixels are white, which is usually the case for text. But the general case involves calculating what the colors should be for an arbitrary background and that takes time. Meanwhile, Word 2013 enjoys cool animations and smooth zooming. Nothing jumps any more. Even the caret (the blinking vertical line at the text insertion point) glides from one position to the next as you type. Jerking movement just isn’t considered cool any more. Well animations and zooms have to be faster than human response times in order to appear smooth. And that rules out ClearType in animated scenarios at least with present generation hardware. And in future scenarios, screens will have sufficiently high resolution that gray-scale anti-aliasing should suffice."

For the same reasons related to animation performance, the color-aware version of ClearType was abandoned in Metro and the Windows 8 (and 10) start menus.

ClearType is a registered trademark and Microsoft claims protection under the following U.S. patents:


The ClearType name was also used to refer to the screens of Microsoft Surface tablets. ClearType HD Display includes a 1366×768 screen, while ClearType Full HD Display includes a 1920×1080 screen.




</doc>
<doc id="7682" url="https://en.wikipedia.org/wiki?curid=7682" title="Centriole">
Centriole

In cell biology a centriole is a cylindrical cellular organelle composed mainly of a protein called tubulin. Centrioles are found in most eukaryotic cells. A bound pair of centrioles, surrounded by a shapeless mass of dense material, called the pericentriolar material (PCM), makes up a structure called a centrosome.

Centrioles are present in the cells of most eukaryotes, for example those of animals. However, they are absent from conifers (pinophyta), flowering plants (angiosperms) and most fungi, and are only present in the male gametes of charophytes, bryophytes, seedless vascular plants, cycads, and ginkgo.

Centrioles are typically made up of nine sets of short microtubule triplets, arranged in a cylinder.
Deviations from this structure include crabs and "Drosophila melanogaster" embryos, with nine doublets, and "Caenorhabditis elegans" sperm cells and early embryos, with nine singlets.
The main function of centrioles is to produce cilia during interphase and the aster and the spindle during cell division.

Edouard van Beneden made the first observation of centrioles in 1883. In 1895, Theodor Boveri named the organele as "centriole". The pattern of centriole duplication was first worked out independently by Etienne de Harven and Joseph G. Gall c. 1950 

Centrioles are involved in the organization of the mitotic spindle and in the completion of cytokinesis. Centrioles were previously thought to be required for the formation of a mitotic spindle in animal cells. However, more recent experiments have demonstrated that cells whose centrioles have been removed via laser ablation can still progress through the G stage of interphase before centrioles can be synthesized later in a de novo fashion. Additionally, mutant flies lacking centrioles develop normally, although the adult flies' cells lack flagella and cilia and as a result, they die shortly after birth.
The centrioles can self replicate during cell division.

Centrioles are a very important part of centrosomes, which are involved in organizing microtubules in the cytoplasm. The position of the centriole determines the position of the nucleus and plays a crucial role in the spatial arrangement of the cell.

Sperm centrioles are important for 2 functions: (1) to form the sperm flagellum and sperm movement and (2) for the development of the embryo after fertilization.

In organisms with flagella and cilia, the position of these organelles is determined by the mother centriole, which becomes the basal body. An inability of cells to use centrioles to make functional cilia and flagella has been linked to a number of genetic and developmental diseases. In particular, the inability of centrioles to properly migrate prior to ciliary assembly has recently been linked to Meckel-Gruber syndrome.

Proper orientation of cilia via centriole positioning toward the posterior of embryonic node cells is critical for establishing left–right asymmetry during mammalian development.

Before DNA replication, cells contain two centrioles. The older of the two centrioles is termed the "mother centriole", the other the "daughter". During the cell division cycle, a new centriole grows at the proximal end of both mother and daughter centrioles. After duplication, the two centriole pairs (freshly assembled centriole is now a daughter centriole in each pair) will remain attached to each other orthogonally until mitosis. At that point the mother and daughter centrioles separate dependently on an enzyme called separase.

The two centrioles in the centrosome are tied to one another. The mother centriole has radiating appendages at the distal end of its long axis and is attached to its daughter at the proximal end. Each daughter cell formed after cell division will inherit one of these pairs. Centrioles start duplicating when DNA replicates.

The last common ancestor of all eukaryotes was a ciliated cell with centrioles. Some lineages of eukaryotes, such as land plants, do not have centrioles except in their motile male gametes. Centrioles are completely absent from all cells of conifers and flowering plants, which do not have ciliate or flagellate gametes.
It is unclear if the last common ancestor had one or two cilia. Important genes required for centriole growth, like centrins, are only found in eukaryotes and not in bacteria or archaeans.

The word "centriole" () uses combining forms of "centri-" and "-ole", yielding "little central part", which describes a centriole's typical location near the center of the cell.

Typical centrioles are made of 9 triplets of microtubules organized with radial symmetry. Centrioles can vary the number of microtubules and can be made of 9 doublets of microtubules (as in Drosophila melanogaster) or 9 singlets microtubules as in C. elegans. Atypical centrioles are centrioles that do not have microtubules such as the Proximal Centriole-Like found in "Drosophila melanogaster" sperm or their microtubules have no radial symmetry such as in the distal centriole of human spermatozoon.


</doc>
<doc id="7683" url="https://en.wikipedia.org/wiki?curid=7683" title="Creation science">
Creation science

Creation science or scientific creationism is a branch of creationism that claims to provide scientific support for the Genesis creation narrative in the Book of Genesis and disprove or reexplain the scientific facts, theories and paradigms about geology, cosmology, biological evolution, archeology, history, and linguistics.

The overwhelming consensus of the scientific community is that creation science fails to produce scientific hypotheses, and courts have ruled that it is a religious, not a scientific, view. It fails to qualify as a science because it lacks empirical support, supplies no tentative hypotheses, and resolves to describe natural history in terms of scientifically untestable supernatural causes. Creation science is a pseudoscientific attempt to map the Bible into scientific facts. It is viewed by professional biologists as unscholarly, and even as a dishonest and misguided sham, with extremely harmful educational consequences.

Creation science began in the 1960s, as a fundamentalist Christian effort in the United States to prove Biblical inerrancy and nullify the scientific evidence for evolution. It has since developed a sizable religious following in the United States, with creation science ministries branching worldwide. The main ideas in creation science are: the belief in "creation "ex nihilo"" (Latin: out of nothing); the conviction that the Earth was created within the last 6,000–10,000 years; the belief that humans and other life on Earth were created as distinct fixed "baraminological" "kinds"; and the idea that fossils found in geological strata were deposited during a cataclysmic flood which completely covered the entire Earth. As a result, creation science also challenges the commonly accepted geologic and astrophysical theories for the age and origins of the Earth and Universe, which creationists believe are irreconcilable with the account in the Book of Genesis. Creation science proponents often refer to the theory of evolution as "Darwinism" or as "Darwinian evolution."

The creation science texts and curricula that first emerged in the 1960s focused upon concepts derived from a literal interpretation of the Bible and were overtly religious in nature, most notably linking Noah's flood in the Biblical Genesis account to the geological and fossil record in a system termed flood geology. These works attracted little notice beyond the schools and congregations of conservative fundamental and Evangelical Christians until the 1970s when its followers challenged the teaching of evolution in the public schools and other venues in the United States, bringing it to the attention of the public-at-large and the scientific community. Many school boards and lawmakers were persuaded to include the teaching of creation science alongside evolution in the science curriculum. Creation science texts and curricula used in churches and Christian schools were revised to eliminate their Biblical and theological references, and less explicitly sectarian versions of creation science education were introduced in public schools in Louisiana, Arkansas, and other regions in the United States.

The 1982 ruling in "McLean v. Arkansas" found that creation science fails to meet the essential characteristics of science and that its chief intent is to advance a particular religious view. The teaching of creation science in public schools in the United States effectively ended in 1987 following the United States Supreme Court decision in "Edwards v. Aguillard". The court affirmed that a statute requiring the teaching of creation science alongside evolution when evolution is taught in Louisiana public schools was unconstitutional because its sole true purpose was to advance a particular religious belief.

In response to this ruling, drafts of the creation science school textbook "Of Pandas and People" were edited to change references of creation to intelligent design before its publication in 1989. The intelligent design movement promoted this version. Requiring intelligent design to be taught in public school science classes was found to be unconstitutional in the 2005 "Kitzmiller v. Dover Area School District" federal court case.
Creation science is based largely upon chapters 1–11 of the Book of Genesis. These describe how God calls the world into existence through the power of speech ("And God said, Let there be light," etc.) in six days, calls all the animals and plants into existence, and molds the first man from clay and the first woman from a rib taken from the man's side; a worldwide flood destroys all life except for Noah and his family and representatives of the animals, and Noah becomes the ancestor of the 70 "nations" of the world; the nations live together until the incident of the Tower of Babel, when God disperses them and gives them their different languages. Creation science attempts to explain history and science within the span of Biblical chronology, which places the initial act of creation some six thousand years ago.

Most creation science proponents hold fundamentalist or Evangelical Christian beliefs in Biblical literalism or Biblical inerrancy, as opposed to the higher criticism supported by Liberal Christianity in the Fundamentalist–Modernist Controversy. However, there are also examples of Islamic and Jewish scientific creationism that conform to the accounts of creation as recorded in their religious doctrines.

The Seventh-day Adventist Church has a history of support for creation science. This dates back to George McCready Price, an active Seventh-day Adventist who developed views of flood geology, which formed the basis of creation science. This work was continued by the Geoscience Research Institute, an official institute of the Seventh-day Adventist Church, located on its Loma Linda University campus in California.<

Creation science is generally rejected by the Church of England as well as the Roman Catholic Church. The Pontifical Gregorian University has officially discussed intelligent design as a "cultural phenomenon" without scientific elements. The Church of England's official website cites Charles Darwin's local work assisting people in his religious parish.

Creation science rejects evolution's theory of the common descent of all living things on the Earth. Instead, it asserts that the field of evolutionary biology is itself pseudoscientific or even a religion. Creationists argue instead for a system called baraminology, which considers the living world to be descended from uniquely created kinds or "baramins."

Creation science incorporates the concept of catastrophism to reconcile current landforms and fossil distributions with Biblical interpretations, proposing the remains resulted from successive cataclysmic events, such as a worldwide flood and subsequent ice age. It rejects one of the fundamental principles of modern geology (and of modern science generally), uniformitarianism, which applies the same physical and geological laws observed on the Earth today to interpret the Earth's geological history.

Sometimes creationists attack other scientific concepts, like the Big Bang cosmological model or methods of scientific dating based upon radioactive decay. Young Earth creationists also reject current estimates of the age of the universe and the age of the Earth, arguing for creationist cosmologies with timescales much shorter than those determined by modern physical cosmology and geological science, typically less than 10,000 years.

The scientific community has overwhelmingly rejected the ideas put forth in creation science as lying outside the boundaries of a legitimate science. The foundational premises underlying scientific creationism disqualify it as a science because the answers to all inquiry therein are preordained to conform to Bible doctrine, and because that inquiry is constructed upon theories which are not empirically testable in nature. Scientists also deem creation science's attacks against biological evolution to be without scientific merit. Those views of the scientific community were accepted in two significant court decisions in the 1980s which found the field of creation science to be a religious mode of inquiry, not a scientific one.

The teaching of evolution was gradually introduced into more and more public high school textbooks in the United States after 1900, but in the aftermath of the First World War the growth of fundamentalist Christianity gave rise to a creationist opposition to such teaching. Legislation prohibiting the teaching of evolution was passed in certain regions, most notably Tennessee's Butler Act of 1925. The Soviet Union's successful launch of "Sputnik 1" in 1957 sparked national concern that the science education in public schools was outdated. In 1958, the United States passed National Defense Education Act which introduced new education guidelines for science instruction. With federal grant funding, the Biological Sciences Curriculum Study (BSCS) drafted new standards for the public schools' science textbooks which included the teaching of evolution. Almost half the nation's high schools were using textbooks based on the guidelines of the BSCS soon after they were published in 1963. The Tennessee legislature did not repeal the Butler Act until 1967.

Creation science (dubbed "scientific creationism" at the time) emerged as an organized movement during the 1960s. It was strongly influenced by the earlier work of armchair geologist George McCready Price who wrote works such as "The New Geology" (1923) to advance what he termed "new catastrophism" and dispute the current geological time frames and explanations of geologic history. Price's work was cited at the Scopes Trial of 1925, yet although he frequently solicited feedback from geologists and other scientists, they consistently disparaged his work. Price's "new catastrophism" also went largely unnoticed by other creationists until its revival with the 1961 publication of "" by John C. Whitcomb and Henry M. Morris, a work which quickly became an important text on the issue to fundamentalist Christians and expanded the field of creation science beyond critiques of geology into biology and cosmology as well. Soon after its publication, a movement was underway to have the subject taught in United States' public schools.

The various state laws prohibiting teaching of evolution were overturned in 1968 when the United States Supreme Court ruled in "Epperson v. Arkansas" such laws violated the Establishment Clause of the First Amendment to the United States Constitution. This ruling inspired a new creationist movement to promote laws requiring that schools give balanced treatment to creation science when evolution is taught. The 1981 Arkansas Act 590 was one such law that carefully detailed the principles of creation science that were to receive equal time in public schools alongside evolutionary principles. The act defined creation science as follows:

"'Creation-science' means the scientific evidences for creation and inferences from those evidences. Creation-science includes the scientific evidences and related inferences that indicate:

This legislation was examined in "McLean v. Arkansas", and the ruling handed down on January 5, 1982, concluded that creation-science as defined in the act "is simply not science". The judgement defined the following as essential characteristics of science:

The court ruled that creation science failed to meet these essential characteristics and identified specific reasons. After examining the key concepts from creation science, the court found:

The court further noted that no recognized scientific journal had published any article espousing the creation science theory as described in the Arkansas law, and stated that the testimony presented by defense attributing the absence to censorship was not credible.

In its ruling, the court wrote that for any theory to qualify as scientific, the theory must be tentative, and open to revision or abandonment as new facts come to light. It wrote that any methodology which begins with an immutable conclusion which cannot be revised or rejected, regardless of the evidence, is not a scientific theory. The court found that creation science does not culminate in conclusions formed from scientific inquiry, but instead begins with the conclusion, one taken from a literal wording of the Book of Genesis, and seeks only scientific evidence to support it.

The law in Arkansas adopted the same two-model approach as that put forward by the Institute for Creation Research, one allowing only two possible explanations for the origins of life and existence of man, plants and animals: it was either the work of a creator or it was not. Scientific evidence that failed to support the theory of evolution was posed as necessarily scientific evidence in support of creationism, but in its judgment the court ruled this approach to be no more than a "contrived dualism which has not scientific factual basis or legitimate educational purpose."

The judge concluded that "Act 590 is a religious crusade, coupled with a desire to conceal this fact," and that it violated the First Amendment's Establishment Clause.

The decision was not appealed to a higher court, but had a powerful influence on subsequent rulings. Louisiana's 1982 Balanced Treatment for Creation-Science and Evolution-Science Act, authored by State Senator Bill P. Keith, judged in the 1987 United States Supreme Court case "Edwards v. Aguillard", and was handed a similar ruling. It found the law to require the balanced teaching of creation science with evolution had a particular religious purpose and was therefore unconstitutional.

In 1984, "The Mystery of Life's Origin" was first published. It was co-authored by chemist and creationist Charles B. Thaxton with Walter L. Bradley and Roger L. Olsen, the foreword written by Dean H. Kenyon, and sponsored by the Christian-based Foundation for Thought and Ethics (FTE). The work presented scientific arguments against current theories of abiogenesis and offered an hypothesis of special creation instead. While the focus of creation science had until that time centered primarily on the criticism of the fossil evidence for evolution and validation of the creation myth of the Bible, this new work posed the question whether science reveals that even the simplest living systems were far too complex to have developed by natural, unguided processes.

Kenyon later co-wrote with creationist Percival Davis a book intended as a "scientific brief for creationism" to use as a supplement to public high school biology textbooks. Thaxton was enlisted as the book's editor, and the book received publishing support from the FTE. Prior to its release, the 1987 Supreme Court ruling in "Edwards v. Aguillard" barred the teaching of creation science and creationism in public school classrooms. The book, originally titled "Biology and Creation" but renamed "Of Pandas and People", was released in 1989 and became the first published work to promote the anti-evolutionist design argument under the name intelligent design. The contents of the book later became a focus of evidence in the federal court case, "Kitzmiller v. Dover Area School District", when a group of parents filed suit to halt the teaching of intelligent design in Dover, Pennsylvania, public schools. School board officials there had attempted to include "Of Pandas and People" in their biology classrooms and testimony given during the trial revealed the book was originally written as a creationist text but following the adverse decision in the Supreme Court it underwent simple cosmetic editing to remove the explicit allusions to "creation" or "creator," and replace them instead with references to "design" or "designer."

By the mid-1990s, intelligent design had become a separate movement. The creation science movement is distinguished from the intelligent design movement, or neo-creationism, because most advocates of creation science accept scripture as a literal and inerrant historical account, and their primary goal is to corroborate the scriptural account through the use of science. In contrast, as a matter of principle, neo-creationism eschews references to scripture altogether in its polemics and stated goals (see Wedge strategy). By so doing, intelligent design proponents have attempted to succeed where creation science has failed in securing a place in public school science curricula. Carefully avoiding any reference to the identity of the intelligent designer as God in their public arguments, intelligent design proponents sought to reintroduce the creationist ideas into science classrooms while sidestepping the First Amendment's prohibition against religious infringement. However, the intelligent design curriculum was struck down as a violation of the Establishment Clause in "Kitzmiller v. Dover Area School District", the judge in the case ruling "that ID is nothing less than the progeny of creationism."

Today, creation science as an organized movement is primarily centered within the United States. Creation science organizations are also known in other countries, most notably Creation Ministries International which was founded (under the name Creation Science Foundation) in Australia.

Proponents are usually aligned with a Christian denomination, primarily with those characterized as evangelical, conservative, or fundamentalist. While creationist movements also exist in Islam and Judaism, these movements do not use the phrase "creation science" to describe their beliefs.

Creation science has its roots in the work of young Earth creationist George McCready Price disputing modern science's account of natural history, focusing particularly on geology and its concept of uniformitarianism, and his efforts instead to furnish an alternative empirical explanation of observable phenomena which was compatible with strict Biblical literalism. Price's work was later discovered by civil engineer Henry M. Morris, who is now considered to be the father of creation science. Morris and later creationists expanded the scope with attacks against the broad spectrum scientific findings that point to the antiquity of the Universe and common ancestry among species, including growing body of evidence from the fossil record, absolute dating techniques, and cosmogony.

The proponents of creation science often say that they are concerned with religious and moral questions as well as natural observations and predictive hypotheses. Many state that their opposition to scientific evolution is primarily based on religion.

The overwhelming majority of scientists are in agreement that the claims of science are necessarily limited to those that develop from natural observations and experiments which can be replicated and substantiated by other scientists, and that claims made by creation science do not meet those criteria. Duane Gish, a prominent creation science proponent, has similarly claimed, "We do not know how the creator created, what processes He used, "for He used processes which are not now operating anywhere in the natural universe." This is why we refer to creation as special creation. We cannot discover by scientific investigation anything about the creative processes used by the Creator." But he also makes the same claim against science's evolutionary theory, maintaining that on the subject of origins, scientific evolution is a religious theory which cannot be validated by science.

Creation science makes the "a priori" metaphysical assumption that there exists a creator of the life whose origin is being examined. Christian creation science holds that the description of creation is given in the Bible, that the Bible is inerrant in this description (and elsewhere), and therefore empirical scientific evidence must correspond with that description. Creationists also view the preclusion of all supernatural explanations within the sciences as a doctrinaire commitment to exclude the supreme being and miracles. They claim this to be the motivating factor in science's acceptance of Darwinism, a term used in creation science to refer to evolutionary biology which is also often used as a disparagement. Critics argue that creation science is religious rather than scientific because it stems from faith in a religious text rather than by the application of the scientific method. The United States National Academy of Sciences (NAS) has stated unequivocally, "Evolution pervades all biological phenomena. To ignore that it occurred or to classify it as a form of dogma is to deprive the student of the most fundamental organizational concept in the biological sciences. No other biological concept has been more extensively tested and more thoroughly corroborated than the evolutionary history of organisms." Anthropologist Eugenie Scott has noted further, "Religious opposition to evolution propels antievolutionism. Although antievolutionists pay lip service to supposed scientific problems with evolution, what motivates them to battle its teaching is apprehension over the implications of evolution for religion."

Creation science advocates argue that scientific theories of the origins of the Universe, Earth, and life are rooted in "a priori" presumptions of methodological naturalism and uniformitarianism, each of which is disputed. In some areas of science such as chemistry, meteorology or medicine, creation science proponents do not challenge the application of naturalistic or uniformitarian assumptions. Traditionally, creation science advocates have singled out those scientific theories judged to be in conflict with held religious beliefs, and it is against those theories that they concentrate their efforts.

Some mainstream Christian churches criticize creation science on theological grounds, asserting either that religious faith alone should be a sufficient basis for belief in the truth of creation, or that efforts to prove the Genesis account of creation on scientific grounds are inherently futile because reason is subordinate to faith and cannot thus be used to prove it.

Many Christian theologies, including Liberal Christianity, consider the Genesis creation narrative to be a poetic and allegorical work rather than a literal history, and many Christian churches—including the Roman Catholic, Anglican and the more liberal denominations of the Lutheran, Methodist, Congregationalist and Presbyterian faiths—have either rejected creation science outright or are ambivalent to it. Belief in non-literal interpretations of Genesis is often cited as going back to Saint Augustine.

Theistic evolution and evolutionary creationism are theologies that reconcile belief in a creator with biological evolution. Each holds the view that there is a creator but that this creator has employed the natural force of evolution to unfold a divine plan. Religious representatives from faiths compatible with theistic evolution and evolutionary creationism have challenged the growing perception that belief in a creator is inconsistent with the acceptance of evolutionary theory. Spokespersons from the Catholic Church have specifically criticized biblical creationism for relying upon literal interpretations of biblical scripture as the basis for determining scientific fact.

The National Academy of Sciences states that "the claims of creation science lack empirical support and cannot be meaningfully tested" and that "creation science is in fact not science and should not be presented as such in science classes." According to Joyce Arthur writing for "Skeptic" magazine, the "creation 'science' movement gains much of its strength through the use of distortion and scientifically unethical tactics" and "seriously misrepresents the theory of evolution."

Scientists have considered the hypotheses proposed by creation science and have rejected them because of a lack of evidence. Furthermore, the claims of creation science do not refer to natural causes and cannot be subject to meaningful tests, so they do not qualify as scientific hypotheses. In 1987, the United States Supreme Court ruled that creationism is religion, not science, and cannot be advocated in public school classrooms. Most mainline Christian denominations have concluded that the concept of evolution is not at odds with their descriptions of creation and human origins.

A summary of the objections to creation science by scientists follows:

By invoking claims of "abrupt appearance" of species as a miraculous act, creation science is unsuited for the tools and methods demanded by science, and it cannot be considered scientific in the way that the term "science" is currently defined. Scientists and science writers commonly characterize creation science as a pseudoscience.

Historically, the debate of whether creationism is compatible with science can be traced back to 1874, the year science historian John William Draper published his "History of the Conflict between Religion and Science". In it Draper portrayed the entire history of scientific development as a war against religion. This presentation of history was propagated further by followers such as Andrew Dickson White in his two-volume "A History of the Warfare of Science with Theology in Christendom" (1896). Their conclusions have been disputed.

In the United States, the principal focus of creation science advocates is on the government-supported public school systems, which are prohibited by the Establishment Clause from promoting specific religions. Historical communities have argued that Biblical translations contain many translation errors and errata, and therefore that the use of biblical literalism in creation science is self-contradictory.

Creationist biology centers on an idea derived from Genesis that states that life was created by God, in a finite number of "created kinds," rather than through biological evolution from a common ancestor. Creationists consider that any observable speciation descends from these distinctly created kinds through inbreeding, deleterious mutations and other genetic mechanisms. Whereas evolutionary biologists and creationists share similar views of microevolution, creationists disagree that the process of macroevolution can explain common ancestry among organisms far beyond the level of common species. Creationists contend that there is no empirical evidence for new plant or animal species, and deny fossil evidence has ever been found documenting the process.

Popular arguments against evolution have changed since the publishing of Henry M. Morris' first book on the subject, "Scientific Creationism" (1974), but some consistent themes remain: that missing links or gaps in the fossil record are proof against evolution; that the increased complexity of organisms over time through evolution is not possible due to the law of increasing entropy; that it is impossible that the mechanism of natural selection could account for common ancestry; and that evolutionary theory is untestable. The origin of the human species is particularly hotly contested; the fossil remains of purported hominid ancestors are not considered by advocates of creation biology to be evidence for a speciation event involving "Homo sapiens". Creationists also assert that early hominids, are either apes, or humans.

Richard Dawkins has explained evolution as "a theory of gradual, incremental change over millions of years, which starts with something very simple and works up along slow, gradual gradients to greater complexity," and described the existing fossil record as entirely consistent with that process. Biologists emphasize that transitional gaps between those fossils recovered are to be expected, that the existence of any such gaps cannot be invoked to disprove evolution, and that instead the fossil evidence that could be used to disprove the theory would be those fossils which are found and which are entirely inconsistent with what can be predicted or anticipated by the evolutionary model. One example given by Dawkins was, "If there were a single hippo or rabbit in the Precambrian, that would completely blow evolution out of the water. None have ever been found."

Flood geology is a concept based on the belief that most of Earth's geological record was formed by the Great Flood described in the story of Noah's Ark. Fossils and fossil fuels are believed to have formed from animal and plant matter which was buried rapidly during this flood, while submarine canyons are explained as having formed during a rapid runoff from the continents at the end of the flood. Sedimentary strata are also claimed to have been predominantly laid down during or after Noah's flood and orogeny. Flood geology is a variant of catastrophism and is contrasted with geological science in that it rejects standard geological principles such as uniformitarianism and radiometric dating. For example, the Creation Research Society argues that "uniformitarianism is wishful thinking."

Geologists conclude that no evidence for such a flood is observed in the preserved rock layers and moreover that such a flood is physically impossible, given the current layout of land masses. For instance, since Mount Everest currently is approximately 8.8 kilometres in elevation and the Earth's surface area is 510,065,600 km, the volume of water required to cover Mount Everest to a depth of 15 cubits (6.8 m), as indicated by Genesis 7:20, would be 4.6 billion cubic kilometres. Measurements of the amount of precipitable water vapor in the atmosphere have yielded results indicating that condensing all water vapor in a column of atmosphere would produce liquid water with a depth ranging between zero and approximately 70mm, depending on the date and the location of the column. Nevertheless, there continue to be adherents to the belief in flood geology, and in recent years new theories have been introduced such as catastrophic plate tectonics and catastrophic orogeny.

Creationists point to experiments they have performed, which they claim demonstrate that 1.5 billion years of nuclear decay took place over a short period of time, from which they infer that "billion-fold speed-ups of nuclear decay" have occurred, a massive violation of the principle that radioisotope decay rates are constant, a core principle underlying nuclear physics generally, and radiometric dating in particular.

The scientific community points to numerous flaws in the creationists' experiments, to the fact that their results have not been accepted for publication by any peer-reviewed scientific journal, and to the fact that the creationist scientists conducting them were untrained in experimental geochronology. They have also been criticised for widely publicising the results of their research as successful despite their own admission of insurmountable problems with their hypothesis.

The constancy of the decay rates of isotopes is well supported in science. Evidence for this constancy includes the correspondences of date estimates taken from different radioactive isotopes as well as correspondences with non-radiometric dating techniques such as dendrochronology, ice core dating, and historical records. Although scientists have noted slight increases in the decay rate for isotopes subject to extreme pressures, those differences were too small to significantly impact date estimates. The constancy of the decay rates is also governed by first principles in quantum mechanics, wherein any deviation in the rate would require a change in the fundamental constants. According to these principles, a change in the fundamental constants could not influence different elements uniformly, and a comparison between each of the elements' resulting unique chronological timescales would then give inconsistent time estimates.

In refutation of young Earth claims of inconstant decay rates affecting the reliability of radiometric dating, Roger C. Wiens, a physicist specializing in isotope dating states:


In the 1970s, young Earth creationist Robert V. Gentry proposed that radiohaloes in certain granites represented evidence for the Earth being created instantaneously rather than gradually. This idea has been criticized by physicists and geologists on many grounds including that the rocks Gentry studied were not primordial and that the radionuclides in question need not have been in the rocks initially.

Thomas A. Baillieul, a geologist and retired senior environmental scientist with the United States Department of Energy, disputed Gentry's claims in an article entitled, "'Polonium Haloes' Refuted: A Review of 'Radioactive Halos in a Radio-Chronological and Cosmological Perspective' by Robert V. Gentry." Baillieul noted that Gentry was a physicist with no background in geology and given the absence of this background, Gentry had misrepresented the geological context from which the specimens were collected. Additionally, he noted that Gentry relied on research from the beginning of the 20th century, long before radioisotopes were thoroughly understood; that his assumption that a polonium isotope caused the rings was speculative; and that Gentry falsely argued that the half-life of radioactive elements varies with time. Gentry claimed that Baillieul could not publish his criticisms in a reputable scientific journal, although some of Baillieul's criticisms rested on work previously published in reputable scientific journals.

Several attempts have been made by creationists to construct a cosmology consistent with a young Universe rather than the standard cosmological age of the universe, based on the belief that Genesis describes the creation of the Universe as well as the Earth. The primary challenge for young-universe cosmologies is that the accepted distances in the Universe require millions or billions of years for light to travel to Earth (the "starlight problem"). An older creationist idea, proposed by creationist astronomer Barry Setterfield, is that the speed of light has decayed in the history of the Universe. More recently, creationist physicist Russell Humphreys has proposed a hypothesis called "white hole cosmology" which suggests that the Universe expanded out of a white hole less than 10,000 years ago; the apparent age of the universe results from relativistic effects. Humphreys' theory is advocated by creationist organisations such as Answers in Genesis; however because the predictions of Humphreys' cosmology conflict with current observations, it is not accepted by the scientific community.

Various claims are made by creationists concerning alleged evidence that the age of the Solar System is of the order of thousands of years, in contrast to the scientifically accepted age of 4.6 billion years. It is commonly argued that the number of comets in the Solar System is much higher than would be expected given its supposed age. Creationist astronomers express scepticism about the existence of the Kuiper belt and Oort cloud. Creationists also argue that the recession of the Moon from the Earth is incompatible with either the Moon or the Earth being billions of years old. These claims have been refuted by planetologists.

In response to increasing evidence suggesting that Mars once possessed a wetter climate, some creationists have proposed that the global flood affected not only the Earth but also Mars and other planets. People who support this claim include creationist astronomer Wayne Spencer and Russell Humphreys.

An ongoing problem for creationists is the presence of impact craters on nearly all Solar System objects, which is consistent with scientific explanations of solar system origins but creates insuperable problems for young Earth claims. Creationists Harold Slusher and Richard Mandock, along with Glenn Morton (who later repudiated this claim) asserted that impact craters on the Moon are subject to rock flow, and so cannot be more than a few thousand years old. While some creationist astronomers assert that different phases of meteoritic bombardment of the Solar System occurred during creation week and during the subsequent Great Flood, others regard this as unsupported by the evidence and call for further research.






Notable creationist museums in the United States:


</doc>
<doc id="7685" url="https://en.wikipedia.org/wiki?curid=7685" title="List of cartographers">
List of cartographers

Cartography is the study of map making and cartographers are map makers.











</doc>
<doc id="7689" url="https://en.wikipedia.org/wiki?curid=7689" title="Cirth">
Cirth

The Cirth (; plural of certh , in Sindarin meaning runes) are a semi-artificial script, with letters shaped on those of actual runic alphabets, invented by J. R. R. Tolkien for the constructed languages he devised and used in his works. "Cirth" is written with a capital letter when referring to the writing system; the runes themselves can be called "cirth".

In the fictional history of Middle-earth, the original Certhas Daeron was created by the elf Daeron, and was later expanded into what was known as the Angerthas Daeron. Although the Cirth were later largely replaced by the Tengwar, they were adopted by Dwarves to write down their Khuzdul language (Angerthas Moria and Angerthas Erebor) because their straight lines were better suited to carving than the curved strokes of the Tengwar. Cirth was also adapted, in its oldest and simplest form, by various peoples including Men and even Orcs.

During the Chaining of Melkor, the Sindar of Beleriand began developing an alphabet for their language. Its letters were entirely made for carving on wood, stone or metal, hence their angular forms and straight lines. These letters were named "cirth" (sing. "certh"). The corresponding Quenya words are "certar" () and "certa" (). The assignment of values was unsystematic. The form of a certh consisted of a stem and a branch. The attachment of the branch was, if on one side only, usually made on the right side. The reverse was not infrequent, but had no phonetic significance.<br>Two basic principles were followed:
The original display of Cirth should have been this:


The known ancient cirth don't cover all the sounds of Sindarin, since we are missing "rh", "lh", "mh", "y", "œ". Perhaps they were used for the Old Sindarin tongue, and many of the above-mentioned sounds indeed didn't exist in that language. However still frequent sounds "w" and "a" are missing. This indicates that some ancient, unknown cirth could have existed, but didn't make it to the later systems; a fuller table therefore can't be reconstructed.<br>As for the vowel usage, perhaps the certh for "u" possibly was used for "w" (like in early Latin orthography). The certh for "a" can't be guessed, so maybe this sound was meant (like in some Tengwar Modes for Quenya). More possibly it was one of some other cirth that did not survive.<br>Long vowels were evidently indicated by doubling.

The elf Daeron, minstrel of king Thingol of Doriath reorganised the cirth and added new ones, being somehow inspired by Fëanor's Tengwar (therefore this mustn't have occurred before the return of the Noldor) and made the extension of the cirth known as Certhas Daeron (where "Certhas" means "runic alphabet"), used for inscribing names in Menegroth. The Dwarves working for Thingol liked them and adopted them, making them known also in the East.<br>Unlike the previous system, the reversal of the certh had a phonemic significance: reversed cirth were softer versions of their originals. This also gives us another information: perhaps lenited consonants must have started to occur in Sindarin around that time.<br>We know that at one time a sign for the sound "mh" was needed and the most appropriate solution was to revert the certh for "m" to indicate its softening, but it could not be reverted (); therefore "m" was given to the reversible (which until then had a value unknown to us), "mh" to , and the certh got the value of "hw".

Daeron's alphabet was originally used by the Grey Elves (Sindar) in Beleriand. Later the Noldor in Eregion adopted the Cirth, added several more runes to the system and created the Angerthas Daeron (where "Angerthas" means "long rune-rows") sometimes also referred to as Angerthas Eregion. These additional letters were used to represent sounds not found in Sindarin, but present in the tongues of other peoples. The Angerthas Daeron was used primarily for carved inscriptions. For most other forms of written communication the Tengwar were used.<br>Here the Cirth are grouped according to their phonetic features:


For the transliteration of this alphabet, meant to be used for more than one language (for Quenya and Sindarin, at least) and needing a bigger set of sounds, Tolkien thought up to a kind of "general Middle-earth languages phonetic transcription", here used.

Dwarves first came to know the runes of the Noldor during the beginning of the Second Age. They modified them to suit the specific needs of their language, Khuzdul. The Dwarves spread their revised alphabet to Moria, where it came to be known as Angerthas Moria. The Dwarves developed both carved and pen-written forms of the runes. Travelling for trading, they spread their alphabet throughout Middle-earth: as a result, variations of Angerthas Moria were employed by other races for their languages.

Many cirth here stand for sounds not occurring in Khuzdul (at least in published words of Khuzdul: of course, our corpus is very limited to judge the necessity or not, of these sounds). Here they are marked with a black star ().



According to Tolkien's legendarium, after the Second Age, the Cirth were obsoleted by the Tengwar among the western races and remained in use only by Dwarves and Men. The Dwarves developed even pen-written cursive forms, since they used them exclusively in any form of writing communication, even in paper. At the beginning of the Third Age, the Dwarves were driven out of Moria. Some migrated to the Grey Mountains, some to the Iron Hills and Thráin I came to Erebor. There he founded his Dwarf-kingdom. There the "Angerthas Moria" was modified further and some new cirth were added, but some reverted to their Elvish usage, thus creating the Angerthas Erebor variation. This mode was used in Westron by Dwarves.

Many cirth here stand for sounds not occurring in Khuzdul (at least in published words of Khuzdul: of course, our corpus is very limited to judge the necessity or not, of these sounds). Here they are marked with a black star ().

Combining diacritics occur in Angerthas Erebor as well: a circumflex used to denote long consonants, a macron below to indicate a long vowel sound, and an underdot to mark cirth used as numerals.


Tolkien used Angerthas Erebor mode to write English with Cirth at least twice in "The Lord of the Rings":

The Book of Mazarbul shows that additional cirth were introduced in this mode (for a double ligature, for the definite article and for the representation of six diphthongs). These were probably only to be used with English language:

Additionally, is used for English and for .

The Cirth is not the only runic writing system devised by Tolkien for Middle Earth. In fact, he invented a great number of runic alphabets, of which only a few others have been published. Many of them were included in the "Appendix on Runes" in The History of Middle-Earth, vol. VII, The Treason of Isengard, edited by Christopher Tolkien.

According to Tolkien, the earliest Cirth became known to many peoples of Middle-earth like Men, Dwarves or Orcs. The people of Dale and the Rohirrim maintained a simple form of these characters — hence the "branching runes" that appear on the stone floor of the Meduseld in Rohan.

It is speculated that the runes used in "The Hobbit" are indeed the form of Cirth used by the Men of Dale, although Tolkien himself wrote that the letters used for Thror's Map are a form of our ancient runes used to transliterate actual Dwarf-runes.

These runes – used only to write in English – are indeed nearly identical to those of Fuþorc but their sound may change according to their position, just as the Latin letters do. And, in fact, Tolkien's writing mode for these runes is mainly orthographic.<br>
It has one rune for each letter, regardless of pronunciation (for example the rune can sound in the word or in the word or even in the word and in the digraph ).<br>
A few sounds are instead written with the same rune regardless of the letter (e.g. the sound is always written with the rune either if in English it is written as in , as in , or as in ). The letters that are subject to this phonemic spelling are and .<br>
In addition, there are also some runes which stand for particular English digraphs and diphthongs.

Here the runes used in "The Hobbit" are represented along with their English transliteration and Fuþorc equivalent:

Two other runes, not attested in "The Hobbit", were added by Tolkien in order to represent additional English sounds:

<nowiki>*</nowiki> The runes marked with an asterisk are not attested in real-life fuþorc. These are completely new characters invented by Tolkien himself. These were included in Unicode 7.0 (see below).

It must be noticed that Tolkien always wrote the English digraph (representing the sound , or , like in ) in runes as .
<br>There is no rune to transliterate : the digraph (representing the sound , like in ) is always rendered in runes as .

This table could be helpful for the transcription of and in runes:

Not all the runes mentioned in "The Hobbit" are Dwarf-runes. The swords found in the Trolls' cave (which were from the ancient kingdom of Gondolin) bore runes that Gandalf could not read. In fact, the swords Glamdring and Orcrist, forged in Gondolin, bore a type of letters known as Gondolinic runes. They seem to have been obsoleted and forgotten by the Third Age, and this is supported by the fact that only Elrond could read the inscriptions of the swords.<br>
Tolkien devised this runic alphabet in a very early stage of his shaping of Middle-earth, but they are known to us only from a slip of paper written by J.R.R. Tolkien, a photocopy of which Christopher Tolkien sent to Paul Nolan Hyde in February 1992, who published it, together with an extensive analysis, in the 1992 Summer issue of Mythlore, no. 69.
The system provides sounds not found in the known Elven languages of the First Age, but perhaps it was designed for notating a variety of languages. However, the consonants seem to be, more or less, the same found in Welsh phonology, a theory supported by the fact that Tolkien was heavily influenced by Welsh when creating Elven languages.

Many letters have shapes also found in the historical runic alphabets, but their sound values are only similar in a few of the vowels. Rather, the system of assignment of sound values is much more systematic in the Cirth than in the historical runes (e.g., voiced variants of a voiceless sound are expressed by an additional stroke). A similar system has been proposed for a few historical runes but is in any case much more obscure.

The division between the older Cirth of Daeron and their adaptation by Dwarves and Men has been interpreted as a parallel drawn by Tolkien to the development of the Fuþorc to the Younger Fuþark. The original Elvish Cirth "as supposed products of a superior culture" are focused on logical arrangement and a close connection between form and value whereas the adaptations by mortal races introduced irregularities. Similar to the Germanic tribes who had no written literature and used only simple runes before their conversion to Christianity, the Sindar Elves of Beleriand with their Cirth were introduced to the more elaborate Tengwar of Fëanor when the Noldor Elves returned to Middle-earth from the lands of the divine Valar.

Equivalents for most but not all cirth can be found in the Runic block of Unicode.

Three J. R. R. Tolkien-specific letters were added in June, 2014 with the release of Unicode 7.0:
A formal Unicode proposal to encode Cirth as a separate script was made in September 1997 by Michael Everson.
No action was taken by the Unicode Technical Committee (UTC) but Cirth appears in the Roadmap to the SMP.

Unicode Private Use Area layouts for Cirth are defined at the ConScript Unicode Registry (CSUR) and the Under-ConScript Unicode Registry (UCSUR).

Two different layouts are defined by the CSUR/UCSUR:

Without proper rendering support, you may see question marks, boxes, or other symbols below instead of Cirth.


</doc>
<doc id="7697" url="https://en.wikipedia.org/wiki?curid=7697" title="Lockheed C-130 Hercules">
Lockheed C-130 Hercules

The Lockheed C-130 Hercules is a four-engine turboprop military transport aircraft designed and built originally by Lockheed (now Lockheed Martin). Capable of using unprepared runways for takeoffs and landings, the C-130 was originally designed as a troop, medevac, and cargo transport aircraft. The versatile airframe has found uses in a variety of other roles, including as a gunship (AC-130), for airborne assault, search and rescue, scientific research support, weather reconnaissance, aerial refueling, maritime patrol, and aerial firefighting. It is now the main tactical airlifter for many military forces worldwide. More than 40 variants of the Hercules, including a civilian one marketed as the Lockheed L-100, operate in more than 60 nations.

The C-130 entered service with the U.S. in the 1950s, followed by Australia and many other nations. During its years of service, the Hercules family has participated in numerous military, civilian and humanitarian aid operations. In 2007, the C-130 became the fifth aircraft to mark 50 years of continuous service with its original primary customer, which is the United States Air Force for the C-130. The C-130 Hercules is the longest continuously produced military aircraft at over 60 years, with the updated Lockheed Martin C-130J Super Hercules currently being produced.

The Korean War showed that World War II-era piston-engine transports—Fairchild C-119 Flying Boxcars, Douglas C-47 Skytrains and Curtiss C-46 Commandos—were no longer adequate. Thus, on 2 February 1951, the United States Air Force issued a General Operating Requirement (GOR) for a new transport to Boeing, Douglas, Fairchild, Lockheed, Martin, Chase Aircraft, North American, Northrop, and Airlifts Inc. The new transport would have a capacity of 92 passengers, 72 combat troops or 64 paratroopers in a cargo compartment that was approximately long, high, and wide. Unlike transports derived from passenger airliners, it was to be designed specifically as a combat transport with loading from a hinged loading ramp at the rear of the fuselage.

A key feature was the introduction of the Allison T56 turboprop powerplant, which was developed for the C-130. At the time, the turboprop was a new application of gas turbines, which offered greater range at propeller-driven speeds compared to pure turbojets, which were faster but consumed more fuel. They also produced much more power for their weight than piston engines.

The Hercules resembled a larger four-engine brother to the C-123 Provider with a similar wing and cargo ramp layout that evolved from the Chase XCG-20 Avitruc, which in turn, was first designed and flown as a cargo glider in 1947. The Boeing C-97 Stratofreighter also had a rear ramp, which made it possible to drive vehicles onto the plane (also possible with forward ramp on a C-124). The ramp on the Hercules was also used to airdrop cargo, which included low-altitude extraction for Sheridan tanks and even dropping large improvised "daisy cutter" bombs.

The new Lockheed cargo plane design possessed a range of , takeoff capability from short and unprepared strips, and the ability to fly with one engine shut down. Fairchild, North American, Martin, and Northrop declined to participate. The remaining five companies tendered a total of ten designs: Lockheed two, Boeing one, Chase three, Douglas three, and Airlifts Inc. one. The contest was a close affair between the lighter of the two Lockheed (preliminary project designation L-206) proposals and a four-turboprop Douglas design.

The Lockheed design team was led by Willis Hawkins, starting with a 130-page proposal for the "Lockheed L-206". Hall Hibbard, Lockheed vice president and chief engineer, saw the proposal and directed it to Kelly Johnson, who did not care for the low-speed, unarmed aircraft, and remarked, "If you sign that letter, you will destroy the Lockheed Company." Both Hibbard and Johnson signed the proposal and the company won the contract for the now-designated Model 82 on 2 July 1951.

The first flight of the "YC-130" prototype was made on 23 August 1954 from the Lockheed plant in Burbank, California. The aircraft, serial number "53-3397", was the second prototype, but the first of the two to fly. The YC-130 was piloted by Stanley Beltz and Roy Wimmer on its 61-minute flight to Edwards Air Force Base; Jack Real and Dick Stanton served as flight engineers. Kelly Johnson flew chase in a Lockheed P2V Neptune.

After the two prototypes were completed, production began in Marietta, Georgia, where over 2,300 C-130s have been built through 2009.

The initial production model, the "C-130A", was powered by Allison T56-A-9 turboprops with three-blade propellers and originally equipped with the blunt nose of the prototypes. Deliveries began in December 1956, continuing until the introduction of the "C-130B" model in 1959. Some A-models were equipped with skis and re-designated "C-130D". As the C-130A became operational with Tactical Air Command (TAC), the C-130's lack of range became apparent and additional fuel capacity was added with wing pylon-mounted tanks outboard of the engines; this added 6,000 lb of fuel capacity for a total capacity of 40,000 lb.

The C-130B model was developed to complement the A-models that had previously been delivered, and incorporated new features, particularly increased fuel capacity in the form of auxiliary tanks built into the center wing section and an AC electrical system. Four-bladed Hamilton Standard propellers replaced the Aeroproducts three-blade propellers that distinguished the earlier A-models. The C-130B had ailerons with boost increased from to , as well as uprated engines and four-blade propellers that were standard until the J-model's introduction.

An electronic reconnaissance variant of the C-130B was designated C-130B-II. A total of 13 aircraft were converted. The C-130B-II was distinguished by its false external wing fuel tanks, which were disguised signals intelligence (SIGINT) receiver antennas. These pods were slightly larger than the standard wing tanks found on other C-130Bs. Most aircraft featured a swept blade antenna on the upper fuselage, as well as extra wire antennas between the vertical fin and upper fuselage not found on other C-130s. Radio call numbers on the tail of these aircraft were regularly changed so as to confuse observers and disguise their true mission.

The extended-range "C-130E" model entered service in 1962 after it was developed as an interim long-range transport for the Military Air Transport Service. Essentially a B-model, the new designation was the result of the installation of 1,360 US gal (5,150 L) "Sargent Fletcher" external fuel tanks under each wing's midsection and more powerful Allison T56-A-7A turboprops. The hydraulic boost pressure to the ailerons was reduced back to as a consequence of the external tanks' weight in the middle of the wingspan. The E model also featured structural improvements, avionics upgrades and a higher gross weight. Australia took delivery of 12 C130E Hercules during 1966–67 to supplement the 12 C-130A models already in service with the RAAF. Sweden and Spain fly the TP-84T version of the C-130E fitted for aerial refueling capability.

The "KC-130" tankers, originally "C-130F" procured for the US Marine Corps (USMC) in 1958 (under the designation "GV-1") are equipped with a removable 3,600 US gal (13,626 L) stainless steel fuel tank carried inside the cargo compartment. The two wing-mounted hose and drogue aerial refueling pods each transfer up to 300 US gal per minute (19 L per second) to two aircraft simultaneously, allowing for rapid cycle times of multiple-receiver aircraft formations, (a typical tanker formation of four aircraft in less than 30 minutes). The US Navy's "C-130G" has increased structural strength allowing higher gross weight operation.

The "C-130H" model has updated Allison T56-A-15 turboprops, a redesigned outer wing, updated avionics and other minor improvements. Later "H" models had a new, fatigue-life-improved, center wing that was retrofitted to many earlier H-models. For structural reasons, some models are required to land with certain amounts of fuel when carrying heavy cargo, reducing usable range. The H model remains in widespread use with the United States Air Force (USAF) and many foreign air forces. Initial deliveries began in 1964 (to the RNZAF), remaining in production until 1996. An improved C-130H was introduced in 1974, with Australia purchasing 12 of type in 1978 to replace the original 12 C-130A models, which had first entered RAAF Service in 1958. The U.S. Coast Guard employs the HC-130H for long-range search and rescue, drug interdiction, illegal migrant patrols, homeland security, and logistics.
C-130H models produced from 1992 to 1996 were designated as C-130H3 by the USAF. The "3" denoting the third variation in design for the H series. Improvements included ring laser gyros for the INUs, GPS receivers, a partial glass cockpit (ADI and HSI instruments), a more capable APN-241 color radar, night vision device compatible instrument lighting, and an integrated radar and missile warning system. The electrical system upgrade included Generator Control Units (GCU) and Bus Switching units (BSU) to provide stable power to the more sensitive upgraded components.
The equivalent model for export to the UK is the "C-130K", known by the Royal Air Force (RAF) as the "Hercules C.1". The "C-130H-30" ("Hercules C.3" in RAF service) is a stretched version of the original Hercules, achieved by inserting a 100 in (2.54 m) plug aft of the cockpit and an 80 in (2.03 m) plug at the rear of the fuselage. A single C-130K was purchased by the Met Office for use by its Meteorological Research Flight, where it was classified as the "Hercules W.2". This aircraft was heavily modified (with its most prominent feature being the long red and white striped atmospheric probe on the nose and the move of the weather radar into a pod above the forward fuselage). This aircraft, named "Snoopy", was withdrawn in 2001 and was then modified by Marshall of Cambridge Aerospace as flight-testbed for the A400M turbine engine, the TP400. The C-130K is used by the RAF Falcons for parachute drops. Three C-130K (Hercules C Mk.1P) were upgraded and sold to the Austrian Air Force in 2002.

The "MC-130E Combat Talon" was developed for the USAF during the Vietnam War to support special operations missions in Southeast Asia, and led to both the "MC-130H Combat Talon II" as well as a family of other special missions aircraft. 37 of the earliest models currently operating with the Air Force Special Operations Command (AFSOC) are scheduled to be replaced by new-production MC-130J versions. The EC-130 Commando Solo is another special missions variant within AFSOC, albeit operated solely by an AFSOC-gained wing in the Pennsylvania Air National Guard, and is a psychological operations/information operations (PSYOP/IO) platform equipped as an aerial radio station and television stations able to transmit messaging over commercial frequencies. Other versions of the EC-130, most notably the EC-130H Compass Call, are also special variants, but are assigned to the Air Combat Command (ACC). The AC-130 gunship was first developed during the Vietnam War to provide close air support and other ground-attack duties.
The "HC-130" is a family of long-range search and rescue variants used by the USAF and the U.S. Coast Guard. Equipped for deep deployment of Pararescuemen (PJs), survival equipment, and (in the case of USAF versions) aerial refueling of combat rescue helicopters, HC-130s are usually the on-scene command aircraft for combat SAR missions (USAF only) and non-combat SAR (USAF and USCG). Early USAF versions were also equipped with the Fulton surface-to-air recovery system, designed to pull a person off the ground using a wire strung from a helium balloon. The John Wayne movie "The Green Berets" features its use. The Fulton system was later removed when aerial refueling of helicopters proved safer and more versatile. The movie "The Perfect Storm" depicts a real life SAR mission involving aerial refueling of a New York Air National Guard HH-60G by a New York Air National Guard HC-130P.

The "C-130R" and "C-130T" are U.S. Navy and USMC models, both equipped with underwing external fuel tanks. The USN C-130T is similar, but has additional avionics improvements. In both models, aircraft are equipped with Allison T56-A-16 engines. The USMC versions are designated "KC-130R" or "KC-130T" when equipped with underwing refueling pods and pylons and are fully night vision system compatible.

The RC-130 is a reconnaissance version. A single example is used by the Islamic Republic of Iran Air Force, the aircraft having originally been sold to the former Imperial Iranian Air Force.

The "Lockheed L-100 (L-382)" is a civilian variant, equivalent to a C-130E model without military equipment. The L-100 also has two stretched versions.

In the 1970s, Lockheed proposed a C-130 variant with turbofan engines rather than turboprops, but the U.S. Air Force preferred the takeoff performance of the existing aircraft. In the 1980s, the C-130 was intended to be replaced by the Advanced Medium STOL Transport project. The project was canceled and the C-130 has remained in production.

Building on lessons learned, Lockheed Martin modified a commercial variant of the C-130 into a High Technology Test Bed (HTTB). This test aircraft set numerous short takeoff and landing performance records and significantly expanded the database for future derivatives of the C-130. Modifications made to the HTTB included extended chord ailerons, a long chord rudder, fast-acting double-slotted trailing edge flaps, a high-camber wing leading edge extension, a larger dorsal fin and dorsal fins, the addition of three spoiler panels to each wing upper surface, a long-stroke main and nose landing gear system, and changes to the flight controls and a change from direct mechanical linkages assisted by hydraulic boost, to fully powered controls, in which the mechanical linkages from the flight station controls operated only the hydraulic control valves of the appropriate boost unit. The HTTB first flew on 19 June 1984, with civil registration of N130X. After demonstrating many new technologies, some of which were applied to the C-130J, the HTTB was lost in a fatal accident on 3 February 1993, at Dobbins Air Reserve Base, in Marietta, Georgia. The crash was attributed to disengagement of the rudder fly-by-wire flight control system, resulting in a total loss of rudder control capability while conducting ground minimum control speed tests (Vmcg). The disengagement was a result of the inadequate design of the rudder's integrated actuator package by its manufacturer; the operator's insufficient system safety review failed to consider the consequences of the inadequate design to all operating regimes. A factor which contributed to the accident was the flight crew's lack of engineering flight test training.

In the 1990s, the improved C-130J Super Hercules was developed by Lockheed (later Lockheed Martin). This model is the newest version and the only model in production. Externally similar to the classic Hercules in general appearance, the J model has new turboprop engines, six-bladed propellers, digital avionics, and other new systems.

In 2000, Boeing was awarded a contract to develop an Avionics Modernization Program kit for the C-130. The program was beset with delays and cost overruns until project restructuring in 2007. On 2 September 2009, Bloomberg news reported that the planned Avionics Modernization Program (AMP) upgrade to the older C-130s would be dropped to provide more funds for the F-35, CV-22 and airborne tanker replacement programs. However, in June 2010, Department of Defense approved funding for the initial production of the AMP upgrade kits. Under the terms of this agreement, the USAF has cleared Boeing to begin low-rate initial production (LRIP) for the C-130 AMP. A total of 198 aircraft are expected to feature the AMP upgrade. The current cost per aircraft is although Boeing expects that this price will drop to US$7 million for the 69th aircraft.

In the 2000s, Lockheed Martin and the U.S. Air Force began outfitting and retrofitting C-130s with the eight-blade UTC Aerospace Systems NP2000 propellers.

An engine enhancement program saving fuel and providing lower temperatures in the T56 engine has been approved, and the US Air Force expects to save $2 billion and extend the fleet life.

In October 2010, the Air Force released a capabilities request for information (CRFI) for the development of a new airlifter to replace the C-130. The new aircraft is to carry a 190 percent greater payload and assume the mission of mounted vertical maneuver (MVM). The greater payload and mission would enable it to carry medium-weight armored vehicles and drop them off at locations without long runways. Various options are being considered, including new or upgraded fixed-wing designs, rotorcraft, tiltrotors, or even an airship. Development could start in 2014, and become operational by 2024. The C-130 fleet of around 450 planes would be replaced by only 250 aircraft. The Air Force had attempted to replace the C-130 in the 1970s through the Advanced Medium STOL Transport project, which resulted in the C-17 Globemaster III that instead replaced the C-141 Starlifter. The Air Force Research Laboratory funded Lockheed and Boeing demonstrators for the Speed Agile concept, which had the goal of making a STOL aircraft that can take off and land at speeds as low as on airfields less than 2,000 ft (610 m) long and cruise at Mach 0.8-plus. Boeing's design used upper-surface blowing from embedded engines on the inboard wing and blown flaps for circulation control on the outboard wing. Lockheed's design also used blown flaps outboard, but inboard used patented reversing ejector nozzles. Boeing's design completed over 2,000 hours of windtunnel tests in late 2009. It was a 5 percent-scale model of a narrowbody design with a payload. When the AFRL increased the payload requirement to , they tested a 5% scale model of a widebody design with a take-off gross weight and an "A400M-size" wide cargo box. It would be powered by four IAE V2533 turbofans. In August 2011, the AFRL released pictures of the Lockheed Speed Agile concept demonstrator. A 23% scale model went through wind tunnel tests to demonstrate its hybrid powered lift, which combines a low drag airframe with simple mechanical assembly to reduce weight and better aerodynamics. The model had four engines, including two Williams FJ44 turbofans. On 26 March 2013, Boeing was granted a patent for its swept-wing powered lift aircraft.

As of January 2014, Air Mobility Command, Air Force Materiel Command and the Air Force Research Lab are in the early stages of defining requirements for the C-X next generation airlifter program to replace both the C-130 and C-17. An aircraft would be produced from the early 2030s to the 2040s. If requirements are decided for operating in contested airspace, Air Force procurement of C-130s would end by the end of the decade to not have them serviceable by the 2030s and operated when they cannot perform in that environment. Development of the airlifter depends heavily on the Army's "tactical and operational maneuver" plans. Two different cargo planes could still be created to separately perform tactical and strategic missions, but which course to pursue is to be decided before C-17s need to be retired.

The first batch of C-130A production aircraft were delivered beginning in 1956 to the 463d Troop Carrier Wing at Ardmore AFB, Oklahoma and the 314th Troop Carrier Wing at Sewart AFB, Tennessee. Six additional squadrons were assigned to the 322d Air Division in Europe and the 315th Air Division in the Far East. Additional aircraft were modified for electronics intelligence work and assigned to Rhein-Main Air Base, Germany while modified RC-130As were assigned to the Military Air Transport Service (MATS) photo-mapping division.

In 1958, a U.S. reconnaissance C-130A-II of the 7406th Support Squadron was shot down over Armenia by four Soviet MiG-17s along the Turkish-Armenian border during a routine mission.

Australia became the first non-American force to operate the C-130A Hercules with 12 examples being delivered from late 1958. The Royal Canadian Air Force became another early user with the delivery of four B-models (Canadian designation C-130 Mk I) in October / November 1960.

In 1963, a Hercules achieved and still holds the record for the largest and heaviest aircraft to land on an aircraft carrier. During October and November that year, a USMC KC-130F (BuNo "149798"), loaned to the U.S. Naval Air Test Center, made 29 touch-and-go landings, 21 unarrested full-stop landings and 21 unassisted take-offs on at a number of different weights. The pilot, Lieutenant (later Rear Admiral) James H. Flatley III, USN, was awarded the Distinguished Flying Cross for his role in this test series. The tests were highly successful, but the idea was considered too risky for routine carrier onboard delivery (COD) operations. Instead, the Grumman C-2 Greyhound was developed as a dedicated COD aircraft. The Hercules used in the test, most recently in service with Marine Aerial Refueler Squadron 352 (VMGR-352) until 2005, is now part of the collection of the National Museum of Naval Aviation at NAS Pensacola, Florida.

In 1964, C-130 crews from the 6315th Operations Group at Naha Air Base, Okinawa commenced forward air control (FAC; "Flare") missions over the Ho Chi Minh Trail in Laos supporting USAF strike aircraft. In April 1965 the mission was expanded to North Vietnam where C-130 crews led formations of Martin B-57 Canberra bombers on night reconnaissance/strike missions against communist supply routes leading to South Vietnam. In early 1966 Project Blind Bat/Lamplighter was established at Ubon Royal Thai Air Force Base, Thailand. After the move to Ubon, the mission became a four-engine FAC mission with the C-130 crew searching for targets then calling in strike aircraft. Another little-known C-130 mission flown by Naha-based crews was Operation Commando Scarf, which involved the delivery of chemicals onto sections of the Ho Chi Minh Trail in Laos that were designed to produce mud and landslides in hopes of making the truck routes impassable.

In November 1964, on the other side of the globe, C-130Es from the 464th Troop Carrier Wing but loaned to 322d Air Division in France, took part in Operation Dragon Rouge, one of the most dramatic missions in history in the former Belgian Congo. After communist Simba rebels took white residents of the city of Stanleyville hostage, the U.S. and Belgium developed a joint rescue mission that used the C-130s to drop, air-land and air-lift a force of Belgian paratroopers to rescue the hostages. Two missions were flown, one over Stanleyville and another over Paulis during Thanksgiving weeks. The headline-making mission resulted in the first award of the prestigious MacKay Trophy to C-130 crews.

In the Indo-Pakistani War of 1965, as a desperate measure the transport No. 6 Squadron of the Pakistan Air Force modified its entire small fleet of C-130Bs for use as heavy bombers, capable of carrying up to 20,000 lb (9,072 kg) of bombs on pallets. These improvised bombers were used to hit Indian targets such as bridges, heavy artillery positions, tank formations and troop concentrations. Some C-130s even flew with anti-aircraft guns fitted on their ramp, apparently shooting down some 17 aircraft and damaging 16 others.
In October 1968, a C-130Bs from the 463rd Tactical Airlift Wing dropped a pair of M-121 10,000-pound bombs that had been developed for the massive Convair B-36 Peacemaker bomber but had never been used. The U.S. Army and U.S. Air Force resurrected the huge weapons as a means of clearing landing zones for helicopters and in early 1969 the 463rd commenced Commando Vault missions. Although the stated purpose of COMMANDO VAULT was to clear LZs, they were also used on enemy base camps and other targets.

During the late 1960s, the U.S. was eager to get information on Chinese nuclear capabilities. After the failure of the Black Cat Squadron to plant operating sensor pods near the Lop Nur Nuclear Weapons Test Base using a Lockheed U-2, the CIA developed a plan, named "Heavy Tea", to deploy two battery-powered sensor pallets near the base. To deploy the pallets, a Black Bat Squadron crew was trained in the U.S. to fly the C-130 Hercules. The crew of 12, led by Col Sun Pei Zhen, took off from Takhli Royal Thai Air Force Base in an unmarked U.S. Air Force C-130E on 17 May 1969. Flying for six and a half hours at low altitude in the dark, they arrived over the target and the sensor pallets were dropped by parachute near Anxi in Gansu province. After another six and a half hours of low altitude flight, they arrived back at Takhli. The sensors worked and uploaded data to a U.S. intelligence satellite for six months before their batteries failed. The Chinese conducted two nuclear tests, on 22 September 1969 and 29 September 1969, during the operating life of the sensor pallets. Another mission to the area was planned as Operation "Golden Whip", but was called off in 1970. It is most likely that the aircraft used on this mission was either C-130E serial number 64-0506 or 64-0507 (cn 382-3990 and 382-3991). These two aircraft were delivered to Air America in 1964. After being returned to the U.S. Air Force sometime between 1966 and 1970, they were assigned the serial numbers of C-130s that had been destroyed in accidents. 64-0506 is now flying as 62-1843, a C-130E that crashed in Vietnam on 20 December 1965 and 64-0507 is now flying as 63-7785, a C-130E that had crashed in Vietnam on 17 June 1966.

The A-model continued in service through the Vietnam War, where the aircraft assigned to the four squadrons at Naha AB, Okinawa and one at Tachikawa Air Base, Japan performed yeoman's service, including operating highly classified special operations missions such as the BLIND BAT FAC/Flare mission and FACT SHEET leaflet mission over Laos and North Vietnam. The A-model was also provided to the Republic of Vietnam Air Force as part of the Vietnamization program at the end of the war, and equipped three squadrons based at Tan Son Nhut AFB. The last operator in the world is the Honduran Air Force, which is still flying one of five A model Hercules (FAH "558", c/n 3042) as of October 2009. As the Vietnam War wound down, the 463rd Troop Carrier/Tactical Airlift Wing B-models and A-models of the 374th Tactical Airlift Wing were transferred back to the United States where most were assigned to Air Force Reserve and Air National Guard units.
Another prominent role for the B model was with the United States Marine Corps, where Hercules initially designated as GV-1s replaced C-119s. After Air Force C-130Ds proved the type's usefulness in Antarctica, the U.S. Navy purchased a number of B-models equipped with skis that were designated as LC-130s. C-130B-II electronic reconnaissance aircraft were operated under the SUN VALLEY program name primarily from Yokota Air Base, Japan. All reverted to standard C-130B cargo aircraft after their replacement in the reconnaissance role by other aircraft.

The C-130 was also used in the 1976 Entebbe raid in which Israeli commando forces carried a surprise assault to rescue 103 passengers of an airliner hijacked by Palestinian and German terrorists at Entebbe Airport, Uganda. The rescue force—200 soldiers, jeeps, and a black Mercedes-Benz (intended to resemble Ugandan Dictator Idi Amin's vehicle of state)—was flown over almost entirely at an altitude of less than from Israel to Entebbe by four Israeli Air Force (IAF) Hercules aircraft without mid-air refueling (on the way back, the aircraft refueled in Nairobi, Kenya).

During the Falklands War () of 1982, Argentine Air Force C-130s undertook dangerous re-supply night flights as blockade runners to the Argentine garrison on the Falkland Islands. They also performed daylight maritime survey flights. One was shot down by a Royal Navy Sea Harrier using AIM-9 Sidewinders and cannon. The crew of seven were killed. Argentina also operated two KC-130 tankers during the war, and these refuelled both the Douglas A-4 Skyhawks and Navy Dassault-Breguet Super Étendards; some C-130s were modified to operate as bombers with bomb-racks under their wings. The British also used RAF C-130s to support their logistical operations.
During the Gulf War of 1991 (Operation "Desert Storm"), the C-130 Hercules was used operationally by the U.S. Air Force, U.S. Navy and U.S. Marine Corps, along with the air forces of Australia, New Zealand, Saudi Arabia, South Korea and the UK. The MC-130 Combat Talon variant also made the first attacks using the largest conventional bombs in the world, the BLU-82 "Daisy Cutter" and GBU-43/B "Massive Ordnance Air Blast" bomb, (MOAB). Daisy Cutters were used to clear landing zones and to eliminate mine fields. The weight and size of the weapons make it impossible or impractical to load them on conventional bombers. The GBU-43/B MOAB is a successor to the BLU-82 and can perform the same function, as well as perform strike functions against hardened targets in a low air threat environment.

Since 1992, two successive C-130 aircraft named "Fat Albert" have served as the support aircraft for the U.S. Navy Blue Angels flight demonstration team. "Fat Albert I" was a TC-130G ("151891"), while "Fat Albert II" is a C-130T ("164763"). Although "Fat Albert" supports a Navy squadron, it is operated by the U.S. Marine Corps (USMC) and its crew consists solely of USMC personnel. At some air shows featuring the team, "Fat Albert" takes part, performing flyovers. Until 2009, it also demonstrated its rocket-assisted takeoff (RATO) capabilities; these ended due to dwindling supplies of rockets.

The AC-130 also holds the record for the longest sustained flight by a C-130. From 22 to 24 October 1997, two AC-130U gunships flew 36 hours nonstop from Hurlburt Field, Florida to Taegu (Daegu), South Korea, being refuelled seven times by KC-135 tanker aircraft. This record flight beat the previous record longest flight by over 10 hours and the two gunships took on of fuel. The gunship has been used in every major U.S. combat operation since Vietnam, except for Operation "El Dorado Canyon", the 1986 attack on Libya.
During the invasion of Afghanistan in 2001 and the ongoing support of the International Security Assistance Force (Operation "Enduring Freedom"), the C-130 Hercules has been used operationally by Australia, Belgium, Canada, Denmark, France, Italy, the Netherlands, New Zealand, Norway, Portugal, Romania, South Korea, Spain, the UK and the United States.

During the 2003 invasion of Iraq (Operation "Iraqi Freedom"), the C-130 Hercules was used operationally by Australia, the UK and the United States. After the initial invasion, C-130 operators as part of the Multinational force in Iraq used their C-130s to support their forces in Iraq.

Since 2004, the Pakistan Air Force has employed C-130s in the War in North-West Pakistan. Some variants had forward looking infrared (FLIR Systems Star Safire III EO/IR) sensor balls, to enable close tracking of Islamist militants.

In 2017, France and Germany announced that they are to build up a joint air transport squadron at Evreux Air Base, France, comprising ten C-130J aircraft. Six of these will be operated by Germany. Initial operational capability is expected for 2021 while full operational capability is scheduled for 2024.

The U.S. Forest Service developed the Modular Airborne FireFighting System for the C-130 in the 1970s, which allows regular aircraft to be temporarily converted to an airtanker for fighting wildfires. In the late 1980s, 22 retired USAF C-130As were removed from storage at Davis-Monthan Air Force Base and transferred to the U.S. Forest Service, which then transferred them to six private companies to be converted into air tankers. After one of these aircraft crashed due to wing separation in flight as a result of fatigue stress cracking, the USFS stopped contracting the C-130A for fire fighting. Some operators continue to fly the C-130As for other missions. For example, C-130s were used to spread chemical dispersants onto the "Deepwater Horizon" oil spill in the Gulf Coast in 2010.

A recent development of a C-130–based airtanker is the Retardant Aerial Delivery System developed by Coulson Aviation USA. The system consists of a C-130H/Q retrofitted with an in-floor discharge system, combined with a removable 3,500- or 4,000-gallon water tank. The combined system is FAA certified.

Significant military variants of the C-130 include:


Former operators

The C-130 Hercules has had a low accident rate in general. The Royal Air Force recorded an accident rate of about one aircraft loss per 250,000 flying hours over the last 40 years, placing it behind Vickers VC10s and Lockheed TriStars with no flying losses. USAF C-130A/B/E-models had an overall attrition rate of 5% as of 1989 as compared to 1–2% for commercial airliners in the U.S., according to the NTSB, 10% for B-52 bombers, and 20% for fighters (F-4, F-111), trainers (T-37, T-38), and helicopters (H-3).

A total of 70 aircraft were lost by the U.S. Air Force and the U.S. Marine Corps during combat operations in the Vietnam War in Southeast Asia. By the nature of the Hercules' worldwide service, the pattern of losses provides an interesting barometer of the global hot spots over the past 50 years.










Notes
Citations
Bibliography




</doc>
<doc id="7699" url="https://en.wikipedia.org/wiki?curid=7699" title="Commodore 1570">
Commodore 1570

The Commodore 1570 is a 5¼" floppy disk drive for the Commodore 128 home/personal computer. It is a single-sided, 170 kB version of the Commodore 1571, released as a stopgap measure when Commodore International was unable to provide large enough quantities of 1571s due to a shortage of double-sided drive mechanisms (supplied from an outside manufacturer). Like the 1571, it can read and write both GCR and MFM disk formats.
The 1570 utilizes a 1571 logic board in a cream-colored original-1541-like case with a drive mechanism similar to the 1541's except that it was equipped with track-zero detection. Like the 1571, its built-in DOS provides a data burst mode for transferring data to the C128 computer at a faster speed than a 1541 can. Its ROM also contains some DOS bug fixes that didn't appear in the 1571 until much later. The 1570 can read and write all single-sided CP/M-format disks that the 1571 can access.

Although the 1570 is compatible with the Commodore 64, the C64 isn't capable of taking advantage of the drive's higher-speed operation, and when used with the C64 it's little more than a pricier 1541. Also, many early buyers of the C128 chose to temporarily make do with a 1541 drive, perhaps owned as part of a previous C64 setup, until the 1571 became more widely available.

The drive uses the CPU MOS 6502, floppy controller WD1770 or WD1772, I/O controllers 2x MOS Technology 6522 and 1x MOS Technology 6526.


</doc>
<doc id="7700" url="https://en.wikipedia.org/wiki?curid=7700" title="Commodore 1571">
Commodore 1571

The Commodore 1571 is Commodore's high-end 5¼" floppy disk drive. With its double-sided drive mechanism, it has the ability to use double-sided, double-density (DS/DD) floppy disks natively. This is in contrast to its predecessors, the 1541 and 1570, which can fully read and write such disks only if the user manually flipped them over to access the second side. Because flipping the disk also reverses the direction of rotation, the two methods are not interchangeable; disks which had their back side created in a 1541 by flipping them over would have to be flipped in the 1571 too, and the back side of disks written in a 1571 using the native support for two-sided operation could not be read in a 1541.

The 1571 was released to match the Commodore 128, both design-wise and feature-wise. It was announced in the summer of 1985, at the same time as the C128, and became available in quantity later that year. The later C128"D" had a 1571-compatible drive integrated in the system unit. A double-sided disk on the 1571 would have a capacity of 340 kB (70 tracks, 1,360 disk blocks of 256 bytes each); as 8 kB are reserved for system use (directory and block availability information) and, under of each block serve as pointers to the next logical block, = 337,312 B or about were available for user data. (However, with a program organizing disk storage on its own, all space could be used, e.g. for data disks.)

The 1571 was designed to accommodate the C128's "burst" mode for 2x faster disk access, however the drive cannot use it if connected to older Commodore machines. This mode replaced the slow bit-banging serial routines of the 1541 with a true serial shift register implemented in hardware, thus dramatically increasing the drive speed. Although this originally had been planned when Commodore first switched from the parallel IEEE-488 interface to a custom serial interface (CBM-488), hardware bugs in the VIC-20's 6522 VIA shift register prevented it from working properly.

When connected to a C128, the 1571 would default to double-sided mode, which allowed the drive to read its own 340k disks as well as single-sided 170 kB 1541 disks. If the C128 was switched into C64 mode by typing GO 64 from BASIC, the 1571 will stay in double-sided mode. If C64 mode was activated by holding down the C= key on power-up, the drive would automatically switch to single-sided mode, in which case it is unable to read 340 kB disks (also the default if a 1571 is used with a C64, Plus/4, VIC-20, or PET). A manual command can also be issued from BASIC to switch the 1571 between single and double sided mode. There is also an undocumented command which allows the user to independently control either of the read/write heads of the 1571, making it possible to format both sides of a diskette separate from each other, however the resultant disk cannot be read in a 1541 as it would be spinning in reverse direction when flipped upside down. In the same vein, "flippy" disks created with a 1541 cannot be read on a 1571 with this feature; they must be inserted upside down.

The 1571 is not 100% low-level compatible with the 1541, however this isn't a problem except in some software that uses advanced copy protections such as the RapidLok system found on Microprose and Accolade games.

The 1571 was noticeably quieter than its predecessor and tended to run cooler as well, even though, like the 1541, it had an internal power supply (later Commodore drives, like the 1541-II and the 3½" 1581, came with external power supplies). The 1541-II/1581 power supply makes mention of a 1571-II, hinting that Commodore may have intended to release a version of the 1571 with an external power supply. However, no 1571-IIs are known to exist. The embedded OS in the 1571 was an improvement over the 

Early 1571s had a bug in the ROM-based disk operating system that caused relative files to corrupt if they occupied both sides of the disk. A version 2 ROM was released, but though it cured the initial bug, it introduced some minor quirks of its own - particularly with the 1541 emulation. Curiously, it was also identified as V3.0.

As with the 1541, Commodore initially could not meet demand for the 1571, and that lack of availability and the drive's relatively high price (about US$300) presented an opportunity for cloners. Two 1571 clones appeared, one from Oceanic and one from Blue Chip, but legal action from Commodore quickly drove them from the market.

Commodore announced at the 1985 Consumer Electronics Show a dual-drive version of the 1571, to be called the Commodore 1572, but quickly canceled it, reportedly due to technical difficulties with the 1572 DOS. It would have had four times as much RAM as the 1571 (8 kB), and twice as much ROM (64 kB). The 1572 would have allowed for fast disk backups of non-copy-protected media, much like the old 4040, 8050, and 8250 dual drives.

The 1571 built into the European plastic-case C128 D computer is electronically identical to the stand-alone version, but 1571 version integrated into the later metal-case C128 D (often called C128 DCR, for D Cost-Reduced) differs a lot from the stand-alone 1571. It includes a newer DOS, version 3.1, replaces the MOS Technology CIA interface chip, of which only a few features were used by the 1571 DOS, with a very much simplified chip called 5710, and has some compatibility issues with the stand-alone drive. Because this internal 1571 does not have an unused 8-bit input/output port on any chip, unlike most other Commodore drives, it is not possible to install a parallel cable in this drive, such as that used by SpeedDOS, DolphinDOS and some other fast third-party Commodore DOS replacements.

The drive detects the motor speed and generates an internal data sampling clock signal that matches with the motor speed.

The 1571 uses a saddle canceler when reading the data stream. A correction signal is generated when the raw data pattern on the disk consists of two consecutive zeros. With the GCR recording format a problem occurs in the read signal waveform. The worst case pattern 1001 may cause a saddle condition where a false data bit may occur. The original 1541 drives uses a one-shot to correct the condition. The 1571 uses a gate array to corrected this digitally.

The drive uses the MOS 6502 CPU, WD1770 or WD1772 floppy controller, 2x MOS Technology 6522 I/O controllers and 1x MOS Technology 6526.

Unlike the 1541, which was limited to GCR formatting, the 1571 could read both GCR and MFM disk formats. The version of CP/M included with the C128 supported the following formats:


If the CP/M BIOS is modified, it is possible to read any soft sector 40-track MFM format. Single density (FM) formats are not supported because the density selector pin on the MFM controller chip in the drive is disabled (wired to ground).

A 1571 cannot boot from MFM disks; the user must boot CP/M from a GCR disk and then switch to MFM disks.

With additional software, it was possible to read and write to MS-DOS-formatted floppies as well. Numerous commercial and public-domain programs for this purpose became available, the best-known being SOGWAP's "Big Blue Reader". Although the C128 could not run any DOS-based software, this capability allowed data files to be exchanged with PC users. Reading or disks was possible as well with special software, but the standard format, which used FM rather than MFM encoding, could not be handled by the 1571 hardware without modifying the drive circuitry as the control line that determines if FM or MFM encoding is used by the disc controller chip was permanently wired to ground (MFM mode) rather than being under software control.

In the 1541 format, while 40 tracks are possible for a drive like the 154x/157x, only are used. Commodore chose not to use the upper five tracks by default (or at least to use more than 35) due to the bad quality of some of the drive mechanisms, which did not always work reliably on those tracks.

For compatibility and ease of implementation, the 1571's double-sided format of one logical disk side with was created by putting together the lower 35 physical tracks on each of the physical sides of the disk rather than using two times even though there were no more quality problems with the mechanisms of the 1571 drives.



</doc>
<doc id="7701" url="https://en.wikipedia.org/wiki?curid=7701" title="Cocaine">
Cocaine

Cocaine, also known as coke, is a strong stimulant mostly used as a recreational drug. It is commonly snorted, inhaled as smoke, or dissolved and injected into a vein. Mental effects may include loss of contact with reality, an intense feeling of happiness, or agitation. Physical symptoms may include a fast heart rate, sweating, and large pupils. High doses can result in very high blood pressure or body temperature. Effects begin within seconds to minutes of use and last between five and ninety minutes. Cocaine has a small number of accepted medical uses such as numbing and decreasing bleeding during nasal surgery.
Cocaine is addictive due to its effect on the reward pathway in the brain. After a short period of use, there is a high risk that dependence will occur. Its use also increases the risk of stroke, myocardial infarction, lung problems in those who smoke it, blood infections, and sudden cardiac death. Cocaine sold on the street is commonly mixed with local anesthetics, cornstarch, quinine, or sugar, which can result in additional toxicity. Following repeated doses a person may have decreased ability to feel pleasure and be very physically tired.
Cocaine acts by inhibiting the reuptake of serotonin, norepinephrine, and dopamine. This results in greater concentrations of these three neurotransmitters in the brain. It can easily cross the blood–brain barrier and may lead to the breakdown of the barrier. Cocaine is a naturally occurring substance found in the "coca" plant which is mostly grown in South America. In 2013, 419 kilograms were produced legally. It is estimated that the illegal market for cocaine is 100 to 500 billion USD each year. With further processing crack cocaine can be produced from cocaine.
After cannabis, cocaine is the most frequently used illegal drug globally. Between 14 and 21 million people use the drug each year. Use is highest in North America followed by Europe and South America. Between one and three percent of people in the developed world have used cocaine at some point in their life. In 2013 cocaine use directly resulted in 4,300 deaths, up from 2,400 in 1990. The leaves of the "coca" plant have been used by Peruvians since ancient times. Cocaine was first isolated from the leaves in 1860. Since 1961 the international Single Convention on Narcotic Drugs has required countries to make recreational use of cocaine a crime.

Topical cocaine can be used as a local numbing agent to help with painful procedures in the mouth or nose. TAC is one such formulation used for pediatrics.

Cocaine is now predominantly used for nasal and lacrimal duct surgery. The major disadvantages of this use are cocaine's potential for cardiovascular toxicity, glaucoma, and pupil dilation. Medicinal use of cocaine has decreased as other synthetic local anesthetics such as benzocaine, proparacaine, lidocaine, and tetracaine are now used more often. If vasoconstriction is desired for a procedure (as it reduces bleeding), the anesthetic is combined with a vasoconstrictor such as phenylephrine or epinephrine. Some ENT specialists occasionally use cocaine within the practice when performing procedures such as nasal cauterization. In this scenario dissolved cocaine is soaked into a ball of cotton wool, which is placed in the nostril for the 10–15 minutes immediately before the procedure, thus performing the dual role of both numbing the area to be cauterized, and vasoconstriction. Even when used this way, some of the used cocaine may be absorbed through oral or nasal mucosa and give systemic effects. An alternative method of administration for ENT surgery is mixed with adrenaline and sodium bicarbonate, as Moffett's solution.

Cocaine is a powerful nervous system stimulant. Its effects can last from fifteen or thirty minutes to an hour. The duration of cocaine's effects depends on the amount taken and the route of administration. Cocaine can be in the form of fine white powder, bitter to the taste. When inhaled or injected, it causes a numbing effect. Crack cocaine is a smokeable form of cocaine made into small "rocks" by processing cocaine with sodium bicarbonate (baking soda) and water. Crack cocaine is referred to as "crack" because of the crackling sounds it makes when heated.

Cocaine use leads to increases in alertness, feelings of well-being and euphoria, increased energy and motor activity, and increased feelings of competence and sexuality.

Many users rub the powder along the gum line, or onto a cigarette filter which is then smoked, which numbs the gums and teeth – hence the colloquial names of "numbies", "gummers", or "cocoa puffs" for this type of administration. This is mostly done with the small amounts of cocaine remaining on a surface after insufflation (snorting). Another oral method is to wrap up some cocaine in rolling paper and swallow (parachute) it.

Coca leaves are typically mixed with an alkaline substance (such as lime) and chewed into a wad that is retained in the mouth between gum and cheek (much the same as chewing tobacco is chewed) and sucked of its juices. The juices are absorbed slowly by the mucous membrane of the inner cheek and by the gastrointestinal tract when swallowed. Alternatively, coca leaves can be infused in liquid and consumed like tea. Ingesting coca leaves generally is an inefficient means of administering cocaine.

Because cocaine is hydrolyzed and rendered inactive in the acidic stomach, it is not readily absorbed when ingested alone. Only when mixed with a highly alkaline substance (such as lime) can it be absorbed into the bloodstream through the stomach. The efficiency of absorption of orally administered cocaine is limited by two additional factors. First, the drug is partly catabolized by the liver. Second, capillaries in the mouth and esophagus constrict after contact with the drug, reducing the surface area over which the drug can be absorbed. Nevertheless, cocaine metabolites can be detected in the urine of subjects that have sipped even one cup of coca leaf infusion.

Orally administered cocaine takes approximately 30 minutes to enter the bloodstream. Typically, only a third of an oral dose is absorbed, although absorption has been shown to reach 60% in controlled settings. Given the slow rate of absorption, maximum physiological and psychotropic effects are attained approximately 60 minutes after cocaine is administered by ingestion. While the onset of these effects is slow, the effects are sustained for approximately 60 minutes after their peak is attained.

Contrary to popular belief, both ingestion and insufflation result in approximately the same proportion of the drug being absorbed: 30 to 60%. Compared to ingestion, the faster absorption of insufflated cocaine results in quicker attainment of maximum drug effects. Snorting cocaine produces maximum physiological effects within 40 minutes and maximum psychotropic effects within 20 minutes, however, a more realistic activation period is closer to 5 to 10 minutes. Physiological and psychotropic effects from nasally insufflated cocaine are sustained for approximately 40–60 minutes after the peak effects are attained.

Coca tea, an infusion of coca leaves, is also a traditional method of consumption. The tea has often been recommended for travelers in the Andes to prevent altitude sickness. However, its actual effectiveness has never been systematically studied. This method of consumption has been practised for many centuries by the indigenous tribes of South America. One specific purpose of ancient coca leaf consumption was to increase energy and reduce fatigue in messengers who made multi-day quests to other settlements.

In 1986 an article in the "Journal of the American Medical Association" revealed that U.S. health food stores were selling dried coca leaves to be prepared as an infusion as "Health Inca Tea." While the packaging claimed it had been "decocainized", no such process had actually taken place. The article stated that drinking two cups of the tea per day gave a mild stimulation, increased heart rate, and mood elevation, and the tea was essentially harmless. Despite this, the DEA seized several shipments in Hawaii, Chicago, Georgia, and several locations on the East Coast of the United States, and the product was removed from the shelves.

Nasal insufflation (known colloquially as "snorting", "sniffing", or "blowing") is a common method of ingestion of recreational powdered cocaine. The drug coats and is absorbed through the mucous membranes lining the nasal passages. Cocaine's desired euphoric effects are delayed when snorted through the nose by about five minutes. This occurs because cocaine's absorption is slowed by its constricting effect on the blood vessels of the nose. Insufflation of cocaine also leads to the longest duration of its effects (60–90 minutes). When insufflating cocaine, absorption through the nasal membranes is approximately 30–60%, with higher doses leading to increased absorption efficiency. Any material not directly absorbed through the mucous membranes is collected in mucus and swallowed (this "drip" is considered pleasant by some and unpleasant by others).

In a study of cocaine users, the average time taken to reach peak subjective effects was 14.6 minutes. Any damage to the inside of the nose is because cocaine highly constricts blood vessels – and therefore blood and oxygen/nutrient flow – to that area. Nosebleeds after cocaine insufflation are due to irritation and damage of mucus membranes by foreign particles and adulterants and not the cocaine itself; as a vasoconstrictor, cocaine acts to reduce bleeding.

Rolled up banknotes, hollowed-out pens, cut straws, pointed ends of keys, specialized spoons, long fingernails, and (clean) tampon applicators are often used to insufflate cocaine. Such devices are often called "tooters" by users. The cocaine typically is poured onto a flat, hard surface (such as a mirror, CD case or book) and divided into "bumps", "lines" or "rails", and then insufflated. The amount of cocaine in a line varies widely from person to person and occasion to occasion (the purity of the cocaine is also a factor), but one line is generally considered to be a single dose and is typically 35 mg (a "bump") to 100 mg (a "rail"). As tolerance builds rapidly in the short-term (hours), many lines are often snorted to produce greater effects. A 2001 study reported that the sharing of straws used to "snort" cocaine can spread blood diseases such as hepatitis C.

Drug injection by turning the drug into a solution provides the highest blood levels of drug in the shortest amount of time. Subjective effects not commonly shared with other methods of administration include a ringing in the ears moments after injection (usually when in excess of 120 milligrams) lasting 2 to 5 minutes including tinnitus and audio distortion. This is colloquially referred to as a "bell ringer". In a study of cocaine users, the average time taken to reach peak subjective effects was 3.1 minutes. The euphoria passes quickly. Aside from the toxic effects of cocaine, there is also danger of circulatory emboli from the insoluble substances that may be used to cut the drug. As with all injected illicit substances, there is a risk of the user contracting blood-borne infections if sterile injecting equipment is not available or used. Additionally, because cocaine is a vasoconstrictor, and usage often entails multiple injections within several hours or less, subsequent injections are progressively more difficult to administer, which in turn may lead to more injection attempts and more consequences from improperly performed injection.

An injected mixture of cocaine and heroin, known as "speedball" is a particularly dangerous combination, as the converse effects of the drugs actually complement each other, but may also mask the symptoms of an overdose. It has been responsible for numerous deaths, including celebrities such as comedians/actors John Belushi and Chris Farley, Mitch Hedberg, River Phoenix, grunge singer Layne Staley and actor Philip Seymour Hoffman. Experimentally, cocaine injections can be delivered to animals such as fruit flies to study the mechanisms of cocaine addiction.

Inhalation by smoking cocaine is one of the several ways the drug is consumed. The onset of cocaine's desired euphoric effects is fastest with inhaling cocaine and begins after 3–5 seconds. In contrast, inhalation of cocaine leads to the shortest duration of its effects (5–15 minutes). The two main ways cocaine is smoked are freebasing and by using cocaine which has been converted to smokable "crack cocaine". Cocaine is smoked by inhaling the vapor produced when solid cocaine is heated to the point that it sublimates. In a 2000 Brookhaven National Laboratory medical department study, based on self reports of 32 abusers who participated in the study,"peak high" was found at mean of 1.4min +/- 0.5 minutes. Pyrolysis products of cocaine that occur only when heated/smoked have been shown to change the effect profile, "i.e." anhydroecgonine methyl ester when co-administered with cocaine increases the dopamine in CPu and NAc brain regions, and has M- and M- receptor affinity.

Smoking freebase or crack cocaine is most often accomplished using a pipe made from a small glass tube, often taken from "love roses", small glass tubes with a paper rose that are promoted as romantic gifts. These are sometimes called "stems", "horns", "blasters" and "straight shooters". A small piece of clean heavy copper or occasionally stainless steel scouring padoften called a "brillo" (actual Brillo Pads contain soap, and are not used) or "chore" (named for Chore Boy brand copper scouring pads)serves as a reduction base and flow modulator in which the "rock" can be melted and boiled to vapor. Crack smokers also sometimes smoke through a soda can with small holes on the side or bottom. Crack is smoked by placing it at the end of the pipe; a flame held close to it produces vapor, which is then inhaled by the smoker. The effects, felt almost immediately after smoking, are very intense and do not last long usually 2 to 10 minutes. When smoked, cocaine is sometimes combined with other drugs, such as cannabis, often rolled into a joint or blunt. Powdered cocaine is also sometimes smoked, though heat destroys much of the chemical; smokers often sprinkle it on cannabis. The language referring to paraphernalia and practices of smoking cocaine vary, as do the packaging methods in the street level sale.

Another way users consume cocaine is by making it into a suppository which they then insert into the anus or vagina. The drug is then absorbed by the membranes of these body parts. Little research has been focused on the suppository (anal or vaginal insertion) method of administration, also known as "plugging". This method of administration is commonly administered using an oral syringe. Cocaine can be dissolved in water and withdrawn into an oral syringe which may then be lubricated and inserted into the anus or vagina before the plunger is pushed. Anecdotal evidence of its effects is infrequently discussed, possibly due to social taboos in many cultures. The rectum and the vaginal canal is where the majority of the drug would be taken up through the membranes lining its walls.

With excessive or prolonged use, the drug can cause itching, fast heart rate, hallucinations, and paranoid delusions. Overdoses cause hyperthermia and a marked elevation of blood pressure, which can be life-threatening, arrhythmias, and death.

Anxiety, paranoia, and restlessness can also occur, especially during the comedown. With excessive dosage, tremors, convulsions and increased body temperature are observed. Severe cardiac adverse events, particularly sudden cardiac death, become a serious risk at high doses due to cocaine's blocking effect on cardiac sodium channels.
Chronic cocaine intake causes strong imbalances of transmitter levels in order to compensate extremes. Thus, receptors disappear from the cell surface or reappear on it, resulting more or less in an "off" or "working mode" respectively, or they change their susceptibility for binding partners (ligands)mechanisms called downregulation and upregulation. However, studies suggest cocaine abusers do not show normal age-related loss of striatal dopamine transporter (DAT) sites, suggesting cocaine has neuroprotective properties for dopamine neurons. Possible side effects include insatiable hunger, aches, insomnia/oversleeping, lethargy, and persistent runny nose. Depression with suicidal ideation may develop in very heavy users. Finally, a loss of vesicular monoamine transporters, neurofilament proteins, and other morphological changes appear to indicate a long term damage of dopamine neurons. All these effects contribute a rise in tolerance thus requiring a larger dosage to achieve the same effect.
The lack of normal amounts of serotonin and dopamine in the brain is the cause of the dysphoria and depression felt after the initial high. Physical withdrawal is not dangerous. Physiological changes caused by cocaine withdrawal include vivid and unpleasant dreams, insomnia or hypersomnia, increased appetite and psychomotor retardation or agitation.

Physical side effects from chronic smoking of cocaine include coughing up blood, bronchospasm, itching, fever, diffuse alveolar infiltrates without effusions, pulmonary and systemic eosinophilia, chest pain, lung trauma, sore throat, asthma, hoarse voice, dyspnea (shortness of breath), and an aching, flu-like syndrome. Cocaine constricts blood vessels, dilates pupils, and increases body temperature, heart rate, and blood pressure. It can also cause headaches and gastrointestinal complications such as abdominal pain and nausea. A common but untrue belief is that the smoking of cocaine chemically breaks down tooth enamel and causes tooth decay. However, cocaine does often cause involuntary tooth grinding, known as bruxism, which can deteriorate tooth enamel and lead to gingivitis. Additionally, stimulants like cocaine, methamphetamine, and even caffeine cause dehydration and dry mouth. Since saliva is an important mechanism in maintaining one's oral pH level, chronic stimulant abusers who do not hydrate sufficiently may experience demineralization of their teeth due to the pH of the tooth surface dropping too low (below 5.5). Cocaine use also promotes the formation of blood clots. This increase in blood clot formation is attributed to cocaine-associated increases in the activity of plasminogen activator inhibitor, and an increase in the number, activation, and aggregation of platelets.

Chronic intranasal usage can degrade the cartilage separating the nostrils (the septum nasi), leading eventually to its complete disappearance. Due to the absorption of the cocaine from cocaine hydrochloride, the remaining hydrochloride forms a dilute hydrochloric acid.

Cocaine may also greatly increase this risk of developing rare autoimmune or connective tissue diseases such as lupus, Goodpasture syndrome, vasculitis, glomerulonephritis, Stevens–Johnson syndrome, and other diseases. It can also cause a wide array of kidney diseases and kidney failure.

Cocaine use leads to an increased risk of hemorrhagic and ischemic strokes. Cocaine use also increases the risk of having a heart attack.

Cocaine addiction occurs through ΔFosB overexpression in the nucleus accumbens, which results in altered transcriptional regulation in neurons within the nucleus accumbens.

ΔFosB levels have been found to increase upon the use of cocaine. Each subsequent dose of cocaine continues to increase ΔFosB levels with no ceiling of tolerance. Elevated levels of ΔFosB leads to increases in brain-derived neurotrophic factor (BDNF) levels, which in turn increases the number of dendritic branches and spines present on neurons involved with the nucleus accumbens and prefrontal cortex areas of the brain. This change can be identified rather quickly, and may be sustained weeks after the last dose of the drug.

Transgenic mice exhibiting inducible expression of ΔFosB primarily in the nucleus accumbens and dorsal striatum exhibit sensitized behavioural responses to cocaine. They self-administer cocaine at lower doses than control, but have a greater likelihood of relapse when the drug is withheld. ΔFosB increases the expression of AMPA receptor subunit GluR2 and also decreases expression of dynorphin, thereby enhancing sensitivity to reward.

Cocaine dependence is a form of psychological dependence that develops from regular cocaine use and produces a withdrawal state with emotional-motivational deficits upon cessation of cocaine use.

Cocaine is known to have a number of deleterious effects during pregnancy. Pregnant people who use cocaine have an elevated risk of placental abruption, a condition where the placenta detaches from the uterus and causes bleeding. Due to its vasoconstrictive and hypertensive effects, they are also at risk for hemorrhagic stroke and myocardial infarction. Cocaine is also teratogenic, meaning that it can cause birth defects and fetal malformations. In-utero exposure to cocaine is associated with behavioral abnormalities, cognitive impairment, cardiovascular malformations, intrauterine growth restriction, preterm birth, urinary tract malformations, and cleft lip and palate.

The pharmacodynamics of cocaine involve the complex relationships of neurotransmitters (inhibiting monoamine uptake in rats with ratios of about: serotonin:dopamine = 2:3, serotonin:norepinephrine = 2:5). The most extensively studied effect of cocaine on the central nervous system is the blockade of the dopamine transporter protein. Dopamine transmitter released during neural signaling is normally recycled via the transporter; i.e., the transporter binds the transmitter and pumps it out of the synaptic cleft back into the presynaptic neuron, where it is taken up into storage vesicles. Cocaine binds tightly at the dopamine transporter forming a complex that blocks the transporter's function. The dopamine transporter can no longer perform its reuptake function, and thus dopamine accumulates in the synaptic cleft.

Cocaine's affects certain serotonin (5-HT) receptors; in particular, it has been shown to antagonize the 5-HT3 receptor, which is a ligand-gated ion channel. The overabundance of 5-HT3 receptors in cocaine conditioned rats display this trait, however the exact effect of 5-HT3 in this process is unclear. The 5-HT2 receptor (particularly the subtypes 5-HT2AR, 5-HT2BR and 5-HT2CR) are involved in the locomotor-activating effects of cocaine.

Cocaine has been demonstrated to bind as to directly stabilize the DAT transporter on the open outward-facing conformation. Further, cocaine binds in such a way as to inhibit a hydrogen bond innate to DAT. Cocaine's binding properties are such that it attaches so this hydrogen bond will not form and is blocked from formation due to the tightly locked orientation of the cocaine molecule. Research studies have suggested that the affinity for the transporter is not what is involved in habituation of the substance so much as the conformation and binding properties to where and how on the transporter the molecule binds.

Sigma receptors are affected by cocaine, as cocaine functions as a sigma ligand agonist. Further specific receptors it has been demonstrated to function on are NMDA and the D1 dopamine receptor.

Cocaine also blocks sodium channels, thereby interfering with the propagation of action potentials; thus, like lignocaine and novocaine, it acts as a local anesthetic. It also functions on the binding sites to the dopamine and serotonin sodium dependent transport area as targets as separate mechanisms from its reuptake of those transporters; unique to its local anesthetic value which makes it in a class of functionality different from both its own derived phenyltropanes analogues which have that removed. In addition to this cocaine has some target binding to the site of the Kappa-opioid receptor as well. Cocaine also causes vasoconstriction, thus reducing bleeding during minor surgical procedures. The locomotor enhancing properties of cocaine may be attributable to its enhancement of dopaminergic transmission from the substantia nigra. Recent research points to an important role of circadian mechanisms and clock genes in behavioral actions of cocaine.

Cocaine can often cause reduced food intake, many chronic users lose their appetite and can experience severe malnutrition and significant weight loss. Cocaine effects, further, are shown to be potentiated for the user when used in conjunction with new surroundings and stimuli, and otherwise novel environs.

Cocaine has a short half life of 0.7-1.5 hours and is extensively metabolized by cholinesterase enzymes (primarily in the liver and plasma), with only about 1% excreted unchanged in the urine. The metabolism is dominated by hydrolytic ester cleavage, so the eliminated metabolites consist mostly of benzoylecgonine (BE), the major metabolite, and other significant metabolites in lesser amounts such as ecgonine methyl ester (EME) and ecgonine. Further minor metabolites of cocaine include norcocaine, p-hydroxycocaine, m-hydroxycocaine, p-hydroxybenzoylecgonine (pOHBE), and m-hydroxybenzoylecgonine. If consumed with alcohol, cocaine combines with alcohol in the liver to form cocaethylene. Studies have suggested cocaethylene is both more euphoric, and has a higher cardiovascular toxicity than cocaine by itself.

Depending on liver and kidney function, cocaine metabolites are detectable in urine. Benzoylecgonine can be detected in urine within four hours after cocaine intake and remains detectable in concentrations greater than 150 ng/mL typically for up to eight days after cocaine is used. Detection of accumulation of cocaine metabolites in hair is possible in regular users until the sections of hair grown during use are cut or fall out.

Cocaine in its purest form is a white, pearly product. Cocaine appearing in powder form is a salt, typically cocaine hydrochloride. Street cocaine is often adulterated or "cut" with talc, lactose, sucrose, glucose, mannitol, inositol, caffeine, procaine, phencyclidine, phenytoin, lignocaine, strychnine, amphetamine, or heroin.

The color of "crack" cocaine depends upon several factors including the origin of the cocaine used, the method of preparation – with ammonia or baking soda – and the presence of impurities, but will generally range from white to a yellowish cream to a light brown. Its texture will also depend on the adulterants, origin and processing of the powdered cocaine, and the method of converting the base. It ranges from a crumbly texture, sometimes extremely oily, to a hard, almost crystalline nature.

Cocaine – a tropane alkaloid – is a weakly alkaline compound, and can therefore combine with acidic compounds to form various salts. The hydrochloride (HCl) salt of cocaine is by far the most commonly encountered, although the sulfate (-SO) and the nitrate (-NO) are occasionally seen. Different salts dissolve to a greater or lesser extent in various solvents – the hydrochloride salt is polar in character and is quite soluble in water.

As the name implies, "freebase" is the base form of cocaine, as opposed to the salt form. It is practically insoluble in water whereas hydrochloride salt is water-soluble.

Smoking freebase cocaine has the additional effect of releasing methylecgonidine into the user's system due to the pyrolysis of the substance (a side effect which insufflating or injecting powder cocaine does not create). Some research suggests that smoking freebase cocaine can be even more cardiotoxic than other routes of administration because of methylecgonidine's effects on lung tissue and liver tissue.

Pure cocaine is prepared by neutralizing its compounding salt with an alkaline solution, which will precipitate to non-polar basic cocaine. It is further refined through aqueous-solvent liquid–liquid extraction.

Crack is a lower purity form of free-base cocaine that is usually produced by neutralization of cocaine hydrochloride with a solution of baking soda (sodium bicarbonate, NaHCO) and water, producing a very hard/brittle, off-white-to-brown colored, amorphous material that contains sodium carbonate, entrapped water, and other by-products as the main impurities.

The "freebase" and "crack" forms of cocaine are usually administered by vaporization of the powdered substance into smoke, which is then inhaled.

The origin of the name "crack" comes from the "crackling" sound (and hence the onomatopoeic moniker "crack") that is produced when the cocaine and its impurities (i.e. water, sodium bicarbonate) are heated past the point of vaporization.

Pure cocaine base/crack can be smoked because it vaporizes smoothly, with little or no decomposition at , which is below the boiling point of water.

In contrast, cocaine hydrochloride does not vaporize until heated to a much higher temperature (about 197 °C), and considerable decomposition/burning occurs at these high temperatures. This effectively destroys some of the cocaine and yields a sharp, acrid, and foul-tasting smoke.

Smoking or vaporizing cocaine and inhaling it into the lungs produces an almost immediate "high" that can be very powerful (and addicting) quite rapidly – this initial crescendo of stimulation is known as a "rush". While the stimulating effects may last for hours, the euphoric sensation is very brief, prompting the user to smoke more immediately.

Coca herbal infusion (also referred to as coca tea) is used in coca-leaf producing countries much as any herbal medicinal infusion would elsewhere in the world. The free and legal commercialization of dried coca leaves under the form of filtration bags to be used as "coca tea" has been actively promoted by the governments of Peru and Bolivia for many years as a drink having medicinal powers. Visitors to the city of Cuzco in Peru, and La Paz in Bolivia are greeted with the offering of coca leaf infusions (prepared in teapots with whole coca leaves) purportedly to help the newly arrived traveler overcome the malaise of high altitude sickness. The effects of drinking coca tea are a mild stimulation and mood lift. It does not produce any significant numbing of the mouth nor does it give a rush like snorting cocaine. In order to prevent the demonization of this product, its promoters publicize the unproven concept that much of the effect of the ingestion of coca leaf infusion would come from the secondary alkaloids, as being not only quantitatively different from pure cocaine but also qualitatively different.

It has been promoted as an adjuvant for the treatment of cocaine dependence. In one controversial study, coca leaf infusion was used—in addition to counseling—to treat 23 addicted coca-paste smokers in Lima, Peru. Relapses fell from an average of four times per month before treatment with coca tea to one during the treatment. The duration of abstinence increased from an average of 32 days prior to treatment to 217 days during treatment. These results suggest that the administration of coca leaf infusion plus counseling would be an effective method for preventing relapse during treatment for cocaine addiction. Importantly, these results also suggest strongly that the primary pharmacologically active metabolite in coca leaf infusions is actually cocaine and not the secondary alkaloids.

The cocaine metabolite benzoylecgonine can be detected in the urine of people a few hours after drinking one cup of coca leaf infusion.

The first synthesis and elucidation of the cocaine molecule was by Richard Willstätter in 1898. Willstätter's synthesis derived cocaine from tropinone. Since then, Robert Robinson and Edward Leete have made significant contributions to the mechanism of the synthesis. (-NO)

The additional carbon atoms required for the synthesis of cocaine are derived from acetyl-CoA, by addition of two acetyl-CoA units to the "N"-methyl-Δ-pyrrolinium cation. The first addition is a Mannich-like reaction with the enolate anion from acetyl-CoA acting as a nucleophile towards the pyrrolinium cation. The second addition occurs through a Claisen condensation. This produces a racemic mixture of the 2-substituted pyrrolidine, with the retention of the thioester from the Claisen condensation. In formation of tropinone from racemic ethyl [2,3-13C](Nmethyl-2-pyrrolidinyl)-3-oxobutanoate there is no preference for either stereoisomer. In the biosynthesis of cocaine, however, only the (S)-enantiomer can cyclize to form the tropane ring system of cocaine. The stereoselectivity of this reaction was further investigated through study of prochiral methylene hydrogen discrimination. This is due to the extra chiral center at C-2. This process occurs through an oxidation, which regenerates the pyrrolinium cation and formation of an enolate anion, and an intramolecular Mannich reaction. The tropane ring system undergoes hydrolysis, SAM-dependent methylation, and reduction via NADPH for the formation of methylecgonine. The benzoyl moiety required for the formation of the cocaine diester is synthesized from phenylalanine via cinnamic acid. Benzoyl-CoA then combines the two units to form cocaine.

The biosynthesis begins with L-Glutamine, which is derived to L-ornithine in plants. The major contribution of L-ornithine and L-arginine as a precursor to the tropane ring was confirmed by Edward Leete. Ornithine then undergoes a pyridoxal phosphate-dependent decarboxylation to form putrescine. In animals, however, the urea cycle derives putrescine from ornithine. L-ornithine is converted to L-arginine, which is then decarboxylated via PLP to form agmatine. Hydrolysis of the imine derives "N"-carbamoylputrescine followed with hydrolysis of the urea to form putrescine. The separate pathways of converting ornithine to putrescine in plants and animals have converged. A SAM-dependent "N"-methylation of putrescine gives the "N"-methylputrescine product, which then undergoes oxidative deamination by the action of diamine oxidase to yield the aminoaldehyde. Schiff base formation confirms the biosynthesis of the "N"-methyl-Δ-pyrrolinium cation.

The biosynthesis of the tropane alkaloid, however, is still uncertain. Hemscheidt proposes that Robinson's acetonedicarboxylate emerges as a potential intermediate for this reaction. Condensation of "N"-methylpyrrolinium and acetonedicarboxylate would generate the oxobutyrate. Decarboxylation leads to tropane alkaloid formation.

The reduction of tropinone is mediated by NADPH-dependent reductase enzymes, which have been characterized in multiple plant species. These plant species all contain two types of the reductase enzymes, tropinone reductase I and tropinone reductase II. TRI produces tropine and TRII produces pseudotropine. Due to differing kinetic and pH/activity characteristics of the enzymes and by the 25-fold higher activity of TRI over TRII, the majority of the tropinone reduction is from TRI to form tropine.

Cocaine and its major metabolites may be quantified in blood, plasma, or urine to monitor for abuse, confirm a diagnosis of poisoning, or assist in the forensic investigation of a traffic or other criminal violation or a sudden death. Most commercial cocaine immunoassay screening tests cross-react appreciably with the major cocaine metabolites, but chromatographic techniques can easily distinguish and separately measure each of these substances. When interpreting the results of a test, it is important to consider the cocaine usage history of the individual, since a chronic user can develop tolerance to doses that would incapacitate a cocaine-naive individual, and the chronic user often has high baseline values of the metabolites in his system. Cautious interpretation of testing results may allow a distinction between passive or active usage, and between smoking versus other routes of administration. In 2011, researchers at John Jay College of Criminal Justice reported that dietary zinc supplements can mask the presence of cocaine and other drugs in urine. Similar claims have been made in web forums on that topic.

According to a 2016 United Nations report, England and Wales are the countries with the highest rate of cocaine usage (2.4% of adults in the previous year). Other countries where the usage rate meets or exceeds 1.5% are Spain and Scotland (2.2%), the United States (2.1%), Australia (2.1%), Uruguay (1.8%), Brazil (1.75%), Chile (1.73%), the Netherlands (1.5%) and Ireland (1.5%).

Cocaine is the second most popular illegal recreational drug in Europe (behind cannabis). Since the mid-1990s, overall cocaine usage in Europe has been on the rise, but usage rates and attitudes tend to vary between countries. European countries with the highest usage rates are the United Kingdom, Spain, Italy, and the Republic of Ireland.

Approximately 12 million Europeans (3.6%) have used cocaine at least once, 4 million (1.2%) in the last year, and 2 million in the last month (0.5%).

About 3.5 million or 87.5% of those who have used the drug in the last year are young adults (15–34 years old). Usage is particularly prevalent among this demographic: 4% to 7% of males have used cocaine in the last year in Spain, Denmark, Republic of Ireland, Italy, and the United Kingdom. The ratio of male to female users is approximately 3.8:1, but this statistic varies from 1:1 to 13:1 depending on country.

In 2014 London had the highest amount of cocaine in its sewage out of 50 European cities.

Cocaine is the second most popular illegal recreational drug in the United States (behind cannabis) and the U.S. is the world's largest consumer of cocaine. Cocaine is commonly used in middle to upper-class communities and is known as a "rich man's drug". It is also popular amongst college students, as a party drug. A study throughout the entire United States has reported that around 48 percent of people who graduated high school in 1979 have used cocaine recreationally during some point in their lifetime, compared to approximately 20 percent of students who graduated between the years of 1980 and 1995.
Its users span over different ages, races, and professions. In the 1970s and 1980s, the drug became particularly popular in the disco culture as cocaine usage was very common and popular in many discos such as Studio 54.

For over a thousand years South American indigenous peoples have chewed the leaves of "Erythroxylon coca", a plant that contains vital nutrients as well as numerous alkaloids, including cocaine. The coca leaf was, and still is, chewed almost universally by some indigenous communities. The remains of coca leaves have been found with ancient Peruvian mummies, and pottery from the time period depicts humans with bulged cheeks, indicating the presence of something on which they are chewing. There is also evidence that these cultures used a mixture of coca leaves and saliva as an anesthetic for the performance of trepanation.

When the Spanish arrived in South America, most at first ignored aboriginal claims that the leaf gave them strength and energy, and declared the practice of chewing it the work of the Devil. But after discovering that these claims were true, they legalized and taxed the leaf, taking 10% off the value of each crop. In 1569, Spanish botanist Nicolás Monardes described the indigenous peoples' practice of chewing a mixture of tobacco and coca leaves to induce "great contentment":
In 1609, Padre Blas Valera wrote:
Although the stimulant and hunger-suppressant properties of coca had been known for many centuries, the isolation of the cocaine alkaloid was not achieved until 1855. Various European scientists had attempted to isolate cocaine, but none had been successful for two reasons: the knowledge of chemistry required was insufficient at the time, and contemporary conditions of sea-shipping from South America could degrade the cocaine in the plant samples available to European chemists.

The cocaine alkaloid was first isolated by the German chemist Friedrich Gaedcke in 1855. Gaedcke named the alkaloid "erythroxyline", and published a description in the journal "Archiv der Pharmazie."

In 1856, Friedrich Wöhler asked Dr. Carl Scherzer, a scientist aboard the "Novara" (an Austrian frigate sent by Emperor Franz Joseph to circle the globe), to bring him a large amount of coca leaves from South America. In 1859, the ship finished its travels and Wöhler received a trunk full of coca. Wöhler passed on the leaves to Albert Niemann, a Ph.D. student at the University of Göttingen in Germany, who then developed an improved purification process.

Niemann described every step he took to isolate cocaine in his dissertation titled "Über eine neue organische Base in den Cocablättern" ("On a New Organic Base in the Coca Leaves"), which was published in 1860—it earned him his Ph.D. and is now in the British Library. He wrote of the alkaloid's "colourless transparent prisms" and said that "Its solutions have an alkaline reaction, a bitter taste, promote the flow of saliva and leave a peculiar numbness, followed by a sense of cold when applied to the tongue." Niemann named the alkaloid "cocaine" from "coca" (from Quechua "cuca") + suffix "ine". Because of its use as a local anesthetic, a suffix "-caine" was later extracted and used to form names of synthetic local anesthetics.

The first synthesis and elucidation of the structure of the cocaine molecule was by Richard Willstätter in 1898. It was the first biomimetic synthesis of an organic structure recorded in academic chemical literature. The synthesis started from tropinone, a related natural product and took five steps.

With the discovery of this new alkaloid, Western medicine was quick to exploit the possible uses of this plant.

In 1879, Vassili von Anrep, of the University of Würzburg, devised an experiment to demonstrate the analgesic properties of the newly discovered alkaloid. He prepared two separate jars, one containing a cocaine-salt solution, with the other containing merely salt water. He then submerged a frog's legs into the two jars, one leg in the treatment and one in the control solution, and proceeded to stimulate the legs in several different ways. The leg that had been immersed in the cocaine solution reacted very differently from the leg that had been immersed in salt water.

Karl Koller (a close associate of Sigmund Freud, who would write about cocaine later) experimented with cocaine for ophthalmic usage. In an infamous experiment in 1884, he experimented upon himself by applying a cocaine solution to his own eye and then pricking it with pins. His findings were presented to the Heidelberg Ophthalmological Society. Also in 1884, Jellinek demonstrated the effects of cocaine as a respiratory system anesthetic. In 1885, William Halsted demonstrated nerve-block anesthesia, and James Leonard Corning demonstrated peridural anesthesia. 1898 saw Heinrich Quincke use cocaine for spinal anesthesia.

In 1859, an Italian doctor, Paolo Mantegazza, returned from Peru, where he had witnessed first-hand the use of coca by the local indigenous peoples. He proceeded to experiment on himself and upon his return to Milan he wrote a paper in which he described the effects. In this paper he declared coca and cocaine (at the time they were assumed to be the same) as being useful medicinally, in the treatment of "a furred tongue in the morning, flatulence, and whitening of the teeth."

A chemist named Angelo Mariani who read Mantegazza's paper became immediately intrigued with coca and its economic potential. In 1863, Mariani started marketing a wine called Vin Mariani, which had been treated with coca leaves, to become cocawine. The ethanol in wine acted as a solvent and extracted the cocaine from the coca leaves, altering the drink's effect. It contained 6 mg cocaine per ounce of wine, but Vin Mariani which was to be exported contained 7.2 mg per ounce, to compete with the higher cocaine content of similar drinks in the United States. A "pinch of coca leaves" was included in John Styth Pemberton's original 1886 recipe for Coca-Cola, though the company began using decocainized leaves in 1906 when the Pure Food and Drug Act was passed.

In 1879 cocaine began to be used to treat morphine addiction. Cocaine was introduced into clinical use as a local anesthetic in Germany in 1884, about the same time as Sigmund Freud published his work "Über Coca", in which he wrote that cocaine causes:

In 1885 the U.S. manufacturer Parke-Davis sold cocaine in various forms, including cigarettes, powder, and even a cocaine mixture that could be injected directly into the user's veins with the included needle. The company promised that its cocaine products would "supply the place of food, make the coward brave, the silent eloquent and render the sufferer insensitive to pain."

By the late Victorian era, cocaine use had appeared as a vice in literature. For example, it was injected by Arthur Conan Doyle's fictional Sherlock Holmes, generally to offset the boredom he felt when he was not working on a case.

In early 20th-century Memphis, Tennessee, cocaine was sold in neighborhood drugstores on Beale Street, costing five or ten cents for a small boxful. Stevedores along the Mississippi River used the drug as a stimulant, and white employers encouraged its use by black laborers.

In 1909, Ernest Shackleton took "Forced March" brand cocaine tablets to Antarctica, as did Captain Scott a year later on his ill-fated journey to the South Pole.

During the mid-1940s, amidst World War II, cocaine was considered for inclusion as an ingredient of a future generation of 'pep pills' for the German military, code named D-IX.

In modern popular culture references to the drug are prevalent, in it the drug has a glamorous image associated with the rich, famous and powerful with it also making users to "feel rich and beautiful". In addition the pace of modern society − such as in finance − gives many the incentive to make use of the drug.

In many countries, cocaine is a popular recreational drug. In the United States, the development of "crack" cocaine introduced the substance to a generally poorer inner-city market. Use of the powder form has stayed relatively constant, experiencing a new height of use during the late 1990s and early 2000s in the U.S., and has become much more popular in the last few years in the UK. 

Cocaine use is prevalent across all socioeconomic strata, including age, demographics, economic, social, political, religious, and livelihood. 

The estimated U.S. cocaine market exceeded US$70 billion in street value for the year 2005, exceeding revenues by corporations such as Starbucks. There is a tremendous demand for cocaine in the U.S. market, particularly among those who are making incomes affording luxury spending, such as single adults and professionals with discretionary income. Cocaine's status as a club drug shows its immense popularity among the "party crowd".

In 1995 the World Health Organization (WHO) and the United Nations Interregional Crime and Justice Research Institute (UNICRI) announced in a press release the publication of the results of the largest global study on cocaine use ever undertaken. However, a decision by an American representative in the World Health Assembly banned the publication of the study, because it seemed to make a case for the positive uses of cocaine. An excerpt of the report strongly conflicted
with accepted paradigms, for example "that occasional cocaine use does not typically lead to severe or even minor physical or social problems." In the sixth meeting of the B committee, the US representative threatened that "If World Health Organization activities relating to drugs failed to reinforce proven drug control approaches, funds for the relevant programs should be curtailed". This led to the decision to discontinue publication. A part of the study was recuperated and published in 2010, including profiles of cocaine use in 20 countries, but are unavailable .

In October 2010 it was reported that the use of cocaine in Australia has doubled since monitoring began in 2003.

A problem with illegal cocaine use, especially in the higher volumes used to combat fatigue (rather than increase euphoria) by long-term users, is the risk of ill effects or damage caused by the compounds used in adulteration. Cutting or "stepping on" the drug is commonplace, using compounds which simulate ingestion effects, such as Novocain (procaine) producing temporary anesthaesia, as many users believe a strong numbing effect is the result of strong and/or pure cocaine, ephedrine or similar stimulants that are to produce an increased heart rate. The normal adulterants for profit are inactive sugars, usually mannitol, creatine or glucose, so introducing active adulterants gives the illusion of purity and to 'stretch' or make it so a dealer can sell more product than without the adulterants. The adulterant of sugars allows the dealer to sell the product for a higher price because of the illusion of purity and allows sale of more of the product at that higher price, enabling dealers to significantly increase revenue with little additional cost for the adulterants. A 2007 study by the European Monitoring Centre for Drugs and Drug Addiction showed that the purity levels for street purchased cocaine was often under 5% and on average under 50% pure.

The production, distribution, and sale of cocaine products is restricted (and illegal in most contexts) in most countries as regulated by the Single Convention on Narcotic Drugs, and the United Nations Convention Against Illicit Traffic in Narcotic Drugs and Psychotropic Substances. In the United States the manufacture, importation, possession, and distribution of cocaine are additionally regulated by the 1970 Controlled Substances Act.

Some countries, such as Peru and Bolivia permit the cultivation of coca leaf for traditional consumption by the local indigenous population, but nevertheless, prohibit the production, sale, and consumption of cocaine. The provisions as to how much a coca farmer can yield annually is protected by laws such as the Bolivian Cato accord. In addition, some parts of Europe and Australia allow processed cocaine for medicinal uses only.

Cocaine is a Schedule 8 prohibited substance in Australia under the Poisons Standard (July 2016). A schedule 8 substance is a controlled Drug – Substances which should be available for use but require restriction of manufacture, supply, distribution, possession and use to reduce abuse, misuse and physical or psychological dependence.

In Western Australia under the Misuse of Drugs Act 1981 4.0g of cocaine is the amount of prohibited drugs determining a court of trial, 2.0g is the amount of cocaine required for the presumption of intention to sell or supply and 28.0g is the amount of cocaine required for purposes of drug trafficking.

The US federal government instituted a national labeling requirement for cocaine and cocaine-containing products through the Pure Food and Drug Act of 1906. The next important federal regulation was the Harrison Narcotics Tax Act of 1914. While this act is often seen as the start of prohibition, the act itself was not actually a prohibition on cocaine, but instead set up a regulatory and licensing regime. The Harrison Act did not recognize addiction as a treatable condition and therefore the therapeutic use of cocaine, heroin or morphine to such individuals was outlawed leading a 1915 editorial in the journal "American Medicine" to remark that the addict "is denied the medical care he urgently needs, open, above-board sources from which he formerly obtained his drug supply are closed to him, and he is driven to the underworld where he can get his drug, but of course, surreptitiously and in violation of the law." The Harrison Act left manufacturers of cocaine untouched so long as they met certain purity and labeling standards. Despite that cocaine was typically illegal to sell and legal outlets were rarer, the quantities of legal cocaine produced declined very little. Legal cocaine quantities did not decrease until the Jones–Miller Act of 1922 put serious restrictions on cocaine manufactures.

In 2004, according to the United Nations, 589 tonnes of cocaine were seized globally by law enforcement authorities. Colombia seized 188 t, the United States 166 t, Europe 79 t, Peru 14 t, Bolivia 9 t, and the rest of the world 133 t.

Because of the drug's potential for addiction and overdose, cocaine is generally treated as a "hard drug", with severe penalties for possession and trafficking. Demand remains high, and consequently, black market cocaine is quite expensive. Unprocessed cocaine, such as coca leaves, are occasionally purchased and sold, but this is exceedingly rare as it is much easier and more profitable to conceal and smuggle it in powdered form. The scale of the market is immense: 770 tonnes times $100 per gram retail = up to $77 billion.

Until 2012, Colombia was the world's leading producer of cocaine. Three-quarters of the world's annual yield of cocaine has been produced in Colombia, both from cocaine base imported from Peru (primarily the Huallaga Valley) and Bolivia, and from locally grown coca. There was a 28% increase from the amount of potentially harvestable coca plants which were grown in Colombia in 1998. This, combined with crop reductions in Bolivia and Peru, made Colombia the nation with the largest area of coca under cultivation after the mid-1990s. Coca is grown for traditional purposes by indigenous communities, a use which is still present and is permitted by Colombian laws only makes up a small fragment of total coca production, most of which is used for the illegal drug trade.

An interview with a coca farmer published in 2003 described a mode of production by acid-base extraction that has changed little since 1905. Roughly of leaves were harvested per hectare, six times per year. The leaves were dried for half a day, then chopped into small pieces with a string trimmer and sprinkled with a small amount of powdered cement (replacing sodium carbonate from former times). Several hundred pounds of this mixture were soaked in of gasoline for a day, then the gasoline was removed and the leaves were pressed for remaining liquid, after which they could be discarded. Then battery acid (weak sulfuric acid) was used, one bucket per of leaves, to create a phase separation in which the cocaine free base in the gasoline was acidified and extracted into a few buckets of "murky-looking smelly liquid". Once powdered caustic soda was added to this, the cocaine precipitated and could be removed by filtration through a cloth. The resulting material, when dried, was termed "pasta" and sold by the farmer. The 3750 pound yearly harvest of leaves from a hectare produced of "pasta", approximately 40–60% cocaine. Repeated recrystallization from solvents, producing "pasta lavada" and eventually crystalline cocaine were performed at specialized laboratories after the sale.

Attempts to eradicate coca fields through the use of defoliants have devastated part of the farming economy in some coca growing regions of Colombia, and strains appear to have been developed that are more resistant or immune to their use. Whether these strains are natural mutations or the product of human tampering is unclear. These strains have also shown to be more potent than those previously grown, increasing profits for the drug cartels responsible for the exporting of cocaine. Although production fell temporarily, coca crops rebounded in numerous smaller fields in Colombia, rather than the larger plantations.

The cultivation of coca has become an attractive economic decision for many growers due to the combination of several factors, including the lack of other employment alternatives, the lower profitability of alternative crops in official crop substitution programs, the eradication-related damages to non-drug farms, the spread of new strains of the coca plant due to persistent worldwide demand.

The latest estimate provided by the U.S. authorities on the annual production of cocaine in Colombia refers to 290 metric tons.
As of the end of 2011, the seizure operations of Colombian cocaine carried out in different countries have totaled 351.8 metric tons of cocaine, i.e. 121.3% of Colombia's annual production according to the U.S. Department of State's estimates.
Synthetic cocaine would be highly desirable to the illegal drug industry as it would eliminate the high visibility and low reliability of offshore sources and international smuggling, replacing them with clandestine domestic laboratories, as are common for illicit methamphetamine. However, natural cocaine remains the lowest cost and highest quality supply of cocaine. Actual full synthesis of cocaine is rarely done. Formation of inactive enantiomers (cocaine has 4 chiral centres – "1R", "2R", "3S", and "5S" – hence a total potential of 16 possible enantiomers and diastereoisomers) plus synthetic by-products limits the yield and purity.
Names like "synthetic cocaine" and "new cocaine" have been misapplied to phencyclidine (PCP) and various designer drugs.

Organized criminal gangs operating on a large scale dominate the cocaine trade. Most cocaine is grown and processed in South America, particularly in Colombia, Bolivia, Peru, and smuggled into the United States and Europe, the United States being the world's largest consumer of cocaine, where it is sold at huge markups; usually in the US at $80–120 for 1 gram, and $250–300 for 3.5 grams (⅛ of an ounce, or an "eight ball").

, cocaine shipments from South America transported through Mexico or Central America were generally moved over land or by air to staging sites in northern Mexico. The cocaine is then broken down into smaller loads for smuggling across the U.S.–Mexico border. The primary cocaine importation points in the United States have been in Arizona, southern California, southern Florida, and Texas. Typically, land vehicles are driven across the U.S.–Mexico border. Sixty-five percent of cocaine enters the United States through Mexico, and the vast majority of the rest enters through Florida. , the Sinaloa Cartel is the most active drug cartel involved in smuggling illicit drugs like cocaine into the United States and trafficking them throughout the United States.

Cocaine traffickers from Colombia and Mexico have established a labyrinth of smuggling routes throughout the Caribbean, the Bahama Island chain, and South Florida. They often hire traffickers from Mexico or the Dominican Republic to transport the drug using a variety of smuggling techniques to U.S. markets. These include airdrops of in the Bahama Islands or off the coast of Puerto Rico, mid-ocean boat-to-boat transfers of , and the commercial shipment of tonnes of cocaine through the port of Miami.

Another route of cocaine traffic goes through Chile, which is primarily used for cocaine produced in Bolivia since the nearest seaports lie in northern Chile. The arid Bolivia–Chile border is easily crossed by 4×4 vehicles that then head to the seaports of Iquique and Antofagasta. While the price of cocaine is higher in Chile than in Peru and Bolivia, the final destination is usually Europe, especially Spain where drug dealing networks exist among South American immigrants.

Cocaine is also carried in small, concealed, kilogram quantities across the border by couriers known as "mules" (or "mulas"), who cross a border either legally, for example, through a port or airport, or illegally elsewhere. The drugs may be strapped to the waist or legs or hidden in bags, or hidden in the body. If the mule gets through without being caught, the gangs will reap most of the profits. If he or she is caught, however, gangs will sever all links and the mule will usually stand trial for trafficking alone.

Bulk cargo ships are also used to smuggle cocaine to staging sites in the western Caribbean–Gulf of Mexico area. These vessels are typically 150–250-foot (50–80 m) coastal freighters that carry an average cocaine load of approximately 2.5 tonnes. Commercial fishing vessels are also used for smuggling operations. In areas with a high volume of recreational traffic, smugglers use the same types of vessels, such as go-fast boats, as those used by the local populations.

Sophisticated drug subs are the latest tool drug runners are using to bring cocaine north from Colombia, it was reported on 20 March 2008. Although the vessels were once viewed as a quirky sideshow in the drug war, they are becoming faster, more seaworthy, and capable of carrying bigger loads of drugs than earlier models, according to those charged with catching them.

Cocaine is readily available in all major countries' metropolitan areas. According to the "Summer 1998 Pulse Check", published by the U.S. Office of National Drug Control Policy, cocaine use had stabilized across the country, with a few increases reported in San Diego, Bridgeport, Miami, and Boston. In the West, cocaine usage was lower, which was thought to be due to a switch to methamphetamine among some users; methamphetamine is cheaper, three and a half times more powerful, and lasts 12–24 times longer with each dose. Nevertheless, the number of cocaine users remain high, with a large concentration among urban youth.

In addition to the amounts previously mentioned, cocaine can be sold in "bill sizes": for example, $10 might purchase a "dime bag", a very small amount (0.1–0.15 g) of cocaine. Twenty dollars might purchase 0.15–0.3 g. However, in lower Texas, it is sold cheaper due to it being easier to receive: a dime for $10 is 0.4 g, a 20 is 0.8–1.0 g and an 8-ball (3.5 g) is sold for $60 to $80, depending on the quality and dealer. These amounts and prices are very popular among young people because they are inexpensive and easily concealed on one's body. Quality and price can vary dramatically depending on supply and demand, and on geographic region.

In 2008, the European Monitoring Centre for Drugs and Drug Addiction reports that the typical retail price of cocaine varied between €50 and €75 per gram in most European countries, although Cyprus, Romania, Sweden and Turkey reported much higher values.

World annual cocaine consumption, as of 2000, stood at around 600 tonnes, with the United States consuming around 300 t, 50% of the total, Europe about 150 t, 25% of the total, and the rest of the world the remaining 150 t or 25%. It is estimated that 1.5 million people in the United States used cocaine in 2010 down from 2.4 million in 2006. Conversely, cocaine use appears to be increasing in Europe with the highest prevalences in Spain, the United Kingdom, Italy, and Ireland.

The 2010 UN World Drug Report concluded that "it appears that the North American cocaine market has declined in value from US$47 billion in 1998 to US$38 billion in 2008. Between 2006 and 2008, the value of the market remained basically stable".

In 2005, researchers proposed the use of cocaine in conjunction with phenylephrine administered in the form of an eye drop as a diagnostic test for Parkinson's disease.




</doc>
<doc id="7706" url="https://en.wikipedia.org/wiki?curid=7706" title="Cartesian coordinate system">
Cartesian coordinate system

A Cartesian coordinate system is a coordinate system that specifies each point uniquely in a plane by a pair of numerical coordinates, which are the signed distances to the point from two fixed perpendicular directed lines, measured in the same unit of length. Each reference line is called a "coordinate axis" or just "axis" (plural "axes") of the system, and the point where they meet is its "origin", at ordered pair . The coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes, expressed as signed distances from the origin.

One can use the same principle to specify the position of any point in three-dimensional space by three Cartesian coordinates, its signed distances to three mutually perpendicular planes (or, equivalently, by its perpendicular projection onto three mutually perpendicular lines). In general, "n" Cartesian coordinates (an element of real "n"-space) specify the point in an "n"-dimensional Euclidean space for any dimension "n". These coordinates are equal, up to sign, to distances from the point to "n" mutually perpendicular hyperplanes.

The invention of Cartesian coordinates in the 17th century by René Descartes (Latinized name: "Cartesius") revolutionized mathematics by providing the first systematic link between Euclidean geometry and algebra. Using the Cartesian coordinate system, geometric shapes (such as curves) can be described by Cartesian equations: algebraic equations involving the coordinates of the points lying on the shape. For example, a circle of radius 2, centered at the origin of the plane, may be described as the set of all points whose coordinates "x" and "y" satisfy the equation .

Cartesian coordinates are the foundation of analytic geometry, and provide enlightening geometric interpretations for many other branches of mathematics, such as linear algebra, complex analysis, differential geometry, multivariate calculus, group theory and more. A familiar example is the concept of the graph of a function. Cartesian coordinates are also essential tools for most applied disciplines that deal with geometry, including astronomy, physics, engineering and many more. They are the most common coordinate system used in computer graphics, computer-aided geometric design and other geometry-related data processing.

The adjective "Cartesian" refers to the French mathematician and philosopher René Descartes who published this idea in 1637. It was independently discovered by Pierre de Fermat, who also worked in three dimensions, although Fermat did not publish the discovery. The French cleric Nicole Oresme, used constructions similar to Cartesian coordinates well before the time of Descartes and Fermat.

Both Descartes and Fermat used a single axis in their treatments and have a variable length measured in reference to this axis. The concept of using a pair of axes was introduced later, after Descartes' "La Géométrie" was translated into Latin in 1649 by Frans van Schooten and his students. These commentators introduced several concepts while trying to clarify the ideas contained in Descartes' work.

The development of the Cartesian coordinate system would play a fundamental role in the development of the calculus by Isaac Newton and Gottfried Wilhelm Leibniz. The two-coordinate description of the plane was later generalized into the concept of vector spaces.

Many other coordinate systems have been developed since Descartes, such as the polar coordinates for the plane, and the spherical and cylindrical coordinates for three-dimensional space.

Choosing a Cartesian coordinate system for a one-dimensional space that is, for a straight line—involves choosing a point "O" of the line (the origin), a unit of length, and an orientation for the line. An orientation chooses which of the two half-lines determined by "O" is the positive, and which is negative; we then say that the line "is oriented" (or "points") from the negative half towards the positive half. Then each point "P" of the line can be specified by its distance from "O", taken with a + or − sign depending on which half-line contains "P".

A line with a chosen Cartesian system is called a number line. Every real number has a unique location on the line. Conversely, every point on the line can be interpreted as a number in an ordered continuum such as the real numbers.

A Cartesian coordinate system in two dimensions (also called a rectangular coordinate system or an orthogonal coordinate system) is defined by an ordered pair of perpendicular lines (axes), a single unit of length for both axes, and an orientation for each axis. The point where the axes meet is taken as the origin for both, thus turning each axis into a number line. For any point "P", a line is drawn through "P" perpendicular to each axis, and the position where it meets the axis is interpreted as a number. The two numbers, in that chosen order, are the "Cartesian coordinates" of "P". The reverse construction allows one to determine the point "P" given its coordinates.

The first and second coordinates are called the "abscissa" and the "ordinate" of "P", respectively; and the point where the axes meet is called the "origin" of the coordinate system. The coordinates are usually written as two numbers in parentheses, in that order, separated by a comma, as in . Thus the origin has coordinates , and the points on the positive half-axes, one unit away from the origin, have coordinates and .

In mathematics, physics, and engineering, the first axis is usually defined or depicted as horizontal and oriented to the right, and the second axis is vertical and oriented upwards. (However, in some computer graphics contexts, the ordinate axis may be oriented downwards.) The origin is often labeled "O", and the two coordinates are often denoted by the letters "X" and "Y", or "x" and "y". The axes may then be referred to as the "X"-axis and "Y"-axis. The choices of letters come from the original convention, which is to use the latter part of the alphabet to indicate unknown values. The first part of the alphabet was used to designate known values. 

A Euclidean plane with a chosen Cartesian coordinate system "Cartesian plane". In a Cartesian plane one can define canonical representatives of certain geometric figures, such as the unit circle (with radius equal to the length unit, and center at the origin), the unit square (whose diagonal has endpoints at and ), the unit hyperbola, and so on.

The two axes divide the plane into four right angles, called "quadrant". The quadrants may be named or numbered in various ways, but the quadrant where all coordinates are positive is usually called the "first quadrant".

If the coordinates of a point are , then its distances from the "X"-axis and from the "Y"-axis are |"y"| and |"x"|, respectively; where |...| denotes the absolute value of a number.

A Cartesian coordinate system for a three-dimensional space consists of an ordered triplet of lines (the "axes") that go through a common point (the "origin"), and are pair-wise perpendicular; an orientation for each axis; and a single unit of length for all three axes. As in the two-dimensional case, each axis becomes a number line. For any point "P" of space, one considers a plane through "P" perpendicular to each coordinate axis, and interprets the point where that plane cuts the axis as a number. The Cartesian coordinates of "P" are those three numbers, in the chosen order The reverse construction determines the point "P" given its three coordinates.

Alternatively, each coordinate of a point "P" can be taken as the distance from "P" to the plane defined by the other two axes, with the sign determined by the orientation of the corresponding axis. 

Each pair of axes defines a "coordinate plane". These planes divide space into eight trihedra, called "octants".

The coordinates are usually written as three numbers (or algebraic formulas) surrounded by parentheses and separated by commas, as in or . Thus, the origin has coordinates , and the unit points on the three axes are , , and .

There are no standard names for the coordinates in the three axes. The coordinates are often denoted by the letters "X", "Y", and "Z" (or "x", "y", and "z"), in which case the lines are called the "X"-, "Y"-, and "Z"-axis, respectively. Then the coordinate planes can be referred to as the "XY"-, "YZ"-, and "XZ"-planes.

In mathematics, physics, and engineering contexts, the first two axes are often defined or depicted as horizontal, with the third axis pointing up. In that case the third coordinate may be called "height" or "altitude". The orientations are usually chosen so that the 90 degree angle from the first axis to the second axis looks counter-clockwise when seen from the point ; a convention that is commonly called "the right hand rule".

A Euclidean plane with a chosen Cartesian system is called a Cartesian plane. Since Cartesian coordinates are unique and non-ambiguous, the points of a Cartesian plane can be identified with pairs of real numbers; that is with the Cartesian product formula_1, where formula_2 is the set of all reals. In the same way, the points in any Euclidean space of dimension "n" be identified with the tuples (lists) of "n" real numbers, that is, with the Cartesian product formula_3.

The concept of Cartesian coordinates generalizes to allow axes that are not perpendicular to each other, and/or different units along each axis. In that case, each coordinate is obtained by projecting the point onto one axis along a direction that is parallel to the other axis (or, in general, to the hyperplane defined by all the other axes). In such an oblique coordinate system the computations of distances and angles must be modified from that in standard Cartesian systems, and many standard formulas (such as the Pythagorean formula for the distance) do not hold (see affine plane).

The Cartesian coordinates of a point are usually written in parentheses and separated by commas, as in or . The origin is often labelled with the capital letter "O". In analytic geometry, unknown or generic coordinates are often denoted by the letters ("x", "y") in the plane, and ("x", "y", "z") in three-dimensional space. This custom comes from a convention of algebra, which uses letters near the end of the alphabet for unknown values (such as were the coordinates of points in many geometric problems), and letters near the beginning for given quantities.

These conventional names are often used in other domains, such as physics and engineering, although other letters may be used. For example, in a graph showing how a pressure varies with time, the graph coordinates may be denoted "p" and "t". Each axis is usually named after the coordinate which is measured along it; so one says the "x-axis", the "y-axis", the "t-axis", etc.

Another common convention for coordinate naming is to use subscripts, as ("x", "x", ..., "x") for the "n" coordinates in an "n"-dimensional space, especially when "n" is greater than 3 or unspecified. Some authors prefer the numbering ("x", "x", ..., "x"). These notations are especially advantageous in computer programming: by storing the coordinates of a point as an array, instead of a record, the subscript can serve to index the coordinates.

In mathematical illustrations of two-dimensional Cartesian systems, the first coordinate (traditionally called the abscissa) is measured along a horizontal axis, oriented from left to right. The second coordinate (the ordinate) is then measured along a vertical axis, usually oriented from bottom to top. Young children learning the Cartesian system, commonly learn the order to read the values before cementing the "x"-, "y"-, and "z"-axis concepts, by starting with 2D mnemonics (e.g. 'Walk along the hall then up the stairs' akin to straight across the "x"-axis then up vertically along the "y"-axis).

Computer graphics and image processing, however, often use a coordinate system with the "y"-axis oriented downwards on the computer display. This convention developed in the 1960s (or earlier) from the way that images were originally stored in display buffers.

For three-dimensional systems, a convention is to portray the "xy"-plane horizontally, with the "z"-axis added to represent height (positive up). Furthermore, there is a convention to orient the "x"-axis toward the viewer, biased either to the right or left. If a diagram (3D projection or 2D perspective drawing) shows the "x"- and "y"-axis horizontally and vertically, respectively, then the "z"-axis should be shown pointing "out of the page" towards the viewer or camera. In such a 2D diagram of a 3D coordinate system, the "z"-axis would appear as a line or ray pointing down and to the left or down and to the right, depending on the presumed viewer or camera perspective. In any diagram or display, the orientation of the three axes, as a whole, is arbitrary. However, the orientation of the axes relative to each other should always comply with the right-hand rule, unless specifically stated otherwise. All laws of physics and math assume this right-handedness, which ensures consistency.

For 3D diagrams, the names "abscissa" and "ordinate" are rarely used for "x" and "y", respectively. When they are, the "z"-coordinate is sometimes called the applicate. The words "abscissa", "ordinate" and "applicate" are sometimes used to refer to coordinate axes rather than the coordinate values.

The axes of a two-dimensional Cartesian system divide the plane into four infinite regions, called quadrants, each bounded by two half-axes. These are often numbered from 1st to 4th and denoted by Roman numerals: I (where the signs of the two coordinates are I (+,+), II (−,+), III (−,−), and IV (+,−). When the axes are drawn according to the mathematical custom, the numbering goes counter-clockwise starting from the upper right ("north-east") quadrant.

Similarly, a three-dimensional Cartesian system defines a division of space into eight regions or octants, according to the signs of the coordinates of the points. The convention used for naming a specific octant is to list its signs, e.g. or . The generalization of the quadrant and octant to an arbitrary number of dimensions is the orthant, and a similar naming system applies.

The Euclidean distance between two points of the plane with Cartesian coordinates formula_4 and formula_5 is

This is the Cartesian version of Pythagoras's theorem. In three-dimensional space, the distance between points formula_7 and formula_8 is

which can be obtained by two consecutive applications of Pythagoras' theorem.

The Euclidean transformations or Euclidean motions are the (bijective) mappings of points of the Euclidean plane to themselves which preserve distances between points. There are four types of these mappings (also called isometries): translations, rotations, reflections and glide reflections.

Translating a set of points of the plane, preserving the distances and directions between them, is equivalent to adding a fixed pair of numbers to the Cartesian coordinates of every point in the set. That is, if the original coordinates of a point are , after the translation they will be

To rotate a figure counterclockwise around the origin by some angle formula_11 is equivalent to replacing every point with coordinates ("x","y") by the point with coordinates ("x<nowiki>'</nowiki>","y<nowiki>'</nowiki>"), where

Thus:

formula_14

If are the Cartesian coordinates of a point, then are the coordinates of its reflection across the second coordinate axis (the y-axis), as if that line were a mirror. Likewise, are the coordinates of its reflection across the first coordinate axis (the x-axis). In more generality, reflection across a line through the origin making an angle formula_11 with the x-axis, is equivalent to replacing every point with coordinates by the point with coordinates , where

Thus:
formula_18

A glide reflection is the composition of a reflection across a line followed by a translation in the direction of that line. It can be seen that the order of these operations does not matter (the translation can come first, followed by the reflection).

These Euclidean transformations of the plane can all be described in a uniform way by using matrices. The result formula_19 of applying a Euclidean transformation to a point formula_20 is given by the formula

where "A" is a 2×2 orthogonal matrix and is an arbitrary ordered pair of numbers; that is,

where

To be "orthogonal", the matrix "A" must have orthogonal rows with same Euclidean length of one, that is,

and

This is equivalent to saying that "A" times its transpose must be the identity matrix. If these conditions do not hold, the formula describes a more general affine transformation of the plane provided that the determinant of "A" is not zero.

The formula defines a translation if and only if "A" is the identity matrix. The transformation is a rotation around some point if and only if "A" is a rotation matrix, meaning that

A reflection or glide reflection is obtained when,

Assuming that translation is not used transformations can be combined by simply multiplying the associated transformation matrices.

Another way to represent coordinate transformations in Cartesian coordinates is through affine transformations. In affine transformations an extra dimension is added and all points are given a value of 1 for this extra dimension. The advantage of doing this is that point translations can be specified in the final column of matrix "A". In this way, all of the euclidean transformations become transactable as matrix point multiplications. The affine transformation is given by:

Using affine transformations multiple different euclidean transformations including translation can be combined by simply multiplying the corresponding matrices.

An example of an affine transformation which is not a Euclidean motion is given by scaling. To make a figure larger or smaller is equivalent to multiplying the Cartesian coordinates of every point by the same positive number "m". If are the coordinates of a point on the original figure, the corresponding point on the scaled figure has coordinates

If "m" is greater than 1, the figure becomes larger; if "m" is between 0 and 1, it becomes smaller.

A shearing transformation will push the top of a square sideways to form a parallelogram. Horizontal shearing is defined by:

Shearing can also be applied vertically:

Fixing or choosing the "x"-axis determines the "y"-axis up to direction. Namely, the "y"-axis is necessarily the perpendicular to the "x"-axis through the point marked 0 on the "x"-axis. But there is a choice of which of the two half lines on the perpendicular to designate as positive and which as negative. Each of these two choices determines a different orientation (also called "handedness") of the Cartesian plane.

The usual way of orienting the axes, with the positive "x"-axis pointing right and the positive "y"-axis pointing up (and the "x"-axis being the "first" and the "y"-axis the "second" axis) is considered the "positive" or "standard" orientation, also called the "right-handed" orientation.

A commonly used mnemonic for defining the positive orientation is the "right-hand rule". Placing a somewhat closed right hand on the plane with the thumb pointing up, the fingers point from the "x"-axis to the "y"-axis, in a positively oriented coordinate system.

The other way of orienting the axes is following the "left hand rule", placing the left hand on the plane with the thumb pointing up.

When pointing the thumb away from the origin along an axis towards positive, the curvature of the fingers indicates a positive rotation along that axis.

Regardless of the rule used to orient the axes, rotating the coordinate system will preserve the orientation. Switching any two axes will reverse the orientation, but switching both will leave the orientation unchanged.

Once the "x"- and "y"-axes are specified, they determine the line along which the "z"-axis should lie, but there are two possible directions on this line. The two possible coordinate systems which result are called 'right-handed' and 'left-handed'. The standard orientation, where the "xy"-plane is horizontal and the "z"-axis points up (and the "x"- and the "y"-axis form a positively oriented two-dimensional coordinate system in the "xy"-plane if observed from "above" the "xy"-plane) is called right-handed or positive.

The name derives from the right-hand rule. If the index finger of the right hand is pointed forward, the middle finger bent inward at a right angle to it, and the thumb placed at a right angle to both, the three fingers indicate the relative directions of the "x"-, "y"-, and "z"-axes in a "right-handed" system. The thumb indicates the "x"-axis, the index finger the "y"-axis and the middle finger the "z"-axis. Conversely, if the same is done with the left hand, a left-handed system results.

Figure 7 depicts a left and a right-handed coordinate system. Because a three-dimensional object is represented on the two-dimensional screen, distortion and ambiguity result. The axis pointing downward (and to the right) is also meant to point "towards" the observer, whereas the "middle"-axis is meant to point "away" from the observer. The red circle is "parallel" to the horizontal "xy"-plane and indicates rotation from the "x"-axis to the "y"-axis (in both cases). Hence the red arrow passes "in front of" the "z"-axis.

Figure 8 is another attempt at depicting a right-handed coordinate system. Again, there is an ambiguity caused by projecting the three-dimensional coordinate system into the plane. Many observers see Figure 8 as "flipping in and out" between a convex cube and a concave "corner". This corresponds to the two possible orientations of the coordinate system. Seeing the figure as convex gives a left-handed coordinate system. Thus the "correct" way to view Figure 8 is to imagine the "x"-axis as pointing "towards" the observer and thus seeing a concave corner.
A point in space in a Cartesian coordinate system may also be represented by a position vector, which can be thought of as an arrow pointing from the origin of the coordinate system to the point. If the coordinates represent spatial positions (displacements), it is common to represent the vector from the origin to the point of interest as formula_33. In two dimensions, the vector from the origin to the point with Cartesian coordinates (x, y) can be written as:

where formula_35, and formula_36 are unit vectors in the direction of the "x"-axis and "y"-axis respectively, generally referred to as the "standard basis" (in some application areas these may also be referred to as versors). Similarly, in three dimensions, the vector from the origin to the point with Cartesian coordinates formula_37 can be written as:

where formula_39 is the unit vector in the direction of the z-axis.

There is no "natural" interpretation of multiplying vectors to obtain another vector that works in all dimensions, however there is a way to use complex numbers to provide such a multiplication. In a two dimensional cartesian plane, identify the point with coordinates with the complex number . Here, i is the imaginary unit and is identified with the point with coordinates , so it is not the unit vector in the direction of the "x"-axis. Since the complex numbers can be multiplied giving another complex number, this identification provides a means to "multiply" vectors. In a three dimensional cartesian space a similar identification can be made with a subset of the quaternions.

Cartesian coordinates are an abstraction that have a multitude of possible applications in the real world. However, three constructive steps are involved in superimposing coordinates on a problem application. 1) Units of distance must be decided defining the spatial size represented by the numbers used as coordinates. 2) An origin must be assigned to a specific spatial location or landmark, and 3) the orientation of the axes must be defined using available directional cues for all but one axis.

Consider as an example superimposing 3D Cartesian coordinates over all points on the Earth (i.e. geospatial 3D). What units make sense? Kilometers are a good choice, since the original definition of the kilometer was geospatial...10,000 km equalling the surface distance from the Equator to the North Pole. Where to place the origin? Based on symmetry, the gravitational center of the Earth suggests a natural landmark (which can be sensed via satellite orbits). Finally, how to orient X-, Y- and Z-axis directions? The axis of Earth's spin provides a natural direction strongly associated with "up vs. down", so positive Z can adopt the direction from geocenter to North Pole. A location on the Equator is needed to define the X-axis, and the prime meridian stands out as a reference direction, so the X-axis takes the direction from geocenter out to [ 0 degrees longitude, 0 degrees latitude ]. Note that with 3 dimensions, and two perpendicular axes directions pinned down for X and Z, the Y-axis is determined by the first two choices. In order to obey the right-hand rule, the Y-axis must point out from the geocenter to [ 90 degrees longitude, 0 degrees latitude ]. So what are the geocentric coordinates of the Empire State Building in New York City? Using [ longitude = −73.985656, latitude = 40.748433 ], Earth radius = 40,000/2π, and transforming from spherical --> Cartesian coordinates, you can estimate the geocentric coordinates of the Empire State Building, [ "x", "y", "z" ] = [ 1330.53 km, –4635.75 km, 4155.46 km ]. GPS navigation relies on such geocentric coordinates.

In engineering projects, agreement on the definition of coordinates is a crucial foundation. One cannot assume that coordinates come predefined for a novel application, so knowledge of how to erect a coordinate system where there is none is essential to applying René Descartes' ingenious thinking.

While spatial apps employ identical units along all axes, in business and scientific apps, each axis may have different units of measurement associated with it (such as kilograms, seconds, pounds, etc.). Although four- and higher-dimensional spaces are difficult to visualize, the algebra of Cartesian coordinates can be extended relatively easily to four or more variables, so that certain calculations involving many variables can be done. (This sort of algebraic extension is what is used to define the geometry of higher-dimensional spaces.) Conversely, it is often helpful to use the geometry of Cartesian coordinates in two or three dimensions to visualize algebraic relationships between two or three of many non-spatial variables.

The graph of a function or relation is the set of all points satisfying that function or relation. For a function of one variable, "f", the set of all points , where is the graph of the function "f". For a function "g" of two variables, the set of all points , where is the graph of the function "g". A sketch of the graph of such a function or relation would consist of all the salient parts of the function or relation which would include its relative extrema, its concavity and points of inflection, any points of discontinuity and its end behavior. All of these terms are more fully defined in calculus. Such graphs are useful in calculus to understand the nature and behavior of a function or relation.






</doc>
<doc id="7708" url="https://en.wikipedia.org/wiki?curid=7708" title="Commandant of the Marine Corps">
Commandant of the Marine Corps

The Commandant of the Marine Corps (CMC) is normally the highest-ranking officer in the United States Marine Corps and is a member of the Joint Chiefs of Staff. The CMC reports directly to the United States Secretary of the Navy and is responsible for ensuring the organization, policy, plans, and programs for the Marine Corps as well as advising the President, the Secretary of Defense, the National Security Council, the Homeland Security Council, and the Secretary of the Navy on matters involving the Marine Corps. Under the authority of the Secretary of the Navy, the CMC designates Marine personnel and resources to the commanders of Unified Combatant Commands. The Commandant performs all other functions prescribed in Section 5043 in Title 10 of the United States Code or delegates those duties and responsibilities to other officers in his administration in his name. As with the other joint chiefs, the Commandant is an administrative position and has no operational command authority over United States Marine Corps forces.

The Commandant is nominated by the President for a four-year term of office and must be confirmed by the Senate. By statute, the Commandant is appointed as a four-star general while serving in office. "The Commandant is directly responsible to the Secretary of the Navy for the total performance of the Marine Corps. This includes the administration, discipline, internal organization, training, requirements, efficiency, and readiness of the service. The Commandant is also responsible for the operation of the Marine Corps material support system." Since 1801, the official residence of the Commandant has been located in the Marine Barracks in Washington, D.C. and his main offices are in Arlington County, Virginia.

The responsibilities of the Commandant are outlined in Title 10, Section 5043, the United States Code and the position is "subject to the authority, direction, and control of the Secretary of the Navy." As stated in the U.S. Code, the Commandant "shall preside over the Headquarters, Marine Corps, transmit the plans and recommendations of the Headquarters, Marine Corps, to the Secretary and advise the Secretary with regard to such plans and recommendations, after approval of the plans or recommendations of the Headquarters, Marine Corps, by the Secretary, act as the agent of the Secretary in carrying them into effect, exercise supervision, consistent with the authority assigned to commanders of unified or specified combatant commands under chapter 6 of this title, over such of the members and organizations of the Marine Corps and the Navy as the Secretary determines, perform the duties prescribed for him by section 171 of this title and other provisions of law and perform such other military duties, not otherwise assigned by law, as are assigned to him by the President, the Secretary of Defense, or the Secretary of the Navy."

Thirty-seven men have served as the Commandant of the Marine Corps. The first Commandant was Samuel Nicholas, who took office as a captain, though there was no office titled "Commandant" at the time, and the Second Continental Congress had authorized that the senior-most Marine could take a rank up to Colonel. The longest-serving was Archibald Henderson, sometimes referred to as the ""Grand old man of the Marine Corps"" due to his thirty-nine-year tenure. In the history of the United States Marine Corps, only one Commandant has ever been fired from the job: Anthony Gale, as a result of a court-martial in 1820.




</doc>
<doc id="7710" url="https://en.wikipedia.org/wiki?curid=7710" title="California Department of Transportation">
California Department of Transportation

The California Department of Transportation (Caltrans) is an executive department of the US state of California. The department is part of the cabinet-level California State Transportation Agency (CalSTA). Caltrans is headquartered in Sacramento.

Caltrans manages the state's highway system, which includes the California Freeway and Expressway System, and is involved with public transportation systems throughout the state. It supports Amtrak California and Amtrak's Capitol Corridor. 

In 2015, Caltrans released a new mission statement: "Provide a safe, sustainable, integrated and efficient transportation system to enhance California’s economy and livability."

The earliest predecessor of Caltrans was the Bureau of Highways, which was created by the California Legislature and signed into law by Governor James Budd in 1895. This agency consisted of three commissioners who were charged with analyzing the state road system and making recommendations. At the time, there was no state highway system, since roads were purely a local responsibility. California's roads consisted of crude dirt roads maintained by county governments, as well as some paved roads within city boundaries, and this ad hoc system was no longer adequate for the needs of the state's rapidly growing population. After the commissioners submitted their report to the governor on November 25, 1896, the legislature replaced the Bureau with the Department of Highways.

Due to the state's weak fiscal condition and corrupt politics, little progress was made until 1907, when the legislature replaced the Department of Highways with the Department of Engineering, within which there was a Division of Highways. California voters approved an US$18 million bond issue for the construction of a state highway system in 1910, and the first California Highway Commission was convened in 1911. On August 7, 1912, the department broke ground on its first construction project, the section of El Camino Real between South San Francisco and Burlingame, which later became part of California State Route 82. The year 1912 also saw the founding of the Transportation Laboratory and the creation of seven administrative divisions, which are the predecessors of the 12 district offices in use . The original seven division headquarters were located in:

In 1913, the California State Legislature began requiring vehicle registration and allocated the resulting funds to support regular highway maintenance.

In 1921, the state legislature turned the Department of Engineering into the Department of Public Works.

The history of Caltrans and its predecessor agencies during the 20th century was marked by many firsts. It was one of the first agencies in the United States to paint centerlines on highways statewide; the first to build a freeway west of the Mississippi River; the first to build a four-level stack interchange; the first to develop and deploy non-reflective raised pavement markers, better known as Botts' dots; and one of the first to implement dedicated freeway-to-freeway connector ramps for high-occupancy vehicle lanes.

In late 1972, the legislature approved a reorganization, suggested by a study initiated by then-Governor Ronald Reagan, in which the Department of Public Works was merged with the Department of Aeronautics to become the modern California Department of Transportation.

For administrative purposes, Caltrans divides the State of California into 12 districts, supervised by district offices. Most districts cover multiple counties; District 12 (Orange County) is the only district with one county. The largest districts by population are District 4 (San Francisco Bay Area) and District 7 (Los Angeles and Ventura counties). Like most state agencies, Caltrans maintains its headquarters in Sacramento, which is covered by District 3.



</doc>
<doc id="7712" url="https://en.wikipedia.org/wiki?curid=7712" title="Continuation War">
Continuation War

The Continuation War was a conflict fought by Finland and Nazi Germany, as co-belligerents, against the Soviet Union (USSR) from 1941 to 1944, during World War II. In Russian historiography, the war is called the Soviet–Finnish Front of the Great Patriotic War. Germany regarded its operations in the region as part of its overall war efforts on the Eastern Front and provided Finland with critical material support and military assistance.
The Continuation War began 15 months after the end of the Winter War, also fought between Finland and the USSR. There have been a number of reasons proposed for the Finnish decision to invade, with regaining territory lost during the Winter War being regarded as the most common. Other justifications for the conflict included President Ryti's vision of a Greater Finland and Commander-in-Chief Mannerheim's desire to liberate Karelia. Plans for the attack were developed jointly between the "Wehrmacht" and a small faction of Finnish political and military leaders with the rest of the government remaining ignorant. Despite the co-operation in this conflict, Finland never formally signed the Tripartite Pact that had established the Axis powers and justified its alliance with Germany as self-defense.

In June 1941, with the start of the German invasion of the Soviet Union, the Finnish Defence Forces launched their offensive following Soviet airstrikes. By September 1941, Finland occupied East Karelia and reversed its post-Winter War concessions to the Soviet Union along the Karelian Isthmus and in Ladoga Karelia. The Finnish Army halted its offensive past the old border, around from the centre of Leningrad and participated in besieging the city by cutting its northern supply routes and digging in until 1944. In Lapland, joint German–Finnish forces failed to capture Murmansk or cut the Kirov (Murmansk) Railway, a transit route for lend-lease equipment to the USSR. The conflict stabilised with only minor skirmishes until the tide of the war turned against the Germans and the Soviet Union's strategic Vyborg–Petrozavodsk Offensive crushed Finnish resistance in June 1944. The attack drove the Finns from most of the territories they had gained during the war, but the Finnish Army managed to halt the offensive in August 1944.

Hostilities between Finland and the USSR ended with a ceasefire, which was called on 5 September, and formalised by the signing of the Moscow Armistice on 19 September. One of the conditions of this agreement was the expulsion, or disarming, of any German troops in Finnish territory, which led to the Lapland War between the former co-belligerents. World War II was concluded formally for Finland and the minor Axis powers with the signing of the Paris Peace Treaties in 1947. The treaties resulted in the restoration of borders per the 1940 Moscow Peace Treaty, the ceding of the municipality of Petsamo () and the leasing of Porkkala Peninsula to the USSR. Furthermore, Finland was required to pay $300 million in war reparations to the USSR.

63,200 Finns and 23,200 Germans died or went missing during the war in addition to 158,000 and 60,400 wounded, respectively. Estimates of dead or missing Soviets range from 250,000 to 305,000 while 575,000 have been estimated to have been wounded or fallen sick.

On 23 August 1939, the Soviet Union (USSR) and Germany signed the Molotov–Ribbentrop Pact, in which the two parties agreed to divide the independent countries of Finland, Estonia, Latvia, Lithuania, Poland, and Romania into spheres of interest, with Finland falling within the Soviet sphere. Shortly after, Germany invaded Poland leading to the United Kingdom (UK) and France declaring war on Germany. The Soviet Union invaded eastern Poland on 17 September. Moscow turned its attention to the Baltic states, demanding that they allow Soviet military bases to be established and troops stationed on their soil. The Baltic governments acquiesced to these demands and signed agreements in September and October.

In October 1939, the Soviet Union attempted to negotiate with Finland to cede Finnish territory on the Karelian Isthmus and the islands of the Gulf of Finland, and to establish a Soviet military base near the Finnish capital of Helsinki. The Finnish government refused, and the Red Army invaded Finland on 30 November 1939. The USSR was expelled from the League of Nations and condemned by the international community for the illegal attack. Foreign support for Finland was promised, but very little actual help materialised, except from Sweden. The Moscow Peace Treaty concluded the 105-day Winter War on 13 March 1940 and started the Interim Peace. By the terms of the treaty, Finland ceded 11 per cent of its national territory and 13 percent of its economic capacity to the Soviet Union. Some 420,000 evacuees were resettled from the ceded territories. Finland avoided total conquest of the country by the Soviet Union and retained its sovereignty.

Prior to the war, Finnish foreign policy had been based on multilateral guarantees of support from the League of Nations and Nordic countries, but this policy was considered a failure. After the war, Finnish public opinion favored the reconquest of Finnish Karelia. The government declared national defence to be its first priority, and military expenditure rose to nearly half of public spending. Finland purchased and received donations of war materiel during and immediately after the Winter War. Likewise, Finnish leadership wanted to preserve the spirit of unanimity that was felt throughout the country during the Winter War. The divisive White Guard tradition of the Finnish Civil War's 16 May victory-day celebration was therefore discontinued.

The Soviet Union had received the Hanko Naval Base, on Finland's southern coast near the capital Helsinki, where it deployed over 30,000 Soviet military personnel. Relations between Finland and the Soviet Union remained strained after the signing of the one-sided peace treaty, and there were disputes regarding the implementation of the treaty. Finland sought security against further territorial depredations by the USSR and proposed mutual defence agreements with Norway and Sweden, but these initiatives were quashed by Moscow.

After the Winter War, Germany was viewed with distrust by the Finnish, as it was considered an ally of the Soviet Union. Nonetheless, the Finnish government sought to restore diplomatic relations with Germany, but also continued its Western-oriented policy and negotiated a war trade agreement with the United Kingdom. The agreement was renounced after the German invasion of Denmark and Norway on 9 April 1940 resulted in the UK cutting all trade and traffic communications with the Nordic countries. With the fall of France, a Western orientation was no longer considered a viable option in Finnish foreign policy. On 15 and 16 June, the Soviet Union occupied the Baltic states without resistance and Soviet puppet regimes were installed. Within two months Estonia, Latvia and Lithuania were incorporated into the USSR as Soviet republics and by mid-1940, the two remaining northern democracies, Finland and Sweden, were encircled by the hostile states of Germany and the Soviet Union.

On 23 June, shortly after the Soviet occupation of the Baltic states began, Soviet Foreign Minister Vyacheslav Molotov contacted the Finnish government demanding that a mining license be issued to the USSR for the nickel mines in the municipality of Petsamo () or, alternatively, permit the establishment of a joint Soviet-Finnish company to operate there. A license to mine the deposit had already been granted to a British-Canadian company, and the demand was rejected by Finland. The following month, the Soviets demanded that Finland destroy the fortifications on the Åland islands and grant the USSR the right to use Finnish railways to transport Soviet troops to the newly-acquired Soviet base at Hanko. The Finns very reluctantly agreed to these demands. On 24 July, Molotov accused the Finnish government of persecuting the Finland – Soviet Union Peace and Friendship Society, a pro-communist group, and soon afterwards publicly declared support for the group. The society organised demonstrations in Finland, some of which turned into riots.

Russian sources, such as the book "Stalin's Missed Chance", maintain that Soviet policies leading up to the Continuation War were best explained as defensive measures by offensive means. The Soviet division of occupied Poland with Germany, the Soviet occupations of Lithuania, Latvia and Estonia, and the Soviet invasion of Finland in the Winter War are described as elements in the Soviet construction of a security zone, or buffer region, against the perceived threat from the capitalist powers of Western Europe. The Russian sources see the post-World War II establishment of Soviet satellite states in the Warsaw Pact countries and the Finno-Soviet Treaty of 1948 as the culmination of the Soviet defence plan. Western historians, such as Norman Davies and John Lukacs, dispute this view and describe pre-war Soviet policy as an attempt to stay out of the war and regain land lost after the fall of the Russian Empire.

On 31 July 1940, German Chancellor Adolf Hitler gave the order to start planning an assault on the Soviet Union, meaning Germany had to reassess its position regarding both Finland and Romania. Until then, Germany had rejected Finnish appeals to purchase arms, but with the prospect of an invasion of Russia, this policy was reversed, and in August the secret sale of weapons to Finland was permitted. Military authorities signed an agreement on 12 September, and an official exchange of diplomatic notes was sent on 22 September. At the same time, German troops were allowed to transit through Sweden and Finland. This change in policy meant Germany had effectively redrawn the border of the German and Soviet spheres of influence, violating the Molotov-Ribbentrop Pact.

In response to this new situation, Molotov visited Berlin on 12–13 November 1940. He requested that Germany withdraw its troops from Finland and stop enabling Finnish anti-Soviet sentiments. He also reminded the Germans of the 1939 Soviet–German non-aggression pact. Hitler inquired how the USSR planned to settle the "Finnish question", to which Molotov responded that it would mirror the events in Bessarabia and the Baltic states. Hitler rejected this course of action. In December, the Soviet Union, Germany and the UK all voiced opinions concerning suitable Finnish presidential candidates. Risto Ryti was the sole candidate not objected to by any of the three powers and was elected on 19 December.

In January 1941, Moscow demanded Finland relinquish control of the Petsamo mining area to the Soviets, but Finland, emboldened by a rebuilt defence force and German support, rejected the proposition. On 18 December 1940, Hitler officially approved Operation Barbarossa, paving the way for the German invasion of the Soviet Union, in which he expected both Finland and Romania to participate. During this period, Finnish Major General Paavo Talvela met with German Franz Halder and Hermann Göring in Berlin. This was the first time the Germans had advised the Finnish government, in carefully couched diplomatic terms, that they were preparing for war with the Soviet Union. Outlines of the actual plan were revealed in January 1941 and regular contact between Finnish and German military leaders began in February.

In the late spring of 1941, the USSR made a number of goodwill gestures to prevent Finland from completely falling under German influence. Ambassador Ivan Zotov was replaced with the more flexible Pavel Orlov. Furthermore, the Soviet government announced that it no longer opposed a rapprochement between Finland and Sweden. These conciliatory measures, however, did not have any effect on Finnish policy. Finland wished to re-enter World War II mainly because of the Soviet invasion of Finland during the Winter War, which had taken place after Finnish intentions of relying on the League of Nations and Nordic neutrality to avoid conflicts had failed from lack of outside support. Finland primarily aimed to reverse its territorial losses from the March 1940 Moscow Peace Treaty and, depending on the success of the German invasion of the Soviet Union, to possibly expand its borders, especially into East Karelia. Some right-wing groups, such as the Academic Karelia Society, supported a Greater Finland ideology.

The matter of when and why Finland prepared for war is still somewhat opaque. Historian William R. Trotter stated that "it has so far proven impossible to pinpoint the exact date on which Finland was taken into confidence about Operation Barbarossa" and that "neither the Finns nor the Germans were entirely candid with one another as to their national aims and methods. In any case, the step from contingency planning to actual operations, when it came, was little more than a formality." 

The inner circle of Finnish leadership, led by Ryti and Mannerheim, actively planned joint operations with Germany under a veil of ambiguous neutrality and without formal agreements, after an alliance with Sweden proved fruitless—according to a meta-analysis by Finnish historian Olli Vehviläinen. He likewise refuted the so-called "driftwood theory" that Finland was merely a piece of driftwood swept uncontrollably in the rapids of great-power politics. Even then, most historians conclude that Finland did not have any realistic alternatives to cooperating with Germany at the time. On 20 May, the Germans invited a number of Finnish officers to discuss the coordination of Operation Barbarossa. The participants met on 25–28 May in Salzburg and Berlin, and continued their meeting in Helsinki from 3 to 6 June. They agreed upon the arrival of German troops, Finnish mobilization, and a general division of operations. They also agreed that the Finnish Army would start mobilization on 15 June, but the Germans did not reveal the actual date of the assault. The Finnish decisions were made by the inner circle of political and military leaders, without the knowledge of the rest of the government, who were not informed until 9 June that mobilization of reservists, due to tensions between Germany and the Soviet Union, would be required.

Finland never signed the Tripartite Pact, which had been signed by all de jure Axis powers. The Finnish leadership and Mannerheim, in particular, clearly stated they would fight against the Soviets only to the extent necessary to redress the balance of the 1940 treaty. For Hitler, the distinction was irrelevant as he saw Finland as an ally.

The Northern Front () of the Leningrad Military District was commanded by Lieutenant General Markian Popov and numbered around 450,000 soldiers in 18 divisions and 40 independent battalions in the Finnish region. During the Interim Peace, the Soviet Military had relaid operational plans to conquer Finland, but with Operation Barbarossa, the USSR required its best units and latest materiel to be deployed against the Germans, and thus abandoned plans for a renewed offensive against Finland. The 23rd Army was deployed in the Karelian Isthmus, the 7th Army to Ladoga Karelia and the 14th Army to the Murmansk–Salla area of Lapland. The Northern Front also commanded 8 aviation divisions. As the initial German strike against the Soviet Air Forces had not affected air units located near Finland, it could deploy around 700 aircraft supported by a number of Soviet Navy wings. The Red Banner Baltic Fleet comprised 2 battleships, 2 light cruisers, 47 destroyers or large torpedo boats, 75 submarines, over 200 smaller craft as well as hundreds of aircraft—and outnumbered the "Kriegsmarine".

The Finnish Army () mobilised between 475,000 and 500,000 soldiers in 14 divisions and 3 brigades for the invasion, commanded by Field Marshal () Mannerheim. The army was organised as follows:

Although initially deployed for a static defence, the Finnish Army was to later launch an attack to the south, on both sides of Lake Ladoga, putting pressure on Leningrad and thus supporting the advance of the German Army Group North. Finnish intelligence had overestimated the strength of the Red Army, when in fact it was numerically inferior to Finnish forces at various points along the border. The army, especially its artillery, was stronger than it had been during the Winter War but included only one armoured battalion and had a general lack of motorised transportation. The Finnish Air Force () had 235 aircraft in July 1941 and 384 by September 1944, despite losses. Even with the increase in aircraft, the air force was constantly outnumbered by the Soviets.

The Army of Norway, or , comprising four divisions totaling 67,000 German soldiers, held the arctic front, which stretched approximately through Finnish Lapland. This army would also be tasked with striking Murmansk and the Kirov (Murmansk) Railway during Operation Silver Fox. The Army of Norway was under the direct command of the "Oberkommando des Heeres" () and was organised into Mountain Corps Norway and XXXVI Mountain Corps with the Finnish Finnish III Corps and 14th Division attached to it. The "Oberkommando der Luftwaffe" () assigned 60 aircraft from "Luftflotte 5" (Air Fleet 5) to provide air support to the Army of Norway and the Finnish Army, in addition to its main responsibility of defending Norwegian air space. In contrast to the front in Finland, a total of 149 divisions and 3,050,000 soldiers were deployed for the rest of Operation Barbarossa.

In the evening of 21 June 1941, German minelayers hiding in the Archipelago Sea deployed two large minefields across the Gulf of Finland. Later that night, German bombers flew along the gulf to Leningrad, mining the harbour and the river Neva, making a refueling stop at Utti, Finland, on the return leg. In the early hours of 22 June, Finnish forces launched Operation Kilpapurjehdus ("Regatta"), deploying troops in the demilitarised Åland Islands. Although the 1921 Åland convention had clauses allowing Finland to defend the islands in the event of an attack, the coordination of this operation with the German invasion and the arrest of the Soviet consulate staff stationed on the islands, meant that the deployment was a deliberate violation of the treaty, according to Finnish historian Mauno Jokipii.

Following the launch of Operation Barbarossa at around 3:15 a.m. on 22 June 1941, the Soviet Union sent 7 bombers on a retaliatory airstrike into Finland, hitting targets at 6:06 a.m. Helsinki time as reported by the Finnish coastal defence ship Väinämöinen. On the morning of 25 June, the Soviet Union launched another air offensive, with 460 fighters and bombers targeting 19 airfields in Finland, however inaccurate intelligence and poor bombing accuracy resulted in several raids hitting Finnish cities, or municipalities, causing considerable damage. 23 Soviet bombers were lost in this strike while the Finnish forces did not lose aircraft. Although the USSR claimed that the airstrikes were directed against German targets, particularly airfields, in Finland, the Finnish government used the attacks as justification for the approval of a "defensive war". According to historian David Kirby, the message was intended more for public opinion in Finland than abroad, where the country was viewed as an ally of the Axis powers.

The Finnish plans for the offensive in Ladoga Karelia were finalised on 28 June, and the first stages of the operation began on 10 July. By 16 July, VI Corps had reached the northern shore of Lake Ladoga, dividing the Soviet 7th Army, which had been tasked with defending the area. The USSR struggled to contain the German assault, and soon the Soviet high command, "Stavka", pulled all available units stationed along the Finnish border into the beleaguered front line. Additional reinforcements were drawn from the 237th Rifle Division and the Soviet 10th Mechanised Corps, excluding the 198th Motorised Division, both of which were stationed in Ladoga Karelia, but this stripped much of the reserve strength of the Soviet units defending that area. The Finnish II Corps started its offensive in the north of the Karelian Isthmus on 31 July. Other Finnish forces reached the shores of Lake Ladoga on 9 August, encircling most of the three defending Soviet divisions on the northwestern coast of the lake in a pocket ("motti" in Finnish); these divisions were later evacuated across the lake. On 22 August, the Finnish IV Corps began its offensive south of II Corps and advanced towards Vyborg (). By 23 August, II Corps had reached the Vuoksi River to the east and encircled the Soviet forces defending Vyborg.

The Soviet order to withdraw came too late, resulting in significant losses in materiel, although most of the troops were later evacuated via the Koivisto Islands. After suffering severe losses, the Soviet 23rd Army was unable to halt the offensive, and by 2 September the Finnish Army had reached the old 1939 border. The advance by Finnish and German forces split the Soviet Northern Front into the Leningrad Front and the Karelian Front. On 31 August, Finnish Headquarters ordered II and IV Corps, which had advanced the furthest, to halt their advance along a line that ran from the Gulf of Finland via Beloostrov– Sestra River– Okhta River–Lembolovo to Ladoga. The line ran past the former 1939 border, and approximately from Leningrad. There, they were ordered to take up a defensive position. On 1 September, the IV Corps engaged and defeated the Soviet 23rd Army near the town of Porlampi. Sporadic fighting continued around Beloostrov until the Soviets evicted the Finns on 20 September. The front on the Isthmus stabilised and the Siege of Leningrad began.

The Finnish Army of Karelia started its attack in East Karelia towards Petrozavodsk, Lake Onega and the Svir River on 9 September. German Army Group North advanced from the south of Leningrad towards the Svir River and captured Tikhvin but were forced to retreat to the Volkhov River by Soviet counterattacks. Soviet forces repeatedly attempted to expel the Finns from their bridgehead south of the Svir during October and December but were repulsed; Soviet units attacked the German 163rd Infantry Division in October 1941, which was operating under Finnish command across the Svir, but failed to dislodge it. Despite these failed attacks, the Finnish attack in East Karelia had been blunted and their advance had halted by 6 December. During the five-month campaign, the Finns suffered 75,000 casualties, of whom 26,355 had died, while the Soviets had 230,000 casualties, of whom 50,000 became prisoners of war.

The German objective in Finnish Lapland was to take Murmansk and cut the Kirov (Murmansk) Railway running from Murmansk to Leningrad by capturing Salla and Kandalaksha. Murmansk was the only year-round ice-free port in the north and a threat to the nickel mine at Petsamo. The joint Finnish–German Operation Silver Fox (; ) was started on 29 June 1941 by the German Army of Norway, which had the Finnish 3rd and 6th Divisions under its command, against the defending Soviet 14th Army and 54th Rifle Division. By November, the operation had stalled from the Kirov Railway due to unacclimatised German troops, heavy Soviet resistance, poor terrain, arctic weather and diplomatic pressure by the United States on the Finns regarding the lend-lease deliveries to Murmansk. The offensive and its three sub-operations failed to achieve their objectives. Both sides dug in and the arctic theatre remained stable, excluding minor skirmishes, until the Soviet Petsamo–Kirkenes Offensive in October 1944.

The crucial arctic lend-lease convoys from the US and the UK via Murmansk and Kirov Railway to the bulk of the Soviet forces continued throughout World War II. The US supplied almost $11 billion in materials: 400,000 jeeps and trucks; 12,000 armored vehicles (including 7,000 tanks, which could equip some 20 US armoured divisions); 11,400 aircraft; and of food. As a similar example, British shipments of Matilda, Valentine and Tetrarch tanks accounted for only 6 percent of total Soviet tank production but over 25 percent of medium and heavy tanks produced for the Red Army.

The "Wehrmacht" rapidly advanced deep into Soviet territory early in the Operation Barbarossa campaign, leading the Finnish government to believe that Germany would defeat the Soviet Union quickly. President Ryti envisioned a Greater Finland, where Finland and other Finnic people would live inside a "natural defence borderline" by incorporating the Kola Peninsula, East Karelia and perhaps even northern Ingria. In public, the proposed frontier was introduced with the slogan "short border, long peace". Some members of the Finnish Parliament, such as the Social Democratic Party and the Swedish People's Party, opposed the idea, arguing that maintaining the 1939 frontier would be enough. Finnish Commander-in-Chief, Field Marshal C. G. E. Mannerheim, often called the war an anti-Communist crusade, hoping to defeat "Bolshevism once and for all". On 10 July, Mannerheim drafted his order of the day, the Sword Scabbard Declaration, in which he pledged to liberate Karelia; in December 1941 in private letters, he made known his doubts of the need to push beyond the previous borders. The Finnish government assured the United States that it was unaware of the order.

According to Vehviläinen, most Finns thought that the scope of the new offensive was only to regain what had been taken in the Winter War. He further stated that the term 'Continuation War' was created at the start of the conflict by the Finnish government to justify the invasion to the population as a continuation of the defensive Winter War. The government also wished to emphasise that it was not an official ally of Germany, but a 'co-belligerent' fighting against a common enemy and with purely Finnish aims. Vehviläinen wrote that the authenticity of the government's claim changed when the Finnish Army crossed the old frontier of 1939 and began to annex Soviet territory. British author Jonathan Clements asserted that by December 1941, Finnish soldiers had started questioning whether they were fighting a war of national defence or foreign conquest.

By the autumn of 1941, the Finnish military leadership started to doubt Germany's capability to finish the war quickly. The Finnish Defence Forces suffered relatively severe losses during their advance, and, overall, German victory became uncertain as German troops were halted near Moscow. German troops in northern Finland faced circumstances they were unprepared for and failed to reach their targets. As the front lines stabilised, Finland attempted to start peace negotiations with the USSR. Mannerheim refused to assault Leningrad and tie Finland to its German allies inextricably, regarding his objectives for the war to be achieved, a decision which angered the Germans.Due to the war effort, the Finnish economy suffered from a lack of labour, as well as food shortages and increased prices. To combat this, the Finnish government demobilised part of the army to prevent industrial and agricultural production from collapsing. In October, Finland informed Germany that it would need of grain to manage until next year's harvest. The German authorities would have rejected the request, but Hitler himself agreed. Annual grain deliveries of equaled almost half of the Finnish domestic crop. In November, Finland joined the Anti-Comintern Pact.

Finland maintained good relations with a number of other Western powers. Foreign volunteers from Sweden and Estonia were among the foreigners who joined Finnish ranks; Infantry Regiment 200, called ("Finnish boys"), mostly comprised Estonians, while the Swedes mustered the Swedish Volunteer Battalion. The Finnish government stressed that Finland was fighting as a co-belligerent with Germany against the USSR only to protect itself and that it was still the same democratic country as it had been in the Winter War. For example, Finland maintained diplomatic relations with the exiled Norwegian government and more than once criticised German occupation policy in Norway. Relations between Finland and the United States were more complex; the US public was sympathetic to the "brave little democracy" and had anti-communist sentiments. At first, the United States sympathised with the Finnish cause, but the situation became problematic after the Finnish Army crossed the 1939 border. Finnish and German troops were a threat to the Kirov Railway and the northern supply line between the Western Allies and the Soviet Union. On 25 October 1941, the US demanded that Finland cease all hostilities against the USSR and withdraw behind the 1939 border. In public, President Ryti rejected the demands, but in private, he wrote to Mannerheim on 5 November asking him to halt the offensive. Mannerheim agreed and secretly instructed General Hjalmar Siilasvuo and his III Corps to end the assault on the Kirov Railway.

On 12 July 1941, the United Kingdom signed an agreement of joint action with the Soviet Union. Under German pressure, Finland closed the British legation in Helsinki, cutting diplomatic relations with the UK on 1 August. The most sizeable British action on Finnish soil was the Raid on Kirkenes and Petsamo, an aircraft-carrier strike on German and Finnish ships on 31 July 1941. The attack achieved little, except the loss of one Norwegian ship and three British aircraft, but it was intended to demonstrate British support for its Soviet ally. From September to October in 1941, a total of 39 Hawker Hurricanes of No. 151 Wing RAF, based at Murmansk, reinforced and provided pilot-training to the Soviet Air Forces during Operation Benedict to protect arctic convoys. On 28 November, the UK presented Finland an ultimatum demanding that the Finns cease military operations by 3 December. Unofficially, Finland informed the Western powers that Finnish troops would halt their advance in the next few days. The reply did not satisfy the United Kingdom, which declared war on Finland on 6 December. The Commonwealth nations of Canada, Australia, the British Raj and New Zealand soon followed suit. In private, British Prime Minister Winston Churchill had sent a letter to Mannerheim on 29 November, in which he was "deeply grieved" that the UK would have to declare war on Finland because of the UK's alliance with the USSR. Mannerheim returned British volunteers under his command to the United Kingdom via Sweden. According to Clements, the war was mostly for appearances' sake.

Unconventional warfare was fought in both the Finnish and Soviet wildernesses. Finnish long-range reconnaissance patrols, organised both by the Intelligence Division's Detached Battalion 4 and by local units, patrolled behind Soviet lines. Soviet partisans, both resistance fighters and regular long-range patrol detachments, conducted a number of operations in Finland and in Eastern Karelia from 1941 to 1944. In summer 1942, the USSR formed the 1st Partisan Brigade. The unit was 'partisan' in name only, as it was essentially 600 men and women on long-range patrol intended to disrupt Finnish operations. The 1st Partisan Brigade was able to infiltrate beyond Finnish patrol lines, but was intercepted, and rendered ineffective, in August 1942 at Lake Segozero. Irregular partisans distributed propaganda newspapers, such as Finnish translations of the official Communist Party paper "Pravda" (). Notable Soviet politician, Yuri Andropov, took part in these partisan guerrilla actions. Finnish sources state that, although Soviet partisan activity in East Karelia disrupted Finnish military supply and communication assets, almost two thirds of the attacks targeted civilians, killing 200 and injuring 50, including children and elderly.

Between 1942 and 1943, military operations were limited, although the front did see some action. In January 1942, the Soviet Karelian Front attempted to retake Medvezhyegorsk (), which had been lost to the Finns in late 1941. With the arrival of spring in April, Soviet forces went on the offensive on the Svir River front, in the Kestenga () region further north in Lapland as well as in the far north at Petsamo with the 14th Rifle Division's amphibious landings supported by the Northern Fleet. All Soviet offensives started promisingly, but due either to the Soviets overextending their lines or stubborn defensive resistance, the offensives were repulsed. After Finnish and German counterattacks in Kestenga, the front lines were generally stalemated. In September 1942, the USSR attacked again at Medvezhyegorsk, but despite five days of fighting, the Soviets only managed to push the Finnish lines back on a roughly -long stretch of the front. Later that month, a Soviet landing with two battalions in Petsamo was defeated by a German counterattack. In November 1941, Hitler decided to separate the German forces fighting in Lapland from the Army of Norway and create the Army of Lapland, commanded by Eduard Dietl through . In June 1942, the Army of Lapland was redesignated the 20th Mountain Army.

In the early stages of the war, the Finnish Army overran the former 1939 border, but ceased their advance from the center of Leningrad. Multiple authors have stated that Finland participated in the Siege of Leningrad (), but the full extent and nature of their participation is debated and a clear consensus has yet to emerge. American historian David Glantz, writes that the Finnish Army generally maintained their lines and contributed little to the siege from 1941 to 1944, whereas Russian historian Nikolai Baryshnikov stated in 2002 that Finland tacitly supported Hitler's starvation policy for the city. However, in 2009 British historian Michael Jones refuted Baryshnikov's claim and asserted that the Finnish Army cut off the city's northern supply routes but did not take further military action. In 2006, American author Lisa A. Kirchenbaum wrote that the siege started "when German and Finnish troops severed all land routes in and out of Leningrad."

According to Clements, Mannerheim personally refused Hitler's request of assaulting Leningrad during their meeting on 4 June 1942. Mannerheim explained to Hitler that "Finland had every reason to wish to stay out of any further provocation of the Soviet Union." In 2014, author Jeff Rutherford described the city as being "ensnared" between the German and Finnish armies. British historian John Barber described it as a "siege by the German and Finnish armies from 8 September 1941 to 27 January 1944 [...]" in his foreword in 2017. Likewise, in 2017, Alexis Peri wrote that the city was "completely cut off, save a heavily patrolled water passage over Lake Ladoga" by "Hitler's Army Group North and his Finnish allies."

The 150 speedboats, 2 minelayers and 4 steamships of the Finnish Ladoga Naval Detachment, as well as numerous shore batteries, had been stationed on Lake Ladoga since August 1941. Finnish Lieutenant General Paavo Talvela proposed on 17 May 1942 to create a joint Finnish–German–Italian unit on the lake to disrupt Soviet supply convoys to Leningrad. The unit was named Naval Detachment K and comprised four Italian MAS torpedo motorboats of the XII Squadriglia MAS, four German KM-type minelayers and the Finnish torpedo-motorboat "Sisu". The detachment began operations on August 1942 and sank numerous smaller Soviet watercraft and flatboats and assaulted enemy bases and beach fronts until it was dissolved in the winter of 1942–43. Twenty-three Siebel ferries and nine infantry transports of the German "Einsatzstab Fähre Ost" were also deployed to Lake Ladoga and unsuccessfully assaulted the island of Sukho, which protected the main supply route to Leningrad, on October 1942.

Despite the siege of the city, the Soviet Baltic Fleet was still able to operate from Leningrad. The Finnish Navy's flagship had been sunk in September 1941 in the gulf by mines during the failed diversionary Operation Nordwind. In early 1942, Soviet forces recaptured the island of Gogland, but lost it and the Bolshoy Tyuters islands to Finnish forces later in spring 1942. During the winter between 1941 and 1942, the Soviet Baltic Fleet decided to use their large submarine fleet in offensive operations. Though initial submarine operations in the summer of 1942 were successful, the and Finnish Navy soon intensified their anti-submarine efforts, making Soviet submarine operations later in 1942 costly. The underwater offensive carried out by the Soviets convinced the Germans to lay anti-submarine nets as well as supporting minefields between Porkkala Peninsula and Naissaar, which proved to be an insurmountable obstacle for Soviet submarines. On the Arctic Ocean, Finnish radio intelligence intercepted Allied messages on supply convoys to Murmansk, such as PQ-17 and PQ-18, and relayed the information to the "Abwehr", German intelligence.

On 19 July 1941, the Finns created a military administration in occupied East Karelia with the goal of preparing the region for eventual incorporation into Finland. The Finns aimed to expel the local Russian population, who were deemed "non-national", from the area once the war was over, and replace them with Finnic peoples, such as Karelians, Finns, Estonians, Ingrians and Vepsians. Most of the East Karelian population had already been evacuated before the Finnish forces arrived, but about 85,000 people — mostly elderly, women and children — were left behind, less than half of whom were Karelians. A significant number of civilians, almost 30 percent of the remaining Russians, were interned in concentration camps.
The winter between 1941 and 1942 was particularly harsh for the Finnish urban population due to poor harvests and a shortage of agricultural labourers. However, conditions were much worse for Russians in Finnish concentration camps. More than 3,500 people died, mostly from starvation, amounting to 13.8 per cent of those detained, while the corresponding figure for the free population of the occupied territories was 2.6 per cent, and 1.4 per cent for Finland. Conditions gradually improved, ethnic discrimination in wage levels and food rations was terminated, and new schools were established for the Russian-speaking population the following year, after Commander-in-Chief Mannerheim called for the International Committee of the Red Cross from Geneva to inspect the camps. By the end of the occupation, mortality rates had dropped to the same levels as in Finland.

Finland had a small Jewish population of approximately 2,300 people, of whom 300 were refugees. They had full civil rights and fought with other Finns in the ranks of the Finnish Army. The field synagogue in East Karelia was one of the very few functioning synagogues on the Axis side during the war. There were several cases of Jewish officers of the Finnish Army being awarded the German Iron Cross, which they declined. German soldiers were treated by Jewish medical officers—who sometimes saved the soldiers' lives. German command mentioned Finnish Jews at the Wannsee Conference in January 1942, wishing to transport them to the Majdanek concentration camp in occupied Poland. "SS" leader Heinrich Himmler also raised the topic of Finnish Jews during his visit in Finland in the summer of 1942; Finnish Prime Minister Jukka Rangell replied that Finland did not have a Jewish question. In November 1942, Arno Anthoni, head of State Police, deported eight Jewish refugees to the "Gestapo" in secret, raising protests among Finnish Social Democrat Party ministers. Only one of the deportees survived. After the incident, the Finnish government refused to transfer any more Jews to German detainment.

Finland began to seek an exit from the war after the German defeat at the Battle of Stalingrad in February 1943. Prime Minister Edwin Linkomies formed a new cabinet in March 1943 with peace as the top priority. Similarly, the Finns were distressed by the Allied Invasion of Sicily in July and the German defeat in the Battle of Kursk in August. Negotiations were conducted intermittently during 1943–1944 between Finland, the Western Allies and the USSR, but no agreement was reached. Stalin decided to force Finland to surrender with a bombing campaign on Helsinki, starting in February 1944. It included three major air attacks totaling over 6,000 sorties. Finnish anti-aircraft defence repelled the raids and only five per cent of the dropped bombs hit their planned targets. In Helsinki, decoy searchlights and fires were placed outside the city to deceive Soviet bombers into dropping their payloads on unpopulated areas. Major air attacks also hit Oulu and Kotka, but pre-emptive radio intelligence and effective defence kept the number of casualties low.

The Soviet Leningrad–Novgorod Offensive finally lifted the Siege of Leningrad on 26–27 January 1944 and pushed Army Group North to Ida-Viru County on the Estonian border. Stiff German and Estonian defence in Narva from February to August prevented the use of occupied Estonia as a favourable base for Soviet amphibious and air assaults against Helsinki and other Finnish coastal cities in support of a land offensive. Field Marshal Mannerheim had reminded the German command on numerous occasions that should German troops withdraw from Estonia, Finland would be forced to make peace, even on extremely unfavourable terms. Finland would abandon peace negotiations in April 1944 due to the unfavourable terms the USSR demanded.

On 9 June 1944, the Soviet Leningrad Front launched an offensive against Finnish positions on the Karelian Isthmus and in the area of Lake Ladoga, timed to coincide with Operation Overlord in Normandy as agreed during the Tehran Conference. The main objective of the offensive was to force Finland out of the war. Along the -wide breakthrough, the Red Army concentrated 3,000 guns and mortars. In some places, the concentration of artillery pieces exceeded 200 guns for every kilometre of front or one for every . Soviet artillery fired over 80,000 rounds along the front on the Karelian Isthmus. On the second day of the offensive, the artillery barrages and superior number of Soviet forces crushed the main Finnish defence line. The Red Army penetrated the second line of defence, the Vammelsuu–Taipale line (VT line), by the sixth day and recaptured Vyborg almost without resistance on 20 June. The Soviet breakthrough on the Karelian Isthmus forced the Finns to reinforce the area, thus allowing the concurrent Soviet offensive in East Karelia to meet less resistance and to recapture Petrozavodsk by 28 June 1944. 

On 25 June, the Red Army reached the third line of defence, the Viipuri–Kuparsaari–Taipale line (VKT line), and the decisive Battle of Tali-Ihantala began, which has been described as the largest battle in Nordic military history. By this point, the Finnish Army had retreated around to approximately the same line of defence they had held at the end of the Winter War. Finland especially lacked modern anti-tank weaponry that could stop Soviet heavy armour, such as the KV-1 or IS-2. Thus, German Foreign Minister Joachim von Ribbentrop offered German hand-held "Panzerfaust" and "Panzerschreck" antitank weapons in exchange for a guarantee that Finland would not seek a separate peace with the USSR. On 26 June, President Risto Ryti gave the guarantee as a personal undertaking, which he, Field Marshal Mannerheim and Prime Minister Edwin Linkomies intended to legally last only for the remainder of Ryti's presidency. In addition to delivering thousands of anti-tank weapons, Hitler sent the 122nd Infantry Division and the half-strength 303rd Assault Gun Brigade armed with Sturmgeschütz III tank destroyers as well as the Luftwaffe's Detachment Kuhlmey to provide temporary support in the most vulnerable sectors. With the new supplies and assistance from Germany, the Finnish Army halted the numerically and materially superior Soviet advance at Tali-Ihantala on 9 July 1944 and stabilised the front.

More battles were fought toward the end of the war, the last of which was the Battle of Ilomantsi, fought between 26 July and 13 August 1944 and resulting in a Finnish victory with the destruction of two Soviet divisions. Resisting the Soviet offensive had exhausted Finnish resources. Despite German support under the Ryti-Ribbentrop Agreement, it was asserted that the country was unable to blunt another major offensive. Soviet victories against German Army Groups Center and North during Operation Bagration made the situation even more dire for Finland. With no imminent further Soviet offensives, Finland sought to leave the war. On 1 August, President Ryti resigned and on 4 August, Field Marshal Mannerheim was sworn in as the new president. He annulled the agreement between Ryti and Ribbentrop on 17 August, thus allowing Finland to again sue for peace with the USSR; peace terms from Moscow arrived on 29 August.

Finland was required to return to the borders agreed to in the 1940 Moscow Peace Treaty, demobilise its armed forces, fulfill war reparations and cede the municipality of Petsamo. The Finns were also required to immediately end any diplomatic relations with Germany and expel the from Finnish territory by 15 September 1944; any troops remaining were to be disarmed, arrested and turned over to the Allies. The Parliament of Finland accepted the terms in a secret meeting on 2 September and requested that official negotiations for an armistice begin. The Finnish Army implemented a ceasefire at 8:00 a.m. Helsinki time on 4 September; the Red Army followed suit a day later. On 14 September, a delegation led by Finnish Prime Minister Antti Hackzell and Foreign Minister Carl Enckell began negotiating, with the USSR and the United Kingdom, the final terms of the Moscow Armistice, which eventually included additional stipulations from the Soviets. They were presented by Molotov on 18 September and accepted by the Finnish Parliament a day later.

The motivations for the Soviet peace agreement with Finland are debated. Several Western historians stated that the original Soviet designs for Finland were no different from their designs for the Baltic countries. American political scientist Dan Reiter asserted that for Moscow, the control of Finland was necessary. Reiter and British historian Victor Rothwell both quoted Molotov telling his Lithuanian counterpart in 1940, when the USSR effectively annexed Lithuania, that minor states such as Finland, "will be included within the honourable family of Soviet peoples." Reiter stated that concern over severe losses pushed Stalin into accepting a limited outcome in the war rather than pursuing annexation, although some Soviet documents called for military occupation of Finland. He also wrote that Stalin had described territorial concessions, reparations and military bases as his objective with Finland to representatives from the UK, in December 1941, and the US, in March 1943, as well as the Tehran Conference. He believed that in the end "Stalin's desire to crush Hitler quickly and decisively without distraction from the Finnish sideshow" concluded the war.

Russian historian Nikolai Baryshnikov disputed the view that the Soviet Union sought to deprive Finland of its independence. He argued that there is no documentary evidence for such claims and that the Soviet government was always open for negotiations. Baryshnikov cited, for example, the then-public-information chief of Finnish Headquarters, Major Kalle Lehmus, to show that Finnish leadership had learned of the limited Soviet plans for Finland by at least July 1944 after intelligence revealed that some Soviet divisions were to be transferred to reserve in Leningrad. Finnish historian Heikki Ylikangas stated similar findings in 2009. According to him, the USSR refocused its efforts in the summer of 1944, from the Finnish front to defeating Germany and that Mannerheim received intelligence from Colonel Aladár Paasonen in June 1944 that the Soviet Union was aiming for peace, not occupation.

According to Finnish historians, the casualties of the Finnish Defence Forces amounted to 63,204 dead or missing and around 158,000 wounded. Officially, the Soviets captured 2,377 Finnish POWs, although Finnish researchers estimated the number to be around 3,500 prisoners. 939 Finnish civilians died in air raids and 190 civilians were killed by Soviet partisans. Germany suffered approximately 84,000 casualties in the Finnish front, 16,400 killed, 60,400 wounded and 6,800 missing. In addition to the original peace terms of restoring the 1940 border, Finland was required to pay war reparations to the USSR, conduct domestic war-responsibility trials, lease Porkkala Peninsula to the Soviets as well as ban fascist elements and allow left-wing groups, such as the Communist Party of Finland. A Soviet-led Allied Control Commission was installed to enforce and monitor the peace agreement in Finland. The requirement to disarm or expel any German troops left on Finnish soil by 15 September 1944 eventually escalated into the Lapland War between Finland and Germany and the evacuation of the 200,000-strong 20th Mountain Army to Norway.

The Soviet demand for $600 million in war indemnities was reduced to $300 million (equivalent to $ million in ), most likely due to pressure from the US and the UK. After the ceasefire, the USSR insisted that the payments should be based on 1938 prices, which doubled the de facto amount. The temporary Moscow Armistice was finalised without changes later in the Paris Peace Treaties, 1947. Henrik Lunde noted that Finland survived World War II without losing its independence—unlike many of Germany's allies. Likewise, Helsinki, along with Moscow, was the only capital of a World War II combatant nation that was not occupied in continental Europe. In the longer term, Peter Provis analysed that by following self-censorship and limited appeasement policies as well as by fulfilling the USSR's demands, Finland avoided the fate of other nations that were annexed by the Soviets.

Many civilians who had been displaced after the Winter War had moved back into Karelia during the Continuation War and now had to be evacuated from Karelia again. Of the 260,000 civilians who had moved back into the Karelia, only 19 chose to remain and become Soviet citizens. Most of the Ingrian Finns together with Votes and Izhorians living in German-occupied Ingria had been evacuated to Finland in 1943–1944. After the armistice, Finland was forced to return the evacuees. Soviet authorities did not allow the 55,733 returnees to resettle in Ingria and instead deported the Ingrian Finns to central regions of the USSR.

The war is considered a Soviet victory. According to Finnish historians, Soviet casualties in the Continuation War were not accurately recorded and various approximations have arisen. Russian historian Grigori Krivosheev estimated in 1997 that around 250,000 were killed or missing in action while 575,000 were medical casualties (385,000 wounded and 190,000 sick). Finnish author Nenye and others stated in 2016 that at least 305,000 were confirmed dead, or missing, according to the latest research and the number of wounded certainly exceeded 500,000. The number of Soviet prisoners of war in Finland was estimated by Finnish historians to be around 64,000, 56,000 of whom were captured in 1941. Around 2,600 to 2,800 Soviet prisoners of war were rendered to Germany in exchange for roughly 2,200 Finnic prisoners of war. Of the Soviet prisoners, at least 18,318 were documented to have died in Finnish prisoner of war camps. The extent of Finland's participation in the Siege of Leningrad, and whether Soviet civilian casualties during the siege should be attributed to the Continuation War, is debated and is without a clear consensus. (The estimates of civilian deaths during the siege range from 632,253 to 1,042,000).





</doc>
<doc id="7713" url="https://en.wikipedia.org/wiki?curid=7713" title="Chinese remainder theorem">
Chinese remainder theorem

The Chinese remainder theorem is a theorem of number theory, which states that if one knows the remainders of the Euclidean division of an integer by several integers, then one can determine uniquely the remainder of the division of by the product of these integers, under the condition that the divisors are pairwise coprime.

The theorem was first discovered in the 3rd century AD by the Chinese mathematician Sunzi in "Sunzi Suanjing".

The Chinese remainder theorem is widely used for computing with large integers, as it allows replacing a computation for which one knows a bound on the size of the result by several similar computations on small integers.

The Chinese remainder theorem (expressed in terms of congruences) is true over every principal ideal domain. It has been generalized to any commutative ring, with a formulation involving ideals.

The earliest known statement of the theorem, as a problem with specific numbers, appears in the 3rd-century book "Sunzi Suanjing" by the Chinese mathematician Sunzi:

Sunzi's work contains neither a proof nor a full algorithm. What amounts to an algorithm for solving this problem was described by Aryabhata (6th century). Special cases of the Chinese remainder theorem were also known to Brahmagupta (7th century), and appear in Fibonacci's Liber Abaci (1202). The result was later generalized with a complete solution called "Dayanshu" () in Qin Jiushao's 1247 "Mathematical Treatise in Nine Sections" (, "Shushu Jiuzhang").

The notion of congruences was first introduced and used by Gauss in his "Disquisitiones Arithmeticae" of 1801. Gauss illustrates the Chinese remainder theorem on a problem involving calendars, namely, "to find the years that have a certain period number with respect to the solar and lunar cycle and the Roman indiction." Gauss introduces a procedure for solving the problem that had already been used by Euler but was in fact an ancient method that had appeared several times.

Let be integers greater than 1, which are often called "moduli" or "divisors". Let us denote by the product of the .

The Chinese remainder theorem asserts that if the are pairwise coprime, and if are integers such that for every , then there is one and only one integer , such that and the remainder of the Euclidean division of by is for every .

This may be restated as follows in term of congruences:
If the are pairwise coprime, and if are any integers, then there exists an integer such that

and any two such are congruent modulo .

In abstract algebra, the theorem is often restated as: if the are pairwise coprime, the map
defines a ring isomorphism

between the ring of integers modulo and the direct product of the rings of integers modulo the . This means that for doing a sequence of arithmetic operations in formula_4 one may do the same computation independently in each formula_5 and then get the result by applying the isomorphism (from the right to the left). This may be much faster than the direct computation if and the number of operations are large. This is widely used, under the name "multi-modular computation", for linear algebra over the integers or the rational numbers.

The theorem can also be restated in the language of combinatorics as the fact that the infinite arithmetic progressions of integers form a Helly family.

The existence and the uniqueness of the solution may be proven independently. However, the first proof of existence, given below, uses this uniqueness.

Suppose that and are both solutions to all the congruences. As and give the same remainder, when divided by , their difference is a multiple of each . As the are pairwise coprime, their product divides also , and thus and are congruent modulo . If and are supposed to be non negative and less than (as in the first statement of the theorem), then their difference may be a multiple of only if .

The map
maps congruence classes modulo to sequences of congruence classes modulo . The proof of uniqueness shows that this map is injective. As the domain and the codomain of this map have the same number of elements, the map is also surjective, which proves the existence of the solution.

This proof is very simple but does not provide any direct way for computing a solution. Moreover, it cannot be generalized to other situations where the following proof can.

Existence may be established by an explicit construction of . This construction may be split into two steps, firstly by solving the problem in the case of two moduli, and the second one by extending this solution to the general case by induction on the number of moduli.

We want to solve the system:
where formula_8 and formula_9 are coprime.

Bézout's identity asserts the existence of two integers formula_10 and formula_11 such that 
The integers formula_10 and formula_11 may be computed by the extended Euclidean algorithm.

A solution is given by
Indeed, 
implying that formula_17 The second congruence is proved similarly.

Let us consider a sequence of congruence equations:
where the formula_19 are pairwise coprime. The two first equations have a solution formula_20 provided by the method of the previous section. The set of the solutions of these two first equations is the set of all solutions of the equation

As the other formula_19 are coprime with formula_23 this reduces solving the initial problem of equations to a similar problem with formula_24 equations. Iterating the process, one gets eventually the solutions of the initial problem.

For constructing a solution, it is not necessary to make an induction on the number of moduli. However, such a direct construction involves more computation with large numbers, which makes it less efficient and less used. Nevertheless, Lagrange interpolation is a special case of this construction, applied to polynomials instead of integers.

Let formula_25 be the product of all moduli but one. As the formula_19 are pairwise coprime, formula_27 and formula_19 are coprime. Thus Bézout's identity applies, and there exist integers formula_29 and formula_30 such that

A solution of the system of congruences is
In fact, as formula_33 is a multiple of formula_19 for formula_35
we have
for every formula_37

Let us consider a system of congruences:
where the formula_19 are pairwise coprime, and let formula_40 In this section several methods are described for computing the unique solution for formula_41, such that formula_42 and these methods are applied on the example:

It is easy to check whether a value of is a solution: it suffices to compute the remainder of the Euclidean division of by each . Thus, to find the solution, it suffices to check successively the integers from to until finding the solution.

Although very simple this method is very inefficient: for the simple example considered here, integers (including ) have to be checked for finding the solution . This is an exponential time algorithm, as the size of the input is, up to a constant factor, the number of digits of , and the average number of operations is of the order of .

Therefore, this method is rarely used, for hand-written computation as well on computers.

The search of the solution may be made dramatically faster by sieving. For this method, we suppose, without loss of generality, that formula_44 (if it were not the case, it would suffice to replace each formula_45 by the remainder of its division by formula_19). This implies that the solution belongs to the arithmetic progression
By testing the values of these numbers modulo formula_48 one eventually finds a solution formula_49 of the two first congruences. Then the solution belongs to the arithmetic progression 
Testing the values of these numbers modulo formula_51, and continuing until every modulus has been tested gives eventually the solution.

This method is faster if the moduli have been ordered by decreasing value, that is if formula_52 For the example, this gives the following computation. We consider first the numbers that are congruent to 4 modulo 5 (the largest modulus), which are 4, , , ... For each of them, compute the remainder by 4 (the second largest modulus) until getting a number congruent to 3 modulo 4. Then one can proceed by adding at each step, and computing only the remainders by 3. This gives

This method works well for hand-written computation with a product of moduli that is not too big. However, it is much slower than other methods, for very large products of moduli. Although dramatically faster than the systematic search, this method also has an exponential time complexity and is therefore not used on computers.

The constructive existence proof shows that, in the case of two moduli, the solution may be obtained by the computation of the Bézout coefficients of the moduli, followed by a few multiplications, additions and reductions modulo formula_53 (for getting a result in the interval formula_54). As the Bézout's coefficients may be computed with the extended Euclidean algorithm, the whole computation, at most, has a quadratic time complexity of formula_55 where formula_56 denotes the number of digits of formula_57

For more than two moduli, the method for two moduli allows the replacement of any two congruences by a single congruence modulo the product of the moduli. Iterating this process provides eventually the solution with a complexity, which is quadratic in the number of digits of the product of all moduli. This quadratic time complexity does not depend on the order in which the moduli are regrouped. One may regroup the two first moduli, then regroup the resulting modulus with the next one, and so on. This strategy is the easiest to implement, but it also requires more computation involving large numbers.

Another strategy consists in partitioning the moduli in pairs whose product have comparable sizes (as much as possible), applying, in parallel, the method of two moduli to each pair, and iterating with a number of moduli approximatively divided by two. This method allows an easy parallelization of the algorithm. Also, if fast algorithms (that is algorithms working in quasilinear time) are used for the basic operations, this method provides an algorithm for the whole computation that works in quasilinear time.

On the current example (which has only three moduli), both strategies are identical and work as follows.

Bézout's identity for 3 and 4 is
Putting this in the formula given for proving the existence gives 
for a solution of the two first congruences, the other solutions being obtained by adding to −9 any multiple of . One may continue with any of these solutions, but the solution is smaller (in absolute value) and thus leads probably to an easier computation

Bézout identity for 5 and 3×4 = 12 is
Applying the same formula again, we get a solution of the problem:
The other solutions are obtained by adding any multiple of , and the smallest positive solution is .

The system of congruences solved by the Chinese remainder theorem may be rewritten as a system of simultaneous linear Diophantine equations:
where the unknown integers are formula_41 and the formula_64 Therefore, every general method for solving such systems may be used for finding the solution of Chinese remainder theorem, such as the reduction of the matrix of the system to Smith normal form or Hermite normal form. However, as usual when using a general algorithm for a more specific problem, this approach is less efficient than the method of the preceding section, based on a direct use of Bézout's identity.

In , the Chinese remainder theorem has been stated in three different ways: in terms of remainders, of congruences, and of a ring isomorphism. The statement in terms of remainders does not apply, in general, to principal ideal domains, as remainders are not defined in such rings. However, the two other versions make sense over a principal ideal domain : it suffices to replace "integer" by "element of the domain" and formula_65 by . These two versions of the theorem are true in this context, because the proofs (except for the first existence proof), are based on Euclid's lemma and Bézout's identity, which are true over every principal domain.

However, in general, the theorem is only an existence theorem and does not provide any way for computing the solution, unless one has an algorithm for computing the coefficients of Bézout's identity.

The statement in terms of remainders given in cannot be generalized to any principal ideal domain, but its generalization to Euclidean domains is straightforward. The univariate polynomials over a field is the typical example of a Euclidean domain, which is not the integers. Therefore, we state the theorem for the case of a ring of univariate domain formula_66 over a field formula_67 For getting the theorem for a general Euclidean domain, it suffices to replace the degree by the Euclidean function of the Euclidean domain.

The Chinese remainder theorem for polynomials is thus: Let formula_68 (the moduli) be, for , pairwise coprime polynomials in formula_66. Let formula_70 be the degree of formula_68, and formula_72 be the sum of the formula_73
If formula_74 are polynomials such that formula_75 or formula_76 for every , then, there is one and only one polynomial formula_77, such that formula_78 and the remainder of the Euclidean division of formula_77 by formula_68 is formula_81 for every .

The construction of the solution may be done as in or . However, the latter construction may be simplified by using, as follows, partial fraction decomposition instead of extended Euclidean algorithm.

Thus, we want to find a polynomial formula_77, which satisfies the congruences
for formula_84

Let us consider the polynomials

The partial fraction decomposition of formula_86 gives polynomials formula_87 with degrees formula_88 such that
and thus

Then a solution of the simultaneous congruence system is given by the polynomial

In fact, we have
for formula_93

This solution may have a degree larger that formula_94 The unique solution of degree less than formula_72 may be deduced by considering the remainder formula_96 of the Euclidean division of formula_97 by formula_98 This solution is 

A special case of Chinese remainder theorem for polynomials is Lagrange interpolation. For this, let us consider monic polynomials of degree one:

They are pairwise coprime if the formula_101 are all different. The remainder of the division by formula_68 of a polynomial formula_77 is formula_104

Now, let formula_105 be constants (polynomials of degree 0) in formula_67 Both Lagrange interpolation and Chinese remainder theorem assert the existence of a unique polynomial formula_107 of degree less than formula_108 such that

for every formula_37

Lagrange interpolation formula is exactly the result, in this case, of the above construction of the solution. More precisely, let 

The partial fraction decomposition of formula_112 is 

In fact, reducing the right-hand side to a common denominator one gets 

and the numerator is equal to one, as being a polynomial of degree less than formula_115 which takes the value one for formula_108 different values of formula_117

Using the above general formula, we get the Lagrange interpolation formula:

Hermite interpolation is an application of Chinese remainder theorem for univariate polynomials, which may involve moduli of arbitrary degrees (Lagrange interpolation involves only moduli of degree one).

The problem consists of finding a polynomial of the least possible degree, such that the polynomial and its first derivatives take given values at some fixed points.

More precisely, let formula_119 be formula_108 elements of the ground field formula_121 and, for formula_122 let formula_123 be the values of the first formula_124 derivatives of the sought polynomial at formula_101 (including the 0th derivative, which is the value of the polynomial itself). The problem is to find a polynomial formula_77 such that its "j"th derivative takes the value formula_127 at formula_128 for formula_129 and formula_130

Let us consider the polynomial
This is the Taylor polynomial of order formula_132 at formula_101, of the unknown polynomial formula_134 Therefore, we must have

Conversely, any polynomial formula_136 that satisfies these formula_108 congruences, in particular verifies, for any formula_138
therefore formula_68 is its Taylor polynomial of order formula_141 at formula_101, that is, formula_77 solves the initial Hermite interpolation problem.
The Chinese remainder theorem asserts that there exists exactly one polynomial of degree less than the sum of the formula_144 which satisfies these formula_108 congruences.

There are several ways for computing the solution formula_134 One may use the method described at the beginning of . One may also use the constructions given in or .

The Chinese remainder theorem can be generalized to non-coprime moduli. Let formula_147 be any integers, let formula_148, and consider the system of congruences:
If formula_150, then this system of equations has a unique solution modulo formula_151. Otherwise, it has no solutions.

If we use Bézout's identity to write formula_152, then the solution is
This defines an integer, as divides both and . Otherwise, the proof is very similar to that for coprime moduli.

The Chinese remainder theorem can be generalized to any ring, by using coprime ideals (also called comaximal ideals). Two ideals and are coprime if there are elements formula_154 and formula_155 such that formula_156 This relation plays the role of Bézout's identity in the proofs related to this generalization, which, otherwise are very similar. The generalization may be stated as follows.

Let be two-sided ideals of a ring formula_157 that are pairwise coprime, and be their intersection. Then we have the isomorphism: 
between the quotient ring formula_159 and the direct product of the formula_160
where "formula_161" denotes the image of the element formula_41 in the quotient ring defined by the ideal formula_163
Moreover, if formula_157 is commutative, then the ideal intersection is equal to the product of the ideals formula_165

The Chinese remainder theorem has been used to construct a Gödel numbering for sequences, which is involved in the proof of Gödel's incompleteness theorems.

The prime-factor FFT algorithm (also called Good-Thomas algorithm) uses the Chinese remainder theorem for reducing the computation of a fast Fourier transform of size formula_53 to the computation of two fast Fourier transforms of smaller sizes formula_8 and formula_9 (providing that formula_8 and formula_9 are coprime).

Most implementations of RSA use the Chinese remainder theorem during signing of HTTPS certificates and during decryption.

The Chinese remainder theorem can also be used in secret sharing, which consists of distributing a set of shares among a group of people who, all together (but no one alone), can recover a certain secret from the given set of shares. Each of the shares is represented in a congruence, and the solution of the system of congruences using the Chinese remainder theorem is the secret to be recovered. Secret sharing using the Chinese remainder theorem uses, along with the Chinese remainder theorem, special sequences of integers that guarantee the impossibility of recovering the secret from a set of shares with less than a certain cardinality.

The range ambiguity resolution techniques used with medium pulse repetition frequency radar can be seen as a special case of the Chinese remainder theorem.

Dedekind's theorem on the linear independence of characters. Let be a monoid and an integral domain, viewed as a monoid by considering the multiplication on . Then any finite family of distinct monoid homomorphisms is linearly independent. In other words, every family of elements satisfying 

must be equal to the family .

Proof. First assume that is a field, otherwise, replace the integral domain by its quotient field, and nothing will change. We can linearly extend the monoid homomorphisms to -algebra homomorphisms , where is the monoid ring of over . Then, by linearity, the condition

yields

Next, for the two -linear maps and are not proportional to each other. Otherwise and would also be proportional, and thus equal since as monoid homomorphisms they satisfy: , which contradicts the assumption that they are distinct.

Therefore, the kernels and are distinct. Since is a field, is a maximal ideal of for every . Because they are distinct and maximal the ideals and are coprime whenever . The Chinese Remainder Theorem (for general rings) yields an isomorphism:

where

Consequently, the map

is surjective. Under the isomorphisms the map corresponds to:

Now,

yields

for every vector in the image of the map . Since is surjective, this means that

for every vector

Consequently, . QED.






</doc>
<doc id="7716" url="https://en.wikipedia.org/wiki?curid=7716" title="Cyril M. Kornbluth">
Cyril M. Kornbluth

Cyril M. Kornbluth (July 2, 1923 – March 21, 1958) was an American science fiction author and a member of the Futurians. He used a variety of pen-names, including Cecil Corwin, S. D. Gottesman, Edward J. Bellin, Kenneth Falconer, Walter C. Davies, Simon Eisner, Jordan Park, Arthur Cooke, Paul Dennis Lavond, and Scott Mariner. The "M" in Kornbluth's name may have been in tribute to his wife, Mary Byers; Kornbluth's colleague and collaborator Frederik Pohl confirmed Kornbluth's lack of any actual middle name in at least one interview.

Kornbluth was born and grew up in the uptown Manhattan neighborhood of Inwood, in New York City. He was of Polish Jewish descent, the son of a "second-generation [American] Jew" who ran his own tailor shop. According to his widow, Kornbluth was a "precocious child", learning to read by the age of three and writing his own stories by the time he was seven. He graduated from high school at thirteen, received a CCNY scholarship at fourteen, and was "thrown out for leading a student strike" before graduating.

As a teenager, he became a member of the Futurians, an influential group of science fiction fans and writers. While a member of the Futurians, he met and became friends with Frederik Pohl, Donald A. Wollheim, Robert A. W. Lowndes, and his future wife Mary Byers. He also participated in the Fantasy Amateur Press Association.

Kornbluth served in the US Army during World War II (European 'Theatre'). He received a Bronze Star for his service in the Battle of the Bulge, where he served as a member of a heavy machine gun crew. Upon his discharge, he returned to finish his education, which had been interrupted by the war, at the University of Chicago. While living in Chicago he also worked at Trans-Radio Press, a news wire service. In 1951 he started writing full-time, returning to the East Coast where he collaborated on novels with his old Futurian friends Frederik Pohl and Judith Merril.

Kornbluth began writing at 15. His first solo story, "The Rocket of 1955", was published in Richard Wilson's fanzine "Escape" (Vol. 1, No 2, August 1939); his first collaboration, "Stepsons of Mars," written with Richard Wilson and published under the name "Ivar Towers", appeared in the April 1940 "Astonishing". His other short fiction includes "The Little Black Bag", "The Marching Morons", "The Altar at Midnight", "MS. Found in a Chinese Fortune Cookie", "Gomez" and "The Advent on Channel 12".

"The Little Black Bag" was first adapted for television live on the television show "Tales of Tomorrow" on May 30, 1952. It was later adapted for television by the BBC in 1969 for its "Out of the Unknown" series. In 1970, the same story was adapted by Rod Serling for an episode of his "Night Gallery" series. This dramatization starred Burgess Meredith as the alcoholic Dr. William Fall, who had long lost his doctor's license and become a homeless alcoholic. He finds a bag containing advanced medical technology from the future (2098), which, after an unsuccessful attempt to pawn it, he uses benevolently.

"The Marching Morons" is a look at a far future in which the world's population consists of five billion idiots and a few million geniuses – the precarious minority of the "elite" working desperately to keep things running behind the scenes. In his introduction to "The Best of C.M. Kornbluth", Pohl states that "The Marching Morons" is a direct sequel to "The Little Black Bag": it is easy to miss this, as "Bag" is set in the contemporary present while "Morons" takes place several centuries from now, and there is no character who appears in both stories. The titular black bag in the first story is actually an artifact from the time period of "The Marching Morons": a medical kit filled with self-driven instruments enabling a far-future moron to "play doctor". A future Earth similar to "The Marching Morons" – a civilisation of morons protected by a small minority of hidden geniuses – is used again in the final stages of Kornbluth & Pohl's "Search the Sky".

"MS. Found in a Chinese Fortune Cookie" (1957) is supposedly written by Kornbluth using notes by "Cecil Corwin", who has been declared insane and incarcerated, and who smuggles out in fortune cookies the ultimate secret of life. This fate is said to be Kornbluth's response to the unauthorized publication of "Mask of Demeter" (as by "Corwin" and "Martin Pearson" (Donald A. Wollheim)) in Wollheim's anthology "Prize Science Fiction" in 1953.

Biographer Mark Rich describes the 1958 story "Two Dooms" as one of several stories which are "concern[ed] with the ethics of theoretical science" and which "explore moral quandaries of the atomic age":

Many of Kornbluth's novels were written as collaborations: either with Judith Merril (using the pseudonym Cyril Judd), or with Frederik Pohl. These include "Gladiator-At-Law" and "The Space Merchants". "The Space Merchants" contributed significantly to the maturing and to the wider academic respectability of the science fiction genre, not only in America but also in Europe. Kornbluth also wrote several novels under his own name, including "The Syndic" and "Not This August".

Kornbluth died at age 34 in Levittown, New York. Scheduled to meet with Bob Mills in New York City to interview for the position of editor of "The Magazine of Fantasy & Science Fiction", Kornbluth had to shovel snow from his driveway, which delayed him. Running to meet his train, he suffered a fatal heart attack on the platform of the station.

A number of short stories remained unfinished at Kornbluth's death; these were eventually completed and published by Pohl. One of these stories, "The Meeting" ("The Magazine of Fantasy & Science Fiction", November 1972), was the co-winner of the 1973 Hugo Award for Best Short Story; it tied with R. A. Lafferty's "Eurema's Dam." Almost all of Kornbluth's solo SF stories have been collected as "His Share of Glory: The Complete Short Science Fiction of C. M. Kornbluth" (NESFA Press, 1997).

Frederik Pohl, in his autobiography "The Way the Future Was", Damon Knight, in his memoir "The Futurians", and Isaac Asimov, in his memoirs "In Memory Yet Green" and "I. Asimov: A Memoir", all give descriptions of Kornbluth as a man of odd personal habits and eccentricities.

Kornbluth, for example, decided to educate himself by reading his way through an entire encyclopedia from A to Z; in the course of this effort, he acquired a great deal of esoteric knowledge that found its way into his stories, in alphabetical order by subject. When Kornbluth wrote a story that mentioned the "ballista", an Ancient Roman weapon, Pohl knew that Kornbluth had finished the A's and had started on the B's.

According to Pohl, Kornbluth never brushed his teeth, and they were literally green. Deeply embarrassed by this, Kornbluth developed the habit of holding his hand in front of his mouth when speaking.

Kornbluth disliked black coffee, but felt obliged to acquire a taste for it because he believed that professional authors were "supposed to" drink black coffee. He trained himself by putting gradually less cream into each cup of coffee he drank, until he eventually "weaned himself" (Knight's description) and switched to black coffee.



Spider Robinson praised this collection, saying "I haven't enjoyed a book so much in years." Mark Rich wrote, "Critics judging Kornbluth by this anthology, edited by Pohl, have seen a growing bitterness in his later stories. This reflects editorial choice more than reality, because Kornbluth also wrote delightful humor in his last years, in stories not collected here. These tales demonstrate Kornbluth's effective use of everyday individuals from a variety of ethnic backgrounds as well as his well-tuned ear for dialect."




Kornbluth's name is mentioned in Lemony Snicket's "Series of Unfortunate Events" as a member of V.F.D., a secret organization dedicated to the promotion of literacy, classical learning, and crime prevention.




</doc>
<doc id="7720" url="https://en.wikipedia.org/wiki?curid=7720" title="Coprophagia">
Coprophagia

Coprophagia () or coprophagy () is the consumption of feces. The word is derived from the Greek κόπρος "copros", "feces" and φαγεῖν "phagein", "to eat". "Coprophagy" refers to many kinds of feces-eating, including eating feces of other species (heterospecifics), of other individuals (allocoprophagy), or one's own (autocoprophagy) – those once deposited or taken directly from the anus.

In humans, coprophagia has been observed in individuals with mental illnesses, as well as an unconventional sexual act. Some animal species eat feces as a normal behavior, in particular in the Lagomorpha (rabbits, hares, and pikas) doing so to allow tough plant materials to be digested more efficiently by passing twice through the digestive tract. Other species may do so only under unusual conditions.

Coprophagia has been observed in some people with schizophrenia and pica.

Coprophagous insects consume and redigest the feces of large animals. These feces contain substantial amounts of semi-digested food, particularly in the case of herbivores, owing to the inefficiency of the large animals' digestive systems. Two feces-eating insects are certain species of fly and the dung beetle. In regards to the dung beetle, their diet is mainly in the form of the microorganism-rich liquid component that is present in the dung of mammals, while they take advantage of the fibrous material in the dung for the female to lay her fertilized eggs in.

Termites eat one another's feces as a means of obtaining their hindgut protists. Termites and protists have a symbiotic relationship (e.g. with the protozoan that allows the termites to digest the cellulose in their diet). For example, in one group of termites, there is a three-way symbiotic relationship: termites of the family Rhinotermitidae, cellulolytic protists of the genus "Pseudotrichonympha" in the guts of these termites, and intracellular bacterial symbionts of the protists.

Domesticated and wild mammals are sometimes coprophagic, and in some species this forms an essential part of their method of digesting tough plant material.

Dogs may be coprophagic, possibly to rebalance their microbiome or to ingest missing nutrients.

Species within the Lagomorpha (rabbits, hares, and pikas) produce two types of fecal pellets: hard ones, and soft ones called cecotropes. Animals in these species reingest their cecotropes, to extract further nutrients. Cecotropes derive from chewed plant material that collects in the cecum, a chamber between the large and small intestine, containing large quantities of symbiotic bacteria that help with the digestion of cellulose and also produce certain B vitamins. After excretion of the soft cecotrope, it is again eaten whole by the animal and redigested in a special part of the stomach. The pellets remain intact for up to six hours in the stomach; the bacteria within continue to digest the plant carbohydrates. This double-digestion process enables these animals to use nutrients that they may have missed during the first passage through the gut, as well as the nutrients formed by the microbial activity and thus ensures that maximum nutrients are derived from the food they eat. This process serves the same purpose within these animals as rumination (cud-chewing) does in cattle and sheep. 

Cattle in the United States are often fed chicken litter. There are concerns that the practice of feeding chicken litter to cattle could lead to bovine spongiform encephalopathy (mad-cow disease) because of the crushed bone meal in chicken feed. The U.S. Food and Drug Administration regulates this practice by attempting to prevent the introduction of any part of a cow's brain or spinal cord into livestock feed. Other countries, like Canada, have banned chicken litter for use as a livestock feed.

The young of elephants, giant pandas, koalas and hippos eat the feces of their mothers or other animals in the herd, in order to obtain the bacteria required to properly digest vegetation found in their ecosystems. When such animals are born, their intestines are sterile and do not contain these bacteria. Without doing this they would be unable to obtain any nutritional value from plants.

Hamsters, guinea pigs, chinchillas and naked mole-rat eat their own droppings, which are thought to be a source of vitamins B and K, produced by gut bacteria. Gorillas have been recorded to consume their feces extremely rarely, possibly out of boredom, a desire for warm food, or to reingest seeds contained in the feces.

Pigs sometimes eat the feces of herbivores that leave a significant amount of semi-digested matter, including their own. In some cultures it was common for poor families to collect horse feces to feed their pigs, which contributes to the risk of parasite infection. The pig toilet is an application of porcine coprophagy to human sanitation.

Some carnivorous plants, such as pitcher plants of the genus "Nepenthes", obtain nourishment from the feces of commensal animals.

Lewin reported that "... consumption of fresh, warm camel feces has been recommended by Bedouins as a remedy for bacterial dysentery; its efficacy (probably attributable to the antibiotic subtilisin from "Bacillus subtilis") was anecdotally confirmed by German soldiers in Africa during World War II".

Centuries ago, physicians tasted their patients' feces, to better judge their state and condition.

Coprophagia is sometimes depicted in pornography, usually under the term "scat" (from "scatology").

"The 120 Days of Sodom", a novel by the Marquis de Sade written in 1785, is full of detailed descriptions of erotic sadomasochistic coprophagia. Thomas Pynchon's award-winning 1973 novel "Gravity's Rainbow" contains a detailed scene of coprophagia. François Rabelais, in his classic "Gargantua and Pantagruel", often employs the expression "mâche-merde" or "mâchemerde", meaning "shit-chewer". This in turn comes from the Greek comedians Aristophanes and particularly Menander, who often use the term skatophagos ("σκατοϕφάγος"). The Austrian actor and pornographic director created the series "Avantgarde Extreme" and "Portrait Extrem", which explores coprophagy, coprophilia and urolagnia. Modern Russian writer Vladimir Sorokin's novel "Norma" describes a society where coprophagia is institutionalised and mandatory.




</doc>
<doc id="7721" url="https://en.wikipedia.org/wiki?curid=7721" title="C. L. Moore">
C. L. Moore

Catherine Lucille Moore (January 24, 1911 – April 4, 1987) was an American science fiction and fantasy writer, who first came to prominence in the 1930s writing as C. L. Moore. She was among the first women to write in the science fiction and fantasy genres, though earlier woman writers in these genres include Clare Winger Harris, Greye La Spina, and Francis Stevens, amongst others. Nevertheless, Moore's work paved the way for many other female speculative fiction writers. 

Moore married her first husband Henry Kuttner in 1940, and most of her work from 1940-1958 (Kuttner's death) was written by the couple collaboratively. They were prolific co-authors under their own names, although more often under any one of several pseudonyms.

As "Catherine Kuttner", she had a brief career as a television scriptwriter from 1958 to 1962. She retired from writing in 1963. 

Moore was born on January 24, 1911 in Indianapolis, Indiana. She was chronically ill as a child and spent much of her time reading literature of the fantastic. She left college during the Great Depression to work as a secretary at the Fletcher Trust Company in Indianapolis.

"The Vagabond", a student-run magazine at Indiana University, published three of her stories when she was a student there. The three short stories, all with a fantasy theme and all credited to "Catherine Moore", appeared in 1930/31. Her first professional sales appeared in pulp magazines beginning in 1933. Her decision to publish under the name "C.L. Moore" stemmed not from a desire to hide her gender, but to keep her employers at Fletcher Trust from knowing that she was working as a writer on the side.

Her early work included two significant series in "Weird Tales", then edited by Farnsworth Wright. One features the rogue and adventurer Northwest Smith wandering through the Solar System; the other features the swordswoman/warrior Jirel of Joiry, one of the first female protagonists in sword-and-sorcery fiction. Both series are sometimes named for their lead characters. One of the Northwest Smith stories, "Nymph of Darkness" ("Fantasy Magazine" (April 1935); expurgated version, "Weird Tales" (Dec 1939)), was written in collaboration with Forrest J Ackerman.

The most famous Northwest Smith story is "Shambleau", which was also Moore's first professional sale. It originally appeared in the November 1933 issue of "Weird Tales", netting her $100, and later becoming a popular anthology reprint. Her most famous Jirel story is also the first one, "Black God's Kiss", which was the cover story in the October 1934 issue of "Weird Tales", subtitled "the weirdest story ever told" (see figure). Moore's early stories were notable for their emphasis on the senses and emotions, which was unusual in genre fiction at the time.

Moore's work also appeared in "Astounding Science Fiction" magazine throughout the 1940s. Several stories written for that magazine were later collected in her first published book, "Judgment Night" (1952) One of them, the novella "No Woman Born" (1944), was to be included in more than 10 different science fiction anthologies including "The Best of C. L. Moore".

Included in that collection were "Judgment Night" (first published in August and September 1943), the lush rendering of a future galactic empire with a sober meditation on the nature of power and its inevitable loss; "The Code" (July 1945), an homage to the classic Faust with modern theories and Lovecraftian dread; "Promised Land" (February 1950) and "Heir Apparent" (July 1950), both documenting the grim twisting that mankind must undergo in order to spread into the Solar System; and "Paradise Street" (September 1950), a futuristic take on the Old West conflict between lone hunter and wilderness-taming settlers.

Moore met Henry Kuttner, also a science fiction writer, in 1936 when he wrote her a fan letter under the impression that "C. L. Moore" was a man. They soon collaborated on a story that combined Moore’s signature characters, Northwest Smith and Jirel of Joiry: "Quest of the Starstone" (1937).

Moore and Kuttner married in 1940 and thereafter wrote many of their stories in collaboration, sometimes under their own names, but more often using the joint pseudonyms C. H. Liddell, Lawrence O'Donnell, or Lewis Padgett — most commonly the latter, a combination of their mothers' maiden names. Moore still occasionally wrote solo work during this period, including the frequently anthologized "No Woman Born" (1944). A selection of Moore's solo short fiction work from 1942 through 1950 was collected in 1952's "Judgement Night". Moore's only solo novel, "Doomsday Morning" appeared in 1957.

The vast majority of Moore's work in the period, though, was written as part of a very prolific partnership. Working together, the couple managed to combine Moore's style with Kuttner's more cerebral storytelling. They continued to work in sf and fantasy, and their works include two frequently anthologized sf classics: "Mimsy Were the Borogoves" (February 1943), the basis for the film "The Last Mimzy" (2007), and "Vintage Season" (September 1946), the basis for the film "Timescape" (1992). As "Lewis Padgett" they also penned two mystery novels: "The Brass Ring" (1946) and "The Day He Died" (1947).

After Kuttner's death in 1958, Moore continued teaching her writing course at the University of Southern California but permanently retired from writing any further literary fiction. Instead, working as "Catherine Kuttner", she carved out a short-lived career as a scriptwriter for Warner Brothers television, writing episodes of the westerns "Sugarfoot", "Maverick", and "The Alaskans", as well as the detective series "77 Sunset Strip", all between 1958 and 1962. However, upon marrying Thomas Reggie (who was not a writer) in 1963, she ceased writing entirely.

Moore was the author guest of honor at Kansas City, MO's fantasy and science fiction convention BYOB-Con 6, held over the U. S. Memorial Day weekend in May, 1976.

In 1981, Moore received two annual awards for her career in fantasy literature: the World Fantasy Award for Life Achievement, chosen by a panel of judges at the World Fantasy Convention, and the Gandalf Grand Master Award, chosen by vote of participants in the World Science Fiction Convention. (Thus she became the eighth and final Grand Master of Fantasy, sponsored by the Swordsmen and Sorcerers' Guild of America, in partial analogy to the Grand Master of Science Fiction sponsored by the Science Fiction Writers of America.)

Moore was an active member of the Tom and Terri Pinckard Science Fiction literary salon and a frequent contributor to literary discussions with the regular membership, including Robert Bloch, George Clayton Johnson, Larry Niven, Jerry Pournelle, Norman Spinrad, A. E. van Vogt, and others, as well as many visiting writers and speakers.

She developed Alzheimer's disease but that was not obvious for several years. She had ceased to attend the meetings when she was nominated to be the first woman Grand Master of the Science Fiction Writers of America; the nomination was withdrawn at the request of her husband, Thomas Reggie, who said the award and ceremony would be at best confusing and likely upsetting to her, given the progress of her disease. That caused dismay among the former SFWA presidents, for she was a great favorite to receive the award. (Former presidents and current officers select a living writer as Grand Master of SF, no more than one annually.)

Moore died on April 4, 1987 at her home in Hollywood, California after a long battle with Alzheimer's.

The Science Fiction and Fantasy Hall of Fame inducted Moore in 1998, its third class of two deceased and two living writers.


Bleiler, E.F. "Fantasy, Horror...and Sex: The Early Stories of C.L. Moore". "Scream Factory" (1988): 41-47



</doc>
<doc id="7722" url="https://en.wikipedia.org/wiki?curid=7722" title="Compactron">
Compactron

Compactrons are a type of thermionic valve, or vacuum tube, which contain multiple electrode structures packed into a single enclosure. They were designed to compete with early transistor electronics and were used in televisions, radios, and similar roles.

The Compactron was a trade name applied to multi-electrode structure tubes specifically constructed on a 12-pin Duodecar base. This vacuum tube family was introduced in 1961 by General Electric in Owensboro, Kentucky to compete with transistorized electronics during the solid state transition. Television sets were a primary application. The idea of multi-electrode tubes itself was far from new and indeed the Loewe company of Germany was producing multi-electrode tubes as far back as 1926, and they even included all of the required passive components as well.

Use was prevalent in televisions because transistors were slow to achieve the high power and frequency capabilities needed particularly in color television sets. The first portable color television, the General Electric Porta-Color, was designed using 13 tubes, 10 of which were Compactrons. Even before the compactron design was unveiled, nearly all tube based electronic equipment used multi-electrode tubes of one type or another. Virtually every AM/FM radio receiver of the 1950's and 60's used a 6AK8 (EABC80) tube (or equivalent) consisting of three diodes and a triode which was designed in 1954.

Compactron's integrated valve design helped lower power consumption and heat generation (they were to tubes what integrated circuits were to transistors). Compactrons were also used in a few high end Hi-Fi stereos. They were also used by the Ampeg guitar amplifier company in some of their guitar amps. No modern tube based Hi-Fi systems are known to use this tube type, as simpler and more readily available tubes have again filled this niche.

A distinguishing feature of most Compactrons is the placement of the evacuation tip on the bottom end, rather than the top end as was customary with "miniature" tubes, and a characteristic 3/4" diameter circle pin pattern.

Examples of Compactrons type types include:

Due to their specific applications in television circuits, many different Compactron types were produced. Almost all were assigned using standard US tube numbers.

Integrated circuits (of the analogue and digital type) gradually took over all of the functions that the Compactron was designed for. "Hybrid" television sets produced in the early to mid-1970s made use of a combination of tubes (typically Compactrons), transistors, and integrated circuits in the same set. By the mid-1980s this type of tube was functionally obsolete. Compactrons simply don't exist in any TV sets designed after 1986. Other specialist uses of the tube declined in parallel with the television set manufacture. Manufacture of Compactrons ceased in the early 1990s. New old stock replacements for almost all Compactron types produced are easily found for sale on the Internet.


</doc>
<doc id="7723" url="https://en.wikipedia.org/wiki?curid=7723" title="Carmichael number">
Carmichael number

In number theory, a Carmichael number is a composite number formula_1 which satisfies the modular arithmetic congruence relation:
for all integers formula_3 which are relatively prime to formula_1.

They are named for Robert Carmichael.
The Carmichael numbers are the subset "K" of the Knödel numbers.

Equivalently, a Carmichael number is a composite number formula_1 for which
for all integers formula_3.
Fermat's little theorem states that if "p" is a prime number, then for any integer "b", the number "b" − "b" is an integer multiple of "p". Carmichael numbers are composite numbers which have this property. Carmichael numbers are also called Fermat pseudoprimes or absolute Fermat pseudoprimes. A Carmichael number will pass a Fermat primality test to every base "b" relatively prime to the number, even though it is not actually prime.
This makes tests based on Fermat's Little Theorem less effective than strong probable prime tests such as the Baillie-PSW primality test and the Miller–Rabin primality test.

However, no Carmichael number is either an Euler-Jacobi pseudoprime or a strong pseudoprime to every base relatively prime to it

so, in theory, either an Euler or a strong probable prime test could prove that a Carmichael number is, in fact, composite.

As numbers become larger, Carmichael numbers become increasingly rare. For example, there are 20,138,200 Carmichael numbers between 1 and 10 (approximately one in 50 trillion (5·10) numbers).

An alternative and equivalent definition of Carmichael numbers is given by Korselt's criterion.

It follows from this theorem that all Carmichael numbers are odd, since any even composite number that is square-free (and hence has only one prime factor of two) will have at least one odd prime factor, and thus formula_13 results in an even dividing an odd, a contradiction. (The oddness of Carmichael numbers also follows from the fact that formula_14 is a Fermat witness for any even composite number.)
From the criterion it also follows that Carmichael numbers are cyclic. Additionally, it follows that there are no Carmichael numbers with exactly two prime factors.

Korselt was the first who observed the basic properties of Carmichael numbers, but he did not give any examples. In 1910, Carmichael found the first and smallest such number, 561, which explains the name "Carmichael number".

That 561 is a Carmichael number can be seen with Korselt's criterion. Indeed, formula_15 is square-free and formula_16, formula_17 and formula_18.

The next six Carmichael numbers are :

These first seven Carmichael numbers, from 561 to 8911, were all found by the Czech mathematician Václav Šimerka in 1885 (thus preceding not just Carmichael but also Korselt, although Šimerka did not find anything like Korselt's criterion). His work, however, remained unnoticed.

J. Chernick proved a theorem in 1939 which can be used to construct a subset of Carmichael numbers. The number formula_25 is a Carmichael number if its three factors are all prime. Whether this formula produces an infinite quantity of Carmichael numbers is an open question (though it is implied by Dickson's conjecture).

Paul Erdős heuristically argued there should be infinitely many Carmichael numbers. In 1994 it was shown by W. R. (Red) Alford, Andrew Granville and Carl Pomerance that there really do exist infinitely many Carmichael numbers. Specifically, they showed that for sufficiently large formula_1, there are at least formula_27 Carmichael numbers between 1 and formula_1.

Löh and Niebuhr in 1992 found some very large Carmichael numbers, including one with 1,101,518 factors and over 16 million digits.

Carmichael numbers have at least three positive prime factors. For some fixed "R", there are infinitely many Carmichael numbers with exactly "R" factors; in fact, there are infinitely many such R.

The first Carmichael numbers with formula_29 prime factors are :

The first Carmichael numbers with 4 prime factors are :

The second Carmichael number (1105) can be expressed as the sum of two squares in more ways than any smaller number. The third Carmichael number (1729) is the Hardy-Ramanujan Number: the smallest number that can be expressed as the sum of two cubes (of positive numbers) in two different ways.

Let formula_30 denote the number of Carmichael numbers less than or equal to formula_31. The distribution of Carmichael numbers by powers of 10 :

In 1953, Knödel proved the upper bound:

for some constant formula_33.

In 1956, Erdős improved the bound to

for some constant formula_35. He further gave a heuristic argument suggesting that this upper bound should be close to the true growth rate of formula_30. The table below gives approximate minimal values for the constant "k" in the Erdős bound for formula_37 as "n" grows:

In the other direction, Alford, Granville and Pomerance proved in 1994 that for sufficiently large "X",

In 2005, this bound was further improved by Harman to

who subsequently improved the exponent to formula_40.
Regarding the asymptotic distribution of Carmichael numbers, there have been several conjectures. In 1956, Erdős conjectured that there were formula_41 Carmichael numbers for "X" sufficiently large. In 1981, Pomerance sharpened Erdős' heuristic arguments to conjecture that there are

Carmichael numbers up to "X". However, inside current computational ranges (such as the counts of Carmichael numbers performed by Pinch up to 10), these conjectures are not yet borne out by the data.

The notion of Carmichael number generalizes to a Carmichael ideal in any number field "K". For any nonzero prime ideal formula_43 in formula_44, we have <math>\alpha^


</doc>
<doc id="7727" url="https://en.wikipedia.org/wiki?curid=7727" title="Controlled Substances Act">
Controlled Substances Act

The Controlled Substances Act (CSA) is the statute establishing federal U.S. drug policy under which the manufacture, importation, possession, use, and distribution of certain substances is regulated. It was passed by the 91st United States Congress as Title II of the Comprehensive Drug Abuse Prevention and Control Act of 1970 and signed into law by President Richard Nixon. The Act also served as the national implementing legislation for the Single Convention on Narcotic Drugs.

The legislation created five Schedules (classifications), with varying qualifications for a substance to be included in each. Two federal agencies, the Drug Enforcement Administration (DEA) and the Food and Drug Administration (FDA), determine which substances are added to or removed from the various schedules, although the statute passed by Congress created the initial listing. Congress has sometimes scheduled other substances through legislation such as the Hillory J. Farias and Samantha Reid Date-Rape Prevention Act of 2000, which placed gamma hydroxybutyrate (GHB) in Schedule I and sodium oxybate (the ISOLATED sodium salt in GHB) in Schedule III. Classification decisions are required to be made on criteria including potential for abuse (an undefined term), currently accepted medical use in treatment in the United States, and international treaties.

The nation first outlawed addictive drugs in the early 1900s and the International Opium Convention helped lead international agreements regulating trade. The Food and Drugs Act of 1906 was the beginning of over 200 laws concerning public health and consumer protections. Others were the Federal Food, Drug, and Cosmetic Act (1938), and the Kefauver Harris Amendment of 1962.

In 1969, President Richard Nixon announced that the Attorney General, John N. Mitchell, was preparing a comprehensive new measure to more effectively meet the narcotic and dangerous drug problems at the federal level by combining all existing federal laws into a single new statute. With the help of White House Counsel head John Dean, the Executive Director of the Shafer Commission Michael Sonnenreich, and the Director of the BNDD John Ingersoll creating and writing the legislation, Mitchell was able to present Nixon with the bill. 

The CSA not only combined existing federal drug laws and expanded their scope, but it also changed the nature of federal drug law policies and expanded Federal law enforcement pertaining to controlled substances.
Title II, Part F of the Comprehensive Drug Abuse Prevention and Control Act of 1970 established the National Commission on Marijuana and Drug Abuse—known as the Shafer Commission after its chairman, Raymond P. Shafer—to study cannabis abuse in the United States. During his presentation of the commission's First Report to Congress, Sonnenreich and Shafer recommended the decriminalization of marijuana in small amounts, with Shafer stating,
Rufus King notes that this stratagem was similar to that used by Harry Anslinger when he consolidated the previous anti-drug treaties into the Single Convention and took the opportunity to add new provisions that otherwise might have been unpalatable to the international community. According to David T. Courtwright, "the Act was part of an omnibus reform package designed to rationalize, and in some respects to liberalize, American drug policy." (Courtwright noted that the Act became, not libertarian, but instead repressionistic to the point of tyrannical, in its intent.) It eliminated mandatory minimum sentences and provided support for drug treatment and research. King notes that the rehabilitation clauses were added as a compromise to Senator Jim Hughes, who favored a moderate approach. The bill, as introduced by Senator Everett Dirksen, ran to 91 pages. While it was being drafted, the Uniform Controlled Substances Act, to be passed by state legislatures, was also being drafted by the Department of Justice; it's wording closely mirrored the Controlled Substances Act.

Since its enactment in 1970, the Act has been amended numerous times:

The Controlled Substances Act consists of 2 subchapters. Subchapter I defines Schedules I-V, lists chemicals used in the manufacture of controlled substances, and differentiates lawful and unlawful manufacturing, distribution, and possession of controlled substances, including possession of Schedule I drugs for personal use; this subchapter also specifies the dollar amounts of fines and durations of prison terms for violations. Subchapter II describes the laws for exportation and importation of controlled substances, again specifying fines and prison terms for violations.

The Drug Enforcement Administration was established in 1973, combining the Bureau of Narcotics and Dangerous Drugs (BNDD) and Customs' drug agents. Proceedings to add, delete, or change the schedule of a drug or other substance may be initiated by the DEA, the Department of Health and Human Services (HHS), or by petition from any interested party, including the manufacturer of a drug, a medical society or association, a pharmacy association, a public interest group concerned with drug abuse, a state or local government agency, or an individual citizen. When a petition is received by the DEA, the agency begins its own investigation of the drug.

The DEA also may begin an investigation of a drug at any time based upon information received from laboratories, state and local law enforcement and regulatory agencies, or other sources of information. Once the DEA has collected the necessary data, the Deputy Administrator of DEA, requests from HHS a scientific and medical evaluation and recommendation as to whether the drug or other substance should be controlled or removed from control. This request is sent to the Assistant Secretary of Health of HHS. Then, HHS solicits information from the Commissioner of the Food and Drug Administration and evaluations and recommendations from the National Institute on Drug Abuse and, on occasion, from the scientific and medical community at large. The Assistant Secretary, by authority of the Secretary, compiles the information and transmits back to the DEA a medical and scientific evaluation regarding the drug or other substance, a recommendation as to whether the drug should be controlled, and in what schedule it should be placed.

The HHS recommendation on scheduling is binding to the extent that if HHS recommends, based on its medical and scientific evaluation, that the substance not be controlled, then the DEA may not control the substance. Once the DEA has received the scientific and medical evaluation from HHS, the DEA Administrator evaluates all available data and makes a final decision whether to propose that a drug or other substance be controlled and into which schedule it should be placed. Under certain circumstances, the Government may temporarily schedule a drug without following the normal procedure. An example is when international treaties require control of a substance. In addition, allows the Attorney General to temporarily place a substance in Schedule I "to avoid an imminent hazard to the public safety". Thirty days' notice is required before the order can be issued, and the scheduling expires after a year; however, the period may be extended six months if rulemaking proceedings to permanently schedule the drug are in progress. In any case, once these proceedings are complete, the temporary order is automatically vacated. Unlike ordinary scheduling proceedings, such temporary orders are not subject to judicial review.

The CSA also creates a closed system of distribution for those authorized to handle controlled substances. The cornerstone of this system is the registration of all those authorized by the DEA to handle controlled substances. All individuals and firms that are registered are required to maintain complete and accurate inventories and records of all transactions involving controlled substances, as well as security for the storage of controlled substances.

The Congressional findings in 21 USC §§ , , and state that a major purpose of the CSA is to "enable the United States to meet all of its obligations" under international treaties. The CSA bears many resemblances to these Conventions. Both the CSA and the treaties set out a system for classifying controlled substances in several Schedules in accordance with the binding scientific and medical findings of a public health authority. Under of the CSA, that authority is the Secretary of Health and Human Services (HHS). Under Article 3 of the Single Convention and Article 2 of the Convention on Psychotropic Substances, the World Health Organization is that authority.

The domestic and international legal nature of these treaty obligations must be considered in light of the supremacy of the United States Constitution over treaties or acts and the equality of treaties and Congressional acts. In "Reid v. Covert" the Supreme Court of the United States addressed both these issues directly and clearly holding:

According to the Cato Institute, these treaties only bind (legally obligate) the United States to comply with them as long as that nation agrees to remain a state party to these treaties. The U.S. Congress and the President of the United States have the absolute sovereign right to withdraw from or abrogate at any time these two instruments, in accordance with said nation's Constitution, at which point these treaties will cease to bind that nation in any way, shape, or form.

A provision for automatic compliance with treaty obligations is found at , which also establishes mechanisms for amending international drug control regulations to correspond with HHS findings on scientific and medical issues. If control of a substance is mandated by the Single Convention, the Attorney General is required to "issue an order controlling such drug under the schedule he deems most appropriate to carry out such obligations," without regard to the normal scheduling procedure or the findings of the HHS Secretary. However, the Secretary has great influence over any drug scheduling proposal under the Single Convention, because requires the Secretary the power to "evaluate the proposal and furnish a recommendation to the Secretary of State which shall be binding on the representative of the United States in discussions and negotiations relating to the proposal."

Similarly, if the United Nations Commission on Narcotic Drugs adds or transfers a substance to a Schedule established by the Convention on Psychotropic Substances, so that current U.S. regulations on the drug do not meet the treaty's requirements, the Secretary is required to issue a recommendation on how the substance should be scheduled under the CSA. If the Secretary agrees with the Commission's scheduling decision, he can recommend that the Attorney General initiate proceedings to reschedule the drug accordingly. If the HHS Secretary disagrees with the UN controls, however, the Attorney General must temporarily place the drug in Schedule IV or V (whichever meets the minimum requirements of the treaty) and exclude the substance from any regulations not mandated by the treaty, while the Secretary is required to request that the Secretary of State take action, through the Commission or the UN Economic and Social Council, to remove the drug from international control or transfer it to a different Schedule under the Convention. The temporary scheduling expires as soon as control is no longer needed to meet international treaty obligations.

This provision was invoked in 1984 to place Rohypnol (flunitrazepam) in Schedule IV. The drug did not then meet the Controlled Substances Act's criteria for scheduling; however, control was required by the Convention on Psychotropic Substances. In 1999, an FDA official explained to Congress:

The Cato Institute's "Handbook for Congress" calls for repealing the CSA, an action that would likely bring the United States into conflict with international law, were the United States not to exercise its sovereign right to withdraw from and/or abrogate the Single Convention on Narcotic Drugs and/or the 1971 Convention on Psychotropic Substances prior to repealing the Controlled Substances Act. The exception would be if the U.S. were to claim that the treaty obligations violate the United States Constitution. Many articles in these treaties—such as Article 35 and Article 36 of the Single Convention—are prefaced with phrases such as "Having due regard to their constitutional, legal and administrative systems, the Parties shall . . ." or "Subject to its constitutional limitations, each Party shall . . ." According to former United Nations Drug Control Programme Chief of Demand Reduction Cindy Fazey, "This has been used by the USA not to implement part of article 3 of the 1988 Convention, which prevents inciting others to use narcotic or psychotropic drugs, on the basis that this would be in contravention of their constitutional amendment guaranteeing freedom of speech".

There are five different Schedules of controlled substances, numbered IV. The CSA describes the different Schedules based on three factors:

The following table gives a summary of the different Schedules.

Placing a drug or other substance in a certain Schedule or removing it from a certain Schedule is primarily based on 21 USC §§ , , , , , , and . Every schedule otherwise requires finding and specifying the "potential for abuse" before a substance can be placed in that schedule. The specific classification of any given drug or other substance is usually a source of controversy, as is the purpose and effectiveness of the entire regulatory scheme.

Some have argued that this is an important exemption, since alcohol and tobacco are two of the most widely used drugs in the United States. Also of significance, the exclusion of alcohol includes wine which is sacramentally used by many major religious denominations in the United States.

Schedule I substances are described as those that have the following findings:
No prescriptions may be written for Schedule I substances, and such substances are subject to production quotas which the DEA imposes.

Under the DEA's interpretation of the CSA, a drug does not necessarily have to have the same "high potential for abuse" as heroin, for example, to merit placement in Schedule I:

Sentences for first-time, non-violent offenders convicted of trafficking in Schedule I drugs can easily turn into "de facto" life sentences when multiple sales are prosecuted in one proceeding. Sentences for violent offenders are much higher.

Drugs listed in this control schedule include:


Schedule II substances are those that have the following findings:
Except when dispensed directly to an ultimate user by a practitioner other than a pharmacist, no controlled substance in Schedule II, which is a prescription drug as determined under the Federal Food, Drug, and Cosmetic Act (21 USC 301 "et seq."), may be dispensed without the written prescription of a practitioner, except that in emergency situations, as prescribed by the Secretary by regulation after consultation with the Attorney General, such drug may be dispensed upon oral prescription in accordance with section 503(b) of that Act (21 USC 353 (b)). With exceptions, an original prescription is always required even though faxing in a prescription in advance to a pharmacy by a prescriber is allowed. Prescriptions shall be retained in conformity with the requirements of section 827 of this title. No prescription for a controlled substance in schedule II may be refilled. Notably no emergency situation provisions exist outside the Controlled Substances Act's "closed system" although this closed system may be unavailable or nonfunctioning in the event of accidents in remote areas or disasters such as hurricanes and earthquakes. Acts which would widely be considered morally imperative remain offenses subject to heavy penalties.

These drugs vary in potency: for example fentanyl is about 80 times as potent as morphine (heroin is roughly two times as potent). More significantly, they vary in nature. Pharmacology and CSA scheduling have a weak relationship.

Because refills of prescriptions for Schedule II substances are not allowed, it can be burdensome to both the practitioner and the patient if the substances are to be used on a long-term basis. To provide relief, in 2007, was amended (at ) to allow practitioners to write up to three prescriptions at once, to provide up to a 90-day supply, specifying on each the earliest date on which it may be filled.

Drugs in this schedule include:

Schedule III substances are those that have the following findings:
Except when dispensed directly by a practitioner, other than a pharmacist, to an ultimate user, no controlled substance in schedule III or IV, which is a prescription drug as determined under the Federal Food, Drug, and Cosmetic Act (21 USC 301 "et seq."), may be dispensed without a written or oral prescription in conformity with section 503(b) of that Act (21 USC 353 (b)). Such prescriptions may not be filled or refilled more than six months after the date thereof or be refilled more than five times after the date of the prescription unless renewed by the practitioner. A prescription for controlled substances in Schedules III, IV, and V issued by a practitioner, may be communicated either orally, in writing, or by facsimile to the pharmacist, and may be refilled if so authorized on the prescription or by call-in. Control of wholesale distribution is somewhat less stringent than Schedule II drugs. Provisions for emergency situations are less restrictive within the "closed system" of the Controlled Substances Act than for Schedule II though no schedule has provisions to address circumstances where the closed system is unavailable, nonfunctioning or otherwise inadequate.

Drugs in this schedule include:

"Placement on schedules; findings required
Schedule IV substances are those that have the following findings:
Control measures are similar to Schedule III. Prescriptions for Schedule IV drugs may be refilled up to five times within a six-month period. A prescription for controlled substances in Schedules III, IV, and V issued by a practitioner, may be communicated either orally, in writing, or by facsimile to the pharmacist, and may be refilled if so authorized on the prescription or by call-in.

Drugs in this schedule include:

Schedule V substances are those that have the following findings:
No controlled substance in schedule V which is a drug may be distributed or dispensed other than for a medical purpose. A prescription for controlled substances in Schedules III, IV, and V issued by a practitioner, may be communicated either orally, in writing, or by facsimile to the pharmacist, and may be refilled if so authorized on the prescription or by call-in.

Drugs in this schedule include:

The Controlled Substances Act also provides for federal regulation of precursors used to manufacture some of the controlled substances. The DEA list of chemicals is actually modified when the United States Attorney General determines that illegal manufacturing processes have changed.

In addition to the CSA, due to pseudoephedrine (PSE) and ephedrine being widely used in the manufacture of methamphetamine, the U.S. Congress passed the Methamphetamine Precursor Control Act which places restrictions on the sale of any medicine containing pseudoephedrine. That bill was then superseded by the Combat Methamphetamine Epidemic Act of 2005, which was passed as an amendment to the Patriot Act renewal and included wider and more comprehensive restrictions on the sale of PSE-containing products. This law requires customer signature of a "log-book" and presentation of valid photo ID in order to purchase PSE-containing products from all retailers.

Additionally, the law restricts an individual to the retail purchase of no more than three packages or 3.6 grams of such product per day per purchase – and no more than 9 grams in a single month. A violation of this statute constitutes a misdemeanor. Retailers now commonly require PSE-containing products to be sold behind the pharmacy or service counter. This affects many preparations which were previously available over-the-counter without restriction, such as Actifed and its generic equivalents.

There has been criticism against the schedule classifications of the listed drugs and substances in the CSA, citing undefined terms.
Some criticism has arisen due to research that has found several substances on the list of Schedule I substances to have actual accepted medical uses and low abuse potential, despite the requirement for a Schedule I listing mandating that any substance so scheduled have both a high potential for abuse and no accepted medical use. One such example is the legalization of marijuana in some capacity in over 29 states.

Similar legislation outside of the United States:



</doc>
<doc id="7728" url="https://en.wikipedia.org/wiki?curid=7728" title="Claude Piron">
Claude Piron

Claude Piron (26 February 1931 – 22 January 2008), also known by the pseudonym Johán Valano, was a Swiss psychologist, Esperantist, translator, and writer. He worked as a translator for the United Nations from 1956 to 1961 and then for the World Health Organization.

He was a prolific author of Esperanto works. He spoke Esperanto from childhood and used it in Japan, the People's Republic of China, Uzbekistan, Kazakhstan, in Africa and Latin America, and in nearly all the countries of Europe.

Piron was a psychotherapist and taught from 1973 to 1994 in the psychology department at the University of Geneva in Switzerland. His French-language book "Le défi des langues — Du gâchis au bon sens" (The Language Challenge: From Chaos to Common Sense, 1994) is a kind of psychoanalysis of international communication. A Portuguese version, "O desafio das linguas", was published in 2002 (Campinas, São Paulo, Pontes).

In a lecture on the current system of international communication Piron argued that "Esperanto relies entirely on innate reflexes" and "differs from all other languages in that you can always trust your natural tendency to generalize patterns... The same neuropsychological law...—called by Jean Piaget "generalizing assimilation"—applies to word formation as well as to grammar."

His diverse Esperanto writings include instructional books, books for beginners, novels, short stories, poems, articles and non-fiction books. His most famous works are "Gerda malaperis!" and "La Bona Lingvo" (The Good Language).

"Gerda malaperis!" is a novella which uses basic grammar and vocabulary in the first chapter and builds up to expert Esperanto by the end, including word lists so that beginners may easily follow along.

In "La Bona Lingvo", Piron captures the basic linguistic and social aspects of Esperanto. He argues strongly for imaginative use of the basic Esperanto morpheme inventory and word-formation techniques, and against unnecessary importation of neologisms from European languages. He also presents the idea that, once one has learned enough vocabulary to express himself, it is easier to think clearly in Esperanto than in many other languages.

Piron is the author of a book in French, "Le bonheur clés en main" (The Keys to Happiness), which distinguishes among pleasure, happiness and joy. He shows how one may avoid contributing to his own "anti-happiness" ("l'anti-bonheur") and how one may expand the areas of happiness in his life. Piron's view is that, while one may desire happiness, desire is not enough. Just as people must do certain things in order to become physically stronger, they must do certain things in order to become happier. Those are the things that he describes in this book.




</doc>
<doc id="7729" url="https://en.wikipedia.org/wiki?curid=7729" title="Captain America">
Captain America

Captain America is a fictional superhero appearing in American comic books published by Marvel Comics. Created by cartoonists Joe Simon and Jack Kirby, the character first appeared in "Captain America Comics" #1 (cover dated March 1941) from Timely Comics, a predecessor of Marvel Comics. Captain America was designed as a patriotic supersoldier who often fought the Axis powers of World War II and was Timely Comics' most popular character during the wartime period. The popularity of superheroes waned following the war and the "Captain America" comic book was discontinued in 1950, with a short-lived revival in 1953. Since Marvel Comics revived the character in 1964, Captain America has remained in publication.

The character wears a costume bearing an American flag motif, and he utilizes a nearly indestructible shield which he throws as a projectile. Captain America is the alter ego of Steve Rogers, a frail young man enhanced to the peak of human perfection by an experimental serum to aid the United States government's efforts in World War II. Near the end of the war, he was trapped in ice and survived in suspended animation until he was revived in the present day. Although Captain America often struggles to maintain his ideals as a man out of his time with its modern realities, he remains a highly respected figure in his community which includes becoming the long-time leader of the Avengers.

Captain America was the first Marvel Comics character to appear in media outside comics with the release of the 1944 movie serial, "Captain America". Since then, the character has been featured in other films and television series. In the Marvel Cinematic Universe (MCU), the character is portrayed by Chris Evans in "", "The Avengers", "", "", "Ant-Man", "", "", "" (2018) and the fourth Avengers film.

Captain America is ranked sixth on IGN's "Top 100 Comic Book Heroes of All Time" in 2011, second in their list of "The Top 50 Avengers" in 2012, and second in their "Top 25 best Marvel superheroes" list in 2014.

In 1940, writer Joe Simon conceived the idea for Captain America and made a sketch of the character in costume. "I wrote the name 'Super American' at the bottom of the page," Simon said in his autobiography, and then decided: 
Simon recalled in his autobiography that Timely Comics publisher Martin Goodman gave him the go-ahead and directed that a Captain America solo comic book series be published as soon as possible. Needing to fill a full comic with primarily one character's stories, Simon did not believe that his regular creative partner, artist Jack Kirby, could handle the workload alone:

Al Liederman would ink that first issue, which was lettered by Simon and Kirby's regular letterer, Howard Ferguson.

Simon said Captain America was a consciously political creation; he and Kirby were morally repulsed by the actions of Nazi Germany in the years leading up to the United States' involvement in World War II and felt war was inevitable: "The opponents to the war were all quite well organized. We wanted to have our say too."

Captain America Comics #1 — cover-dated March 1941 and on sale December 20, 1940, a year before the attack on Pearl Harbor, but a full year into World War II — showed the protagonist punching Nazi leader Adolf Hitler; it sold nearly one million copies. While most readers responded favorably to the comic, some took objection. Simon noted, "When the first issue came out we got a lot of ... threatening letters and hate mail. Some people really opposed what Cap stood for." The threats, which included menacing groups of people loitering out on the street outside of the offices, proved so serious that police protection was posted with New York Mayor Fiorello La Guardia personally contacting Simon and Kirby to give his support.

Though preceded as a "patriotically themed superhero" by MLJ's The Shield, Captain America immediately became the most prominent and enduring of that wave of superheroes introduced in American comic books prior to and during World War II, as evidenced by the unusual move at the time of premiering the character in his own title instead of an anthology title first. This popularity drew the attention and a complaint from MLJ that the character's triangular shield too closely resembled the chest symbol of their Shield character. In response, Goodman had Simon and Kirby create a distinctive round shield for issue 2, which went on to become an iconic element of the character. With his sidekick Bucky, Captain America faced Nazis, Japanese, and other threats to wartime America and the Allies. Stanley Lieber, now better known as Stan Lee, contributed to the character in issue #3 in the filler text story "Captain America Foils the Traitor's Revenge", which introduced the character's use of his shield as a returning throwing weapon. Captain America soon became Timely's most popular character and even had a fan-club called the "Sentinels of Liberty".

Circulation figures remained close to a million copies per month after the debut issue, which outstripped even the circulation of news magazines such as "Time" during the period. After the Simon and Kirby team moved to DC Comics in late 1941, having produced "Captain America Comics" through issue #10 (January 1942), Al Avison and Syd Shores became regular pencillers of the celebrated title, with one generally inking over the other. The character was featured in "All Winners Comics" #1–19 (Summer 1941 – Fall 1946), "Marvel Mystery Comics" #80–84 and #86–92, "USA Comics" #6–17 (Dec. 1942 – Fall 1945), and "All Select Comics" #1–10 (Fall 1943 – Summer 1946).

In the post-war era, with the popularity of superheroes fading, Captain America led Timely's first superhero team, the All-Winners Squad, in its two published adventures, in "All Winners Comics" #19 and #21 (Fall–Winter 1946; there was no issue #20). After Bucky was shot and wounded in a 1948 "Captain America" story, he was succeeded by Captain America's girlfriend, Betsy Ross, who became the superheroine Golden Girl. "Captain America Comics" ran until issue #73 (July 1949), at which time the series was retitled Captain America's Weird Tales for two issues, with the finale being a horror/suspense anthology issue with no superheroes.

Atlas Comics attempted to revive its superhero titles when it reintroduced Captain America, along with the original Human Torch and the Sub-Mariner, in "Young Men" #24 (Dec. 1953). Billed as "Captain America, Commie Smasher!" Captain America appeared during the next year in "Young Men" #24–28 and "Men's Adventures" #27–28, as well as in issues #76–78 of an eponymous title. Atlas' attempted superhero revival was a commercial failure, and the character's title was canceled with "Captain America" #78 (Sept. 1954).

In the Human Torch story titled "Captain America" in Marvel Comics' "Strange Tales" #114 (Nov. 1963), writer-editor Stan Lee and artist and co-plotter Jack Kirby depicted the brash young Fantastic Four member Johnny Storm, the Human Torch, in an exhibition performance with Captain America, described as a legendary World War II and 1950s superhero who has returned after many years of apparent retirement. The 18-page story ends with this Captain America revealed as an impostor: it was actually the villain the Acrobat, a former circus performer the Torch had defeated in "Strange Tales" #106, who broke two thieves out of jail, hoping to draw the police away while trying to rob the local bank. Afterward, Storm digs out an old comic book in which Captain America is shown to be Steve Rogers. A caption in the final panel says this story was a test to see if readers would like Captain America to return. According to Lee, fan response to the tryout was very enthusiastic.

Captain America was then formally reintroduced in "The Avengers" #4 (March 1964), which explained that in the final days of World War II, he had fallen from an experimental drone plane into the North Atlantic Ocean and spent decades frozen in a block of ice in a state of suspended animation. The hero found a new generation of readers as leader of that superhero team. Following the success of other Marvel characters introduced during the 1960s, Captain America was recast as a hero "haunted by past memories, and trying to adapt to 1960s society".

After then guest-starring in the feature "Iron Man" in "Tales of Suspense" #58 (Oct. 1964), Captain America gained his own solo feature in that "split book", beginning the following issue. Issue #63 (March 1965), which retold Captain America's origin, through issue #71 (Nov. 1965) was a period feature set during World War II and co-starred Captain America's Golden Age sidekick, Bucky. Kirby drew all but two of the stories in "Tales of Suspense," which became "Captain America" with #100 (April 1968); Gil Kane and John Romita Sr., each filled in once. Several stories were finished by penciller-inker George Tuska over Kirby layouts, with one finished by Romita Sr. and another by penciller Dick Ayers and inker John Tartaglione. Kirby's regular inkers on the series were Frank Giacoia (as "Frank Ray") and Joe Sinnott, though Don Heck and Golden Age Captain America artist Syd Shores inked one story each.

This series — considered "Captain America" volume one by comics researchers and historians, following the 1940s "Captain America Comics" and its 1950s numbering continuation of "Tales of Suspense" — ended with #454 (Aug. 1996).

This series was almost immediately followed by the 13-issue "Captain America" vol. 2 (Nov. 1996 – Nov. 1997, part of the "Heroes Reborn" crossover), the 50-issue "Captain America" vol. 3 (Jan. 1998 – Feb. 2002), the 32-issue "Captain America" vol. 4 (June 2002 – Dec. 2004), and "Captain America" vol. 5 (Jan. 2005 – Aug. 2011). Beginning with the 600th overall issue (Aug. 2009), "Captain America" resumed its original numbering, as if the series numbering had continued uninterrupted after #454.

As part of the aftermath of Marvel Comics' company-crossover storyline "Civil War", Steve Rogers was ostensibly killed in "Captain America" vol. 5, #25 (March 2007).

The storyline of Rogers' return began in issue #600. Rogers, who was not dead but caroming through time, returned to the present day in the six-issue miniseries "" (Sept. 2009 – March 2010).

After Rogers' return, Barnes, at Rogers' insistence, continued as Captain America, beginning in the one-shot comic "Captain America: Who Will Wield the Shield?" (Feb. 2010). While Bucky Barnes continued adventuring in the pages of "Captain America", Steve Rogers received his own miniseries ("Steve Rogers: Super-Soldier") as well as taking on the leadership position in a new "Secret Avengers" ongoing series.

Spinoff series included "Captain America Sentinel of Liberty" (Sept. 1998 – Aug. 1999) and "Captain America and the Falcon" (May 2004 – June 2005). The 1940s Captain America appeared alongside the 1940s Human Torch and Sub-Mariner in the 12-issue miniseries "Avengers/Invaders". The 2007 mini-series "Captain America: The Chosen", written by David Morrell and penciled by Mitchell Breitweiser, depicts a dying Steve Rogers' final minutes, at S.H.I.E.L.D. headquarters, as his spirit guides James Newman, a young American Marine fighting in Afghanistan. "The Chosen" is not part of the main Marvel Universe continuity.

During the "Two Americas" storyline that ran in issues #602-605, the series drew controversy for the similarity between protesters depicted in the comic and the Tea Party movement. Particularly drawing scorn was a panel of a protester holding sign that read "Tea Bag the Libs Before They Tea Bag You!" Also drawing controversy were remarks made by the Falcon implying that the crowd is racist. In his column on Comic Book Resources, Marvel Comics Editor-in-Chief Joe Quesada apologized for the sign, claiming that it was mistake added by the letterer at the last minute.

The character, first as agent Steve Rogers and later after resuming his identity as Captain America, appeared as a regular character throughout the 2010–2013 "Avengers" series, from issue #1 (July 2010) through its final issue #34 (January 2013). The character appeared as agent Steve Rogers as a regular character in the 2010–2013 "Secret Avengers" series, from issue #1 (July 2010) through issue #21 (March 2012); the character made guest appearances as Captain America in issues #21.1, #22–23, #35, and the final issue of the series #37 (March 2013).

Marvel stated in May 2011 that Rogers, following the public death of Bucky Barnes in the "Fear Itself" miniseries, would resume his Captain America identity in a sixth volume of "Captain America", by writer Ed Brubaker and artist Steve McNiven. The "Captain America" title continued from issue #620 featuring team up stories with Bucky (#620-#628), Hawkeye (#629-#632), Iron Man (#633–635), Namor (#635.1), and Black Widow (#636-#640), and the title ended its print run with issue #640.

Captain America is a regular character in "Uncanny Avengers" (2012), beginning with issue #1 as part of Marvel NOW!. "Captain America" vol. 7 was launched in November 2012 with a January 2013 cover date by writer Rick Remender and artist John Romita Jr..

On July 16, 2014 Marvel Comics announced that the mantle of Captain America would be passed on by Rogers (who in the most recent storyline has been turned into a 90-year-old man) to his long-time ally The Falcon, with the series being relaunched as "All-New Captain America".
Marvel announced that Rogers will become Captain America once again in the comic series "Captain America: Steve Rogers". This new series follows the events of “,” in which Captain America is restored to his youthful state following an encounter with the sentient Cosmic Cube, Kobik, and his past is drastically rewritten under the instructions of the Red Skull. Afterward, Captain America plots to set himself and Hydra in a position where they can conquer America in Marvel’s event “Secret Empire.”

In 1966 Joe Simon sued the owners of Marvel Comics, asserting that he—not Marvel—was legally entitled to renew the copyright upon the expiration of the original 28-year term. The two parties settled out of court, with Simon agreeing to a statement that the character had been created under terms of employment by the publisher, and therefore it was work for hire owned by them.

In 1999, Simon filed to claim the copyright to Captain America under a provision of the Copyright Act of 1976 which allowed the original creators of works that had been sold to corporations to reclaim them after the original 56-year copyright term (but not the longer term enacted by the new legislation) had expired. Marvel Entertainment challenged the claim, arguing that the settlement of Simon's 1966 suit made the character ineligible for termination of the copyright transfer. Simon and Marvel settled out of court in 2003, in a deal that paid Simon royalties for merchandising and licensing use of the character.

Steven Rogers was born in the Lower East Side of Manhattan, New York City, in 1920 to poor Irish immigrants, Sarah and Joseph Rogers. Joseph died when Steve was a child, and Sarah died of pneumonia while Steve was a teen. By early 1940, before America's entry into World War II, Rogers is a tall, scrawny fine arts student specializing in illustration and a comic book writer and artist.

Disturbed by Adolf Hitler's rise to power, Rogers attempts to enlist but is rejected due to his frail body. His resolution attracts the notice of U.S. Army General Chester Phillips and "Project: Rebirth". Rogers is used as a test subject for the Super-Soldier project, receiving a special serum made by "Dr. Josef Reinstein", later retroactively changed to a code name for the scientist Abraham Erskine.

The serum is a success and transforms Steve Rogers into a nearly perfect human being with peak strength, agility, stamina, and intelligence. The success of the program leaves Erskine wondering about replicating the experiment on other human beings. The process itself has been inconsistently detailed: While in the original material Rogers is shown receiving injections of the Super-Serum, when the origin was retold in the 1960s, the Comic Code Authority had already put a veto over graphic description of drug intake and abuse, and thus the Super-Serum was retconned into an oral formula. Later accounts hint at a combination of oral and intravenous treatments with a strenuous training regimen, culminating in the Vita-Ray exposure.

Erskine refused to write down every crucial element of the treatment, leaving behind a flawed, imperfect knowledge of the steps. Thus, when the Nazi spy Heinz Kruger killed him, Erskine's method of creating new Super-Soldiers died. Captain America, in his first act after his transformation, avenges Erskine. In the 1941 origin story and in "Tales of Suspense" #63, Kruger dies when running into machinery but is not killed by Rogers; in the "Captain America" #109 and #255 revisions, Rogers causes the spy's death by punching him into machinery.

Unable to create new Super-Soldiers and willing to hide the Project Rebirth fiasco, the American government casts Rogers as a patriotic superhero, able to counter the menace of the Red Skull as a counter-intelligence agent. He is supplied with a patriotic uniform of his own design, a bulletproof shield, a personal side arm, and the codename Captain America, while posing as a clumsy infantry private at Camp Lehigh in Virginia. He forms a friendship with the camp's teenage mascot, James Buchanan "Bucky" Barnes.

Barnes learns of Rogers' dual identity and offers to keep the secret if he can become Captain America's sidekick. During their adventures, Franklin D. Roosevelt presents Captain America with a new shield, forged from an alloy of steel and vibranium, fused by an unknown catalyst, so effective that it replaces his own firearm. Throughout World War II, Captain America and Bucky fight the Nazi menace both on their own and as members of the superhero team the Invaders as seen in the 1970s comic of the same name. Captain America fights in numerous battles in World War II, primarily as a member of 1st Battalion, 26th Infantry Regiment "Blue Spaders". Captain America battles a number of criminal menaces on American soil, including a wide variety of costumed villains: the Wax Man, the Hangman, the Fang, the Black Talon, and the White Death, among others.

In addition to Bucky, Captain America was occasionally assisted by the Sentinels of Liberty. Sentinels of Liberty was the title given to members of the "Captain America Comics" fan club who Captain America sometimes addressed as an aside, or as characters in the "Captain America Comics" stories.

In late April 1945, during the closing days of World War II, Captain America and Bucky try to stop the villainous Baron Zemo from destroying an experimental drone plane. Zemo launches the plane with an armed explosive on it with Rogers and Barnes in hot pursuit. The pair reaches the plane just before take off. When Bucky tries to defuse the bomb, it explodes in mid-air. Rogers is hurled into the freezing waters of the North Atlantic. Both are presumed dead, though it is later revealed that neither one died.

Captain America appeared in comics for the next few years, changing from World War II-era hero fighting the Nazis to confronting the United States' newest enemy, Communism. The revival of the character in the mid-1950s was short-lived, and events during that time period are later retconned to show that multiple people operated using the code name to explain the changes in the character. These post World War II successors are listed as William Naslund and Jeffrey Mace.

The last of these other official Captains, William Burnside, was a history graduate enamored with the Captain America mythos, having his appearance surgically altered to resemble Rogers and legally changing his name to "Steve Rogers", becoming the new "1950s Captain America". He self-administered to himself and his pupil James "Jack" Monroe a flawed, incomplete copy of the Super-Serum, which made no mention about the necessary Vita-Ray portion of the treatment. As a result, while Burnside and Monroe became the new Captain America and Bucky, they became violently paranoid, often raving about innocent people being communist sympathizers during the height of the Red Scare of the 1950s. Their insanity forced the U.S. government to place them in indefinite cryogenic storage until they could be cured of their mental illness. Monroe would later be cured and assume the Nomad identity.

Years later, the superhero team the Avengers discovers Steve Rogers' body in the North Atlantic. After he revives, they piece together that Rogers has been preserved in a block of ice since 1945, surviving because of his enhancements from Project: Rebirth. The block began to melt after the Sub-Mariner, enraged that an Inuit tribe is worshipping the frozen figure, throws it into the ocean. Rogers accepts membership in the Avengers, and his experience in individual combat service and his time with the Invaders makes him a valuable asset. He quickly assumes leadership and has typically returned to that position throughout the team's history.
Captain America is plagued by guilt for having been unable to prevent Bucky's death. Although he takes the young Rick Jones (who closely resembles Bucky) under his tutelage, he refuses for some time to allow Jones to take up the Bucky identity, not wishing to be responsible for another youth's death. Insisting that his hero move on from that loss, Jones convinces Rogers to let him don the Bucky costume, but this partnership lasts only a short time; a disguised Red Skull, impersonating Rogers with the help of the Cosmic Cube, drives Jones away.

Rogers reunites with his old war comrade Nick Fury, who is similarly well-preserved due to the "Infinity Formula". As a result, Rogers regularly undertakes missions for the security agency S.H.I.E.L.D., for which Fury is public director. Through Fury, Rogers befriends Sharon Carter, a S.H.I.E.L.D. agent, with whom he eventually begins a romantic relationship.

Rogers later meets and trains Sam Wilson, who becomes the superhero the Falcon, the first African-American superhero in mainstream comic books. The characters established an enduring friendship and adventuring partnership, sharing the series title for some time as "Captain America and the Falcon". The two later encounter the revived but still insane 1950s Captain America. Although Rogers and the Falcon defeat the faux Rogers and Jack Monroe, Rogers becomes deeply disturbed that he could have suffered his counterpart's fate. During this period, Rogers temporarily gains super strength.

The series dealt with the Marvel Universe's version of the Watergate scandal, making Rogers so uncertain about his role that he abandons his Captain America identity in favor of one called Nomad, emphasizing the word's meaning as "man without a country". During this time, several men unsuccessfully assume the Captain America identity. Rogers eventually re-assumes it after coming to consider that the identity could be a symbol of American ideals and not its government; it's a personal conviction epitomized when he later confronted a corrupt Army officer attempting to manipulate him by appealing to his loyalty, "I'm loyal to nothing, General ... except the [American] Dream." Jack Monroe, cured of his mental instability, later takes up the Nomad alias. Sharon Carter is believed to have been killed while under the mind control of Dr. Faustus.

The 1980s included a run by writer Roger Stern and artist John Byrne. Stern had Rogers consider a run for President of the United States in "Captain America" #250 (June 1980), an idea originally developed by Roger McKenzie and Don Perlin. Stern, in his capacity as editor of the title, originally rejected the idea but later changed his mind about the concept. McKenzie and Perlin received credit for the idea on the letters page at Stern's insistence. Stern additionally introduced a new love interest, law student Bernie Rosenthal, in "Captain America" #248 (Aug. 1980).
Writer J. M. DeMatteis revealed the true face and full origin of the Red Skull in "Captain America" #298–300, and had Captain America take on Jack Monroe, Nomad, as a partner for a time. Around this time, the heroes gathered by the Beyonder elect Rogers as leader during their stay on Battleworld in the 1984 miniseries "Secret Wars". Homophobia is dealt with as Rogers runs into a childhood friend named Arnold Roth who is gay.

Mark Gruenwald became the writer of the series with issue #307 (July 1985) and wrote 137 issues for 10 consecutive years from until #443 (Sept. 1995), the most issues by any single author in the character's history. Gruenwald created several new foes, including Crossbones and the Serpent Society. Other Gruenwald characters included Diamondback, Super Patriot, and Demolition Man. Gruenwald explored numerous political and social themes as well, such as extreme idealism when Captain America fights the anti-nationalist terrorist Flag-Smasher; and vigilantism when he hunts the murderous Scourge of the Underworld.

Rogers receives a large back-pay reimbursement dating back to his disappearance at the end of World War II, and a government commission orders him to work directly for the U.S. government. Already troubled by the corruption he had encountered with the Nuke incident in New York City, Rogers chooses instead to resign his identity, and then takes the alias of "the Captain". A replacement Captain America, John Walker, struggles to emulate Rogers' ideals until pressure from hidden enemies helps to drive Walker insane. Rogers returns to the Captain America identity while a recovered Walker becomes the U.S. Agent.

Sometime afterward, Rogers avoids the explosion of a methamphetamine lab, but the drug triggers a chemical reaction in the Super-Soldier serum in his system. To combat the reaction, Rogers has the serum removed from his body and trains constantly to maintain his physical condition. A retcon later establishes that the serum was not a drug "per se", which would have metabolized out of his system, but in fact a virus-like organism that effected a biochemical and genetic change. This additionally explained how nemesis the Red Skull, who at the time inhabited a body cloned from Rogers' cells, has the formula in his body.

Because of his altered biochemistry, Rogers' body begins to deteriorate, and for a time he must wear a powered exoskeleton and is eventually placed again in suspended animation. During this time, he is given a transfusion of blood from the Red Skull, which cures his condition and stabilizes the Super-Soldier virus in his system. Captain America returns to crime fighting and the Avengers.

Following Gruenwald's departure from the series, Mark Waid took over and resurrected Sharon Carter as Cap's love interest. The title was then relaunched under Rob Liefeld as Cap became part of the Heroes Reborn universe for 13 issues before another relaunch restored Waid to the title in an arc that saw Cap lose his shield for a time using an energy based shield as a temporary replacement. Following Waid's run, Dan Jurgens took over and introduced new foe Protocide, a failed recipient of the Super Soldier serum prior to the experiment that successfully created Rogers. Some time after this, Rogers' original shield was retrieved, but subtle damage sustained during the battle with the Beyonder resulted in it being shattered and a 'vibranium cancer' being triggered that would destroy all vibranium in the world, with Rogers nearly being forced to destroy the shield before a confrontation with the villain Klaw saw Klaw's attacks unwittingly repair the shield's fractured molecular bonds and negate the cancer.

In the aftermath of the September 11 terrorist attacks, Rogers reveals his identity to the world and establishes a residence in the Red Hook neighborhood of Brooklyn, New York, as seen in "Captain America" vol. 4, #1–7 (June 2002 – Feb. 2003). Following the disbandment of the Avengers in the "Avengers Disassembled" story arc, Rogers, now employed by S.H.I.E.L.D., discovers Bucky is alive, having been saved and deployed by the Soviets as the Winter Soldier. Rogers resumes his on-again, off-again relationship with S.H.I.E.L.D. agent Sharon Carter. After a mass supervillain break-out of the Raft, Rogers and Tony Stark assemble a new team of Avengers to hunt the escapees.
In the 2006–2007 company-wide story arc "Civil War", Rogers opposes the new mandatory federal registration of super-powered beings, and leads the underground anti-registration movement. After significant rancor and danger to the public as the two sides clash, Captain America voluntarily surrenders and orders the Anti-Registration forces to stand down, feeling that the fight has reached a point where the principle originally cited by the anti-registration forces has been lost.

In the story arc "The Death of Captain America", Rogers is fatally shot by Sharon Carter, whose actions are manipulated by the villain Dr. Faustus. The miniseries "" #1–5 (June–Aug. 2007) examines the reaction of the stunned superhero community to Rogers' assassination, with each of the five issues focusing a different character's reaction. Bucky takes on the mantle of Captain America, per Rogers' antemortem request.

"" #1 (Aug. 2009) reveals that Rogers did not die, as the gun Sharon Carter had been hypnotized into firing at Rogers caused his consciousness to phase in and out of space and time, appearing at various points in his lifetime. Although Rogers manages to relay a message to the future by giving a time-delayed command to the Vision during the Kree-Skrull War, the Skull returns Rogers to the present, where he takes control of Rogers' mind and body. Rogers eventually regains control, and, with help from his allies, defeats the Skull. In the subsequent one-shot comic "Captain America: Who Will Wield the Shield?", Rogers formally grants Bucky his Captain America shield and asks him to continue as Captain America. The President of the United States grants Rogers a full pardon for his anti-registration actions.

Following the company-wide "Dark Reign" and "Siege" story arcs, the Steve Rogers character became part of the "Heroic Age" arc.

The President of the United States appoints Rogers, in his civilian identity, as ""America's top cop"" and head of the nation's security, replacing Norman Osborn as the tenth "Executive Director of S.H.I.E.L.D.". The Superhuman Registration Act is repealed and Rogers re-establishes the superhero team the Avengers, spearheaded by Iron Man, Thor, and Bucky as Captain America. In the miniseries "Steve Rogers: Super Soldier", he encounters Jacob Erskine, the grandson of Professor Abraham Erskine and the son of Tyler Paxton, one of Rogers' fellow volunteers in the Super-Soldier program. Shortly afterward, Rogers becomes leader of the Secret Avengers, a black-ops superhero team.

During the "Fear Itself" storyline, Steve Rogers is present when the threat of the Serpent is known. Following the apparent death of Bucky at the hands of Sin (in the form of Skadi), Steve Rogers ends up changing into his Captain America uniform. When the Avengers and the New Avengers are fighting Skadi, the Serpent ends up joining the battle and breaks Captain America's shield with his bare hands. Captain America and the Avengers teams end up forming a militia for a last stand against the forces of the Serpent. When it comes to the final battle, Captain America uses Thor's hammer to fight Skadi until Thor manages to kill the Serpent. In the aftermath of the battle, Iron Man presents him with his reforged shield, now stronger for its uru-infused enhancements despite the scar it bears. It is then revealed that Captain America, Nick Fury, and Black Widow are the only ones who know that Bucky actually survived the fight with Skadi as Bucky resumes his identity as Winter Soldier.

In the "Avengers vs. X-Men" story arc, Captain America attempts to apprehend Hope Summers of the X-Men. She is the targeted vessel for the Phoenix Force, a destructive cosmic entity. Captain America believes that this Phoenix Force is too dangerous to entrust in one person and seeks to prevent Hope from having it. Cyclops and the X-Men believe that the Phoenix Force will save their race, and oppose Captain America's wishes. The result is a series of battles that eventually take both teams to the blue area of the moon. The Phoenix Force eventually possesses the five X-Men present, leaving the Avengers at an extreme disadvantage. The "Phoenix Five", who become corrupted by the power of the Phoenix, are eventually defeated and scattered, with Cyclops imprisoned for turning the world into a police state and murdering Charles Xavier after being pushed too far, only for him to note that, in the end, he was proven right about the Phoenix's intentions. From there, Captain America proceeds to assemble the Avengers Unity Squad, a new team of Avengers composed of both classic Avengers and X-Men.

After Cyclops was incarcerated, and Steve accepted the Avengers should have done more to help mutants, and allowed the world to hate them, he started planning a new sub-team of Avengers in the hopes of unifying mutant and humankind alike. He chose Havok to lead his team and become the new face to represent mutants as Professor X and Cyclops once were.

Their first threat was the return of the Red Skull- more specifically, a clone of the Skull created in 1942 and kept in stasis in the event of the original's death- who usurped Professor X's body to provide himself with telepathic powers, which he would use to provoke citizens of New York into a mass assault against mutants, or anyone who could be one, and force the Scarlet Witch and Rogue to allow themselves to be attacked. With the help of the S-Man Honest John, he managed to even manipulate Thor.

The Red Skull's skills were still erratic, and could not completely control Captain America, an attack against him was enough of a distraction to lose control of Rogue and the Scarlet Witch. After being overpowered by the rest of the Uncanny Avengers, the Red Skull escapes, but promises to return. In the aftermath, both Rogue and the Scarlet Witch joined the team.

During a battle with an enemy called the Iron Nail, the Super-Soldier Serum within Rogers's body was neutralized, causing him to age rapidly to match his chronological age of over 90 years. No longer able to take part in field missions but retaining his sharp mind, Rogers decided to take on a role as mission coordinator, organizing the Avengers' plans of attack from the mansion, while appointing Sam Wilson as his official "replacement" as Captain America. When various Avengers and X-Men were inverted into villains and several villains inverted into heroism due to a miscast spell by the Scarlet Witch and Doctor Doom, Rogers not only coordinated the efforts of Spider-Man and the inverted villains, now called the "Astonishing Avengers", but also donned his old armor to battle the inverted Falcon, until the heroes and villains could be returned to normal with the aid of the White Skull (the inverted Red Skull).

During the "Time Runs Out" storyline, Steve Rogers wears armor when he confronts Iron Man. The ensuing fight between the two old friends led Steve Rogers to force Iron Man to admit that he had lied to him and all of their allies, when he had known about the incursions between alternate Earths all along, but Iron Man also confessed that he wouldn't change a thing. The final incursion started and Earth-1610 started approaching Earth-616 while Iron Man and Steve Rogers kept fighting. Earth-1610's S.H.I.E.L.D. launched a full invasion to destroy Earth-616, where Tony Stark and Steve Rogers were crushed by a Helicarrier.

As part of the "All-New, All-Different Marvel", Steve Rogers became the new Chief of Civilian Oversight for S.H.I.E.L.D. He returned to the Uncanny Avengers where the team is now using the Schaefer Theater as their headquarters.

Steve Rogers later has an encounter with an alternate Logan from Earth-807128. After defeating Logan and bringing him to Alberta, Canada, Rogers tried to "reassure" Logan that this was not "his" past by showing him the adamantium-frozen body of Earth-616's Logan. This sight reminds Logan of the need to enjoy being alive rather than brooding over the ghosts of his past. Although he told Steve Rogers what he had experienced in his timeline, Logan declined Steve's offer of help.

During the 2016 "" storyline, Steve Rogers learns from Rick Jones that S.H.I.E.L.D. has established Pleasant Hill, a gated community where they use Kobik to transform villains into ordinary citizens. When Rogers is brought to Pleasant Hill, he confronts Maria Hill about the Kobik project. Their argument is interrupted when Baron Helmut Zemo and Fixer restore the inmates to normal. After Hill is injured, Rogers convinces Zemo to let Hill get medical attention. Rogers is then escorted to Dr. Erik Selvig's clinic by Father Patrick. Selvig tells Rogers that Kobik is at the Pleasant Hill Bowling Alley. During an attempt to reason with Kobik, Rogers is attacked by Crossbones. Before Rogers can be killed, Kobik uses her abilities to restore him back to his prime. Declaring that "It's good to be back," Steve defeats Crossbones as Captain America and the Winter Soldier catch up with him. They resume their search for Kobik, and discover that Baron Zemo had Fixer invent a device that would make Kobik subservient to them. Rogers rallies the heroes so that they can take the fight to Zemo. In the aftermath of the incident, Steve and Sam plan to keep what happened at Pleasant Hill under wraps for the time being.

In "Captain America: Steve Rogers" #1 (July 2016), the final panel apparently revealed that Rogers has been a Hydra double-agent since his early youth. This is subsequently revealed to be the result of Kobik's restoration of Rogers' youth, as she had been taught by the Red Skull that Hydra was good for the world, and having the mind of a four-year-old child, Kobik changed reality so that Rogers would be the greatest man he could be: believing Hydra to be good, Kobik permanently altered his memories so that Rogers believed that he had always been a member of Hydra. Some of Rogers' original heroic attributes remain intact, such as covering the death of another Hydra member within S.H.I.E.L.D., Erik Selvig, as well as knowing of Jack Flag's tragic life and his immortality, which is why Steve pushes him from Zemo's airplane (resulting in coma, not death). Additionally, it is revealed that Rogers' abusive father, Joseph, was actually killed by Hydra, and that Hydra deceived him into thinking Joseph died of a heart attack. It is also revealed that Rogers witnessed his mother, Sarah, being killed by Sinclair's Hydra goons and kidnapped him, which is the reason why Steve held a grudge towards Hydra's evilness and plans to kill the Red Skull's clone and restore Hydra's lost honor. As part of his long-term plans, Steve further compromised Sam Wilson's current image as 'the' Captain America by using his greater familiarity with the shield to deliberately put Wilson in a position where he would be unable to use the shield to save a senator from Flag-Smasher, with the final goal of demoralizing Sam to the point where he will return the shield to Rogers of his own free will, not wanting to kill Wilson and risk creating a martyr.

During the 2016 "Civil War II" storyline, with the discovery of new Inhuman Ulysses – who has the ability to "predict" the future by calculating complex patterns – Rogers has set out to prevent Ulysses from learning of his true plans and allegiance. Rogers does this by "forcing" certain predictions on him, such as anonymously providing Bruce Banner with new gamma research to provoke a vision that would drive the Avengers to kill Banner, although this plan has apparently backfired with a recent vision showing the new Spider-Man standing over the dead Steve Rogers. Despite this revelation, Rogers presents himself as the voice of reason by allowing Spider-Man to flee with Thor. This inspires doubt in Tony Stark for his current stance by suggesting that he is just acting against Danvers because he does not like being top dog. He then goes to Washington, D.C., the location seen in Ulysses' vision, to talk to Spider-Man, who was trying to understand the vision like he was. When Captain Marvel attempts to arrest Spider-Man, Tony, wearing the War Machine armor, confronts her and the two begin to fight.

Later, Rogers goes to Sokovia and joins forces with Black Widow to liberate freedom fighters from a prison so they can reclaim their country. After that, he goes to his base where Doctor Selvig expresses concern of his plan to kill the Red Skull. He then reveals that he has Baron Zemo in a cell, planning to recruit him. He eventually kills the Skull after the villain is captured by the Unity Squad and the Xavier brain fragment extracted by the Beast, Rogers throwing the Skull out of a window over a cliff after Sin and Crossbones affirm their new allegiance to Rogers.

In the 2017 "Secret Empire" storyline, Rogers, as the head of S.H.I.E.L.D, uses a subsequent alien invasion and a mass supervillain assault in order to neutralize the superheroes that might oppose him, and seeks the Cosmic Cube to bring about a reality in which Hydra won World War II. When Rick smuggles information about the Cube's rewriting of Rogers' reality to the remaining free Avengers, a disheveled, bearded man in a torn World War II army uniform appears who introduces himself as Steve Rogers. As the Avengers and Hydra search for fragments of the shattered Cube, it is revealed that this amnesic Steve Rogers is actually a manifestation of Rogers existing within the Cube itself, created by Kobik's memories of Rogers before he was converted to Hydra, as she comes to recognize that her decision to 'rewrite' Rogers as an agent of Hydra was wrong. Although Hydra Rogers is able to mostly reassemble the Cosmic Cube, Sam Wilson and Bucky are able to use a fragment of the cube to restore the 'memory' of pre-Hydra Rogers in the Cube to corporeal existence, allowing him to defeat his Hydra self, subsequently using the Cube to undo most of the damage caused by Hydra manipulating reality even if the physical damage remains. 'Hydra Cap' continues to exist as a separate entity and is kept trapped in a prison where he is the only inmate, mocking the restored Rogers about the challenge he will face rebuilding his reputation even as Rogers muses that this experience will teach everyone not to place such blind trust in another.

Captain America has no superhuman powers, but through the Super-Soldier Serum and "Vita-Ray" treatment, he is transformed and his strength, endurance, agility, speed, reflexes, durability, and healing are at the zenith of natural human potential. Rogers' body regularly replenishes the super-soldier serum; it does not wear off.

Although he lacks superhuman strength, Captain America is one of the few mortal beings who has been deemed worthy enough to wield Thor's hammer Mjolnir.

Rogers' battle experience and training make him an expert tactician and an excellent field commander, with his teammates frequently deferring to his orders in battle. Thor has stated that Rogers is one of the very few humans he will take orders from and follow "through the gates of Hades". Rogers' reflexes and senses are extraordinarily keen. He has blended Boxing, Judo, Karate, Jujutsu, Kickboxing, and gymnastics into his own unique fighting style and is a master of multiple martial arts. Years of practice with his near-indestructible shield make him able to aim and throw it with almost unerring accuracy. His skill with his shield is such that he can attack multiple targets in succession with a single throw or even cause a boomerang-like return from a throw to attack an enemy from behind. In canon, he is regarded by other skilled fighters as one of the best hand-to-hand combatants in the Marvel Universe, limited only by his human physique. Although the super-soldier serum is an important part of his strength, Rogers has shown himself still sufficiently capable against stronger opponents, even when the serum has been deactivated reverting him to his pre-Captain America physique.

Rogers has vast U.S. military knowledge and is often shown to be familiar with ongoing, classified Defense Department operations. He is an expert in combat strategy, survival, acrobatics, parkour, military strategy, piloting, and demolitions. Despite his high profile as one of the world's most popular and recognizable superheroes, Rogers has a broad understanding of the espionage community, largely through his ongoing relationship with S.H.I.E.L.D.

The formula enhances all of his metabolic functions and prevents the build-up of fatigue poisons in his muscles, giving him endurance far in excess of an ordinary human being. This accounts for many of his extraordinary feats, including bench pressing and running a mile (1.6 km) in 73 seconds (49 mph/78 kph, nearly twice the maximum speed achieved by the best human sprinters). Furthermore, his enhancements are the reason why he was able to survive being frozen in suspended animation for decades. He is highly resistant to hypnosis or gases that could limit his focus. The secrets of creating a super-soldier were lost with the death of its creator, Dr. Abraham Erskine. In the ensuing decades there have been numerous attempts to recreate Erskine's treatment, only to have them end in failure. Even worse, the attempts have instead often created psychopathic supervillains of which Captain America's 1950s imitator and Nuke are the most notorious examples.

Captain America has used multiple shields throughout his history, the most prevalent of which is a nigh-indestructible disc-shaped shield made from an experimental alloy of steel and the fictional vibranium. The shield was cast by American metallurgist Dr. Myron MacLain, who was contracted by the U.S. government, from orders of President Franklin D. Roosevelt, to create an impenetrable substance to use for tanks during World War II. This alloy was created by accident and never duplicated, although efforts to reverse-engineer it resulted in the discovery of adamantium.

Captain America often uses his shield as an offensive throwing weapon. The first instance of Captain America's trademark ricocheting shield-toss occurs in Stan Lee's first comics writing, the two-page text story "Captain America Foils the Traitor's Revenge" in "Captain America Comics" #3 (May 1941). The legacy of the shield among other comics characters includes the time-traveling mutant superhero Cable telling Captain America that his shield still exists in one of the possible futures; Cable carries it into battle and brandishes it as a symbol.

When without his trademark shield, Captain America sometimes uses other shields made from less durable metals such as steel, or even a photonic energy shield designed to mimic a vibranium matrix. Rogers, having relinquished his regular shield to Barnes, carried a variant of the energy shield which can be used with either arm, and used to either block attacks or as an improvised offensive weapon able to cut through metal with relative ease. Much like his Vibranium shield, the energy shield can be thrown, including ricocheting off multiple surfaces and returning to his hand.

Captain America's uniform is made of a fire-retardant material, and he wears a lightweight, bulletproof duralumin scale armor beneath his uniform for added protection. Originally, Rogers' mask was a separate piece of material, but an early engagement had it dislodged, thus almost exposing his identity. To prevent a recurrence of the situation, Rogers modified the mask with connecting material to his uniform, an added benefit of which was extending his armor to cover his previously exposed neck. As a member of the Avengers, Rogers has an Avengers priority card, which serves as a communications device.

Captain America has used a custom specialized motorcycle, modified by the S.H.I.E.L.D. weapons laboratory, as well as a custom-built battle van, constructed by the Wakanda Design Group with the ability to change its color for disguise purposes (red, white and blue), and fitted to store and conceal the custom motorcycle in its rear section with a frame that allows Rogers to launch from the vehicle riding it.

Captain America has faced numerous foes in over 70 years of published adventures. Many of his recurring foes embody ideologies contrary to the American values that Captain America is shown to strive for and believes in. Some examples of these opposing values are Nazism (Red Skull, Baron Zemo), neo-Nazism (Crossbones, Doctor Faustus), technocratic fascism (AIM, Arnim Zola), Communism (Aleksander Lukin), anarchism (Flag Smasher) and international and domestic terrorism (Hydra).



</doc>
<doc id="7730" url="https://en.wikipedia.org/wiki?curid=7730" title="Cyclops (disambiguation)">
Cyclops (disambiguation)

A cyclops is a one-eyed monster in Greek mythology.

Cyclops or The Cyclops may also refer to:














</doc>
<doc id="7731" url="https://en.wikipedia.org/wiki?curid=7731" title="Christian countercult movement">
Christian countercult movement

The Christian countercult movement or Christian anti-cult movement is a social movement of certain Protestant evangelical and fundamentalist and other Christian ministries ("discernment ministries") and individual activists who oppose religious sects they consider "cults".

Christian countercult-activism stems mainly from evangelicalism or fundamentalism. The countercult movement asserts that particular Christian sects whose beliefs they deem partially or wholly not in accordance with the Bible are erroneous. It also states that a religious sect can be considered a cult if its beliefs involve a denial of what they view as any of the essential Christian teachings (such as salvation, the Trinity, Jesus himself as a person, the ministry and miracles of Jesus, his crucifixion, his resurrection, the Second Coming and the Rapture).

Countercult ministries often concern themselves with religious sects which consider themselves Christian but which hold beliefs thought to contradict the Bible. Such sects may include: The Church of Jesus Christ of Latter-day Saints, the Unification Church, Christian Science, and Jehovah's Witnesses. Anti-Catholic ideology has led some Protestants to classify Catholicism as a cult. John Highham described anti-Catholicism as "the most luxuriant, tenacious tradition of paranoiac agitation in American history". Some also denounce non-Christian religions such as Islam, Wicca, Paganism, New Age groups, Buddhism, Hinduism and other religions.

Countercult literature usually expresses doctrinal or theological concerns and a missionary or apologetic purpose. It presents a rebuttal by emphasizing the teachings of the Bible against the beliefs of non-fundamental Christian sects. Christian countercult activist writers also emphasize the need for Christians to evangelize to followers of cults. Some Christians also share concerns similar to those of the secular anti-cult movement.

The movement publishes its views through a variety of media, including books, magazines, and newsletters, radio broadcasting; audio and videocassette production, direct-mail appeals, proactive evangelistic encounters, professional and avocational websites, as well as lecture series, training workshops and counter-cult conferences.

Christians have applied theological criteria to assess the teachings of non-orthodox movements throughout church history. The Apostles themselves were involved in challenging the doctrines and claims of various teachers. The Apostle Paul wrote an entire epistle, Galatians, antagonistic to the teachings of a Jewish sect that claimed adherence to the teachings of both Jesus and Moses (cf. Acts 15: & Gal. 1:6-10). The Apostle John devoted his first Epistle to countering early proto-gnostic cults that had arisen in the first century, all claiming to be "Christian" (1 Jn. 2:19).

The early Church in the post-apostolic period was much more involved in "defending its frontiers against alternative soteriologies — either by defining its own position with greater and greater exactness, or by attacking other religions, and particularly the Hellenistic mysteries." In fact, a good deal of the early Christian literature is devoted to the exposure and refutation of unorthodox theology, mystery religions and Gnostic groups. Irenaeus, Tertullian and Hippolytus of Rome were among the greatest early Christian apologetes who engaged in critical analyses of unorthodox theology, Greco-Roman pagan religions, and Gnostic groups.

In the Protestant traditions some of the earliest writings opposing unorthodox groups like Swedenborg's teachings, can be traced back to John Wesley, Alexander Campbell and Princeton Theological Seminary theologians like Charles Hodge and B. B. Warfield. The first known usage of the term "cult" by a Protestant apologist to denote a group is heretical or unorthodox is in "Anti-Christian Cults" by A. H. Barrington, published in 1898.

Quite a few of the pioneering apologists were Baptist pastors, like I. M. Haldeman, or participants in the Plymouth Brethren, like William C. Irvine and Sydney Watson. Watson wrote a series of didactic novels like "Escaped from the Snare: Christian Science", "Bewitched by Spiritualism", and "The Gilded Lie", as warnings of the dangers posed by cultic groups. Watson's use of fiction to counter the cults has been repeated by later novelists like Frank E. Peretti.

The early twentieth century apologists generally applied the words "heresy" and "sects" to groups like the Christadelphians, Mormons, Jehovah's Witnesses, Spiritualists, and Theosophists. This was reflected in several chapters contributed to the multi-volume work released in 1915 "The Fundamentals", where apologists criticised the teachings of Charles Taze Russell, Mary Baker Eddy, the Mormons and Spiritualists.

Since the 1940s, the approach of traditional Christians was to apply the meaning of "cult" such that it included those religious groups who use other scriptures beside the Bible or have teachings and practices deviating from traditional Christian teachings and practices. Some examples of sources (with published dates where known) that documented this approach are:


One of the first prominent counter-cult apologists was Jan Karel van Baalen (1890–1968), an ordained minister in the Christian Reformed Church in North America. His book, "The Chaos of Cults", which was first published in 1938, became a classic in the field as it was repeatedly revised and updated until 1962.

Historically, one of the most important protagonists of the movement was Walter Martin (1928–89), whose numerous books include the 1955 "The Rise of the Cults: An Introductory Guide to the Non-Christian Cults" and the 1965 "The Kingdom of the Cults: An Analysis of Major Cult Systems in the Present Christian Era", which continues to be influential. He became well known in conservative Christian circles through a radio program, "The Bible Answer Man", currently hosted by Hank Hanegraaff.

In "The Rise of the Cults" Martin gave the following definition of a cult:

By cultism we mean the adherence to doctrines which are pointedly contradictory to orthodox Christianity and which yet claim the distinction of either tracing their origin to orthodox sources or of being in essential harmony with those sources. Cultism, in short, is any major deviation from orthodox Christianity relative to the cardinal doctrines of the Christian faith.

As Martin's definition suggests, the countercult ministries concentrate on non-traditional groups that claim to be Christian, so chief targets have been The Church of Jesus Christ of Latter-day Saints (i.e., "Mormons"), Jehovah's Witnesses, Armstrongism, Christian Science and the Unification Church, but also smaller groups like the Swedenborgian Church

Various other conservative Christian leaders—among them John Ankerberg and Norman Geisler—have emphasized themes similar to Martin's. Perhaps more importantly, numerous other well-known conservative Christian leaders as well as many conservative pastors have accepted Martin's definition of a cult as well as his understanding of the groups to which he gave that label. Dave Breese summed up this kind of definition in these words:
A cult is a religious perversion. It is a belief and practice in the world of religion which calls for devotion to a religious view or leader centered in false doctrine. It is an organized heresy. A cult may take many forms but it is basically a religious movement which distorts or warps orthodox faith to the point where truth becomes perverted into a lie. A cult is impossible to define except against the absolute standard of the teaching of Holy Scripture.

Kenne "Ken" Silva is said by other discernment bloggers to have pioneered online discernment ministry. Ken was a Baptist pastor who ran the discernment blog Apprising. Silva wrote many blog articles about the Emerging Church, the Word of Faith Movement, Mormonism, the Jehovah's Witness, the Gay Christian Movement, and many other groups. He started his blog in 2005 and wrote there until his death in 2014.

Silva's work paved the way for other internet discernment ministries such as Pirate Christian Radio, a group of blogs and podcasts founded by Lutheran Pastor Chris Rosebrough in 2008, and Pulpit & Pen, a discernment blog founded by Baptist Pastor and polemicist J.D. Hall.

Since the 1980s the term "new religions" or "new religious movements" has slowly entered into Evangelical usage alongside the word "cult". Some book titles use both terms.

The acceptance of these alternatives to the word "cult" in Evangelicalism reflects, in part, the wider usage of such language in the sociology of religion.

The term "countercult apologetics" first appeared in Protestant Evangelical literature as a self-designation in the late 1970s and early 1980s in articles by Ronald Enroth and David Fetcho, and by Walter Martin in "Martin Speaks Out on the Cults". A mid-1980s debate about apologetic methodology between Ronald Enroth and J. Gordon Melton, led the latter to place more emphasis in his publications on differentiating the Christian countercult from the secular anti-cult. Eric Pement urged Melton to adopt the label "Christian countercult", and since the early 1990s the terms has entered into popular usage and is recognised by sociologists such as Douglas Cowan.

The only existing umbrella organization within the countercult movement in the USA is the EMNR (Evangelical Ministries to New Religions) founded in 1982 which has the evangelical Lausanne Covenant as governing document and which stresses mission, scholarship, accountability and networking.

While the greatest number of countercult ministries are found in the USA, ministries exist in Australia, Brazil, Canada, Denmark, England, Ethiopia, Germany, Hungary, Italy, Mexico, New Zealand, Philippines, Romania, Russia, Sweden, and Ukraine. A comparison between the methods employed in the USA and other nations discloses some similarities in emphasis, but also other nuances in emphasis. The similarities are that globally these ministries share a common concern about the evangelization of people in cults and new religions. There is also often a common thread of comparing orthodox doctrines and biblical passages with the teachings of the groups under examination. However, in some of the European and southern hemisphere contexts, confrontational methods of engagement are not always relied on, and dialogical approaches are sometimes advocated.

A group of organizations which originated within the context of established religion is working in more general fields of cult-awareness, especially in Europe. Their leaders are theologians, and they are often social ministries affiliated to big churches.




The phenomena of "cults" has also entered into the discourses of Christian missions and theology of religions. An initial step in this direction occurred in 1980 when the Lausanne Committee for World Evangelization convened a mini-consultation in Thailand. From that consultation a position paper was produced. The issue was revisited at the Lausanne Forum in 2004 with another paper. The latter paper adopts a different methodology to that advocated in 1980.

In the 1990s discussions in academic missions and theological journals indicate that another trajectory is emerging which reflects the influence of contextual missions theory. Advocates of this approach maintain that apologetics as a tool needs to be retained, but do not favour a confrontational style of engagement.

Countercult apologetics has several variations and methods employed in analysing and responding to cults. The different nuances in countercult apologetics have been discussed by John A. Saliba and Philip Johnson.

The dominant method is the emphasis on detecting unorthodox or heretical doctrines and contrasting those with orthodox interpretations of the Bible and early creedal documents. Some apologists, such as Francis J. Beckwith, have emphasised a philosophical approach, pointing out logical, epistemological and metaphysical problems within the teachings of a particular group. Another approach involves former members of cultic groups recounting their spiritual autobiographies, which highlight experiences of disenchantment with the group, unanswered questions and doubts about commitment to the group, culminating in the person's conversion to Evangelical Christianity.

Apologists like Dave Hunt in "Peace, Prosperity and the Coming Holocaust" and Hal Lindsey in "The Terminal Generation" have tended to interpret the phenomena of cults as part of the burgeoning evidence of signs that Christ's Second Advent is close at hand. Both Hunt, and Constance Cumbey, have applied a conspiracy model to interpreting the emergence of New Age spirituality and linking that to speculations about fulfilled prophecies heralding Christ's reappearance.






</doc>
<doc id="7732" url="https://en.wikipedia.org/wiki?curid=7732" title="Professor X">
Professor X

Professor Charles Francis Xavier (colloquial: Professor X) is a fictional character appearing in American comic books published by Marvel Comics and is the founder and leader of the X-Men. Created by writer Stan Lee and artist Jack Kirby, the character first appeared in "The X-Men" #1 (September 1963).

Xavier is a member of a subspecies of humans known as mutants, who are born with superhuman abilities. The founder of the X-Men, Xavier is an exceptionally powerful telepath who can read and control the minds of others. To both shelter and train mutants from around the world, he runs a private school in Salem Center, located in Westchester County, New York. Xavier also strives to serve a greater good by promoting peaceful coexistence and equality between humans and mutants in a world where zealous anti-mutant bigotry is widespread.

Throughout much of the character's history in comics, Xavier is a paraplegic variously using either a wheelchair or a modified version of one. One of the world's most powerful mutant telepaths, Xavier is a scientific genius and a leading authority in genetics. Furthermore, he has shown noteworthy talents in devising equipment to greatly enhance psionic powers. Xavier is perhaps best known in this regard for the creation of a device called Cerebro, a technology that serves to detect and track those individuals possessing the mutant gene, at the same time greatly expanding the gifts of those with existing psionic abilities.

From a social policy and philosophical perspective, Xavier deeply resents the violent methods of those like his former close friend and occasional enemy, the supervillain Magneto. Instead, he has presented his platform of uncompromising pacifism to see his dream to fruition - one that seeks to live harmoniously alongside humanity, just the same as it desires full-fledged civil rights and equality for all mutants. Xavier's actions and goals in life have therefore often been compared to those of Martin Luther King Jr. for his involvement with the American civil rights struggle, whereas Magneto is often compared with the more militant civil rights activist Malcolm X.

The character's creation and development occurred simultaneously with the civil rights struggle, taking place in the 1960s, while Xavier's first appearance dates to 1963. The fictionalized plight in the comics of mutantkind faced with exceptional intolerance and prejudice was done in large part to better illustrate to audiences of the day what was transpiring across the United States, just the same as it also served to further promote ideals of tolerance and equality for all.

Patrick Stewart portrayed the character in seven films in the "X-Men" film series and in various video games, while James McAvoy portrayed a younger version of the character in the 2011 prequel "". Both actors reprised the role in the film "". McAvoy continued the role in "", and Stewart appeared in "Logan". McAvoy’s Professor X made a cameo in Deadpool 2. McAvoy will reprise his role in "Dark Phoenix".

Created by writer Stan Lee and artist/co-writer Jack Kirby, Professor X first appeared in "X-Men" #1 (September 1963).

Stan Lee has stated that the physical inspiration of Professor Xavier was from Academy Award–winning actor Yul Brynner. Professor Xavier's character development has been inspired by Martin Luther King, Jr.

Writer Scott Lobdell established Xavier's middle name to be Francis in "Uncanny X-Men" #328 (January 1996).

Xavier’s goals are to promote the peaceful affirmation of mutant rights, to mediate the co-existence of mutants and humans, to protect mutants from violent humans, and to protect society from antagonistic mutants, including his old friend, Magneto. To achieve these aims, he founded Xavier's School for Gifted Youngsters (later named the Xavier Institute) to teach mutants to explore and control their powers. Its first group of students was the original X-Men (Cyclops, Iceman, Marvel Girl, Angel, and Beast). Xavier's students consider him a visionary and often refer to their mission as "Xavier's dream". He is highly regarded by others in the Marvel Universe, respected by various governments, and trusted by several other superhero teams, including the Avengers and the Fantastic Four. However, he also has a manipulative streak which has resulted in several significant fallings-out with allies and students.

He often acts as a public advocate for mutant rights and is the authority most of the Marvel superhero community turns to for advice on mutants. Despite this, his status as a mutant himself and originator of the X-Men only became public during the 2001 story "E Is for Extinction". He also appears in almost all of the X-Men animated series and in many video games, although usually as a non-playable character because of his disability. Patrick Stewart plays him in the 2000s film series, as well as providing his voice in some of the X-Men video games (including some not connected to the film series).
According to "BusinessWeek", Charles Xavier is listed as one of the top ten most intelligent fictional characters in American comics.

In a number of comics, Xavier is shown to have a dark side, a part of himself that he struggles to suppress. Perhaps the most notable appearance of this character element is in the Onslaught storyline, in which the crossover event's antagonist is a physical manifestation of that dark side. Also, Onslaught is created in the most violent act Xavier claims to have done: erasing the mind of Magneto. In "X-Men" #106 (August 1977), the new X-Men fight images of the original team, which have been created by what Xavier says is his "evil self ... who would use his powers for personal gain and conquest", which he says he is normally able to keep in check. In the 1984 four-part series titled "The X-Men and the Micronauts", Xavier's dark desires manifest themselves as the Entity and threaten to destroy the Micronauts' universe.

In other instances, Xavier is shown to be secretive and manipulative. During the Onslaught storyline, the X-Men find Xavier's files, the "Xavier Protocols", which detail how to kill many of the characters, including Xavier himself, should the need ever arise, such as if they went rogue. "Astonishing X-Men" vol. 3, #12 (August 2005) reveals that when Xavier realizes that the Danger Room has become sentient, he keeps it trapped and experiments on it for years, an act that Cyclops calls "the oppression of a new life" and equates to humanity's treatment of mutants (however, "X-Men Legacy" #220 - 224 reveals that Xavier did not intend for the Danger Room to become sentient: it was an accident, and Xavier sought a way to free Danger, but was unable to find a way to accomplish this without deleting her sentience as well).

Charles Francis Xavier was born in New York City to the wealthy Dr. Brian Xavier, a well-respected nuclear scientist, and Sharon Xavier. The family lives on a very grand mansion estate in Westchester County because of the riches his father's nuclear research has brought them. He later grows up to attend Oxford University, where he earns a Professorship in Genetics and other science fields, and goes on to live in first Oxford and then London for a number of years. Crucially, as he enters late adolescence, Xavier inherits the mansion-house he was raised in, enabling him not only to continue to live in it, but also to turn it in to Xavier's School For Gifted Youngsters, which he begins together with the first of the X-Men.

Brian, his father, dies in an accident when Xavier is still very young, and Brian's science partner Kurt Marko comforts and then marries the grieving Sharon. When Xavier's telepathic mutant powers emerge, he discovers Marko cares only about his mother's money.
After the wedding, Kurt moves in with the Xaviers, bringing with him his son Cain. Kurt quickly grows neglectful of Sharon, driving her to alcoholism, and abuses both Charles and Cain. Cain takes out his frustrations and insecurities on his stepbrother. Charles uses his telepathic powers to read Cain's mind and explore the extent of his psychological damage, which only leads to Cain becoming more aggressive toward him and the young Xavier feeling Cain's pain firsthand.

Sharon dies soon after, and a fight erupts between Cain and Charles that causes some of Kurt's lab equipment to explode. Mortally wounded, Kurt drags the two children out before dying, and admits he was partly responsible for Brian's death.

With help from his superhuman powers and natural genius, Xavier becomes an excellent student and athlete, though he gives up the latter, believing his powers give him an unfair advantage. Due to his powers, by the time he graduates from high school, Charles loses all of his hair. He graduates with honors at the age of 16 from Bard College. In graduate studies, he receives Ph.D.s in Genetics, Biophysics, Psychology, and Anthropology with a two-year residence at Pembroke College, Oxford University. He also receives an M.D. in Psychiatry while spending several years in London. He is later appointed Adjunct Professor at Columbia University. "Origins of Marvel Comics: X-Men" #1 (2010) presents a different version of events, suggesting a scholarship to Oxford University rescued him from his abusive home, after which he "never looked back", suggesting he began his academic career as a very young man at Oxford. His stepbrother is resentful of him.

At graduate school, he meets a Scottish girl named Moira Kinross, a fellow genetics student with whom he falls in love. The two agree to get married, but soon, Xavier is drafted into the Korean War. He carves himself a niche as a soldier in search and rescue missions alongside Shadowcat's father, Carmen Pryde, and witnesses Cain's transformation into Juggernaut when he touches a ruby with an inscription on it in an underground temple. During the war, he receives a letter from Moira telling him that she is breaking up with him. He later discovers that Moira married her old boyfriend Joseph MacTaggert, who abuses her.

Deeply depressed when Moira broke off their engagement without explanation, Xavier began traveling around the world as an adventurer after leaving the army. In Cairo, he meets a young girl named Ororo Munroe (later known as Storm), who is a pickpocket, and the Shadow King, a powerful mutant who is posing as Egyptian crime lord Amahl Farouk. Xavier defeats the Shadow King, barely escaping with his life. This encounter leads to Xavier's decision to devote his life to protecting humanity from evil mutants and safeguarding innocent mutants from human oppression.
Xavier visits his friend Daniel Shomron, who runs a clinic for traumatized Holocaust victims in Haifa, Israel. There, he meets a man going by the name of Magnus (who would later become Magneto), a Holocaust survivor who works as a volunteer in the clinic, and Gabrielle Haller, a woman driven into a catatonic coma by the trauma she experienced. Xavier uses his mental powers to break her out of her catatonia and the two fall in love. Xavier and Magneto become good friends, although neither immediately reveals to the other that he is a mutant. The two hold lengthy debates hypothesizing what will happen if humanity is faced with a new super-powered race of humans. While Xavier is optimistic, Magneto's experiences in the Holocaust lead him to believe that humanity will ultimately oppress the new race of humans as they have done with other minorities. The two friends reveal their powers to each other when they fight Nazi Baron Wolfgang von Strucker and his Hydra agents, who kidnap Gabrielle because she knows the location of their secret cache of gold. Magneto attempts to kill Strucker but Xavier stops him. Realizing that his and Xavier's views on mutant-human relations are incompatible, Magneto leaves with the gold. Charles stays in Israel for some time, but he and Gabrielle separate on good terms, neither knowing that she is pregnant with his son, who grows up to become the mutant Legion.

In a strange town near the Himalayas, Xavier encounters an alien calling himself Lucifer, the advance scout for an invasion by his race, and foils his plans. In retaliation, Lucifer drops a huge stone block on Xavier, crippling his legs. After Lucifer leaves, a young woman named Sage hears Xavier's telepathic cries for help and rescues him, bringing him to safety, beginning a long alliance between the two.

In a hospital in India, he is brought to an American nurse, Amelia Voght, who looks after him and, as she sees to his recovery, they fall in love. When he is released from the hospital, the two moved into an apartment in Bombay together. Amelia is troubled to find Charles studying mutation, as she is a mutant and unsettled by it, though she calms when he reveals himself to be a mutant as well. They eventually move to the United States, living on Xavier's family estate. But the night Scott Summers moves into Xavier's mansion, Amelia leaves him, believing Charles would have changed his view and that mutants should lie low. Yet he is recruiting them to what she believes is a lost cause. Charles tries to force her to stay with his mental powers, but immediately ashamed by this, lets her go. She later becomes a disciple of Magneto.

Over the years, Charles makes a name for himself as geneticist and psychologist, apparently renowned enough that the Greys were referred to him when no other expert could help their catatonic daughter, Jean. Xavier trains her in the use of her telekinesis, while inhibiting her telepathic abilities until she matures. Around this time, he also starts working with fellow mutation expert, Karl Lykos, as well as Moira MacTaggert again, who built a mutant research station on Muir Island. Apparently, Charles had gotten over Moira in his travels to the Greek island of Kirinos. Xavier discusses his candidates for recruitment to his personal strike force, the X-Men, with Moira, including those he passes over, which are Kurt Wagner, Piotr Rasputin, Pietro and Wanda Maximoff, and Ororo Munroe. Xavier also trains Tessa in order to spy on Sebastian Shaw.

Xavier founded Xavier's School for Gifted Youngsters, which provides a safe haven for mutants and teaches them to master their abilities. In addition, he seeks to foster mutant-human relations by providing his superhero team, the X-Men, as an example of mutants acting in good faith, as he told FBI agent Fred Duncan. With his inherited fortune, he uses his ancestral mansion at 1407 Graymalkin Lane in Salem Center, Westchester County, New York as a base of operations with technologically advanced facilities, including the Danger Room - later, Fantomex mentions that Xavier is a billionaire with a net worth of $3.5 billion. Presenting the image of a stern teacher, Xavier makes his students endure a rigorous training regime.

Xavier's first five students are Marvel Girl, Cyclops, Beast, Iceman, and Angel, who become the original X-Men. After he completes recruiting the original team of X-Men, he sends them into battle with Magneto.

Throughout most of his time with the team, Xavier uses his telepathic powers to keep in constant contact with his students and provides instructions and advice when needed. In addition, he uses a special machine called Cerebro, which enhances his ability to detect mutants and to allow the team to find new students in need of the school.

Among the obstacles Xavier faces is his old friend, Magneto, who has grown into an advocate of mutant superiority since their last encounter and who believes the only solution to mutant persecution is domination over humanity.

When anthropologist Bolivar Trask resurfaces the "mutant problem", Xavier counters him in a televised debate, however, he appears arrogant and Trask sends his mutant-hunting robot Sentinels to terrorize mutants. The X-Men dispatch them, but Trask sees the error in his ways too late as he is killed by his creations.

At one point, Xavier seemingly dies during the X-Men's battle with the sub-human Grotesk, but it is later revealed that Xavier arranged for a reformed former villain named Changeling to impersonate him while he went into hiding to plan a defense against an invasion by the extraterrestrial Z'Nox, imparting a portion of his telepathic abilities to the Changeling to complete the disguise.

When the X-Men are captured by the sentient island Krakoa, Xavier assembles a new team to rescue them, including Cyclops' and Havok's long-lost brother, Vulcan, along with Darwin, Petra, and Sway. This new team, composed of students of Dr. Moira MacTaggert, was sent to rescue the original X-Men from Krakoa. However, after rescuing Cyclops, McTaggert's former students were seemingly killed. Upon Cyclops' return, Xavier removed Cyclops' memories of the death of Vulcan and his teammates; and began assembling yet another team of X-Men.

Xavier's subsequent rescue team consists of Banshee, Colossus, Sunfire, Nightcrawler, Storm, Wolverine, and Thunderbird. After the mission, the older team of X-Men, except for Cyclops, leave the school, believing they no longer belong there, and Xavier mentors the new X-Men.

Xavier forms a psychic bond across galaxies with Princess Lilandra from the Shi'ar Empire. When they finally meet, it is love at first sight. She implores the professor to stop her mad brother, Shi'ar Emperor D'Ken, and he instantly aids her by deploying his X-Men. When Jean Grey returns from the Savage Land to tell him that all the X-Men are dead, he shuts down the school and travels with Lilandra to her kingdom, where she is crowned Empress and he is treated like a child or a trophy husband.

Xavier senses the changes taking place in Jean Grey, and returns to Earth to help and resume leadership of the X-Men. Shortly thereafter he battles his pupil after she becomes Dark Phoenix and destroys a populated planet in the Shi'ar Empire. It hurts Xavier to be on the opposite side of Lilandra, but he has no other choice but to challenge the Shi'ar Imperial Guard to a duel over the fate of the Phoenix. Xavier would have lost against the greater power of the Dark Phoenix, but thanks to the help Jean Grey gives him (fighting her Phoenix persona), Xavier emerges victorious; she later commits suicide in order to prevent herself from endangering more innocent lives.

When the X-Men fight members of the extraterrestrial race known as the Brood, Xavier is captured by them, and implanted with a Brood egg, which places Xavier under the Brood's control. During this time, Xavier assembles a team of younger mutants called the New Mutants, secretly intended to be prime hosts for reproduction of the aliens. The X-Men discover this and return to free Xavier, but they are too late to prevent his body from being destroyed with a Brood Queen in its place; however, his soul remains intact. The X-Men and Starjammers subdue this monstrous creature containing Xavier's essence, but the only way to restore him is to clone a new body using tissue samples he donated to the Starjammers and transfer his consciousness into the clone body. This new body possesses functional legs, though the psychosomatic pain Xavier experienced after living so long as a paraplegic takes some time to subside. Subsequently, he even joins the X-Men in the field, but later decides not to continue this practice after realizing that his place is at the school, as the teacher of the New Mutants.

After taking a teaching position at Columbia University in "Uncanny X-Men" #192, Xavier is severely injured and left for dead as the victim of a hate crime. Callisto and her Morlocks, a group of underground-dwelling mutants, get him to safety. One of the Morlocks partially restores Xavier's health, but Callisto warns Xavier that he is not fully healed and that he must spend more time recuperating and restrain himself from exerting his full strength or powers, or his health might fail again. Xavier hides his injuries from the others and resumes his life.

Charles meets with former lover Gabrielle Haller on Muir Isle and discovers that they had a child. The boy, David, is autistic and suffers from multiple personality disorder and has vast psionic powers like his father. After helping him and his team to escape from David's mind, Xavier promises he will always be there for him.

A reformed Magneto is arrested and put on trial. Xavier attends the trial to defend his friend. Andrea and Andreas Strucker, the children of presumed dead Baron von Strucker, crash the courtroom to attack Magneto and Xavier. Xavier is seriously injured. Dying, he asks a shocked Magneto to look after the X-Men for him. Lilandra, who has a psychic bond with Xavier, feels that he is in great danger and heads to Earth. There, she and Corsair take Xavier with them so Shi'ar advanced technology can heal him.

Xavier leaves Magneto in charge of the school, but some of the X-Men are unwilling to forgive their former enemy. Cyclops loses a duel for the leadership of the X-Men against Storm, then leaves them and joins the other four original X-Men to form a new team called X-Factor.

In the meantime, Charles becomes stranded in space with the Starjammers, but he is reunited with his lover Lilandra and relishes his carefree lifestyle. He serves as a member of the Starjammers aboard the starship "Starjammer", mobile in the Shi'ar Galaxy. He becomes consort to the Princess-Majestrix Lilandra while in exile, and when she later resumes her throne he takes up residence with her in the Imperial palace on the Shi'ar homeworld. Xavier joins Lilandra in her cause to overthrow her sister Deathbird, taking on the powers of Phoenix temporarily wherein he is named Bald Phoenix by Corsair, but sees that he must return to help the X-Men.

Xavier eventually becomes imprisoned by the Skrulls during their attempted invasion of the Shi'ar Empire. Xavier breaks free from imprisonment by Warskrull Prime, and is reunited with the X-Men. A healthy Xavier returns from the Shi'ar Empire and is reunited with both the current and original X-Men teams, and resumes his leadership responsibilities of the united teams. In a battle with his old foe, the Shadow King, in the "Muir Island Saga", Xavier's spine is shattered, returning him to his former paraplegic state, while his son David is seemingly killed. In the following months, Xavier rebuilds the mansion, which previously was rebuilt with Shi'ar technology, and restructures the X-Men into two teams.

While holding a mutant rights speech, Xavier is nearly assassinated by Stryfe in the guise of Cable, being infected with a fatal techno-organic virus. For reasons of his own, the villain Apocalypse saves him. As a temporary side-effect, he gains full use of his legs and devotes his precious time to the youngest recruit on his team, Jubilee.

With all his students now highly trained adults, Professor Xavier renames his school the Xavier Institute For Higher Learning. Also, he assumes control of a private institution, the Massachusetts Academy, making it a new School for Gifted Youngsters. Another group of young mutants is trained here, Generation X, with Banshee and Emma Frost as headmaster and headmistress, respectively.

Professor X is for a time the unknowing host of the evil psionic entity Onslaught, the result of a previous battle with Magneto. In that battle, Magneto uses his powers to rip out the adamantium bonded to Wolverine's skeleton, and a furious Xavier wipes Magneto's mind, leaving him in a coma. From the psychic trauma of Xavier using his powers so violently and the mixing of Magneto's and Xavier's repressed anger, Onslaught is born. Onslaught wreaks havoc, destroying much of Manhattan, until many of Marvel's superheroes—including the Avengers, the Fantastic Four and the Hulk—destroy him. Xavier is left without his telepathy and, overcome with guilt, leaves the X-Men and is incarcerated for his actions. He later returns to the X-Men after "", in which he is shocked by the cruel act of being turned over to the mutant-hating Bastion, following a clash with the sentient Cerebro and a team of impostor X-Men.

Xavier questions his dream again and Magneto shortly thereafter is confronted by the X-Men. After the battle, the UN concedes Genosha to Magnus, and Wolverine is angered by Xavier stopping him from getting his revenge on Magneto. Charles and Logan are later trapped in a dimension with different laws of physics, wherein they have to coordinate their moves together and, in the process, gain a better understanding of the other's views.

Apocalypse kidnaps the fabled "Twelve" special mutants (Xavier included) whose combined energies would grant him omnipotence. After Apocalypse's defeat with the help of Skrull mutants, Xavier goes with the young Skrulls known as Cadre K to train them and free them from their oppressors, and eventually returns to aid in Legacy Virus research.

Mystique and her Brotherhood start a deadly assault on Muir Isle by releasing an altered form of the Legacy Virus, all in retaliation against the election campaign of Robert Kelly, a seeming mutant-hater. Mystique blows up Moira MacTaggert's laboratory complex, fatally wounding her. Charles goes to the astral plane to meet with her and retrieve information on the cure to the Legacy Virus, but after gathering the information does not want to leave her alone. If not for Jean and Cable talking him down and pulling him back, the professor would have died with his first love, who states she has no regrets.

As Beast cures the Legacy Virus, many infected Genoshan mutants recover overnight, providing Magneto, the current ruler of Genosha, with an army to start the third World War. He demands Earth's governments accept him as their leader, and abducts and crucifies Xavier in Magda Square for all to see. A loyal member of Magneto's Acolytes, Amelia Voght, can't stand to see her former lover punished in such a manner and sets him free. Jean Grey and rather untrained newcomers, as most of the team are elsewhere, distract Magneto and Wolverine guts him. Xavier is too late to intervene.

Xavier's evil twin Cassandra Nova, whom Xavier attempted to kill while they were both in their mother's womb, orders a group of rogue Sentinels to destroy the independent mutant nation of Genosha. Magneto, who is Genosha's leader, appears to die along with the vast majority of the nation's inhabitants. Nova then takes over Xavier's body. Posing as Xavier, she reveals his mutation to the world, something he needed to do but did not want to sully his reputation over, before going into space and crippling the Shi'ar Empire. The X-Men restore Xavier, but Lilandra, believing that too much disaster has come from the Shi'ar's involvement with the X-Men, annuls her marriage to Xavier. Lilandra previously had gone insane and tried to assassinate Charles on a trip to Mumbai. During this period, a mutant named Xorn joins the X-Men. Xorn uses his healing power to restore Xavier's use of his legs.

When the X-Men receive a distress call from a Scottish island, they are surprised to find Juggernaut with nowhere to go, as the island was destroyed by his further-mutated partner in crime, Black Tom Cassidy, who died. Xavier reaches out to his stepbrother and offers him a place in his mansion, with Cain reluctantly accepting. The Juggernaut redeems himself over the next few weeks and joins the X-Men. Xavier finds out that Cain's father preferred him to his own flesh and blood and that they both thought they deserved the abuse they incurred by Kurt; Cain believed this because his father loved someone else's child more than him, and Charles felt guilty about getting in the way. That it is why neither of them stopped Kurt Marko with their powers.

Now outed as a mutant, Xavier makes speeches to the public about mutant tolerance. He also founds the X-Corporation, or X-Corp (not to be confused with the X-Corps), with offices all over the world. The purpose of the X-Corp is to watch over mutant rights and help mutants in need. As a result of being out, the school no longer hides the fact that it is a school for mutants and it opens its doors for more mutant (and even human) students to come in. A student named Quentin Quire and members of his gang start a riot at the Xavier Institute during an open house at the school. As a result, Quire and two other students are killed. Uncertain about his dream's validity, Xavier announces that he will step down as headmaster and be succeeded by Jean Grey. Afterwards, Xorn reveals himself to be Magneto, having apparently not died in the Sentinel raid on Genosha. Magneto undoes the restoration of Xavier's ability to walk, kidnaps him, and destroys the X-Mansion (killing several of the students). Then Xorn/Magneto assaults New York, where Cyclops, Fantomex and a few students confront him. After the rest of the X-Men arrive, Xorn/Magneto kills Jean Grey with an electromagnetically induced stroke, and Wolverine decapitates him. With Jean dead, Xavier leaves the school to Cyclops and Emma Frost, to bury Xorn/Magneto in Genosha. In a retcon of Grant Morrison's storyline, there Xavier meets the "real" Magneto, who mysteriously survived Cassandra Nova's assault. The two resolve their differences and attempt to restore their friendship, leading a team of mutants, the Genoshan Excalibur, to rebuild and restore order to the destroyed island nation.

At the mansion, the Danger Room (the X-Men’s simulated reality training chamber) gains sentience, christens itself "Danger", assumes a humanoid form, and attacks the X-Men before leaving to kill Xavier. With Magneto's help, Xavier holds off Danger until the X-Men arrive. Danger flees, but not before revealing to Colossus that Xavier has known it to be sentient ever since he upgraded it. Colossus is especially offended by this because he had been held captive and experimented upon by Danger's ally, Ord of the Breakworld. Ashamed, Xavier tries to explain to them that by the time he realized what was happening, he could see no other course. The disgusted X-Men leave.

In a prelude to "House of M", Magneto's daughter Scarlet Witch suffers a mental breakdown and causes the death of several Avengers. Magneto brings her to Xavier and asks him to use his mental powers to help her. Although aided by Doctor Strange and the appearance of Cassandra Nova, Xavier is unsuccessful. Xavier orders a meeting of the X-Men and Avengers to decide Wanda's fate. Her brother Quicksilver, believing the heroes plan to kill her, speeds off to Genosha and convinces Wanda that she could right the wrongs she inflicted by using her powers to alter reality.

Quicksilver somehow forces a tearful Wanda to reveal to him her heart's desires of Magneto, the assembled New Avengers, and the X-Men, and then uses her powers to make them all real. Thanks to Magneto, though, this re-imagined world is a place where a much more numerous mutant-kind are the dominant species, humans a disenfranchised and oppressed 'silent majority', and Magneto himself rules supreme. In this reality, the only proof that Charles Xavier ever existed is a secret monument in Magneto's palace garden, with the engraved message "He died so Genosha could live".

After mutant Layla Miller restores the memories of some of the X-Men and Avengers, they head to Genosha where they discover that Magneto has erected a memorial garden for Xavier commemorating his death. Emma is horrified until Cloak fades into the grave and discovers there is no body inside. After a battle, Scarlet Witch again uses her powers to restore reality and, as a slight against her father, causes a large majority mutants to lose their powers, leaving the mutant race on the brink of extinction and causing the lost powers to become an energy mass, the Collective. With reality restored, Xavier is still missing and the X-Men are unable to detect him with Cerebro".

Xavier returns when Cyclops' and Havok's long-lost brother, Vulcan, is revived by the Collective energy released as a result of the "House of M" incident. Vulcan then attacks the X-Men. Xavier, now depowered but able to walk in the wake of "House of M", reveals that he had gathered and trained another team of X-Men (this one composed of students of Dr. Moira MacTaggert) sometime between the original team and the new X-Men team introduced in "Giant Size X-Men" #1. This team included Vulcan as a member. Like the "Giant Size" X-Men team, McTaggert's former students were sent to rescue the original X-Men from Krakoa, the living island. However, after rescuing Cyclops, McTaggert's former students were seemingly killed. Upon Cyclops' return, Xavier removed Cyclops' memories of the death of Vulcan and his teammates and began assembling the "Giant Size" X-Men. Vulcan skirmishes with the X-Men and eventually flees into space.

In spite of Cyclops' feelings, Xavier forms a new team including Havok and Darwin, the lone other survivors of Moira's students. Xavier seeks to confront Vulcan before he can enact his vengeance against the Shi'ar empire, which killed Vulcan's mother. While en route to the Shi'ar homeworld, Xavier is abducted and is later thrown into the M'Kraan Crystal by Vulcan. Darwin follows Xavier into the crystal and pulls Xavier out. This somehow restores Xavier's lost telepathy. With help from his longtime lover, Lilandra, Xavier escapes back to Earth with several of his X-Men.

Upon Xavier's return to Earth, as seen in the "World War Hulk" storyline, he begins to search for lost mutants such as Magneto. Charles' search for more mutants is interrupted by the Hulk, who was sent into extraterrestrial exile by the Illuminati, a group of powerful superbeings to which Xavier belongs. Xavier had no part in (and did not know of) the Hulk exile decision, but Xavier admits to Hulk that he would have concurred to a temporary exile so Bruce Banner could be cured of transforming into the Hulk. However, he also tells the Hulk he would not have agreed to permanent exile. Xavier attempts to surrender to the Hulk, but after viewing the X-Mansion's large graveyard dedicated to post-M-Day mutant deaths, The Hulk concludes the mutants have suffered enough and leaves the Mansion grounds on his own accord. While the X-Men tend to the wounded, Cyclops finally forgives Professor X.

While using Cerebra and talking to Beast during the "" storyline, Charles detects a new mutant so powerful it fries Cerebra's system. He asks Cyclops to send out a team to find out about the mutant. Once the team has come back empty handed, he argues with Scott for not telling him about the team he deployed to find former Acolytes. Scott tells him outright that he doesn't need him to run the X-Men anymore. This upsets Charles and annoys him later on when he overhears Cyclops briefing X-Factor on the situation. He also approaches the New X-Men in an attempt to help them figure out a non-violent way to help against the Purifiers, but is quickly rebuked by Surge, who questions where he was when they were getting attacked the first time, and that they didn't need to learn from him. Charles questions Cyclops' decision to send X-Force to hunt down his own son, Cable, in front of the students. Cyclops then tells Xavier that he is a distraction that will keep getting in the way and that he must leave the mansion. Xavier is contacted by Cable, who lost the mutant newborn to the traitorous actions of Bishop, who in turn lost the child to the Marauders, and tells him that he is the only one who can help Cable save the future. In the final fight, Xavier is accidentally shot in the head by Bishop. Immediately afterward Xavier's body disappears and Cyclops declares that there are no more X-Men.

Professor Xavier survives Bishop's gunshot but falls into a coma. Xavier is kidnapped by Exodus, Tempo, and Karima Shapandar. Exodus tries to heal Xavier, Xavier mentally fights Exodus. Exodus finally approaches Magneto, who is apparently still depowered, for help. Magneto and Karima Shapandar are able to stir Xavier's memories and coax him out of his coma, though Xavier remains slightly confused and partly amnesiac. Later, Exodus confronts Magneto about Joanna Cargill's injury (Magneto was forced to shoot a laser through her eyeball in order to prevent her attempted an assassination of Xavier). Exodus nearly kills Magneto, and Xavier drags Exodus onto the Astral Plane, putting Xavier's own newly restored mind at stake. Xavier defeats Exodus after a harrowing psionic battle, and Exodus reveals the reason he abducted Xavier and to restore his mind: Exodus wants Xavier to lead the Acolytes and find the mutant messiah child (now under the guardianship of Cable) in order to indoctrinate the child into their cause. Xavier refuses. Emma Frost's telepathy picks up on the psychic fight, and Emma informs Cyclops that Xavier is alive. Xavier parts company with Magneto and Karima to try to regain his lost memories by visiting people from his past.

The first person Charles visits is Carter Ryking, who had gone insane after losing his powers. Charles reads Carter's memories and discovers that when the two were children they were used as test subjects by Nathan Milbury of the Black Womb Project, with the approval of Charles' father, Doctor Brian Xavier. Xavier makes the connection Milbury and X-Men villain, Mister Sinister, who has apparently long been manipulating Charles' life in addition to other X-Men. Afterwards, he discovers he has been targeted by assassins.

Charles eventually discovers Mister Sinister had set up Charles, Sebastian Shaw, Juggernaut, and Ryking as potential new hosts for Sinister's mind. Bleeding slowly to death, he apparently gives in to Sinister becoming the new Mister Sinister. But in reality, Xavier is still battling Sinister for control of his body. As Sebastian Shaw and Gambit destroy Sinister's Cronus Machine, the device that he used to transfer his consciousness into new hosts, Xavier drives Sinister out of his body permanently. Xavier thanks Shaw and Gambit for their help and declares he must go and see Cyclops immediately. Professor X returns to the X-Mansion to find it destroyed after recent events. Afterwards, Xavier leaves the ruins of the X-Mansion to secretly meet up Cyclops by psychically coercing his former student for the visit. Xavier explains to Cyclops about the recent events with Mr. Sinister and tries to explain to Cyclops how Sinister has been manipulating Scott's and Jean's lives since when they were children. Xavier attempts to have Scott give him permission to scan Scott's mind for traces of Sinister's influences, but instead, Scott turns the tables on Xavier by revealing that he has secretly invited Emma Frost into their entire meeting and also into Xavier's mind. While in his mind, Emma forces Xavier to relive each of his morally ambiguous decisions under altruistic pretenses. As the issue continues, Charles realizes his human arrogance and that while some of his decisions were morally wrong, he must move forward with his life and deal with the consequences. Emma ends her incursion into Xavier's mind by reminding him of Moira MacTaggert's last words. As he reflects on Moira's words, Xavier gives Cyclops his blessing to lead the X-Men and leaves to find his own path. Following his encounter with Wolverine (in the "Original Sin" Arc) Professor Xavier seeks out his step-brother, the unstoppable Juggernaut in an attempt to reform him. After a conversation about the meaning of the word "Juggernaut" and a review of Juggernaut and Xavier's shared history Xavier offers Cain an empty box as a gift. Confused by Xavier's gift Cain attempts to kill the Professor bringing an entire sports bar down over their heads in the process. Later Cain battles the X-Men in his full Juggernaut armor and conquers the planet. Just as everything appears to be under the Juggernaut's control Xavier reappears and informs him that everything that has just taken place except for Juggernaut destroying the bar took place in Cain's mind. A baffled Cain demands to know how Xavier managed to overcome his psychically shielded helmet to which the Professor replies that he decided to visit Cain in his sleep. Professor Xavier then informs him that he now understands Cain as a person and that he will not attempt to get in his way or reform him again. But Xavier also warns Cain that if he gets in the way of the Professor's path to redemption Xavier will stop him permanently. Following his encounter with Cain it has been revealed that Xavier is now searching for Rogue.

After his bruising encounter with Cyclops and Emma Frost, Professor X is forced to revisit the biggest challenge and the biggest failure of his career, Wolverine, when the feral mutant asks for Charles' help in freeing his son from the clutches of the Hellfire Club. As the two search for Daken, Wolverine reveals that when he first joined the X-Men he attempted to assassinate Xavier due to some unknown programming. In response, the Professor broke Logan's mind and rebuilt it so that any and all programming he received was forgotten. Logan also revealed that the real reason Xavier asked him to join the X-Men was that Charles "needed a weapon". Eventually Professor Xavier and Wolverine locate Sebastian Shaw's mansion and attack his minions, just as they are about to enter a bomb explodes from within catching them both off guard. From the wreckage emerges an angry Sebastian who immobilizes Wolverine. Meanwhile, Miss Sinister knocks Daken unconscious and has him taken to the med lab in the mansion's basement. As Shaw prepares to deliver a killing blow to Xavier, Wolverine recovers and stops him telling Xavier to rescue his son. Professor Xavier locates the med lab and after a quick psychic battle with Miss Sinister enters Daken's fractured mind. While in Daken's mind Xavier discovers Romulus's psychic tampering and comments that Daken's mind is even more broken than Wolverine's was. Before Xavier can heal Daken a psychic bomb explodes causing Xavier to become comatose and Daken to wake up. Miss Sinister arrives and attempts to manipulate Daken who reveals that the psychic bomb in his head restored his memories and stabs Miss Sinister in the chest. Meanwhile, Wolverine defeats Shaw and enters the mansion to find Daken standing over an unconscious Xavier preparing to kill him. Wolverine tells Daken that he won't let him hurt Xavier and the two fight. Overcome with guilt over what happened to Daken and Itsu, Wolverine allows himself to be beaten. Just as Daken appears to have won Xavier pulls both of them onto the astral plane revealing that the psychic bomb had little effect on him because his psyche was already shattered. Xavier then explains to Wolverine and Daken that Romulus is solely responsible for Itsu's death and that he lied to Daken about everything because he wanted Wolverine to become his weapon. As the three converse, Daken returns to the physical plane and prevents Shaw from killing Xavier. With the truth revealed Wolverine and Daken decide to kill Romulus. As the two depart Wolverine tells Xavier that he forgives him for all of the dark moments in their history. Wolverine acknowledges that Professor Xavier allowed him to become a hero. Wolverine then tells the Professor that he hopes he will one day be able to forgive him for choosing to kill Romulus.

Professor Xavier recruits Gambit to go with him to Australia to find and help Rogue who is currently staying at the X-Men's old base in the Outback; unaware Danger is using Rogue as a conduit for her revenge against him.

In a prelude to the "Secret Invasion" storyline, Professor X was at the meeting of the Illuminati when it came to the discussion about the Skrulls planning an invasion by taking out Earth's heroes and posing as them. He claims he was unable to distinguish that Black Bolt had been replaced by a Skrull, and his powers were tested quickly by the Black Bolt Skrull. Professor X leaves after learning even he can no longer trust the others, yet appears to have severely restricted the number of people he informs of the forthcoming alien invasion, as the X-Men were not prepared for the Skrulls, at least at first. Xavier has not seen again during the events of "Secret Invasion", though his X-Men in San Francisco are successful at repelling the invaders there through the use of the modified Legacy Virus.

During the "Dark Reign" storyline, Professor X convinces Exodus to disband the Acolytes. A H.A.M.M.E.R. helicopter arrives and from inside appears Norman Osborn, who wants to talk to him. During the Dark Avengers' arrival in San Francisco to enforce martial law and squelch the anti-mutant riots occurring in the city, Xavier appears (back in his wheelchair) in the company of Norman Osborn and publicly denounces Cyclops' actions and urges him to turn himself in. However, this Xavier was revealed to be Mystique who Osborn recruited to impersonate Xavier in public. The real Xavier is shown in prison on Alcatraz and slowly being stripped of his telepathic powers while in psionic contact with Beast, who was arrested earlier for his part in the anti-mutant riots. It was also revealed by Emma Frost that she and Professor X are both Omega Class Telepaths when she manages to detect the real Professor X. Professor X helps Emma Frost enter Sentry's mind. However, as Emma frees him of the Void's influence, a minute sliver of the entity itself remains in her mind. Xavier quickly tells her to remain in her diamond armor state to prevent the Void from gaining access to her psi-powers. Professor X is later seen with Emma Frost where Beast is recuperating.

After the events of "Utopia", Xavier has come to live on the risen Asteroid M, rechristened Utopia, along with the rest of the X-Men, X-Club, and mutant refugees and is also allowed to join the Utopia lead council (Cyclops, Storm, Namor, Iceman, Beast, Wolverine and Emma Frost). While he no longer continues to openly question every move that Cyclops makes, he is still concerned about some of his leadership decisions. Xavier had wanted to return to the mainland in order to clear his name, but in the aftermath of Osborn declaring Utopia as a mutant detention area, Cyclops refused to let him leave, stating that it would be a tactical advantage to have him as an ace in the hole in case the need arose. To that end, he has kept Xavier out of the field and instead relied on Emma Frost, Psylocke and the Stepford Cuckoos respectively for their own psionic talents. While attending the funeral of Yuriko Takiguchi, Magneto arrives at Utopia, apparently under peaceful motives. Xavier does not believe it, and attacks Magneto telepathically, causing Cyclops to force him to stand down. He later apologizes to Magneto for acting out of his old passions from their complicated relationship, which Magneto accepts.

During the "" storyline, Professor Xavier is seen on Utopia delivering a eulogy at Nightcrawler's funeral. Like the other X-Men, he is deeply saddened by Kurt's death and anxious about the arrival of Cable and Hope. Xavier is seen using his powers to help his son Legion control his many personalities and battle the Nimrods. At the conclusion of Second Coming Professor X is seen surveying the aftermath of the battle from a helicopter. As Hope descends to the ground and cradles Cable's lifeless arm, Xavier reflects on everything that has transpired and states that, while he feels that Hope has indeed come to save mutant kind and revive his dream, she is still only a young woman and will have a long and difficult journey before she can truly achieve her potential.

During the "Avengers vs. X-Men" storyline, the Phoenix Force is split into five pieces and bonded with Cyclops, Emma Frost, Namor, Colossus and Magik (who become known as the Phoenix Five). Eventually, Cyclops and Frost come to possess the full Phoenix Force, and Professor X is instrumental in confronting them both, and dies in the ensuing battle with Cyclops. The Phoenix Force is subsequently forced to abandon Cyclops as a host by the efforts of both Hope Summers and the Scarlet Witch.

Xavier's body is later stolen by the Red Skull's S-Men while the group also captures Rogue and Scarlet Witch. Xavier's brain is removed and fused to the brain of the Red Skull. After Rogue and Scarlet Witch snapped out of the fight they were in, they find the lobotomized body of Professor X. Red Skull uses the new powers conferred upon him by Professor X's brain to provoke anti-mutant riots. His plans are foiled by the Avengers and the X-Men, and the Skull escapes.

Professor X's spirit is later seen in the Heaven dimension along with Nightcrawler's spirit at the time when Azazel invades Heaven.

During the "AXIS" storyline, a fragment of Professor X's psyche (which had escaped the scrubbing of his memories) still existed in Red Skull's mind preventing him from unleashing the full potential of Professor X's powers. During a fight with the Stark Sentinels, Doctor Strange and Scarlet Witch attempt to cast a spell to invert the axis of Red Skull's brain and bring out the fragment of Professor X to defeat Onslaught. Doctor Strange was targeted and captured by the Sentinels before they could cast the spell. When Magneto arrived with his supervillain allies, Doctor Doom and Scarlet Witch attempted to cast the inversion spell again and Red Onslaught was knocked unconscious and reverted to his Red Skull form. Although they did not know whether Professor X was now in control, the Avengers decided to be cautious and take Red Skull to Stark Tower. It was later revealed that the spell had actually caused all the heroes and villains present to undergo a "moral inversion" rather than simply bringing out Professor X in the Skull, with the result that the Skull and other villains became heroic while the Avengers and X-Men present became villainous. Eventually, the inversion was undone. 

After the Skull mounts a telepathic assault that nearly allows him to take control of the Avengers, he is defeated when Deadpool places Magneto's old helmet on Rogue, allowing her to knock out the Skull and take him to Beast. Beast is subsequently able to perform brain surgery on the Skull, extracting the part of Xavier's brain that was grafted onto the villain's own brain without causing any apparent damage to the Skull. Rogers attempts to claim the fragment for himself, but Rogue flies up and incinerates the fragment with the aid of the Human Torch, the two expressing hope that Xavier will rest in peace.

The astral form of Professor Xavier has since been revealed to be imprisoned in the Astral Plane after Shadow King somehow acquired it upon Professor X's death. After what appeared to be years in the Astral Plane, Professor X is able to trick Shadow King into playing him in a 'game' that lures Rogue, Mystique and Fantomex onto the Astral Plane, while turning others into carriers for the Shadow King's 'contagious' psychic essence. With the Shadow King certain of his victory, he fails to realize that Xavier's apparent 'surrender' to his game was really just him biding his time until the Shadow King's influence was distracted long enough for him to drop his already-subtly-weakened guard long enough for Xavier to break his bonds, luring in the three aforementioned X-Men as their identities were already fundamentally malleable. With the Shadow King defeated, Xavier is apparently returned to the real world in the body of Fantomex, Fantomex reasoning that nobody really knows who he is as an individual beyond his status as one of the X-Men whereas this act of sacrifice will ensure that he is remembered for a great deed.

Proteus has spent years trapped in a psionic hellscape of the Astral Plane, where The Shadow King reigned surpreme, only to escape last issue. Part of the reason that he could was the escape of Charles Xavier (who now chooses to go by X, since he is now in a younger body after escaping), and now X leads the X-Men directly into an ambush, as Proteus has warped an entire village with his powers, leading to a mind-to-mind battle that leaves X on the receiving end of a psychic beatdown.

Proteus has started his garden and his seeds are planted all over the world. Psylocke is in command and has a plan which mainly consists of Archangel using metal and Mystique morphing into his mother. Once they drain him, Rogue and Bishop convert his energy and release him back to the universe. Whilst this all went down Psylocke and X combined forces to burn out the seeds across the planet. As they are working on it they discover they are not enough to accomplish the task. X mentions the network of psychics the Shadow King was using and that Betsy who is in control should tap into it. She agrees and does so yet unbeknownst to her X was possessed by the Shadow King who violently erupts from X's head.

Following X’s apparent death after the Shadow King exploded from his skull, the psychic villain tears the X-Men apart until X literally pulls himself back together (a feat he later refuses to explain), and he and Psylocke team up to harness the power of all of Earth’s psychics to destroy the Shadow King. As Psylocke says she feels no psychic trace of him anywhere, X implants comforting post-hypnotic psychic suggestions in his allies and then erases their memories (including allowing Warren Worthington to switch between his identities at will). Only Psylocke’s memory is left intact, with X telling her she’ll be the one to “keep him honest” while he embarks on a new mission.

Professor X is a mutant who possesses vast telepathic powers, and is among the strongest and most powerful telepaths in the Marvel Universe. He is able to perceive the thoughts of others or project his own thoughts within a radius of approximately . Xavier's telepathy once covered the entire world; although following this, Magneto altered the Earth's electromagnetic field to restrict Xavier's telepathic range. While not on Earth, Xavier's natural telepathic abilities have reached across space to make universal mental contact with multiple alien races. With extreme effort, he can also greatly extend the range of his telepathy. He can learn foreign languages by reading the language centers of the brain of someone adept, and alternately "teach" languages to others in the same manner. Xavier once trained a new group of mutants mentally, subjectively making them experience months of training together, while only hours passed in the real world.

Xavier's vast psionic powers enable him to manipulate the minds of others, warp perceptions to make himself seem invisible, project mental illusions, cause loss of particular memories or total amnesia, and induce pain or temporary mental and/or physical paralysis in others. Within close range, he can manipulate almost any number of minds for such simple feats. However, he can only take full possession of one other mind at a time, and must strictly be within that person's physical presence. He is one of the few telepaths skilled enough to communicate with animals and even share their perceptions. He can also telepathically take away or control people's natural bodily functions and senses, such as sight, hearing, smell, taste, or even mutant powers. A side effect of his telepathy is that he has an eidetic memory. He has displayed telepathic prowess sufficient to confront Ego the Living Planet (while aided by Cadre K) as well as narrowly defeat Exodus. However, he cannot permanently "reprogram" human minds to believe what he might want them to believe even if he wanted to do so, explaining that the mind is an organism that would always recall the steps necessary for it to reach the present and thus 'rewrite' itself to its original setting if he tried to change it. However, his initial reprogramming of Wolverine lasted several years, despite Wolverine overcoming the reprogramming much faster than an ordinary human because of his healing factor.

He is able to project from his mind 'bolts' composed of psychic energy, enabling him to stun the mind of another person into unconsciousness, inflict mental trauma, or even cause death. These 'bolts' inflict damage only upon other minds, having a negligible effect on non-mental beings, if any. The manner in which Xavier's powers function indicates that his telepathy is physical in some way, as it can be enhanced by physical means (for example, Cerebro), but can also be disrupted by physical means (for example, Magneto's alteration of the Earth's magnetic field).

Xavier can perceive the distinct mental presence/brain waves of other superhuman mutants within a small radius of himself. To detect mutants to a wider area beyond this radius, he must amplify his powers through Cerebro and subsequently Cerebra, computer devices of his own design which are sensitive to the psychic/physical energies produced by the mind.

Professor X can project his astral form into a psychic dimension known as the astral plane. There, he can use his powers to create objects, control his surroundings, and even control and destroy the astral forms of others. He cannot project this form over long distances.

"Uncanny X-Men" writer Ed Brubaker has claimed that, after being de-powered by the Scarlet Witch, and then re-powered by the M'Kraan Crystal, Charles' telepathy is more powerful than was previously known. However, the extent of this enhancement is unknown.

Charles Xavier is a genius with multiple doctorates. He is a world-renowned geneticist, a leading expert in mutation, possesses considerable knowledge of various life sciences, and is the inventor of Cerebro. He possesses Ph.D.s in Genetics, Biophysics, Psychology, and Anthropology, and an M.D. in Psychiatry. He is highly talented in devising equipment for utilizing and enhancing psionic powers. He is also a great tactician and strategist, effectively evaluating situations and devising swift responses.

During his travels in Asia, Xavier learned martial arts, acquiring "refined combat skills" according to Magneto. When these skills are coordinated in tandem with his telepathic abilities, Xavier is a dangerous unarmed combatant, capable of sensing the intentions of others and countering them with superhuman efficiency. He also has extensive knowledge of pressure points.

Charles Xavier was also given possession of the Mind Infinity Gem. It allows the user to boost mental power and access the thoughts and dreams of other beings. Backed by the Power Gem, it is possible to access all minds in existence simultaneously. Like all other former Illuminati members, Xavier has sworn to never use the gem and to keep its location hidden.

The Xavier Protocols are a set of doomsday plans created by Professor X. The protocols detail the best way to kill many powerful mutant characters, including the X-Men and Xavier himself, should they become too large of a danger. The Xavier Protocols are first mentioned during the "Onslaught" crossover and first seen in "Excalibur" #100 in Moira MacTaggert's lab. Charles Xavier compiled a list of the Earth's most powerful mutants and plans on how to defeat them if they become a threat to the world. They are first used after Onslaught grows too powerful. Only parts of the actual protocols are ever shown. In the "" crossover Bastion obtains an encrypted copy of the protocols, intending to use them against the X-Men. However, Cable infiltrates the X-Mansion and secures all encrypted files before Bastion has a chance to decrypt them. Due to the tampering of Bastion and his Sentinels, the X-Mansion computer system Cerebro gains autonomy and seeks to destroy the X-Men by employing its knowledge of the Xavier Protocols. In a virtual environment created by Professor X, Cerebro executes the Xavier Protocols against the X-Men.

Each protocol is activated by the presence of a different combination of X-Men and were written by Xavier himself:

Other X-Men who have faced their Xavier Protocols are Colossus, Rogue, Shadowcat, Nightcrawler, Storm, and Gambit.

Professor X has appeared on a number of animated television shows including the "X-Men animated series" voiced by Cedric Smith, "" voiced by David Kaye, and in "Wolverine and the X-Men" voiced by Jim Ward. 

He has appeared in nine live-action "X-Men" feature films to date. He is played by Patrick Stewart in "X-Men", "X2", "", "", "The Wolverine", and "Logan", and by James McAvoy in "" and "". Both actors play him at different time-periods in "". 

He has also appeared in a number of books and video games.




</doc>
<doc id="7734" url="https://en.wikipedia.org/wiki?curid=7734" title="Central Pacific Railroad">
Central Pacific Railroad

The Central Pacific Railroad (CPRR) was a rail route between California and Utah built eastwards from the West Coast in the 1860s, to complete the western part of the "First Transcontinental Railroad" in North America. It later became part of the Union Pacific Railroad.

Many 19th century national proposals to build a transcontinental railroad failed because of the energy consumed by political disputes over slavery. With the secession of the South, the modernizers in the Republican Party controlled the US Congress. They passed legislation authorizing the railroad, with financing in the form of government railroad bonds. These were all eventually repaid with interest. The government and the railroads both shared in the increased value of the land grants, which the railroads developed. The construction of the railroad also secured for the government the economical "safe and speedy transportation of the mails, troops, munitions of war, and public stores."

Planned by Theodore Judah, the Central Pacific Railroad was authorized by Congress in 1862. It was financed and built through "The Big Four" (who called themselves "The Associates"): Sacramento, California businessmen Leland Stanford, Collis Huntington, Charles Crocker, and Mark Hopkins. Crocker was in charge of construction. Construction crews comprised 12,000 Chinese emigrant workers by 1868, when they constituted eighty percent of the entire work force. They laid the first rails in 1863. The "Golden spike", connecting the western railroad to the Union Pacific Railroad at Promontory, Utah, was hammered on May 10, 1869. Coast-to-coast train travel in eight days became possible, replacing months-long sea voyages and lengthy, hazardous travel by wagon trains.

In 1885 the Central Pacific Railroad was leased by the Southern Pacific Company. Technically the CPRR remained a corporate entity until 1959, when it was formally merged into Southern Pacific. (It was reorganized in 1899 as the Central Pacific "Railway".) The original right-of-way is now controlled by the Union Pacific, which bought Southern Pacific in 1996.

The Union Pacific-Central Pacific (Southern Pacific) mainline followed the historic Overland Route from Omaha, Nebraska to San Francisco Bay.

Chinese labor was the most vital source for constructing the railroad. Fifty Chinese laborers were hired by the Central Pacific Railroad in February 1865, and soon more and more Chinese men were hired. Working conditions were harsh, and Chinese men were compensated less than their white counterparts. Chinese men were paid thirty-one dollars each month, and while white workers were paid the same, they were also given room and board.

Construction of the road was financed primarily by 30-year, 6% U.S. government bonds authorized by Sec. 5 of the Pacific Railroad Act of 1862. They were issued at the rate of $16,000 per mile of tracked grade completed west of the designated base of the Sierra Nevada range near Roseville, CA where California state geologist Josiah Whitney had determined were the geologic start of the Sierras' foothills. Sec. 11 of the Act also provided that the issuance of bonds "shall be treble the number per mile" (to $48,000) for tracked grade completed over and within the two mountain ranges (but limited to a total of at this rate), and "doubled" (to $32,000) per mile of completed grade laid between the two mountain ranges. The U.S. Government Bonds, which constituted a lien upon the railroads and all their fixtures, were repaid in full (and with interest) by the company as and when they became due.

Sec. 10 of the 1864 amending Pacific Railroad Act (13 Statutes at Large, 356) additionally authorized the company to issue its own "First Mortgage Bonds" in total amounts up to (but not exceeding) that of the bonds issued by the United States. Such company-issued securities had priority over the original Government Bonds. (Local and state governments also aided the financing, although the City and County of San Francisco did not do so willingly. This materially slowed early construction efforts.) Sec. 3 of the 1862 Act granted the railroads of public land for every mile laid, except where railroads ran through cities and crossed rivers. This grant was apportioned in 5 sections on alternating sides of the railroad, with each section measuring by . These grants were later doubled to per mile of grade by the 1864 Act.

Although the Pacific Railroad eventually benefited the Bay Area, the City and County of San Francisco obstructed financing it during the early years of 1863-1865. When Stanford was Governor of California, the Legislature passed on April 22, 1863, "An Act to Authorize the Board of Supervisors of the City and County of San Francisco to take and subscribe One Million Dollars to the Capital Stock of the Western Pacific Rail Road Company and the Central Pacific Rail Road Company of California and to provide for the payment of the same and other matters relating thereto" (which was later amended by Section Five of the "Compromise Act" of April 4, 1864). On May 19, 1863, the electors of the City and County of San Francisco passed this bond by a vote of 6,329 to 3,116, in a highly controversial Special Election.

The City and County's financing of the investment through the issuance and delivery of Bonds was delayed for two years, when Mayor Henry P. Coon, and the County Clerk, Wilhelm Loewy, each refused to countersign the Bonds. It took legal actions to force them to do so: in 1864 the Supreme Court of the State of California ordered them under Writs of Mandamus ("The People of the State of California "ex rel" the Central Pacific Railroad Company vs. Henry P. Coon, Mayor; Henry M. Hale, Auditor; and Joseph S. Paxson, Treasurer, of the City and County of San Francisco." 25 Cal. 635) and in 1865, a legal judgment against Loewy ("The People "ex rel" The Central Pacific Railroad Company of California vs. The Board of Supervisors of the City and County of San Francisco, and Wilhelm Lowey, Clerk" 27 Cal. 655) directing that the Bonds be countersigned and delivered. In 1863 the legislature's forcing of City and County action became known as the "Dutch Flat Swindle". Critics claimed the CPRR intended to build a railroad only as far as Dutch Flat, to connect to the Dutch Flat Wagon Road which they already controlled.

A replica of the Sacramento, California Central Pacific Railroad passenger station is part of the California State Railroad Museum, located in the Old Sacramento State Historic Park.

Nearly all the company's early correspondence is preserved at Syracuse University, as part of the Collis Huntington Papers collection. It has been released on microfilm (133 reels). The following libraries have the microfilm: University of Arizona at Tucson; and Virginia Commonwealth University at Richmond. Additional collections of manuscript letters are held at Stanford University and the Mariners' Museum at Newport News, Virginia. Alfred A. Hart was the official photographer of the CPRR construction.

The Central Pacific's first three locomotives were of the then common 4-4-0 type, although with the American Civil War raging in the east, they had difficulty acquiring engines from eastern builders, who at times only had smaller 4-2-4 or 4-2-2 types available. Until the completion of the Transcontinental rail link and the railroad's opening of its own shops, all locomotives had to be purchased by builders in the northeastern U.S. The engines had to be dismantled, loaded on a ship, which would embark on a four-month journey that went around South America's Cape Horn until arriving in Sacramento where the locomotives would be unloaded, re-assembled, and placed in service.

Locomotives at the time came from many manufacturers, such as Cooke, Schenectady, Mason, Rogers, Danforth, Norris, Booth, and McKay & Aldus, among others. The railroad had been on rather unfriendly terms with the Baldwin Locomotive Works, one of the more well-known firms. It is not clear as to the cause of this dispute, though some attribute it to the builder insisting on cash payment (though this has yet to be verified). Consequently, the railroad refused to buy engines from Baldwin, and three former Western Pacific Railroad (which the CP had absorbed in 1870) engines were the only Baldwin engines owned by the Central Pacific. The Central Pacific's dispute with Baldwin remained unresolved until well after the road had been acquired by the Southern Pacific.

In the 1870s, the road opened up its own locomotive construction facilities in Sacramento. Central Pacific's 173 was rebuilt by these shops and served as the basis for CP's engine construction. The locomotives built before the 1870s were given names as well as numbers. By the 1870s, it was decided to eliminate the names and as each engine was sent to the shops for service, their names would be removed. However, one engine that was built in the 1880s did receive a name: the El Gobernador.

Construction of the rails was often dangerous work. Towards the end of construction, almost all workers were Chinese immigrants. The ethnicity of workers depended largely on the "gang" of workers/specific area on the rails they were working.

The following CP engines have been preserved:

1861

1862
1863
1864

1865
1866

1867

1868
1869
1870
1876
1877
1883
1885
1888
1899
1959






</doc>
